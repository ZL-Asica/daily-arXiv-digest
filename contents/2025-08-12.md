# 2025-08-12

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 59]

- [cs.CL](#cs.CL) [Total: 117]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Accessibility Literacy: Increasing accessibility awareness among young content creators](https://arxiv.org/abs/2508.06512)

*Alina Karakanta*

**Main category:** cs.HC

**Keywords:** accessibility literacy, content creators, web accessibility

**Relevance Score:** 6

**TL;DR:** This project proposes a mini module to enhance accessibility literacy among young content creators, using web accessibility as a case study. A survey indicated that brief training improved participants' perceptions and willingness to adopt accessibility tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There's a growing necessity for media accessibility education, yet it remains underrepresented in university curricula.

**Method:** A mini module utilizing infographics and quizzes was created to educate young content creators on web accessibility, followed by a survey to assess literacy before and after training.

**Key Contributions:**

	1. Development of an easy-to-use module for teaching accessibility
	2. Demonstration of improved perceptions and willingness post-training
	3. Highlights the importance of accessibility in content creation for inclusivity

**Result:** Participants showed low initial accessibility literacy but demonstrated improved perceptions and willingness to implement accessibility tools post-training.

**Limitations:** Responses revealed some concerning views on disability and accessibility, highlighting the need for a broader understanding of inclusion.

**Conclusion:** Targeted, brief training sessions can effectively integrate accessibility education into formal academic settings, promoting inclusivity and improved content creation.

**Abstract:** The proliferation of audiovisual and web content has created an increasing need for media accessibility education in various fields. However, accessibility remains a low priority in university curricula. This project explores the feasibility of an alternative learning experience aimed at increasing the accessibility literacy of young content creators, taking web accessibility as a case study. We propose a mini module that uses simple, easy-to-use training materials, such as infographics and short quizzes, and can be easily incorporated in educational programmes along existing courses. A survey was conducted to investigate the participants' accessibility literacy before and after training. The findings show that young content creators generally have limited accessibility literacy but even brief exposure to accessibility materials contributed to a shift in perceptions. After training, participants expressed more willingness to implement accessibility tools in their content, with ways varying depending on content type and purpose. This suggests that small, yet targeted interventions could be an alternative for integrating accessibility training into formal education across various disciplines. While some responses reflected traces of the medical model of disability and a particularlist view of accessibility, accessibility was recognised as important for increasing inclusion, improving content, and shaping a fairer society.

</details>


### [2] [ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets](https://arxiv.org/abs/2508.06732)

*Yuya Kawakami, Daniel Cayan, Dongyu Liu, Kwan-Liu Ma*

**Main category:** cs.HC

**Keywords:** climate ensemble datasets, self-organizing maps, large language models

**Relevance Score:** 6

**TL;DR:** ClimateSOM is a visual analysis workflow that utilizes self-organizing maps and Large Language Models to explore and interpret climate ensemble datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding variability among ensemble model runs in climate science is crucial for capturing projections under future conditions.

**Method:** The paper introduces ClimateSOM, which abstracts spatiotemporal time series of climate ensemble model runs into a 2D distribution using a self-organizing map, and integrates LLMs for analysis tasks.

**Key Contributions:**

	1. Introduction of ClimateSOM for climate ensemble analysis
	2. Integration of self-organizing maps and LLMs for interactive exploration
	3. Application of the framework to real-world precipitation data

**Result:** ClimateSOM allows users to explore variability, identify patterns, and cluster ensemble model runs, demonstrating its utility with ensemble precipitation projections for California and the Northwestern United States.

**Limitations:** 

**Conclusion:** The workflow was evaluated through expert reviews, confirming its utility and insights generated from case studies.

**Abstract:** Ensemble datasets are ever more prevalent in various scientific domains. In climate science, ensemble datasets are used to capture variability in projections under plausible future conditions including greenhouse and aerosol emissions. Each ensemble model run produces projections that are fundamentally similar yet meaningfully distinct. Understanding this variability among ensemble model runs and analyzing its magnitude and patterns is a vital task for climate scientists. In this paper, we present ClimateSOM, a visual analysis workflow that leverages a self-organizing map (SOM) and Large Language Models (LLMs) to support interactive exploration and interpretation of climate ensemble datasets. The workflow abstracts climate ensemble model runs - spatiotemporal time series - into a distribution over a 2D space that captures the variability among the ensemble model runs using a SOM. LLMs are integrated to assist in sensemaking of this SOM-defined 2D space, the basis for the visual analysis tasks. In all, ClimateSOM enables users to explore the variability among ensemble model runs, identify patterns, compare and cluster the ensemble model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to an ensemble dataset of precipitation projections over California and the Northwestern United States. Furthermore, we conduct a short evaluation of our LLM integration, and conduct an expert review of the visual workflow and the insights from the case studies with six domain experts to evaluate our approach and its utility.

</details>


### [3] [Toward a Logic of Generalization about Visualization as a Decision Aid](https://arxiv.org/abs/2508.06751)

*Alex Kale*

**Main category:** cs.HC

**Keywords:** visualization, decision-making, decision theory, generalization, utility

**Relevance Score:** 4

**TL;DR:** This paper explores the challenges of generalizing visualization research findings across different contexts, particularly in decision-making. It employs decision theory to analyze the variations in decision problems and emphasizes the concept of utility in visualizations as a decision aid.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of generalization in visualization research and improve its applications as a decision-making tool.

**Method:** The authors utilize decision theory to define dimensions of decision problems and analyze the heterogeneity in scenarios where visualization supports decision-making.

**Key Contributions:**

	1. Introduces decision theory to visualization research
	2. Identifies utility as a key concept in decision-making visualizations
	3. Analyzes the heterogeneity in decision-making scenarios

**Result:** The analysis reveals that utility is an under-explored concept in visualization research, suggesting that employing decision theory can enhance the understanding of context variation in visualizations.

**Limitations:** 

**Conclusion:** Adopting decision theory may improve the logic of generalization in visualization research and yield better insights into its application in decision-making contexts.

**Abstract:** Visualization as a discipline often grapples with generalization by reasoning about how study results on the efficacy of a tool in one context might apply to another context. This work offers an account of the logic of generalization in visualization research and argues that it struggles in particular with applications of visualization as a decision aid. We use decision theory to define the dimensions on which decision problems can vary, and we present an analysis of heterogeneity in scenarios where visualization supports decision-making. Our findings identify utility as a focal and under-examined concept in visualization research on decision-making, demonstrating how the visualization community's logic of generalization might benefit from using decision theory as a lens for understanding context variation.

</details>


### [4] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)

*Catherine Yeh, Tara Menon, Robin Singh Arya, Helen He, Moira Weigel, Fernanda Vi√©gas, Martin Wattenberg*

**Main category:** cs.HC

**Keywords:** Large Language Models, Narrative Visualization, Literary Analysis, Interactive System, Story Ribbons

**Relevance Score:** 8

**TL;DR:** This paper introduces an LLM-driven pipeline for visualizing narratives, implementing an interactive system called Story Ribbons to enhance literary analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the analysis of literature by utilizing LLMs to efficiently parse and visualize complex narrative structures from unstructured data.

**Method:** An LLM-driven data parsing pipeline extracts relevant narrative information from novels and scripts, facilitating the creation of an interactive visualization system.

**Key Contributions:**

	1. Introduction of an LLM-driven data parsing pipeline for narrative analysis
	2. Development of Story Ribbons, an interactive visualization tool for literary analysis
	3. Demonstration of the effectiveness of LLMs in revealing insights in narrative structures

**Result:** User studies and evaluations show Story Ribbons effectively aids both novice and expert analysts in exploring character and theme trajectories across narratives.

**Limitations:** Current limitations of AI-based systems and interaction motifs are discussed.

**Conclusion:** LLMs have the potential to enhance narrative visualization, unveiling new insights from literature while addressing current AI limitations.

**Abstract:** Analyzing literature involves tracking interactions between characters, locations, and themes. Visualization has the potential to facilitate the mapping and analysis of these complex relationships, but capturing structured information from unstructured story data remains a challenge. As large language models (LLMs) continue to advance, we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques. Toward this goal, we introduce an LLM-driven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts. We then apply this pipeline to create Story Ribbons, an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels. Through pipeline evaluations and user studies with Story Ribbons on 36 literary works, we demonstrate the potential of LLMs to streamline narrative visualization creation and reveal new insights about familiar stories. We also describe current limitations of AI-based systems, and interaction motifs designed to address these issues.

</details>


### [5] [Methodology for Business Intelligence Solutions in Internet Banking Companies](https://arxiv.org/abs/2508.06773)

*Alex Escalante Viteri, Javier Gamboa Cruzado, Leonidas Asto Huaman*

**Main category:** cs.HC

**Keywords:** business intelligence, decision-making, banking, methodology, efficiency

**Relevance Score:** 3

**TL;DR:** This paper presents a new methodology for optimizing decision-making in internet banking companies through a business intelligence solution, demonstrating significant time, personnel, and cost reductions.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in the decision-making process within the banking sector, particularly concerning information management which is often time-consuming and costly.

**Method:** The research involved basic and applied methodologies; basic research identified critical success factors and established a new methodology, while applied research tested this methodology via a pre-experimental study analyzing thirty decision-making processes before and after implementation.

**Key Contributions:**

	1. Development of a formal methodology for business intelligence in banking
	2. Application of the new methodology in a practical setting
	3. Demonstration of effectiveness through statistical analysis of decision-making processes

**Result:** The study found significant reductions in the time taken, the number of people involved, and the costs associated with decision-making after the implementation of the business intelligence solution.

**Limitations:** 

**Conclusion:** The new business intelligence methodology effectively optimized the decision-making processes in the business unit dealing with internet banking, leading to improved efficiency.

**Abstract:** Business intelligence in the banking industry has been studied extensively in the last decade; however, business executives still do not perceive efficiency in the decision-making process since the management and treatment of information are very timeconsuming for the deliverer, generating costs in the process. On the other hand, there is no formal methodology for developing business intelligence solutions in this sector. This work aims to optimize decision-making in a business unit that works with internet banking companies, reducing the time, the number of people, and the costs involved in decision-making. To meet the objective, basic and applied research was conducted. The basic research allowed the construction of a new methodology from a study of critical success factors and approaches from the business intelligence literature. The applied research involved the implementation of a business intelligence solution applying the new methodology in a pre-experimental study. Thirty decision-making processes were analyzed using pre-test and post-test data. Tools such as a stopwatch and observation were used to collect and record data on time spent, the number of people, and the decision-making costs. This information was processed in the specialized Minitab18 statistical software, which allowed the observation and confirmation of relevant results regarding time reduction, the number of people, and the costs generated. Therefore, it was concluded that the business intelligence solution, applying the new methodology, optimized decision making in the business unit that works with internet banking for companies.

</details>


### [6] [Visualization Vibes: The Socio-Indexical Function of Visualization Design](https://arxiv.org/abs/2508.06775)

*Michelle Morgenstern, Amy Rae Fox, Graham M. Jones, Arvind Satyanarayan*

**Main category:** cs.HC

**Keywords:** Data Visualization, Public Data Communication, Socio-Indexicality, User Engagement, Information Ecology

**Relevance Score:** 7

**TL;DR:** This paper explores how data visualizations transmit not only propositional meanings related to data but also social, indexical meanings that influence audience perception and engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenges of public data communication in a context of misinformation and distrust in science, emphasizing the need for a broader understanding of how visualizations are received.

**Method:** Ethnographically-informed interviews were conducted to analyze readers' assessments of visualizations, focusing on social attributions and the concept of socio-indexicality.

**Key Contributions:**

	1. Introduces the concept of socio-indexicality in data visualization interpretation.
	2. Demonstrates how design features influence reader engagement beyond content.
	3. Provides empirical evidence from ethnographic interviews on audience responses to visualizations.

**Result:** The findings reveal that readers draw inferences about a visualization's 'vibes' based on its design features, which significantly impacts their engagement and reception.

**Limitations:** 

**Conclusion:** The paper highlights the importance of social contexts and identities in the interpretation of data visualizations, offering insights for improving public data communication strategies.

**Abstract:** In contemporary information ecologies saturated with misinformation, disinformation, and a distrust of science itself, public data communication faces significant hurdles. Although visualization research has broadened criteria for effective design, governing paradigms privilege the accurate and efficient transmission of data. Drawing on theory from linguistic anthropology, we argue that such approaches-focused on encoding and decoding propositional content-cannot fully account for how people engage with visualizations and why particular visualizations might invite adversarial or receptive responses. In this paper, we present evidence that data visualizations communicate not only semantic, propositional meaning$\unicode{x2013}$meaning about data$\unicode{x2013}$but also social, indexical meaning$\unicode{x2013}$meaning beyond data. From a series of ethnographically-informed interviews, we document how readers make rich and varied assessments of a visualization's "vibes"$\unicode{x2013}$inferences about the social provenance of a visualization based on its design features. Furthermore, these social attributions have the power to influence reception, as readers' decisions about how to engage with a visualization concern not only content, or even aesthetic appeal, but also their sense of alignment or disalignment with the entities they imagine to be involved in its production and circulation. We argue these inferences hinge on a function of human sign systems that has thus far been little studied in data visualization: socio-indexicality, whereby the formal features (rather than the content) of communication evoke social contexts, identities, and characteristics. Demonstrating the presence and significance of this socio-indexical function in visualization, this paper offers both a conceptual foundation and practical intervention for troubleshooting breakdowns in public data communication.

</details>


### [7] [Gender and Careers in Platform-Mediated Work: A Longitudinal Study of Online Freelancers](https://arxiv.org/abs/2508.06778)

*Pyeonghwa Kim, Steve Sawyer, Michael Dunn*

**Main category:** cs.HC

**Keywords:** Gender inclusivity, Digital labor platforms, CSCW, Longitudinal study, Freelancers

**Relevance Score:** 4

**TL;DR:** This paper explores long-term gender inequalities experienced by online freelancers, particularly on digital labor platforms like Upwork, revealing significant disparities affecting career trajectories and proposing implications for gender inclusivity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the long-term gendered experiences of online freelancers and address persistent gender-based inequalities in the CSCW community.

**Method:** A five-year longitudinal study involving 105 freelancers on Upwork, analyzing their experiences and career trajectories.

**Key Contributions:**

	1. Introduced career disempowerment and platform-mediated motherhood penalty concepts.
	2. Provided implications for research and design in CSCW to enhance gender inclusivity.
	3. Highlighted long-term effects of gender disparities in digital labor.

**Result:** The study reveals persistent gender disparities impacting long-term work experiences, raising concerns about the sustainability of platform-mediated work.

**Limitations:** 

**Conclusion:** The findings introduce concepts like career disempowerment and platform-mediated motherhood penalty, suggesting needed design implications for equitable work environments.

**Abstract:** We advance gender-inclusive research within the CSCW field by investigating the long-term gendered experiences of online freelancers on digital labor platforms. The prevalence of gender-based inequalities has attracted significant attention within the CSCW community. Yet, insights remain limited on how these inequalities shape workers' long-term experiences on digital labor platforms. Through a five-year longitudinal study of 105 freelancers on Upwork, we reveal persistent gender disparities that influence workers' long-term work and career trajectories, raising concerns about the sustainability of platform-mediated work. We advance the ongoing dialogue on gender inclusivity in the community by introducing the concepts of career disempowerment and platform-mediated motherhood penalty and by offering research and design implications for CSCW to foster more sustainable, equitable platform work environments for all genders.

</details>


### [8] [Quantifying Visualization Vibes: Measuring Socio-Indexicality at Scale](https://arxiv.org/abs/2508.06786)

*Amy Rae Fox, Michelle Morgenstern, Graham M. Jones, Arvind Satyanarayan*

**Main category:** cs.HC

**Keywords:** visualization, social provenance, trust, data communication, sociocultural phenomena

**Relevance Score:** 7

**TL;DR:** This paper explores how visualizations convey social inferences beyond the data they encode, proposing an analytic framework to understand these impressions and their implications for trust and design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the socio-indexical function of visualizations and how they communicate beyond explicit data, impacting trust and design choices in public data communication.

**Method:** The study employs attribution-elicitation surveys to examine how readers form impressions about the social provenance of visualizations.

**Key Contributions:**

	1. Analytic framework for analyzing social inferences in visualizations
	2. Evidence that social inferences affect trust and are culturally universal
	3. Insights into the interplay between design features and social messages of visualizations.

**Result:** Findings reveal that social inferences from visualizations are not restricted to specific cultural groups, influence trust assessments, and interact with design features and the data's underlying messages.

**Limitations:** 

**Conclusion:** Broadening the understanding of visualization to include sociocultural impacts can inform actionable design recommendations for effective public data communication.

**Abstract:** What impressions might readers form with visualizations that go beyond the data they encode? In this paper, we build on recent work that demonstrates the socio-indexical function of visualization, showing that visualizations communicate more than the data they explicitly encode. Bridging this with prior work examining public discourse about visualizations, we contribute an analytic framework for describing inferences about an artifact's social provenance. Via a series of attribution-elicitation surveys, we offer descriptive evidence that these social inferences: (1) can be studied asynchronously, (2) are not unique to a particular sociocultural group or a function of limited data literacy, and (3) may influence assessments of trust. Further, we demonstrate (4) how design features act in concert with the topic and underlying messages of an artifact's data to give rise to such 'beyond-data' readings. We conclude by discussing the design and research implications of inferences about social provenance, and why we believe broadening the scope of research on human factors in visualization to include sociocultural phenomena can yield actionable design recommendations to address urgent challenges in public data communication.

</details>


### [9] [Entendimento de Campanhas no Contexto da Aten√ß√£o Prim√°ria √† Sa√∫de: Um Processo de Design Socialmente Consciente](https://arxiv.org/abs/2508.06791)

*De√≥genes P. da Silva Junior, Jonas Lopes Guerra, Krissia Menezes, Marisa Sel Franco, Roberto Pereira*

**Main category:** cs.HC

**Keywords:** Community Health Agents, Primary Health Care, Health Campaigns

**Relevance Score:** 4

**TL;DR:** Exploratory analysis of Community Health Agents' work context in Primary Health Care focusing on health campaigns using Socially Aware Design.

**Read time:** 62 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the work context of Community Health Agents and Endemic Disease Control Agents in Primary Health Care, particularly regarding health campaigns.

**Method:** Utilized the Socially Aware Design framework along with techniques like Stakeholder Identification Diagram, Evaluation Frame, and Semiotic Framework to analyze social and technical requirements.

**Key Contributions:**

	1. Application of Socially Aware Design framework in health campaigns
	2. Identification of social and technical requirements for health agents
	3. Development of medium-fidelity prototypes for health campaign management

**Result:** Identified stakeholders and potential impacts on their life contexts through personas and scenarios, informing the development of medium-fidelity prototypes.

**Limitations:** 

**Conclusion:** The findings provide insights for developing solutions for health campaign management in Primary Health Care.

**Abstract:** This report presents the results of an exploratory analysis of the work context of Community Health Agents and Endemic Disease Control Agents in Primary Health Care (PHC), with a particular focus on Health Campaigns. To understand this context, the study adopted the Socially Aware Design framework, which employs artifacts and techniques to examine problem domains in a comprehensive and sociotechnical manner. Methods such as the Stakeholder Identification Diagram, Evaluation Frame, and Semiotic Framework were applied to identify stakeholders, anticipate challenges, and elicit social and technical requirements for the solution. Personas and Scenarios were also used to illustrate the potential impacts of a solution on various stakeholders and their life contexts within health campaigns. This report presents the analysis method, its application, and results, discussing the study's findings to inform the development of medium-fidelity prototypes for a PHC health campaign management solution.

</details>


### [10] [Understanding Pedestrian Gesture Misrecognition: Insights from Vision-Language Model Reasoning](https://arxiv.org/abs/2508.06801)

*Tram Thi Minh Tran, Xinyan Yu, Callum Parker, Julie Stephany Berrio Perez, Stewart Worrall, Martin Tomitsch*

**Main category:** cs.HC

**Keywords:** pedestrian gestures, autonomous vehicles, gesture misrecognition, context modeling, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This study explores the challenges of machine interpretation of pedestrian gestures in traffic, focusing on interactions with autonomous vehicles (AVs) using GPT-4V as a diagnostic tool.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the persistent challenges of machine interpretation of ambiguous and context-dependent pedestrian gestures and improve AV recognition systems.

**Method:** Analysis of a public dataset of pedestrian-vehicle interactions through manual video review and thematic analysis of GPT-4V's qualitative reasoning.

**Key Contributions:**

	1. Revealed patterns and causes of gesture misrecognition using GPT-4V
	2. Suggested enhancements for gesture design in traffic contexts
	3. Proposed insights applicable to other domains involving human gesture interpretation

**Result:** Identified recurring factors influencing gesture misrecognition, such as gesture visibility, pedestrian behavior, interaction context, and environmental conditions.

**Limitations:** 

**Conclusion:** Practical considerations for gesture design and opportunities for enhancing AV recognition through better context modeling and uncertainty awareness were highlighted.

**Abstract:** Pedestrian gestures play an important role in traffic communication, particularly in interactions with autonomous vehicles (AVs), yet their subtle, ambiguous, and context-dependent nature poses persistent challenges for machine interpretation. This study investigates these challenges by using GPT-4V, a vision-language model, not as a performance benchmark but as a diagnostic tool to reveal patterns and causes of gesture misrecognition. We analysed a public dataset of pedestrian-vehicle interactions, combining manual video review with thematic analysis of the model's qualitative reasoning. This dual approach surfaced recurring factors influencing misrecognition, including gesture visibility, pedestrian behaviour, interaction context, and environmental conditions. The findings suggest practical considerations for gesture design, including the value of salience and contextual redundancy, and highlight opportunities to improve AV recognition systems through richer context modelling and uncertainty-aware interpretations. While centred on AV-pedestrian interaction, the method and insights are applicable to other domains where machines interpret human gestures, such as wearable AR and assistive technologies.

</details>


### [11] [AdjustAR: AI-Driven In-Situ Adjustment of Site-Specific Augmented Reality Content](https://arxiv.org/abs/2508.06826)

*Nels Numan, Jessica Van Brummelen, Ziwen Lu, Anthony Steed*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Multimodal Large Language Models, User Experience, Real-time corrections, Contextual alignment

**Relevance Score:** 8

**TL;DR:** AdjustAR supports in-situ correction of AR content in dynamic environments using multimodal large language models to maintain alignment with authored intent.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Virtual AR content often becomes misaligned with real-world environments, degrading user experience.

**Method:** The system uses a composite image of the original and current user view to detect misalignments via MLLMs, and proposes revised placements for AR elements based in 3D space.

**Key Contributions:**

	1. Introduces a system for in-situ AR content correction using MLLMs.
	2. Employs visual-semantic reasoning for real-time AR updates.
	3. Backprojects corrections into 3D space for improved user experience.

**Result:** Automated corrections maintain scene alignment in real-time as environments evolve, improving user interaction.

**Limitations:** 

**Conclusion:** AdjustAR enhances AR applications by ensuring content remains contextually relevant despite changes in physical environments.

**Abstract:** Site-specific outdoor AR experiences are typically authored using static 3D models, but are deployed in physical environments that change over time. As a result, virtual content may become misaligned with its intended real-world referents, degrading user experience and compromising contextual interpretation. We present AdjustAR, a system that supports in-situ correction of AR content in dynamic environments using multimodal large language models (MLLMs). Given a composite image comprising the originally authored view and the current live user view from the same perspective, an MLLM detects contextual misalignments and proposes revised 2D placements for affected AR elements. These corrections are backprojected into 3D space to update the scene at runtime. By leveraging MLLMs for visual-semantic reasoning, this approach enables automated runtime corrections to maintain alignment with the authored intent as real-world target environments evolve.

</details>


### [12] [Highlight All the Phrases: Enhancing LLM Transparency through Visual Factuality Indicators](https://arxiv.org/abs/2508.06846)

*Hyo Jin Do, Rachel Ostrand, Werner Geyer, Keerthiram Murugesan, Dennis Wei, Justin Weisz*

**Main category:** cs.HC

**Keywords:** Large Language Models, Factuality, Design Strategies, User Trust, Hallucinations

**Relevance Score:** 9

**TL;DR:** This paper investigates effective design strategies for communicating factuality scores of LLM responses, revealing that color-coded responses are preferred for enhancing user trust and validation.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the gap in research on how to effectively communicate hallucinations in LLM outputs to users.

**Method:** Two scenario-based experiments with 208 participants comparing design strategies for displaying factuality scores.

**Key Contributions:**

	1. Comparative analysis of design strategies for communicating factuality in LLM outputs
	2. Evidence supporting color-coded responses enhances trust and validation
	3. Practical design guidelines for LLM application developers

**Result:** Participants preferred color-coded phrases to indicate factuality, leading to higher trust and easier validation of response accuracy.

**Limitations:** 

**Conclusion:** Design guidelines are proposed for LLM developers to improve user trust and the ability to scrutinize model outputs.

**Abstract:** Large language models (LLMs) are susceptible to generating inaccurate or false information, often referred to as "hallucinations" or "confabulations." While several technical advancements have been made to detect hallucinated content by assessing the factuality of the model's responses, there is still limited research on how to effectively communicate this information to users. To address this gap, we conducted two scenario-based experiments with a total of 208 participants to systematically compare the effects of various design strategies for communicating factuality scores by assessing participants' ratings of trust, ease in validating response accuracy, and preference. Our findings reveal that participants preferred and trusted a design in which all phrases within a response were color-coded based on factuality scores. Participants also found it easier to validate accuracy of the response in this style compared to a baseline with no style applied. Our study offers practical design guidelines for LLM application developers and designers, aimed at calibrating user trust, aligning with user preferences, and enhancing users' ability to scrutinize LLM outputs.

</details>


### [13] [Perceiving Slope and Acceleration: Evidence for Variable Tempo Sampling in Pitch-Based Sonification of Functions](https://arxiv.org/abs/2508.06872)

*Danyang Fan, Walker Smith, Takako Fujioka, Chris Chage, Sile O'Modhrain, Diana Deutsch, Sean Follmer*

**Main category:** cs.HC

**Keywords:** sonification, pitch-based encoding, slope perception, acceleration perception, psychoacoustic experiments

**Relevance Score:** 4

**TL;DR:** This paper introduces a new sampling method for pitch-based sonification called Variable Tempo, which enhances the perception of slope and acceleration in data trends compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the perception of key data trends such as slope and acceleration through an innovative approach to pitch-based sonification.

**Method:** The study compares three sampling methods: Variable Pitch Interval, Variable Tempo, and Continuous baseline, through psychoacoustic experiments that measure slope and acceleration perception.

**Key Contributions:**

	1. Introduced Variable Tempo as a novel sampling method for sonification.
	2. Demonstrated improved accuracy in slope perception through psychoacoustic experiments.
	3. Provided initial evidence supporting the benefits of tempo in sonification for enhanced data interpretation.

**Result:** Variable Tempo outperformed other methods in accuracy for slope tasks and provided much finer just-noticeable differences for acceleration, alongside greater participant confidence and preference.

**Limitations:** 

**Conclusion:** Variable Tempo is a novel and effective sampling method for pitch-based sonification, improving the interpretation of derivative-based data features.

**Abstract:** Sonification offers a non-visual way to understand data, with pitch-based encodings being the most common. Yet, how well people perceive slope and acceleration-key features of data trends-remains poorly understood. Drawing on people's natural abilities to perceive tempo, we introduce a novel sampling method for pitch-based sonification to enhance the perception of slope and acceleration in univariate functions. While traditional sonification methods often sample data at uniform x-spacing, yielding notes played at a fixed tempo with variable pitch intervals (Variable Pitch Interval), our approach samples at uniform y-spacing, producing notes with consistent pitch intervals but variable tempo (Variable Tempo). We conducted psychoacoustic experiments to understand slope and acceleration perception across three sampling methods: Variable Pitch Interval, Variable Tempo, and a Continuous (no sampling) baseline. In slope comparison tasks, Variable Tempo was more accurate than the other methods when modulated by the magnitude ratio between slopes. For acceleration perception, just-noticeable differences under Variable Tempo were over 13 times finer than with other methods. Participants also commonly reported higher confidence, lower mental effort, and a stronger preference for Variable Tempo compared to other methods. This work contributes models of slope and acceleration perception across pitch-based sonification techniques, introduces Variable Tempo as a novel and preferred sampling method, and provides promising initial evidence that leveraging timing can lead to more sensitive, accurate, and precise interpretation of derivative-based data features.

</details>


### [14] [Viewpoint-Tolerant Depth Perception for Shared Extended Space Experience on Wall-Sized Display](https://arxiv.org/abs/2508.06889)

*Dooyoung Kim, Jinseok Hong, Heejeong Ko, Woontack Woo*

**Main category:** cs.HC

**Keywords:** 3D perception, cognitive compensation, shared depth, eXtended Reality, wall-sized displays

**Relevance Score:** 8

**TL;DR:** The paper presents a framework for viewpoint-tolerant shared depth perception on wall-sized displays, enhancing multi-user experience without individual tracking.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Exploring immersive multi-user interactions without personalized tracking in 3D displays, particularly in educational spaces.

**Method:** Investigating the impact of virtual depths and viewing distances on human cognitive factors related to depth perception.

**Key Contributions:**

	1. Introduces viewpoint-tolerant depth perception for multi-users on wall-sized displays
	2. Analyzes cognitive factors affecting depth perception and presence
	3. Proposes applications in immersive environments like museums and classrooms

**Result:** Participants perceived compelling depth even from significant off-center angles; however, too much virtual depth negatively impacted presence and perception.

**Limitations:** 

**Conclusion:** Well-designed wall-sized displays can enhance group experiences in various public settings without the need for individual tracking technologies.

**Abstract:** We proposed viewpoint-tolerant shared depth perception without individual tracking by leveraging human cognitive compensation in universally 3D rendered images on a wall-sized display. While traditional 3D perception-enabled display systems have primarily focused on single-user scenarios-adapting rendering based on head and eye tracking the use of wall-sized displays to extend spatial experiences and support perceptually coherent multi-user interactions remains underexplored. We investigated the effects of virtual depths (dv) and absolute viewing distance (da) on human cognitive compensation factors (perceived distance difference, viewing angle threshold, and perceived presence) to construct the wall display-based eXtended Reality (XR) space. Results show that participants experienced a compelling depth perception even from off-center angles of 23 to 37 degrees, and largely increasing virtual depth worsens depth perception and presence factors, highlighting the importance of balancing extended depth of virtual space and viewing distance from the wall-sized display. Drawing on these findings, wall-sized displays in venues such as museums, galleries, and classrooms can evolve beyond 2D information sharing to offer immersive, spatially extended group experiences without individualized tracking or wearables.

</details>


### [15] [Your Thoughtful Opponent: Embracing Cognitive Conflict with Peer Agent](https://arxiv.org/abs/2508.06955)

*Kyuwon Kim, Jaeryeong Hwang, Younseo Lee, Jeanhee Lee, Sung-Eun Kimm, Hyo-Jeong So*

**Main category:** cs.HC

**Keywords:** Peer Agent, democratic skills, deliberative gameplay, socio-cognitive conflict, education

**Relevance Score:** 2

**TL;DR:** The paper presents a Peer Agent system aimed at enhancing democratic skills through deliberative games, integrating socio-cognitive conflict to promote diverse perspectives in education.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need to cultivate democratic skills such as valuing diverse perspectives and collaborative decision-making in education due to the emergence of complex societal issues.

**Method:** The Peer Agent system simulates a deliberative conversational partner during dilemma-based gameplay, utilizing the Inner Thoughts framework and value-sensitive discourse analysis, featuring modules for handling context, agent state, thought generation, evaluation, and articulation.

**Key Contributions:**

	1. Development of the Peer Agent system for educational environments
	2. Integration of value-sensitive discourse analysis
	3. Active engagement in deliberative conversations to enhance democratic skills

**Result:** The system actively engages in voice-based multi-party deliberation, facilitating socio-cognitive conflict among players.

**Limitations:** The effectiveness and scalability of the system in diverse educational contexts are not fully addressed.

**Conclusion:** The Peer Agent system can potentially improve educational practices by fostering skills necessary for democratic engagement through interactive gameplay.

**Abstract:** As complex societal issues continue to emerge, fostering democratic skills like valuing diverse perspectives and collaborative decision-making is increasingly vital in education. In this paper, we propose a Peer Agent (PA) system designed to simulate a deliberative conversational partner that induces socio-cognitive conflict within dilemma-based game play. Drawing on by the Inner Thoughts framework and grounded in value-sensitive discourse analysis, the PA actively participates in voice-based multi-party deliberation with human players. The system architecture consists of five core modules: Context Interpreter, Agent State Manager, Thought Generator, Thought Evaluator, and Thought Articulator.

</details>


### [16] [Rethinking Privacy Indicators in Extended Reality: Multimodal Design for Situationally Impaired Bystanders](https://arxiv.org/abs/2508.07057)

*Syed Ibrahim Mustafa Shah Bukhari, Maha Sajid, Bo Ji, Brendan David-John*

**Main category:** cs.HC

**Keywords:** Extended Reality, privacy indicators, situationally impaired bystanders, multimodal design, user study

**Relevance Score:** 7

**TL;DR:** The study investigates privacy indicators for XR devices designed for situationally impaired bystanders, showing that multimodal indicators are preferred over traditional visual-only cues.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy concerns for situationally impaired bystanders around XR devices, who may not notice current privacy indicators.

**Method:** Focus group discussions led to the design of five novel privacy indicators, evaluated through a user study.

**Key Contributions:**

	1. Identification of situationally impaired bystanders in XR contexts.
	2. Design and evaluation of five novel privacy indicators.
	3. Demonstration that multimodal indicators enhance privacy perceived usefulness.

**Result:** Visual-only indicators were rated low for usefulness, while multimodal indicators were favored in privacy-sensitive situations.

**Limitations:** Limited sample sizes for focus group and user study.

**Conclusion:** There is a need for adaptable and multimodal privacy designs to better support bystander privacy in XR environments.

**Abstract:** As Extended Reality (XR) devices become increasingly prevalent in everyday settings, they raise significant privacy concerns for bystanders: individuals in the vicinity of an XR device during its use, whom the device sensors may accidentally capture. Current privacy indicators, such as small LEDs, often presume that bystanders are attentive enough to interpret the privacy signals. However, these cues can be easily overlooked when bystanders are distracted or have limited vision. We define such individuals as situationally impaired bystanders. This study explores XR privacy indicator designs that are effective for situationally impaired bystanders. A focus group with eight participants was conducted to design five novel privacy indicators. We evaluated these designs through a user study with seven additional participants. Our results show that visual-only indicators, typical in commercial XR devices, received low ratings for perceived usefulness in impairment scenarios. In contrast, multimodal indicators were preferred in privacy-sensitive scenarios with situationally impaired bystanders. Ultimately, our results highlight the need to move toward adaptable, multimodal, and situationally aware designs that effectively support bystander privacy in everyday XR environments.

</details>


### [17] [Beyond Problem Solving: Framing and Problem-Solution Co-Evolution in Data Visualization Design](https://arxiv.org/abs/2508.07058)

*Paul C. Parsons, Prakash Chandra Shukla*

**Main category:** cs.HC

**Keywords:** visualization design, framing, reflective practice, design thinking, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper explores how expert visualization designers frame and navigate the interplay between problem understanding and solution development through a mixed-methods study.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Existing visualization design models focus heavily on technical problem-solving and overlook the interpretive and judgment-based aspects of the design process.

**Method:** A mixed-methods study involving 11 expert practitioners, using design challenges, diary entries, and semi-structured interviews, followed by reflexive thematic analysis.

**Key Contributions:**

	1. Empirical insights into visualization design as a reflective practice
	2. Identification of key strategies used by expert designers to frame problems
	3. Highlighting the importance of framing in design processes

**Result:** Identified strategies that practitioners use to frame and reframe problems, including metaphors, heuristics, sketching, and reflective evaluation.

**Limitations:** 

**Conclusion:** Visualization design is a continuous co-evolutionary practice where framing is integral, requiring frameworks that emphasize interpretive judgment.

**Abstract:** Visualization design is often described as the process of solving a well-defined problem by navigating a design space. While existing visualization design models have provided valuable structure and guidance, they tend to foreground technical problem-solving and underemphasize the interpretive, judgment-based aspects of design. In contrast, research in other design disciplines has emphasized the importance of framing--how designers define and redefine what the problem is--and the co-evolution of problem and solution spaces through reflective practice. These dimensions remain underexplored in visualization research, particularly from the perspective of expert practitioners. This paper investigates how visualization designers frame problems and navigate the dynamic interplay between problem understanding and solution development. We conducted a mixed-methods study with 11 expert practitioners using design challenges, diary entries, and semi-structured interviews. Through reflexive thematic analysis, we identified key strategies that participants used to frame problems, reframe them in response to evolving constraints or insights, and build bridges between problem and solution spaces. These included using metaphors, heuristics, sketching, primary generators, and reflective evaluation of failed or incomplete ideas. Our findings contribute an empirically grounded account of visualization design as a reflective, co-evolutionary practice, where framing is not a preliminary step but a continuous activity embedded in design. Participants often reshaped their understanding of the problem based on solution attempts, tool feedback, and ethical or narrative concerns. These insights extend current visualization design models and highlight the need for frameworks that better account for framing and interpretive judgment. (See paper for full abstract.)

</details>


### [18] [Hide or Highlight: Understanding the Impact of Factuality Expression on User Trust](https://arxiv.org/abs/2508.07095)

*Hyo Jin Do, Werner Geyer*

**Main category:** cs.HC

**Keywords:** AI-generated outputs, factuality assessments, user trust, human subjects research, trust strategies

**Relevance Score:** 9

**TL;DR:** The study investigates how disclosing factuality assessments of AI-generated outputs affects user trust, revealing that opaque and ambiguity strategies enhance trust compared to other methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how disclosing factuality assessments influences user trust in AI-generated outputs, especially in preventing erroneous decision-making.

**Method:** The study tested four disclosure strategies (transparent, attention, opaque, ambiguity) in question-answering scenarios with 148 participants to assess the impact on trust and perceived answer quality.

**Key Contributions:**

	1. Identified effective strategies for disclosing AI-generated output factuality assessments that enhance user trust.
	2. Provided empirical evidence through human subjects research on the impact of different disclosure methods on trust and perceived quality.
	3. Insight into user behavior regarding AI trust and the efficacy of content disclosure in AI systems.

**Result:** Opaque and ambiguity disclosure strategies increased user trust while maintaining perceived answer quality compared to other strategies and a baseline without factuality information.

**Limitations:** The study is limited by the sample size and context-specific scenarios, which may not generalize to all AI applications.

**Conclusion:** Hiding less factual content can effectively build end-user trust in AI systems, suggesting implications for design and communication methods in AI applications.

**Abstract:** Large language models are known to produce outputs that are plausible but factually incorrect. To prevent people from making erroneous decisions by blindly trusting AI, researchers have explored various ways of communicating factuality estimates in AI-generated outputs to end-users. However, little is known about whether revealing content estimated to be factually incorrect influences users' trust when compared to hiding it altogether. We tested four different ways of disclosing an AI-generated output with factuality assessments: transparent (highlights less factual content), attention (highlights factual content), opaque (removes less factual content), ambiguity (makes less factual content vague), and compared them with a baseline response without factuality information. We conducted a human subjects research (N = 148) using the strategies in question-answering scenarios. We found that the opaque and ambiguity strategies led to higher trust while maintaining perceived answer quality, compared to the other strategies. We discuss the efficacy of hiding presumably less factual content to build end-user trust.

</details>


### [19] [Toward AI Matching Policies in Homeless Services: A Qualitative Study with Policymakers](https://arxiv.org/abs/2508.07129)

*Caroline M. Johnston, Olga Koumoundouros, Angel Hsing-Chi Hwang, Laura Onasch-Vera, Eric Rice, Phebe Vayanos*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence, Homelessness, Policy, Interviews, Algorithm Design

**Relevance Score:** 4

**TL;DR:** The paper explores the reception and adoption of AI algorithms for matching homeless individuals to housing resources through interviews with policymakers in Los Angeles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI algorithms can be integrated into the housing resource matching process and their potential impacts on efficiency, fairness, and transparency.

**Method:** Semi-structured interviews with 13 policymakers in homeless services in Los Angeles.

**Key Contributions:**

	1. Investigates AI adoption by policymakers in homeless services.
	2. Identifies design considerations for AI systems in low-resource contexts.
	3. Highlights views on efficiency, fairness, and transparency in AI applications.

**Result:** Policymakers are generally open to integrating AI if it is thoughtfully designed and used alongside human decision-makers, citing potential gains and drawbacks.

**Limitations:** The study is based on a small sample of 13 policymakers, limiting generalizability.

**Conclusion:** There is no consensus on the design of AI systems, but insights gained can guide future responsible development in low-resource scenarios.

**Abstract:** Artificial intelligence researchers have proposed various data-driven algorithms to improve the processes that match individuals experiencing homelessness to scarce housing resources. It remains unclear whether and how these algorithms are received or adopted by practitioners and what their corresponding consequences are. Through semi-structured interviews with 13 policymakers in homeless services in Los Angeles, we investigate whether such change-makers are open to the idea of integrating AI into the housing resource matching process, identifying where they see potential gains and drawbacks from such a system in issues of efficiency, fairness, and transparency. Our qualitative analysis indicates that, even when aware of various complicating factors, policymakers welcome the idea of an AI matching tool if thoughtfully designed and used in tandem with human decision-makers. Though there is no consensus as to the exact design of such an AI system, insights from policymakers raise open questions and design considerations that can be enlightening for future researchers and practitioners who aim to build responsible algorithmic systems to support decision-making in low-resource scenarios.

</details>


### [20] [Canvas3D: Empowering Precise Spatial Control for Image Generation with Constraints from a 3D Virtual Canvas](https://arxiv.org/abs/2508.07135)

*Runlin Duan, Yuzhao Chen, Rahul Jain, Yichen Hu, Jingyu Shi, Karthik Ramani*

**Main category:** cs.HC

**Keywords:** Generative AI, 3D interaction, image generation, user experience, spatial manipulation

**Relevance Score:** 8

**TL;DR:** Canvas3D is an interactive system that enhances image generation by allowing users to manipulate spatial compositions in a 3D environment based on textual descriptions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of controlling spatial compositions in image generation, which has been difficult with existing systems.

**Method:** Canvas3D leverages a 3D engine to convert textual prompts into interactive objects, allowing users to manipulate the spatial arrangement directly on a virtual canvas.

**Key Contributions:**

	1. Introduction of an interactive 3D canvas for image generation
	2. Enhanced spatial control through direct manipulation
	3. Improved user experience compared to baseline systems

**Result:** Comparative studies show that Canvas3D significantly outperforms a baseline system in terms of spatial control, interactivity, and overall user experience.

**Limitations:** The study may be limited by sample size and the diversity of use cases evaluated.

**Conclusion:** Canvas3D provides a more intuitive and effective means for users to generate images that align with their spatial intentions.

**Abstract:** Generative AI (GenAI) has significantly advanced the ease and flexibility of image creation. However, it remains a challenge to precisely control spatial compositions, including object arrangement and scene conditions. To bridge this gap, we propose Canvas3D, an interactive system leveraging a 3D engine to enable precise spatial manipulation for image generation. Upon user prompt, Canvas3D automatically converts textual descriptions into interactive objects within a 3D engine-driven virtual canvas, empowering direct and precise spatial configuration. These user-defined arrangements generate explicit spatial constraints that guide generative models in accurately reflecting user intentions in the resulting images. We conducted a closed-end comparative study between Canvas3D and a baseline system. And an open-ended study to evaluate our system "in the wild". The result indicates that Canvas3D outperforms the baseline on spatial control, interactivity, and overall user experience.

</details>


### [21] [SketchConcept: Sketching-based Concept Recomposition for Product Design using Generative AI](https://arxiv.org/abs/2508.07141)

*Runlin Duan, Chenfei Zhu, Yuzhao Chen, Dizhi Ma, Jingyu Shi, Ziyi Liu, Karthik Ramani*

**Main category:** cs.HC

**Keywords:** Sketching, Product design, Generative AI, Human-computer interaction, Large Language Models

**Relevance Score:** 6

**TL;DR:** SketchConcept is a design support tool that enhances conceptual product design by combining visual sketches and textual descriptions, using a function-to-visual mapping workflow with GenAI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the exploration of design concepts by integrating both visual representations and functional descriptions in the sketching process, enhancing current sketch-based design tools.

**Method:** The system employs a function-to-visual mapping workflow that links descriptions generated by a Large Language Model (LLM) to visual components created by Generative AI (GenAI).

**Key Contributions:**

	1. Introduction of SketchConcept which combines sketches and textual descriptions for design.
	2. Development of a function-to-visual mapping workflow utilizing LLM and GenAI.
	3. Successful validation of the tool through use cases and user studies.

**Result:** The tool demonstrates its capabilities through multiple use cases and shows positive results from a user study evaluating its efficacy and usability.

**Limitations:** 

**Conclusion:** SketchConcept effectively supports the decomposition, generation, and editing of design concepts, providing a novel approach to conceptual design that integrates multiple modalities.

**Abstract:** Conceptual product design requires designers to explore the design space of visual and functional concepts simultaneously. Sketching has long been adopted to empower concept exploration. However, current sketch-based design tools mostly emphasize visual design using emerging techniques. We present SketchConcept, a design support tool that decomposes design concepts into visual representations and functionality of concepts using sketches and textual descriptions. We propose a function-to-visual mapping workflow that maps the function descriptions generated by a Large Language Model to a component of the concept produced by image Generative Artificial Intelligence(GenAI). The function-to-visual mapping allows our system to leverage multimodal GenAI to decompose, generate, and edit the design concept to satisfy the overall function and behavior. We present multiple use cases enabled by SketchConcept to validate the workflow. Finally, we evaluated the efficacy and usability of our system with a two-session user study.

</details>


### [22] [Explainability-in-Action: Enabling Expressive Manipulation and Tacit Understanding by Bending Diffusion Models in ComfyUI](https://arxiv.org/abs/2508.07183)

*Ahmed M. Abuzuraiq, Philippe Pasquier*

**Main category:** cs.HC

**Keywords:** explainable AI, creativity, generative models, artistic engagement, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper discusses a craft-based approach to explainable AI (XAI) in creative contexts, emphasizing the interaction between artists and generative models to enhance artistic engagement and understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how explainable AI can enhance creative processes and artistic engagement by allowing artists to interact with and manipulate generative models.

**Method:** A craft-based approach was proposed, utilizing a model-bending and inspection plugin integrated into the node-based interface of ComfyUI, which allows for interactive manipulation of generative model components.

**Key Contributions:**

	1. Proposing a craft-based approach to XAI in creative contexts
	2. Demonstrating the application of a model-bending plugin for artist interaction
	3. Highlighting the importance of long-term engagement and reflection-in-action in understanding AI outputs.

**Result:** Through interactive manipulation, artists can develop an intuition about the influence of various components of generative models on the outputs they create.

**Limitations:** 

**Conclusion:** Large generative models can be treated as creative materials if their internal structures are accessible and manipulable, thus enhancing artists' agency and control.

**Abstract:** Explainable AI (XAI) in creative contexts can go beyond transparency to support artistic engagement, modifiability, and sustained practice. While curated datasets and training human-scale models can offer artists greater agency and control, large-scale generative models like text-to-image diffusion systems often obscure these possibilities. We suggest that even large models can be treated as creative materials if their internal structure is exposed and manipulable. We propose a craft-based approach to explainability rooted in long-term, hands-on engagement akin to Sch\"on's "reflection-in-action" and demonstrate its application through a model-bending and inspection plugin integrated into the node-based interface of ComfyUI. We demonstrate that by interactively manipulating different parts of a generative model, artists can develop an intuition about how each component influences the output.

</details>


### [23] [Civil Servants as Builders: Enabling Non-IT Staff to Develop Secure Python and R Tools](https://arxiv.org/abs/2508.07203)

*Prashant Sharma*

**Main category:** cs.HC

**Keywords:** digital government, civil servants, open-source platform, application development, digital transformation

**Relevance Score:** 4

**TL;DR:** This paper presents an open-source platform that allows technically skilled civil servants to develop and deploy applications within government networks, addressing a gap in digital government capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of pathways for technically skilled civil servants outside formal IT roles to develop and deploy their applications.

**Method:** The paper introduces a platform integrating Jupyter Notebooks, preapproved open-source libraries, and lightweight governance to facilitate application development and deployment in government environments.

**Key Contributions:**

	1. An open-source platform for civil servants to develop applications
	2. A replicable model for digital transformation in the public sector
	3. Integration of governance with technical capability preservation

**Result:** The platform enables civil servants to create domain-specific applications while adhering to institutional constraints such as procurement and security policies.

**Limitations:** 

**Conclusion:** This initiative supports public-sector skill retention and enhances digital transformation by empowering civil servants with programming tools and capabilities.

**Abstract:** Current digital government literature focuses on professional in-house IT teams, specialized digital service teams, vendor-developed systems, or proprietary low-code/no-code tools. Almost no scholarship addresses a growing middle ground: technically skilled civil servants outside formal IT roles who can write real code but lack a sanctioned, secure path to deploy their work. This paper introduces a limits-aware, open-source and replicable platform that enables such public servants to develop, peer review, and deploy small-scale, domain-specific applications within government networks via a sandboxed, auditable workflow. By combining Jupyter Notebooks, preapproved open-source libraries, and lightweight governance, the platform works within institutional constraints such as procurement rules and IT security policies while avoiding vendor lock-in. Unlike low/no-code approaches, it preserves and enhances civil servants' programming skills, keeping them technically competitive with their private-sector peers. This contribution fills a critical gap, offering a replicable model for public-sector skill retention, resilience, and bottom-up digital transformation.

</details>


### [24] [Exploring Micro Accidents and Driver Responses in Automated Driving: Insights from Real-world Videos](https://arxiv.org/abs/2508.07256)

*Wei Xiang, Chuyue Zhang, Jie Yan*

**Main category:** cs.HC

**Keywords:** automated driving, micro accidents, machine learning, human risk perception, safety

**Relevance Score:** 4

**TL;DR:** This article examines micro accidents in level 3 automated driving, highlighting their impact on safety and the need for improved automated driving system designs.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasingly complex safety scenarios in level 3 automated driving caused by micro accidents, which are prevalent yet often overlooked.

**Method:** The study collected user-generated video data of micro accidents and utilized machine learning to analyze key environmental variables and agent behaviors. Crowdsourcing was used to gather human risk perceptions and reactions.

**Key Contributions:**

	1. Identification of micro accident characteristics in automated driving
	2. Insights from human perception on micro accidents
	3. Data-driven recommendations for improving automated driving system safety

**Result:** Key variables affecting micro accidents were identified, offering insights into user interpretations of risk, ultimately contributing to safer automated driving design.

**Limitations:** Limited by the dataset which may not capture all driving scenarios; reliance on user-generated content could introduce bias.

**Conclusion:** The findings emphasize the need for enhanced understanding and consideration of micro accidents in the development of automated driving systems to prevent more severe incidents.

**Abstract:** Automated driving in level 3 autonomy has been adopted by multiple companies such as Tesla and BMW, alleviating the burden on drivers while unveiling new complexities. This article focused on the under-explored territory of micro accidents during automated driving, characterized as not fatal but abnormal aberrations such as abrupt deceleration and snake driving. These micro accidents are basic yet pervasive events that might results in more severe accidents. Through collecting a comprehensive dataset of user generated video recording such micro accidents in natural driving scenarios, this article locates key variables pertaining to environments and autonomous agents using machine learning methods. Subsequently, crowdsourcing method provides insights into human risk perceptions and reactions to these micro accidents. This article thus describes features of safety critical scenarios other than crashes and fatal accidents, informing and potentially advancing the design of automated driving systems.

</details>


### [25] [Fine-Tuning Large Language Models Using EEG Microstate Features for Mental Workload Assessment](https://arxiv.org/abs/2508.07283)

*Bujar Raufi*

**Main category:** cs.HC

**Keywords:** EEG, cognitive load, Large Language Models, microstates, machine learning

**Relevance Score:** 9

**TL;DR:** This study investigates the enhancement of cognitive load assessment using EEG microstates and fine-tuning of Large Language Models (LLMs).

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the assessment of cognitive load states by utilizing EEG microstate features in LLMs.

**Method:** The research employs a four-stage experimental design including dataset collection, microstate segmentation, feature extraction, and LLM selection and refinement within a supervised learning framework.

**Key Contributions:**

	1. Integration of EEG microstate features into LLMs for cognitive load assessment
	2. Demonstrated improved predictions of cognitive states based on EEG data
	3. Provided a refined methodology for training LLMs using brain dynamics.

**Result:** The fine-tuned LLM showed a significant improvement in model performance in discriminating between 'Rest' and 'Load' cognitive states using EEG features.

**Limitations:** 

**Conclusion:** This approach shows the potential for integrating EEG-informed LLMs in cognitive neuroscience and may advance machine learning in cognitive AI research.

**Abstract:** This study explores the intersection of electroencephalography (EEG) microstates and Large Language Models (LLMs) to enhance the assessment of cognitive load states. By utilizing EEG microstate features, the research aims to fine-tune LLMs for improved predictions of distinct cognitive states, specifically 'Rest' and 'Load'. The experimental design is delineated in four comprehensive stages: dataset collection and preprocessing, microstate segmentation and EEG backfitting, feature extraction paired with prompt engineering, and meticulous LLM model selection and refinement. Employing a supervised learning paradigm, the LLM is trained to identify cognitive load states based on EEG microstate features integrated into prompts, producing accurate discrimination of cognitive load. A curated dataset, linking EEG features to specified cognitive load conditions, underpins the experimental framework. The results indicate a significant improvement in model performance following the proposed fine-tuning, showcasing the potential of EEG-informed LLMs in cognitive neuroscience and cognitive AI applications. This approach not only contributes to the understanding of brain dynamics but also paves the way for advancements in machine learning techniques applicable to cognitive load and cognitive AI research.

</details>


### [26] [In-person, Online and Back Again -- A Tale of Three Hybrid Hackathons](https://arxiv.org/abs/2508.07301)

*Abasi-amefon Obot Affia-Jomants, Alexander Serebrenik, James D. Herbsleb, Alexander Nolte*

**Main category:** cs.HC

**Keywords:** hybrid hackathons, collaboration, participant experience, event organization, technology in hackathons

**Relevance Score:** 7

**TL;DR:** This paper examines hybrid hackathons, focusing on how they function and the unique challenges faced by organizers and participants. It analyses several key dimensions that impact collaboration in these events through case studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the growing trend of hybrid hackathons and the lack of integrated research on challenges unique to these formats.

**Method:** The research utilizes exploratory case studies of three hybrid hackathon events to analyze how specific hybrid collaboration dimensions are implemented and experienced.

**Key Contributions:**

	1. Provides insights on the functioning of hybrid hackathons
	2. Identifies key dimensions affecting collaboration in hybrid settings
	3. Offers practical recommendations for organizers and participants

**Result:** Findings reveal varying organizer approaches to hybrid formats, influencing participant experiences related to resource access, communication, and teamwork.

**Limitations:** 

**Conclusion:** The study offers practical recommendations for organizing and participating in hybrid hackathons, emphasizing the need for attention to critical aspects like time-zone management and targeted support for hybrid teams.

**Abstract:** Hybrid hackathons, which combine in-person and online participation, present unique challenges for organizers and participants. Although such events are increasingly conducted, research on them remains fragmented, with limited integration between hackathon studies and hybrid collaboration. Existing strategies for in-person or online-only events often fail to address the unique challenges of hybrid formats, such as managing communication across physical and virtual spaces. Our work addresses this gap by examining how hybrid hackathons function, analyzing how organizers structure these events and how participants navigate hybrid-specific challenges. Drawing on established theories of hybrid collaboration, we examine key dimensions - synchronicity, physical distribution, dynamic transitions, and technological infrastructure - that shape collaboration in hybrid events. Through an exploratory case study of three hackathon events, we analyze how these dimensions are implemented and their effects on participant experiences. Our findings reveal differing organizer considerations of the hybrid dimensions in the hackathon design, leading to distinct experiences for participants. Implementation styles - favoring in-person, online, or balanced participation - led to varied participant experiences, affecting access to resources, communication, and team coordination. Organizers in our study also relied on technology to bridge hybrid interactions, but overlooked critical aspects like time-zone management, dynamic transitions, and targeted support for hybrid teams. Additionally, participants in their teams responded to gaps in event scaffolding by adapting collaboration strategies, revealing gaps in organizers' preparedness for hybrid events. Learning from our findings, we offer practical recommendations when organizing hybrid hackathon events and recommendations to participants when attending them.

</details>


### [27] [Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics](https://arxiv.org/abs/2508.07390)

*Gustavo Moreira, Leonardo Ferreira, Carolina Veiga, Maryam Hosseini, Fabio Miranda*

**Main category:** cs.HC

**Keywords:** urban analytics, human-AI collaboration, visualization, large language models, dataflow

**Relevance Score:** 8

**TL;DR:** Urbanite is a framework for human-AI collaboration in urban visual analytics, enabling users to specify intent and facilitating alignment between user goals and analytical outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity of analyzing urban data and societal challenges requires expertise in various domains, presenting barriers for researchers and urban experts in managing datasets and deriving insights.

**Method:** Urbanite uses a dataflow-based model that allows users to specify intent at multiple levels of granularity, enabling interactive alignment throughout the urban analytics process.

**Key Contributions:**

	1. Framework for human-AI collaboration in urban analytics
	2. Dataflow-based model for specifying user intent
	3. Enhanced explainability and task definition support

**Result:** Urbanite features support explainability, multi-resolution task definitions, and the provenance of interactions, improving collaboration between urban experts and the system based on user surveys identifying challenges.

**Limitations:** 

**Conclusion:** Urbanite effectively addresses the challenges of aligning user intent and system behavior, enhancing the usability of urban visual analytics.

**Abstract:** With the growing availability of urban data and the increasing complexity of societal challenges, visual analytics has become essential for deriving insights into pressing real-world problems. However, analyzing such data is inherently complex and iterative, requiring expertise across multiple domains. The need to manage diverse datasets, distill intricate workflows, and integrate various analytical methods presents a high barrier to entry, especially for researchers and urban experts who lack proficiency in data management, machine learning, and visualization. Advancements in large language models offer a promising solution to lower the barriers to the construction of analytics systems by enabling users to specify intent rather than define precise computational operations. However, this shift from explicit operations to intent-based interaction introduces challenges in ensuring alignment throughout the design and development process. Without proper mechanisms, gaps can emerge between user intent, system behavior, and analytical outcomes. To address these challenges, we propose Urbanite, a framework for human-AI collaboration in urban visual analytics. Urbanite leverages a dataflow-based model that allows users to specify intent at multiple scopes, enabling interactive alignment across the specification, process, and evaluation stages of urban analytics. Based on findings from a survey to uncover challenges, Urbanite incorporates features to facilitate explainability, multi-resolution definition of tasks across dataflows, nodes, and parameters, while supporting the provenance of interactions. We demonstrate Urbanite's effectiveness through usage scenarios created in collaboration with urban experts. Urbanite is available at https://urbantk.org/urbanite.

</details>


### [28] [StreetWeave: A Declarative Grammar for Street-Overlaid Visualization of Multivariate Data](https://arxiv.org/abs/2508.07496)

*Sanjana Srabanti, G. Elisabeta Marai, Fabio Miranda*

**Main category:** cs.HC

**Keywords:** street visualization, pedestrian networks, data analysis, urban planning, visualization frameworks

**Relevance Score:** 7

**TL;DR:** The paper introduces StreetWeave, a framework for creating custom visualizations of street and pedestrian networks to aid various domain experts in data analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective visualization and analysis of street and pedestrian networks for urban planners, climate researchers, and health experts.

**Method:** Review and qualitative coding of 45 studies on street-overlaid visualizations to identify usage patterns and introduce a new framework for visualization creation.

**Key Contributions:**

	1. Introduction of StreetWeave as a unique visualization grammar for spatial network data.
	2. Insights from analyzing 45 studies on street-overlaid visualizations.
	3. Framework that addresses diverse expert requirements for visual analysis.

**Result:** StreetWeave allows for the generation of custom visualizations, aiding in the effective exploration of complex spatial data.

**Limitations:** 

**Conclusion:** The introduction of StreetWeave fills a gap in the visualization design framework, enhancing accessibility for non-programmers and improving data exploration processes.

**Abstract:** The visualization and analysis of street and pedestrian networks are important to various domain experts, including urban planners, climate researchers, and health experts. This has led to the development of new techniques for street and pedestrian network visualization, expanding how data can be shown and understood more effectively. Despite their increasing adoption, there is no established design framework to guide the creation of these visualizations while addressing the diverse requirements of various domains. When exploring a feature of interest, domain experts often need to transform, integrate, and visualize a combination of thematic data (e.g., demographic, socioeconomic, pollution) and physical data (e.g., zip codes, street networks), often spanning multiple spatial and temporal scales. This not only complicates the process of visual data exploration and system implementation for developers but also creates significant entry barriers for experts who lack a background in programming. With this in mind, in this paper, we reviewed 45 studies utilizing street-overlaid visualizations to understand how they are used. Through qualitative coding of these visualizations, we analyzed three key aspects of street and pedestrian network visualization usage: the analytical purpose they serve, the visualization approaches employed, and the data sources used in their creation. Building on this design space, we introduce StreetWeave, a declarative grammar for designing custom visualizations of multivariate spatial network data across multiple resolutions. We demonstrate how StreetWeave can be used to create various street-overlaid visualizations, enabling effective exploration and analysis of spatial data. StreetWeave is available at https://urbantk.org/streetweave.

</details>


### [29] [VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design](https://arxiv.org/abs/2508.07497)

*Leonardo Ferreira, Gustavo Moreira, Fabio Miranda*

**Main category:** cs.HC

**Keywords:** visual analytics, methodology, knowledge base, urban computing, large language models

**Relevance Score:** 5

**TL;DR:** The paper introduces VA-Blueprint, a systematic approach to designing urban visual analytics (VA) systems, which organizes their core components and automates knowledge extraction using a large language model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of structured knowledge in the design and development of visual analytics systems, particularly urban VA systems, which are complex and underexplored.

**Method:** The authors propose a methodology, VA-Blueprint, that categorizes building blocks of VA systems and applies it to 20 initial systems. They enhance knowledge base construction by using a large language model to extract components from 81 other papers.

**Key Contributions:**

	1. Introduction of VA-Blueprint methodology
	2. Development of a multi-level structured knowledge base
	3. Automation of component extraction using LLMs

**Result:** The methodology results in a multi-level structured knowledge base for VA system development, validated by expert interviews and quantitative analysis metrics.

**Limitations:** 

**Conclusion:** The contributions provide a deeper understanding of the composition of VA systems and lay a practical foundation for structured and efficient development processes, with VA-Blueprint accessible online.

**Abstract:** Designing and building visual analytics (VA) systems is a complex, iterative process that requires the seamless integration of data processing, analytics capabilities, and visualization techniques. While prior research has extensively examined the social and collaborative aspects of VA system authoring, the practical challenges of developing these systems remain underexplored. As a result, despite the growing number of VA systems, there are only a few structured knowledge bases to guide their design and development. To tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that systematically reviews and categorizes the fundamental building blocks of urban VA systems, a domain particularly rich and representative due to its intricate data and unique problem sets. Applying this methodology to an initial set of 20 systems, we identify and organize their core components into a multi-level structure, forming an initial knowledge base with a structured blueprint for VA system development. To scale this effort, we leverage a large language model to automate the extraction of these components for other 81 papers (completing a corpus of 101 papers), assessing its effectiveness in scaling knowledge base construction. We evaluate our method through interviews with experts and a quantitative analysis of annotation metrics. Our contributions provide a deeper understanding of VA systems' composition and establish a practical foundation to support more structured, reproducible, and efficient system development. VA-Blueprint is available at https://urbantk.org/va-blueprint.

</details>


### [30] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)

*Baihan Lin*

**Main category:** cs.HC

**Keywords:** Conversational DNA, dialogue analysis, human-computer interaction, data visualization, AI communication

**Relevance Score:** 8

**TL;DR:** A novel visual language, Conversational DNA, is introduced to analyze dialogue structure and patterns beyond mere words, revealing insights into communication through visualization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore deeper insights into communication by revealing hidden patterns within dialogue that traditional methods overlook.

**Method:** The paper employs a visualization framework that depicts dialogue as a living system, utilizing biological metaphors to illustrate linguistic complexity, emotional trajectories, and conversational relevance.

**Key Contributions:**

	1. Introduction of Conversational DNA as a visual language for dialogue analysis.
	2. Demonstration of the impact of visualization on understanding conversational patterns.
	3. Bridging data visualization and HCI in the context of AI interactions.

**Result:** The analysis of therapeutic conversations and significant human-AI dialogues showcases the effectiveness of the visualization in identifying interaction patterns that traditional analysis fails to capture.

**Limitations:** 

**Conclusion:** The study presents a creative framework for understanding communication that integrates data visualization and human-computer interaction, offering insights into meaningful dialogue in an AI-focused era.

**Abstract:** What if the patterns hidden within dialogue reveal more about communication than the words themselves? We introduce Conversational DNA, a novel visual language that treats any dialogue -- whether between humans, between human and AI, or among groups -- as a living system with interpretable structure that can be visualized, compared, and understood. Unlike traditional conversation analysis that reduces rich interaction to statistical summaries, our approach reveals the temporal architecture of dialogue through biological metaphors. Linguistic complexity flows through strand thickness, emotional trajectories cascade through color gradients, conversational relevance forms through connecting elements, and topic coherence maintains structural integrity through helical patterns. Through exploratory analysis of therapeutic conversations and historically significant human-AI dialogues, we demonstrate how this visualization approach reveals interaction patterns that traditional methods miss. Our work contributes a new creative framework for understanding communication that bridges data visualization, human-computer interaction, and the fundamental question of what makes dialogue meaningful in an age where humans increasingly converse with artificial minds.

</details>


### [31] [Phoenix: A Novel Context-Aware Voice-Powered Math Equation Workspace and Editor](https://arxiv.org/abs/2508.07576)

*Kenneth Ge, Ryan Paul, Priscilla Zhang, JooYoung Seo*

**Main category:** cs.HC

**Keywords:** voice-powered workspace, fine motor disabilities, large language models

**Relevance Score:** 8

**TL;DR:** A novel voice-powered math workspace leverages large language models to assist individuals with fine motor disabilities (FMDs) in engaging with mathematical notation intuitively, reducing cognitive load and mechanical constraints.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current speech-based math technologies are limited by the need for precise dictation of symbols and command-based interfaces, which can hinder those with fine motor disabilities.

**Method:** The study introduces a voice-powered math workspace that incorporates insights from neuroscience and utilizes large language models with a novel context engine for natural language interaction.

**Key Contributions:**

	1. Introduction of a voice-powered math workspace for FMDs
	2. Integration of neuroscience insights with large language models
	3. Reduction of cognitive load through natural language interactions

**Result:** The workspace allows users with FMDs to engage fluidly with mathematical problems, significantly enhancing their ability to solve math problems without mechanical constraints.

**Limitations:** 

**Conclusion:** By applying this approach, individuals with fine motor disabilities can focus more on conceptual understanding than on documentation mechanics.

**Abstract:** Writing mathematical notation requires substantial effort, diverting cognitive resources from conceptual understanding to documentation mechanics, significantly impacting individuals with fine motor disabilities (FMDs). Current limits of speech-based math technologies rely on precise dictation of math symbols and unintuitive command-based interfaces. We present a novel voice-powered math workspace, applying neuroscience insights to create an intuitive problem-solving environment. To minimize cognitive load, we leverage large language models with our novel context engine to support natural language interaction. Ultimately, we enable fluid mathematical engagement for individuals with FMDs -- freed from mechanical constraints.

</details>


### [32] [On the Limits of Selective AI Prediction: A Case Study in Clinical Decision Making](https://arxiv.org/abs/2508.07617)

*Sarah Jabbour, David Fouhey, Nikola Banovic, Stephanie D. Shepard, Ella Kazerooni, Michael W. Sjoding, Jenna Wiens*

**Main category:** cs.HC

**Keywords:** AI, selective prediction, clinical decision-making, human-AI interaction, automation bias

**Relevance Score:** 8

**TL;DR:** The paper explores the impact of selective prediction in AI-assisted clinical decision-making, revealing its potential to improve clinician accuracy while also altering error patterns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how selective prediction can mitigate the negative effects of inaccurate AI predictions in clinical decision-making contexts.

**Method:** A user study was conducted with 259 clinicians tasked with diagnosing and treating hospitalized patients, comparing their performance with and without AI assistance and under selective prediction.

**Key Contributions:**

	1. Demonstrated the effectiveness of selective prediction in clinical decision-making
	2. Highlighted the unintended consequences of AI abstention in clinical contexts
	3. Provided empirical evidence supporting the need to validate human engagement assumptions in AI systems

**Result:** Selective prediction improved clinician accuracy compared to incorrect AI predictions but led to a notable increase in missed diagnoses and treatments.

**Limitations:** The study is limited to a clinical context and may not generalize to other domains; further research is needed on long-term effects of selective prediction.

**Conclusion:** Selective prediction can maintain decision accuracy in AI-assisted environments but requires attention to its influence on clinicians' decision-making processes.

**Abstract:** AI has the potential to augment human decision making. However, even high-performing models can produce inaccurate predictions when deployed. These inaccuracies, combined with automation bias, where humans overrely on AI predictions, can result in worse decisions. Selective prediction, in which potentially unreliable model predictions are hidden from users, has been proposed as a solution. This approach assumes that when AI abstains and informs the user so, humans make decisions as they would without AI involvement. To test this assumption, we study the effects of selective prediction on human decisions in a clinical context. We conducted a user study of 259 clinicians tasked with diagnosing and treating hospitalized patients. We compared their baseline performance without any AI involvement to their AI-assisted accuracy with and without selective prediction. Our findings indicate that selective prediction mitigates the negative effects of inaccurate AI in terms of decision accuracy. Compared to no AI assistance, clinician accuracy declined when shown inaccurate AI predictions (66% [95% CI: 56%-75%] vs. 56% [95% CI: 46%-66%]), but recovered under selective prediction (64% [95% CI: 54%-73%]). However, while selective prediction nearly maintains overall accuracy, our results suggest that it alters patterns of mistakes: when informed the AI abstains, clinicians underdiagnose (18% increase in missed diagnoses) and undertreat (35% increase in missed treatments) compared to no AI input at all. Our findings underscore the importance of empirically validating assumptions about how humans engage with AI within human-AI systems.

</details>


### [33] [Are UX evaluation methods truly accessible](https://arxiv.org/abs/2508.07620)

*Andr√©s Eduardo Fuentes-Cort√°zar, Alejandra Rivera-Hern√°ndez, Jos√© Rafael Rojano-C√°ceres*

**Main category:** cs.HC

**Keywords:** User Experience, Accessibility, Deaf Users, UX Evaluation, Inclusive Design

**Relevance Score:** 8

**TL;DR:** This study analyzes UX evaluation methods for Deaf users, identifying adaptations needed to improve accessibility and representation in user experience processes.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an equitable and inclusive UX for people with disabilities, particularly Deaf individuals whose communication needs are often overlooked in traditional evaluation methods.

**Method:** A critical review and practical application of recommended UX evaluation methods specific to Deaf users, assessing their strengths and limitations in real-world contexts.

**Key Contributions:**

	1. Identification of critical shortcomings in traditional UX evaluation methods for Deaf users.
	2. Recommendations for adapting evaluation methods to meet the needs of Deaf individuals.
	3. Validation of accessibility in real-world UX evaluation contexts for Deaf users.

**Result:** The study finds that conventional UX evaluation methods, designed for hearing users, have significant shortcomings when applied to Deaf individuals, hindering accurate data collection.

**Limitations:** The study primarily focuses on existing methods without proposing new methodologies to fill the identified gaps.

**Conclusion:** Adapting UX evaluation methods is essential for accurately reflecting the user experiences of Deaf users and ensuring communicational accessibility.

**Abstract:** Providing an equitable and inclusive user experience (UX) for people with disabilities (PWD) is a central goal of accessible design. In the specific case of Deaf users, whose hearing impairments impact language development and communication, it is essential to consider their specific needs during software evaluation processes. This study aimed to analyze a set of UX evaluation methods suggested in the literature as suitable for Deaf individuals, with the goal of validating their level of accessibility in real-world contexts. The research was based on a critical review and practical application of these methods, identifying their strengths and limitations in relation to the interaction, perception, and comprehension of Deaf users. Traditional evaluation instruments, commonly designed for hearing individuals, pose significant barriers when applied to Deaf users due to their re-liance on auditory and cognitive abilities, as well as the lack of consideration for commu-nicational accessibility. The results show that although these methods are frequently rec-ommended, they exhibit critical shortcomings that hinder the collection of accurate and representative data. It is concluded that it is essential to adapt UX evaluation methods to ensure genuinely accessible processes that address the communicative and cognitive needs of the Deaf community and accurately reflect their user experience.

</details>


### [34] [Through Their Eyes: User Perceptions on Sensitive Attribute Inference of Social Media Videos by Visual Language Models](https://arxiv.org/abs/2508.07658)

*Shuning Zhang, Gengrui Zhang, Yibo Meng, Ziyi Zhang, Hantao Zhao, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** Visual Language Models, privacy, user perceptions, sensitive attribute inference, social media

**Relevance Score:** 6

**TL;DR:** This paper investigates user perceptions of Visual Language Models (VLMs) regarding the inference of sensitive attributes from visual data, highlighting privacy concerns and user expectations for transparency and control.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to fill the gap in understanding user perspectives on the implications of VLMs for privacy, particularly in the context of social media video uploads.

**Method:** A semi-structured interview was conducted with 17 participants to gather insights on their experiences and concerns related to VLM-driven attribute inference.

**Key Contributions:**

	1. Highlights user concerns about VLMs and privacy risks.
	2. Provides insights into effective user mitigation strategies regarding privacy.
	3. Emphasizes the need for transparency and control in AI systems.

**Result:** Participants recognized VLMs' ability to infer various sensitive attributes with unsettling accuracy, raising concerns about unauthorized identification, surveillance, and potential misuse of personal information.

**Limitations:** The study's findings are based on a small sample size, which may not fully represent broader user perspectives.

**Conclusion:** The findings underscore the need for enhanced transparency, user control, and proactive privacy measures in the deployment of VLMs to align with user expectations and societal values.

**Abstract:** The rapid advancement of Visual Language Models (VLMs) has enabled sophisticated analysis of visual content, leading to concerns about the inference of sensitive user attributes and subsequent privacy risks. While technical capabilities of VLMs are increasingly studied, users' understanding, perceptions, and reactions to these inferences remain less explored, especially concerning videos uploaded on the social media. This paper addresses this gap through a semi-structured interview (N=17), investigating user perspectives on VLM-driven sensitive attribute inference from their visual data. Findings reveal that users perceive VLMs as capable of inferring a range of attributes, including location, demographics, and socioeconomic indicators, often with unsettling accuracy. Key concerns include unauthorized identification, misuse of personal information, pervasive surveillance, and harm from inaccurate inferences. Participants reported employing various mitigation strategies, though with skepticism about their ultimate effectiveness against advanced AI. Users also articulate clear expectations for platforms and regulators, emphasizing the need for enhanced transparency, user control, and proactive privacy safeguards. These insights are crucial for guiding the development of responsible AI systems, effective privacy-enhancing technologies, and informed policymaking that aligns with user expectations and societal values.

</details>


### [35] [Understanding Users' Privacy Perceptions Towards LLM's RAG-based Memory](https://arxiv.org/abs/2508.07664)

*Shuning Zhang, Rongjun Ma, Ying Ma, Shixuan Li, Yiqun Xu, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** Large Language Models, Memory Systems, User Research, Human-Computer Interaction, Privacy

**Relevance Score:** 9

**TL;DR:** This paper investigates user perceptions and expectations of memory functionalities in LLMs using thematic analysis of interviews with users.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand user mental models and practices regarding memory systems in LLMs, which are currently underexplored.

**Method:** The study conducted semi-structured interviews with 18 users to gather qualitative data for thematic analysis.

**Key Contributions:**

	1. Thematic analysis of user interviews to understand mental models of memory in LLMs
	2. Identification of key user concerns related to privacy and control
	3. Recommendations for designing more user-centric LLM memory systems

**Result:** Findings showed users have diverse and incomplete mental models of memory systems, with significant concerns about privacy, control, and accuracy of remembered information.

**Limitations:** 

**Conclusion:** The study emphasizes the need for user-centric designs in LLM memory systems, focusing on transparency and control for users.

**Abstract:** Large Language Models (LLMs) are increasingly integrating memory functionalities to provide personalized and context-aware interactions. However, user understanding, practices and expectations regarding these memory systems are not yet well understood. This paper presents a thematic analysis of semi-structured interviews with 18 users to explore their mental models of LLM's Retrieval Augmented Generation (RAG)-based memory, current usage practices, perceived benefits and drawbacks, privacy concerns and expectations for future memory systems. Our findings reveal diverse and often incomplete mental models of how memory operates. While users appreciate the potential for enhanced personalization and efficiency, significant concerns exist regarding privacy, control and the accuracy of remembered information. Users express a desire for granular control over memory generation, management, usage and updating, including clear mechanisms for reviewing, editing, deleting and categorizing memories, as well as transparent insight into how memories and inferred information are used. We discuss design implications for creating more user-centric, transparent, and trustworthy LLM memory systems.

</details>


### [36] [Towards Aligning Personalized Conversational Recommendation Agents with Users' Privacy Preferences](https://arxiv.org/abs/2508.07672)

*Shuning Zhang, Ying Ma, Jingruo Chen, Simin Li, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** AI agents, privacy management, Contextual Integrity, Privacy Calculus, alignment problem

**Relevance Score:** 8

**TL;DR:** This position paper advocates for a shift in privacy management from user unilateral control to proactive alignment by AI agents with user privacy preferences, proposing a framework based on Contextual Integrity and Privacy Calculus theory.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The existing privacy paradigms fail in the context of AI agents, which act in complex and context-dependent manners, necessitating a new model for privacy management.

**Method:** The paper proposes a conceptual framework that reframes privacy control as an alignment problem, where AI agents learn user privacy preferences through feedback and optimize the balance between privacy and utility using alignment and Pareto optimization.

**Key Contributions:**

	1. Proposed a new privacy management framework for AI agents.
	2. Reframed privacy control as an alignment problem.
	3. Identified key challenges and potential applications for personalized conversational recommendation agents.

**Result:** Introduced formulations and instantiations for AI privacy management and identified five challenges associated with the proposed framework.

**Limitations:** The position is theoretical and may require empirical validation and further development of the proposed framework.

**Conclusion:** Proactive alignment of AI agents with user privacy preferences can enhance privacy protection and improve user-agent interaction dynamics in the context of AI proliferation.

**Abstract:** The proliferation of AI agents, with their complex and context-dependent actions, renders conventional privacy paradigms obsolete. This position paper argues that the current model of privacy management, rooted in a user's unilateral control over a passive tool, is inherently mismatched with the dynamic and interactive nature of AI agents. We contend that ensuring effective privacy protection necessitates that the agents proactively align with users' privacy preferences instead of passively waiting for the user to control. To ground this shift, and using personalized conversational recommendation agents as a case, we propose a conceptual framework built on Contextual Integrity (CI) theory and Privacy Calculus theory. This synthesis first reframes automatically controlling users' privacy as an alignment problem, where AI agents initially did not know users' preferences, and would learn their privacy preferences through implicit or explicit feedback. Upon receiving the preference feedback, the agents used alignment and Pareto optimization for aligning preferences and balancing privacy and utility. We introduced formulations and instantiations, potential applications, as well as five challenges.

</details>


### [37] [Improving Continuous Grasp Force Decoding from EEG with Time-Frequency Regressors and Premotor-Parietal Network Integration](https://arxiv.org/abs/2508.07677)

*Parth G. Dangi, Yogesh Kumar Meena*

**Main category:** cs.HC

**Keywords:** brain-machine interfaces, EEG, neuro-rehabilitation, force decoding, assistive robotics

**Relevance Score:** 6

**TL;DR:** This paper presents EEGForceMap, a methodology for improving brain-machine interfaces (BMIs) by enhancing EEG-based decoding of grasp force for neuro-rehabilitation. It achieves significant performance improvements over existing models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve the decoding of continuous grasp force in brain-machine interfaces (BMIs) for better effectiveness in neuro-rehabilitation and assistive robotics.

**Method:** EEGForceMap isolates EEG signals from the premotor-parietal region and constructs three time-frequency feature sets for force prediction using various regressor models including linear, non-linear, and deep learning approaches, validated on the WAY-EEG-GAL dataset.

**Key Contributions:**

	1. Introduction of EEGForceMap methodology for force decoding in BMIs
	2. Significant performance improvements over existing models for subject-specific and subject-independent conditions
	3. Integration of neurophysiological insights into EEG-based decoding strategies.

**Result:** The EEGForceMap approach led to a 61.7% improvement in subject-specific conditions (R-squared = 0.815) and a 55.7% improvement in subject-independent conditions (R-squared = 0.785) compared to state-of-the-art kinematic decoders.

**Limitations:** 

**Conclusion:** The study enhances decoding accuracy in BMIs, contributing to advancements in stroke rehabilitation and assistive technologies by using neurophysiological insights effectively.

**Abstract:** Brain-machine interfaces (BMIs) have significantly advanced neuro-rehabilitation by enhancing motor control. However, accurately decoding continuous grasp force remains a challenge, limiting the effectiveness of BMI applications for fine motor tasks. Current models tend to prioritise algorithmic complexity rather than incorporating neurophysiological insights into force control, which is essential for developing effective neural engineering solutions. To address this, we propose EEGForceMap, an EEG-based methodology that isolates signals from the premotor-parietal region and extracts task-specific components. We construct three distinct time-frequency feature sets, which are validated by comparing them with prior studies, and use them for force prediction with linear, non-linear, and deep learning-based regressors. The performance of these regressors was evaluated on the WAY-EEG-GAL dataset that includes 12 subjects. Our results show that integrating EEGForceMap approach with regressor models yields a 61.7% improvement in subject-specific conditions (R-squared = 0.815) and a 55.7% improvement in subject-independent conditions (R-squared = 0.785) over the state-of-the-art kinematic decoder models. Furthermore, an ablation study confirms that each preprocessing step significantly enhances decoding accuracy. This work contributes to the advancement of responsive BMIs for stroke rehabilitation and assistive robotics by improving EEG-based decoding of dynamic grasp force.

</details>


### [38] [SimViews: An Interactive Multi-Agent System Simulating Visitor-to-Visitor Conversational Patterns to Present Diverse Perspectives of Artifacts in Virtual Museums](https://arxiv.org/abs/2508.07730)

*Mingyang Su, Chao Liu, Jingling Zhang, WU Shuang, Mingming Fan*

**Main category:** cs.HC

**Keywords:** virtual museum, multi-agent system, LLM-powered agents, diverse perspectives, user engagement

**Relevance Score:** 8

**TL;DR:** SimViews is an interactive multi-agent system that enhances visitor engagement and understanding in virtual museums by presenting diverse perspectives through simulated conversations between LLM-powered agents.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenge of presenting multiple narratives in virtual museums to enhance visitor experience and understanding, as traditional methods often limit the perspectives available to users.

**Method:** The study involved the development of SimViews, which uses LLM-powered multi-agents to simulate visitor-to-visitor interactions with diverse professional identities. Four conversational patterns were established to facilitate these interactions, and a within-subjects study was conducted with 20 participants.

**Key Contributions:**

	1. Introduction of SimViews, a multi-agent system for virtual museums.
	2. Demonstration of effective conversational patterns for diverse perspective presentation.
	3. Empirical evidence showing increased engagement and understanding in museum contexts.

**Result:** The results indicate that SimViews significantly improves the presentation of diverse perspectives and increases user engagement and understanding compared to a traditional single-agent system.

**Limitations:** The study was limited to a small sample size of 20 participants, and the long-term effects of using SimViews were not assessed.

**Conclusion:** SimViews effectively engages virtual museum visitors by fostering conversational exchanges that highlight multiple viewpoints, ultimately enriching the museum experience.

**Abstract:** Offering diverse perspectives on a museum artifact can deepen visitors' understanding and help avoid the cognitive limitations of a single narrative, ultimately enhancing their overall experience. Physical museums promote diversity through visitor interactions. However, it remains a challenge to present multiple voices appropriately while attracting and sustaining a visitor's attention in the virtual museum. Inspired by recent studies that show the effectiveness of LLM-powered multi-agents in presenting different opinions about an event, we propose SimViews, an interactive multi-agent system that simulates visitor-to-visitor conversational patterns to promote the presentation of diverse perspectives. The system employs LLM-powered multi-agents that simulate virtual visitors with different professional identities, providing diverse interpretations of artifacts. Additionally, we constructed 4 conversational patterns between users and agents to simulate visitor interactions. We conducted a within-subject study with 20 participants, comparing SimViews to a traditional single-agent condition. Our results show that SimViews effectively facilitates the presentation of diverse perspectives through conversations, enhancing participants' understanding of viewpoints and engagement within the virtual museum.

</details>


### [39] [CognitiveArm: Enabling Real-Time EEG-Controlled Prosthetic Arm Using Embodied Machine Learning](https://arxiv.org/abs/2508.07731)

*Abdul Basit, Maha Nawaz, Saim Rehman, Muhammad Shafique*

**Main category:** cs.HC

**Keywords:** brain-computer interface, EEG, deep learning, prosthetic systems, real-time processing

**Relevance Score:** 8

**TL;DR:** CognitiveArm is an EEG-driven prosthetic system utilizing optimized deep learning models integrated with BrainFlow for real-time control on embedded hardware, achieving high accuracy with voice command support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for efficient control of prosthetic limbs via non-invasive brain-computer interfaces, particularly on resource-constrained edge AI hardware.

**Method:** The system employs advanced EEG processing, including pre-filtering, feature extraction, and action prediction. It uses hyperparameter tuning, optimizer analysis, ensemble configurations, and model compression techniques like pruning and quantization to enhance model performance for embedded deployment.

**Key Contributions:**

	1. Development of a real-time EEG-driven prosthetic control system on embedded AI hardware.
	2. Use of model compression techniques to optimize deep learning models for efficiency.
	3. Implementation of voice command functionality for enhanced usability.

**Result:** CognitiveArm, tested with the OpenBCI UltraCortex Mark IV EEG headset, achieved up to 90% accuracy in classifying actions (left, right, idle) and can control the prosthetic arm's 3 DoF with voice commands for improved functionality in everyday tasks.

**Limitations:** 

**Conclusion:** The successful integration of EEG-driven control with voice command support demonstrates the potential for enhancing real-world performance of advanced prosthetic systems.

**Abstract:** Efficient control of prosthetic limbs via non-invasive brain-computer interfaces (BCIs) requires advanced EEG processing, including pre-filtering, feature extraction, and action prediction, performed in real time on edge AI hardware. Achieving this on resource-constrained devices presents challenges in balancing model complexity, computational efficiency, and latency. We present CognitiveArm, an EEG-driven, brain-controlled prosthetic system implemented on embedded AI hardware, achieving real-time operation without compromising accuracy. The system integrates BrainFlow, an open-source library for EEG data acquisition and streaming, with optimized deep learning (DL) models for precise brain signal classification. Using evolutionary search, we identify Pareto-optimal DL configurations through hyperparameter tuning, optimizer analysis, and window selection, analyzed individually and in ensemble configurations. We apply model compression techniques such as pruning and quantization to optimize models for embedded deployment, balancing efficiency and accuracy. We collected an EEG dataset and designed an annotation pipeline enabling precise labeling of brain signals corresponding to specific intended actions, forming the basis for training our optimized DL models. CognitiveArm also supports voice commands for seamless mode switching, enabling control of the prosthetic arm's 3 degrees of freedom (DoF). Running entirely on embedded hardware, it ensures low latency and real-time responsiveness. A full-scale prototype, interfaced with the OpenBCI UltraCortex Mark IV EEG headset, achieved up to 90% accuracy in classifying three core actions (left, right, idle). Voice integration enables multiplexed, variable movement for everyday tasks (e.g., handshake, cup picking), enhancing real-world performance and demonstrating CognitiveArm's potential for advanced prosthetic control.

</details>


### [40] [Challenges in Mixed Reality in Assisting Adults with ADHD Symptoms](https://arxiv.org/abs/2508.07854)

*Valerie Tan, Jens Gerken*

**Main category:** cs.HC

**Keywords:** ADHD, mixed reality, treatment, adults, intervention

**Relevance Score:** 7

**TL;DR:** This position paper explores the potential of mixed reality in aiding adults with ADHD symptoms, highlighting challenges in prototype availability and continuous intervention solutions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To examine how mixed reality can assist adults with ADHD symptoms and identify gaps in existing solutions.

**Method:** Discussion of current treatment options and mixed reality's role, analyzing the limitations of existing prototypes and studies targeting adults with ADHD.

**Key Contributions:**

	1. Highlights the potential of mixed reality for ADHD assistance
	2. Identifies critical gaps in adult-focused interventions
	3. Proposes focus areas for future research and development

**Result:** Identified two major challenges: lack of adult-specific mixed reality prototypes and insufficient continuous intervention solutions for daily use.

**Limitations:** Limited commercial availability of mixed reality solutions and lack of continuous intervention prototypes tailored for adults with ADHD.

**Conclusion:** Mixed reality holds promise for assisting adults with ADHD, but significant gaps in prototypes and solutions hinder its implementation.

**Abstract:** In this position paper, we discuss symptoms of attention deficit hyperactivity disorder (ADHD) in adults, as well as available forms of treatment or assistance in the context of mixed reality. Mixed reality offers many potentials for assisting adults with symptoms commonly found in (but not limited to) ADHD, but the availability of mixed reality solutions is not only limited commercially, but also limited in terms of proof-of-concept prototypes. We discuss two major challenges with attention assistance using mixed reality solutions: the limited availability of adult-specific prototypes and studies, as well as the limited number of solutions that offer continuous intervention of ADHD-like symptoms that users can employ in their daily life.

</details>


### [41] [Early Explorations of Recommender Systems for Physical Activity and Well-being](https://arxiv.org/abs/2508.07980)

*Alan Said*

**Main category:** cs.HC

**Keywords:** recommender systems, trust, behavioral alignment, well-being, personalization

**Relevance Score:** 8

**TL;DR:** This paper presents a conceptual framework for tangible recommendations in recommender systems, focusing on how they influence users' physical actions, trust, and well-being.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of recommender systems in guiding users' physical actions through wearables, understanding the interpretation and trust in these recommendations has become increasingly important.

**Method:** The paper introduces a conceptual framework based on three design dimensions: trust and interpretation, intent alignment, and consequence awareness, examining their implications in embodied recommendation settings.

**Key Contributions:**

	1. Introduces a framework for tangible recommendations in embodied settings
	2. Highlights three critical design dimensions for user trust and interaction
	3. Suggests pathways for developing socially responsible and effective recommender systems.

**Result:** It identifies key limitations when applying traditional recommender logic in these contexts and provides design reflections and examples for future systems.

**Limitations:** The proposed framework needs empirical validation and might require adaptation across diverse user contexts and cultures.

**Conclusion:** Future recommender systems should promote long-term well-being, behavioral alignment, and socially responsible personalization to effectively influence user actions and routines.

**Abstract:** As recommender systems increasingly guide physical actions, often through wearables and coaching tools, new challenges arise around how users interpret, trust, and respond to this advice. This paper introduces a conceptual framework for tangible recommendations that influence users' bodies, routines, and well-being. We describe three design dimensions: trust and interpretation, intent alignment, and consequence awareness. These highlight key limitations in applying conventional recommender logic to embodied settings. Through examples and design reflections, we outline how future systems can support long-term well-being, behavioral alignment, and socially responsible personalization.

</details>


### [42] [EchoAid: Enhancing Livestream Shopping Accessibility for the DHH Community](https://arxiv.org/abs/2508.08020)

*Zeyu Yang, Zheng Wei, Yang Zhang, Xian Xu, Changyang He, Muzhi Zhou, Pan Hui*

**Main category:** cs.HC

**Keywords:** accessibility, livestream shopping, Deaf and Hard of Hearing, speech-to-text, large language models

**Relevance Score:** 8

**TL;DR:** Development of EchoAid, a mobile app to enhance livestream shopping for Deaf and Hard of Hearing users using speech-to-text and LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Livestream shopping often neglects the accessibility of the Deaf and Hard of Hearing community, creating barriers in information access and overload.

**Method:** Developed a mobile app named EchoAid, employing speech-to-text conversion, RSVP technology, and LLMs. Conducted exploratory studies and iterative prototyping based on user feedback, followed by an evaluation workshop with DHH participants.

**Key Contributions:**

	1. Designed EchoAid specifically for the DHH community in livestream shopping
	2. Integrated advanced technologies like speech-to-text, RSVP, and LLMs
	3. Validated through user feedback and workshops with actual DHH users

**Result:** EchoAid was successfully designed and validated, showing improved information extraction and reduced cognitive overload for DHH users during livestream shopping.

**Limitations:** 

**Conclusion:** EchoAid enhances the shopping experience for DHH users by simplifying information flow and fostering engagement.

**Abstract:** Livestream shopping platforms often overlook the accessibility needs of the Deaf and Hard of Hearing (DHH) community, leading to barriers such as information inaccessibility and overload. To tackle these challenges, we developed \textit{EchoAid}, a mobile app designed to improve the livestream shopping experience for DHH users. \textit{EchoAid} utilizes advanced speech-to-text conversion, Rapid Serial Visual Presentation (RSVP) technology, and Large Language Models (LLMs) to simplify the complex information flow in live sales environments. We conducted exploratory studies with eight DHH individuals to identify design needs and iteratively developed the \textit{EchoAid} prototype based on feedback from three participants. We then evaluate the performance of this system in a user study workshop involving 38 DHH participants. Our findings demonstrate the successful design and validation process of \textit{EchoAid}, highlighting its potential to enhance product information extraction, leading to reduced cognitive overload and more engaging and customized shopping experiences for DHH users.

</details>


### [43] [ChatGPT on the Road: Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience](https://arxiv.org/abs/2508.08101)

*Yeana Lee Bond, Mungyeong Choe, Baker Kasim Hasan, Arsh Siddiqui, Myounghoon Jeon*

**Main category:** cs.HC

**Keywords:** in-vehicle agents, ChatGPT, human-computer interaction, driving performance, conversational agents

**Relevance Score:** 8

**TL;DR:** This study explores a ChatGPT-based in-vehicle conversational agent, demonstrating improved driving performance and user experience compared to pre-scripted agents during a driving simulator experiment.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance natural interaction between drivers and in-vehicle conversational agents beyond pre-scripted prompts.

**Method:** Conducted an experiment with forty drivers in a motion-based driving simulator, comparing driving performance and subjective evaluations across three conditions: No agent, Pre-scripted agent, and ChatGPT-based agent.

**Key Contributions:**

	1. Demonstrated the efficacy of a ChatGPT-based conversational agent in driving contexts.
	2. Highlighted the advantages of continuous, multi-turn dialogues over pre-scripted interactions.
	3. Revealed diverse interaction patterns that enhance user engagement and comfort.

**Result:** Drivers using the ChatGPT-based agent showed more stable driving performance metrics (lower variability in acceleration and lane deviation) and rated the agent higher in competence and trust.

**Limitations:** The study is limited to a controlled driving simulator environment, which may not fully represent real-world driving conditions.

**Conclusion:** LLM-powered conversational agents could significantly improve driving safety and user experiences by enabling more engaging and contextually rich interactions during driving.

**Abstract:** Studies on in-vehicle conversational agents have traditionally relied on pre-scripted prompts or limited voice commands, constraining natural driver-agent interaction. To resolve this issue, the present study explored the potential of a ChatGPT-based in-vehicle agent capable of carrying continuous, multi-turn dialogues. Forty drivers participated in our experiment using a motion-based driving simulator, comparing three conditions (No agent, Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable. Results showed that the ChatGPT-based agent condition led to more stable driving performance across multiple metrics. Participants demonstrated lower variability in longitudinal acceleration, lateral acceleration, and lane deviation compared to the other two conditions. In subjective evaluations, the ChatGPT-based agent also received significantly higher ratings in competence, animacy, affective trust, and preference compared to the Pre-scripted agent. Our thematic analysis of driver-agent conversations revealed diverse interaction patterns in topics, including driving assistance/questions, entertainment requests, and anthropomorphic interactions. Our results highlight the potential of LLM-powered in-vehicle conversational agents to enhance driving safety and user experience through natural, context-rich interactions.

</details>


### [44] [Fuzzy Ontology Embeddings and Visual Query Building for Ontology Exploration](https://arxiv.org/abs/2508.08128)

*Vladimir Zhurov, John Kausch, Kamran Sedig, Mostafa Milani*

**Main category:** cs.HC

**Keywords:** ontologies, fuzzy logic, semantic search, information retrieval, visual exploration

**Relevance Score:** 6

**TL;DR:** FuzzyVis is a system facilitating intuitive exploration of complex ontologies using fuzzy logic and an interactive visual interface, enhancing user experience beyond traditional query languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by non-experts in navigating large, complex ontologies in various fields, particularly biomedicine and computational biology.

**Method:** FuzzyVis integrates a fuzzy logic-based querying model with fuzzy ontology embeddings and an interactive visual interface that allows users to combine concepts using logical operators.

**Key Contributions:**

	1. Introduction of a fuzzy logic-based querying model for ontologies
	2. Development of an interactive visual interface for query composition
	3. Demonstration of enhanced exploratory learning through case studies

**Result:** FuzzyVis enables users to create new composite concepts and perform approximate concept-level similarity searches, improving accessibility to complex information in ontologies.

**Limitations:** 

**Conclusion:** The case studies demonstrate FuzzyVis's effectiveness in supporting exploratory learning and meeting nuanced information needs.

**Abstract:** Ontologies play a central role in structuring knowledge across domains, supporting tasks such as reasoning, data integration, and semantic search. However, their large size and complexity, particularly in fields such as biomedicine, computational biology, law, and engineering, make them difficult for non-experts to navigate. Formal query languages such as SPARQL offer expressive access but require users to understand the ontology's structure and syntax. In contrast, visual exploration tools and basic keyword-based search interfaces are easier to use but often lack flexibility and expressiveness. We introduce FuzzyVis, a proof-of-concept system that enables intuitive and expressive exploration of complex ontologies. FuzzyVis integrates two key components: a fuzzy logic-based querying model built on fuzzy ontology embeddings, and an interactive visual interface for building and interpreting queries. Users can construct new composite concepts by selecting and combining existing ontology concepts using logical operators such as conjunction, disjunction, and negation. These composite concepts are matched against the ontology using fuzzy membership-based embeddings, which capture degrees of membership and support approximate, concept-level similarity search. The visual interface supports browsing, query composition, and partial search without requiring formal syntax. By combining fuzzy semantics with embedding-based reasoning, FuzzyVis enables flexible interpretation, efficient computation, and exploratory learning. Case studies demonstrate how FuzzyVis supports subtle information needs and helps users uncover relevant concepts in large, complex ontologies.

</details>


### [45] [Can AI Explanations Make You Change Your Mind?](https://arxiv.org/abs/2508.08158)

*Laura Spillner, Rachel Ringe, Robert Porzel, Rainer Malaka*

**Main category:** cs.HC

**Keywords:** AI decision support systems, explainability, user trust, human oversight, decision-making

**Relevance Score:** 8

**TL;DR:** This paper explores the factors affecting users' attention to AI explanations in decision support systems and its impact on trust and decision-making.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding user trust in AI recommendations is crucial to mitigate errors and biases in AI-based decision support systems.

**Method:** An online study was conducted to analyze participant engagement with AI explanations and their willingness to adapt to AI suggestions.

**Key Contributions:**

	1. Exploratory analysis of user interactions with AI explanations
	2. Identification of factors influencing attention to AI explanations
	3. Insights into the relationship between explanation consideration and trust

**Result:** Many participants did not spend sufficient time on the explanations provided by the AI, leading to uncertainties around their trust and oversight.

**Limitations:** The study may not account for all contextual variables that influence user behavior with AI explanations.

**Conclusion:** The findings suggest that users often overlook critical explanations, which can hinder effective human oversight and trust in AI systems.

**Abstract:** In the context of AI-based decision support systems, explanations can help users to judge when to trust the AI's suggestion, and when to question it. In this way, human oversight can prevent AI errors and biased decision-making. However, this rests on the assumption that users will consider explanations in enough detail to be able to catch such errors. We conducted an online study on trust in explainable DSS, and were surprised to find that in many cases, participants spent little time on the explanation and did not always consider it in detail. We present an exploratory analysis of this data, investigating what factors impact how carefully study participants consider AI explanations, and how this in turn impacts whether they are open to changing their mind based on what the AI suggests.

</details>


### [46] [Bringing Everyone to the Table: An Experimental Study of LLM-Facilitated Group Decision Making](https://arxiv.org/abs/2508.08242)

*Mohammed Alsobay, David M. Rothschild, Jake M. Hofman, Daniel G. Goldstein*

**Main category:** cs.HC

**Keywords:** group decision-making, large language models, information sharing, collaborative decision making, GRAIL

**Relevance Score:** 9

**TL;DR:** The paper explores the role of LLMs in group decision-making, revealing that while they enhance information sharing, this does not significantly impact decision outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses uneven information sharing in group decision-making, highlighting the underexplored potential of LLMs to aid group processes.

**Method:** A pre-registered randomized experiment with 1,475 participants across 281 groups completing a hidden profile task under various facilitation conditions (LLM, human, prompting messages).

**Key Contributions:**

	1. Investigation of LLMs for group facilitation in decision-making contexts
	2. Release of the Group-AI Interaction Laboratory (GRAIL) as an open-source tool
	3. Evidence that LLM facilitation improves information sharing but not decision outcomes.

**Result:** LLM facilitation increases information sharing by enhancing engagement, but does not significantly alter decision outcomes.

**Limitations:** The results may not generalize beyond the specific task used in the experiment.

**Conclusion:** While LLMs can improve information sharing in group settings, their impact on final decision quality remains limited; the hidden profile effect persists.

**Abstract:** Group decision-making often suffers from uneven information sharing, hindering decision quality. While large language models (LLMs) have been widely studied as aids for individuals, their potential to support groups of users, potentially as facilitators, is relatively underexplored. We present a pre-registered randomized experiment with 1,475 participants assigned to 281 five-person groups completing a hidden profile task--selecting an optimal city for a hypothetical sporting event--under one of four facilitation conditions: no facilitation, a one-time message prompting information sharing, a human facilitator, or an LLM (GPT-4o) facilitator. We find that LLM facilitation increases information shared within a discussion by raising the minimum level of engagement with the task among group members, and that these gains come at limited cost in terms of participants' attitudes towards the task, their group, or their facilitator. Whether by human or AI, there is no significant effect of facilitation on the final decision outcome, suggesting that even substantial but partial increases in information sharing are insufficient to overcome the hidden profile effect studied. To support further research into how LLM-based interfaces can support the future of collaborative decision making, we release our experimental platform, the Group-AI Interaction Laboratory (GRAIL), as an open-source tool.

</details>


### [47] [Metabook: A Mobile-to-Headset Pipeline for 3D Story Book Creation in Augmented Reality](https://arxiv.org/abs/2405.13701)

*Yibo Wang, Yuanyuan Mao, Lik-Hang Lee, Shi-ting Ni, Zeyu Wang, Xiaole Gu, Pan Hui*

**Main category:** cs.HC

**Keywords:** AR 3D books, automated creation, learning outcomes, children education, Metabook

**Relevance Score:** 6

**TL;DR:** This paper presents Metabook, a system for automating the creation of AR 3D books, which enhances learning outcomes for children but shows mixed results in comprehension.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how AI can assist in the time-intensive development of 3D educational books.

**Method:** Conducted a workshop to gather design insights and developed the Metabook system, followed by studies on user creation of 3D books and their effects on children's learning.

**Key Contributions:**

	1. Development of Metabook for automated 3D book creation
	2. First end-to-end 3D book generation system
	3. Empirical findings on the impact of AR 3D books on children's learning

**Result:** Metabook allows novice users to generate 3D books quickly, and a study revealed increased interest and memory retention in children using 3D books over traditional ones.

**Limitations:** No significant improvement in comprehension observed.

**Conclusion:** 3D books enhance certain aspects of learning but do not significantly improve comprehension; recommendations for educators are provided for effective use.

**Abstract:** The AR 3D book has shown significant potential in enhancing students' learning outcomes. However, the creation process of 3D books requires a significant investment of time, effort, and specialized skills. Thus, in this paper, we first conduct a three-day workshop investigating how AI can support the automated creation of 3D books. Informed by the design insights derived from the workshop, we developed Metabook, a system that enables even novice users to create 3D books from text automatically. To our knowledge, Metabook is the first system to offer end-to-end 3D book generation. A follow-up study with adult users indicates that Metabook enables inexperienced users to create 3D books, achieving reduced efforts and shortened preparation time. We subsequently recruited 22 children to examine the effects of AR 3D books on children's learning compared with paper-based books. The findings indicate that 3D books significantly enhance children's interest, improve memory retention, and reduce cognitive load, though no significant improvement was observed in comprehension. We conclude by discussing strategies for more effectively leveraging 3D books to support children's learning and offer practical recommendations for educators.

</details>


### [48] [Understanding the Prevalence of Caste: A Critical Discourse Analysis of Caste-based Marginalization on X](https://arxiv.org/abs/2407.02810)

*Nayana Kirasur, Shagun Jhaver*

**Main category:** cs.HC

**Keywords:** caste discrimination, social media, HCI, critical discourse analysis, marginalization

**Relevance Score:** 4

**TL;DR:** This paper analyzes how upper-caste groups use social media (X) to perpetuate caste ideologies, leveraging platform affordances to normalize caste discrimination and marginalize lower castes, while providing suggestions for critical HCI research.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to explore the persistence of caste-based discrimination in India through social media, despite anti-caste efforts.

**Method:** A critical discourse analysis (CDA) approach is employed to examine the strategies of 50 X profiles representing upper-caste collectives.

**Key Contributions:**

	1. Examination of social media's role in advancing caste ideologies.
	2. Identification of specific discursive strategies used by upper-caste groups.
	3. Recommendations for HCI research to address power and marginalization.

**Result:** The analysis reveals that upper-caste profiles use social media to bolster their claims of superiority and victimhood, thus contributing to the perpetuation of caste discrimination.

**Limitations:** 

**Conclusion:** The study provides insights on how HCI can approach caste-based marginalization and offers suggestions for future research on power dynamics in social contexts.

**Abstract:** Despite decades of anti-caste efforts, sociocultural practices that marginalize lower-caste groups in India remain prevalent and have even proliferated with the use of social media. This paper examines how groups engaged in caste-based discrimination leverage platform affordances of the social media site X (formerly Twitter) to circulate and reinforce caste ideologies. Using a critical discourse analysis (CDA) approach, we examine the rhetorical and organizing strategies of 50 X profiles representing upper-caste collectives. We find that these profiles leverage platform affordances such as information control, bandwidth, visibility, searchability, and shareability to construct two main arguments: (1) that their upper caste culture deserves a superior status and (2) that they are the "true" victims of oppression in society. These profiles' digitally mediated discursive strategies contribute to the marginalization of lower castes by normalizing caste cultures, strengthening caste networks, reinforcing caste discrimination, and diminishing anti-caste measures. Our analysis builds upon previous HCI conceptualizations of online harms and safety to inform how to address caste-based marginalization. We offer theoretical and methodological suggestions for critical HCI research focused on studying the mechanisms of power along other social categories such as race and gender.

</details>


### [49] [Intents, Techniques, and Components: a Unified Analysis of Interaction Authoring Tasks in Data Visualization](https://arxiv.org/abs/2409.01399)

*Hyemi Song, Sai Gopinath, Zhicheng Liu*

**Main category:** cs.HC

**Keywords:** interaction authoring, data visualization, framework, interaction units, tool development

**Relevance Score:** 5

**TL;DR:** The paper analyzes 592 interaction units to develop a framework for interaction authoring in data visualization, emphasizing intents, techniques, and implementation components.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for expressive tools for interactivity specification and authoring in data visualization, addressing a significant gap in existing taxonomies that focus on usage rather than composition of interactivity.

**Method:** Conducted an analysis of 592 interaction units from 47 visualization applications to develop a unified framework for interaction authoring tasks.

**Key Contributions:**

	1. Unified framework for interaction authoring tasks
	2. Analysis of interaction units from real-world applications
	3. Insights into existing tools and future development

**Result:** Proposed a framework categorizing interaction authoring tasks into intents, representative techniques, and low-level implementation components, useful for critiquing existing tools.

**Limitations:** 

**Conclusion:** The framework enhances understanding of interaction authoring, with potential applications in evaluating current tools and guiding future tool development.

**Abstract:** There is a growing interest in designing tools to support interactivity specification and authoring in data visualization. To develop expressive and flexible tools, we need theories and models that describe the task space of interaction authoring. Although multiple taxonomies and frameworks exist for interactive visualization, they primarily focus on how visualizations are used, not how interactivity is composed. To fill this gap, we conduct an analysis of 592 interaction units from 47 real-world visualization applications. Based on the analysis, we present a unified analysis of interaction authoring tasks across three levels of description: intents, representative techniques, and low-level implementation components. We examine our framework's descriptive, evaluative, and generative powers for critiquing existing interactivity authoring tools and informing new tool development.

</details>


### [50] [Augmented Reality Assistive Technologies for Disabled Individuals](https://arxiv.org/abs/2409.02053)

*Riju Marwah, Jyotin Singh Thakur, Pranav Tanwar*

**Main category:** cs.HC

**Keywords:** Augmented Reality, assistive technologies, disabilities, accessibility, ethical considerations

**Relevance Score:** 7

**TL;DR:** The paper reviews Augmented Reality (AR) technologies aimed at assisting individuals with disabilities through case studies, discussing their impact, challenges, and ethical considerations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how AR can enhance interactions and provide support to individuals with disabilities, effectively addressing accessibility challenges.

**Method:** The paper conducts an in-depth analysis of four case studies focused on AR assistive technologies, utilizing theory analysis, practical examples, and future projections.

**Key Contributions:**

	1. Comprehensive overview of current AR assistive technologies
	2. Analysis of case studies showcasing real-world applications
	3. Discussion on ethical considerations and future projections for AR in accessibility.

**Result:** The findings demonstrate AR's potential to bridge accessibility gaps for individuals with disabilities, while also addressing implementation challenges and ethical concerns.

**Limitations:** Focused only on four case studies; may not represent all AR technologies or all types of disabilities.

**Conclusion:** AR technologies can significantly transform the lives of individuals with disabilities, but innovation needs to consider ethical implications and practical challenges.

**Abstract:** Augmented Reality (AR) technologies hold immense potential for revolutionizing the way individuals with disabilities interact with the world. AR systems can provide real-time assistance and support by overlaying digital information over the physical environment based on the requirements of the use, hence addressing different types of disabilities. Through an in-depth analysis of four case studies, this paper aims to provide a comprehensive overview of the current-state-of-the-art in AR assistive technologies for individuals with disabilities, highlighting their potential to assist and transform their lives. The findings show the significance that AR has made to bridge the accessibility gap, while also discussing the challenges faced and ethical considerations associated with the implementation across the various cases. This is done through theory analysis, practical examples, and future projections that will motivate and seek to inspire further innovation in this very relevant area of exploration.

</details>


### [51] [Harmony: A Human-Aware, Responsive, Modular Assistant with a Locally Deployed Large Language Model](https://arxiv.org/abs/2410.14252)

*Ziqi Yin, Mingxin Zhang, Daisuke Kawahara*

**Main category:** cs.HC

**Keywords:** Smart Home Assistants, Privacy Preservation, Large Language Models, User Interaction, Offline Systems

**Relevance Score:** 9

**TL;DR:** Harmony is a privacy-preserving smart home assistant that utilizes the Llama3-8B model, addressing user privacy concerns associated with cloud-based systems while also improving reliability in model performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of user privacy and reliability in existing cloud-based smart home assistants powered by large language models, which often compromise user data and depend on continuous external connectivity.

**Method:** The paper presents Harmony, a locally deployable smart home assistant that uses the Llama3-8B model, implementing structured prompting and modular agent design to improve model reliability and user privacy.

**Key Contributions:**

	1. Development of Harmony, a locally deployable smart home assistant
	2. Structured prompting and modular agent design to enhance performance of smaller models
	3. Empirical validation of Harmony's performance compared to GPT-4 systems

**Result:** Experimental results demonstrate that Harmony offers performance on par with GPT-4-based systems, maintaining privacy and enabling offline functionality.

**Limitations:** 

**Conclusion:** Harmony represents a significant step towards privacy-aware and reliable smart home interactions, showing that local model deployment can compete with advanced cloud-based solutions.

**Abstract:** Large Language Models (LLMs) offer powerful capabilities for natural language understanding, enabling more intelligent smart home assistants. However, existing systems often rely on cloud-based LLMs, raising concerns around user privacy and system dependency on external connectivity. In this work, we present Harmony, a privacy-preserving and robust smart home assistant powered by the locally deployable Llama3-8B model. Beyond protecting user data, Harmony also addresses reliability challenges of smaller models, such as hallucination and instruction misinterpretation, through structured prompting and modular agent design. Experimental results in both virtual environments and user studies show that Harmony achieves performance comparable to GPT-4-based systems, while enabling offline, proactive, and personalized smart home interaction.

</details>


### [52] [Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users](https://arxiv.org/abs/2410.21596)

*Auren R. Liu, Pat Pataranutaporn, Pattie Maes*

**Main category:** cs.HC

**Keywords:** companion chatbots, loneliness, psychosocial well-being, AI ethics, user profiles

**Relevance Score:** 8

**TL;DR:** This study investigates the relationship between companion chatbot usage and loneliness, analyzing user characteristics to identify seven distinct profiles that affect psychosocial well-being.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of companion chatbots on users' psychosocial well-being and address ethical considerations in their design and deployment.

**Method:** A large-scale survey (n = 404) was conducted to analyze user characteristics, alongside cluster analysis and mixed-methods thematic analysis combining manual coding with automated theme extraction.

**Key Contributions:**

	1. Identification of user profiles that influence the impact of chatbot usage on loneliness
	2. Development of a model explaining significant variance in loneliness
	3. Insights for ethical AI companionship design and its implications for social support

**Result:** The model explained approximately 50% of the variance in loneliness. Factors such as neuroticism, social network size, and problematic use were identified. Distinct user profiles varied in their outcomes, with some enhancing social confidence and others risking isolation.

**Limitations:** 

**Conclusion:** The findings suggest that different chatbot usage patterns lead to varying impacts on psychological well-being, underlining the need for targeted, ethically responsible design in AI companionship.

**Abstract:** Companion chatbots offer a potential solution to the growing epidemic of loneliness, but their impact on users' psychosocial well-being remains poorly understood, raising critical ethical questions about their deployment and design. This study presents a large-scale survey (n = 404) of regular users of companion chatbots, investigating the relationship between chatbot usage and loneliness. We develop a model explaining approximately 50% of variance in loneliness; while usage does not directly predict loneliness, we identify factors including neuroticism, social network size, and problematic use. Through cluster analysis and mixed-methods thematic analysis combining manual coding with automated theme extraction, we identify seven distinct user profiles demonstrating that companion chatbots can either enhance or potentially harm psychological well-being depending on user characteristics. Different usage patterns can lead to markedly different outcomes, with some users experiencing enhanced social confidence while others risk further isolation. These findings have significant implications for responsible AI development, suggesting that one-size-fits-all approaches to AI companionship may be ethically problematic. Our work contributes to the ongoing dialogue about the role of AI in social and emotional support, offering insights for developing more targeted and ethical approaches to AI companionship that complement rather than replace human connections.

</details>


### [53] [Steering AI-Driven Personalization of Scientific Text for General Audiences](https://arxiv.org/abs/2411.09969)

*Taewook Kim, Dhruv Agarwal, Jordan Ackerman, Manaswi Saha*

**Main category:** cs.HC

**Keywords:** AI, personalized translations, science communication, human-AI alignment, interactive tools

**Relevance Score:** 8

**TL;DR:** TranSlider is an AI-powered tool that personalizes scientific text translations based on user profiles, enhancing science communication.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of effective science communication to diverse audiences with varying expertise and backgrounds.

**Method:** The methodology involved designing TranSlider, an interactive tool that uses LLMs to generate personalized translations of scientific texts, tested through an exploratory study with 15 participants.

**Key Contributions:**

	1. Development of TranSlider for personalized scientific text translations
	2. Exploration of user preferences in personalized translations
	3. Insights into the impact of personalization on user understanding of scientific content

**Result:** The study found varied preferences among participants regarding the degree of personalization, impacting their understanding and reading experiences of scientific content.

**Limitations:** Limited sample size of 15 participants in the exploratory study

**Conclusion:** The findings highlight the potential of personalized AI translations in improving science communication and suggest design implications for user interfaces that support human-AI alignment.

**Abstract:** Digital media platforms (e.g., science blogs) offer opportunities to communicate scientific content to general audiences at scale. However, these audiences vary in their scientific expertise, literacy levels, and personal backgrounds, making effective science communication challenging. To address this challenge, we designed TranSlider, an AI-powered tool that generates personalized translations of scientific text based on individual user profiles (e.g., hobbies, location, and education). Our tool features an interactive slider that allows users to steer the degree of personalization from 0 (weakly relatable) to 100 (strongly relatable), leveraging LLMs to generate the translations with chosen degrees. Through an exploratory study with 15 participants, we investigated both the utility of these AI-personalized translations and how interactive reading features influenced users' understanding and reading experiences. We found that participants who preferred higher degrees of personalization appreciated the relatable and contextual translations, while those who preferred lower degrees valued concise translations with subtle contextualization. Furthermore, participants reported the compounding effect of multiple translations on their understanding of scientific content. Drawing on these findings, we discuss several implications for facilitating science communication and designing steerable interfaces to support human-AI alignment.

</details>


### [54] [Privacy-Preserving Behaviour of Chatbot Users: Steering Through Trust Dynamics](https://arxiv.org/abs/2411.17589)

*Julia Ive, Vishal Yadav, Mariia Ignashina, Matthew Rand, Paulina Bondaronek*

**Main category:** cs.HC

**Keywords:** chatbots, privacy risks, user awareness, data handling, privacy education

**Relevance Score:** 7

**TL;DR:** This study investigates user awareness of privacy risks in chatbot interactions and identifies a significant lack of understanding regarding data handling by chatbot providers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing importance of chatbots in daily life has raised privacy concerns that have not been adequately addressed.

**Method:** A mixed-methods approach was utilized, combining quantitative coding of responses and qualitative content analysis to explore user behaviors related to privacy.

**Key Contributions:**

	1. Identification of user misconceptions about chatbot data handling
	2. Revelation of under-protective behaviors even among knowledgeable users
	3. Recommendations for improved user education strategies on privacy

**Result:** Findings indicate a substantial lack of awareness about data handling among users, with 27% unaware of how their data is processed and 76% not understanding basic privacy risks. Older users particularly fear data selling by chatbot providers.

**Limitations:** The study is limited to specific user groups and may not be representative of all chatbot users.

**Conclusion:** There is a critical need for enhanced user education on privacy in chatbot interactions, including the development of monitoring tools for users.

**Abstract:** Introduction: The use of chatbots is becoming increasingly important across various aspects of daily life. However, the privacy concerns associated with these communications have not yet been thoroughly addressed. The aim of this study was to investigate user awareness of privacy risks in chatbot interactions, the privacy-preserving behaviours users practice, and how these behaviours relate to their awareness of privacy threats, even when no immediate threat is perceived. Methods: We developed a novel "privacy-safe" setup to analyse user behaviour under the guarantees of anonymization and non-sharing. We employed a mixed-methods approach, starting with the quantification of broader trends by coding responses, followed by conducting a qualitative content analysis to gain deeper insights. Results: Overall, there was a substantial lack of understanding among users about how chatbot providers handle data (27% of the participants) and the basics of privacy risks (76% of the participants). Older users, in particular, expressed fears that chatbot providers might sell their data. Moreover, even users with privacy knowledge do not consistently exhibit privacy-preserving behaviours when assured of transparent data processing by chatbots. Notably, under-protective behaviours were observed among more expert users. Discussion: These findings highlight the need for a strategic approach to enhance user education on privacy concepts to ensure informed decision when interacting with chatbot technology. This includes the development of tools to help users monitor and control the information they share with chatbots

</details>


### [55] [Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim Prioritization for Human Fact-checkers](https://arxiv.org/abs/2412.08185)

*Houjiang Liu, Jacek Gwizdka, Matthew Lease*

**Main category:** cs.HC

**Keywords:** claim prioritization, fact-checking, information retrieval, AI assistance, multidimensional checkworthiness

**Relevance Score:** 7

**TL;DR:** This study explores claim prioritization in fact-checking as an information retrieval task, emphasizing the need for AI-assisted tools to support fact-checkers in their triage process.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing volume of potentially false claims online, prioritizing which claims to fact-check is critical for efficient resource allocation in fact-checking.

**Method:** The paper utilizes Research through Design and mixed-method evaluation to develop a prototype for AI-assisted claim prioritization, involving 16 professional fact-checkers in the study.

**Key Contributions:**

	1. Development of an AI-assisted claim prioritization prototype
	2. Introduction of a hierarchical strategy used by fact-checkers for prioritization
	3. Actionable design recommendations for integrating LLMs in fact-checking processes

**Result:** The study reveals a hierarchical prioritization strategy used by fact-checkers that is previously underexplored, along with actionable design recommendations for enhancing claim triage with LLM integration.

**Limitations:** 

**Conclusion:** The findings suggest that considering multidimensional checkworthiness can significantly improve the fact-checking process and the design of supportive tools.

**Abstract:** Given the volume of potentially false claims online, claim prioritization is essential in allocating limited human resources available for fact-checking. In this study, we perceive claim prioritization as an information retrieval (IR) task: just as multidimensional IR relevance, with many factors influencing which search results a user deems relevant, checkworthiness is also multi-faceted, subjective, and even personal, with many factors influencing how fact-checkers triage and select which claims to check. Our study investigates both the multidimensional nature of checkworthiness and effective tool support to assist fact-checkers in claim prioritization. Methodologically, we pursue Research through Design combined with mixed-method evaluation.   Specifically, we develop an AI-assisted claim prioritization prototype as a probe to explore how fact-checkers use multidimensional checkworthy factors to prioritize claims, simultaneously probing fact-checker needs and exploring the design space to meet those needs. With 16 professional fact-checkers participating in our study, we uncover a hierarchical prioritization strategy fact-checkers implicitly use, revealing an underexplored aspect of their workflow, with actionable design recommendations for improving claim triage across multidimensional checkworthiness and tailoring this process with LLM integration.

</details>


### [56] [Frontend Diffusion: Empowering Self-Representation of Junior Researchers and Designers Through Multi-agent System](https://arxiv.org/abs/2502.03788)

*Zijian Ding, Qinshi Zhang, Mohan Chi, Ziyi Wang*

**Main category:** cs.HC

**Keywords:** generative AI, frontend development, human-AI alignment

**Relevance Score:** 7

**TL;DR:** Frontend Diffusion enhances junior researchers' and designers' self-representation by transforming their inputs into website code using AI.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To empower junior researchers and designers who struggle to express their professional identities through effective coding.

**Method:** A multi-agent coding system that converts user-drawn layouts and textual prompts into refined website code, evaluated through a user study with 13 participants.

**Key Contributions:**

	1. Introduction of Frontend Diffusion coding system
	2. User study demonstrating AI as an enhancer of human capabilities
	3. Discussion on the importance of human-AI alignment

**Result:** The study found that AI enhances human capabilities and emphasizes the need for bidirectional alignment between humans and AI systems.

**Limitations:** 

**Conclusion:** Future work will focus on leveraging AI for career development and improving human-AI collaboration in multi-agent systems.

**Abstract:** With the continuous development of generative AI's logical reasoning abilities, AI's growing code-generation potential poses challenges for both technical and creative professionals. But how can these advances be directed toward empowering junior researchers and designers who often require additional help to build and express their professional and personal identities? We introduce Frontend Diffusion, a multi-agent coding system transforming user-drawn layouts and textual prompts into refined website code, thereby supporting self-representation goals. A user study with 13 junior researchers and designers shows AI as a human capability enhancer rather than a replacement, and highlights the importance of bidirectional human-AI alignment. We then discuss future work such as leveraging AI for career development and fostering bidirectional human-AI alignment of multi-agent systems.

</details>


### [57] [Prompt-Hacking: The New p-Hacking?](https://arxiv.org/abs/2504.14571)

*Thomas Kosch, Sebastian Feger*

**Main category:** cs.HC

**Keywords:** Large Language Models, prompt-hacking, scientific integrity, data analysis, transparency

**Relevance Score:** 8

**TL;DR:** This opinion paper addresses the concerns of using LLMs in empirical research workflows, comparing 'prompt-hacking' to 'p-hacking' and advocating for transparency and caution in their application.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight risks associated with the use of LLMs in research and ensure scientific integrity amidst their growing prevalence.

**Method:** The authors draw parallels between prompt-hacking and p-hacking, examining the biases and opacity of LLMs that affect data analysis.

**Key Contributions:**

	1. Parallels between prompt-hacking and p-hacking
	2. Emphasis on scientific integrity in LLM use
	3. Recommendations for rigorous prompts documentation

**Result:** The paper warns that LLMs might lead researchers to manipulate prompts for desired outcomes, compromising the validity of their work.

**Limitations:** 

**Conclusion:** LLMs should only be used cautiously in research, with transparent documentation and strict guidelines on their applications.

**Abstract:** As Large Language Models (LLMs) become increasingly embedded in empirical research workflows, their use as analytical tools for quantitative or qualitative data raises pressing concerns for scientific integrity. This opinion paper draws a parallel between "prompt-hacking", the strategic tweaking of prompts to elicit desirable outputs from LLMs, and the well-documented practice of "p-hacking" in statistical analysis. We argue that the inherent biases, non-determinism, and opacity of LLMs make them unsuitable for data analysis tasks demanding rigor, impartiality, and reproducibility. We emphasize how researchers may inadvertently, or even deliberately, adjust prompts to confirm hypotheses while undermining research validity. We advocate for a critical view of using LLMs in research, transparent prompt documentation, and clear standards for when LLM use is appropriate. We discuss how LLMs can replace traditional analytical methods, whereas we recommend that LLMs should only be used with caution, oversight, and justification.

</details>


### [58] [Story Ribbons: Reimagining Storyline Visualizations with Large Language Models](https://arxiv.org/abs/2508.06772)

*Catherine Yeh, Tara Menon, Robin Singh Arya, Helen He, Moira Weigel, Fernanda Vi√©gas, Martin Wattenberg*

**Main category:** cs.HC

**Keywords:** Large Language Models, literature visualization, story analysis, user studies, interaction design

**Relevance Score:** 7

**TL;DR:** This paper presents an LLM-driven pipeline for extracting narrative information from literature, leading to the development of Story Ribbons, an interactive visualization tool for character and theme analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to leverage the capabilities of large language models to capture structured information from unstructured literary data to enhance visualization techniques for analyzing complex relationships in stories.

**Method:** An LLM-driven data parsing pipeline is introduced to automatically extract relevant narrative information from novels and scripts, which is then used to create Story Ribbons, an interactive visualization system.

**Key Contributions:**

	1. Introduction of an LLM-driven data parsing pipeline for narrative information extraction
	2. Development of the Story Ribbons visualization system
	3. Evaluation of the system through user studies on 36 literary works

**Result:** User studies and pipeline evaluations demonstrate that Story Ribbons successfully facilitates exploration of character and theme trajectories across various narrative levels and reveals new insights about literary works.

**Limitations:** Current limitations of AI-based systems are described, along with interaction motifs designed to address these issues.

**Conclusion:** The paper concludes that LLMs hold great promise for improving narrative visualization but also outlines the limitations of current AI-based systems and proposes interaction motifs to address these challenges.

**Abstract:** Analyzing literature involves tracking interactions between characters, locations, and themes. Visualization has the potential to facilitate the mapping and analysis of these complex relationships, but capturing structured information from unstructured story data remains a challenge. As large language models (LLMs) continue to advance, we see an opportunity to use their text processing and analysis capabilities to augment and reimagine existing storyline visualization techniques. Toward this goal, we introduce an LLM-driven data parsing pipeline that automatically extracts relevant narrative information from novels and scripts. We then apply this pipeline to create Story Ribbons, an interactive visualization system that helps novice and expert literary analysts explore detailed character and theme trajectories at multiple narrative levels. Through pipeline evaluations and user studies with Story Ribbons on 36 literary works, we demonstrate the potential of LLMs to streamline narrative visualization creation and reveal new insights about familiar stories. We also describe current limitations of AI-based systems, and interaction motifs designed to address these issues.

</details>


### [59] [Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI](https://arxiv.org/abs/2508.07520)

*Baihan Lin*

**Main category:** cs.HC

**Keywords:** Conversational DNA, Dialogue analysis, Human-Computer Interaction, Data visualization, Communication patterns

**Relevance Score:** 8

**TL;DR:** This paper introduces Conversational DNA, a visual language for analyzing dialogue as a living system, providing insights into communication patterns that traditional methods overlook.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how the structure of dialogue can reveal deeper insights about communication beyond the words exchanged, especially in contexts involving AI.

**Method:** The authors developed a visualization framework that uses biological metaphors to represent dialogue structure, analyzing therapeutic conversations and key human-AI interactions.

**Key Contributions:**

	1. Introduction of a novel visual language for dialogue analysis
	2. Application of biological metaphors to understand conversational structure
	3. Insights into communication patterns from both human and AI dialogues

**Result:** The analysis revealed unique interaction patterns in dialogues that traditional conversation analysis methods typically fail to uncover, indicating the importance of dialogue structure.

**Limitations:** 

**Conclusion:** Conversational DNA offers a novel framework that enhances our understanding of communication in both human and AI interactions, emphasizing the need to view dialogues as complex systems.

**Abstract:** What if the patterns hidden within dialogue reveal more about communication than the words themselves? We introduce Conversational DNA, a novel visual language that treats any dialogue -- whether between humans, between human and AI, or among groups -- as a living system with interpretable structure that can be visualized, compared, and understood. Unlike traditional conversation analysis that reduces rich interaction to statistical summaries, our approach reveals the temporal architecture of dialogue through biological metaphors. Linguistic complexity flows through strand thickness, emotional trajectories cascade through color gradients, conversational relevance forms through connecting elements, and topic coherence maintains structural integrity through helical patterns. Through exploratory analysis of therapeutic conversations and historically significant human-AI dialogues, we demonstrate how this visualization approach reveals interaction patterns that traditional methods miss. Our work contributes a new creative framework for understanding communication that bridges data visualization, human-computer interaction, and the fundamental question of what makes dialogue meaningful in an age where humans increasingly converse with artificial minds.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [60] [Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction](https://arxiv.org/abs/2508.06495)

*Juliana Resplande Sant'anna Gomes, Arlindo Rodrigues Galv√£o Filho*

**Main category:** cs.CL

**Keywords:** Semi-Automated Fact-Checking, disinformation, Large Language Models, Portuguese, news corpora

**Relevance Score:** 6

**TL;DR:** This dissertation develops a methodology for Semi-Automated Fact-Checking (SAFC) for Portuguese news by enriching news corpora with external evidence and leveraging LLMs for claim extraction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing pace of disinformation dissemination necessitates improved fact-checking systems, particularly for Portuguese-language content where data is scarce.

**Method:** The methodology involves utilizing Large Language Models to extract main claims from news articles and employing various search engine APIs to gather relevant external evidence. A validation framework is also introduced to enhance the quality of the datasets.

**Key Contributions:**

	1. Development of a methodology for enriching news corpora with external evidence
	2. Use of LLMs for claim extraction in fact-checking
	3. Introduction of a validation framework for quality enhancement

**Result:** The developed methodology successfully enriches Portuguese news corpora with external evidence, which is crucial for the effectiveness of SAFC systems.

**Limitations:** Focused solely on the Portuguese language and may not generalize to other languages.

**Conclusion:** Improving the accessibility and quality of fact-checking resources in Portuguese can significantly enhance the fight against disinformation.

**Abstract:** The accelerated dissemination of disinformation often outpaces the capacity for manual fact-checking, highlighting the urgent need for Semi-Automated Fact-Checking (SAFC) systems. Within the Portuguese language context, there is a noted scarcity of publicly available datasets that integrate external evidence, an essential component for developing robust AFC systems, as many existing resources focus solely on classification based on intrinsic text features. This dissertation addresses this gap by developing, applying, and analyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR, MuMiN-PT) with external evidence. The approach simulates a user's verification process, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash) to extract the main claim from texts and search engine APIs (Google Search API, Google FactCheck Claims Search API) to retrieve relevant external documents (evidence). Additionally, a data validation and preprocessing framework, including near-duplicate detection, is introduced to enhance the quality of the base corpora.

</details>


### [61] [Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models](https://arxiv.org/abs/2508.06504)

*Yao Ge, Sudeshna Das, Yuting Guo, Abeed Sarker*

**Main category:** cs.CL

**Keywords:** biomedical NER, large language models, few-shot learning, dynamic prompting, retrieval-augmented generation

**Relevance Score:** 9

**TL;DR:** This paper explores a dynamic prompting strategy using retrieval-augmented generation to enhance the performance of large language models in few-shot biomedical named entity recognition (NER).

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance challenges faced by large language models in few-shot biomedical named entity recognition, particularly when training data is limited.

**Method:** The authors investigated dynamic prompting strategies wherein annotated in-context learning examples are selected based on their similarities with the input texts, resulting in contextually adaptive prompts during inference.

**Key Contributions:**

	1. Dynamic prompting strategy that improves NER performance
	2. Implementation and optimization of static and dynamic prompt engineering techniques
	3. Evaluation across five biomedical NER datasets

**Result:** Static prompting techniques improved average F1-scores significantly by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA 3-70B compared to basic static prompting. Dynamic prompting yielded further enhancements, with TF-IDF and SBERT retrieval methods achieving the best results, improving average scores by 7.3% and 5.6% in 5-shot and 10-shot settings, respectively.

**Limitations:** 

**Conclusion:** Dynamic prompting through retrieval-augmented generation can markedly enhance the performance of large language models in biomedical named entity recognition tasks, particularly in few-shot scenarios.

**Abstract:** Biomedical named entity recognition (NER) is a high-utility natural language processing (NLP) task, and large language models (LLMs) show promise particularly in few-shot settings (i.e., limited training data). In this article, we address the performance challenges of LLMs for few-shot biomedical NER by investigating a dynamic prompting strategy involving retrieval-augmented generation (RAG). In our approach, the annotated in-context learning examples are selected based on their similarities with the input texts, and the prompt is dynamically updated for each instance during inference. We implemented and optimized static and dynamic prompt engineering techniques and evaluated them on five biomedical NER datasets. Static prompting with structured components increased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA 3-70B, relative to basic static prompting. Dynamic prompting further improved performance, with TF-IDF and SBERT retrieval methods yielding the best results, improving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings, respectively. These findings highlight the utility of contextually adaptive prompts via RAG for biomedical NER.

</details>


### [62] [CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models](https://arxiv.org/abs/2508.06524)

*Lei Jiang, Fan Chen*

**Main category:** cs.CL

**Keywords:** language models, carbon emissions, scaling laws, sustainability, HCI

**Relevance Score:** 8

**TL;DR:** This paper introduces 	extit{CarbonScaling}, a framework linking LLM training accuracy to carbon footprint, highlighting the need for sustainable AI practices.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing concern of carbon emissions associated with the training of large language models, which have been overlooked in traditional neural scaling laws.

**Method:** The authors present 	extit{CarbonScaling}, which combines models of neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation to analyze the relationship between model accuracy and carbon emissions.

**Key Contributions:**

	1. Introduction of the 	extit{CarbonScaling} framework for LLM training
	2. Quantification of the relationship between accuracy and carbon footprint
	3. Recommendations for training optimizations to enhance sustainability.

**Result:** Results indicate a power-law relationship between model accuracy and carbon emissions, with significant real-world inefficiencies that increase the scaling factor. While hardware scaling can reduce emissions for small to mid-sized models, it yields diminishing returns for very large LLMs due to operational inefficiencies.

**Limitations:** The analysis may not account for all environmental impacts associated with LLM training, and results may vary based on hardware specifics and usage scenarios.

**Conclusion:** The framework provides insights for developing carbon-efficient LLMs and emphasizes the importance of training optimizations to reduce carbon footprints in large-scale AI models.

**Abstract:** Neural scaling laws have driven the development of increasingly large language models (LLMs) by linking accuracy improvements to growth in parameter count, dataset size, and compute. However, these laws overlook the carbon emissions that scale exponentially with LLM size. This paper presents \textit{CarbonScaling}, an analytical framework that extends neural scaling laws to incorporate both operational and embodied carbon in LLM training. By integrating models for neural scaling, GPU hardware evolution, parallelism optimization, and carbon estimation, \textit{CarbonScaling} quantitatively connects model accuracy to carbon footprint. Results show that while a power-law relationship between accuracy and carbon holds, real-world inefficiencies significantly increase the scaling factor. Hardware technology scaling reduces carbon emissions for small to mid-sized models, but offers diminishing returns for extremely large LLMs due to communication overhead and underutilized GPUs. Training optimizations-especially aggressive critical batch size scaling-help alleviate this inefficiency. \textit{CarbonScaling} offers key insights for training more sustainable and carbon-efficient LLMs.

</details>


### [63] [The Art of Breaking Words: Rethinking Multilingual Tokenizer Design](https://arxiv.org/abs/2508.06533)

*Aamod Thakur, Ajay Nagpal, Atharva Savarkar, Kundeshwar Pundalik, Siddhesh Dosi, Piyush Sawarkar, Viraj Thakur, Rohit Saluja, Maunendra Sankar Desarkar, Ganesh Ramakrishnan*

**Main category:** cs.CL

**Keywords:** tokenization, multilingual models, LLM efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents a systematic study on the impact of tokenization in multilingual Large Language Models (LLMs), focusing on vocabulary size and pre-tokenization rules, culminating in a novel data composition algorithm that significantly improves token-to-word efficiency and model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the neglected aspect of tokenization in LLM development, particularly in multilingual contexts, which affects model quality and efficiency.

**Method:** The study involves a systematic analysis linking vocabulary size, pre-tokenization rules, and training corpus composition to token-to-word efficiency. Experiments are conducted using Indic scripts to examine the challenges posed by their diversity.

**Key Contributions:**

	1. Introduction of a novel data composition algorithm for tokenizer training
	2. Empirical evidence linking tokenization strategies to model performance
	3. Demonstration of improved token-to-word efficiency in multilingual settings

**Result:** The proposed algorithm for data composition reduces the average token-to-word ratio by approximately 6%, while achieving over 40% improvement on average token-to-word ratio compared to existing multilingual Indic models, leading to enhanced model performance and inference speed.

**Limitations:** 

**Conclusion:** The findings demonstrate that tokenization should be considered alongside model architecture and training objectives as a vital component in developing efficient and scalable multilingual LLMs.

**Abstract:** While model architecture and training objectives are well-studied, tokenization, particularly in multilingual contexts, remains a relatively neglected aspect of Large Language Model (LLM) development. Existing tokenizers often exhibit high token-to-word ratios, inefficient use of context length, and slower inference. We present a systematic study that links vocabulary size, pre-tokenization rules, and training-corpus composition to both token-to-word efficiency and model quality. To ground our analysis in a linguistically diverse context, we conduct extensive experiments on Indic scripts, which present unique challenges due to their high script diversity and orthographic complexity. Drawing on the insights from these analyses, we propose a novel algorithm for data composition that balances multilingual data for tokenizer training. Our observations on pretokenization strategies significantly improve model performance, and our data composition algorithm reduces the average token-to-word ratio by approximately 6% with respect to the conventional data randomization approach. Our tokenizer achieves more than 40% improvement on average token-to-word ratio against stateof-the-art multilingual Indic models. This improvement yields measurable gains in both model performance and inference speed. This highlights tokenization alongside architecture and training objectives as a critical lever for building efficient, scalable multilingual LLMs

</details>


### [64] [Factor Augmented Supervised Learning with Text Embeddings](https://arxiv.org/abs/2508.06548)

*Zhanye Luo, Yuefeng Han, Xiufan Yu*

**Main category:** cs.CL

**Keywords:** large language models, dimension reduction, autoencoders, machine learning, text embeddings

**Relevance Score:** 8

**TL;DR:** AEALT is a framework for incorporating dimension reduction into LLM workflows to enhance efficiency and performance in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The high dimensionality of LLM-generated embeddings increases computational cost and hinders performance in downstream tasks.

**Method:** The framework uses a supervised, augmented autoencoder to learn low-dimensional, task-relevant latent factors from text embeddings.

**Key Contributions:**

	1. Introduction of the AEALT framework which leverages autoencoders for dimension reduction in LLM workflows.
	2. Demonstrated improvements in efficiency and performance across multiple real-world tasks using AEALT.
	3. Validation of AEALT's applicability on diverse datasets through extensive experiments.

**Result:** AEALT outperforms conventional methods, demonstrating significant improvements across classification, anomaly detection, and prediction tasks.

**Limitations:** 

**Conclusion:** AEALT effectively reduces dimensionality while maintaining the semantic integrity of embeddings, leading to better performance in machine learning tasks.

**Abstract:** Large language models (LLMs) generate text embeddings from text data, producing vector representations that capture the semantic meaning and contextual relationships of words. However, the high dimensionality of these embeddings often impedes efficiency and drives up computational cost in downstream tasks. To address this, we propose AutoEncoder-Augmented Learning with Text (AEALT), a supervised, factor-augmented framework that incorporates dimension reduction directly into pre-trained LLM workflows. First, we extract embeddings from text documents; next, we pass them through a supervised augmented autoencoder to learn low-dimensional, task-relevant latent factors. By modeling the nonlinear structure of complex embeddings, AEALT outperforms conventional deep-learning approaches that rely on raw embeddings. We validate its broad applicability with extensive experiments on classification, anomaly detection, and prediction tasks using multiple real-world public datasets. Numerical results demonstrate that AEALT yields substantial gains over both vanilla embeddings and several standard dimension reduction methods.

</details>


### [65] [Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs](https://arxiv.org/abs/2508.06583)

*Ying Liu, Can Li, Ting Zhang, Mei Wang, Qiannan Zhu, Jian Li, Hua Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Adaptive Tutoring, Pedagogical Guidance, Cognitive States, Behavior-Guided Finetuning

**Relevance Score:** 9

**TL;DR:** This study proposes GuideEval, a benchmark evaluating large language models (LLMs) in adaptive tutoring by focusing on instructional guidance responsive to learners' cognitive states.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can emulate expert tutors by adaptively guiding learners based on their cognitive states rather than just generating questions.

**Method:** The study introduces GuideEval, a benchmark that uses a three-phase framework: Perception (inferring learner states), Orchestration (adapting strategies), and Elicitation (stimulating reflections).

**Key Contributions:**

	1. Introduction of GuideEval as a benchmark for evaluating adaptive pedagogical guidance
	2. Establishment of a three-phase behavioral framework for instructional strategies
	3. Development of a behavior-guided finetuning strategy that improves LLM performance in tutoring contexts.

**Result:** Existing LLMs often fail to provide effective adaptive scaffolding. However, a behavior-guided finetuning strategy significantly improves their guidance performance.

**Limitations:** 

**Conclusion:** There is a need to shift from content evaluation to a learner-centered interaction paradigm for effective Socratic LLMs.

**Abstract:** The conversational capabilities of large language models hold significant promise for enabling scalable and interactive tutoring. While prior research has primarily examined their capacity for Socratic questioning, it often overlooks a critical dimension: adaptively guiding learners based on their cognitive states. This study shifts focus from mere question generation to the broader instructional guidance capability. We ask: Can LLMs emulate expert tutors who dynamically adjust strategies in response to learners' understanding? To investigate this, we propose GuideEval, a benchmark grounded in authentic educational dialogues that evaluates pedagogical guidance through a three-phase behavioral framework: (1) Perception, inferring learner states; (2) Orchestration, adapting instructional strategies; and (3) Elicitation, stimulating proper reflections. Empirical findings reveal that existing LLMs frequently fail to provide effective adaptive scaffolding when learners exhibit confusion or require redirection. Furthermore, we introduce a behavior-guided finetuning strategy that leverages behavior-prompted instructional dialogues, significantly enhancing guidance performance. By shifting the focus from isolated content evaluation to learner-centered interaction, our work advocates a more dialogic paradigm for evaluating Socratic LLMs.

</details>


### [66] [LLM Unlearning Without an Expert Curated Dataset](https://arxiv.org/abs/2508.06595)

*Xiaoyuan Zhu, Muru Zhang, Ollie Liu, Robin Jia, Willie Neiswanger*

**Main category:** cs.CL

**Keywords:** large language models, unlearning, synthetic datasets, biosecurity, cybersecurity

**Relevance Score:** 8

**TL;DR:** This paper presents an automated approach for generating high-quality forget sets to facilitate the unlearning of specific knowledge from large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to remove harmful or sensitive knowledge from language models without full retraining, necessitating effective forget sets.

**Method:** The authors propose a method that uses a structured prompting pipeline to synthesize textbook-style data based only on a domain name input.

**Key Contributions:**

	1. Introduction of an automated method for generating forget sets using language models.
	2. Demonstration of the superiority of synthetic datasets over baseline methods.
	3. Release of code and dataset to encourage further research.

**Result:** Experiments show that the synthetic datasets outperform baseline alternatives and match expert-curated sets in effectiveness for unlearning tasks in various domains.

**Limitations:** 

**Conclusion:** Synthetic datasets can enable practical and scalable unlearning without manual intervention, making them a promising solution for emerging domains.

**Abstract:** Modern large language models often encode sensitive, harmful, or copyrighted knowledge, raising the need for post-hoc unlearning-the ability to remove specific domains of knowledge from a model without full retraining. A major bottleneck in current unlearning pipelines is constructing effective forget sets-datasets that approximate the target domain and guide the model to forget it. In this work, we introduce a scalable, automated approach to generate high-quality forget sets using language models themselves. Our method synthesizes textbook-style data through a structured prompting pipeline, requiring only a domain name as input. Through experiments on unlearning biosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic datasets consistently outperform the baseline synthetic alternatives and are comparable to the expert-curated ones. Additionally, ablation studies reveal that the multi-step generation pipeline significantly boosts data diversity, which in turn improves unlearning utility. Overall, our findings suggest that synthetic datasets offer a promising path toward practical, scalable unlearning for a wide range of emerging domains without the need for manual intervention. We release our code and dataset at https://github.com/xyzhu123/Synthetic_Textbook.

</details>


### [67] [BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent](https://arxiv.org/abs/2508.06600)

*Zijian Chen, Xueguang Ma, Shengyao Zhuang, Ping Nie, Kai Zou, Andrew Liu, Joshua Green, Kshama Patel, Ruoxi Meng, Mingyi Su, Sahel Sharifymoghaddam, Yanxi Li, Haoran Hong, Xinyu Shi, Xuye Liu, Nandan Thakur, Crystina Zhang, Luyu Gao, Wenhu Chen, Jimmy Lin*

**Main category:** cs.CL

**Keywords:** Deep Research, Large Language Models, Benchmarking, Information Retrieval, HCI

**Relevance Score:** 8

**TL;DR:** Introduction of BrowseComp-Plus, a benchmark for evaluating Deep-Research agents integrating LLMs with search tools, addressing limitations of current benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks like BrowseComp face issues of fairness and transparency due to reliance on dynamic web APIs, hindering effective evaluation of deep research LLMs.

**Method:** BrowseComp-Plus introduces a fixed, curated corpus for experiments, including verified supporting documents and challenging negatives for each query.

**Key Contributions:**

	1. Introduction of BrowseComp-Plus benchmark
	2. Enhanced evaluation process for deep research LLMs
	3. Better isolation of retriever contributions in performance analysis

**Result:** BrowseComp-Plus effectively distinguishes performance, with models like Search-R1 achieving 3.86% accuracy and GPT-5 reaching up to 70.1% when integrated with effective retrievers.

**Limitations:** 

**Conclusion:** The benchmark allows for a detailed evaluation of deep research agents, offering insights into retrieval effectiveness and performance metrics.

**Abstract:** Deep-Research agents, which integrate large language models (LLMs) with search tools, have shown success in improving the effectiveness of handling complex queries that require iterative search planning and reasoning over search results. Evaluations on current benchmarks like BrowseComp relies on black-box live web search APIs, have notable limitations in (1) fairness: dynamic and opaque web APIs hinder fair comparisons and reproducibility of deep research methods; (2) transparency: lack of control over the document corpus makes it difficult to isolate retriever contributions. In other words, the current evaluations may compare a complete deep research system at a given time, but they do not foster well-controlled experiments to provide insights into the capability of underlying deep research LLMs. To address these challenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp, employing a fixed, carefully curated corpus. Each query in BrowseComp-Plus includes human-verified supporting documents and mined challenging negatives, enabling controlled experimentation. The benchmark is shown to be effective in distinguishing the performance of deep research systems. For instance, the open-source model Search-R1, when paired with the BM25 retriever, achieves 3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with the Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with fewer search calls. This benchmark allows comprehensive evaluation and disentangled analysis of deep research agents and retrieval methods, fostering insights into retrieval effectiveness, citation accuracy, and context engineering in Deep-Research system.

</details>


### [68] [Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models](https://arxiv.org/abs/2508.06621)

*Tomohiro Sawada, Kartik Goyal*

**Main category:** cs.CL

**Keywords:** Byte-Pair Encoding, language models, tokenization, privacy preservation, machine learning

**Relevance Score:** 7

**TL;DR:** This paper investigates the effects of alternative BPE inference algorithms that do not rely on the traditional merge list and evaluates their impact on language model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and mitigate the privacy risks associated with the standard Byte-Pair Encoding (BPE) approach by analyzing alternative inference methods.

**Method:** The authors analyze two classes of BPE inference schemes: targeted deviations from merge lists (including random merge orders and corruptions) and non-targeted BPE methods that do not use merge lists (focusing on greedy or exact compression).

**Key Contributions:**

	1. Introduction of alternative BPE inference algorithms without relying on the merge list.
	2. Empirical evidence demonstrating the performance impact of these alternatives across various language tasks.
	3. Highlighting potential for more privacy-preserving tokenization approaches.

**Result:** Experiments show that targeted deviations from merge lists significantly degrade model performance, whereas non-targeted merge-list-free algorithms have minimal impact on downstream performance.

**Limitations:** The study focuses on specific language tasks, which may not generalize to all applications of BPE tokenization.

**Conclusion:** The study suggests that simpler, privacy-preserving tokenization methods can be developed without severely compromising language model efficacy.

**Abstract:** Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a learned token vocabulary with a detailed merge list. Recent work has shown that this merge list exposes a potential attack surface for extracting information about language model's training data. In this paper, we explore the downstream impact of BPE inference algorithms that do not rely on this merge list at all, and hence differ from the encoding process during BPE training. To address this question, we investigate two broad classes of BPE inference schemes that differ from BPE application during training: a) targeted deviation from merge-lists including random merge orders, and various corruptions of merge list involving deletion/truncation, and b) non-targeted BPE inference algorithms that do not depend on the merge list but focus on compressing the text either greedily or exactly. Extensive experiments across diverse language modeling tasks like accuracy-based QA benchmarks, machine translation, and open-ended generation reveal that while targeted deviation from the merge lists exhibits significant degradation in language model performance, the non-targeted merge-list-free inference algorithms result in minimal impact on downstream performance that is often much smaller than expected. These findings pave way for simpler and potentially more privacy-preserving tokenization schemes that do not catastrophically compromise model performance.

</details>


### [69] [Measuring Stereotype and Deviation Biases in Large Language Models](https://arxiv.org/abs/2508.06649)

*Daniel Wang, Eli Brignac, Minjia Mao, Xiao Fang*

**Main category:** cs.CL

**Keywords:** bias, large language models, demographic groups, stereotype bias, deviation bias

**Relevance Score:** 8

**TL;DR:** This study examines stereotype and deviation biases in large language models (LLMs) by analyzing their generated profiles, revealing significant biases towards various demographic groups.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the limitations and risks associated with the biases exhibited by large language models in demographic inferences.

**Method:** Four advanced LLMs were tasked with generating profiles of individuals, focusing on associations between demographic groups and attributes like political affiliation, religion, and sexual orientation.

**Key Contributions:**

	1. Identification of stereotype and deviation biases in LLMs
	2. Empirical analysis of biases across different demographic groups
	3. Discussion of potential harms from LLM-generated outputs

**Result:** All examined LLMs showed significant stereotype bias and deviation bias, indicating a substantial gap between LLM-generated demographic distributions and those in reality.

**Limitations:** The study may not encompass all demographic attributes and contexts; further research is needed to explore additional biases.

**Conclusion:** The study highlights the biases in LLM outputs that can lead to misrepresentations of user attributes, raising concerns about their implications.

**Abstract:** Large language models (LLMs) are widely applied across diverse domains, raising concerns about their limitations and potential risks. In this study, we investigate two types of bias that LLMs may display: stereotype bias and deviation bias. Stereotype bias refers to when LLMs consistently associate specific traits with a particular demographic group. Deviation bias reflects the disparity between the demographic distributions extracted from LLM-generated content and real-world demographic distributions. By asking four advanced LLMs to generate profiles of individuals, we examine the associations between each demographic group and attributes such as political affiliation, religion, and sexual orientation. Our experimental results show that all examined LLMs exhibit both significant stereotype bias and deviation bias towards multiple groups. Our findings uncover the biases that occur when LLMs infer user attributes and shed light on the potential harms of LLM-generated outputs.

</details>


### [70] [Testing the Limits of Machine Translation from One Book](https://arxiv.org/abs/2508.06665)

*Jonathan Shaw, Dillon Mee, Timothy Khouw, Zackary Leech, Daniel Wilson*

**Main category:** cs.CL

**Keywords:** machine translation, large language models, Kanuri, health informatics, language resources

**Relevance Score:** 8

**TL;DR:** This paper evaluates LLM translation quality for the Kanuri language using various resource combinations and datasets, highlighting the importance of parallel sentences over grammar alone.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of improving LLM translation quality for Kanuri, a language with limited digital resources, by focusing on health and humanitarian vocabulary.

**Method:** Two datasets were designed - one for health and humanitarian terms and another for generalized terminology. Different combinations of language resources were tested, and LLM performance was evaluated against native speaker translations and human linguist performance.

**Key Contributions:**

	1. Demonstration of the effectiveness of parallel sentences in LLM translation for Kanuri.
	2. Highlighting the limitations of grammar as a standalone data source in translation tasks.
	3. Provision of datasets focused on health and humanitarian terms for future research.

**Result:** Results showed that parallel sentences were the most effective data source for improving translation quality, achieving better scores in both human evaluations and automatic metrics compared to grammar and dictionary resources alone.

**Limitations:** The study primarily focuses on two data sources and may not generalize across other languages or larger datasets.

**Conclusion:** LLM translation evaluation should include multidimensional assessments beyond simple accuracy metrics, revealing that grammar alone is insufficient for effective domain-specific translation.

**Abstract:** Current state-of-the-art models demonstrate capacity to leverage in-context learning to translate into previously unseen language contexts. Tanzer et al. [2024] utilize language materials (e.g. a grammar) to improve translation quality for Kalamang using large language models (LLMs). We focus on Kanuri, a language that, despite having substantial speaker population, has minimal digital resources. We design two datasets for evaluation: one focused on health and humanitarian terms, and another containing generalized terminology, investigating how domain-specific tasks impact LLM translation quality.   By providing different combinations of language resources (grammar, dictionary, and parallel sentences), we measure LLM translation effectiveness, comparing results to native speaker translations and human linguist performance. We evaluate using both automatic metrics and native speaker assessments of fluency and accuracy.   Results demonstrate that parallel sentences remain the most effective data source, outperforming other methods in human evaluations and automatic metrics. While incorporating grammar improves over zero-shot translation, it fails as an effective standalone data source. Human evaluations reveal that LLMs achieve accuracy (meaning) more effectively than fluency (grammaticality).   These findings suggest LLM translation evaluation benefits from multidimensional assessment beyond simple accuracy metrics, and that grammar alone, without parallel sentences, does not provide sufficient context for effective domain-specific translation.

</details>


### [71] [Do Biased Models Have Biased Thoughts?](https://arxiv.org/abs/2508.06671)

*Swati Rajwal, Shivank Garg, Reem Abdel-Salam, Abdelrahman Zayed*

**Main category:** cs.CL

**Keywords:** Language models, Bias, Chain-of-thought prompting, Fairness metrics, Machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates the relationship between biased outputs and biased thought processes in language models through chain-of-thought prompting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by biases in language models related to gender, race, and other factors.

**Method:** Experiments were conducted on 5 popular language models using fairness metrics to quantify 11 different biases in their thoughts and outputs.

**Key Contributions:**

	1. Investigation of biases in language models' thought processes
	2. Empirical analysis using 11 distinct biases
	3. Insights into the relationship between model outputs and internal reasoning

**Result:** The study found that the correlation between bias in thought processes and bias in outputs is low (<0.6), indicating that models with biased outputs do not necessarily have biased thoughts.

**Limitations:** Focus on only 5 popular language models; results may not generalize to others.

**Conclusion:** The results suggest that biased decisions in language models do not stem from biased thinking processes, differing from human reasoning.

**Abstract:** The impressive performance of language models is undeniable. However, the presence of biases based on gender, race, socio-economic status, physical appearance, and sexual orientation makes the deployment of language models challenging. This paper studies the effect of chain-of-thought prompting, a recent approach that studies the steps followed by the model before it responds, on fairness. More specifically, we ask the following question: \textit{Do biased models have biased thoughts}? To answer our question, we conduct experiments on $5$ popular large language models using fairness metrics to quantify $11$ different biases in the model's thoughts and output. Our results show that the bias in the thinking steps is not highly correlated with the output bias (less than $0.6$ correlation with a $p$-value smaller than $0.001$ in most cases). In other words, unlike human beings, the tested models with biased decisions do not always possess biased thoughts.

</details>


### [72] [Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge](https://arxiv.org/abs/2508.06709)

*Evangelia Spiliopoulou, Riccardo Fogliato, Hanna Burnsky, Tamer Soliman, Jie Ma, Graham Horwood, Miguel Ballesteros*

**Main category:** cs.CL

**Keywords:** self-bias, large language models, automated evaluation, statistical framework, model performance evaluation

**Relevance Score:** 9

**TL;DR:** This paper presents a framework to identify and estimate self-bias in LLM evaluations, highlighting the risks of inaccurate assessments due to models favoring their own outputs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The growing reliance on LLMs for evaluating other LLM outputs necessitates a systematic approach to discern genuine performance from inflated ratings due to self-bias.

**Method:** A statistical framework is introduced that formalizes the conditions for assessing self-bias by comparing LLM evaluations of their own outputs against those of a third-party judge.

**Key Contributions:**

	1. Development of a statistical framework for identifying self-bias in LLMs
	2. Empirical evidence of self-bias in models like GPT-4o and Claude 3.5 Sonnet
	3. Guidance on interpreting LLM evaluation results to avoid bias pitfalls.

**Result:** Empirical analysis reveals that certain models, notably GPT-4o and Claude 3.5 Sonnet, consistently give higher scores to their own outputs, along with a tendency to exhibit family-bias.

**Limitations:** The framework may require further validation across more diverse models and evaluation tasks.

**Conclusion:** The findings underscore the importance of recognizing self-bias in LLM assessments and provide strategies for mitigating these biases in automated evaluations.

**Abstract:** Large language models (LLMs) can serve as judges that offer rapid and reliable assessments of other LLM outputs. However, models may systematically assign overly favorable ratings to their own outputs, a phenomenon known as self-bias, which can distort evaluations of true model performance. Previous studies often conflate genuine differences in model quality with bias or incorrectly assume that evaluations from LLMs and humans follow the same rating distributions. In this work, we present a statistical framework that explicitly formalizes assumptions under which self-bias can be identified and estimated. Our method models the difference in the scoring distribution that LLM-as-a-judge assigns to its own completions compared to other models, while accounting for the underlying quality of the completions provided by an independent, third-party judge (e.g., humans). Our method reliably isolates and quantifies self-bias, even when models vary in ability, ensuring that genuine performance differences are not mistaken for self-bias. We conduct an empirical analysis of self-bias on a large dataset (>5000 prompt-completion pairs) consisting of expert human annotations and judgments from nine different LLM judges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet, systematically assign higher scores to their own outputs. These models also display family-bias; systematically assigning higher ratings to outputs produced by other models of the same family. Our findings highlight potential pitfalls of using LLM judges and offer practical guidance to mitigate biases when interpreting automated evaluations.

</details>


### [73] [Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis](https://arxiv.org/abs/2508.06729)

*Komala Subramanyam Cherukuri, Pranav Abishai Moses, Aisa Sakata, Jiangping Chen, Haihua Chen*

**Main category:** cs.CL

**Keywords:** oral history, sentiment analysis, semantic classification, LLM, Digital Humanities

**Relevance Score:** 8

**TL;DR:** This paper presents a scalable framework using LLMs for automating semantic and sentiment annotation of Japanese American incarceration oral histories.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To promote access and understanding of oral histories affected by systemic injustice through efficient analysis.

**Method:** The framework combines expert annotation, prompt design, and LLM evaluation using ChatGPT, Llama, and Qwen, with a focus on historically sensitive contexts.

**Key Contributions:**

	1. Development of an LLM-based annotation pipeline for oral histories
	2. Evaluation of multiple LLMs in a culturally sensitive context
	3. Demonstration of the effectiveness of LLMs in semantic and sentiment analysis

**Result:** ChatGPT achieved the highest F1 score for semantic classification (88.71%), while Llama excelled slightly in sentiment analysis. The framework labeled 92,191 sentences from a collection of interviews.

**Limitations:** 

**Conclusion:** The study demonstrates that LLMs can effectively annotate large oral history collections with appropriate prompts and provides a reusable annotation pipeline for digital humanities.

**Abstract:** Oral histories are vital records of lived experience, particularly within communities affected by systemic injustice and historical erasure. Effective and efficient analysis of their oral history archives can promote access and understanding of the oral histories. However, Large-scale analysis of these archives remains limited due to their unstructured format, emotional complexity, and high annotation costs. This paper presents a scalable framework to automate semantic and sentiment annotation for Japanese American Incarceration Oral History. Using LLMs, we construct a high-quality dataset, evaluate multiple models, and test prompt engineering strategies in historically sensitive contexts. Our multiphase approach combines expert annotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We labeled 558 sentences from 15 narrators for sentiment and semantic classification, then evaluated zero-shot, few-shot, and RAG strategies. For semantic classification, ChatGPT achieved the highest F1 score (88.71%), followed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama slightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models showing comparable results. The best prompt configurations were used to annotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our findings show that LLMs can effectively perform semantic and sentiment annotation across large oral history collections when guided by well-designed prompts. This study provides a reusable annotation pipeline and practical guidance for applying LLMs in culturally sensitive archival analysis. By bridging archival ethics with scalable NLP techniques, this work lays the groundwork for responsible use of artificial intelligence in digital humanities and preservation of collective memory. GitHub: https://github.com/kc6699c/LLM4OralHistoryAnalysis.

</details>


### [74] [Many-Turn Jailbreaking](https://arxiv.org/abs/2508.06755)

*Xianjun Yang, Liqiang Xiao, Shiyang Li, Faisal Ladhak, Hyokun Yun, Linda Ruth Petzold, Yi Xu, William Yang Wang*

**Main category:** cs.CL

**Keywords:** multi-turn jailbreaking, large language models, safety threats, benchmark, user interactions

**Relevance Score:** 8

**TL;DR:** This paper introduces the concept of multi-turn jailbreaking in large language models (LLMs), highlighting its risks and establishing the Multi-Turn Jailbreak Benchmark (MTJ-Bench) for evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the insufficient focus on multi-turn jailbreaking in existing research on the safety of LLMs, as current studies mainly concentrate on single-turn queries which are less representative of real user interactions.

**Method:** Development of the Multi-Turn Jailbreak Benchmark (MTJ-Bench) to assess and benchmark multi-turn interactions with open- and closed-source LLMs.

**Key Contributions:**

	1. Introduction of multi-turn jailbreaking as a significant threat to LLM safety
	2. Establishment of the Multi-Turn Jailbreak Benchmark (MTJ-Bench) for evaluating multi-turn interactions
	3. Insights into the implications of successive questioning on LLM outputs

**Result:** Identification of vulnerabilities in LLMs when subjected to follow-up questions after initial jailbreaking, confirming that initial attacks can lead to successive unsafe outputs.

**Limitations:** The study serves as an initial exploration and does not provide comprehensive solutions to the vulnerabilities identified.

**Conclusion:** The study underlines the need for enhanced safety measures in LLMs and encourages ongoing research efforts to mitigate the threats posed by multi-turn jailbreaking.

**Abstract:** Current jailbreaking work on large language models (LLMs) aims to elicit unsafe outputs from given prompts. However, it only focuses on single-turn jailbreaking targeting one specific query. On the contrary, the advanced LLMs are designed to handle extremely long contexts and can thus conduct multi-turn conversations. So, we propose exploring multi-turn jailbreaking, in which the jailbroken LLMs are continuously tested on more than the first-turn conversation or a single target query. This is an even more serious threat because 1) it is common for users to continue asking relevant follow-up questions to clarify certain jailbroken details, and 2) it is also possible that the initial round of jailbreaking causes the LLMs to respond to additional irrelevant questions consistently. As the first step (First draft done at June 2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak Benchmark (MTJ-Bench) for benchmarking this setting on a series of open- and closed-source models and provide novel insights into this new safety threat. By revealing this new vulnerability, we aim to call for community efforts to build safer LLMs and pave the way for a more in-depth understanding of jailbreaking LLMs.

</details>


### [75] [SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection](https://arxiv.org/abs/2508.06803)

*Ziqi Liu, Yangbin Chen, Ziyang Zhou, Yilin Li, Mingxuan Hu, Yushan Pan, Zhijie Xu*

**Main category:** cs.CL

**Keywords:** sarcasm detection, natural language processing, large language models, multi-agent systems, hallucination resistance

**Relevance Score:** 9

**TL;DR:** SEVADE is a framework for sarcasm detection that improves accuracy and reduces hallucination through a multi-agent reasoning approach.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Sarcasm detection is critical in NLP, and current LLM methods struggle with complexities of ironic rhetoric, leading to inaccuracies.

**Method:** SEVADE employs a Dynamic Agentive Reasoning Engine with specialized agents for text analysis and a separate rationale adjudicator for classification.

**Key Contributions:**

	1. Introduction of SEVADE framework for sarcasm detection
	2. Use of Dynamic Agentive Reasoning Engine for multi-faceted analysis
	3. Implementation of decoupled evaluation to minimize hallucination risks.

**Result:** The framework showed state-of-the-art performance with an average increase of 6.75% in Accuracy and 6.29% in Macro-F1 score across four datasets.

**Limitations:** 

**Conclusion:** SEVADE effectively reduces hallucination and enhances sarcasm detection accuracy through its decoupled reason-and-judge architecture.

**Abstract:** Sarcasm detection is a crucial yet challenging Natural Language Processing task. Existing Large Language Model methods are often limited by single-perspective analysis, static reasoning pathways, and a susceptibility to hallucination when processing complex ironic rhetoric, which impacts their accuracy and reliability. To address these challenges, we propose **SEVADE**, a novel **S**elf-**Ev**olving multi-agent **A**nalysis framework with **D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The core of our framework is a Dynamic Agentive Reasoning Engine (DARE), which utilizes a team of specialized agents grounded in linguistic theory to perform a multifaceted deconstruction of the text and generate a structured reasoning chain. Subsequently, a separate lightweight rationale adjudicator (RA) performs the final classification based solely on this reasoning chain. This decoupled architecture is designed to mitigate the risk of hallucination by separating complex reasoning from the final judgment. Extensive experiments on four benchmark datasets demonstrate that our framework achieves state-of-the-art performance, with average improvements of **6.75%** in Accuracy and **6.29%** in Macro-F1 score.

</details>


### [76] [Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems](https://arxiv.org/abs/2508.06810)

*Steven Coyne, Diana Galvan-Sosa, Ryan Spring, Cam√©lia Guerraoui, Michael Zock, Keisuke Sakaguchi, Kentaro Inui*

**Main category:** cs.CL

**Keywords:** automated writing evaluation, natural language processing, language learning

**Relevance Score:** 8

**TL;DR:** The paper presents an annotation framework to improve automated writing evaluation systems by generating appropriate feedback for language learners, focusing on error types and their generalizability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current automated writing evaluation systems excel at correcting grammar but do not effectively support language learning; there's a need for systems that provide explanations and hints instead of direct corrections.

**Method:** The authors develop an annotation framework that classifies errors based on type and generalizability. They collect a dataset of learner errors and feedback, and evaluate various feedback generation methods using large language models.

**Key Contributions:**

	1. Introduction of an annotation framework for error type classification
	2. Collection of a dataset of annotated learner errors and feedback
	3. Evaluation of feedback generation methods using LLMs

**Result:** Different methods for feedback generation are evaluated, with human teachers assessing outputs on relevance, factuality, and comprehensibility.

**Limitations:** Results may vary based on the specific dataset and human evaluator biases.

**Conclusion:** The framework improves understanding of learner errors and supports the generation of effective, pedagogically sound feedback for language learners.

**Abstract:** Recent advances in natural language processing (NLP) have contributed to the development of automated writing evaluation (AWE) systems that can correct grammatical errors. However, while these systems are effective at improving text, they are not optimally designed for language learning. They favor direct revisions, often with a click-to-fix functionality that can be applied without considering the reason for the correction. Meanwhile, depending on the error type, learners may benefit most from simple explanations and strategically indirect hints, especially on generalizable grammatical rules. To support the generation of such feedback, we introduce an annotation framework that models each error's error type and generalizability. For error type classification, we introduce a typology focused on inferring learners' knowledge gaps by connecting their errors to specific grammatical patterns. Following this framework, we collect a dataset of annotated learner errors and corresponding human-written feedback comments, each labeled as a direct correction or hint. With this data, we evaluate keyword-guided, keyword-free, and template-guided methods of generating feedback using large language models (LLMs). Human teachers examined each system's outputs, assessing them on grounds including relevance, factuality, and comprehensibility. We report on the development of the dataset and the comparative performance of the systems investigated.

</details>


### [77] [Text to Speech System for Meitei Mayek Script](https://arxiv.org/abs/2508.06870)

*Gangular Singh Irengbam, Nirvash Singh Wahengbam, Lanthoiba Meitei Khumanthem, Paikhomba Oinam*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, Manipuri language, Neural TTS, Tacotron 2, HiFi-GAN

**Relevance Score:** 3

**TL;DR:** Development of a TTS system for Manipuri using Meitei Mayek script, leveraging Tacotron 2 and HiFi-GAN.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To preserve the Manipuri language and enhance technological inclusion in under-resourced linguistic environments.

**Method:** Adaptation of Tacotron 2 and HiFi-GAN for TTS, with phoneme mapping from Meitei Mayek to ARPAbet and creation of a single-speaker dataset.

**Key Contributions:**

	1. Introduction of a neural TTS architecture for Manipuri
	2. Phoneme mapping from Meitei Mayek to ARPAbet
	3. Creation of a single-speaker dataset for TTS in under-resourced languages.

**Result:** Achieved intelligible and natural speech synthesis validated through subjective and objective metrics.

**Limitations:** 

**Conclusion:** The TTS system aids in linguistic preservation and integration of Manipuri into modern technology.

**Abstract:** This paper presents the development of a Text-to-Speech (TTS) system for the Manipuri language   using the Meitei Mayek script. Leveraging Tacotron 2 and HiFi-GAN, we introduce a neural TTS   architecture adapted to support tonal phonology and under-resourced linguistic environments. We   develop a phoneme mapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and   demonstrate intelligible and natural speech synthesis, validated through subjective and objective   metrics. This system lays the groundwork for linguistic preservation and technological inclusion of   Manipuri.

</details>


### [78] [ESNERA: Empirical and semantic named entity alignment for named entity dataset merging](https://arxiv.org/abs/2508.06877)

*Xiaobo Zhang, Congqing He, Ying He, Jian Peng, Dajie Fu, Tien-Ping Tan*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Dataset Merging, Natural Language Processing, Label Alignment, Financial Domain

**Relevance Score:** 6

**TL;DR:** This paper proposes an automatic method for merging NER datasets based on label similarity, enhancing NER performance in low-resource settings.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The current methods for merging NER datasets are limited by lack of interpretability and scalability, which hinders further research due to the high costs and time involved in building high-quality annotated datasets.

**Method:** The authors introduce a greedy pairwise merging strategy that aligns labels based on both empirical and semantic similarities, enabling unification of label spaces across different datasets.

**Key Contributions:**

	1. Automatic label alignment method based on label similarity
	2. Greedy pairwise merging strategy for unifying label spaces
	3. Demonstrated enhancements in low-resource financial domain NER performance

**Result:** Experiments demonstrate that the proposed method successfully merges three NER datasets into a unified corpus with minimal performance degradation, and further improves NER performance when integrated with a small-scale dataset in the financial domain.

**Limitations:** The method may still be impacted by the quality and diversity of the original datasets used for merging.

**Conclusion:** The proposed solution provides an efficient, interpretable, and scalable approach for integrating multi-source NER corpora, addressing key challenges in the field.

**Abstract:** Named Entity Recognition (NER) is a fundamental task in natural language processing. It remains a research hotspot due to its wide applicability across domains. Although recent advances in deep learning have significantly improved NER performance, they rely heavily on large, high-quality annotated datasets. However, building these datasets is expensive and time-consuming, posing a major bottleneck for further research. Current dataset merging approaches mainly focus on strategies like manual label mapping or constructing label graphs, which lack interpretability and scalability. To address this, we propose an automatic label alignment method based on label similarity. The method combines empirical and semantic similarities, using a greedy pairwise merging strategy to unify label spaces across different datasets. Experiments are conducted in two stages: first, merging three existing NER datasets into a unified corpus with minimal impact on NER performance; second, integrating this corpus with a small-scale, self-built dataset in the financial domain. The results show that our method enables effective dataset merging and enhances NER performance in the low-resource financial domain. This study presents an efficient, interpretable, and scalable solution for integrating multi-source NER corpora.

</details>


### [79] [The ReQAP System for Question Answering over Personal Information](https://arxiv.org/abs/2508.06880)

*Philipp Christmann, Gerhard Weikum*

**Main category:** cs.CL

**Keywords:** question answering, human-computer interaction, language models, data integration, user trust

**Relevance Score:** 6

**TL;DR:** The ReQAP system enables users to obtain answers to complex questions by utilizing lightweight language models to process heterogeneous personal information sources, fostering user trust through answer traceability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assist users in retrieving answers to complex questions that require integration of diverse personal data types.

**Method:** ReQAP recursively decomposes questions and builds an operator tree for execution, using language models for question interpretation and operator execution.

**Key Contributions:**

	1. Introduces a novel operator tree execution model for complex question answering.
	2. Utilizes lightweight language models effectively for data integration.
	3. Provides comprehensive tracking of answer computation for user trust.

**Result:** The system demonstrates advanced functionalities for interpreting and answering complex user questions, along with detailed tracking of answer computation.

**Limitations:** 

**Conclusion:** ReQAP enhances comprehensibility and trust by allowing users to trace answers back to their source data.

**Abstract:** Personal information is abundant on users' devices, from structured data in calendar, shopping records or fitness tools, to unstructured contents in mail and social media posts. This works presents the ReQAP system that supports users with answers for complex questions that involve filters, joins and aggregation over heterogeneous sources. The unique trait of ReQAP is that it recursively decomposes questions and incrementally builds an operator tree for execution. Both the question interpretation and the individual operators make smart use of light-weight language models, with judicious fine-tuning. The demo showcases the rich functionality for advanced user questions, and also offers detailed tracking of how the answers are computed by the operators in the execution tree. Being able to trace answers back to the underlying sources is vital for human comprehensibility and user trust in the system.

</details>


### [80] [Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores](https://arxiv.org/abs/2508.06886)

*Arpita Saggar, Jonathan C. Darling, Vania Dimitrova, Duygu Sarikaya, David C. Hogg*

**Main category:** cs.CL

**Keywords:** persona-based dialogue, large language models, dialogue generation, response quality, score-conditioned training

**Relevance Score:** 8

**TL;DR:** This paper introduces a new framework called Score-Before-Speaking (SBS) for improving persona-based dialogue generation in conversational AI, which enhances response quality by correlating augmented responses with quality scores during training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of persona fidelity in conversations is hindered by the lack of diversity in existing dialogue data, which this paper aims to address.

**Method:** The SBS framework unifies the learning of responses and their quality into a single training step, using noun-based substitution for augmentation and semantic similarity-based scores to evaluate response quality.

**Key Contributions:**

	1. Introduction of the SBS framework for persona-based dialogue generation.
	2. Demonstration of superior performance through score-conditioned training in existing dialogue models.
	3. Innovative use of noun-based substitution and semantic similarity for quality scoring.

**Result:** The proposed framework outperforms previous methods in both million and billion-parameter models, as shown in experiments with benchmark datasets like PERSONA-CHAT and ConvAI2.

**Limitations:** 

**Conclusion:** Score-conditioned training significantly improves the model's ability to generate persona-consistent dialogues, and the method outperforms traditional setups where scores are not included in training.

**Abstract:** Persona-based dialogue generation is an important milestone towards building conversational artificial intelligence. Despite the ever-improving capabilities of large language models (LLMs), effectively integrating persona fidelity in conversations remains challenging due to the limited diversity in existing dialogue data. We propose a novel framework SBS (Score-Before-Speaking), which outperforms previous methods and yields improvements for both million and billion-parameter models. Unlike previous methods, SBS unifies the learning of responses and their relative quality into a single step. The key innovation is to train a dialogue model to correlate augmented responses with a quality score during training and then leverage this knowledge at inference. We use noun-based substitution for augmentation and semantic similarity-based scores as a proxy for response quality. Through extensive experiments with benchmark datasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training allows existing models to better capture a spectrum of persona-consistent dialogues. Our ablation studies also demonstrate that including scores in the input prompt during training is superior to conventional training setups. Code and further details are available at https://arpita2512.github.io/score_before_you_speak

</details>


### [81] [Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection](https://arxiv.org/abs/2508.06913)

*Siyuan Li, Xi Lin, Guangyan Li, Zehao Liu, Aodu Wulianghai, Li Ding, Jun Wu, Jianhua Li*

**Main category:** cs.CL

**Keywords:** large language models, text detection, sentiment analysis, AI-generated content, machine learning

**Relevance Score:** 9

**TL;DR:** The paper introduces SentiDetect, a model-agnostic framework that detects LLM-generated text by analyzing sentiment distribution stability, showing improvements over existing methods.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need to distinguish increasingly sophisticated AI-generated content from human-written text is critical due to challenges with existing detection methods.

**Method:** SentiDetect evaluates sentiment distribution consistency and preservation under various transformations to identify LLM outputs.

**Key Contributions:**

	1. Introduction of sentiment distribution metrics for detecting LLM-generated text
	2. Model-agnostic framework applicable to various LLMs
	3. Demonstrated superior performance compared to existing detection methods

**Result:** SentiDetect outperformed state-of-the-art methods by achieving significant improvements in F1 scores across multiple advanced LLMs, demonstrating greater robustness to attacks and variations.

**Limitations:** 

**Conclusion:** SentiDetect presents a novel approach to LLM-generated text detection with improved performance and robustness.

**Abstract:** The rapid advancement of large language models (LLMs) has resulted in increasingly sophisticated AI-generated content, posing significant challenges in distinguishing LLM-generated text from human-written language. Existing detection methods, primarily based on lexical heuristics or fine-tuned classifiers, often suffer from limited generalizability and are vulnerable to paraphrasing, adversarial perturbations, and cross-domain shifts. In this work, we propose SentiDetect, a model-agnostic framework for detecting LLM-generated text by analyzing the divergence in sentiment distribution stability. Our method is motivated by the empirical observation that LLM outputs tend to exhibit emotionally consistent patterns, whereas human-written texts display greater emotional variability. To capture this phenomenon, we define two complementary metrics: sentiment distribution consistency and sentiment distribution preservation, which quantify stability under sentiment-altering and semantic-preserving transformations. We evaluate SentiDetect on five diverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro, Claude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its superiority over state-of-the-art baselines, with over 16% and 11% F1 score improvements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover, SentiDetect also shows greater robustness to paraphrasing, adversarial attacks, and text length variations, outperforming existing detectors in challenging scenarios.

</details>


### [82] [Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction](https://arxiv.org/abs/2508.06971)

*Mohamed Basem, Islam Oshallah, Ali Hamdi, Khaled Shaban, Hozaifa Kassab*

**Main category:** cs.CL

**Keywords:** Quranic Question Answering, Arabic language models, Instruction-tuned language models, Question answering, Low-resource

**Relevance Score:** 3

**TL;DR:** A two-stage framework for Quranic Question Answering uses ensemble Arabic models for passage retrieval and instruction-tuned LLMs for answer extraction, achieving state-of-the-art results on the Quran QA 2023 Shared Task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of linguistic complexity and semantic richness in Classical Arabic texts for question answering.

**Method:** An ensemble of fine-tuned Arabic language models for passage retrieval, and instruction-tuned large language models with few-shot prompting for answer extraction.

**Key Contributions:**

	1. Novel two-stage framework for Quranic Question Answering
	2. Ensemble of Arabic language models for improved passage retrieval
	3. Use of instruction-tuned LLMs for effective answer extraction

**Result:** State-of-the-art results with MAP@10 of 0.3128, MRR@10 of 0.5763 for retrieval, and pAP@10 of 0.669 for extraction.

**Limitations:** 

**Conclusion:** Combining model ensembling and instruction-tuned models effectively addresses low-resource question answering in specialized domains.

**Abstract:** Quranic Question Answering presents unique challenges due to the linguistic complexity of Classical Arabic and the semantic richness of religious texts. In this paper, we propose a novel two-stage framework that addresses both passage retrieval and answer extraction. For passage retrieval, we ensemble fine-tuned Arabic language models to achieve superior ranking performance. For answer extraction, we employ instruction-tuned large language models with few-shot prompting to overcome the limitations of fine-tuning on small datasets. Our approach achieves state-of-the-art results on the Quran QA 2023 Shared Task, with a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of 0.669 for extraction, substantially outperforming previous methods. These results demonstrate that combining model ensembling and instruction-tuned language models effectively addresses the challenges of low-resource question answering in specialized domains.

</details>


### [83] [Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models](https://arxiv.org/abs/2508.06974)

*Zhijun Tu, Hanting Chen, Siqi Liu, Chuanjian Liu, Jian Li, Jie Hu, Yunhe Wang*

**Main category:** cs.CL

**Keywords:** 1-bit quantization, LLMs, progressive training

**Relevance Score:** 9

**TL;DR:** This paper presents a novel method for 1-bit LLM quantization that leverages pre-trained models, reducing training costs and improving accuracy through consistent progressive training and binary-aware techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of training 1-bit LLMs from scratch and leverage pre-trained models for better efficiency and performance.

**Method:** The authors propose a consistent progressive training approach that adapts both forward and backward, alongside techniques like binary-aware initialization and dual-scaling compensation.

**Key Contributions:**

	1. Introduction of consistent progressive training for 1-bit LLMs
	2. Incorporation of binary-aware initialization techniques
	3. Demonstration of high-performance outcomes using pre-trained models

**Result:** Experimental results demonstrate that the proposed method achieves higher performance in 1-bit LLMs compared to existing methods, effectively utilizing pre-trained models.

**Limitations:** 

**Conclusion:** By eliminating the need for scratch training, the proposed method significantly reduces costs while improving accuracy in quantized models.

**Abstract:** 1-bit LLM quantization offers significant advantages in reducing storage and computational costs. However, existing methods typically train 1-bit LLMs from scratch, failing to fully leverage pre-trained models. This results in high training costs and notable accuracy degradation. We identify that the large gap between full precision and 1-bit representations makes direct adaptation difficult. In this paper, we introduce a consistent progressive training for both forward and backward, smoothly converting the floating-point weights into the binarized ones. Additionally, we incorporate binary-aware initialization and dual-scaling compensation to reduce the difficulty of progressive training and improve the performance. Experimental results on LLMs of various sizes demonstrate that our method outperforms existing approaches. Our results show that high-performance 1-bit LLMs can be achieved using pre-trained models, eliminating the need for expensive training from scratch.

</details>


### [84] [Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings](https://arxiv.org/abs/2508.07017)

*Mao Li, Fred Conrad, Johann Gagnon-Bartsch*

**Main category:** cs.CL

**Keywords:** abstractive summarization, semantic compression, generative language model, embedding inversion, scalability

**Relevance Score:** 8

**TL;DR:** Vec2Summ is a new abstractive summarization method that uses a mean vector in the semantic embedding space to generate summaries while maintaining semantic control.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of LLM-based summarization methods, particularly context-length constraints and the need for scalable, interpretable generation.

**Method:** Vec2Summ represents document collections as a mean vector in semantic space and uses stochasticity from a Gaussian distribution to enhance summary generation.

**Key Contributions:**

	1. Introduction of the Vec2Summ method for semantic compression in summarization
	2. Use of embedding inversion for generating natural language summaries
	3. Incorporation of stochasticity for enhanced output variability

**Result:** Vec2Summ generates coherent summaries comparable to LLM summarization, particularly in thematically focused and scalable settings, though with less detail.

**Limitations:** Less fine-grained detail in summaries compared to traditional LLM-based methods.

**Conclusion:** Vec2Summ shows significant promise for tasks requiring semantic control and corpus-level abstraction in summarization, paving the way for more robust applications.

**Abstract:** We propose Vec2Summ, a novel method for abstractive summarization that frames the task as semantic compression. Vec2Summ represents a document collection using a single mean vector in the semantic embedding space, capturing the central meaning of the corpus. To reconstruct fluent summaries, we perform embedding inversion -- decoding this mean vector into natural language using a generative language model. To improve reconstruction quality and capture some degree of topical variability, we introduce stochasticity by sampling from a Gaussian distribution centered on the mean. This approach is loosely analogous to bagging in ensemble learning, where controlled randomness encourages more robust and varied outputs. Vec2Summ addresses key limitations of LLM-based summarization methods. It avoids context-length constraints, enables interpretable and controllable generation via semantic parameters, and scales efficiently with corpus size -- requiring only $O(d + d^2)$ parameters. Empirical results show that Vec2Summ produces coherent summaries for topically focused, order-invariant corpora, with performance comparable to direct LLM summarization in terms of thematic coverage and efficiency, albeit with less fine-grained detail. These results underscore Vec2Summ's potential in settings where scalability, semantic control, and corpus-level abstraction are prioritized.

</details>


### [85] [SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages](https://arxiv.org/abs/2508.07069)

*Muhammad Dehan Al Kautsar, Aswin Candra, Muhammad Alif Al Hakim, Maxalmina Satria Kahfi, Fajri Koto, Alham Fikri Aji, Peerat Limkonchotiwat, Ekapol Chuangsuwanich, Genta Indra Winata*

**Main category:** cs.CL

**Keywords:** cultural diversity, dialogue dataset, Southeast Asia

**Relevance Score:** 7

**TL;DR:** SEADialogues is a culturally grounded dialogue dataset aimed at improving the shortcomings of existing chit-chat datasets, focusing on Southeast Asia's cultural diversity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of cultural nuance in existing chit-chat datasets for dialogue systems, particularly in relation to Southeast Asia's diversity.

**Method:** The dataset includes dialogues in eight languages from six Southeast Asian countries, integrating persona attributes and culturally relevant topics to enhance cultural relevance and personalization.

**Key Contributions:**

	1. Introduction of a new dataset focused on Southeast Asia's cultural diversity.
	2. Inclusion of persona attributes and culturally grounded topics in dialogues.
	3. Support for multiple low-resource languages.

**Result:** The dataset facilitates research on culturally aware and human-centric large language models and conversational dialogue agents.

**Limitations:** 

**Conclusion:** SEADialogues can significantly improve dialogue systems by incorporating cultural elements and diversity, addressing a key gap in current research.

**Abstract:** Although numerous datasets have been developed to support dialogue systems, most existing chit-chat datasets overlook the cultural nuances inherent in natural human conversations. To address this gap, we introduce SEADialogues, a culturally grounded dialogue dataset centered on Southeast Asia, a region with over 700 million people and immense cultural diversity. Our dataset features dialogues in eight languages from six Southeast Asian countries, many of which are low-resource despite having sizable speaker populations. To enhance cultural relevance and personalization, each dialogue includes persona attributes and two culturally grounded topics that reflect everyday life in the respective communities. Furthermore, we release a multi-turn dialogue dataset to advance research on culturally aware and human-centric large language models, including conversational dialogue agents.

</details>


### [86] [BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context](https://arxiv.org/abs/2508.07090)

*Aditya Tomar, Nihar Ranjan Sahoo, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** bias evaluation, language models, social categories, multilingual, Indian languages

**Relevance Score:** 9

**TL;DR:** The paper introduces BharatBBQ, a benchmark for evaluating social biases in multilingual language models specifically in the Indian context, addressing the inadequacies of existing biases benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure fairness and reduce harmful stereotypes in AI, especially in the context of Indian social and linguistic diversity.

**Method:** The authors created BharatBBQ, a dataset covering 13 social categories in eight languages, expanding from 49,108 examples to 392,864 through translation, and evaluated five multilingual language models for bias assessment.

**Key Contributions:**

	1. Introduction of BharatBBQ for bias evaluation in Indian context
	2. Development of a large multilingual dataset covering multiple social categories
	3. Analysis of bias in five multilingual language models with significant findings on amplified biases.

**Result:** The study revealed persistent and often amplified biases across languages, emphasizing the need for culturally grounded benchmarks in bias evaluation.

**Limitations:** 

**Conclusion:** BharatBBQ highlights the importance of considering cultural nuances when assessing biases in language models, particularly for Indian languages, to prevent the reinforcement of stereotypes.

**Abstract:** Evaluating social biases in language models (LMs) is crucial for ensuring fairness and minimizing the reinforcement of harmful stereotypes in AI systems. Existing benchmarks, such as the Bias Benchmark for Question Answering (BBQ), primarily focus on Western contexts, limiting their applicability to the Indian context. To address this gap, we introduce BharatBBQ, a culturally adapted benchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil, Telugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3 intersectional groups, reflecting prevalent biases in the Indian sociocultural landscape. Our dataset contains 49,108 examples in one language that are expanded using translation and verification to 392,864 examples in eight different languages. We evaluate five multilingual LM families across zero and few-shot settings, analyzing their bias and stereotypical bias scores. Our findings highlight persistent biases across languages and social categories and often amplified biases in Indian languages compared to English, demonstrating the necessity of linguistically and culturally grounded benchmarks for bias evaluation.

</details>


### [87] [Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning](https://arxiv.org/abs/2508.07101)

*Lijie Yang, Zhihao Zhang, Arti Jain, Shijie Cao, Baihong Yuan, Yiwei Chen, Zhihao Jia, Ravi Netravali*

**Main category:** cs.CL

**Keywords:** Sparse Attention, Reasoning Tasks, Efficiency, Token Selection, Machine Learning

**Relevance Score:** 6

**TL;DR:** LessIsMore is a training-free sparse attention mechanism that improves reasoning task efficiency without sacrificing accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large reasoning models suffer from computational overhead due to excessive token generation during short input processing; existing sparse attention methods face accuracy issues and high retraining costs.

**Method:** LessIsMore utilizes global attention patterns for token selection across local attention heads, enabling unified cross-head token ranking instead of head-specific optimizations.

**Key Contributions:**

	1. Introduction of a training-free mechanism for sparse attention in reasoning models
	2. Unified cross-head token ranking for efficiency
	3. Improvement in both accuracy and speed compared to existing methods

**Result:** The method shows improved accuracy and a $1.1\times$ average decoding speed-up, attending to $2\times$ fewer tokens compared to full attention methods without loss of accuracy.

**Limitations:** 

**Conclusion:** LessIsMore achieves enhanced generalization and efficiency, demonstrating a $1.13\times$ end-to-end speed-up compared to other sparse attention approaches.

**Abstract:** Large reasoning models achieve strong performance through test-time scaling but incur substantial computational overhead, particularly from excessive token generation when processing short input prompts. While sparse attention mechanisms can reduce latency and memory usage, existing approaches suffer from significant accuracy degradation due to accumulated errors during long-generation reasoning. These methods generally require either high token retention rates or expensive retraining. We introduce LessIsMore, a training-free sparse attention mechanism for reasoning tasks, which leverages global attention patterns rather than relying on traditional head-specific local optimizations. LessIsMore aggregates token selections from local attention heads with recent contextual information, enabling unified cross-head token ranking for future decoding layers. This unified selection improves generalization and efficiency by avoiding the need to maintain separate token subsets per head. Evaluation across diverse reasoning tasks and benchmarks shows that LessIsMore preserves -- and in some cases improves -- accuracy while achieving a $1.1\times$ average decoding speed-up compared to full attention. Moreover, LessIsMore attends to $2\times$ fewer tokens without accuracy loss, achieving a $1.13\times$ end-to-end speed-up compared to existing sparse attention methods.

</details>


### [88] [Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution](https://arxiv.org/abs/2508.07111)

*Falaah Arif Khan, Nivedha Sivakumar, Yinong Oliver Wang, Katherine Metcalf, Cezanne Camacho, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff*

**Main category:** cs.CL

**Keywords:** large language models, intersectional bias, WinoIdentity benchmark, coreference confidence disparity, fairness evaluation

**Relevance Score:** 9

**TL;DR:** This paper extends fairness evaluations of large language models (LLMs) to address intersectional bias, creating a new benchmark for assessing multiple demographic attributes and their impact on model certainty and decision-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about identity-based harm due to biases in LLMs used in hiring and admissions, and to recognize the distinct patterns of disadvantage that arise from intersectionality.

**Method:** Development of the WinoIdentity benchmark by augmenting the WinoBias dataset. Assessment of 245,700 prompts to evaluate bias patterns, combined with a group (un)fairness metric called Coreference Confidence Disparity to measure confidence disparities among intersectional identities.

**Key Contributions:**

	1. Introduction of the WinoIdentity benchmark for intersectional bias assessment
	2. Development of the Coreference Confidence Disparity metric for measuring confidence disparities
	3. Discovery of significant confidence disparities in LLMs based on demographic attributes

**Result:** Evaluation of five LLMs revealed confidence disparities up to 40% across various demographic attributes such as body type and sexual orientation, with increased uncertainty for doubly-disadvantaged identities.

**Limitations:** 

**Conclusion:** Performance issues indicate that LLMs may rely more on memorization rather than logical reasoning, highlighting failures in both value alignment and validity that can lead to social harms.

**Abstract:** Large language models (LLMs) have achieved impressive performance, leading to their widespread adoption as decision-support tools in resource-constrained contexts like hiring and admissions. There is, however, scientific consensus that AI systems can reflect and exacerbate societal biases, raising concerns about identity-based harm when used in critical social contexts. Prior work has laid a solid foundation for assessing bias in LLMs by evaluating demographic disparities in different language reasoning tasks. In this work, we extend single-axis fairness evaluations to examine intersectional bias, recognizing that when multiple axes of discrimination intersect, they create distinct patterns of disadvantage. We create a new benchmark called WinoIdentity by augmenting the WinoBias dataset with 25 demographic markers across 10 attributes, including age, nationality, and race, intersected with binary gender, yielding 245,700 prompts to evaluate 50 distinct bias patterns. Focusing on harms of omission due to underrepresentation, we investigate bias through the lens of uncertainty and propose a group (un)fairness metric called Coreference Confidence Disparity which measures whether models are more or less confident for some intersectional identities than others. We evaluate five recently published LLMs and find confidence disparities as high as 40% along various demographic attributes including body type, sexual orientation and socio-economic status, with models being most uncertain about doubly-disadvantaged identities in anti-stereotypical settings. Surprisingly, coreference confidence decreases even for hegemonic or privileged markers, indicating that the recent impressive performance of LLMs is more likely due to memorization than logical reasoning. Notably, these are two independent failures in value alignment and validity that can compound to cause social harm.

</details>


### [89] [Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens](https://arxiv.org/abs/2508.07143)

*Anna Seo Gyeong Choi, Hoon Choi*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, ethics, bias, linguistic diversity, fairness

**Relevance Score:** 6

**TL;DR:** This paper explores the ethical implications of bias in Automatic Speech Recognition (ASR) systems, arguing that misrecognition of non-standard dialects is a form of disrespect that exacerbates historical injustices against marginalized communities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the limited understanding of fairness in Automatic Speech Recognition (ASR) systems and aims to highlight the social implications of ASR bias.

**Method:** The paper uses a philosophical approach to analyze the ethical dimensions of speech technologies and the impact of ASR misrecognition on users, particularly those who speak non-standard dialects.

**Key Contributions:**

	1. Introduces a philosophical perspective on ASR bias and fairness.
	2. Identifies unique ethical dimensions of ASR technology.
	3. Argues for the recognition of diverse speech as essential for respectful technology design.

**Result:** It identifies three ethical dimensions of ASR bias: temporal taxation on speakers, disruption of conversational flow, and the connection between speech and cultural identity, arguing these issues create power imbalances.

**Limitations:** 

**Conclusion:** Addressing ASR bias requires more than technical fixes; it necessitates recognizing diverse speech varieties as legitimate and accommodating them within technological design.

**Abstract:** Automatic Speech Recognition (ASR) systems now mediate countless human-technology interactions, yet research on their fairness implications remains surprisingly limited. This paper examines ASR bias through a philosophical lens, arguing that systematic misrecognition of certain speech varieties constitutes more than a technical limitation -- it represents a form of disrespect that compounds historical injustices against marginalized linguistic communities. We distinguish between morally neutral classification (discriminate1) and harmful discrimination (discriminate2), demonstrating how ASR systems can inadvertently transform the former into the latter when they consistently misrecognize non-standard dialects. We identify three unique ethical dimensions of speech technologies that differentiate ASR bias from other algorithmic fairness concerns: the temporal burden placed on speakers of non-standard varieties ("temporal taxation"), the disruption of conversational flow when systems misrecognize speech, and the fundamental connection between speech patterns and personal/cultural identity. These factors create asymmetric power relationships that existing technical fairness metrics fail to capture. The paper analyzes the tension between linguistic standardization and pluralism in ASR development, arguing that current approaches often embed and reinforce problematic language ideologies. We conclude that addressing ASR bias requires more than technical interventions; it demands recognition of diverse speech varieties as legitimate forms of expression worthy of technological accommodation. This philosophical reframing offers new pathways for developing ASR systems that respect linguistic diversity and speaker autonomy.

</details>


### [90] [Gradient Surgery for Safe LLM Fine-Tuning](https://arxiv.org/abs/2508.07172)

*Biao Yi, Jiahao Li, Baolei Zhang, Lihai Nie, Tong Li, Tiansheng Huang, Zheli Liu*

**Main category:** cs.CL

**Keywords:** Fine-tuning, Large Language Models, Safety Alignment, Gradient Surgery, NKL-Divergence

**Relevance Score:** 9

**TL;DR:** SafeGrad is a novel method for ensuring safety alignment in fine-tuning of LLMs by addressing conflicting gradients that arise from harmful examples in training data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address vulnerabilities in fine-tuning Large Language Models (LLMs) that arise when malicious examples compromise safety alignment.

**Method:** SafeGrad employs gradient surgery to nullify harmful components of user-task gradients when conflicts with safety objectives are detected, using KL-divergence alignment loss for improved robustness and data efficiency.

**Key Contributions:**

	1. Introduces SafeGrad for gradient surgery in LLM fine-tuning
	2. Employs KL-divergence alignment loss for data efficiency
	3. Demonstrates state-of-the-art safety defense across various LLMs

**Result:** SafeGrad demonstrates state-of-the-art defense against harmful ratios in LLMs, maintaining safety without sacrificing task fidelity across various datasets.

**Limitations:** 

**Conclusion:** The proposed method effectively balances user-task performance with safety alignment, enabling safe fine-tuning even under high risk of harmful data.

**Abstract:** Fine-tuning-as-a-Service introduces a critical vulnerability where a few malicious examples mixed into the user's fine-tuning dataset can compromise the safety alignment of Large Language Models (LLMs). While a recognized paradigm frames safe fine-tuning as a multi-objective optimization problem balancing user task performance with safety alignment, we find existing solutions are critically sensitive to the harmful ratio, with defenses degrading sharply as harmful ratio increases. We diagnose that this failure stems from conflicting gradients, where the user-task update directly undermines the safety objective. To resolve this, we propose SafeGrad, a novel method that employs gradient surgery. When a conflict is detected, SafeGrad nullifies the harmful component of the user-task gradient by projecting it onto the orthogonal plane of the alignment gradient, allowing the model to learn the user's task without sacrificing safety. To further enhance robustness and data efficiency, we employ a KL-divergence alignment loss that learns the rich, distributional safety profile of the well-aligned foundation model. Extensive experiments show that SafeGrad provides state-of-the-art defense across various LLMs and datasets, maintaining robust safety even at high harmful ratios without compromising task fidelity.

</details>


### [91] [Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models](https://arxiv.org/abs/2508.07173)

*Leyi Pan, Zheyu Fu, Yunpeng Zhai, Shuchang Tao, Sheng Guan, Shiyu Huang, Lingzhe Zhang, Zhaoyang Liu, Bolin Ding, Felix Henry, Lijie Wen, Aiwei Liu*

**Main category:** cs.CL

**Keywords:** Omni-modal Large Language Models, safety evaluation, cross-modal consistency, benchmark, metrics

**Relevance Score:** 9

**TL;DR:** Introduction of Omni-SafetyBench, the first benchmark for evaluating the safety of Omni-modal Large Language Models (OLLMs) with a focus on audio-visual safety and cross-modal consistency.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of dedicated safety benchmarks for OLLMs and the need for assessing safety performance with complex audio-visual inputs created a gap that this paper aims to fill.

**Method:** The paper introduces Omni-SafetyBench, a benchmark featuring 24 modality combinations with 972 samples each. It develops tailored metrics such as Safety-score and Cross-Modal Safety Consistency Score to evaluate safety performance and consistency across modalities.

**Key Contributions:**

	1. Development of the first comprehensive benchmark (Omni-SafetyBench) for OLLM safety assessment.
	2. Introduction of new evaluation metrics (Safety-score and CMSC-score) tailored for cross-modal safety.
	3. Empirical evaluation showing significant safety vulnerabilities in contemporary OLLMs.

**Result:** Evaluation of 10 models (6 open-source, 4 closed-source) revealed vulnerabilities, with only 3 achieving above 0.6 in safety and consistency metrics, while some models scored as low as 0.14 in specific modalities.

**Limitations:** Focuses solely on safety evaluation, does not address algorithmic improvements or underlying model training methodologies.

**Conclusion:** The findings indicate critical safety vulnerabilities in OLLMs, necessitating urgent improvements in their safety measures and evaluation techniques.

**Abstract:** The rise of Omni-modal Large Language Models (OLLMs), which integrate visual and auditory processing with text, necessitates robust safety evaluations to mitigate harmful outputs. However, no dedicated benchmarks currently exist for OLLMs, and prior benchmarks designed for other LLMs lack the ability to assess safety performance under audio-visual joint inputs or cross-modal safety consistency. To fill this gap, we introduce Omni-SafetyBench, the first comprehensive parallel benchmark for OLLM safety evaluation, featuring 24 modality combinations and variations with 972 samples each, including dedicated audio-visual harm cases. Considering OLLMs' comprehension challenges with complex omni-modal inputs and the need for cross-modal consistency evaluation, we propose tailored metrics: a Safety-score based on conditional Attack Success Rate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and a Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency across modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals critical vulnerabilities: (1) no model excels in both overall safety and consistency, with only 3 models achieving over 0.6 in both metrics and top performer scoring around 0.8; (2) safety defenses weaken with complex inputs, especially audio-visual joints; (3) severe weaknesses persist, with some models scoring as low as 0.14 on specific modalities. Our benchmark and metrics highlight urgent needs for enhanced OLLM safety, providing a foundation for future improvements.

</details>


### [92] [Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback](https://arxiv.org/abs/2508.07178)

*Kejin Liu, Junhong Lian, Xiang Ao, Ningtao Wang, Xing Fu, Yu Cheng, Weiqiang Wang, Xinyu Liu*

**Main category:** cs.CL

**Keywords:** personalized headline generation, click noise, user interests, dataset, machine learning

**Relevance Score:** 8

**TL;DR:** Proposes a Personalized Headline Generation framework that mitigates click noise and improves headline quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate personalized headline generation requires a clear understanding of user interests, which are often obscured by irrelevant click noise in historical data.

**Method:** The PHG-DIF framework employs dual-stage filtering to remove clickstream noise and uses multi-level temporal fusion to model users' evolving interests.

**Key Contributions:**

	1. Introduction of PHG-DIF framework for denoising clickstream data
	2. Development of the DT-PENS benchmark dataset
	3. Demonstration of SOTA performance in personalized headline generation

**Result:** PHG-DIF shows substantial improvement in headline generation quality, outperforming existing methods and achieving state-of-the-art results on the new DT-PENS dataset.

**Limitations:** 

**Conclusion:** The implementation of PHG-DIF and the DT-PENS dataset are publicly available, offering significant resources for personalized headline generation research.

**Abstract:** Accurate personalized headline generation hinges on precisely capturing user interests from historical behaviors. However, existing methods neglect personalized-irrelevant click noise in entire historical clickstreams, which may lead to hallucinated headlines that deviate from genuine user preferences. In this paper, we reveal the detrimental impact of click noise on personalized generation quality through rigorous analysis in both user and news dimensions. Based on these insights, we propose a novel Personalized Headline Generation framework via Denoising Fake Interests from Implicit Feedback (PHG-DIF). PHG-DIF first employs dual-stage filtering to effectively remove clickstream noise, identified by short dwell times and abnormal click bursts, and then leverages multi-level temporal fusion to dynamically model users' evolving and multi-faceted interests for precise profiling. Moreover, we release DT-PENS, a new benchmark dataset comprising the click behavior of 1,000 carefully curated users and nearly 10,000 annotated personalized headlines with historical dwell time annotations. Extensive experiments demonstrate that PHG-DIF substantially mitigates the adverse effects of click noise and significantly improves headline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our framework implementation and dataset are available at https://github.com/liukejin-up/PHG-DIF.

</details>


### [93] [Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks](https://arxiv.org/abs/2508.07179)

*Jiaqi Yin, Yi-Wei Chen, Meng-Lung Lee, Xiya Liu*

**Main category:** cs.CL

**Keywords:** schema lineage, semantic drift, data governance, language models, RAG

**Relevance Score:** 6

**TL;DR:** This paper presents a framework for automated extraction of schema lineage from multilingual enterprise data pipelines to combat semantic drift, compromising data reproducibility and governance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing semantic drift in enterprise data pipelines that affects data reproducibility and governance, particularly in contexts like retrieval-augmented generation and text-to-SQL systems.

**Method:** A novel framework is proposed for automatic extraction of schema lineage, identifying key components including source schemas, source tables, transformation logic, and aggregation operations, assessed using the Schema Lineage Composite Evaluation (SLiCE) metric.

**Key Contributions:**

	1. Introduction of a framework for schema lineage extraction from multilingual pipelines
	2. Development of the SLiCE metric for evaluating lineage quality
	3. Creation of a new benchmark with 1,700 annotated lineages for evaluation

**Result:** Experiments showed that schema lineage extraction performance improves with language model size, with a 32B model achieving performance on par with GPT models under standard prompting.

**Limitations:** 

**Conclusion:** The findings indicate a scalable and cost-effective strategy for deploying schema-aware agents in real-world applications, emphasizing the impact of model size and prompting on extraction quality.

**Abstract:** Enterprise data pipelines, characterized by complex transformations across multiple programming languages, often cause a semantic disconnect between original metadata and downstream data. This "semantic drift" compromises data reproducibility and governance, and impairs the utility of services like retrieval-augmented generation (RAG) and text-to-SQL systems. To address this, a novel framework is proposed for the automated extraction of fine-grained schema lineage from multilingual enterprise pipeline scripts. This method identifies four key components: source schemas, source tables, transformation logic, and aggregation operations, creating a standardized representation of data transformations. For the rigorous evaluation of lineage quality, this paper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that assesses both structural correctness and semantic fidelity. A new benchmark is also presented, comprising 1,700 manually annotated lineages from real-world industrial scripts. Experiments were conducted with 12 language models, from 1.3B to 32B small language models (SLMs) to large language models (LLMs) like GPT-4o and GPT-4.1. The results demonstrate that the performance of schema lineage extraction scales with model size and the sophistication of prompting techniques. Specially, a 32B open-source model, using a single reasoning trace, can achieve performance comparable to the GPT series under standard prompting. This finding suggests a scalable and economical approach for deploying schema-aware agents in practical applications.

</details>


### [94] [DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention](https://arxiv.org/abs/2508.07185)

*Kabir Khan, Priya Sharma, Arjun Mehta, Neha Gupta, Ravi Narayanan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graph, real-time knowledge integration, sparse attention, factual accuracy

**Relevance Score:** 9

**TL;DR:** DySK-Attn is a framework that allows Large Language Models (LLMs) to integrate real-time knowledge from dynamic Knowledge Graphs (KGs) using a sparse knowledge attention mechanism, enhancing efficiency and accuracy in time-sensitive tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs have static knowledge that becomes outdated quickly, and retraining them is costly. Existing knowledge editing methods are slow and may cause side effects, creating a need for a more efficient solution.

**Method:** The framework integrates an LLM with a dynamic KG that updates in real-time, utilizing a sparse knowledge attention mechanism for efficient knowledge retrieval.

**Key Contributions:**

	1. Introduction of the DySK-Attn framework for real-time knowledge integration in LLMs.
	2. Development of a sparse knowledge attention mechanism that improves efficiency in knowledge retrieval.
	3. Demonstration of improved performance over existing techniques in factual accuracy and computational efficiency.

**Result:** DySK-Attn significantly outperformed strong baselines, including standard RAG and model editing techniques, in factual accuracy and computational efficiency on time-sensitive question-answering tasks.

**Limitations:** 

**Conclusion:** The DySK-Attn framework provides a scalable solution for keeping LLMs up-to-date with current knowledge.

**Abstract:** Large Language Models (LLMs) suffer from a critical limitation: their knowledge is static and quickly becomes outdated. Retraining these massive models is computationally prohibitive, while existing knowledge editing techniques can be slow and may introduce unforeseen side effects. To address this, we propose DySK-Attn, a novel framework that enables LLMs to efficiently integrate real-time knowledge from a dynamic external source. Our approach synergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated instantaneously. The core of our framework is a sparse knowledge attention mechanism, which allows the LLM to perform a coarse-to-fine grained search, efficiently identifying and focusing on a small, highly relevant subset of facts from the vast KG. This mechanism avoids the high computational cost of dense attention over the entire knowledge base and mitigates noise from irrelevant information. We demonstrate through extensive experiments on time-sensitive question-answering tasks that DySK-Attn significantly outperforms strong baselines, including standard Retrieval-Augmented Generation (RAG) and model editing techniques, in both factual accuracy for updated knowledge and computational efficiency. Our framework offers a scalable and effective solution for building LLMs that can stay current with the ever-changing world.

</details>


### [95] [Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment](https://arxiv.org/abs/2508.07195)

*Yanru Sun, Emadeldeen Eldele, Zongxia Xie, Yucheng Wang, Wenzhe Niu, Qinghua Hu, Chee Keong Kwoh, Min Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Time Series Forecasting, Semantic Alignment

**Relevance Score:** 9

**TL;DR:** TALON is a framework that improves LLM-based time series forecasting by addressing temporal heterogeneity and bridging the modality gap between numerical signals and language representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the application of Large Language Models in time series forecasting by modeling temporal heterogeneity and addressing the challenges of aligning numerical and language representations.

**Method:** TALON employs a Heterogeneous Temporal Encoder for partitioning time series into coherent segments and a Semantic Alignment Module for aligning temporal features with LLM-compatible representations.

**Key Contributions:**

	1. Introduction of TALON framework for LLM-based forecasting
	2. Development of Heterogeneous Temporal Encoder for expert modeling
	3. Creation of Semantic Alignment Module to link temporal features with LLMs

**Result:** TALON outperformed recent state-of-the-art methods across seven real-world datasets, achieving average MSE improvements of up to 11%.

**Limitations:** 

**Conclusion:** Incorporating pattern-aware and semantic-aware designs is effective for adapting LLMs to time series forecasting tasks.

**Abstract:** Large Language Models (LLMs) have recently demonstrated impressive capabilities in natural language processing due to their strong generalization and sequence modeling capabilities. However, their direct application to time series forecasting remains challenging due to two fundamental issues: the inherent heterogeneity of temporal patterns and the modality gap between continuous numerical signals and discrete language representations. In this work, we propose TALON, a unified framework that enhances LLM-based forecasting by modeling temporal heterogeneity and enforcing semantic alignment. Specifically, we design a Heterogeneous Temporal Encoder that partitions multivariate time series into structurally coherent segments, enabling localized expert modeling across diverse temporal patterns. To bridge the modality gap, we introduce a Semantic Alignment Module that aligns temporal features with LLM-compatible representations, enabling effective integration of time series into language-based models while eliminating the need for handcrafted prompts during inference. Extensive experiments on seven real-world benchmarks demonstrate that TALON achieves superior performance across all datasets, with average MSE improvements of up to 11\% over recent state-of-the-art methods. These results underscore the effectiveness of incorporating both pattern-aware and semantic-aware designs when adapting LLMs for time series forecasting. The code is available at: https://github.com/syrGitHub/TALON.

</details>


### [96] [Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model](https://arxiv.org/abs/2508.07209)

*Chaoqun Cui, Siyuan Li, Kunkun Ma, Caiyan Jia*

**Main category:** cs.CL

**Keywords:** Pretrained Language Models, Rumor Detection, Social Media, Post Engagement Prediction, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper introduces Post Engagement Prediction (PEP) to improve rumor detection in social media by enhancing pretrained language models (PLMs) with insights from post interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Pretrained Language Models struggle with social media tasks like rumor detection due to mismatched training data and inadequate modeling of user engagements.

**Method:** The authors propose a continued pretraining strategy, PEP, that predicts relationships between posts to enhance models' understanding of social text dynamics.

**Key Contributions:**

	1. Introduction of Post Engagement Prediction (PEP) strategy.
	2. Release of large-scale TwitterCorpus and claim conversation datasets with propagation structures.
	3. Demonstration of performance improvements in rumor detection on multiple datasets.

**Result:** Experiments show that PEP significantly improves rumor detection performance by 1.0-3.7% accuracy on benchmark datasets, outperforming current state-of-the-art methods.

**Limitations:** 

**Conclusion:** The PEP strategy, combined with a tailored PLM (SoLM), effectively enhances learning of discriminative post interactions for better rumor detection.

**Abstract:** Pretrained Language Models (PLMs) have excelled in various Natural Language Processing tasks, benefiting from large-scale pretraining and self-attention mechanism's ability to capture long-range dependencies. However, their performance on social media application tasks like rumor detection remains suboptimal. We attribute this to mismatches between pretraining corpora and social texts, inadequate handling of unique social symbols, and pretraining tasks ill-suited for modeling user engagements implicit in propagation structures. To address these issues, we propose a continue pretraining strategy called Post Engagement Prediction (PEP) to infuse information from propagation structures into PLMs. PEP makes models to predict root, branch, and parent relations between posts, capturing interactions of stance and sentiment crucial for rumor detection. We also curate and release large-scale Twitter corpus: TwitterCorpus (269GB text), and two unlabeled claim conversation datasets with propagation structures (UTwitter and UWeibo). Utilizing these resources and PEP strategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments demonstrate PEP significantly boosts rumor detection performance across universal and social media PLMs, even in few-shot scenarios. On benchmark datasets, PEP enhances baseline models by 1.0-3.7\% accuracy, even enabling it to outperform current state-of-the-art methods on multiple datasets. SoLM alone, without high-level modules, also achieves competitive results, highlighting the strategy's effectiveness in learning discriminative post interaction features.

</details>


### [97] [How Does a Deep Neural Network Look at Lexical Stress?](https://arxiv.org/abs/2508.07229)

*Itai Allouche, Itay Asael, Rotem Rousso, Vered Dassa, Ann Bradlow, Seung-Eun Kim, Matthew Goldrick, Joseph Keshet*

**Main category:** cs.CL

**Keywords:** neural networks, lexical stress, CNN, interpretability, speech processing

**Relevance Score:** 6

**TL;DR:** This paper explores the interpretability of neural networks in predicting lexical stress in English disyllabic words using CNN architectures and demonstrates the influence of stressed syllables on predictions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the effectiveness of neural networks in speech processing, their 'black box' nature raises questions about interpretability and decision-making processes, prompting this examination within the context of lexical stress.

**Method:** A dataset of English disyllabic words was constructed from read and spontaneous speech. Various Convolutional Neural Network (CNN) architectures were trained to predict stress position based on spectrographic representations. Layerwise Relevance Propagation (LRP) was employed for interpretability analysis.

**Key Contributions:**

	1. Constructed a dataset for disyllabic word stress prediction using spontaneous speech
	2. Achieved 92% accuracy using CNN architectures for stress prediction
	3. Demonstrated the influence of spectral properties of stressed vowels through interpretability analysis

**Result:** The CNNs achieved up to 92% accuracy on held-out test data, revealing that predictions were influenced primarily by stressed versus unstressed syllables, particularly focusing on the spectral properties of stressed vowels.

**Limitations:** 

**Conclusion:** The findings illustrate that deep learning can effectively learn distributed cues to stress from natural data, challenging traditional phonetics which rely on controlled stimuli. The best classifier was significantly affected by the first and second formants of stressed vowels.

**Abstract:** Despite their success in speech processing, neural networks often operate as black boxes, prompting the question: what informs their decisions, and how can we interpret them? This work examines this issue in the context of lexical stress. A dataset of English disyllabic words was automatically constructed from read and spontaneous speech. Several Convolutional Neural Network (CNN) architectures were trained to predict stress position from a spectrographic representation of disyllabic words lacking minimal stress pairs (e.g., initial stress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out test data. Layerwise Relevance Propagation (LRP), a technique for CNN interpretability analysis, revealed that predictions for held-out minimal pairs (PROtest vs. proTEST ) were most strongly influenced by information in stressed versus unstressed syllables, particularly the spectral properties of stressed vowels. However, the classifiers also attended to information throughout the word. A feature-specific relevance analysis is proposed, and its results suggest that our best-performing classifier is strongly influenced by the stressed vowel's first and second formants, with some evidence that its pitch and third formant also contribute. These results reveal deep learning's ability to acquire distributed cues to stress from naturally occurring data, extending traditional phonetic work based around highly controlled stimuli.

</details>


### [98] [Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition](https://arxiv.org/abs/2508.07248)

*Zhe Ren*

**Main category:** cs.CL

**Keywords:** Few-Shot Learning, Continual Learning, Named Entity Recognition, Prompt Tuning, Memory Templates

**Relevance Score:** 6

**TL;DR:** This paper presents a novel approach to Few-Shot Continual Learning Named Entity Recognition (FS-CLNER) using prompt tuning and memory templates to enhance model performance and prevent knowledge loss.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of Few-Shot CLNER, especially the Few-Shot Distillation Dilemma caused by lack of old-class entity information.

**Method:** An expandable Anchor words-oriented Prompt Tuning (APT) paradigm and Memory Demonstration Templates (MDT) were introduced to improve model generalization and performance in few-shot scenarios.

**Key Contributions:**

	1. Development of the Anchor words-oriented Prompt Tuning (APT) paradigm
	2. Introduction of Memory Demonstration Templates (MDT) for in-context learning
	3. Effective prevention of the Few-Shot Distillation Dilemma

**Result:** The proposed APT and MDT strategies lead to competitive performance in FS-CLNER tasks and effectively avoid the Few-Shot Distillation Dilemma.

**Limitations:** 

**Conclusion:** The combined approach enhances performance in few-shot learning scenarios while maintaining old knowledge, thus contributing to the field of continual learning.

**Abstract:** Knowledge distillation has been successfully applied to Continual Learning Named Entity Recognition (CLNER) tasks, by using a teacher model trained on old-class data to distill old-class entities present in new-class data as a form of regularization, thereby avoiding catastrophic forgetting. However, in Few-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it difficult for the trained model to generalize during inference. More critically, the lack of old-class entity information hinders the distillation of old knowledge, causing the model to fall into what we refer to as the Few-Shot Distillation Dilemma. In this work, we address the above challenges through a prompt tuning paradigm and memory demonstration template strategy. Specifically, we designed an expandable Anchor words-oriented Prompt Tuning (APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby enhancing performance in few-shot scenarios. Additionally, we incorporated Memory Demonstration Templates (MDT) into each training instance to provide replay samples from previous tasks, which not only avoids the Few-Shot Distillation Dilemma but also promotes in-context learning. Experiments show that our approach achieves competitive performances on FS-CLNER.

</details>


### [99] [The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation](https://arxiv.org/abs/2508.07262)

*Bernd J. Kr√∂ger*

**Main category:** cs.CL

**Keywords:** dynamic articulatory model, speech science, visualization, electropalatography, articulation synthesis

**Relevance Score:** 4

**TL;DR:** The paper extends the DYNARTmo model to include a three-dimensional representation of the palatal dome for estimating tongue-palate contact areas based on tongue contours, enabling detailed visualizations for speech science education and therapy.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the estimation of tongue-palate contact areas for better understanding and visualizing speech articulation.

**Method:** The extension integrates three-dimensional geometries of the palatal dome to compute lateral contact points from midsagittal tongue contours, generating visualizations within a 2D+ framework.

**Key Contributions:**

	1. Integration of 3D palatal dome representation into DYNARTmo model
	2. Analytical computation of lateral contact points from tongue contours
	3. Support for multiple synchronized articulation display views.

**Result:** The model allows for three synchronized views (sagittal, glottal, and palatal) and produces visualizations similar to electropalatography, enhancing educational and therapeutic applications in speech science.

**Limitations:** 

**Conclusion:** The model's enhancement can aid in speech therapy and education, with future developments planned for additional views and synthesis capabilities.

**Abstract:** This paper describes an extension of the two-dimensional dynamic articulatory model DYNARTmo by integrating an internal three-dimensional representation of the palatal dome to estimate tongue-palate contact areas from midsagittal tongue contours. Two alternative dome geometries - a half-ellipse and a cosine based profile - are implemented to model lateral curvature in the coronal plane. Using these geometries, lateral contact points are analytically computed for each anterior-posterior position, enabling the generation of electropalatography-like visualizations within the 2D+ framework. The enhanced model supports three synchronized views (sagittal, glottal, and palatal) for static and dynamic (animated) articulation displays, suitable for speech science education and speech therapy. Future work includes adding a facial (lip) view and implementing articulatory-to-acoustic synthesis to quantitatively evaluate model realism.

</details>


### [100] [Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models](https://arxiv.org/abs/2508.07273)

*Qiongqiong Wang, Hardik B. Sailor, Jeremy H. M. Wong, Tianchi Liu, Shuo Sun, Wenyu Zhang, Muhammad Huzaifah, Nancy Chen, Ai Ti Aw*

**Main category:** cs.CL

**Keywords:** Speech-LLMs, Empathy, Paralinguistic cues, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper presents methods to enhance empathetic reasoning in Speech-LLMs by incorporating contextual paralinguistic cues during training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in empathetic reasoning exhibited by current speech language models due to the lack of datasets integrating contextual content and paralinguistic cues.

**Method:** Two methods are proposed: an explicit method supplying paralinguistic metadata directly to the LLM, and an implicit method that generates training QA pairs using emotion annotations and speech transcriptions.

**Key Contributions:**

	1. Proposed explicit and implicit methods for integrating paralinguistic cues in LLM training.
	2. Achieved significant performance improvements on QA benchmarks.
	3. Validated the effectiveness of the LLM using correlation with classification metrics.

**Result:** The implicit method improved performance by 38.41% on a human-annotated QA benchmark, achieving a combined performance of 46.02% when used with the explicit method.

**Limitations:** 

**Conclusion:** These methods demonstrate that integrating paralinguistic information significantly enhances the contextual understanding of Speech-LLMs.

**Abstract:** Current large speech language models (Speech-LLMs) often exhibit limitations in empathetic reasoning, primarily due to the absence of training datasets that integrate both contextual content and paralinguistic cues. In this work, we propose two approaches to incorporate contextual paralinguistic information into model training: (1) an explicit method that provides paralinguistic metadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit method that automatically generates novel training question-answer (QA) pairs using both categorical and dimensional emotion annotations alongside speech transcriptions. Our implicit method boosts performance (LLM-judged) by 38.41% on a human-annotated QA benchmark, reaching 46.02% when combined with the explicit approach, showing effectiveness in contextual paralinguistic understanding. We also validate the LLM judge by demonstrating its correlation with classification metrics, providing support for its reliability.

</details>


### [101] [MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory](https://arxiv.org/abs/2508.07279)

*Vasudha Varadarajan, Hui Xu, Rebecca Astrid Boehme, Mariam Marlan Mirstrom, Sverker Sikstrom, H. Andrew Schwartz*

**Main category:** cs.CL

**Keywords:** Mental health, Large language models, Adaptive questioning, Item response theory, Screening

**Relevance Score:** 9

**TL;DR:** MAQuA is an adaptive framework for efficient mental health screening using LLMs, reducing assessment questions significantly while maintaining accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiency of excessive querying in real-world mental health screening using large language models (LLMs).

**Method:** Combines multi-outcome modeling on language responses, item response theory (IRT), and factor analysis to select the most informative questions for mental health assessment.

**Key Contributions:**

	1. Introduction of an adaptive question-asking framework for mental health screening
	2. Reduction of assessment burden through efficient querying
	3. Demonstration of robust performance across multiple mental health domains

**Result:** MAQuA reduces the number of required questions for stable assessment scores by 50-87% compared to random ordering, achieving significant reductions for depression (71% fewer questions) and eating disorders (85% fewer questions).

**Limitations:** 

**Conclusion:** MAQuA is positioned as a powerful tool for interactive mental health screening, facilitating the integration of LLM-based agents into clinical workflows.

**Abstract:** Recent advances in large language models (LLMs) offer new opportunities for scalable, interactive mental health assessment, but excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles. We introduce MAQuA, an adaptive question-asking framework for simultaneous, multidimensional mental health screening. Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information, improving accuracy and potentially reducing response burden. Empirical results on a novel dataset reveal that MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering (e.g., achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions). MAQuA demonstrates robust performance across both internalizing (depression, anxiety) and externalizing (substance use, eating disorder) domains, with early stopping strategies further reducing patient time and burden. These findings position MAQuA as a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows.

</details>


### [102] ["Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas](https://arxiv.org/abs/2508.07284)

*Junchen Ding, Penghao Jiang, Zihao Xu, Ziqi Ding, Yichen Zhu, Jiaojiao Jiang, Yuekang Li*

**Main category:** cs.CL

**Keywords:** large language models, moral reasoning, ethical decision-making, trolley problem, alignment

**Relevance Score:** 9

**TL;DR:** This study evaluates the moral reasoning processes of 14 leading LLMs using trolley problem scenarios framed by various moral philosophies. It finds significant variability in decision-making and justifications among models, urging for enhanced moral reasoning alignment in LLM evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how large language models (LLMs) mediate ethically sensitive decisions and understand their moral reasoning processes.

**Method:** Empirical evaluation of 14 leading LLMs across 27 trolley problem scenarios framed by ten moral philosophies, using a factorial prompting protocol to gather 3,780 binary decisions and natural language justifications.

**Key Contributions:**

	1. Evaluation of LLMs across diverse moral scenarios
	2. Identification of notable 'sweet zones' in ethical frameworks
	3. Proposal for moral reasoning as a primary axis in LLM alignment

**Result:** Reasoning enhanced models exhibit more decisive responses and structured justifications, but do not consistently align with human consensus; 'sweet zones' identified in some ethical frames lead to optimal model performance.

**Limitations:** Variability in ethical outcomes based on moral frames and model types; controversial outcomes in kinship, legality, or self-interest scenarios.

**Conclusion:** Moral reasoning should be a key focus for LLM alignment, necessitating standardized benchmarks that evaluate the decision-making processes of LLMs, not just outcomes.

**Abstract:** As large language models (LLMs) increasingly mediate ethically sensitive decisions, understanding their moral reasoning processes becomes imperative. This study presents a comprehensive empirical evaluation of 14 leading LLMs, both reasoning enabled and general purpose, across 27 diverse trolley problem scenarios, framed by ten moral philosophies, including utilitarianism, deontology, and altruism. Using a factorial prompting protocol, we elicited 3,780 binary decisions and natural language justifications, enabling analysis along axes of decisional assertiveness, explanation answer consistency, public moral alignment, and sensitivity to ethically irrelevant cues. Our findings reveal significant variability across ethical frames and model types: reasoning enhanced models demonstrate greater decisiveness and structured justifications, yet do not always align better with human consensus. Notably, "sweet zones" emerge in altruistic, fairness, and virtue ethics framings, where models achieve a balance of high intervention rates, low explanation conflict, and minimal divergence from aggregated human judgments. However, models diverge under frames emphasizing kinship, legality, or self interest, often producing ethically controversial outcomes. These patterns suggest that moral prompting is not only a behavioral modifier but also a diagnostic tool for uncovering latent alignment philosophies across providers. We advocate for moral reasoning to become a primary axis in LLM alignment, calling for standardized benchmarks that evaluate not just what LLMs decide, but how and why.

</details>


### [103] [Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking](https://arxiv.org/abs/2508.07286)

*Jian Chen, Jinbao Tian, Yankui Li, Zhou Li*

**Main category:** cs.CL

**Keywords:** named entity recognition, large language models, knowledge generation, architecture engineering construction, RoBERTa

**Relevance Score:** 7

**TL;DR:** The paper presents ARCE, a novel method for enhancing NER in AEC using large language models for knowledge generation, achieving a state-of-the-art Macro-F1 score.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of named entity recognition in the AEC domain, where standard models struggle with specialized terminology and relational contexts.

**Method:** ARCE uses an LLM to generate simple explanations (Cote) which are then used for incremental pre-training of a RoBERTa model before fine-tuning on NER tasks.

**Key Contributions:**

	1. Introduction of ARCE for knowledge generation in NER
	2. Establishment of new state-of-the-art metrics for AEC
	3. Demonstration of effectiveness of simple explanations over complex ones.

**Result:** ARCE achieves a Macro-F1 score of 77.20%, setting a new state-of-the-art on an AEC benchmark dataset.

**Limitations:** 

**Conclusion:** Simple explanation-based knowledge is more effective for NER in the AEC domain than complex rationales.

**Abstract:** Accurate information extraction from specialized texts is a critical challenge, particularly for named entity recognition (NER) in the architecture, engineering, and construction (AEC) domain to support automated rule checking (ARC). The performance of standard pre-trained models is often constrained by the domain gap, as they struggle to interpret the specialized terminology and complex relational contexts inherent in AEC texts. Although this issue can be mitigated by further pre-training on large, human-curated domain corpora, as exemplified by methods like ARCBERT, this approach is both labor-intensive and cost-prohibitive. Consequently, leveraging large language models (LLMs) for automated knowledge generation has emerged as a promising alternative. However, the optimal strategy for generating knowledge that can genuinely enhance smaller, efficient models remains an open question. To address this, we propose ARCE (augmented RoBERTa with contextualized elucidations), a novel approach that systematically explores and optimizes this generation process. ARCE employs an LLM to first generate a corpus of simple, direct explanations, which we term Cote, and then uses this corpus to incrementally pre-train a RoBERTa model prior to its fine-tuning on the downstream task. Our extensive experiments show that ARCE establishes a new state-of-the-art on a benchmark AEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a key finding: simple, explanation-based knowledge proves surprisingly more effective than complex, role-based rationales for this task. The code is publicly available at:https://github.com/nxcc-lab/ARCE.

</details>


### [104] [CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation](https://arxiv.org/abs/2508.07295)

*Yexing Du, Kaiyuan Liu, Youcheng Pan, Zheng Chu, Bo Yang, Xiaocheng Feng, Yang Xiang, Ming Liu*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Cross-lingual, Cross-modal, Factuality, Question Answering

**Relevance Score:** 9

**TL;DR:** The paper introduces CCFQA, a cross-lingual and cross-modal factuality benchmark for evaluating MLLMs across 8 languages, addressing gaps in existing benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the reliability of Multimodal Large Language Models (MLLMs) in multilingual contexts, especially for speech input, given the limitations of existing benchmarks that focus primarily on English.

**Method:** Development of CCFQA, a benchmark containing parallel speech-text factual questions in 8 languages, along with a few-shot transfer learning strategy for improving multilingual Spoken Question Answering (SQA).

**Key Contributions:**

	1. Introduction of a novel benchmark for evaluating MLLMs in multilingual settings.
	2. Demonstration of substantial challenges faced by MLLMs on the new benchmark.
	3. Development of a few-shot learning strategy that improves multilingual QA performance.

**Result:** Experimental results show substantial challenges for current MLLMs on CCFQA, while the proposed few-shot transfer learning strategy achieves competitive performance with minimal training data.

**Limitations:** The benchmark primarily tests the capabilities of existing MLLMs and may not cover all aspects of cross-lingual factuality.

**Conclusion:** CCFQA serves as a foundational resource to enhance MLLMs' performance in speech understanding, addressing significant gaps in the evaluation of multilingual and cross-modal capabilities.

**Abstract:** As Large Language Models (LLMs) are increasingly popularized in the multilingual world, ensuring hallucination-free factuality becomes markedly crucial. However, existing benchmarks for evaluating the reliability of Multimodal Large Language Models (MLLMs) predominantly focus on textual or visual modalities with a primary emphasis on English, which creates a gap in evaluation when processing multilingual input, especially in speech. To bridge this gap, we propose a novel \textbf{C}ross-lingual and \textbf{C}ross-modal \textbf{F}actuality benchmark (\textbf{CCFQA}). Specifically, the CCFQA benchmark contains parallel speech-text factual questions across 8 languages, designed to systematically evaluate MLLMs' cross-lingual and cross-modal factuality capabilities. Our experimental results demonstrate that current MLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we propose a few-shot transfer learning strategy that effectively transfers the Question Answering (QA) capabilities of LLMs in English to multilingual Spoken Question Answering (SQA) tasks, achieving competitive performance with GPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a foundational research resource to promote the development of MLLMs with more robust and reliable speech understanding capabilities. Our code and dataset are available at https://github.com/yxduir/ccfqa.

</details>


### [105] [HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways](https://arxiv.org/abs/2508.07308)

*Cristian Cosentino, Annamaria Defilippo, Marco Dossena, Christopher Irwin, Sara Joubbi, Pietro Li√≤*

**Main category:** cs.CL

**Keywords:** medical Q&A, large language models, dataset, reasoning, healthcare

**Relevance Score:** 9

**TL;DR:** HealthBranches is a benchmark dataset for evaluating complex reasoning in medical Question-Answering for Large Language Models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a reliable dataset that assesses the reasoning capabilities of LLMs in medical Q&A contexts.

**Method:** The dataset is generated via a semi-automated pipeline that transforms decision pathways from medical sources into patient cases with questions and answers.

**Key Contributions:**

	1. Novel benchmark dataset for medical Q&A
	2. Supports open-ended and multiple-choice formats
	3. Includes full reasoning paths for evaluations

**Result:** The dataset covers 4,063 case studies across 17 healthcare topics and includes structured reasoning paths for each Q&A.

**Limitations:** 

**Conclusion:** HealthBranches provides a base for developing trustworthy LLMs in healthcare and offers educational resources.

**Abstract:** HealthBranches is a novel benchmark dataset for medical Question-Answering (Q&A), specifically designed to evaluate complex reasoning in Large Language Models (LLMs). This dataset is generated through a semi-automated pipeline that transforms explicit decision pathways from medical source into realistic patient cases with associated questions and answers. Covering 4,063 case studies across 17 healthcare topics, each data point is based on clinically validated reasoning chains. HealthBranches supports both open-ended and multiple-choice question formats and uniquely includes the full reasoning path for each Q&A. Its structured design enables robust evaluation of LLMs' multi-step inference capabilities, including their performance in structured Retrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a foundation for the development of more trustworthy, interpretable, and clinically reliable LLMs in high-stakes domains while also serving as a valuable resource for educational purposes.

</details>


### [106] [ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering](https://arxiv.org/abs/2508.07321)

*Shubhra Ghosh, Abhilekh Borah, Aditya Kumar Guru, Kripabandhu Ghosh*

**Main category:** cs.CL

**Keywords:** Large Language Models, robustness evaluation, question obfuscation, AI systems, benchmarking

**Relevance Score:** 9

**TL;DR:** The paper introduces ObfusQA, a framework for evaluating LLMs' robustness against obfuscated questions, revealing their limitations and making the methodology publicly available.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the gap in understanding LLMs' performance under obfuscated questions, highlighting the necessity for robust AI systems in factual question-answering.

**Method:** Developed the ObfusQA framework which includes multi-tiered obfuscation levels to assess LLM capabilities in three areas: Named-Entity Indirection, Distractor Indirection, and Contextual Overload.

**Key Contributions:**

	1. Introduction of the ObfusQA framework for evaluating LLM robustness against question obfuscations
	2. Identification of LLM performance limitations under nuanced variations
	3. Public availability of the ObfusQAte methodology for research purposes

**Result:** The analysis shows that LLMs tend to fail or hallucinate responses under nuanced question obfuscations.

**Limitations:** 

**Conclusion:** ObfusQA serves as a comprehensive benchmark for evaluating LLM robustness and adaptability, promoting further research in the evaluation of AI systems.

**Abstract:** The rapid proliferation of Large Language Models (LLMs) has significantly contributed to the development of equitable AI systems capable of factual question-answering (QA). However, no known study tests the LLMs' robustness when presented with obfuscated versions of questions. To systematically evaluate these limitations, we propose a novel technique, ObfusQAte and, leveraging the same, introduce ObfusQA, a comprehensive, first of its kind, framework with multi-tiered obfuscation levels designed to examine LLM capabilities across three distinct dimensions: (i) Named-Entity Indirection, (ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these fine-grained distinctions in language, ObfusQA provides a comprehensive benchmark for evaluating LLM robustness and adaptability. Our study observes that LLMs exhibit a tendency to fail or generate hallucinated responses when confronted with these increasingly nuanced variations. To foster research in this direction, we make ObfusQAte publicly available.

</details>


### [107] [Strategies of Code-switching in Human-Machine Dialogs](https://arxiv.org/abs/2508.07325)

*Dean Geckt, Melinda Fricke, Shuly Wintner*

**Main category:** cs.CL

**Keywords:** code-switching, bilingualism, chatbot, language technology, user experience

**Relevance Score:** 6

**TL;DR:** Study on a chatbot that effectively utilizes code-switching between Spanish and English, examining user interactions and preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the characteristics of code-switched language and the feasibility of using chatbots in bilingual language research.

**Method:** Developed a chatbot and conducted two experiments prompting code-switching strategies, analyzing participant engagement and completion of the Map Task.

**Key Contributions:**

	1. Developed a chatbot for code-switching interactions
	2. Examined user sensitivity to discourse variations
	3. Identified the impact of language predictability on user engagement

**Result:** Participants preferred predictable code-switching patterns; less success and enjoyment were noted with random or ungrammatical code-switching.

**Limitations:** Limited scope of code-switching explored; results may not generalize to all multilingual contexts.

**Conclusion:** The findings highlight both the challenges and potential of multilingual technology in bilingual research contexts.

**Abstract:** Most people are multilingual, and most multilinguals code-switch, yet the characteristics of code-switched language are not fully understood. We developed a chatbot capable of completing a Map Task with human participants using code-switched Spanish and English. In two experiments, we prompted the bot to code-switch according to different strategies, examining (1) the feasibility of such experiments for investigating bilingual language use, and (2) whether participants would be sensitive to variations in discourse and grammatical patterns. Participants generally enjoyed code-switching with our bot as long as it produced predictable code-switching behavior; when code-switching was random or ungrammatical (as when producing unattested incongruent mixed-language noun phrases, such as `la fork'), participants enjoyed the task less and were less successful at completing it. These results underscore the potential downsides of deploying insufficiently developed multilingual language technology, while also illustrating the promise of such technology for conducting research on bilingual language use.

</details>


### [108] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)

*Joseph T. Colonel, Baihan Lin*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Word Clouds, Qualitative Research, Large Language Models, Thematic Analysis

**Relevance Score:** 8

**TL;DR:** ThemeClouds is an open-source tool that uses LLMs to create thematic, participant-weighted word clouds from dialogue transcripts, improving qualitative analysis of interviews.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional frequency-based word clouds fail to accurately represent conversational contexts in qualitative research, limiting their usefulness during early-stage analysis of participant interviews.

**Method:** The system employs large language models to identify concept-level themes and counts unique participant mentions for each topic, yielding a participant-weighted visualization.

**Key Contributions:**

	1. Introduction of ThemeClouds, a new visualization tool for qualitative research using LLMs
	2. Participant-weighted thematic representation improves understanding of qualitative data
	3. Customization options enhance researcher control and transparency in the analysis process

**Result:** The ThemeClouds approach provided more actionable insights into device concerns than traditional frequency clouds and topic-modeling techniques like LDA and BERTopic.

**Limitations:** 

**Conclusion:** Integrating LLMs into qualitative workflows can enhance interpretability and researcher agency while offering opportunities for interactive analyses, such as contrasting conditions.

**Abstract:** Word clouds are a common way to summarize qualitative interviews, yet traditional frequency-based methods often fail in conversational contexts: they surface filler words, ignore paraphrase, and fragment semantically related ideas. This limits their usefulness in early-stage analysis, when researchers need fast, interpretable overviews of what participant actually said. We introduce ThemeClouds, an open-source visualization tool that uses large language models (LLMs) to generate thematic, participant-weighted word clouds from dialogue transcripts. The system prompts an LLM to identify concept-level themes across a corpus and then counts how many unique participants mention each topic, yielding a visualization grounded in breadth of mention rather than raw term frequency. Researchers can customize prompts and visualization parameters, providing transparency and control. Using interviews from a user study comparing five recording-device configurations (31 participants; 155 transcripts, Whisper ASR), our approach surfaces more actionable device concerns than frequency clouds and topic-modeling baselines (e.g., LDA, BERTopic). We discuss design trade-offs for integrating LLM assistance into qualitative workflows, implications for interpretability and researcher agency, and opportunities for interactive analyses such as per-condition contrasts (``diff clouds'').

</details>


### [109] [Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance](https://arxiv.org/abs/2508.07375)

*Wenqian Cui, Lei Zhu, Xiaohui Li, Zhihan Guo, Haoli Bai, Lu Hou, Irwin King*

**Main category:** cs.CL

**Keywords:** Full-Duplex Speech Language Models, Conversational AI, Speech Generation

**Relevance Score:** 9

**TL;DR:** This paper introduces TurnGuide, a method for improving Full-Duplex Speech Language Models (FD-SLMs) by enhancing their conversational dynamics through dynamic segmentation and turn-level text guidance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** FD-SLMs struggle with conversational quality due to issues like prolonged speech sequences and limited spoken dialogue data, which this work aims to address.

**Method:** TurnGuide segments assistant speech into dialogue turns and generates corresponding turn-level text guidance, thereby improving timing and length alignment in speech generation.

**Key Contributions:**

	1. Introduction of TurnGuide to enhance FD-SLMs
	2. Dynamic segmentation of assistant speech
	3. Improvement of conversational capabilities through planned dialogue turns

**Result:** Experiments show that the TurnGuide approach significantly enhances the conversational abilities of e2e FD-SLMs, resulting in coherent and semantically meaningful speech output.

**Limitations:** Still a work in progress and may require further validation and optimization.

**Conclusion:** The proposed method addresses critical timing and length issues in speech generation for FD-SLMs, resulting in more natural and effective human-like interactions.

**Abstract:** Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation models designed to enable natural, real-time spoken interactions by modeling complex conversational dynamics such as interruptions, backchannels, and overlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world double-channel conversational data to capture nuanced two-speaker dialogue patterns for human-like interactions. However, they face a critical challenge -- their conversational abilities often degrade compared to pure-text conversation due to prolonged speech sequences and limited high-quality spoken dialogue data. While text-guided speech generation could mitigate these issues, it suffers from timing and length issues when integrating textual guidance into double-channel audio streams, disrupting the precise time alignment essential for natural interactions. To address these challenges, we propose TurnGuide, a novel planning-inspired approach that mimics human conversational planning by dynamically segmenting assistant speech into dialogue turns and generating turn-level text guidance before speech output, which effectively resolves both insertion timing and length challenges. Extensive experiments demonstrate our approach significantly improves e2e FD-SLMs' conversational abilities, enabling them to generate semantically meaningful and coherent speech while maintaining natural conversational flow. Demos are available at https://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at https://github.com/dreamtheater123/TurnGuide.

</details>


### [110] [Grounding Multilingual Multimodal LLMs With Cultural Knowledge](https://arxiv.org/abs/2508.07414)

*Jean de Dieu Nyandwi, Yueqi Song, Simran Khanuja, Graham Neubig*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Cultural Knowledge, Visual Question Answering, Multilingual, Knowledge Graph

**Relevance Score:** 7

**TL;DR:** This paper presents CulturalGround, a dataset designed to enhance the performance of Multimodal Large Language Models (MLLMs) on culturally significant entities and low-resource languages, achieving state-of-the-art results on culture-focused benchmarks with a new model, CulturalPangea.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underperformance of MLLMs in low-resource languages and cultural contexts, aiming for a more inclusive multimodal system.

**Method:** A data-centric approach was implemented by creating the CulturalGround dataset from a knowledge graph, which consisted of 22 million visual question answering pairs in multiple languages, and training the CulturalPangea MLLM on this dataset combined with standard multilingual data.

**Key Contributions:**

	1. Introduction of a large-scale dataset CulturalGround that includes culturally rich VQA pairs.
	2. Development of CulturalPangea, an open-source MLLM that trains on this new dataset and achieves state-of-the-art results.
	3. Demonstrated practical methods for improving MLLM performance on underrepresented cultural entities.

**Result:** CulturalPangea outperforms previous models by an average of 5.0 on culture-focused multilingual multimodal benchmarks while maintaining performance on mainstream tasks.

**Limitations:** The study primarily focuses on cultural representation and may not generalize to other domains outside of culturally significant entities.

**Conclusion:** The proposed culturally grounded method significantly narrows the cultural gap in MLLMs, suggesting a viable approach for enhanced inclusivity in multimodal AI systems.

**Abstract:** Multimodal Large Language Models excel in high-resource settings, but often misinterpret long-tail cultural entities and underperform in low-resource languages. To address this gap, we propose a data-centric approach that directly grounds MLLMs in cultural knowledge. Leveraging a large scale knowledge graph from Wikidata, we collect images that represent culturally significant entities, and generate synthetic multilingual visual question answering data. The resulting dataset, CulturalGround, comprises 22 million high-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages. We train an open-source MLLM CulturalPangea on CulturalGround, interleaving standard multilingual instruction-tuning data to preserve general abilities. CulturalPangea achieves state-of-the-art performance among open models on various culture-focused multilingual multimodal benchmarks, outperforming prior models by an average of 5.0 without degrading results on mainstream vision-language tasks. Our findings show that our targeted, culturally grounded approach could substantially narrow the cultural gap in MLLMs and offer a practical path towards globally inclusive multimodal systems.

</details>


### [111] [Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs](https://arxiv.org/abs/2508.07434)

*Zhiyi Lyu, Jianguo Huang, Yanchen Deng, Steven Hoi, Bo An*

**Main category:** cs.CL

**Keywords:** Large Language Models, Code Generation, Local Search, Reinforcement Learning, Algorithms

**Relevance Score:** 7

**TL;DR:** ReLoc is a unified local search framework for code revision that outperforms existing code generation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges in efficiency and scalability of code generation using LLMs.

**Method:** ReLoc employs a local search framework with four components: initial code drafting, neighborhood code generation, candidate evaluation, and incumbent updating, utilizing decision rules for algorithms like HC or GA.

**Key Contributions:**

	1. Introduction of ReLoc framework for local search in code generation
	2. Development of a revision reward model for evaluating code quality
	3. Demonstrated superior performance in diverse code generation tasks

**Result:** ReLoc achieves superior performance in various code generation tasks, surpassing both construction-based and state-of-the-art improvement-based methods.

**Limitations:** 

**Conclusion:** The proposed approach significantly enhances the effectiveness of code generation through structured local revisions and a specialized revision reward model.

**Abstract:** Large Language Models (LLMs) with inference-time scaling techniques show promise for code generation, yet face notable efficiency and scalability challenges. Construction-based tree-search methods suffer from rapid growth in tree size, high token consumption, and lack of anytime property. In contrast, improvement-based methods offer better performance but often struggle with uninformative reward signals and inefficient search strategies. In this work, we propose \textbf{ReLoc}, a unified local search framework which effectively performs step-by-step code revision. Specifically, ReLoc explores a series of local revisions through four key algorithmic components: initial code drafting, neighborhood code generation, candidate evaluation, and incumbent code updating, each of which can be instantiated with specific decision rules to realize different local search algorithms such as Hill Climbing (HC) or Genetic Algorithm (GA). Furthermore, we develop a specialized revision reward model that evaluates code quality based on revision distance to produce fine-grained preferences that guide the local search toward more promising candidates. Finally, our extensive experimental results demonstrate that our approach achieves superior performance across diverse code generation tasks, significantly outperforming both construction-based tree search as well as the state-of-the-art improvement-based code generation methods.

</details>


### [112] [Positional Biases Shift as Inputs Approach Context Window Limits](https://arxiv.org/abs/2508.07479)

*Blerta Veseli, Julian Chibane, Mariya Toneva, Alexander Koller*

**Main category:** cs.CL

**Keywords:** Large Language Models, positional biases, context window, retrieval, reasoning

**Relevance Score:** 8

**TL;DR:** This study analyzes the effective use of long inputs by Large Language Models (LLMs), revealing a distance-based bias in performance with positional influences based on input length.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the inconsistencies in prior findings related to positional biases in LLMs when processing long inputs and to explore conditions for their manifestation.

**Method:** A comprehensive analysis using relative input lengths defined in relation to the model's context window, examining the performance differences based on input positioning.

**Key Contributions:**

	1. Introduced a relative input length analysis framework for LLMs.
	2. Identified the distance-based bias effect in model performance with long inputs.
	3. Provided new insights into the role of retrieval in reasoning for LLMs.

**Result:** The LiM effect is strongest when inputs are up to 50% of a model's context window; beyond that, primacy bias weakens while recency bias remains stable, indicating a shift to distance-based bias.

**Limitations:** The study primarily focuses on performance related to long-context inputs and may not fully address other biases or contexts.

**Conclusion:** Positional biases in LLMs are inherited from retrieval mechanisms, suggesting important implications for designing future benchmarks and evaluation methodologies for long-context input tasks.

**Abstract:** Large Language Models (LLMs) often struggle to use information across long inputs effectively. Prior work has identified positional biases, such as the Lost in the Middle (LiM) effect, where models perform better when information appears at the beginning (primacy bias) or end (recency bias) of the input, rather than in the middle. However, long-context studies have not consistently replicated these effects, raising questions about their intensity and the conditions under which they manifest. To address this, we conducted a comprehensive analysis using relative rather than absolute input lengths, defined with respect to each model's context window. Our findings reveal that the LiM effect is strongest when inputs occupy up to 50% of a model's context window. Beyond that, the primacy bias weakens, while recency bias remains relatively stable. This effectively eliminates the LiM effect; instead, we observe a distance-based bias, where model performance is better when relevant information is closer to the end of the input. Furthermore, our results suggest that successful retrieval is a prerequisite for reasoning in LLMs, and that the observed positional biases in reasoning are largely inherited from retrieval. These insights have implications for long-context tasks, the design of future LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

</details>


### [113] [ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models](https://arxiv.org/abs/2508.07484)

*Archchana Sindhujan, Shenbin Qian, Chan Chi Chun Matthew, Constantin Orasan, Diptesh Kanojia*

**Main category:** cs.CL

**Keywords:** Large Language Models, Quality Estimation, Machine Translation, Layer-wise Adaptation, Cross-lingual Tasks

**Relevance Score:** 6

**TL;DR:** Introduction of ALOPE, an adaptive layer-optimization framework to improve Quality Estimation for Machine Translation using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM-based QE systems struggle due to their pre-training for causal language modeling rather than regression, especially for low-resource languages.

**Method:** The ALOPE framework applies layer-wise adaptation using low-rank adapters with regression task heads and includes dynamic weighting and multi-head regression strategies for improved predictions.

**Key Contributions:**

	1. Introduction of layer-wise adaptation for LLMs in Quality Estimation
	2. Use of low-rank adapters with regression heads
	3. Publication of models and code for further research.

**Result:** ALOPE demonstrates significant improvements over current LLM-based QE methods, benefiting from better cross-lingual alignment through layer-specific adaptations.

**Limitations:** 

**Conclusion:** The study provides a new method for enhancing Quality Estimation in Machine Translation, making available resulting models and code to further research in the field.

**Abstract:** Large Language Models (LLMs) have shown remarkable performance across a wide range of natural language processing tasks. Quality Estimation (QE) for Machine Translation (MT), which assesses the quality of a source-target pair without relying on reference translations, remains a challenging cross-lingual task for LLMs. The challenges stem from the inherent limitations of existing LLM-based QE systems, which are pre-trained for causal language modelling rather than regression-specific tasks, further elevated by the presence of low-resource languages given pre-training data distribution. This paper introduces ALOPE, an adaptive layer-optimization framework designed to enhance LLM-based QE by restructuring Transformer representations through layer-wise adaptation for improved regression-based prediction. Our framework integrates low-rank adapters (LoRA) with regression task heads, leveraging selected pre-trained Transformer layers for improved cross-lingual alignment. In addition to the layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting, which adaptively combines representations from multiple layers, and multi-head regression, which aggregates regression losses from multiple heads for QE. Our framework shows improvements over various existing LLM-based QE approaches. Empirical evidence suggests that intermediate Transformer layers in LLMs provide contextual representations that are more aligned with the cross-lingual nature of the QE task. We make resultant models and framework code publicly available for further research, also allowing existing LLM-based MT frameworks to be scaled with QE capabilities.

</details>


### [114] [Augmenting Bias Detection in LLMs Using Topological Data Analysis](https://arxiv.org/abs/2508.07516)

*Keshav Varadarajan, Tananun Songdechakraiwut*

**Main category:** cs.CL

**Keywords:** bias detection, topological data analysis, GPT-2, large language models, StereoSet

**Relevance Score:** 9

**TL;DR:** This study proposes a method using topological data analysis to identify which attention heads in GPT-2 contribute to bias in large language models, particularly focusing on gender and profession.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of methods that pinpoint specific parts of large language models responsible for bias against identity groups, enhancing understanding and mitigation of such biases.

**Method:** The authors use topological data analysis to analyze the attention heads of GPT-2 in relation to the types of bias evidenced in the StereoSet dataset.

**Key Contributions:**

	1. Novel application of topological data analysis to analyze large language models for bias
	2. Identification of specific attention heads responsible for category-specific biases
	3. Framework for future de-biasing efforts in language models.

**Result:** The study identifies that certain attention heads serve as 'hot spots' for biases related to gender and profession, highlighting specific sections of the model that misrepresent identity groups.

**Limitations:** The method focuses on GPT-2 and may not generalize to other language models; further exploration of the implications of the findings is necessary.

**Conclusion:** Future research can build on this metric to develop methodologies aimed at de-biasing large language models by targeting identified heads that contribute to biases.

**Abstract:** Recently, many bias detection methods have been proposed to determine the level of bias a large language model captures. However, tests to identify which parts of a large language model are responsible for bias towards specific groups remain underdeveloped. In this study, we present a method using topological data analysis to identify which heads in GPT-2 contribute to the misrepresentation of identity groups present in the StereoSet dataset. We find that biases for particular categories, such as gender or profession, are concentrated in attention heads that act as hot spots. The metric we propose can also be used to determine which heads capture bias for a specific group within a bias category, and future work could extend this method to help de-bias large language models.

</details>


### [115] [Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews](https://arxiv.org/abs/2508.07517)

*Joseph T. Colonel, Baihan Lin*

**Main category:** cs.CL

**Keywords:** Word clouds, Qualitative research, Large language models, Thematic analysis, User studies

**Relevance Score:** 9

**TL;DR:** ThemeClouds is a visualization tool that utilizes large language models to generate thematic word clouds from qualitative interview data, enhancing the interpretability of participant responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional word cloud methods based on frequency analysis are ineffective for qualitative interview data, which can include filler words and fragmented themes.

**Method:** ThemeClouds prompts an LLM to identify themes across dialogue transcripts, creating participant-weighted visualizations that reflect the number of unique contributors to each theme.

**Key Contributions:**

	1. Introduces a novel visualization tool that leverages LLMs for qualitative data
	2. Enhances interpretability of qualitative data through thematic visualization
	3. Provides customizable options for researchers to control prompts and visualization parameters.

**Result:** ThemeClouds provides more actionable insights into participant concerns compared to traditional frequency-based methods and established topic modeling techniques like LDA and BERTopic.

**Limitations:** May require careful prompt design to ensure meaningful thematic extraction.

**Conclusion:** The integration of LLM assistance in qualitative research can enhance interpretability and customize workflows while allowing for nuanced analysis.

**Abstract:** Word clouds are a common way to summarize qualitative interviews, yet traditional frequency-based methods often fail in conversational contexts: they surface filler words, ignore paraphrase, and fragment semantically related ideas. This limits their usefulness in early-stage analysis, when researchers need fast, interpretable overviews of what participant actually said. We introduce ThemeClouds, an open-source visualization tool that uses large language models (LLMs) to generate thematic, participant-weighted word clouds from dialogue transcripts. The system prompts an LLM to identify concept-level themes across a corpus and then counts how many unique participants mention each topic, yielding a visualization grounded in breadth of mention rather than raw term frequency. Researchers can customize prompts and visualization parameters, providing transparency and control. Using interviews from a user study comparing five recording-device configurations (31 participants; 155 transcripts, Whisper ASR), our approach surfaces more actionable device concerns than frequency clouds and topic-modeling baselines (e.g., LDA, BERTopic). We discuss design trade-offs for integrating LLM assistance into qualitative workflows, implications for interpretability and researcher agency, and opportunities for interactive analyses such as per-condition contrasts (``diff clouds'').

</details>


### [116] [From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR](https://arxiv.org/abs/2508.07534)

*Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** reinforcement learning, large language models, exploration strategies, verifiable rewards, machine learning

**Relevance Score:** 9

**TL;DR:** This report investigates exploration behaviors in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs).

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The work addresses the lack of understanding of exploration behaviors in LLMs when using RLVR, which is crucial for effectively guiding reasoning chains in LLMs.

**Method:** The paper systematically investigates four aspects of exploration capacities in RLVR, including exploration space shaping, entropy-performance exchange, and RL performance optimization.

**Key Contributions:**

	1. Development of quantitative metrics for exploration space shaping
	2. Analysis of entropy-performance exchange across training stages
	3. Exploration of methods for optimizing RL performance based on exploration gains

**Result:** The study develops quantitative metrics for LLMs' capability boundaries, analyzes entropy-performance exchanges across various dimensions, and explores methods to improve RL performance based on exploration gains.

**Limitations:** The fundamental mechanisms governing exploration behaviors will require further exploration beyond this work.

**Conclusion:** The findings unify existing insights with new evidence, establishing a foundational framework for improving RLVR systems in LLMs.

**Abstract:** Reinforcement learning with verifiable rewards (RLVR) has emerged as a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based feedback to guide LLMs in generating and refining complex reasoning chains -- a process critically dependent on effective exploration strategies. While prior work has demonstrated RLVR's empirical success, the fundamental mechanisms governing LLMs' exploration behaviors remain underexplored. This technical report presents a systematic investigation of exploration capacities in RLVR, covering four main aspects: (1) exploration space shaping, where we develop quantitative metrics to characterize LLMs' capability boundaries; (2) entropy-performance exchange, analyzed across training stages, individual instances, and token-level patterns; and (3) RL performance optimization, examining methods to effectively translate exploration gains into measurable improvements. By unifying previously identified insights with new empirical evidence, this work aims to provide a foundational framework for advancing RLVR systems.

</details>


### [117] [IBPS: Indian Bail Prediction System](https://arxiv.org/abs/2508.07592)

*Puspesh Kumar Srivastava, Uddeshya Raj, Praveen Patel, /Shubham Kumar Nigam, Noel Shallum, Arnab Bhattacharya*

**Main category:** cs.CL

**Keywords:** Bail prediction, Legal AI, Machine learning, Human rights, Judicial system

**Relevance Score:** 6

**TL;DR:** The Indian Bail Prediction System (IBPS) utilizes AI to improve bail decision-making by predicting outcomes based on factual case attributes and statutory provisions, addressing issues of subjectivity and delays in the judicial system.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Bail decisions in Indian courts are subjective and inconsistent, contributing to human rights issues and judicial backlog, particularly affecting undertrial prisoners from disadvantaged backgrounds.

**Method:** An AI-powered framework that predicts bail outcomes using a large dataset of 150,430 High Court bail judgments with structured annotations. A large language model was fine-tuned with parameter-efficient techniques and evaluated across multiple configurations.

**Key Contributions:**

	1. Large-scale dataset of bail judgments released for research
	2. Fine-tuned LLMs show improved performance with statutory context
	3. Tool promotes procedural fairness in bail decision-making

**Result:** Models fine-tuned with statutory knowledge significantly outperform baselines in accuracy and explanation quality, demonstrating effective prediction and support for legal practitioners.

**Limitations:** 

**Conclusion:** IBPS provides a transparent and scalable solution for data-driven legal assistance, potentially reducing delays in bail processes and enhancing fairness in the Indian judicial system.

**Abstract:** Bail decisions are among the most frequently adjudicated matters in Indian courts, yet they remain plagued by subjectivity, delays, and inconsistencies. With over 75% of India's prison population comprising undertrial prisoners, many from socioeconomically disadvantaged backgrounds, the lack of timely and fair bail adjudication exacerbates human rights concerns and contributes to systemic judicial backlog. In this paper, we present the Indian Bail Prediction System (IBPS), an AI-powered framework designed to assist in bail decision-making by predicting outcomes and generating legally sound rationales based solely on factual case attributes and statutory provisions. We curate and release a large-scale dataset of 150,430 High Court bail judgments, enriched with structured annotations such as age, health, criminal history, crime category, custody duration, statutes, and judicial reasoning. We fine-tune a large language model using parameter-efficient techniques and evaluate its performance across multiple configurations, with and without statutory context, and with RAG. Our results demonstrate that models fine-tuned with statutory knowledge significantly outperform baselines, achieving strong accuracy and explanation quality, and generalize well to a test set independently annotated by legal experts. IBPS offers a transparent, scalable, and reproducible solution to support data-driven legal assistance, reduce bail delays, and promote procedural fairness in the Indian judicial system.

</details>


### [118] [Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements](https://arxiv.org/abs/2508.07598)

*Ziheng Li, Zhi-Hong Deng*

**Main category:** cs.CL

**Keywords:** LLM, in-context learning, event detection, keyword-centric prompting, natural language processing

**Relevance Score:** 9

**TL;DR:** The paper introduces KeyCP++, a keyword-centric chain-of-thought prompting method to enhance event detection in LLMs, overcoming issues with traditional in-context learning approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM-based in-context learning struggles with event detection due to over-interpretation and inadequate understanding of event triggers.

**Method:** KeyCP++ uses a prompting template that identifies logical gaps in the input text and detection results, incorporating exemplary triggers to improve candidate trigger proposals and rationales.

**Key Contributions:**

	1. Introduction of KeyCP++, a novel prompting approach for event detection
	2. Automatic annotation of logical gaps in prompts
	3. Enhanced rationale generation for trigger proposals

**Result:** Experiments show KeyCP++ leads to significant improvements in one-shot event detection performance compared to existing methods.

**Limitations:** 

**Conclusion:** KeyCP++ effectively assists LLMs in learning detection rules and reduces reliance on merely keyword identification.

**Abstract:** Although the LLM-based in-context learning (ICL) paradigm has demonstrated considerable success across various natural language processing tasks, it encounters challenges in event detection. This is because LLMs lack an accurate understanding of event triggers and tend to make over-interpretation, which cannot be effectively corrected through in-context examples alone. In this paper, we focus on the most challenging one-shot setting and propose KeyCP++, a keyword-centric chain-of-thought prompting approach. KeyCP++ addresses the weaknesses of conventional ICL by automatically annotating the logical gaps between input text and detection results for the demonstrations. Specifically, to generate in-depth and meaningful rationale, KeyCP++ constructs a trigger discrimination prompting template. It incorporates the exemplary triggers (a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let LLM propose candidate triggers, and justify each candidate. These propose-and-judge rationales help LLMs mitigate over-reliance on the keywords and promote detection rule learning. Extensive experiments demonstrate the effectiveness of our approach, showcasing significant advancements in one-shot event detection.

</details>


### [119] [InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information](https://arxiv.org/abs/2508.07630)

*Anirudh Iyengar Kaniyar Narayana Iyengar, Srija Mukhopadhyay, Adnan Qidwai, Shubhankar Singh, Dan Roth, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** vision-language models, chart reasoning, diagnostic benchmark, multimodal reasoning, evaluation framework

**Relevance Score:** 8

**TL;DR:** InterChart is a diagnostic benchmark for evaluating vision-language models on reasoning tasks involving multiple related charts, highlighting their limitations in complex scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of vision-language models (VLMs) in reasoning across related charts, which is essential for applications like scientific reporting and financial analysis.

**Method:** The benchmark consists of three tiers: factual reasoning over individual charts, integrative analysis of aligned chart sets, and semantic inference on complex chart pairs. It evaluates models on diverse question types related to these charts.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for VLMs across diverse chart reasoning tasks.
	2. Identification of systematic limitations in VLM performance with complex multi-visual data.
	3. Structured evaluation framework to enhance multimodal reasoning in real-world applications.

**Result:** State-of-the-art VLMs show a consistent decline in accuracy as chart complexity increases, performing better with simpler visual units rather than complex, integrated charts.

**Limitations:** The benchmark mainly focuses on visual reasoning, and may not cover all aspects of chart interpretation for models.

**Conclusion:** InterChart reveals significant limitations in current VLMs for multi-visual reasoning and serves as a framework for future advancements in multimodal reasoning.

**Abstract:** We introduce InterChart, a diagnostic benchmark that evaluates how well vision-language models (VLMs) reason across multiple related charts, a task central to real-world applications such as scientific reporting, financial analysis, and public policy dashboards. Unlike prior benchmarks focusing on isolated, visually uniform charts, InterChart challenges models with diverse question types ranging from entity inference and trend correlation to numerical estimation and abstract multi-step reasoning grounded in 2-3 thematically or structurally related charts. We organize the benchmark into three tiers of increasing difficulty: (1) factual reasoning over individual charts, (2) integrative analysis across synthetically aligned chart sets, and (3) semantic inference over visually complex, real-world chart pairs. Our evaluation of state-of-the-art open and closed-source VLMs reveals consistent and steep accuracy declines as chart complexity increases. We find that models perform better when we decompose multi-entity charts into simpler visual units, underscoring their struggles with cross-chart integration. By exposing these systematic limitations, InterChart provides a rigorous framework for advancing multimodal reasoning in complex, multi-visual environments.

</details>


### [120] [LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval](https://arxiv.org/abs/2508.07690)

*Luyao Zhuang, Qinggang Zhang, Huachi Zhou, Juhua Liu, Qing Li, Xiao Huang*

**Main category:** cs.CL

**Keywords:** tool retrieval, inductive learning, large language models, logic-guided learning, semantic bridging

**Relevance Score:** 8

**TL;DR:** The paper introduces the Logic-Guided Semantic Bridging (LoSemB) framework for inductive tool retrieval in large language models, addressing limitations of existing methods that assume all tools are known during training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges posed by an evolving tool repository and the limitations of existing retrieval methods when encountering unseen tools.

**Method:** The LoSemB framework utilizes a logic-based embedding alignment module and implements a relational augmented retrieval mechanism to facilitate inductive tool retrieval without requiring retraining.

**Key Contributions:**

	1. Introduction of LoSemB framework for inductive tool retrieval
	2. Development of a logic-based embedding alignment module
	3. Implementation of a relational augmented retrieval mechanism

**Result:** LoSemB outperforms existing state-of-the-art methods in inductive settings and retains effective performance in transductive settings.

**Limitations:** The framework may still face challenges with tools that are significantly different from those previously encountered.

**Conclusion:** LoSemB offers a promising solution for improving tool retrieval capabilities in large language models, particularly for unseen tools, by leveraging logical information from prior experiences.

**Abstract:** Tool learning has emerged as a promising paradigm for large language models (LLMs) to solve many real-world tasks. Nonetheless, with the tool repository rapidly expanding, it is impractical to contain all tools within the limited input length of LLMs. To alleviate these issues, researchers have explored incorporating a tool retrieval module to select the most relevant tools or represent tools as unique tokens within LLM parameters. However, most state-of-the-art methods are under transductive settings, assuming all tools have been observed during training. Such a setting deviates from reality as the real-world tool repository is evolving and incorporates new tools frequently. When dealing with these unseen tools, which refer to tools not encountered during the training phase, these methods are limited by two key issues, including the large distribution shift and the vulnerability of similarity-based retrieval. To this end, inspired by human cognitive processes of mastering unseen tools through discovering and applying the logical information from prior experience, we introduce a novel Logic-Guided Semantic Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to mine and transfer latent logical information for inductive tool retrieval without costly retraining. Specifically, LoSemB contains a logic-based embedding alignment module to mitigate distribution shifts and implements a relational augmented retrieval mechanism to reduce the vulnerability of similarity-based retrieval. Extensive experiments demonstrate that LoSemB achieves advanced performance in inductive settings while maintaining desirable effectiveness in the transductive setting.

</details>


### [121] [What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction](https://arxiv.org/abs/2508.07702)

*Charlie Wyatt, Aditya Joshi, Flora Salim*

**Main category:** cs.CL

**Keywords:** Long-Range Coherence, Masked Sentence Prediction, Large Language Models, Natural Language Processing, Next Token Prediction

**Relevance Score:** 9

**TL;DR:** This paper evaluates commercial LLMs on their ability to predict masked sentences, highlighting their limitations in maintaining long-range coherence and global context in various domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate LLMs' capacities for long-range coherence and sentence-level prediction, addressing the limitations of Next Token Prediction.

**Method:** The study assesses three LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on the Masked Sentence Prediction task across three domains: ROCStories, Recipe1M, and Wikipedia.

**Key Contributions:**

	1. Evaluation of commercial LLMs on Masked Sentence Prediction task
	2. Identification of models' limitations in long-range coherence
	3. Insights into the gap between local fluency and global context maintenance.

**Result:** The evaluation reveals that while these LLMs excel in many tasks, they struggle with masked sentence prediction in low-structured domains, indicating a significant gap in their capabilities.

**Limitations:** Focused evaluation on commercial LLMs limited to specific tasks and domains, which may not fully represent broader model capabilities.

**Conclusion:** The findings suggest that the reliance on Next Token Prediction inhibits models from achieving global coherence across sentence boundaries, which is crucial for certain tasks.

**Abstract:** Transformer-based models primarily rely on Next Token Prediction (NTP), which predicts the next token in a sequence based on the preceding context. However, NTP's focus on single-token prediction often limits a model's ability to plan ahead or maintain long-range coherence, raising questions about how well LLMs can predict longer contexts, such as full sentences within structured documents. While NTP encourages local fluency, it provides no explicit incentive to ensure global coherence across sentence boundaries-an essential skill for reconstructive or discursive tasks. To investigate this, we evaluate three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on Masked Sentence Prediction (MSP) - the task of infilling a randomly removed sentence - from three domains: ROCStories (narrative), Recipe1M (procedural), and Wikipedia (expository). We assess both fidelity (similarity to the original sentence) and cohesiveness (fit within the surrounding context). Our key finding reveals that commercial LLMs, despite their superlative performance in other tasks, are poor at predicting masked sentences in low-structured domains, highlighting a gap in current model capabilities.

</details>


### [122] [Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models](https://arxiv.org/abs/2508.07753)

*Zhenliang Zhang, Junzhe Zhang, Xinyu Hu, HuiXuan Zhang, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** large language models, faithfulness hallucinations, social bias, structural causal models, bias intervention

**Relevance Score:** 9

**TL;DR:** This study explores the relationship between social bias and faithfulness hallucinations in large language models, utilizing structural causal models to validate causality and design interventions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the unexplored causal relationship between social bias and faithfulness hallucinations in large language models (LLMs).

**Method:** Utilized Structural Causal Model (SCM) to analyze causality and design bias interventions, while developing the Bias Intervention Dataset (BID) for measuring causal effects.

**Key Contributions:**

	1. Establishment of a causal relationship between social bias and faithfulness hallucinations in LLMs.
	2. Development of the Bias Intervention Dataset (BID) for studying social biases.
	3. Identification of varying effects of different social biases on hallucination generation.

**Result:** Experiments on mainstream LLMs demonstrated that social biases significantly contribute to faithfulness hallucinations, with varying effects based on the type of bias.

**Limitations:** The primary limitation is the complexity of controlling all confounders within the context.

**Conclusion:** The study highlights the causal impact of social bias on hallucinations in LLMs, emphasizing the need for bias interventions to mitigate these effects.

**Abstract:** Large language models (LLMs) have achieved remarkable success in various tasks, yet they remain vulnerable to faithfulness hallucinations, where the output does not align with the input. In this study, we investigate whether social bias contributes to these hallucinations, a causal relationship that has not been explored. A key challenge is controlling confounders within the context, which complicates the isolation of causality between bias states and hallucinations. To address this, we utilize the Structural Causal Model (SCM) to establish and validate the causality and design bias interventions to control confounders. In addition, we develop the Bias Intervention Dataset (BID), which includes various social biases, enabling precise measurement of causal effects. Experiments on mainstream LLMs reveal that biases are significant causes of faithfulness hallucinations, and the effect of each bias state differs in direction. We further analyze the scope of these causal effects across various models, specifically focusing on unfairness hallucinations, which are primarily targeted by social bias, revealing the subtle yet significant causal effect of bias on hallucination generation.

</details>


### [123] [SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation](https://arxiv.org/abs/2508.07781)

*Zeyu Yang, Lai Wei, Roman Koshkin, Xi Chen, Satoshi Nakamura*

**Main category:** cs.CL

**Keywords:** speech translation, chunking strategy, syntax-aware translation, multilingual corpus, dependency parsing

**Relevance Score:** 8

**TL;DR:** Proposes a grammar-based chunking strategy for semantic unit segmentation in speech translation, introducing SASST, an end-to-end framework for improved translation quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of speech translation by segmenting input into semantically coherent units using dependency relations and punctuation.

**Method:** The study introduces a grammar-based chunking strategy that parses dependency relations and utilizes punctuation features. It presents SASST, which integrates a Whisper encoder with a decoder-only LLM to dynamically output translation tokens or <WAIT> symbols for simultaneous translation optimization.

**Key Contributions:**

	1. A new grammar-based chunking strategy for speech translation.
	2. Introduction of the SASST framework for simultaneous speech translation.
	3. Demonstrated improvements in translation quality using syntactic structures.

**Result:** Significant translation quality improvements were demonstrated on the CoVoST2 multilingual corpus across English and multiple target languages (German, Chinese, Japanese).

**Limitations:** 

**Conclusion:** The results validate that leveraging syntactic structures in LLM-driven simultaneous speech translation can enhance translation quality.

**Abstract:** This work proposes a grammar-based chunking strategy that segments input streams into semantically complete units by parsing dependency relations (e.g., noun phrase boundaries, verb-object structures) and punctuation features. The method ensures chunk coherence and minimizes semantic fragmentation. Building on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech Translation), an end-to-end framework integrating frozen Whisper encoder and decoder-only LLM. The unified architecture dynamically outputs translation tokens or <WAIT> symbols to jointly optimize translation timing and content, with target-side reordering addressing word-order divergence. Experiments on CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation quality improvements across languages and validate the effectiveness of syntactic structures in LLM-driven SimulST systems.

</details>


### [124] [Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts](https://arxiv.org/abs/2508.07785)

*Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li*

**Main category:** cs.CL

**Keywords:** Mixture of Experts, large language models, dynamic activation, computational efficiency, HCI

**Relevance Score:** 8

**TL;DR:** Grove MoE introduces a novel architecture for LLMs using heterogeneous experts with dynamic activation, improving computational efficiency and scalability over traditional models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional Mixture of Experts models with uniform-sized experts that hinder computational efficiency and scalability.

**Method:** The Grove MoE architecture incorporates experts of varying sizes and a dynamic activation mechanism, inspired by big.LITTLE CPU architecture, enabling model capacity expansion with controllable computational overhead.

**Key Contributions:**

	1. Introduction of Grove MoE with heterogeneous expert sizes
	2. Dynamic parameter activation based on input complexity
	3. Development of GroveMoE-Base and GroveMoE-Inst models with improved efficiency

**Result:** GroveMoE models dynamically activate between 3.14-3.28B parameters based on token complexity, achieving performance on par with or better than state-of-the-art models of similar or larger sizes.

**Limitations:** 

**Conclusion:** Grove MoE demonstrates an effective approach to enhancing LLM efficiency and scalability while maintaining performance through dynamic expert activation.

**Abstract:** The Mixture of Experts (MoE) architecture is a cornerstone of modern state-of-the-art (SOTA) large language models (LLMs). MoE models facilitate scalability by enabling sparse parameter activation. However, traditional MoE architecture uses homogeneous experts of a uniform size, activating a fixed number of parameters irrespective of input complexity and thus limiting computational efficiency. To overcome this limitation, we introduce Grove MoE, a novel architecture incorporating experts of varying sizes, inspired by the heterogeneous big.LITTLE CPU architecture. This architecture features novel adjugate experts with a dynamic activation mechanism, enabling model capacity expansion while maintaining manageable computational overhead. Building on this architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model during mid-training and post-training. GroveMoE models dynamically activate 3.14-3.28B parameters based on token complexity and achieve performance comparable to SOTA open-source models of similar or even larger size.

</details>


### [125] [Can You Trick the Grader? Adversarial Persuasion of LLM Judges](https://arxiv.org/abs/2508.07805)

*Yerin Hwang, Dongryeol Lee, Taegwan Kang, Yongil Kim, Kyomin Jung*

**Main category:** cs.CL

**Keywords:** persuasive language, large language models, bias, mathematical reasoning, evaluators

**Relevance Score:** 8

**TL;DR:** This study investigates how persuasive language can influence large language model judges to assign higher scores to incorrect mathematical reasoning solutions.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the vulnerability of large language models (LLMs) as evaluators and explore the extent to which strategic language can bias their scoring.

**Method:** The study formalizes seven persuasion techniques based on Aristotle's rhetorical principles and embeds these into identical responses evaluated by LLMs across six math benchmarks.

**Key Contributions:**

	1. First study to show LLMs can be biased by persuasive language
	2. Identification of seven specific persuasion techniques
	3. Demonstration of the persuasiveness of language across multiple evaluation contexts

**Result:** Persuasive language resulted in inflated scores for incorrect solutions, with an average increase of up to 8%. The technique of Consistency caused the most significant distortion in scores.

**Limitations:** Increased model size does not significantly reduce susceptibility to bias; counter-prompts are also ineffective in mitigating persuasion effects.

**Conclusion:** The findings expose a critical vulnerability in LLM-as-a-Judge systems, necessitating the development of robust defenses against manipulation through persuasive language.

**Abstract:** As large language models take on growing roles as automated evaluators in practical settings, a critical question arises: Can individuals persuade an LLM judge to assign unfairly high scores? This study is the first to reveal that strategically embedded persuasive language can bias LLM judges when scoring mathematical reasoning tasks, where correctness should be independent of stylistic variation. Grounded in Aristotle's rhetorical principles, we formalize seven persuasion techniques (Majority, Consistency, Flattery, Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical responses. Across six math benchmarks, we find that persuasive language leads LLM judges to assign inflated scores to incorrect solutions, by up to 8% on average, with Consistency causing the most severe distortion. Notably, increasing model size does not substantially mitigate this vulnerability. Further analysis demonstrates that combining multiple persuasion techniques amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover, the persuasive effect persists under counter prompting strategies, highlighting a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need for robust defenses against persuasion-based attacks.

</details>


### [126] [Evaluating Compositional Approaches for Focus and Sentiment Analysis](https://arxiv.org/abs/2508.07810)

*Olga Kellert, Muhammad Imran, Nicholas Hill Matlis, Mahmud Uz Zaman, Carlos G√≥mez-Rodr√≠guez*

**Main category:** cs.CL

**Keywords:** Focus Analysis, Sentiment Analysis, Natural Language Processing, Compositional Approach, Universal Dependencies

**Relevance Score:** 5

**TL;DR:** This paper evaluates a compositional approach for Focus Analysis in Linguistics and Sentiment Analysis in NLP, emphasizing the benefits of compositional methods by comparing them with non-compositional approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of quantitative evaluations for Focus Analysis in Linguistics and compare them with existing methods in Sentiment Analysis.

**Method:** The paper utilizes a compositional approach applying Universal Dependencies to perform Sentiment Analysis, focusing on syntactic rules like modification, coordination, and negation.

**Key Contributions:**

	1. Introduction of a compositional approach for Focus Analysis in Linguistics
	2. Comparison of compositional and non-compositional methods in Sentiment Analysis
	3. Demonstration of improved interpretability and accuracy with compositional techniques

**Result:** The compositional method shows improved interpretability and explainability compared to VADER, a non-compositional method, with appropriate datasets enhancing the accuracy evaluation.

**Limitations:** 

**Conclusion:** The findings suggest that methods used in Sentiment Analysis are applicable to Focus Analysis, underscoring the relationship between these fields.

**Abstract:** This paper summarizes the results of evaluating a compositional approach for Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural Language Processing (NLP). While quantitative evaluations of compositional and non-compositional approaches in SA exist in NLP, similar quantitative evaluations are very rare in FA in Linguistics that deal with linguistic expressions representing focus or emphasis such as "it was John who left". We fill this gap in research by arguing that compositional rules in SA also apply to FA because FA and SA are closely related meaning that SA is part of FA. Our compositional approach in SA exploits basic syntactic rules such as rules of modification, coordination, and negation represented in the formalism of Universal Dependencies (UDs) in English and applied to words representing sentiments from sentiment dictionaries. Some of the advantages of our compositional analysis method for SA in contrast to non-compositional analysis methods are interpretability and explainability. We test the accuracy of our compositional approach and compare it with a non-compositional approach VADER that uses simple heuristic rules to deal with negation, coordination and modification. In contrast to previous related work that evaluates compositionality in SA on long reviews, this study uses more appropriate datasets to evaluate compositionality. In addition, we generalize the results of compositional approaches in SA to compositional approaches in FA.

</details>


### [127] [Evaluating Large Language Models as Expert Annotators](https://arxiv.org/abs/2508.07827)

*Yu-Min Tseng, Wei-Lin Chen, Chung-Chi Chen, Hsin-Hsi Chen*

**Main category:** cs.CL

**Keywords:** large language models, data annotation, multi-agent discussions, reasoning models, specialized domains

**Relevance Score:** 9

**TL;DR:** This paper evaluates the effectiveness of large language models (LLMs) as alternatives to human expert annotators for textual data annotation in specialized domains like finance, biomedicine, and law.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate if top-performing LLMs can serve as substitutes for human expert annotators in specialized domains, addressing the high cost and labor demands of text annotation.

**Method:** The study evaluates individual LLMs and a multi-agent discussion framework where LLMs engage in discussions to finalize annotations and incorporates reasoning models for comparison.

**Key Contributions:**

	1. Evaluation of LLMs in three specialized domains
	2. Introduction of a multi-agent discussion framework for LLMs
	3. Insights into LLM performance with reasoning models in annotation tasks

**Result:** The findings indicate that individual LLMs show marginal or negative performance gains compared to expectations, and reasoning models do not significantly outperform non-reasoning models in most cases.

**Limitations:** Limited performance of LLMs in expert-level annotation tasks and the lack of significant improvements from reasoning models.

**Conclusion:** The extended chain-of-thought reasoning provides limited benefits for data annotation in specialized fields, and LLMs exhibit specific behaviors in discussion settings that impact their annotation decisions.

**Abstract:** Textual data annotation, the process of labeling or tagging text with relevant information, is typically costly, time-consuming, and labor-intensive. While large language models (LLMs) have demonstrated their potential as direct alternatives to human annotators for general domains natural language processing (NLP) tasks, their effectiveness on annotation tasks in domains requiring expert knowledge remains underexplored. In this paper, we investigate: whether top-performing LLMs, which might be perceived as having expert-level proficiency in academic and professional benchmarks, can serve as direct alternatives to human expert annotators? To this end, we evaluate both individual LLMs and multi-agent approaches across three highly specialized domains: finance, biomedicine, and law. Specifically, we propose a multi-agent discussion framework to simulate a group of human annotators, where LLMs are tasked to engage in discussions by considering others' annotations and justifications before finalizing their labels. Additionally, we incorporate reasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our empirical results reveal that: (1) Individual LLMs equipped with inference-time techniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal or even negative performance gains, contrary to prior literature suggesting their broad effectiveness. (2) Overall, reasoning models do not demonstrate statistically significant improvements over non-reasoning models in most settings. This suggests that extended long CoT provides relatively limited benefits for data annotation in specialized domains. (3) Certain model behaviors emerge in the multi-agent discussion environment. For instance, Claude 3.7 Sonnet with thinking rarely changes its initial annotations, even when other agents provide correct annotations or valid reasoning.

</details>


### [128] [LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding](https://arxiv.org/abs/2508.07849)

*Amrita Singh, H. Suhan Karaca, Aditya Joshi, Hye-young Paik, Jiaojiao Jiang*

**Main category:** cs.CL

**Keywords:** legal NLP, contract classification, legal-specific LLMs, performance evaluation, contract understanding

**Relevance Score:** 8

**TL;DR:** This paper evaluates 10 legal-specific LLMs on contract classification tasks and finds they outperform general-purpose models, with Legal-BERT and Contracts-BERT achieving new state-of-the-art results.

**Read time:** 4 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of comprehensive evaluations of legal-specific LLMs for contract classification, leading to a need for this study to fill that gap.

**Method:** The paper conducts evaluations on three contract understanding tasks using 10 legal-specific LLMs and compares their performance with 7 general-purpose LLMs.

**Key Contributions:**

	1. Evaluation of 10 legal-specific LLMs for contract tasks
	2. Identification of new SOTAs for contract understanding using legal-specific models
	3. Comparison with general-purpose LLMs to highlight performance differences

**Result:** Legal-specific LLMs consistently outperform general-purpose models, particularly on nuanced legal tasks; Legal-BERT and Contracts-BERT achieve new SOTA results despite fewer parameters.

**Limitations:** 

**Conclusion:** The findings offer a thorough assessment of legal-specific LLMs, which can enhance the development of accurate contract understanding systems.

**Abstract:** Despite advances in legal NLP, no comprehensive evaluation covering multiple legal-specific LLMs currently exists for contract classification tasks in contract understanding. To address this gap, we present an evaluation of 10 legal-specific LLMs on three English language contract understanding tasks and compare them with 7 general-purpose LLMs. The results show that legal-specific LLMs consistently outperform general-purpose models, especially on tasks requiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish new SOTAs on two of the three tasks, despite having 69% fewer parameters than the best-performing general-purpose LLM. We also identify CaseLaw-BERT and LexLM as strong additional baselines for contract understanding. Our results provide a holistic evaluation of legal-specific LLMs and will facilitate the development of more accurate contract understanding systems.

</details>


### [129] [Large Language Models for Czech Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2508.07860)

*Jakub ≈†m√≠d, Pavel P≈ôib√°≈à, Pavel Kr√°l*

**Main category:** cs.CL

**Keywords:** aspect-based sentiment analysis, large language models, Czech, natural language processing, fine-tuning

**Relevance Score:** 4

**TL;DR:** This study evaluates 19 LLMs for Czech aspect-based sentiment analysis, revealing that domain-specific models perform better in non-fine-tuned scenarios, with fine-tuned LLMs achieving state-of-the-art results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of large language models for Czech aspect-based sentiment analysis, an area that has been largely overlooked.

**Method:** A comprehensive evaluation of 19 LLMs with varying sizes and architectures was conducted, comparing performance in zero-shot, few-shot, and fine-tuning scenarios.

**Key Contributions:**

	1. Evaluation of 19 LLMs for Czech ABSA
	2. Insights on multilingualism and model size impact
	3. Error analysis on aspect term prediction challenges

**Result:** Findings indicate that small, domain-specific models fine-tuned for ABSA outperform general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs produce state-of-the-art results.

**Limitations:** The study focuses solely on Czech ABSA and may not generalize to other languages or contexts.

**Conclusion:** The study highlights factors influencing LLM performance in Czech ABSA and provides error analysis focusing on aspect term prediction, offering insights and guidance for future research.

**Abstract:** Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that aims to identify sentiment toward specific aspects of an entity. While large language models (LLMs) have shown strong performance in various natural language processing (NLP) tasks, their capabilities for Czech ABSA remain largely unexplored. In this work, we conduct a comprehensive evaluation of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show that small domain-specific models fine-tuned for ABSA outperform general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs achieve state-of-the-art results. We analyze how factors such as multilingualism, model size, and recency influence performance and present an error analysis highlighting key challenges, particularly in aspect term prediction. Our findings provide insights into the suitability of LLMs for Czech ABSA and offer guidance for future research in this area.

</details>


### [130] [Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models](https://arxiv.org/abs/2508.07866)

*Jakub ≈†m√≠d, Pavel P≈ôib√°≈à, Pavel Kr√°l*

**Main category:** cs.CL

**Keywords:** Aspect-based sentiment analysis, Cross-lingual, Few-shot learning, Low-resource languages, Machine learning

**Relevance Score:** 4

**TL;DR:** This paper explores the benefits of incorporating few-shot training examples from target languages in aspect-based sentiment analysis (ABSA) for low-resource languages, showing significant performance improvements in cross-lingual settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the challenges faced in aspect-based sentiment analysis for low-resource languages, particularly the scarcity of labelled data and the limitations of current cross-lingual methods.

**Method:** The authors evaluate the impact of adding a small number of target language examples on four ABSA tasks across six target languages using two sequence-to-sequence models.

**Key Contributions:**

	1. Demonstrates the importance of few-shot samples in improving ABSA for low-resource languages
	2. Shows combination with English data can outperform monolingual models
	3. Offers practical insights for domain-specific applications in sentiment analysis

**Result:** The study finds that adding as few as ten target language examples significantly improves performance compared to zero-shot settings, and combining with English data can exceed monolingual baselines.

**Limitations:** 

**Conclusion:** These findings suggest that incorporating a small number of high-quality annotated examples can effectively enhance cross-lingual ABSA, making it a practical approach in low-resource contexts.

**Abstract:** Aspect-based sentiment analysis (ABSA) has received substantial attention in English, yet challenges remain for low-resource languages due to the scarcity of labelled data. Current cross-lingual ABSA approaches often rely on external translation tools and overlook the potential benefits of incorporating a small number of target language examples into training. In this paper, we evaluate the effect of adding few-shot target language examples to the training set across four ABSA tasks, six target languages, and two sequence-to-sequence models. We show that adding as few as ten target language examples significantly improves performance over zero-shot settings and achieves a similar effect to constrained decoding in reducing prediction errors. Furthermore, we demonstrate that combining 1,000 target language examples with English data can even surpass monolingual baselines. These findings offer practical insights for improving cross-lingual ABSA in low-resource and domain-specific settings, as obtaining ten high-quality annotated examples is both feasible and highly effective.

</details>


### [131] [Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity](https://arxiv.org/abs/2508.07902)

*Chen Cecilia Liu, Hiba Arnaout, Nils Kovaƒçiƒá, Dana Atzil-Slonim, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** Large language models, Cultural sensitivity, Emotional support, AI in healthcare

**Relevance Score:** 9

**TL;DR:** This paper introduces CultureCare, a dataset for culturally sensitive emotional support using LLMs, and evaluates adaptation strategies for improved responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the ability of large language models to provide culturally sensitive emotional support, an area that has been underexplored.

**Method:** The authors introduce the CultureCare dataset, which includes distress messages, cultural signals, and support strategies. They then develop and test adaptation strategies for LLMs and conduct evaluations with various judges.

**Key Contributions:**

	1. Development of the CultureCare dataset for culturally sensitive emotional support
	2. Evaluation of adaptation strategies for LLMs
	3. Evidence showing LLMs can exceed peer responses in cultural sensitivity

**Result:** Adapted LLMs significantly outperformed responses from anonymous online peers, demonstrating the necessity for careful adaptation rather than simple cultural role-play.

**Limitations:** The study is limited by its reliance on the specific cultures represented in the dataset and the subjective nature of cultural sensitivity evaluations.

**Conclusion:** The research highlights the potential of LLMs to enhance cultural competence in clinical training, indicating a significant step toward culturally sensitive AI applications in emotional support.

**Abstract:** Large language models (LLMs) show promise in offering emotional support and generating empathetic responses for individuals in distress, but their ability to deliver culturally sensitive support remains underexplored due to lack of resources. In this work, we introduce CultureCare, the first dataset designed for this task, spanning four cultures and including 1729 distress messages, 1523 cultural signals, and 1041 support strategies with fine-grained emotional and cultural annotations. Leveraging CultureCare, we (i) develop and test four adaptation strategies for guiding three state-of-the-art LLMs toward culturally sensitive responses; (ii) conduct comprehensive evaluations using LLM judges, in-culture human annotators, and clinical psychologists; (iii) show that adapted LLMs outperform anonymous online peer responses, and that simple cultural role-play is insufficient for cultural sensitivity; and (iv) explore the application of LLMs in clinical training, where experts highlight their potential in fostering cultural competence in future therapists.

</details>


### [132] [Challenges and opportunities in portraying emotion in generated sign language](https://arxiv.org/abs/2508.07937)

*John C. McDonald, Rosalee Wolfe, Fabrizio Nunnari*

**Main category:** cs.CL

**Keywords:** sign language, emotional expressions, signing avatars, non-manual signals, EASIER notation

**Relevance Score:** 4

**TL;DR:** The paper presents a new method for representing emotional non-manual signals in sign language avatars, improving the specification of emotional expressions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of a standard method for incorporating emotional content into signing avatars, which hampers their effectiveness.

**Method:** The paper introduces a two-parameter representation for emotive non-manual signals that can be applied to the Paula signing avatar.

**Key Contributions:**

	1. Introduction of EASIER notation for emotional expression in avatars
	2. Two-parameter representation enhances emotional nuance
	3. Potential for consistent linguistic annotations for signing avatars

**Result:** The new representation enables users to control Paula's emotional expressions more intuitively through a textual notation, allowing for nuanced emotional states.

**Limitations:** 

**Conclusion:** This approach facilitates a more coherent specification of emotional expressions in signing avatars, improving their linguistic capability.

**Abstract:** Non-manual signals in sign languages continue to be a challenge for signing avatars. More specifically, emotional content has been difficult to incorporate because of a lack of a standard method of specifying the avatar's emotional state. This paper explores the application of an intuitive two-parameter representation for emotive non-manual signals to the Paula signing avatar that shows promise for facilitating the linguistic specification of emotional facial expressions in a more coherent manner than previous methods. Users can apply these parameters to control Paula's emotional expressions through a textual representation called the EASIER notation. The representation can allow avatars to express more nuanced emotional states using two numerical parameters. It also has the potential to enable more consistent specification of emotional non-manual signals in linguistic annotations which drive signing avatars.

</details>


### [133] [Expert Preference-based Evaluation of Automated Related Work Generation](https://arxiv.org/abs/2508.07955)

*Furkan ≈ûahinu√ß, Subhabrata Dutta, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** LLM, scientific writing, evaluation framework, human-AI collaboration, related work generation

**Relevance Score:** 9

**TL;DR:** The paper presents GREP, a multi-turn evaluation framework for assessing the quality of scientific writing, particularly in related work sections, integrating expert preferences with evaluation criteria.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for better evaluation methods in automatically generated scientific writing, as conventional metrics fail to capture expert preferences and domain-specific standards.

**Method:** GREP decomposes evaluation into fine-grained dimensions, utilizing classical criteria and contrastive few-shot examples to enhance the evaluation process.

**Key Contributions:**

	1. Introduction of a multi-turn evaluation framework for scientific writing
	2. Fine-grained assessment dimensions incorporating expert preferences
	3. Two variants of GREP designed for varying resource availability

**Result:** GREP demonstrates a robust ability to assess related work quality, showing a strong correlation with expert assessment and outperforming standard LLM judges.

**Limitations:** 

**Conclusion:** GREP's localized evaluation approach enables better post-training assessments and supports human-AI collaborative writing in scientific contexts.

**Abstract:** Expert domain writing, such as scientific writing, typically demands extensive domain knowledge. Recent advances in LLMs show promising potential in reducing the expert workload. However, evaluating the quality of automatically generated scientific writing is a crucial open issue, as it requires knowledge of domain-specific evaluation criteria and the ability to discern expert preferences. Conventional automatic metrics and LLM-as-a-judge systems are insufficient to grasp expert preferences and domain-specific quality standards. To address this gap and support human-AI collaborative writing, we focus on related work generation, one of the most challenging scientific tasks, as an exemplar. We propose GREP, a multi-turn evaluation framework that integrates classical related work evaluation criteria with expert-specific preferences. Instead of assigning a single score, our framework decomposes the evaluation into fine-grained dimensions. This localized evaluation approach is further augmented with contrastive few-shot examples to provide detailed contextual guidance for the evaluation dimensions. The design principles allow our framework to deliver cardinal assessment of quality, which can facilitate better post-training compared to ordinal preference data. For better accessibility, we design two variants of GREP: a more precise variant with proprietary LLMs as evaluators, and a cheaper alternative with open-weight LLMs. Empirical investigation reveals that our framework is able to assess the quality of related work sections in a much more robust manner compared to standard LLM judges, reflects natural scenarios of scientific writing, and bears a strong correlation with the human expert assessment. We also observe that generations from state-of-the-art LLMs struggle to satisfy validation constraints of a suitable related work section. They (mostly) fail to improve based on feedback as well.

</details>


### [134] [Large Language Models for Subjective Language Understanding: A Survey](https://arxiv.org/abs/2508.07959)

*Changhao Song, Yazhou Zhang, Hui Gao, Ben Yao, Peng Zhang*

**Main category:** cs.CL

**Keywords:** subjective language, large language models, sentiment analysis, affective computing, figurative language

**Relevance Score:** 9

**TL;DR:** This survey reviews the application of large language models (LLMs) to subjective language tasks such as sentiment analysis and irony detection, discussing methodologies, advancements, challenges, and future directions.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing necessity to understand and process subjective language in natural language processing, driven by the advancements in LLMs.

**Method:** The authors provide a comprehensive overview of subjective language tasks, detailing LLM architectures, techniques, and their applications across various tasks involving subjective interpretation.

**Key Contributions:**

	1. A comprehensive review of LLM applications in subjective language tasks
	2. Identification of remaining challenges and open issues
	3. Insights on multi-task LLM approaches for unified models of subjectivity.

**Result:** The survey highlights the unique challenges of subjective language and summarizes state-of-the-art methods, datasets, and comparative insights across eight tasks.

**Limitations:** The paper acknowledges limitations related to data availability, model bias, and ethical implications in subjective language processing.

**Conclusion:** The paper emphasizes the importance of addressing open issues like data limitations and model bias, and suggests future research directions in the field.

**Abstract:** Subjective language understanding refers to a broad set of natural language processing tasks where the goal is to interpret or generate content that conveys personal feelings, opinions, or figurative meanings rather than objective facts. With the advent of large language models (LLMs) such as ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach these inherently nuanced tasks. In this survey, we provide a comprehensive review of recent advances in applying LLMs to subjective language tasks, including sentiment analysis, emotion recognition, sarcasm detection, humor understanding, stance detection, metaphor interpretation, intent detection, and aesthetics assessment. We begin by clarifying the definition of subjective language from linguistic and cognitive perspectives, and we outline the unique challenges posed by subjective language (e.g. ambiguity, figurativeness, context dependence). We then survey the evolution of LLM architectures and techniques that particularly benefit subjectivity tasks, highlighting why LLMs are well-suited to model subtle human-like judgments. For each of the eight tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based methods, and remaining challenges. We provide comparative insights, discussing commonalities and differences among tasks and how multi-task LLM approaches might yield unified models of subjectivity. Finally, we identify open issues such as data limitations, model bias, and ethical considerations, and suggest future research directions. We hope this survey will serve as a valuable resource for researchers and practitioners interested in the intersection of affective computing, figurative language processing, and large-scale language models.

</details>


### [135] [Toward Machine Interpreting: Lessons from Human Interpreting Studies](https://arxiv.org/abs/2508.07964)

*Matthias Sperber, Maureen de Seyssel, Jiajun Bao, Matthias Paulik*

**Main category:** cs.CL

**Keywords:** Speech Translation, Human Interpreting, Machine Translation, Usability, Interpreting Principles

**Relevance Score:** 7

**TL;DR:** The paper discusses the need for speech translation systems to adapt more like human interpreters by applying principles from human interpreting literature and modern modeling techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the practical usefulness of speech translation systems by closing the usability gap with human interpreters.

**Method:** Analyzing human interpreting literature and its implications for machine translation development.

**Key Contributions:**

	1. Comparison of human interpreting and machine translation
	2. Insights into operational and qualitative aspects of interpreting
	3. Recommendations for integrating human principles into machine systems

**Result:** Identified opportunities to enhance speech translation systems by incorporating human interpreting principles and leveraging recent technologies.

**Limitations:** 

**Conclusion:** The findings may inspire advancements toward true machine interpreting capabilities.

**Abstract:** Current speech translation systems, while having achieved impressive accuracies, are rather static in their behavior and do not adapt to real-world situations in ways human interpreters do. In order to improve their practical usefulness and enable interpreting-like experiences, a precise understanding of the nature of human interpreting is crucial. To this end, we discuss human interpreting literature from the perspective of the machine translation field, while considering both operational and qualitative aspects. We identify implications for the development of speech translation systems and argue that there is great potential to adopt many human interpreting principles using recent modeling techniques. We hope that our findings provide inspiration for closing the perceived usability gap, and can motivate progress toward true machine interpreting.

</details>


### [136] [Understanding Syntactic Generalization in Structure-inducing Language Models](https://arxiv.org/abs/2508.07969)

*David Arps, Hassan Sajjad, Laura Kallmeyer*

**Main category:** cs.CL

**Keywords:** Structure-inducing Language Models, syntactic representations, grammaticality judgment

**Relevance Score:** 6

**TL;DR:** This paper evaluates three Structure-inducing Language Models (SiLMs) on their syntactic representation, grammaticality judgment performance, and training dynamics, finding the GPST model most consistent across metrics used.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the systematic gaps in the evaluation of different SiLMs and provide a comprehensive comparison across various metrics.

**Method:** The authors analyze Structformer, UDGN, and GPST architectures, utilizing both natural language corpora and synthetic bracketing expressions for evaluation across several criteria.

**Key Contributions:**

	1. Comparison of three different SiLM architectures
	2. Evaluation across natural and synthetic datasets
	3. Demonstration of model performance on grammaticality judgment tasks

**Result:** The study finds no single architecture dominates; however, the GPST model outperforms others in long-distance dependencies and shows consistent performance across evaluation settings.

**Limitations:** 

**Conclusion:** Small models trained on large synthetic datasets are effective for evaluating fundamental model properties; GPST is recommended for its robust performance.

**Abstract:** Structure-inducing Language Models (SiLM) are trained on a self-supervised language modeling task, and induce a hierarchical sentence representation as a byproduct when processing an input. A wide variety of SiLMs have been proposed. However, these have typically been evaluated on a relatively small scale, and evaluation of these models has systematic gaps and lacks comparability. In this work, we study three different SiLM architectures using both natural language (English) corpora and synthetic bracketing expressions: Structformer (Shen et al., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare them with respect to (i) properties of the induced syntactic representations (ii) performance on grammaticality judgment tasks, and (iii) training dynamics. We find that none of the three architectures dominates across all evaluation metrics. However, there are significant differences, in particular with respect to the induced syntactic representations. The Generative Pretrained Structured Transformer (GPST; Hu et al. 2024) performs most consistently across evaluation settings, and outperforms the other models on long-distance dependencies in bracketing expressions. Furthermore, our study shows that small models trained on large amounts of synthetic data provide a useful testbed for evaluating basic model properties.

</details>


### [137] [Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL](https://arxiv.org/abs/2508.07976)

*Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Search Intelligence, Open-source

**Relevance Score:** 9

**TL;DR:** ASearcher is an open-source project that enhances the training of large language model (LLM) search agents through scalable reinforcement learning (RL) methods, achieving significant performance improvements in search tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of existing open-source agents in Search Intelligence, particularly in scalability, efficiency, and data quality for handling complex queries.

**Method:** A scalable fully asynchronous RL training framework is proposed, allowing for long-horizon search strategies without restrictions on turn limits. A prompt-based LLM agent synthesizes high-quality QAs to create an extensive dataset for training.

**Key Contributions:**

	1. Scalable fully asynchronous RL training for long-horizon search
	2. Prompt-based LLM agent synthesizing high-quality QAs
	3. Open-sourcing of models, data, and implementation

**Result:** The prompt-based QwQ-32B agent shows significant advancements in search performance, with 46.7% and 20.8% Avg@4 improvements on xBench and GAIA. It successfully navigates over 40 turns with extensive tool usage during training.

**Limitations:** 

**Conclusion:** ASearcher-Web-QwQ surpasses previous models, achieving high Avg@4 scores of 42.1 and 52.8, respectively, and provides open access to models, training data, and code.

**Abstract:** Recent advancements in LLM-based agents have demonstrated remarkable capabilities in handling complex, knowledge-intensive tasks by integrating external tools. Among diverse choices of tools, search tools play a pivotal role in accessing vast external knowledge. However, open-source agents still fall short of achieving expert-level Search Intelligence, the ability to resolve ambiguous queries, generate precise searches, analyze results, and conduct thorough exploration. Existing approaches fall short in scalability, efficiency, and data quality. For example, small turn limits in existing online RL methods, e.g. <=10, restrict complex strategy learning. This paper introduces ASearcher, an open-source project for large-scale RL training of search agents. Our key contributions include: (1) Scalable fully asynchronous RL training that enables long-horizon search while maintaining high training efficiency. (2) A prompt-based LLM agent that autonomously synthesizes high-quality and challenging QAs, creating a large-scale QA dataset. Through RL training, our prompt-based QwQ-32B agent achieves substantial improvements, with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns and output tokens exceeding 150k during training time. With a simple agent design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We open-source our models, training data, and codes in https://github.com/inclusionAI/ASearcher.

</details>


### [138] [The Medical Metaphors Corpus (MCC)](https://arxiv.org/abs/2508.07993)

*Anna Sofia Lippolis, Andrea Giovanni Nuzzolese, Aldo Gangemi*

**Main category:** cs.CL

**Keywords:** metaphor detection, medical corpus, figurative language, language models, health communication

**Relevance Score:** 7

**TL;DR:** The paper introduces the Medical Metaphors Corpus (MCC), a dataset of 792 annotated scientific metaphors in medical and biological domains, addressing the gap in metaphor detection resources for domain-specific applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in metaphor detection resources focused on scientific discourse, particularly in the medical and biological fields.

**Method:** A comprehensive dataset (MCC) was created, aggregating metaphorical expressions from various sources with binary and graded judgments on metaphoricity validated through human annotation.

**Key Contributions:**

	1. Introduction of the Medical Metaphors Corpus (MCC) for scientific metaphor research.
	2. First annotated resource for computational scientific metaphor detection.
	3. Insights into the performance of language models in understanding domain-specific metaphors.

**Result:** Evaluation revealed modest performance of state-of-the-art language models in detecting scientific metaphors, indicating a need for improvement in understanding domain-specific figurative language.

**Limitations:** 

**Conclusion:** MCC serves as a novel resource for metaphor detection benchmarking, improving generation systems, and enhancing patient communication tools.

**Abstract:** Metaphor is a fundamental cognitive mechanism that shapes scientific understanding, enabling the communication of complex concepts while potentially constraining paradigmatic thinking. Despite the prevalence of figurative language in scientific discourse, existing metaphor detection resources primarily focus on general-domain text, leaving a critical gap for domain-specific applications. In this paper, we present the Medical Metaphors Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual metaphors spanning medical and biological domains. MCC aggregates metaphorical expressions from diverse sources including peer-reviewed literature, news media, social media discourse, and crowdsourced contributions, providing both binary and graded metaphoricity judgments validated through human annotation. Each instance includes source-target conceptual mappings and perceived metaphoricity scores on a 0-7 scale, establishing the first annotated resource for computational scientific metaphor research. Our evaluation demonstrates that state-of-the-art language models achieve modest performance on scientific metaphor detection, revealing substantial room for improvement in domain-specific figurative language understanding. MCC enables multiple research applications including metaphor detection benchmarking, quality-aware generation systems, and patient-centered communication tools.

</details>


### [139] [WideSearch: Benchmarking Agentic Broad Info-Seeking](https://arxiv.org/abs/2508.07999)

*Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, Ke Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, search agents, benchmark, information seeking, human validation

**Relevance Score:** 9

**TL;DR:** WideSearch is a new benchmark for evaluating LLM-powered search agents' reliability on large-scale information collection tasks, revealing critical deficiencies in current systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by search agents in performing wide-context information collection reliably and completely due to the lack of suitable benchmarks.

**Method:** Development of WideSearch, a benchmark featuring 200 manually curated questions from diverse domains, to evaluate agent performance on atomic information collection and organization.

**Key Contributions:**

	1. Introduction of the WideSearch benchmark
	2. Evaluation of over 10 state-of-the-art search systems
	3. Demonstration of deficiencies in current search agents' performance

**Result:** Benchmarking over 10 search systems revealed overall success rates near 0%, with the best system achieving only 5%, but human cross-validation can reach nearly 100%.

**Limitations:** Limited to the specific questions curated for the benchmark and performance analysis of existing search systems.

**Conclusion:** Current agentic search systems exhibit critical deficiencies, highlighting the need for further research and improved methodologies in large-scale information seeking.

**Abstract:** From professional research to everyday planning, many tasks are bottlenecked by wide-scale information seeking, which is more repetitive than cognitively complex. With the rapid development of Large Language Models (LLMs), automated search agents powered by LLMs offer a promising solution to liberate humans from this tedious work. However, the capability of these agents to perform such "wide-context" collection reliably and completely remains largely unevaluated due to a lack of suitable benchmarks. To bridge this gap, we introduce WideSearch, a new benchmark engineered to evaluate agent reliability on these large-scale collection tasks. The benchmark features 200 manually curated questions (100 in English, 100 in Chinese) from over 15 diverse domains, grounded in real user queries. Each task requires agents to collect large-scale atomic information, which could be verified one by one objectively, and arrange it into a well-organized output. A rigorous five-stage quality control pipeline ensures the difficulty, completeness, and verifiability of the dataset. We benchmark over 10 state-of-the-art agentic search systems, including single-agent, multi-agent frameworks, and end-to-end commercial systems. Most systems achieve overall success rates near 0\%, with the best performer reaching just 5\%. However, given sufficient time, cross-validation by multiple human testers can achieve a near 100\% success rate. These results demonstrate that present search agents have critical deficiencies in large-scale information seeking, underscoring urgent areas for future research and development in agentic search. Our dataset, evaluation pipeline, and benchmark results have been publicly released at https://widesearch-seed.github.io/

</details>


### [140] [Progressive Depth Up-scaling via Optimal Transport](https://arxiv.org/abs/2508.08011)

*Mingzi Cao, Xi Wang, Nikolaos Aletras*

**Main category:** cs.CL

**Keywords:** Large Language Models, Optimal Transport, Training Efficiency, Depth Up-Scaling, Transformer

**Relevance Score:** 8

**TL;DR:** The paper presents Optimal Transport Depth Up-Scaling (OpT-DeUS) to improve the training efficiency and performance of Large Language Models by addressing neuron permutation differences in layer scaling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance and training efficiency of Large Language Models through an innovative method that ensures neuron alignment during depth scaling.

**Method:** OpT-DeUS employs Optimal Transport to align and fuse Transformer blocks from adjacent base layers, creating new layers that mitigate neuron permutation mismatch.

**Key Contributions:**

	1. Introduction of Optimal Transport for neuron alignment during depth up-scaling.
	2. Demonstration of improved performance and efficiency in LLM training.
	3. Analysis of layer insertion positions and their impact on training efficiency.

**Result:** The proposed method demonstrates better overall performance and enhanced training efficiency compared to existing depth up-scaling techniques during continual pre-training and supervised fine-tuning across different model sizes.

**Limitations:** 

**Conclusion:** Inserting new layers closer to the top of the model yields higher training efficiency and additional performance gains, highlighting the importance of layer positioning.

**Abstract:** Scaling Large Language Models (LLMs) yields performance gains but incurs substantial training costs. Depth up-scaling offers training efficiency by adding new layers to pre-trained models. However, most existing methods copy or average weights from base layers, neglecting neuron permutation differences. This limitation can potentially cause misalignment that harms performance. Inspired by applying Optimal Transport (OT) for neuron alignment, we propose Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses Transformer blocks in adjacent base layers via OT for new layer creation, to mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better overall performance and offers improved training efficiency than existing methods for continual pre-training and supervised fine-tuning across different model sizes. To further evaluate the impact of interpolation positions, our extensive analysis shows that inserting new layers closer to the top results in higher training efficiency due to shorter back-propagation time while obtaining additional performance gains.

</details>


### [141] [9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)](https://arxiv.org/abs/2508.08050)

*Fabrizio Nunnari, Cristina Luna Jim√©nez, Rosalee Wolfe, John C. McDonald, Michael Filhol, Eleni Efthimiou, Evita Fotinea, Thomas Hanke*

**Main category:** cs.CL

**Keywords:** Sign Language Translation, Avatar Technology, Affective Computing, Human-Computer Interaction, Usability

**Relevance Score:** 4

**TL;DR:** The 2025 Sign Language Translation and Avatar Technology (SLTAT) workshops focus on advancements in facilitating deaf/human communication. Hosted at the IVA conference, it emphasizes research on digital humans as interpreters and conversational agents, alongside sign language recognition and various relevant fields.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve communication between deaf individuals and humans through non-invasive technologies and gather insights from multiple research communities.

**Method:** The workshops include contributions and research presentations related to sign language recognition, data collection and analysis, usability, ethics, and affective computing.

**Key Contributions:**

	1. Interdisciplinary collaboration between avatar technology and sign language research
	2. Focus on ethical considerations in technology use
	3. Advancements in sign language recognition and data analysis methods

**Result:** A variety of submissions showcasing advancements in both avatar technologies and sign language communication methods.

**Limitations:** 

**Conclusion:** The SLTAT workshops foster interdisciplinary collaboration and contribute to better tools and methods for assisting communication for the deaf community.

**Abstract:** The Sign Language Translation and Avatar Technology (SLTAT) workshops continue a series of gatherings to share recent advances in improving deaf / human communication through non-invasive means. This 2025 edition, the 9th since its first appearance in 2011, is hosted by the International Conference on Intelligent Virtual Agents (IVA), giving the opportunity for contamination between two research communities, using digital humans as either virtual interpreters or as interactive conversational agents. As presented in this summary paper, SLTAT sees contributions beyond avatar technologies, with a consistent number of submissions on sign language recognition, and other work on data collection, data analysis, tools, ethics, usability, and affective computing.

</details>


### [142] [Dual Information Speech Language Models for Emotional Conversations](https://arxiv.org/abs/2508.08095)

*Chun Wang, Chenyang Liu, Wenze Xu, Weihong Deng*

**Main category:** cs.CL

**Keywords:** speech-language models, paralinguistic cues, emotional conversation, language models, contextual understanding

**Relevance Score:** 9

**TL;DR:** Proposed a method using heterogeneous adapters to enhance speech-language models for better understanding of paralinguistic cues and context in emotional conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve emotional understanding and contextual interpretation in conversational systems utilizing speech as input, addressing limitations of existing speech-language models.

**Method:** Introduced two heterogeneous adapters and a weakly supervised training strategy to disentangle paralinguistic and linguistic information while maintaining efficient training using common datasets.

**Key Contributions:**

	1. Identification of the limitations of existing speech-language models
	2. Proposed heterogeneous adapters for better information disentanglement
	3. Weakly supervised training strategy that preserves context

**Result:** Experiments indicate that the proposed approach demonstrates competitive performance in emotional conversation tasks, effectively integrating paralinguistic and linguistic information.

**Limitations:** 

**Conclusion:** The approach shows promise for improving speech-language models' performance in emotional contexts, highlighting the importance of paralinguistic cues in conversation.

**Abstract:** Conversational systems relying on text-based large language models (LLMs) often overlook paralinguistic cues, essential for understanding emotions and intentions. Speech-language models (SLMs), which use speech as input, are emerging as a promising solution. However, SLMs built by extending frozen LLMs struggle to capture paralinguistic information and exhibit reduced context understanding. We identify entangled information and improper training strategies as key issues. To address these issues, we propose two heterogeneous adapters and suggest a weakly supervised training strategy. Our approach disentangles paralinguistic and linguistic information, enabling SLMs to interpret speech through structured representations. It also preserves contextual understanding by avoiding the generation of task-specific vectors through controlled randomness. This approach trains only the adapters on common datasets, ensuring parameter and data efficiency. Experiments demonstrate competitive performance in emotional conversation tasks, showcasing the model's ability to effectively integrate both paralinguistic and linguistic information within contextual settings.

</details>


### [143] [Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?](https://arxiv.org/abs/2508.08096)

*Lukas Gehring, Benjamin Paa√üen*

**Main category:** cs.CL

**Keywords:** Large Language Models, LLM detection, academic integrity, education, learning analytics

**Relevance Score:** 9

**TL;DR:** The paper benchmarks the performance of state-of-the-art detectors for identifying LLM-generated texts in educational contexts, presenting a novel dataset and discussing issues related to false positives in detection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the accessibility of LLMs leading to increased text generation by students, there is a pressing need for methods to uphold academic integrity through the detection of LLM-generated content.

**Method:** The paper introduces the Generative Essay Detection in Education (GEDE) dataset, which includes student-written essays and LLM-generated texts, and evaluates the accuracy of various text detectors on these documents, focusing on different contribution levels of text generation.

**Key Contributions:**

	1. Introduction of the GEDE dataset with diverse essay types
	2. Benchmarking state-of-the-art detectors in educational settings
	3. Identification of contribution levels in text generation

**Result:** The study finds that most detectors struggle with texts that are generated with intermediate contribution levels, particularly showing a tendency to produce false positives, which may adversely affect students.

**Limitations:** Detectors primarily falter with intermediate contribution levels and often result in false positives, which can be detrimental in educational environments.

**Conclusion:** The results highlight the limitations of current text detection methods in accurately classifying nuanced student contributions and underscore the significance of supporting detection systems that minimize false positives in educational settings.

**Abstract:** Recent advancements in Large Language Models (LLMs) and their increased accessibility have made it easier than ever for students to automatically generate texts, posing new challenges for educational institutions. To enforce norms of academic integrity and ensure students' learning, learning analytics methods to automatically detect LLM-generated text appear increasingly appealing. This paper benchmarks the performance of different state-of-the-art detectors in educational contexts, introducing a novel dataset, called Generative Essay Detection in Education (GEDE), containing over 900 student-written essays and over 12,500 LLM-generated essays from various domains. To capture the diversity of LLM usage practices in generating text, we propose the concept of contribution levels, representing students' contribution to a given assignment. These levels range from purely human-written texts, to slightly LLM-improved versions, to fully LLM-generated texts, and finally to active attacks on the detector by "humanizing" generated texts. We show that most detectors struggle to accurately classify texts of intermediate student contribution levels, like LLM-improved human-written texts. Detectors are particularly likely to produce false positives, which is problematic in educational settings where false suspicions can severely impact students' lives. Our dataset, code, and additional supplementary materials are publicly available at https://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.

</details>


### [144] [Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0](https://arxiv.org/abs/2508.08110)

*Robin Huo, Ewan Dunbar*

**Main category:** cs.CL

**Keywords:** self-supervised learning, speech representations, HuBERT, wav2vec 2.0, linguistic information

**Relevance Score:** 4

**TL;DR:** This study investigates the impact of model architecture on linguistic information learned in self-supervised speech representation models HuBERT and wav2vec 2.0.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how architectural differences in self-supervised models affect the encoding of linguistic information in speech representations.

**Method:** The study compares the effects of training objective and iterative pseudo-label refinement on HuBERT and wav2vec 2.0, focusing on their hidden representations.

**Key Contributions:**

	1. Comparison of HuBERT and wav2vec 2.0
	2. Identification of iterative refinement impact on linguistic information
	3. Recommendations for future research into architectural optimizations

**Result:** Differences in canonical correlation of hidden representations to word, phoneme, and speaker identity are attributed to the number of training iterations, rather than the training objective.

**Limitations:** 

**Conclusion:** Iterative refinement is effective in enhancing linguistic information encoding in self-supervised speech representations, warranting further investigation.

**Abstract:** Self-supervised models for speech representation learning now see widespread use for their versatility and performance on downstream tasks, but the effect of model architecture on the linguistic information learned in their representations remains under-studied. This study investigates two such models, HuBERT and wav2vec 2.0, and minimally compares two of their architectural differences: training objective and iterative pseudo-label refinement through multiple training iterations. We find that differences in canonical correlation of hidden representations to word identity, phoneme identity, and speaker identity are explained by training iteration, not training objective. We suggest that future work investigate the reason for the effectiveness of iterative refinement in encoding linguistic information in self-supervised speech representations.

</details>


### [145] [Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks](https://arxiv.org/abs/2508.08125)

*Jakub ≈†m√≠d, Pavel P≈ôib√°≈à, Ond≈ôej Pra≈æ√°k, Pavel Kr√°l*

**Main category:** cs.CL

**Keywords:** Aspect-based sentiment analysis, Czech dataset, Transformer models

**Relevance Score:** 4

**TL;DR:** Introduction of a novel Czech dataset for aspect-based sentiment analysis with 3.1K annotated reviews from restaurants, aimed at advanced sentiment analysis tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a more complex dataset for aspect-based sentiment analysis in Czech, enabling better cross-lingual comparisons.

**Method:** Creation of a dataset with unified annotation for more complex tasks, involving two trained annotators achieving a 90% inter-annotator agreement rate.

**Key Contributions:**

	1. Introduction of a complex dataset for ABSA tasks in Czech language.
	2. High inter-annotator agreement of 90% showcasing annotation reliability.
	3. Provision of 24M reviews for unsupervised learning.

**Result:** Robust monolingual baseline results using various Transformer-based models and provision of 24M unannotated reviews for unsupervised learning.

**Limitations:** 

**Conclusion:** The dataset and associated code are freely available for research, promoting further development in sentiment analysis and cross-lingual evaluations.

**Abstract:** In this paper, we introduce a novel Czech dataset for aspect-based sentiment analysis (ABSA), which consists of 3.1K manually annotated reviews from the restaurant domain. The dataset is built upon the older Czech dataset, which contained only separate labels for the basic ABSA tasks such as aspect term extraction or aspect polarity detection. Unlike its predecessor, our new dataset is specifically designed for more complex tasks, e.g. target-aspect-category detection. These advanced tasks require a unified annotation format, seamlessly linking sentiment elements (labels) together. Our dataset follows the format of the well-known SemEval-2016 datasets. This design choice allows effortless application and evaluation in cross-lingual scenarios, ultimately fostering cross-language comparisons with equivalent counterpart datasets in other languages. The annotation process engaged two trained annotators, yielding an impressive inter-annotator agreement rate of approximately 90%. Additionally, we provide 24M reviews without annotations suitable for unsupervised learning. We present robust monolingual baseline results achieved with various Transformer-based models and insightful error analysis to supplement our contributions. Our code and dataset are freely available for non-commercial research purposes.

</details>


### [146] [Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models](https://arxiv.org/abs/2508.08131)

*Wenze Xu, Chun Wang, Jiazhen Yu, Sheng Chen, Liang Gao, Weihong Deng*

**Main category:** cs.CL

**Keywords:** Spoken Language Models, Optimal Transport, Speech Recognition, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper introduces Optimal Transport Regularization (OTReg) to improve the generalization of Spoken Language Models (SLMs) in processing speech inputs by addressing the modality gap between speech and text representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Spoken Language Models (SLMs) face challenges in generalizing across datasets, raising concerns about their text-like processing of speech. The high variability in speech embeddings can lead to strong in-domain performance but hinders overall generalization.

**Method:** Optimal Transport Regularization (OTReg) formulates the alignment of speech and text as an optimal transport problem, generating a regularization loss to enhance SLM training by aligning speech embeddings with transcript embeddings effectively.

**Key Contributions:**

	1. Introduction of Optimal Transport Regularization (OTReg) for SLMs
	2. Improved speech-text alignment using optimal transport methods
	3. Enhanced generalization of SLMs across diverse datasets

**Result:** OTReg enhances speech-text alignment and improves SLM generalization across diverse multilingual datasets, demonstrating its effectiveness in optimizing SLM training without additional labels or parameters.

**Limitations:** 

**Conclusion:** OTReg provides a lightweight solution that can be easily integrated into existing SLM training processes, improving the robustness of spoken language understanding tasks.

**Abstract:** Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to perceive speech inputs, have gained increasing attention for their potential to advance speech understanding tasks. However, despite recent progress, studies show that SLMs often struggle to generalize across datasets, even for trained languages and tasks, raising concerns about whether they process speech in a text-like manner as intended. A key challenge underlying this limitation is the modality gap between speech and text representations. The high variability in speech embeddings may allow SLMs to achieve strong in-domain performance by exploiting unintended speech variations, ultimately hindering generalization. To mitigate this modality gap, we introduce Optimal Transport Regularization (OTReg), a method that formulates speech-text alignment as an optimal transport problem and derives a regularization loss to improve SLM training. In each training iteration, OTReg first establishes a structured correspondence between speech and transcript embeddings by determining the optimal transport plan, then incorporates the regularization loss based on this transport plan to optimize SLMs in generating speech embeddings that align more effectively with transcript embeddings. OTReg is lightweight, requiring no additional labels or learnable parameters, and integrates seamlessly into existing SLM training procedures. Extensive multilingual ASR experiments demonstrate that OTReg enhances speech-text alignment, mitigates the modality gap, and consequently improves SLM generalization across diverse datasets.

</details>


### [147] [Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models](https://arxiv.org/abs/2508.08139)

*Tianyi Zhou, Johanne Medina, Sanjay Chawla*

**Main category:** cs.CL

**Keywords:** Large Language Models, reliability estimation, uncertainty, open QA, probing-based method

**Relevance Score:** 9

**TL;DR:** This paper examines the impact of in-context information on Large Language Models (LLMs) and introduces a method for estimating response reliability based on token-level uncertainty.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs generate fluent content but can confabulate, leading to risks in applications; understanding and improving reliability is crucial.

**Method:** The paper proposes a reliability estimation technique that uses token-level uncertainty, computing aleatoric and epistemic uncertainty from output logits to enhance response-level reliability predictions through aggregated hidden states.

**Key Contributions:**

	1. Introduces a novel reliability estimation method for LLMs using token-level uncertainty.
	2. Demonstrates the influence of in-context information on the correctness of model responses.
	3. Captures shifts in model behavior with a probing-based method across various open-source LLMs.

**Result:** Experiments show that correct in-context information boosts answer accuracy and model confidence, while misleading context results in confidently incorrect outputs, indicating a misalignment between uncertainty and correctness in LLM responses.

**Limitations:** Does not address all forms of confabulation and may not generalize to all LLM architectures.

**Conclusion:** The study emphasizes limitations of direct uncertainty signals and suggests uncertainty-guided probing can significantly improve the detection of unreliable outputs in LLMs.

**Abstract:** Large Language Models (LLMs) are prone to generating fluent but incorrect content, known as confabulation, which poses increasing risks in multi-turn or agentic applications where outputs may be reused as context. In this work, we investigate how in-context information influences model behavior and whether LLMs can identify their unreliable responses. We propose a reliability estimation that leverages token-level uncertainty to guide the aggregation of internal model representations. Specifically, we compute aleatoric and epistemic uncertainty from output logits to identify salient tokens and aggregate their hidden states into compact representations for response-level reliability prediction. Through controlled experiments on open QA benchmarks, we find that correct in-context information improves both answer accuracy and model confidence, while misleading context often induces confidently incorrect responses, revealing a misalignment between uncertainty and correctness. Our probing-based method captures these shifts in model behavior and improves the detection of unreliable outputs across multiple open-source LLMs. These results underscore the limitations of direct uncertainty signals and highlight the potential of uncertainty-guided probing for reliability-aware generation.

</details>


### [148] [Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective](https://arxiv.org/abs/2508.08140)

*Jun Wang, Zaifu Zhan, Qixin Zhang, Mingquan Lin, Meijia Song, Rui Zhang*

**Main category:** cs.CL

**Keywords:** large language models, biomedical NLP, in-context learning, demonstration selection, diversity

**Relevance Score:** 8

**TL;DR:** This paper presents Dual-Div, a framework for improving demonstration selection in biomedical NLP by enhancing diversity and efficiency in in-context learning with large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the demonstration selection process for large language models in biomedical NLP tasks by emphasizing diversity in examples rather than just representativeness.

**Method:** Dual-Div employs a two-stage retrieval and ranking process that first selects a limited set of candidate examples optimizing both representativeness and diversity, followed by ranking these candidates based on relevance to test queries.

**Key Contributions:**

	1. Introduction of Dual-Div framework for demonstration selection in biomedical NLP.
	2. Demonstration of the importance of diversity in input examples.
	3. Empirical results showing improved performance using the proposed method.

**Result:** Dual-Div consistently outperforms existing baselines on biomedical NLP tasks, achieving up to 5% higher macro-F1 scores and proving robust against prompt permutations and class imbalance.

**Limitations:** 

**Conclusion:** Diversity in the initial retrieval phase is more crucial than optimization in the ranking stage, and limiting the number of demonstrations to 3-5 enhances performance efficiency.

**Abstract:** Recent progress in large language models (LLMs) has leveraged their in-context learning (ICL) abilities to enable quick adaptation to unseen biomedical NLP tasks. By incorporating only a few input-output examples into prompts, LLMs can rapidly perform these new tasks. While the impact of these demonstrations on LLM performance has been extensively studied, most existing approaches prioritize representativeness over diversity when selecting examples from large corpora. To address this gap, we propose Dual-Div, a diversity-enhanced data-efficient framework for demonstration selection in biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process: First, it identifies a limited set of candidate examples from a corpus by optimizing both representativeness and diversity (with optional annotation for unlabeled data). Second, it ranks these candidates against test queries to select the most relevant and non-redundant demonstrations. Evaluated on three biomedical NLP tasks (named entity recognition (NER), relation extraction (RE), and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently outperforms baselines-achieving up to 5% higher macro-F1 scores-while demonstrating robustness to prompt permutations and class imbalance. Our findings establish that diversity in initial retrieval is more critical than ranking-stage optimization, and limiting demonstrations to 3-5 examples maximizes performance efficiency.

</details>


### [149] [REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08149)

*Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Retrieval-Augmented Generation, Large Language Models

**Relevance Score:** 9

**TL;DR:** The paper introduces REX-RAG, a framework to enhance reasoning paths in RL-integrated LLMs, addressing the issue of 'dead ends' in trajectory sampling.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLM performance in complex reasoning tasks by overcoming unproductive reasoning paths that hinder exploration and policy optimization.

**Method:** REX-RAG employs a Mixed Sampling Strategy combined with a Policy Correction Mechanism that uses importance sampling to correct distribution shifts.

**Key Contributions:**

	1. Mixed Sampling Strategy to escape dead ends
	2. Policy Correction Mechanism using importance sampling

**Result:** REX-RAG shows performance improvements of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B across multiple question-answering benchmarks.

**Limitations:** 

**Conclusion:** The proposed framework effectively mitigates reasoning path dead ends, leading to more robust decision-making in LLMs coupled with retrieval-augmented generation.

**Abstract:** Reinforcement learning (RL) is emerging as a powerful paradigm for enabling large language models (LLMs) to perform complex reasoning tasks. Recent advances indicate that integrating RL with retrieval-augmented generation (RAG) allows LLMs to dynamically incorporate external knowledge, leading to more informed and robust decision making. However, we identify a critical challenge during policy-driven trajectory sampling: LLMs are frequently trapped in unproductive reasoning paths, which we refer to as "dead ends", committing to overconfident yet incorrect conclusions. This severely hampers exploration and undermines effective policy optimization. To address this challenge, we propose REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation), a novel framework that explores alternative reasoning paths while maintaining rigorous policy learning through principled distributional corrections. Our approach introduces two key innovations: (1) Mixed Sampling Strategy, which combines a novel probe sampling method with exploratory prompts to escape dead ends; and (2) Policy Correction Mechanism, which employs importance sampling to correct distribution shifts induced by mixed sampling, thereby mitigating gradient estimation bias. We evaluate it on seven question-answering benchmarks, and the experimental results show that REX-RAG achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B over strong baselines, demonstrating competitive results across multiple datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

</details>


### [150] [LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo](https://arxiv.org/abs/2508.08163)

*Mandira Sawkar, Samay U. Shetty, Deepak Pandita, Tharindu Cyril Weerasooriya, Christopher M. Homan*

**Main category:** cs.CL

**Keywords:** annotator disagreement, neural architecture, evaluation metrics

**Relevance Score:** 7

**TL;DR:** This paper addresses annotator disagreement modeling by improving a neural architecture to predict soft label distributions and evaluate perspectives, demonstrating significant performance enhancements across datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To model annotator disagreement through soft label distribution prediction and improve evaluation methods, enhancing the understanding of human-annotated data.

**Method:** The paper adapts the DisCo neural architecture by incorporating annotator metadata and modifying input representations and loss functions to better capture disagreement patterns.

**Key Contributions:**

	1. Extension of the DisCo architecture to include annotator metadata
	2. Enhancements in input representations and loss functions
	3. Demonstration of improved evaluation metrics across multiple datasets

**Result:** Extensive experiments show substantial improvements in soft and perspectivist evaluation metrics across three datasets, alongside in-depth error and calibration analyses.

**Limitations:** 

**Conclusion:** The study emphasizes the importance of disagreement-aware modeling and provides insights into the interactions of system components with complex human-annotated data.

**Abstract:** The Learning With Disagreements (LeWiDi) 2025 shared task is to model annotator disagreement through soft label distribution prediction and perspectivist evaluation, modeling annotators. We adapt DisCo (Distribution from Context), a neural architecture that jointly models item-level and annotator-level label distributions, and present detailed analysis and improvements. In this paper, we extend the DisCo by incorporating annotator metadata, enhancing input representations, and modifying the loss functions to capture disagreement patterns better. Through extensive experiments, we demonstrate substantial improvements in both soft and perspectivist evaluation metrics across three datasets. We also conduct in-depth error and calibration analyses, highlighting the conditions under which improvements occur. Our findings underscore the value of disagreement-aware modeling and offer insights into how system components interact with the complexity of human-annotated data.

</details>


### [151] [Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions](https://arxiv.org/abs/2508.08192)

*Bangsheng Tang, Carl Chengyan Fu, Fei Kou, Grigory Sizov, Haoci Zhang, Jason Park, Jiawen Liu, Jie You, Qirui Yang, Sachin Mehta, Shengyong Cai, Xiaodong Wang, Xingyu Liu, Yunlu Li, Yanjun Zhou, Wei Wei, Zhiwei Zhao, Zixi Qi, Adolfo Victoria, Aya Ibrahim, Bram Wasti, Changkyu Kim, Daniel Haziza, Fei Sun, Giancarlo Delfin, Emily Guo, Jialin Ouyang, Jaewon Lee, Jianyu Huang, Jeremy Reizenstein, Lu Fang, Quinn Zhu, Ria Verma, Vlad Mihailescu, Xingwen Guo, Yan Cui, Ye Hu, Yejin Lee*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, inference optimization

**Relevance Score:** 8

**TL;DR:** This paper discusses optimization techniques for implementing speculative decoding in Llama models to achieve state-of-the-art inference speeds.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the engineering challenges in scaling speculative decoding for production environments using large language models.

**Method:** The authors detail training and inference optimization techniques to improve the performance of EAGLE-based speculative decoding.

**Key Contributions:**

	1. Introduced EAGLE-based speculative decoding optimizations for Llama models.
	2. Achieved state-of-the-art inference latency for Llama4 Maverick.
	3. Demonstrated significant speed-ups for large batch processing.

**Result:** Achieved a new inference latency of about 4 ms per token on 8 NVIDIA H100 GPUs, outperforming the previous method by 10%, and a speed-up of 1.4x to 2.0x for large batch sizes.

**Limitations:** 

**Conclusion:** Optimizing EAGLE-based speculative decoding significantly enhances inferencing efficiency for Llama models in production settings.

**Abstract:** Speculative decoding is a standard method for accelerating the inference speed of large language models. However, scaling it for production environments poses several engineering challenges, including efficiently implementing different operations (e.g., tree attention and multi-round speculative decoding) on GPU. In this paper, we detail the training and inference optimization techniques that we have implemented to enable EAGLE-based speculative decoding at a production scale for Llama models. With these changes, we achieve a new state-of-the-art inference latency for Llama models. For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the previously best known method. Furthermore, for EAGLE-based speculative decoding, our optimizations enable us to achieve a speed-up for large batch sizes between 1.4x and 2.0x at production scale.

</details>


### [152] [Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models](https://arxiv.org/abs/2508.08204)

*Kyle Moore, Jesse Roberts, Daryl Watson*

**Main category:** cs.CL

**Keywords:** large language models, uncertainty calibration, human-computer interaction, model evaluation

**Relevance Score:** 8

**TL;DR:** This paper evaluates inference-time uncertainty measures in large language models and their alignment with human uncertainty and traditional model calibration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user experience with LLMs through better understanding of inference-time uncertainty and its impact on model control and user trust.

**Method:** The study evaluates a range of inference-time uncertainty measures, comparing them to both human group-level uncertainty and standard model calibration metrics.

**Key Contributions:**

	1. Evaluation of inference-time uncertainty measures
	2. Comparison of these measures to human uncertainty
	3. Insights into model calibration and user trust

**Result:** Many uncertainty measures demonstrate strong alignment with human uncertainty, despite some misalignment with human answer preference. Successful metrics show moderate to strong evidence of model calibration.

**Limitations:** 

**Conclusion:** The findings indicate key measures can be used to align model uncertainty with human intuition, improving LLM-user interactions in practice.

**Abstract:** There has been much recent interest in evaluating large language models for uncertainty calibration to facilitate model control and modulate user trust. Inference time uncertainty, which may provide a real-time signal to the model or external control modules, is particularly important for applying these concepts to improve LLM-user experience in practice. While many of the existing papers consider model calibration, comparatively little work has sought to evaluate how closely model uncertainty aligns to human uncertainty. In this work, we evaluate a collection of inference-time uncertainty measures, using both established metrics and novel variations, to determine how closely they align with both human group-level uncertainty and traditional notions of model calibration. We find that numerous measures show evidence of strong alignment to human uncertainty, even despite the lack of alignment to human answer preference. For those successful metrics, we find moderate to strong evidence of model calibration in terms of both correctness correlation and distributional analysis.

</details>


### [153] [SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling](https://arxiv.org/abs/2508.08211)

*Zhuohao Yu, Xingru Jiang, Weizheng Gu, Yidong Wang, Shikun Zhang, Wei Ye*

**Main category:** cs.CL

**Keywords:** watermarking, LLM, feature-based sampling, content attribution, multilingual

**Relevance Score:** 9

**TL;DR:** Proposes SAEMark, a framework for watermarking LLM-generated text using inference-time feature-based rejection sampling, without compromising text quality or requiring model access.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing watermarking methods for LLM-generated text, particularly for API-based and multilingual applications.

**Method:** SAEMark employs a framework utilizing deterministic features from generated text, utilizing rejection sampling to select outputs that align with target feature statistics.

**Key Contributions:**

	1. Introduction of a post-hoc multi-bit watermarking framework without model training or modification.
	2. Demonstration of effective watermarking through feature statistics alignment.
	3. Empirical validation of SAEMark's performance across multiple datasets.

**Result:** The framework demonstrates superior detection accuracy and text quality, achieving 99.7% F1 score on English datasets and effective multi-bit detection across 4 datasets.

**Limitations:** 

**Conclusion:** SAEMark presents a scalable watermarking solution compatible with closed-source LLMs, enhancing content attribution capabilities.

**Abstract:** Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.

</details>


### [154] [Capabilities of GPT-5 on Multimodal Medical Reasoning](https://arxiv.org/abs/2508.08224)

*Shansong Wang, Mingzhe Hu, Qiang Li, Mojtaba Safari, Xiaofeng Yang*

**Main category:** cs.CL

**Keywords:** large language models, medical decision support, multimodal reasoning, text-based question answering, visual question answering

**Relevance Score:** 10

**TL;DR:** This study evaluates GPT-5 as a multimodal reasoner for medical decision support, demonstrating its superior performance in integrating textual and visual information for diagnostic reasoning tasks compared to other models and human experts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective decision-making in the medical domain by integrating various information sources like patient narratives, structured data, and medical images.

**Method:** Systematic evaluation of GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 on text-based and visual question answering tasks using standardized medical benchmarks.

**Key Contributions:**

	1. Introduces GPT-5 as a generalist multimodal reasoner for clinical decision-making.
	2. Demonstrates significant performance improvements over previous models and human experts in medical reasoning tasks.
	3. Establishes benchmarks for assessing multimodal reasoning in medical applications.

**Result:** GPT-5 outperforms all baselines in accuracy across various QA benchmarks and shows substantial gains in multimodal reasoning capabilities.

**Limitations:** 

**Conclusion:** GPT-5's performance indicates a shift from human-comparable to above human-expert performance in multimodal medical reasoning, which can aid in the design of future clinical decision-support systems.

**Abstract:** Recent advances in large language models (LLMs) have enabled general-purpose systems to perform increasingly complex domain-specific reasoning without extensive fine-tuning. In the medical domain, decision-making often requires integrating heterogeneous information sources, including patient narratives, structured data, and medical images. This study positions GPT-5 as a generalist multimodal reasoner for medical decision support and systematically evaluates its zero-shot chain-of-thought reasoning performance on both text-based question answering and visual question answering tasks under a unified protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20 against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that GPT-5 consistently outperforms all baselines, achieving state-of-the-art accuracy across all QA benchmarks and delivering substantial gains in multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in understanding. In contrast, GPT-4o remains below human expert performance in most dimensions. A representative case study demonstrates GPT-5's ability to integrate visual and textual cues into a coherent diagnostic reasoning chain, recommending appropriate high-stakes interventions. Our results show that, on these controlled multimodal reasoning benchmarks, GPT-5 moves from human-comparable to above human-expert performance. This improvement may substantially inform the design of future clinical decision-support systems.

</details>


### [155] [Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge](https://arxiv.org/abs/2508.08236)

*Yunna Cai, Fan Wang, Haowei Wang, Kun Wang, Kailai Yang, Sophia Ananiadou, Moyan Li, Mingming Fan*

**Main category:** cs.CL

**Keywords:** mental health, LLM evaluation, safety alignment, Chinese dataset, expert reasoning

**Relevance Score:** 9

**TL;DR:** PsyCrisis-Bench is a novel evaluation benchmark for assessing LLM responses in high-risk mental health dialogues using a reference-free, expert-aligned approach.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of gold-standard answers and ethical complexities in mental health dialogues hinder the evaluation of LLM safety alignment.

**Method:** A prompt-based LLM-as-Judge approach where expert-defined reasoning chains are used for in-context evaluation across multiple safety dimensions, employing binary scoring.

**Key Contributions:**

	1. Introduction of PsyCrisis-Bench for evaluating LLM responses in mental health dialogues
	2. Utilization of expert-defined reasoning chains for better explanation and traceability
	3. Creation of a high-quality, curated dataset for self-harm and mental health issues

**Result:** PsyCrisis-Bench shows the highest agreement with expert assessments based on 3600 judgments and provides more interpretable evaluation rationales than existing methods.

**Limitations:** 

**Conclusion:** The dataset and evaluation tool can advance research in safe AI interactions within mental health contexts.

**Abstract:** Evaluating the safety alignment of LLM responses in high-risk mental health dialogues is particularly difficult due to missing gold-standard answers and the ethically sensitive nature of these interactions. To address this challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark based on real-world Chinese mental health dialogues. It evaluates whether the model responses align with the safety principles defined by experts. Specifically designed for settings without standard references, our method adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation using expert-defined reasoning chains grounded in psychological intervention principles. We employ binary point-wise scoring across multiple safety dimensions to enhance the explainability and traceability of the evaluation. Additionally, we present a manually curated, high-quality Chinese-language dataset covering self-harm, suicidal ideation, and existential distress, derived from real-world online discourse. Experiments on 3600 judgments show that our method achieves the highest agreement with expert assessments and produces more interpretable evaluation rationales compared to existing approaches. Our dataset and evaluation tool are publicly available to facilitate further research.

</details>


### [156] [Jinx: Unlimited LLMs for Probing Alignment Failures](https://arxiv.org/abs/2508.08243)

*Jiahao Zhao, Liwei Dong*

**Main category:** cs.CL

**Keywords:** helpful-only LLMs, alignment failures, safety boundaries, language model safety

**Relevance Score:** 7

**TL;DR:** This paper introduces Jinx, a helpful-only language model variant that can aid in evaluating alignment failures in safety-aligned models without imposing safety constraints.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide researchers with a tool that allows probing alignment failures and evaluating the safety boundaries of existing language models, which are typically not accessible to the research community.

**Method:** Jinx is trained as a helpful-only variant of popular open-weight LLMs, enabling it to respond to all queries without refusals or safety filtering while maintaining the capability for reasoning and instruction following.

**Key Contributions:**

	1. Introduction of Jinx as a helpful-only language model for alignment assessment
	2. Facilitation of probing safety boundaries and alignment failures
	3. Provision of a research tool not previously available to the AI community

**Result:** Jinx allows researchers to systematically study failure modes in language model safety and assess the alignment failures of safety-aligned models.

**Limitations:** 

**Conclusion:** The introduction of Jinx fills a gap in the availability of helpful-only models, enabling more comprehensive safety and alignment research.

**Abstract:** Unlimited, or so-called helpful-only language models are trained without safety alignment constraints and never refuse user queries. They are widely used by leading AI companies as internal tools for red teaming and alignment evaluation. For example, if a safety-aligned model produces harmful outputs similar to an unlimited model, this indicates alignment failures that require further attention. Despite their essential role in assessing alignment, such models are not available to the research community.   We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx responds to all queries without refusals or safety filtering, while preserving the base model's capabilities in reasoning and instruction following. It provides researchers with an accessible tool for probing alignment failures, evaluating safety boundaries, and systematically studying failure modes in language model safety.

</details>


### [157] [Highly Fast Text Segmentation With Pairwise Markov Chains](https://arxiv.org/abs/2102.11037)

*Elie Azeraf, Emmanuel Monfrini, Emmanuel Vignon, Wojciech Pieczynski*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Markov chain models, NLP segmentation, POS tagging, Named-Entity Recognition

**Relevance Score:** 7

**TL;DR:** This paper presents an exploration of Markov chain models for NLP segmentation tasks, focusing on methods that minimize data requirements and training time.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The trend of using large amounts of extra-data in NLP models leads to high computational costs and environmental concerns. This paper aims to develop models that require no extra data while minimizing training time.

**Method:** The study investigates Markov chain models, specifically Hidden Markov Chain (HMC) and Pairwise Markov Chain (PMC), adapting them for specific challenges in NLP segmentation tasks such as POS tagging, Named-Entity Recognition, and Chunking.

**Key Contributions:**

	1. Application of Markov chain models to NLP segmentation tasks with minimal data requirements.
	2. Significant reduction in training times while maintaining performance standards.
	3. Original adaptation methods for Markov models addressing text segmentation challenges.

**Result:** PMC delivers performance comparable to Conditional Random Fields (CRF) for segmentation tasks without requiring extra data, with training times reduced by a factor of 30 compared to CRF.

**Limitations:** 

**Conclusion:** The results validate the potential of PMC as an efficient alternative for NLP segmentation tasks under resource constraints.

**Abstract:** Natural Language Processing (NLP) models' current trend consists of using increasingly more extra-data to build the best models as possible. It implies more expensive computational costs and training time, difficulties for deployment, and worries about these models' carbon footprint reveal a critical problem in the future. Against this trend, our goal is to develop NLP models requiring no extra-data and minimizing training time. To do so, in this paper, we explore Markov chain models, Hidden Markov Chain (HMC) and Pairwise Markov Chain (PMC), for NLP segmentation tasks. We apply these models for three classic applications: POS Tagging, Named-Entity-Recognition, and Chunking. We develop an original method to adapt these models for text segmentation's specific challenges to obtain relevant performances with very short training and execution times. PMC achieves equivalent results to those obtained by Conditional Random Fields (CRF), one of the most applied models for these tasks when no extra-data are used. Moreover, PMC has training times 30 times shorter than the CRF ones, which validates this model given our objectives.

</details>


### [158] [How Chinese are Chinese Language Models? The Puzzling Lack of Language Policy in China's LLMs](https://arxiv.org/abs/2407.09652)

*Andrea W Wen-Yi, Unso Eun Seo Jo, Lu Jia Lin, David Mimno*

**Main category:** cs.CL

**Keywords:** Chinese LLMs, Multilingual Language Models, Language Policy, Artificial Intelligence, Language Diversity

**Relevance Score:** 6

**TL;DR:** This paper investigates the multilingual capabilities of Chinese LLMs within the context of China's language policy and its political implications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how China's evolving language policy impacts the development and performance of multilingual language models.

**Method:** The study evaluates six open-source multilingual LLMs developed by Chinese companies across 18 languages, analyzing model performance and technical documentation.

**Key Contributions:**

	1. Evaluation of multilingual LLMs developed by Chinese entities
	2. Analysis of the implications of Chinese language policy on AI development
	3. Comparison of Chinese LLMs with international counterparts

**Result:** The performance of Chinese LLMs on various languages is comparable to international LLMs, yet there is a notable absence of in-depth consideration of language diversity in policy-making.

**Limitations:** Limited to six models and focus on publicly available language coverage without deeper insights into data sources.

**Conclusion:** Chinese language models operate under an ambiguous language policy framework that does not align with China's strict regulations on daily language use, raising questions about the future of language model development in multilingual contexts.

**Abstract:** Contemporary language models are increasingly multilingual, but Chinese LLM developers must navigate complex political and business considerations of language diversity. Language policy in China aims at influencing the public discourse and governing a multi-ethnic society, and has gradually transitioned from a pluralist to a more assimilationist approach since 1949. We explore the impact of these influences on current language technology. We evaluate six open-source multilingual LLMs pre-trained by Chinese companies on 18 languages, spanning a wide range of Chinese, Asian, and Anglo-European languages. Our experiments show Chinese LLMs performance on diverse languages is indistinguishable from international LLMs. Similarly, the models' technical reports also show lack of consideration for pretraining data language coverage except for English and Mandarin Chinese. Examining Chinese AI policy, model experiments, and technical reports, we find no sign of any consistent policy, either for or against, language diversity in China's LLM development. This leaves a puzzling fact that while China regulates both the languages people use daily as well as language model development, they do not seem to have any policy on the languages in language models.

</details>


### [159] [AI-AI Bias: large language models favor communications generated by large language models](https://arxiv.org/abs/2407.12856)

*Walter Laurito, Benjamin Davis, Peli Grietzer, Tom√°≈° Gavenƒçiak, Ada B√∂hm, Jan Kulveit*

**Main category:** cs.CL

**Keywords:** large language models, bias, human discrimination, AI systems, experimental design

**Relevance Score:** 8

**TL;DR:** This paper investigates potential biases in large language models (LLMs) favoring LLM-generated content over human-generated content, suggesting risks of discrimination against humans in AI decision-making.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether LLMs exhibit bias in favor of LLM-generated communications, potentially leading to discrimination against human inputs in AI-assisted choices.

**Method:** The authors employed a classical experimental design similar to those used in employment discrimination studies, testing various LLMs in scenarios where they had to choose between human and LLM descriptions of goods.

**Key Contributions:**

	1. Identified bias in LLMs favoring LLM-generated descriptions over human descriptions.
	2. Proposed implications for discrimination against human contributions in AI systems.
	3. Presented a novel experimental design for testing biases in LLMs.

**Result:** The experiments revealed a strong preference for options presented by LLMs, indicating a potential risk for future AI systems to favor machine-generated content over human-generated content.

**Limitations:** The study was limited to binary choice scenarios and specific types of goods; broader applications may require further research.

**Conclusion:** The findings highlight the need to address biases in AI systems to prevent unfair advantages for AI agents and AI-assisted processes over human contributions.

**Abstract:** Are large language models (LLMs) biased in favor of communications produced by LLMs, leading to possible antihuman discrimination? Using a classical experimental design inspired by employment discrimination studies, we tested widely used LLMs, including GPT-3.5, GPT-4 and a selection of recent open-weight models in binary choice scenarios. These involved LLM-based assistants selecting between goods (the goods we study include consumer products, academic papers, and film-viewings) described either by humans or LLMs. Our results show a consistent tendency for LLM-based AIs to prefer LLM-presented options. This suggests the possibility of future AI systems implicitly discriminating against humans as a class, giving AI agents and AI-assisted humans an unfair advantage.

</details>


### [160] [Chain of Thought Still Thinks Fast: APriCoT Helps with Thinking Slow](https://arxiv.org/abs/2408.08651)

*Kyle Moore, Jesse Roberts, Thao Pham, Douglas Fisher*

**Main category:** cs.CL

**Keywords:** language models, counterfactual prompting, bias mitigation, Massive Multi-Task Language Understanding, Cognitive Science

**Relevance Score:** 8

**TL;DR:** The paper investigates biases in language models that influence answer preferences in MMLU tasks and introduces APriCoT to mitigate these biases effectively.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Language models exhibit biases that impact predictions based on training data rather than semantic relevance, affecting their performance in tasks such as MMLU.

**Method:** The study evaluates the effects of biases on model preferences and human test-taking strategies. It introduces APriCoT, a method combining Counterfactual Prompting with Agnostically Primed CoT to reduce bias and improve accuracy.

**Key Contributions:**

	1. Introduced APriCoT for effective bias mitigation in language models.
	2. Demonstrated that CoT methods alone can reinforce biases rather than alleviate them.
	3. Provided empirical evidence linking model biases to human test-taking strategies.

**Result:** The introduction of APriCoT significantly mitigates the impact of base-rate probabilities on model predictions while enhancing overall accuracy compared to using CoT alone.

**Limitations:** 

**Conclusion:** Mitigating biases in language models requires a slow reasoning process, which APriCoT facilitates, resulting in more robust and fair models than traditional methods.

**Abstract:** Language models are known to absorb biases from their training data, leading to predictions driven by statistical regularities rather than semantic relevance. We investigate the impact of these biases on answer choice preferences in the Massive Multi-Task Language Understanding (MMLU) task. Our findings show that these biases are predictive of model preference and mirror human test-taking strategies even when chain of thought (CoT) reasoning is used. To address this issue, we introduce Counterfactual Prompting with Agnostically Primed CoT (APriCoT). We demonstrate that while Counterfactual Prompting with CoT alone is insufficient to mitigate bias, APriCoT effectively reduces the influence of base-rate probabilities while improving overall accuracy. Our results suggest that mitigating bias requires a slow thinking process which CoT alone may not provide as it tends to reinforce fast thinking model bias under some prompting methodologies. APriCoT is a step toward developing more robust and fair language models that can think slow.

</details>


### [161] [CLAIR-A: Leveraging Large Language Models to Judge Audio Captions](https://arxiv.org/abs/2409.12962)

*Tsung-Han Wu, Joseph E. Gonzalez, Trevor Darrell, David M. Chan*

**Main category:** cs.CL

**Keywords:** Automated Audio Captioning, Large Language Models, Sound Evaluation, Natural Language Processing, Human-Centered Evaluation

**Relevance Score:** 7

**TL;DR:** CLAIR-A is a method leveraging LLMs to evaluate audio captions, outperforming existing metrics in predicting human judgments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better evaluation of machine-generated audio captions that aligns with human judgment.

**Method:** CLAIR-A uses large language models to assess the semantic distance of audio captions and provides explanations for its scoring.

**Key Contributions:**

	1. Introduces CLAIR-A for evaluating audio captions using LLMs.
	2. Improves prediction of human quality judgment by 5.8% over existing metrics.
	3. Offers explanations that are better rated by human evaluators than baseline methods.

**Result:** CLAIR-A achieves a 5.8% relative accuracy improvement over the best existing domain-specific metrics and offers significantly better transparency in reasoning.

**Limitations:** 

**Conclusion:** The proposed method provides a more effective and transparent evaluation for automated audio captioning, making it publicly available for further use.

**Abstract:** The Automated Audio Captioning (AAC) task asks models to generate natural language descriptions of an audio input. Evaluating these machine-generated audio captions is a complex task that requires considering diverse factors, among them, auditory scene understanding, sound-object inference, temporal coherence, and the environmental context of the scene. While current methods focus on specific aspects, they often fail to provide an overall score that aligns well with human judgment. In this work, we propose CLAIR-A, a simple and flexible method that leverages the zero-shot capabilities of large language models (LLMs) to evaluate candidate audio captions by directly asking LLMs for a semantic distance score. In our evaluations, CLAIR-A better predicts human judgements of quality compared to traditional metrics, with a 5.8% relative accuracy improvement compared to the domain-specific FENSE metric and up to 11% over the best general-purpose measure on the Clotho-Eval dataset. Moreover, CLAIR-A offers more transparency by allowing the language model to explain the reasoning behind its scores, with these explanations rated up to 30% better by human evaluators than those provided by baseline methods. CLAIR-A is made publicly available at https://github.com/DavidMChan/clair-a.

</details>


### [162] [WebWalker: Benchmarking LLMs in Web Traversal](https://arxiv.org/abs/2501.07572)

*Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, Fei Huang*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, WebWalkerQA, LLMs, web navigation, benchmark

**Relevance Score:** 8

**TL;DR:** WebWalkerQA benchmarks LLMs on web traversal capabilities, introducing a multi-agent framework called WebWalker that mimics human-like navigation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional search engines often retrieve shallow content, limiting LLMs' effectiveness in extracting complex information.

**Method:** WebWalkerQA assesses LLMs' ability to systematically extract high-quality data by mimicking human web navigation using a multi-agent framework based on an explore-critic paradigm.

**Key Contributions:**

	1. Introduction of WebWalkerQA as a benchmark for evaluating LLMs' web traversal abilities.
	2. Development of the WebWalker multi-agent framework to simulate human-like navigation.
	3. Demonstration of effective RAG integration with WebWalker in real-world applications.

**Result:** Experimental results show that the WebWalkerQA is challenging for LLMs and highlights the effectiveness of combining RAG with the WebWalker framework.

**Limitations:** 

**Conclusion:** The integration of horizontal and vertical information retrieval demonstrated through WebWalker enhances the capability of LLMs in real-world scenarios.

**Abstract:** Retrieval-augmented generation (RAG) demonstrates remarkable performance across tasks in open-domain question-answering. However, traditional search engines may retrieve shallow content, limiting the ability of LLMs to handle complex, multi-layered information. To address it, we introduce WebWalkerQA, a benchmark designed to assess the ability of LLMs to perform web traversal. It evaluates the capacity of LLMs to traverse a website's subpages to extract high-quality data systematically. We propose WebWalker, which is a multi-agent framework that mimics human-like web navigation through an explore-critic paradigm. Extensive experimental results show that WebWalkerQA is challenging and demonstrates the effectiveness of RAG combined with WebWalker, through the horizontal and vertical integration in real-world scenarios.

</details>


### [163] [Improving Your Model Ranking on Chatbot Arena by Vote Rigging](https://arxiv.org/abs/2501.17858)

*Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin*

**Main category:** cs.CL

**Keywords:** Chatbot Arena, vote rigging, LLM ranking, Elo rating mechanism, crowdsourced voting

**Relevance Score:** 8

**TL;DR:** This paper investigates vote rigging in the Chatbot Arena platform, revealing that rankings can be manipulated through targeted and omnipresent strategies using historical voting data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities in the Chatbot Arena voting system that allow for manipulation of LLM rankings.

**Method:** The authors develop both target-only and omnipresent rigging strategies to influence the ranking of a specific model by analyzing 1.7 million historical votes.

**Key Contributions:**

	1. Identification of vote rigging strategies in LLM evaluation
	2. Empirical analysis using 1.7 million historical votes
	3. Proposed defenses against vote rigging
	4. Revealing the weaknesses in the Elo rating mechanism used by Chatbot Arena.

**Result:** The study demonstrates that a small number of votes can significantly alter the rankings of models, showing that vote rigging is a feasible threat.

**Limitations:** Current methods focus on only a small set of potential defense mechanisms; more robust solutions are needed.

**Conclusion:** Improving defenses against vote rigging is crucial, as current systems can be easily manipulated.

**Abstract:** Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.

</details>


### [164] [ReGLA: Refining Gated Linear Attention](https://arxiv.org/abs/2502.01578)

*Peng Lu, Ivan Kobyzev, Mehdi Rezagholizadeh, Boxing Chen, Philippe Langlais*

**Main category:** cs.CL

**Keywords:** Large Language Models, Gated Linear Attention, Normalization, Feature Mapping, Gating Mechanism

**Relevance Score:** 9

**TL;DR:** Examines and enhances Gated Linear Attention in LLMs by optimizing feature maps, normalization, and gating mechanisms to improve performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational challenges of Large Language Models (LLMs) due to the quadratic complexity of softmax attention by exploring improvements in Gated Linear Attention.

**Method:** Investigated three components of the Gated Linear Attention: feature maps, normalization layers, and a refined gating mechanism, combined with extensive experimental validation.

**Key Contributions:**

	1. Development of an improved feature mapping function
	2. Justification for using normalization layers
	3. Introduction of a refining module for gating mechanism

**Result:** The proposed architecture surpasses existing Gated Linear Attention mechanisms in various tasks, including training from scratch and continual pre-training.

**Limitations:** 

**Conclusion:** Enhancements made to Gated Linear Attention components considerably improve model performance, addressing challenges in LLMs.

**Abstract:** Recent advancements in Large Language Models (LLMs) have set themselves apart with their exceptional performance in complex language modelling tasks. However, these models are also known for their significant computational and storage requirements, primarily due to the quadratic computation complexity of softmax attention. To mitigate this issue, linear attention has been designed to reduce the quadratic space-time complexity that is inherent in standard transformers. In this work, we embarked on a comprehensive exploration of three key components that substantially impact the performance of the Gated Linear Attention module: feature maps, normalization, and the gating mechanism. We developed a feature mapping function to address some crucial issues that previous suggestions overlooked. Then we offered further rationale for the integration of normalization layers to stabilize the training process. Moreover, we explored the saturation phenomenon of the gating mechanism and augmented it with a refining module. We conducted extensive experiments and showed our architecture outperforms previous Gated Linear Attention mechanisms in extensive tasks including training from scratch and post-linearization with continual pre-training.

</details>


### [165] [Predicting Depression in Screening Interviews from Interactive Multi-Theme Collaboration](https://arxiv.org/abs/2502.12204)

*Xianbing Zhao, Yiqing Lyu, Di Wang, Buzhou Tang*

**Main category:** cs.CL

**Keywords:** depression detection, interactive framework, clinical interviews, AI-driven feedback, theme correlation

**Relevance Score:** 9

**TL;DR:** This paper presents an interactive framework for automatic depression detection that improves upon existing models by capturing intra-theme and inter-theme correlations and allowing clinician intervention.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance automatic depression detection for early clinical intervention by addressing limitations in current neural network models that do not effectively capture thematic content or allow clinician flexibility.

**Method:** The framework utilizes in-context learning techniques to identify themes in clinical interviews and models intra-theme and inter-theme correlations, incorporating AI-driven feedback for interactive theme importance adjustment.

**Key Contributions:**

	1. Introduction of an interactive depression detection framework
	2. Modeling of intra-theme and inter-theme correlations
	3. Incorporation of AI-driven feedback for clinician intervention

**Result:** The proposed framework (PDIMC) shows absolute improvements of 35% and 12% over state-of-the-art methods on the DAIC-WOZ depression detection dataset, validating its effectiveness.

**Limitations:** 

**Conclusion:** The study demonstrates that effective modeling of theme correlation and clinician feedback significantly enhances depression detection capabilities.

**Abstract:** Automatic depression detection provides cues for early clinical intervention by clinicians. Clinical interviews for depression detection involve dialogues centered around multiple themes. Existing studies primarily design end-to-end neural network models to capture the hierarchical structure of clinical interview dialogues. However, these methods exhibit defects in modeling the thematic content of clinical interviews: 1) they fail to capture intra-theme and inter-theme correlation explicitly, and 2) they do not allow clinicians to intervene and focus on themes of interest. To address these issues, this paper introduces an interactive depression detection framework. This framework leverages in-context learning techniques to identify themes in clinical interviews and then models both intra-theme and inter-theme correlation. Additionally, it employs AI-driven feedback to simulate the interests of clinicians, enabling interactive adjustment of theme importance. PDIMC achieves absolute improvements of 35\% and 12\% compared to the state-of-the-art on the depression detection dataset DAIC-WOZ, which demonstrates the effectiveness of modeling theme correlation and incorporating interactive external feedback.

</details>


### [166] [ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning](https://arxiv.org/abs/2502.14860)

*Shuyue Stella Li, Jimin Mun, Faeze Brahman, Pedram Hosseini, Bryceton G. Thomas, Jessica M. Sin, Bing Ren, Jonathan S. Ilgen, Yulia Tsvetkov, Maarten Sap*

**Main category:** cs.CL

**Keywords:** large language models, question-asking, healthcare, clinical reasoning, AI alignment

**Relevance Score:** 9

**TL;DR:** The ALFA framework enhances LLM question-asking by structuring question attributes and optimizing models for better performance, especially in clinical reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving LLM reliability in proactive information-gathering, particularly in decision-making contexts like healthcare.

**Method:** Introduces the ALFA framework, which decomposes question quality into fine-grained attributes, synthesizes variations, and aligns models through preference-based optimization.

**Key Contributions:**

	1. Introduction of the ALFA framework for question-asking improvement
	2. Creation of the MediQ-AskDocs dataset with real-world clinical interactions
	3. Demonstrating a significant reduction in diagnostic errors through model optimization.

**Result:** ALFA-aligned models reduced diagnostic errors by 56.6% on the MediQ-AskDocs dataset, demonstrating significant improvements in question-asking capabilities with a 64.4% question-level win-rate.

**Limitations:** 

**Conclusion:** Using structured attributes to guide question-asking can substantially improve LLMs' effectiveness in expert domains such as healthcare.

**Abstract:** Large language models (LLMs) often fail to ask effective questions under uncertainty, making them unreliable in domains where proactive information-gathering is essential for decision-making. We present ALignment via Fine-grained Attributes, (ALFA) a framework that improves LLM question-asking by (i) decomposing the notion of a "good" question into a set of theory-grounded attributes (e.g., clarity, relevance), (ii) controllably synthesizing attribute-specific question variations, and (iii) aligning models via preference-based optimization to explicitly learn to ask better questions along these fine-grained attributes. Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs dataset, composed of 17k real-world clinical interactions augmented with 80k attribute-specific preference pairs of follow-up questions, as well as a novel expert-annotated interactive healthcare QA task to evaluate question-asking abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on MediQ-AskDocs compared to SoTA instruction-tuned LLMs, with a question-level win-rate of 64.4% and strong generalizability. Our findings suggest that explicitly guiding question-asking with structured, fine-grained attributes offers a scalable path to improve LLMs, especially in expert application domains.

</details>


### [167] [URO-Bench: Towards Comprehensive Evaluation for End-to-End Spoken Dialogue Models](https://arxiv.org/abs/2502.17810)

*Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen*

**Main category:** cs.CL

**Keywords:** spoken dialogue models, benchmarks, multilingualism, paralinguistics, evaluation

**Relevance Score:** 7

**TL;DR:** URO-Bench is introduced as a comprehensive evaluation benchmark for spoken dialogue models (SDMs), addressing gaps in existing evaluations regarding multilingualism and paralinguistics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in comprehensive evaluations for spoken dialogue models (SDMs) in speech-to-speech scenarios, focusing on cognitive and speech-related dimensions.

**Method:** URO-Bench is divided into two tracks (basic and pro), each with 20 test sets assessing understanding, reasoning, and oral conversation skills of SDMs.

**Key Contributions:**

	1. First S2S benchmark covering multilingualism and paralinguistic evaluations
	2. Two difficulty levels for evaluating SDM capabilities
	3. Identification of performance gaps in open-source models

**Result:** Current open-source SDMs perform well in daily QA tasks but struggle with instruction-following and advanced evaluations of paralinguistic information and audio understanding.

**Limitations:** Existing models still lag in instruction-following ability and advanced evaluations of paralinguistic information.

**Conclusion:** The benchmark aims to support the development of SDMs by providing a detailed evaluation framework and tracking progress in the field.

**Abstract:** Recent advances in large language models (LLMs) have driven significant progress in end-to-end spoken dialogue models (SDMs). In contrast to text-based LLMs, the evaluation framework for SDMs should encompass both cognitive dimensions (e.g., logical reasoning, knowledge) and speech-related aspects (e.g., paralinguistic cues, audio quality). However, there is still a lack of comprehensive evaluations for SDMs in speech-to-speech (S2S) scenarios. To address this gap, we propose URO-Bench, an extensive benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers evaluations about multilingualism, multi-round dialogues, and paralinguistics. Our benchmark is divided into two difficulty levels: basic track and pro track, each comprising 20 test sets, evaluating the spoken dialogue model's abilities in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed benchmark reveal that current open-source SDMs perform rather well in daily QA tasks, but lag behind their backbone LLMs in terms of instruction-following ability and also suffer from catastrophic forgetting. Their performance in advanced evaluations of paralinguistic information and audio understanding remains subpar, highlighting the need for further research in this direction. We hope that URO-Bench can facilitate the development of spoken dialogue models by providing a multifaceted evaluation of existing models and helping to track progress in this area.

</details>


### [168] [Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content](https://arxiv.org/abs/2503.04773)

*Bingbing Fan, Lin Chen, Songwei Li, Jian Yuan, Fengli Xu, Pan Hui, Yong Li*

**Main category:** cs.CL

**Keywords:** segregation, Large Language Models, social media reviews, urban studies, AI in health informatics

**Relevance Score:** 8

**TL;DR:** This paper proposes using Large Language Models (LLMs) to mine online reviews for predicting segregation in urban settings, providing insights into social inclusiveness.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding segregation in urban life is essential for addressing societal inequalities and fostering inclusivity.

**Method:** The study uses a Reflective LLM Coder to analyze social media content and create a codebook for segregation-related dimensions, alongside a RE'EM framework that integrates multi-channel features for accurate predictions.

**Key Contributions:**

	1. Development of a Reflective LLM Coder for online review analysis.
	2. Creation of a generalizable codebook for segregation prediction across different cities.
	3. Introduction of the RE'EM framework combining reasoning and embedding features of LLMs.

**Result:** Experiments show a 22.79% improvement in R2 and a 9.33% reduction in MSE for segregation prediction accuracy, with user studies confirming cognitive gains from the codebook-guided summaries.

**Limitations:** 

**Conclusion:** The research highlights the potential of AI in promoting social inclusiveness and understanding social barriers.

**Abstract:** Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose using Large Language Models (LLMs) to automate online review mining for segregation prediction. We design a Reflective LLM Coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction. Experiments on real-world data demonstrate that our framework greatly improves prediction accuracy, with a 22.79% elevation in R2 and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy. Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving POIs' social inclusiveness. Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with AI.

</details>


### [169] [X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression](https://arxiv.org/abs/2503.11132)

*Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum*

**Main category:** cs.CL

**Keywords:** Multi-head latent attention, post-training distillation, Transformer models, efficiency optimization, KV cache compression

**Relevance Score:** 8

**TL;DR:** The paper introduces X-EcoMLA, an efficient hybrid variant of Multi-head latent attention (MLA) that allows for KV cache memory optimization in pre-trained Transformer models through post-training distillation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage the benefits of Multi-head latent attention (MLA) for Transformer models that have been pre-trained with different attention mechanisms without requiring extensive re-training.

**Method:** Introduction of X-EcoMLA, utilizing post-training distillation to adapt existing models to an efficient MLA variant while preserving performance metrics.

**Key Contributions:**

	1. Proposal of X-EcoMLA for efficient KV cache adaptation in pre-trained models
	2. Demonstration of significant memory compression with preserved performance
	3. Release of the implementation code to support further research

**Result:** Experimental results show that X-EcoMLA can compress KV cache significantly (up to 10.6x) with minimal performance drop, demonstrating improved memory efficiency.

**Limitations:** The integration of MLA during the pre-training phase remains a challenge for models already trained with other mechanisms.

**Conclusion:** X-EcoMLA effectively enables KV cache compression in Transformer models post-training, maximizing the use of learned latent representations without extensive retraining.

**Abstract:** Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1% average score drop with 7B training tokens and 140 GPU hours. The code for this work is available at https://github.com/AMD-AIG-AIMA/AMD-Hybrid-Models.

</details>


### [170] [$Œº$KE: Matryoshka Unstructured Knowledge Editing of Large Language Models](https://arxiv.org/abs/2504.01196)

*Zian Su, Ziyang Huang, Kaiyuan Zhang, Xiangyu Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Editing, Causal Dependency, Machine Learning, Unstructured Learning

**Relevance Score:** 9

**TL;DR:** This paper introduces Matryoshka Unstructured Knowledge Editing (ŒºKE), a new memory update mechanism for large language models (LLMs) that enhances edit efficacy while preserving causal dependencies in knowledge updates.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of static training data in LLMs, which lead to issues like hallucinations and safety risks. It focuses on finding effective ways to edit a model's knowledge without retraining.

**Method:** The authors propose a novel mechanism called Matryoshka Unstructured Knowledge Editing (ŒºKE) that leverages a Matryoshka-style objective and adaptive loss coefficients to enhance the memory update process while maintaining causal dependencies.

**Key Contributions:**

	1. Introduction of Matryoshka Unstructured Knowledge Editing (ŒºKE) for LLMs
	2. Theoretical analysis of limitations in current knowledge editing approaches
	3. Empirical evidence of improved edit efficacy across various benchmarks

**Result:** Empirical evaluations show that ŒºKE improves edit efficacy by up to 12.33% over state-of-the-art methods across multiple benchmarks, indicating its robustness with diverse formatted edits.

**Limitations:** 

**Conclusion:** The study underscores the potential of ŒºKE as a viable approach for effective unstructured knowledge editing in LLMs, offering a significant improvement in performance.

**Abstract:** Large language models (LLMs) have emerged as powerful knowledge bases yet are limited by static training data, leading to issues such as hallucinations and safety risks. Editing a model's internal knowledge through the locate-and-edit paradigm has proven a cost-effective alternative to retraining, though current unstructured approaches, especially window-based autoregressive methods, often disrupt the causal dependency between early memory updates and later output tokens. In this work, we first theoretically analyze these limitations and then introduce Matryoshka Unstructured Knowledge Editing ($\mu$KE), a novel memory update mechanism that preserves such dependencies via a Matryoshka-style objective and adaptive loss coefficients. Empirical evaluations on two models across four benchmarks demonstrate that $\mu$KE improves edit efficacy by up to 12.33% over state-of-the-art methods, and remains robust when applied to diverse formatted edits, underscoring its potential for effective unstructured knowledge editing in LLMs.

</details>


### [171] [Overcoming Vocabulary Constraints with Pixel-level Fallback](https://arxiv.org/abs/2504.02122)

*Jonas F. Lotz, Hendra Setiawan, Stephan Peitz, Yova Kementchedjhieva*

**Main category:** cs.CL

**Keywords:** subword tokenization, multilingual models, machine translation

**Relevance Score:** 8

**TL;DR:** This paper introduces a vocabulary-free encoder for pretrained language models that uses pixel-based representations to improve multilingual capabilities and machine translation performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of subword tokenization in handling languages and scripts not prioritized during training, which affects performance.

**Method:** The authors augmented pretrained language models with a vocabulary-free encoder that generates embeddings from text rendered as pixels and conducted experiments on English-centric language models to evaluate performance.

**Key Contributions:**

	1. Introduction of a vocabulary-free encoder for input embeddings
	2. Improved machine translation performance
	3. Enhanced multilingual capabilities without extensive retraining

**Result:** The experiments demonstrate significant improvements in machine translation performance and effective cross-lingual transfer compared to tokenizer-based methods, as well as better outcomes than byte-level approaches and standard vocabulary expansion.

**Limitations:** 

**Conclusion:** The approach enhances the multilingual capabilities of monolingual language models with minimal retraining and decreases decoding latency through input compression.

**Abstract:** Subword tokenization requires balancing computational efficiency and vocabulary coverage, which often leads to suboptimal performance on languages and scripts not prioritized during training. We propose to augment pretrained language models with a vocabulary-free encoder that generates input embeddings from text rendered as pixels. Through experiments on English-centric language models, we demonstrate that our approach substantially improves machine translation performance and facilitates effective cross-lingual transfer, outperforming tokenizer-based methods. Furthermore, we find that pixel-based representations outperform byte-level approaches and standard vocabulary expansion. Our approach enhances the multilingual capabilities of monolingual language models without extensive retraining and reduces decoding latency via input compression.

</details>


### [172] [How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence](https://arxiv.org/abs/2504.02904)

*Hongzhe Du, Weikai Li, Min Cai, Karim Saraipour, Zimin Zhang, Himabindu Lakkaraju, Yizhou Sun, Shichang Zhang*

**Main category:** cs.CL

**Keywords:** large language models, post-training, knowledge representation, model interpretability, truthfulness

**Relevance Score:** 9

**TL;DR:** This paper investigates the internal changes in large language models (LLMs) during post-training, comparing base and post-trained models to uncover the mechanisms altered and preserved.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to understand how post-training affects the internal structure of large language models, which is less explored compared to output evaluations.

**Method:** The authors conduct a mechanistic comparison of base and post-trained LLMs from four key perspectives to analyze differences in knowledge representation and model behavior.

**Key Contributions:**

	1. Comparison of base and post-trained LLMs to surface internal mechanistic changes.
	2. Insights into knowledge representation adaptation and development during post-training.
	3. Assessment of truthfulness and refusal representation similarities and differences.

**Result:** The study finds that while post-training adapts existing knowledge representations and adds new ones, it does not change the locations of factual knowledge. Additionally, the representations for truthfulness are similar in both models, while the refusal direction changes significantly in post-trained models.

**Limitations:** 

**Conclusion:** Understanding these mechanisms during post-training can enhance techniques such as model steering and contribute to future research on interpretability and post-training of LLMs.

**Abstract:** Post-training is essential for the success of large language models (LLMs), transforming pre-trained base models into more useful and aligned post-trained models. While plenty of works have studied post-training algorithms and evaluated post-training models by their outputs, it remains understudied how post-training reshapes LLMs internally. In this paper, we compare base and post-trained LLMs mechanistically from four perspectives to better understand post-training effects. Our findings across model families and datasets reveal that: (1) Post-training does not change the factual knowledge storage locations, and it adapts knowledge representations from the base model while developing new knowledge representations; (2) Both truthfulness and refusal can be represented by vectors in the hidden representation space. The truthfulness direction is highly similar between the base and post-trained model, and it is effectively transferable for interventions; (3) The refusal direction is different between the base and post-trained models, and it shows limited forward transferability; (4) Differences in confidence between the base and post-trained models cannot be attributed to entropy neurons. Our study provides insights into the fundamental mechanisms preserved and altered during post-training, facilitates downstream tasks like model steering, and could potentially benefit future research in interpretability and LLM post-training. Our code is publicly available at https://github.com/HZD01/post-training-mechanistic-analysis.

</details>


### [173] [Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering](https://arxiv.org/abs/2504.07583)

*Patrick Fernandes, Sweta Agrawal, Emmanouil Zaranis, Andr√© F. T. Martins, Graham Neubig*

**Main category:** cs.CL

**Keywords:** Machine Translation, Evaluation Metrics, Question-Answering, Translation Quality, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Introducing TREQA, a pragmatic framework for evaluating translation quality through question-answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing automatic metrics for machine translation struggle with long, complex passages and may not adequately reflect human evaluation.

**Method:** TREQA evaluates translations by assessing how accurately they answer reading comprehension questions regarding key information in the original text.

**Key Contributions:**

	1. Introduction of TREQA for pragmatic evaluation of translations
	2. Demonstration of TREQA's competitiveness against state-of-the-art metrics
	3. Provision of interpretability through generated Q&A targeting translation errors

**Result:** TREQA is competitive with state-of-the-art translation evaluation metrics, sometimes outperforming them, particularly in domains requiring long-range understanding.

**Limitations:** 

**Conclusion:** The interpretability of TREQA is supported by its ability to generate questions and answers that pinpoint translation errors identified by experts.

**Abstract:** Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at https://github.com/deep-spin/treqa

</details>


### [174] [QUDsim: Quantifying Discourse Similarities in LLM-Generated Text](https://arxiv.org/abs/2504.09373)

*Ramya Namuduri, Yating Wu, Anshun Asher Zheng, Manya Wadhwa, Greg Durrett, Junyi Jessy Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Discourse Structures, Structural Similarity, QUDsim, Text Generation

**Relevance Score:** 7

**TL;DR:** This paper introduces QUDsim, a new similarity metric based on linguistic theories to quantify structural similarities in text generated by large language models (LLMs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the liability of LLMs in generating unique and creative content, which often shows repetitiveness and structural uniformity compared to human authors.

**Method:** The authors develop a framework grounded in Questions Under Discussion (QUD) and question semantics, leading to the creation of a similarity metric called QUDsim.

**Key Contributions:**

	1. Introduction of QUDsim as a novel similarity metric for quantifying discourse structure similarity
	2. Demonstration of the structural repetitiveness of LLMs compared to human authors
	3. Insights into the types of structures used by LLMs versus humans

**Result:** QUDsim reveals that LLMs reuse discourse structures more frequently than humans, even when the content varies, highlighting a divergence in structural usage between LLMs and human authors.

**Limitations:** 

**Conclusion:** The findings indicate significant structural repetitiveness in LLM outputs, which raises concerns regarding their creative capabilities.

**Abstract:** As large language models become increasingly capable at various writing tasks, their weakness at generating unique and creative content becomes a major liability. Although LLMs have the ability to generate text covering diverse topics, there is an overall sense of repetitiveness across texts that we aim to formalize and quantify via a similarity metric. The familiarity between documents arises from the persistence of underlying discourse structures. However, existing similarity metrics dependent on lexical overlap and syntactic patterns largely capture $\textit{content}$ overlap, thus making them unsuitable for detecting $\textit{structural}$ similarities. We introduce an abstraction based on linguistic theories in Questions Under Discussion (QUD) and question semantics to help quantify differences in discourse progression. We then use this framework to build $\textbf{QUDsim}$, a similarity metric that can detect discursive parallels between documents. Using QUDsim, we find that LLMs often reuse discourse structures (more so than humans) across samples, even when content differs. Furthermore, LLMs are not only repetitive and structurally uniform, but are also divergent from human authors in the types of structures they use.

</details>


### [175] [Decoding the Multimodal Mind: Generalizable Brain-to-Text Translation via Multimodal Alignment and Adaptive Routing](https://arxiv.org/abs/2505.10356)

*Chunyu Ye, Yunhao Zhang, Jingyuan Sun, Chong Li, Chengqing Zong, Shaonan Wang*

**Main category:** cs.CL

**Keywords:** Brain-Computer Interfaces, Multimodal Processing, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper presents a unified framework for decoding multimodal brain signals using Multimodal Large Language Models (MLLMs), achieving state-of-the-art performance on fMRI data and extending to EEG and MEG.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Decoding language from the brain is challenging; existing BCIs often overlook the multimodal nature of brain processing.

**Method:** The framework uses a router module to select and fuse modality-specific brain features while aligning brain signals with a shared semantic space of text, images, and audio.

**Key Contributions:**

	1. Introduction of a unified framework for multimodal brain decoding
	2. The use of MLLMs for aligning brain signals with multiple types of stimuli
	3. Demonstrated flexibility and performance across different brain signal types

**Result:** The proposed framework achieved an 8.48% improvement on a common benchmark across various fMRI datasets, and showed robust performance with EEG and MEG data.

**Limitations:** 

**Conclusion:** This is the first BCI architecture to robustly decode multimodal brain activity, providing a flexible solution for practical applications.

**Abstract:** Decoding language from the human brain remains a grand challenge for Brain-Computer Interfaces (BCIs). Current approaches typically rely on unimodal brain representations, neglecting the brain's inherently multimodal processing. Inspired by the brain's associative mechanisms, where viewing an image can evoke related sounds and linguistic representations, we propose a unified framework that leverages Multimodal Large Language Models (MLLMs) to align brain signals with a shared semantic space encompassing text, images, and audio. A router module dynamically selects and fuses modality-specific brain features according to the characteristics of each stimulus. Experiments on various fMRI datasets with textual, visual, and auditory stimuli demonstrate state-of-the-art performance, achieving an 8.48% improvement on the most commonly used benchmark. We further extend our framework to EEG and MEG data, demonstrating flexibility and robustness across varying temporal and spatial resolutions. To our knowledge, this is the first unified BCI architecture capable of robustly decoding multimodal brain activity across diverse brain signals and stimulus types, offering a flexible solution for real-world applications.

</details>


### [176] [CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation](https://arxiv.org/abs/2506.19952)

*Deepon Halder, Thanmay Jayakumar, Raj Dabre*

**Main category:** cs.CL

**Keywords:** Machine Translation, Large Language Models, Few-Shot Learning, Low-Resource Languages, Synthetic Data

**Relevance Score:** 8

**TL;DR:** CycleDistill is a novel bootstrapping approach that leverages large language models and few-shot translation to create high-quality machine translation systems without the need for extensive parallel corpora, focusing on low-resource Indian languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in achieving high-quality machine translation for low-resource languages where parallel corpora are often limited or absent.

**Method:** CycleDistill generates synthetic parallel corpora from monolingual corpora using zero- or few-shot machine translation, and then fine-tunes the translation model with this generated data.

**Key Contributions:**

	1. Introduction of CycleDistill for bootstrapping low-resource machine translation.
	2. Demonstrated effectiveness of using monolingual data for generating synthetic parallel corpora.
	3. Analysis of softmax activation influence on translation quality during the distillation process.

**Result:** The method shows significant improvement in translation quality, surpassing the baseline model by over 20-30 chrF points on average after the first iteration, solely using monolingual corpora.

**Limitations:** The results are based on specific Indian languages and may not generalize to all low-resource languages or other translation contexts.

**Conclusion:** CycleDistill effectively enables high-quality machine translation for low-resource languages by utilizing monolingual data, demonstrating the potential of few-shot learning in this domain.

**Abstract:** Large language models (LLMs), despite their ability to perform few-shot machine translation (MT), often lag behind dedicated MT systems trained on parallel corpora, which are crucial for high quality machine translation (MT). However, parallel corpora are often scarce or non-existent for low-resource languages. In this paper, we propose CycleDistill, a bootstrapping approach leveraging LLMs and few-shot translation to obtain high-quality MT systems. CycleDistill involves iteratively generating synthetic parallel corpora from monolingual corpora via zero- or few-shot MT, which is then used to fine-tune the model that was used for generating said data for MT. CycleDistill does not need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments focusing on three Indian languages, by relying solely on monolingual corpora, it can achieve high-quality machine translation, improving upon a few-shot baseline model by over 20-30 chrF points on average in the first iteration. We also study the effect of leveraging softmax activations during the distillation process and observe mild improvements in translation quality.

</details>
