# 2025-09-10

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 19]

- [cs.CL](#cs.CL) [Total: 54]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Short-Term Gaze Prediction: Analysis of Individual Differences, Typical and Extreme-Case Errors](https://arxiv.org/abs/2509.07126)

*Kateryna Melnyk, Lee Friedman, Oleg Komogortsev*

**Main category:** cs.HC

**Keywords:** gaze prediction, recurrent neural networks, transformers, LSTM, eye movement

**Relevance Score:** 7

**TL;DR:** This article investigates short-term gaze prediction using recurrent neural networks and transformers, comparing the performance of LSTM, transformer-encoder, and a classification-predictor model.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of different models for gaze prediction and analyze performance variations across subjects and eye movement types.

**Method:** A three-layer LSTM, a simple transformer-encoder model, and a classification-predictor network were employed, evaluated on ocular fixations and saccades across various amplitudes and individual differences.

**Key Contributions:**

	1. Comparison of LSTM, transformer-encoder, and classification-predictor models for gaze prediction.
	2. Insights into the performance trade-offs of various models.
	3. Discussion on the importance of analyzing extreme cases in gaze prediction.

**Result:** LSTM generally outperformed others on fixations and saccades, while TF and ClPr had better precision in post-saccadic periods. Performance varied significantly in extreme cases based on eye movement types.

**Limitations:** Does not provide exhaustive insights on all gaze prediction scenarios; focused on specific eye movement types and subjects.

**Conclusion:** Model performance varies based on eye movement types and individual differences, suggesting future work should focus on extreme case analysis and practical model selection insights.

**Abstract:** Gaze prediction is a diverse field of study with multiple research focuses and practical applications. This article investigates how recurrent neural networks and transformers perform short-term gaze prediction. We used three models: a three-layer long-short-term memory (LSTM) network, a simple transformer-encoder model (TF), and a classification-predictor network (ClPr), which simultaneously classifies the signal into eye movement events and predicts the positions of gaze. The performance of the models was evaluated for ocular fixations and saccades of various amplitudes and as a function of individual differences in both typical and extreme cases. On average, LSTM performed better on fixations and saccades, whereas TF and ClPr demonstrated more precise results for post-saccadic periods. In extreme cases, the best-performing models vary depending on the type of eye movement. We reviewed the difference between the median $P_{50}$ and high-percentile $P_{95}$ error profiles across subjects. The subjects for which the models perform the best overall do not necessarily exhibit the lowest $P_{95}$ values, which supports the idea of analyzing extreme cases separately in future work. We explore the trade-offs between the proposed solutions and provide practical insights into model selection for gaze prediction.

</details>


### [2] [Wellbeing-Centered UX: Supporting Content Moderators](https://arxiv.org/abs/2509.07187)

*Diana Mihalache, Dalila Szostak*

**Main category:** cs.HC

**Keywords:** User Experience, Wellbeing, Content Moderation, UX Design, Product Development

**Relevance Score:** 7

**TL;DR:** This chapter addresses the impact of user experience (UX) on the wellbeing of human content moderators in content moderation processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight how content moderators face challenges due to sensitive content, monotonous tasks, and inadequate tools, and to emphasize the need for wellbeing in UX design.

**Method:** The chapter offers a framework and strategies for integrating wellbeing considerations into UX disciplines such as research, writing, and design.

**Key Contributions:**

	1. Framework for wellbeing in UX
	2. Practical strategies for implementation in UX disciplines
	3. Roadmap for supporting content moderators

**Result:** Provides a roadmap for improving user experiences for content moderators, which can enhance both user satisfaction and business outcomes.

**Limitations:** 

**Conclusion:** Incorporating wellbeing in the product development lifecycle can lead to better support for content moderators, ultimately benefiting users and businesses.

**Abstract:** This chapter focuses on the intersection of user experience (UX) and wellbeing in the context of content moderation. Human content moderators play a key role in protecting end users from harm by detecting, evaluating, and addressing content that may violate laws or product policies. They face numerous challenges, including exposure to sensitive content, monotonous tasks, and complex decisions, which are often exacerbated by inadequate tools. This chapter explains the importance of incorporating wellbeing considerations throughout the product development lifecycle, offering a framework and practical strategies for implementation across key UX disciplines: research, writing, and design. By examining these considerations, this chapter provides a roadmap for creating user experiences that support content moderators, benefiting both the user and the business.

</details>


### [3] [Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data](https://arxiv.org/abs/2509.07202)

*Khushiyant*

**Main category:** cs.HC

**Keywords:** EEG, large language models, assistive technology, brain-computer interfaces, transfer learning

**Relevance Score:** 9

**TL;DR:** This paper presents a novel method for EEG-based text production that integrates LLMs and classifiers to enhance efficiency and performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenges in EEG-based text production that require extensive data and processing power.

**Method:** A classifier-LLM architecture combined with a Recurrent Neural Network (RNN) encoder is proposed to lower data and compute requirements while enhancing performance.

**Key Contributions:**

	1. Introduction of a novel classifier-LLM architecture with RNN encoder for EEG-based text production
	2. Achieves comparable performance to state-of-the-art methods with significantly reduced data requirements
	3. Demonstrates effective transfer learning potential in brain-computer interfaces.

**Result:** The proposed method improves overall performance by 10% compared to current methodologies, making EEG-based text production more accessible and efficient.

**Limitations:** 

**Conclusion:** Integrating LLMs with EEG decoding offers potential for better assistive technologies, improving communication for those with motor limitations.

**Abstract:** Text generating capabilities have undergone a substantial transformation with the introduction of large language models (LLMs). Electroencephalography (EEG)-based text production is still difficult, though, because it requires a lot of data and processing power. This paper introduces a new method that combines the use of the Gemma 2B LLM with a classifier-LLM architecture to incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically lowers the amount of data and compute power needed while achieving performance close to that of cutting-edge methods. Notably, compared to current methodologies, our methodology delivers an overall performance improvement of 10%. The suggested architecture demonstrates the possibility of effective transfer learning for EEG-based text production, remaining strong and functional even in the face of data limits. This work highlights the potential of integrating LLMs with EEG decoding to improve assistive technologies and improve independence and communication for those with severe motor limitations. Our method pushes the limits of present capabilities and opens new paths for research and application in brain-computer interfaces by efficiently using the strengths of pre-trained language models. This makes EEG-based text production more accessible and efficient.

</details>


### [4] [In the Queue: Understanding How Reddit Moderators Use the Modqueue](https://arxiv.org/abs/2509.07314)

*Tanvi Bajpai, Eshwar Chandrasekharan*

**Main category:** cs.HC

**Keywords:** moderation, Reddit, human-computer interaction, surveys, community management

**Relevance Score:** 4

**TL;DR:** This paper surveys Reddit moderators to understand their use of the moderation queue, revealing diverse practices and identifying challenges faced in moderation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how moderators use the moderation queue on Reddit and identify common challenges they face.

**Method:** A survey of 110 moderators overseeing over 400 subreddits was conducted to gather data on modqueue practices and issues.

**Key Contributions:**

	1. Insight into moderator practices and challenges in using the modqueue on Reddit
	2. Identification of the need for customizable moderation tools
	3. Recommendations for improving platform infrastructure to support diverse workflows.

**Result:** The study found varied practices in moderation queue usage among moderators, with persistent challenges in review coordination and reliance on external tools, indicating the need for a more customizable solution.

**Limitations:** The study is based on self-reported data from a subset of Reddit moderators, which may not fully represent all users.

**Conclusion:** The modqueue is not a universal tool for moderation, highlighting the need for improved infrastructure to better support diverse moderator workflows.

**Abstract:** On Reddit, the moderation queue (modqueue) is a primary interface for moderators to review reported content. Despite its central role in Reddit's community-reliant moderation model, little is known about how moderators actually use it in practice. To address this gap, we surveyed 110 moderators, who collectively oversee more than 400 unique subreddits, and asked them about their usage of the modqueue. Modqueue practices vary widely: some moderators approach it as a daily checklist, others as a hub to infer community-wide patterns, and many still find the queue insufficient to inform their moderation decisions. We also identify persistent challenges around review coordination, inconsistent interface signals, and reliance on third-party tools. Taken together, we show the modqueue is neither a one-size-fits-all solution nor sufficient on its own for supporting moderator review. Our work highlights design opportunities for more modular, integrated, and customizable platform infrastructures that better support the diversity of moderator workflows.

</details>


### [5] [SpecifyUI: Supporting Iterative UI Design Intent Expression through Structured Specifications and Generative AI](https://arxiv.org/abs/2509.07334)

*Yunnong Chen, Chengwei Shi, Liuqing Chen*

**Main category:** cs.HC

**Keywords:** Large Language Models, Human-Computer Interaction, UI Design

**Relevance Score:** 9

**TL;DR:** This paper introduces SPEC, a structured representation for UI design with LLMs, and SpecifyUI, an interactive system that enhances designer intent capture and controllability in UI generation.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve UI design workflows by addressing challenges in externalizing designer intent and controlling iterative changes in LLM-assisted design.

**Method:** The authors present SPEC, a hierarchical intermediate representation, and SpecifyUI, which employs region segmentation and vision-language models to generate high-fidelity UI designs.

**Key Contributions:**

	1. Introduction of SPEC as a structured representation for UI design.
	2. Development of SpecifyUI which utilizes SPEC to enhance controllability in UI generation.
	3. Demonstrated improvements in design quality and user experience compared to existing systems.

**Result:** SPEC-based generation outperformed traditional prompt-based methods in capturing designer intent, with user studies indicating better intention alignment and user experience with SpecifyUI compared to existing tools.

**Limitations:** 

**Conclusion:** The study establishes SPEC as a novel approach to facilitate iterative and collaborative design processes using LLMs, enhancing the co-creation experience for designers.

**Abstract:** Large language models (LLMs) promise to accelerate UI design, yet current tools struggle with two fundamentals: externalizing designers' intent and controlling iterative change. We introduce SPEC, a structured, parameterized, hierarchical intermediate representation that exposes UI elements as controllable parameters. Building on SPEC, we present SpecifyUI, an interactive system that extracts SPEC from UI references via region segmentation and vision-language models, composes UIs across multiple sources, and supports targeted edits at global, regional, and component levels. A multi-agent generator renders SPEC into high-fidelity designs, closing the loop between intent expression and controllable generation. Quantitative experiments show SPEC-based generation more faithfully captures reference intent than prompt-based baselines. In a user study with 16 professional designers, SpecifyUI significantly outperformed Stitch on intent alignment, design quality, controllability, and overall experience in human-AI co-creation. Our results position SPEC as a specification-driven paradigm that shifts LLM-assisted design from one-shot prompting to iterative, collaborative workflows.

</details>


### [6] [Feed-O-Meter: Fostering Design Feedback Skills through Role-playing Interactions with AI Mentee](https://arxiv.org/abs/2509.07424)

*Hyunseung Lim, Dasom Choi, DaEun Choi, Sooyohn Nam, Hwajung Hong*

**Main category:** cs.HC

**Keywords:** feedback, design education, large language models, role-playing, AI agents

**Relevance Score:** 8

**TL;DR:** Feed-O-Meter is a system utilizing LLM-based agents to enhance feedback skills in design education by allowing students to role-play providing feedback to an AI mentee.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges students face in providing feedback due to a lack of confidence and fear of judgment, limiting their ability to develop essential feedback skills.

**Method:** The system leverages LLM-based agents to create an interactive feedback environment where learners can practice giving feedback and role-play as mentors.

**Key Contributions:**

	1. Introduction of Feed-O-Meter system for feedback practice using LLMs
	2. Empirical findings on engagement and feedback comprehension improvement
	3. Insights into designing AI agents for educational purposes

**Result:** A user study showed that Feed-O-Meter increased engagement and motivation among participants and improved their feedback adjustment for AI mentees.

**Limitations:** The study is limited by its small sample size (N=24) and the reliance on AI agents that may not fully replicate human interactions.

**Conclusion:** The study highlights the potential for AI-powered systems to enhance feedback skills in design education and outlines future directions for development.

**Abstract:** Effective feedback, including critique and evaluation, helps designers develop design concepts and refine their ideas, supporting informed decision-making throughout the iterative design process. However, in studio-based design courses, students often struggle to provide feedback due to a lack of confidence and fear of being judged, which limits their ability to develop essential feedback-giving skills. Recent advances in large language models (LLMs) suggest that role-playing with AI agents can let learners engage in multi-turn feedback without the anxiety of external judgment or the time constraints of real-world settings. Yet prior studies have raised concerns that LLMs struggle to behave like real people in role-play scenarios, diminishing the educational benefits of these interactions. Therefore, designing AI-based agents that effectively support learners in practicing and developing intellectual reasoning skills requires more than merely assigning the target persona's personality and role to the agent. By addressing these issues, we present Feed-O-Meter, a novel system that employs carefully designed LLM-based agents to create an environment in which students can practice giving design feedback. The system enables users to role-play as mentors, providing feedback to an AI mentee and allowing them to reflect on how that feedback impacts the AI mentee's idea development process. A user study (N=24) indicated that Feed-O-Meter increased participants' engagement and motivation through role-switching and helped them adjust feedback to be more comprehensible for an AI mentee. Based on these findings, we discuss future directions for designing systems to foster feedback skills in design education.

</details>


### [7] [Social Media Clones: Exploring the Impact of Social Delegation with AI Clones through a Design Workbook Study](https://arxiv.org/abs/2509.07502)

*Jackie Liu, Mehrnoosh Sadat Shirvani, Hwajung Hong, Ig-Jae Kim, Dongwook Yoon*

**Main category:** cs.HC

**Keywords:** social media, AI clones, user behavior, identity management, interaction design

**Relevance Score:** 6

**TL;DR:** The paper explores the impact of AI-powered social media clones on user behavior and online interactions, highlighting both potential benefits and risks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effects of social media clones on online behavior and relationships due to their integration with user data and personal identity.

**Method:** Conducted semi-structured interviews with 32 social media users, presenting eight speculative clone concepts through a design workbook.

**Key Contributions:**

	1. Identified user behavior changes due to clones' influence
	2. Highlighted risks associated with authenticity in social media
	3. Proposed design considerations for better integration of clones

**Result:** Clones can provide convenience but may threaten authenticity and increase skepticism within online communities, leading users to mimic their clones to avoid interaction issues.

**Limitations:** Limited to user perceptions and speculative concepts without quantitative analysis.

**Conclusion:** The paper discusses the challenges of adopting social media clones and provides design considerations for their effective integration into platforms.

**Abstract:** Social media clones are AI-powered social delegates of ourselves created using our personal data. As our identities and online personas intertwine, these technologies have the potential to greatly enhance our social media experience. If mismanaged, however, these clones may also pose new risks to our social reputation and online relationships. To set the foundation for a productive and responsible integration, we set out to understand how social media clones will impact our online behavior and interactions. We conducted a series of semi-structured interviews introducing eight speculative clone concepts to 32 social media users through a design workbook. Applying existing work in AI-mediated communication in the context of social media, we found that although clones can offer convenience and comfort, they can also threaten the user's authenticity and increase skepticism within the online community. As a result, users tend to behave more like their clones to mitigate discrepancies and interaction breakdowns. These findings are discussed through the lens of past literature in identity and impression management to highlight challenges in the adoption of social media clones by the general public, and propose design considerations for their successful integration into social media platforms.

</details>


### [8] [Digital Twins for Extended Reality Tourism: User Experience Evaluation Across User Groups](https://arxiv.org/abs/2509.07740)

*Maximilian Warsinke, Francesco Vona, Tanja Kojić, Jan-Niklas Voigt-Antons, Sebastian Möller*

**Main category:** cs.HC

**Keywords:** extended reality, user experience, digital twin, augmented reality, virtual reality

**Relevance Score:** 7

**TL;DR:** This study evaluates user experience in XR tourism using AR and VR applications, finding high enjoyment but usability challenges in VR.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess user experience in extended reality tourism and understand how digital twin-based applications perform in this context.

**Method:** A quantitative exploratory approach involving 84 participants from Spain and Germany, assessing UX, task load, presence, cybersickness, and emotional response using standardized questionnaires.

**Key Contributions:**

	1. Evaluation of user experience in XR tourism
	2. Comparison of AR and VR applications
	3. Insights into the impact of demographics on UX metrics

**Result:** Both AR and VR applications yielded low task load and high enjoyment, though VR faced usability and cybersickness issues.

**Limitations:** The study's sample size is limited to Spain and Germany, which may affect generalizability.

**Conclusion:** Well-designed XR experiences are essential, especially for novices, underlining the importance of user experience in digital twin-based XR tourism.

**Abstract:** This study evaluates the user experience (UX) in extended reality (XR) tourism of two digital twin-based applications: an Augmented Reality Virtual Tour (AR-VT) for enhanced on-site visits and a Virtual Reality Virtual Tour (VR-VT) for remote exploration. Using a quantitative exploratory approach, 84 participants from Spain and Germany, divided into three sample groups, assessed UX, task load, presence, cybersickness, and emotional response through standardized questionnaires. Findings indicate that both applications provided a low task load and high enjoyment. The VR-based tour enhanced presence but posed usability and cybersickness challenges, while the AR-based tour achieved high UX ratings, with qualitative feedback suggesting areas for refinement. Correlation analysis revealed significant relationships between age, prior XR experience, and technological affinity with the measured metrics for both applications. These results highlight the importance of well-designed experiences tailored to XR novices, reinforcing the critical role of UX in digital twin-based XR tourism.

</details>


### [9] [Enhancing Online Learning by Integrating Biosensors and Multimodal Learning Analytics for Detecting and Predicting Student Behavior: A Review](https://arxiv.org/abs/2509.07742)

*Alvaro Becerra, Ruth Cobos, Charles Lang*

**Main category:** cs.HC

**Keywords:** Multimodal Learning Analytics, biosensors, student behavior prediction, adaptive learning

**Relevance Score:** 7

**TL;DR:** This systematic review investigates the use of biosensors and Multimodal Learning Analytics (MmLA) to understand and predict student behavior in online learning environments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance engagement and optimize educational outcomes by understanding and predicting student behavior.

**Method:** A systematic review of 54 studies analyzing the integration of biosensors, emotion and attention detection techniques, and advanced machine learning algorithms.

**Key Contributions:**

	1. Integration of biosensors with traditional learning analytics
	2. Identification of current trends and limitations in the field
	3. Emphasis on the potential for personalized and adaptive online learning experiences

**Result:** The synthesis of findings reveals the potential of combining physiological signals with traditional interaction data to gain insights into cognitive states and improve personalized learning experiences.

**Limitations:** Challenges in emotion detection, experimental design, and demographic factors in data collection.

**Conclusion:** Integrating multimodal data can lead to adaptive learning systems that provide real-time feedback and personalized educational interventions.

**Abstract:** In modern online learning, understanding and predicting student behavior is crucial for enhancing engagement and optimizing educational outcomes. This systematic review explores the integration of biosensors and Multimodal Learning Analytics (MmLA) to analyze and predict student behavior during computer-based learning sessions. We examine key challenges, including emotion and attention detection, behavioral analysis, experimental design, and demographic considerations in data collection. Our study highlights the growing role of physiological signals, such as heart rate, brain activity, and eye-tracking, combined with traditional interaction data and self-reports to gain deeper insights into cognitive states and engagement levels. We synthesize findings from 54 key studies, analyzing commonly used methodologies such as advanced machine learning algorithms and multimodal data pre-processing techniques. The review identifies current research trends, limitations, and emerging directions in the field, emphasizing the transformative potential of biosensor-driven adaptive learning systems. Our findings suggest that integrating multimodal data can facilitate personalized learning experiences, real-time feedback, and intelligent educational interventions, ultimately advancing toward a more customized and adaptive online learning experience.

</details>


### [10] [LLMs in Wikipedia: Investigating How LLMs Impact Participation in Knowledge Communities](https://arxiv.org/abs/2509.07819)

*Moyan Zhou, Soobin Cho, Loren Terveen*

**Main category:** cs.HC

**Keywords:** Large Language Models, Wikipedia, Community Engagement, Knowledge Production, Newcomer Involvement

**Relevance Score:** 9

**TL;DR:** This study investigates how LLMs impact editorial contributions in Wikipedia, focusing on the effects on different editor experience levels and the alignment of LLM outputs with community norms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the integration of LLMs in knowledge communities is essential to shape future norms and support effective usage.

**Method:** Interviews with 16 Wikipedia editors who used LLMs for content contributions.

**Key Contributions:**

	1. Investigation of LLM impact on editorial contributions in Wikipedia
	2. Identification of challenges faced by newcomers in aligning LLM outputs with community norms
	3. Proposed design implications for enhancing community engagement with LLMs

**Result:** LLMs influenced content contributions differently for experienced and new editors; aligning LLM outputs with community norms was challenging for newcomers, and responses to LLM-assisted edits varied based on editor expertise.

**Limitations:** 

**Conclusion:** The findings challenge existing newcomer involvement models and suggest design implications for better supporting community engagement with LLMs.

**Abstract:** Large language models (LLMs) are reshaping knowledge production as community members increasingly incorporate them into their contribution workflows. However, participating in knowledge communities involves more than just contributing content - it is also a deeply social process. While communities must carefully consider appropriate and responsible LLM integration, the absence of concrete norms has left individual editors to experiment and navigate LLM use on their own. Understanding how LLMs influence community participation is therefore critical in shaping future norms and supporting effective adoption. To address this gap, we investigated Wikipedia, one of the largest knowledge production communities, to understand 1) how LLMs influence the ways editors contribute content, 2) what strategies editors leverage to align LLM outputs with community norms, and 3) how other editors in the community respond to LLM-assisted contributions. Through interviews with 16 Wikipedia editors who had used LLMs for their edits, we found that 1) LLMs affected the content contributions for experienced and new editors differently; 2) aligning LLM outputs with community norms required tacit knowledge that often challenged newcomers; and 3) as a result, other editors responded to LLM-assisted edits differently depending on the editors' expertise level. Based on these findings, we challenge existing models of newcomer involvement and propose design implications for LLMs that support community engagement through scaffolding, teaching, and context awareness.

</details>


### [11] [NeuroGaze: A Hybrid EEG and Eye-Tracking Brain-Computer Interface for Hands-Free Interaction in Virtual Reality](https://arxiv.org/abs/2509.07863)

*Kyle Coutray, Wanyea Barbel, Zack Groth, Joseph J LaViola Jr*

**Main category:** cs.HC

**Keywords:** Brain-Computer Interfaces, Virtual Reality, EEG, Eye Tracking, Hands-Free Interaction

**Relevance Score:** 8

**TL;DR:** NeuroGaze, a hybrid interface using EEG and eye tracking, enables hands-free interaction in VR, outperforming traditional input methods in error reduction but facing speed challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the use of Brain-Computer Interfaces (BCIs) in daily activities, particularly in Virtual Reality (VR), leveraging consumer-grade devices for hands-free interaction.

**Method:** A 360° cube-selection task was performed by twenty participants using three input methods: VR controllers, gaze with a pinch gesture, and NeuroGaze; performance metrics included task completion time, error rates, and workload assessments using NASA-TLX.

**Key Contributions:**

	1. Introduction of NeuroGaze as a hybrid EEG and gaze-based interaction method for VR.
	2. Demonstration of fewer selection errors with NeuroGaze compared to traditional VR controllers.
	3. Highlighting the ergonomic advantages and inclusivity potential of BCIs in consumer applications.

**Result:** NeuroGaze demonstrated fewer selection errors compared to VR controllers, but longer completion times, indicating a speed-accuracy tradeoff; it required less physical effort.

**Limitations:** NeuroGaze is not yet competitive in speed when compared to traditional methods and user preferences were mixed regarding overall interactions.

**Conclusion:** NeuroGaze is a feasible solution for hands-free interaction in VR, supporting inclusivity and encouraging further development in EEG signal processing and multimodal integration.

**Abstract:** Brain-Computer Interfaces (BCIs) have traditionally been studied in clinical and laboratory contexts, but the rise of consumer-grade devices now allows exploration of their use in daily activities. Virtual reality (VR) provides a particularly relevant domain, where existing input methods often force trade-offs between speed, accuracy, and physical effort. This study introduces NeuroGaze, a hybrid interface combining electroencephalography (EEG) with eye tracking to enable hands-free interaction in immersive VR. Twenty participants completed a 360{\deg} cube-selection task using three different input methods: VR controllers, gaze combined with a pinch gesture, and NeuroGaze. Performance was measured by task completion time and error rate, while workload was evaluated using the NASA Task Load Index (NASA-TLX). NeuroGaze successfully supported target selection with off-the-shelf hardware, producing fewer errors than the alternative methods but requiring longer completion times, reflecting a classic speed-accuracy tradeoff. Workload analysis indicated reduced physical demand for NeuroGaze compared to controllers, though overall ratings and user preferences were mixed. These findings demonstrate the feasibility of hybrid EEG+gaze systems for everyday VR use, highlighting their ergonomic benefits and inclusivity potential. Although not yet competitive in speed, NeuroGaze points toward a practical role for consumer-grade BCIs in accessibility and long-duration applications, and underscores the need for improved EEG signal processing and adaptive multimodal integration to enhance future performance.

</details>


### [12] [An Enactivist Approach to Human-Computer Interaction: Bridging the Gap Between Human Agency and Affordances](https://arxiv.org/abs/2509.07871)

*Angjelin Hila*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Agency, Neurophenomenology, XR, AI

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel framework for understanding human autonomy in HCI by proposing 'feelings of agency' (FoA) as a measurable construct crucial for design in XR and AI contexts.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for new theoretical frameworks arises from emerging paradigms in XR, AI, and BCI contexts that challenge traditional views of human autonomy and agency in HCI.

**Method:** The authors conceptualize human agents as self-organizing systems and operationalize the construct of 'feelings of agency' (FoA) with two subconstructs linked to neurodynamic indicators and phenomenological reports.

**Key Contributions:**

	1. Introduction of 'feelings of agency' (FoA) as a measurable construct.
	2. Refinement of the multifactorial weighting model using integrated neurodynamic indicators.
	3. Provision of actionable insights for digital affordance design in emerging technologies.

**Result:** The framework offers actionable insights for designing digital affordances in XR, BCI, Human AI Interaction, and generative AI environments, with a focus on enhancing human agency.

**Limitations:** 

**Conclusion:** The proposed neurophenomenological indicators can lead to improved design parameters that enhance human agency in rapidly evolving interactive domains.

**Abstract:** Emerging paradigms in XR, AI, and BCI contexts necessitate novel theoretical frameworks for understanding human autonomy and agency in HCI. Drawing from enactivist theories of cognition, we conceptualize human agents as self-organizing, operationally closed systems that actively enact their cognitive domains through dynamic interaction with their environments. To develop measurable variables aligned with this framework, we introduce "feelings of agency" (FoA) as an alternative to the established construct of "sense of agency" (SoA), refining Synofzyk's multifactorial weighting model and offering a novel conceptual pathway for overcoming gaps in the dominant comparator model. We define FoA as comprising two subconstructs: affective engagement and volitional attention, which we operationalize through integrated neurodynamic indicators (valence, arousal, cross frequency coupling within the dorsal attention system) and first-person phenomenological reports. We argue that these neurophenomenological indicators provide richer, more actionable insights for digital affordance design, particularly in XR, BCI, Human AI Interaction (HAX), and generative AI environments. Our framework aims to inform and inspire design parameters that significantly enhance human agency in rapidly evolving interactive domains.

</details>


### [13] [A Robot That Listens: Enhancing Self-Disclosure and Engagement Through Sentiment-based Backchannels and Active Listening](https://arxiv.org/abs/2509.07873)

*Hieu Tran, Go-Eum Cha, Sooyeon Jeong*

**Main category:** cs.HC

**Keywords:** social robots, active listening, human-robot interaction, backchanneling, self-disclosure

**Relevance Score:** 9

**TL;DR:** This paper explores an LLM-powered social robot that implements active listening and backchanneling behaviors to enhance human-robot communication and rapport.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve social robots' capabilities in engaging in meaningful conversations and establishing rapport through socio-emotional intelligence.

**Method:** An experimental study with sixty-five participants comparing interactions with a robot exhibiting active listening and backchanneling, a control robot, and a backchanneling-only robot.

**Key Contributions:**

	1. Demonstrated the efficacy of active listening behaviors in a social robot context.
	2. Provided empirical evidence linking enhanced robot behaviors to increased self-disclosure by users.
	3. Highlighted the importance of socio-emotional intelligence in human-robot interactions.

**Result:** Participants interacting with the active listening robot reported more positive perceptions, higher self-disclosure, and a stronger sense of being listened to compared to other conditions.

**Limitations:** 

**Conclusion:** Incorporating active listening behaviors in social robots can enhance communication and foster deeper human-robot relationships.

**Abstract:** As social robots get more deeply integrated intoour everyday lives, they will be expected to engage in meaningful conversations and exhibit socio-emotionally intelligent listening behaviors when interacting with people. Active listening and backchanneling could be one way to enhance robots' communicative capabilities and enhance their effectiveness in eliciting deeper self-disclosure, providing a sense of empathy,and forming positive rapport and relationships with people.Thus, we developed an LLM-powered social robot that can exhibit contextually appropriate sentiment-based backchannelingand active listening behaviors (active listening+backchanneling) and compared its efficacy in eliciting people's self-disclosurein comparison to robots that do not exhibit any of these listening behaviors (control) and a robot that only exhibitsbackchanneling behavior (backchanneling-only). Through ourexperimental study with sixty-five participants, we found theparticipants who conversed with the active listening robot per-ceived the interactions more positively, in which they exhibited the highest self-disclosures, and reported the strongest senseof being listened to. The results of our study suggest that the implementation of active listening behaviors in social robotshas the potential to improve human-robot communication andcould further contribute to the building of deeper human-robot relationships and rapport.

</details>


### [14] [dciWebMapper2: Enhancing the dciWebMapper framework toward integrated, interactive visualization of linked multi-type maps, charts, and spatial statistics and analysis](https://arxiv.org/abs/2509.07897)

*Sarigai Sarigai, Liping Yang, Katie Slack, Carolyn Fish, Michaela Buenemann, Qiusheng Wu, Yan Lin, Joseph A. Cook, David Jacobs*

**Main category:** cs.HC

**Keywords:** geovisualization, open-source, spatial analysis, interactive maps, cartography

**Relevance Score:** 4

**TL;DR:** This paper presents dciWebMapper2, an open-source framework for interactive geovisualization that supports dynamic spatial analysis across various domains, including climate justice and food access.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for open-source frameworks in interactive geovisualization has increased due to its importance in various disciplines and the necessity for dynamic, multi-attribute spatial analysis and accessible design.

**Method:** The dciWebMapper2 framework incorporates different map types (choropleth, proportional symbol, small multiples, heatmaps) and linked statistical charts along with dropdown-based controls for high-dimensional comparisons, all within a coordinated-view environment.

**Key Contributions:**

	1. Introduction of dciWebMapper2 framework for dynamic geovisualization.
	2. Integration of multiple map types and statistical charts in a single environment.
	3. Open-source and server-free design promoting modularity and reproducibility.

**Result:** The framework's applicability is demonstrated through three use cases that highlight its adaptability for exploratory analysis and its potential to enhance inclusive spatial storytelling in research and civic engagement.

**Limitations:** 

**Conclusion:** dciWebMapper2 offers a versatile foundation for transparent geospatial analysis, promoting reproducibility and sustainable practices in cartography.

**Abstract:** As interactive web-based geovisualization becomes increasingly vital across disciplines, there is a growing need for open-source frameworks that support dynamic, multi-attribute spatial analysis and accessible design. This paper introduces dciWebMapper2, a significant expansion of the original dciWebMapper framework, designed to enable exploratory analysis across domains such as climate justice, food access, and social vulnerability. The enhanced framework integrates multiple map types, including choropleth, proportional symbol, small multiples, and heatmaps, with linked statistical charts (e.g., scatter plots, boxplots) and time sliders, all within a coordinated-view environment. Dropdown-based controls allow flexible, high-dimensional comparisons while maintaining visual clarity. Grounded in cartographic and information visualization principles, dciWebMapper2 is fully open-source, self-contained, and server-free, supporting modularity, reproducibility, and long-term sustainability. Three applied use cases demonstrate its adaptability and potential to democratize interactive web cartography. This work offers a versatile foundation for inclusive spatial storytelling and transparent geospatial analysis in research, education, and civic engagement.

</details>


### [15] [Knowledge Isn't Power: The Ethics of Social Robots and the Difficulty of Informed Consent](https://arxiv.org/abs/2509.07942)

*James M. Berzuk, Lauren Corcoran, Brannen McKenzie-Lefurgey, Katie Szilagyi, James E. Young*

**Main category:** cs.HC

**Keywords:** social robots, human-robot interaction, informed consent, ethical implications, design goals

**Relevance Score:** 8

**TL;DR:** The paper examines the ethical implications of social robots mimicking human behaviors and their impact on user interactions and informed consent.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To address the ethical challenges posed by social robots that mimic human social behaviors, and investigate the implications for informed consent in human-robot interactions.

**Method:** The authors analyze legal principles and ethical consideration surrounding informed consent, applying them to the context of human-robot interaction.

**Key Contributions:**

	1. Examines the impact of social robot behaviors on user interactions and emotional responses.
	2. Applies legal principles of informed consent to the context of social robotics.
	3. Synthesis of design goals aimed at improving ethical standards in human-robot interaction.

**Result:** The paper identifies how social robots can influence user behavior and highlights the differences in consent dynamics compared to human-human interactions.

**Limitations:** The study may not encompass all diverse interaction contexts and could be limited by rapidly advancing technology in robotics.

**Conclusion:** The study proposes design goals for developers of social robots to ensure more ethical interactions and informed consent from users.

**Abstract:** Contemporary robots are increasingly mimicking human social behaviours to facilitate interaction, such as smiling to signal approachability, or hesitating before taking an action to allow people time to react. Such techniques can activate a person's entrenched social instincts, triggering emotional responses as though they are interacting with a fellow human, and can prompt them to treat a robot as if it truly possesses the underlying life-like processes it outwardly presents, raising significant ethical questions. We engage these issues through the lens of informed consent: drawing upon prevailing legal principles and ethics, we examine how social robots can influence user behaviour in novel ways, and whether under those circumstances users can be appropriately informed to consent to these heightened interactions. We explore the complex circumstances of human-robot interaction and highlight how it differs from more familiar interaction contexts, and we apply legal principles relating to informed consent to social robots in order to reconceptualize the current ethical debates surrounding the field. From this investigation, we synthesize design goals for robot developers to achieve more ethical and informed human-robot interaction.

</details>


### [16] [Affordances and Design Principles of The Political Left and Right](https://arxiv.org/abs/2411.02088)

*Felix Anand Epp, Jesse Haapoja, Matti Nelimarkka*

**Main category:** cs.HC

**Keywords:** social media, political ideology, co-design, human-computer interaction, technology values

**Relevance Score:** 6

**TL;DR:** The paper investigates how political ideology influences the design of social media services through co-design workshops with political representatives in Finland.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how societal values, particularly political ideology, influence social media service design.

**Method:** Conducted four co-design workshops with political representatives divided by ideology (right-leaning, left-leaning, and mixed) to generate early-stage design artefacts.

**Key Contributions:**

	1. Examined the influence of political ideologies on social media design
	2. Contrasted design preferences of different political groups
	3. Highlighted the need for inclusion of political values in technology design

**Result:** Identified significant differences in social media features preferred by different political groups, with right-leaning groups favoring market-based visibility and left-leaning groups advocating for more open profiles.

**Limitations:** Study is limited to a specific context (Finland) and may not generalize to all cultures or political systems.

**Conclusion:** Political ideologies have a clear impact on the design of social computing systems, necessitating further research to explore this relationship.

**Abstract:** Like any form of technology, social media services embed values. To examine how societal values may be present in these systems, we focus on exploring political ideology as a value system. We organised four co-design workshops with political representatives from five major parties in Finland to investigate what values they would incorporate into social media services. The participants were divided into one right-leaning group, two left-leaning groups, and one mixed group. This approach allows us to examine the differences in social media services designed by groups with different political ideologies i.e., value systems. We analysed produced artefacts (early-stage paper mockups) to identify different features and affordances for each group and then contrasted the ideological compositions. Our results revealed a clear distinction between groups: the right-leaning group favoured market-based visibility, while left-leaning groups rejected such design principles in favour of open profile work. Additionally, we found tentative differences in design outcomes along the liberal--conservative dimension. These findings underscore the importance of acknowledging existing political value systems in the design of social computing systems. They also highlight the need for further research to map out political ideologies in technology design.

</details>


### [17] [Generative AI as a Tool for Enhancing Reflective Learning in Students](https://arxiv.org/abs/2412.02603)

*Bo Yuan, Jiazi Hu*

**Main category:** cs.HC

**Keywords:** Generative AI, Large Language Models, Reflective Learning, Intelligent Tutoring Systems, Project-Based Learning

**Relevance Score:** 9

**TL;DR:** This study explores the use of generative AI, particularly LLMs, to enhance reflective practice in education by providing scalable, personalized feedback.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional methods of fostering reflective skills, which include scalability issues and challenges in providing individualized feedback.

**Method:** Utilized large language models (LLMs) for prompt engineering and simulated multi-turn dialogues in a project-based learning context to analyze their effectiveness as facilitators of reflective exercises.

**Key Contributions:**

	1. Introduces LLMs as facilitators of reflective learning exercises
	2. Demonstrates effectiveness of prompt engineering for personalized feedback
	3. Evaluates performance of tutors and students using LLM-assisted assessment

**Result:** Found that LLMs, when paired with pedagogically aligned prompts, can enhance engagement and critical thinking, serving as effective tools for scalable reflective guidance.

**Limitations:** 

**Conclusion:** This research contributes to the understanding of AI in reflective pedagogy and opens up new avenues for AI-driven intelligent tutoring systems.

**Abstract:** Reflection is widely recognized as a cornerstone of student development, fostering critical thinking, self-regulation, and deep conceptual understanding. Traditionally, reflective skills have been cultivated through structured feedback, mentorship, and guided self-assessment. However, these approaches often face challenges such as limited scalability, difficulties in delivering individualized feedback, and a shortage of instructors proficient in facilitating meaningful reflection. This study pioneers the use of generative AI, specifically large language models (LLMs), as an innovative solution to these limitations. By leveraging the capacity of LLMs to deliver personalized, context-sensitive feedback at scale, this research investigates their potential to serve as effective facilitators of reflective exercises, sustaining deep engagement and promoting critical thinking. Through in-depth analyses of prompt engineering strategies and simulated multi-turn dialogues grounded in a project-based learning (PBL) context, the study demonstrates that, with pedagogically aligned prompts, LLMs can serve as accessible and adaptive tools for scalable reflective guidance. Furthermore, LLM-assisted evaluation is employed to objectively assess the performance of both tutors and students across multiple dimensions of reflective learning. The findings contribute to the evolving understanding of AI's role in reflective pedagogy and point to new opportunities for advancing AI-driven intelligent tutoring systems.

</details>


### [18] [VoxCity: A Seamless Framework for Open Geospatial Data Integration, Grid-Based Semantic 3D City Model Generation, and Urban Environment Simulation](https://arxiv.org/abs/2504.13934)

*Kunihiko Fujiwara, Ryuta Tsurumi, Tomoki Kiyono, Zicheng Fan, Xiucheng Liang, Binyu Lei, Winston Yap, Koichi Ito, Filip Biljecki*

**Main category:** cs.HC

**Keywords:** 3D city model, urban simulation, environmental analysis, geospatial data, Python package

**Relevance Score:** 4

**TL;DR:** VoxCity is an open-source Python package for grid-based 3D city model generation and environmental simulation, facilitating urban planning by automating data collection and simulation processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the manual effort in preparing 3D city models for urban environment simulations, thereby promoting their widespread use in informed urban planning.

**Method:** VoxCity integrates a generator that collects 3D data of urban features and voxelizes them into a city model, and a simulator that conducts various environmental analyses like solar radiation and view index calculations.

**Key Contributions:**

	1. Development of an automated generator for voxel city model creation
	2. Integration of environmental simulations within the package
	3. Provision of a comprehensive guideline for selecting geospatial data

**Result:** VoxCity successfully generated 3D models for eight cities, demonstrating simulations for solar irradiance and microclimate visualization, while allowing exports to multiple formats.

**Limitations:** 

**Conclusion:** VoxCity can significantly streamline the city modeling process and enhance urban design efforts by providing vital simulation tools and guidelines for data selection.

**Abstract:** Three-dimensional urban environment simulation is a powerful tool for informed urban planning. However, the intensive manual effort required to prepare input 3D city models has hindered its widespread adoption. To address this challenge, we present VoxCity, an open-source Python package that provides a one-stop solution for grid-based 3D city model generation and urban environment simulation for cities worldwide. VoxCity's `generator' subpackage automatically downloads building heights, tree canopy heights, land cover, and terrain elevation within a specified target area, and voxelizes buildings, trees, land cover, and terrain to generate an integrated voxel city model. The `simulator' subpackage enables users to conduct environmental simulations, including solar radiation and view index analyses. Users can export the generated models using several file formats compatible with external software, such as ENVI-met (INX), Blender, and Rhino (OBJ). We generated 3D city models for eight global cities, and demonstrated the calculation of solar irradiance, sky view index, and green view index. We also showcased microclimate simulation and 3D rendering visualization through ENVI-met and Rhino, respectively, through the file export function. Additionally, we reviewed openly available geospatial data to create guidelines to help users choose appropriate data sources depending on their target areas and purposes. VoxCity can significantly reduce the effort and time required for 3D city model preparation and promote the utilization of urban environment simulations. This contributes to more informed urban and architectural design that considers environmental impacts, and in turn, fosters sustainable and livable cities. VoxCity is released openly at https://github.com/kunifujiwara/VoxCity.

</details>


### [19] [Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data](https://arxiv.org/abs/2509.07202)

*Khushiyant*

**Main category:** cs.HC

**Keywords:** EEG, text generation, large language models, brain-computer interfaces, assistive technologies

**Relevance Score:** 9

**TL;DR:** This paper presents a new method that combines a large language model (LLM) with a classifier-LLM architecture to facilitate EEG-based text production, significantly reducing data and compute requirements while improving performance by 10%.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in EEG-based text generation which traditionally require substantial data and processing power.

**Method:** The study introduces a new architecture that uses the Gemma 2B LLM alongside a classifier-LLM architecture with a Recurrent Neural Network (RNN) encoder.

**Key Contributions:**

	1. Introduction of a novel LLM-driven approach for EEG-based text generation.
	2. Performance improvement of 10% in text production over existing methods.
	3. Demonstration of effective transfer learning in low-data scenarios.

**Result:** The proposed method achieves a 10% performance improvement over existing methodologies while greatly reducing the data and computing power needed for EEG-based text generation.

**Limitations:** 

**Conclusion:** The integration of LLMs with EEG decoding significantly enhances assistive technologies, improving independence and communication for users with severe motor limitations and setting a new direction for research in brain-computer interfaces.

**Abstract:** Text generating capabilities have undergone a substantial transformation with the introduction of large language models (LLMs). Electroencephalography (EEG)-based text production is still difficult, though, because it requires a lot of data and processing power. This paper introduces a new method that combines the use of the Gemma 2B LLM with a classifier-LLM architecture to incorporate a Recurrent Neural Network (RNN) encoder. Our approach drastically lowers the amount of data and compute power needed while achieving performance close to that of cutting-edge methods. Notably, compared to current methodologies, our methodology delivers an overall performance improvement of 10%. The suggested architecture demonstrates the possibility of effective transfer learning for EEG-based text production, remaining strong and functional even in the face of data limits. This work highlights the potential of integrating LLMs with EEG decoding to improve assistive technologies and improve independence and communication for those with severe motor limitations. Our method pushes the limits of present capabilities and opens new paths for research and application in brain-computer interfaces by efficiently using the strengths of pre-trained language models. This makes EEG-based text production more accessible and efficient.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [20] [MedBench-IT: A Comprehensive Benchmark for Evaluating Large Language Models on Italian Medical Entrance Examinations](https://arxiv.org/abs/2509.07135)

*Ruggero Marino Lazzaroni, Alessandro Angioi, Michelangelo Puliga, Davide Sanna, Roberto Marras*

**Main category:** cs.CL

**Keywords:** Large Language Models, MedBench-IT, Italian Medical Exams, NLP, Benchmarking

**Relevance Score:** 8

**TL;DR:** MedBench-IT is a benchmark for evaluating LLMs on Italian medical entrance exams, comprising 17,410 expert-written questions across six subjects.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of benchmarks for non-English LLMs in specialized domains, particularly in education, necessitates the creation of resources like MedBench-IT.

**Method:** The benchmark includes multiple-choice questions and evaluates various LLMs for accuracy and practicality, alongside reproducibility and bias analyses.

**Key Contributions:**

	1. Introduction of the first benchmark for Italian medical exams (MedBench-IT)
	2. Evaluation of various LLMs including proprietary and open-source models
	3. Analysis of reasoning prompt effectiveness and question readability correlations.

**Result:** The study found an 88.86% response consistency across subjects and a minimal impact from ordering bias, highlighting the challenges in LLM performance evaluation.

**Limitations:** Focus is on Italian language; results may not generalize to other languages or domains.

**Conclusion:** MedBench-IT serves as a key resource for the Italian NLP community and EdTech developers, contributing to the understanding of LLM capabilities in specialized educational contexts.

**Abstract:** Large language models (LLMs) show increasing potential in education, yet benchmarks for non-English languages in specialized domains remain scarce. We introduce MedBench-IT, the first comprehensive benchmark for evaluating LLMs on Italian medical university entrance examinations. Sourced from Edizioni Simone, a leading preparatory materials publisher, MedBench-IT comprises 17,410 expert-written multiple-choice questions across six subjects (Biology, Chemistry, Logic, General Culture, Mathematics, Physics) and three difficulty levels. We evaluated diverse models including proprietary LLMs (GPT-4o, Claude series) and resource-efficient open-source alternatives (<30B parameters) focusing on practical deployability.   Beyond accuracy, we conducted rigorous reproducibility tests (88.86% response consistency, varying by subject), ordering bias analysis (minimal impact), and reasoning prompt evaluation. We also examined correlations between question readability and model performance, finding a statistically significant but small inverse relationship. MedBench-IT provides a crucial resource for Italian NLP community, EdTech developers, and practitioners, offering insights into current capabilities and standardized evaluation methodology for this critical domain.

</details>


### [21] [The ML-SUPERB 2.0 Challenge: Towards Inclusive ASR Benchmarking for All Language Varieties](https://arxiv.org/abs/2509.07139)

*William Chen, Chutong Meng, Jiatong Shi, Martijn Bartelds, Shih-Heng Wang, Hsiu-Hsuan Wang, Rafael Mosquera, Sara Hincapie, Dan Jurafsky, Antonis Anastasopoulos, Hung-yi Lee, Karen Livescu, Shinji Watanabe*

**Main category:** cs.CL

**Keywords:** speech recognition, multilingual, ASR, community challenge, DynaBench

**Relevance Score:** 7

**TL;DR:** The Interspeech 2025 ML-SUPERB 2.0 Challenge evaluates state-of-the-art multilingual ASR models across 200+ languages, accents, and dialects, demonstrating significant improvements in speech technology inclusivity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unequal distribution of advancements in multilingual automatic speech recognition (ASR) across various languages and variations.

**Method:** A new test suite was constructed with data from over 200 languages, accents, and dialects, and an online evaluation server was introduced for flexible model designs in the challenge.

**Key Contributions:**

	1. Creation of a comprehensive test suite for multilingual ASR
	2. Successful improvements over baseline ASR models
	3. Emphasis on community-driven solutions for inclusive speech technology

**Result:** Five submissions from three teams surpassed baseline performances; the top submission improved language identification (LID) accuracy by 23% and reduced character error rate (CER) by 18% on a general test set, with even greater improvements on accented and dialectal data.

**Limitations:** 

**Conclusion:** Community challenges play a crucial role in enhancing the inclusivity of speech technologies by fostering innovation and performance improvements among diverse model submissions.

**Abstract:** Recent improvements in multilingual ASR have not been equally distributed across languages and language varieties. To advance state-of-the-art (SOTA) ASR models, we present the Interspeech 2025 ML-SUPERB 2.0 Challenge. We construct a new test suite that consists of data from 200+ languages, accents, and dialects to evaluate SOTA multilingual speech models. The challenge also introduces an online evaluation server based on DynaBench, allowing for flexibility in model design and architecture for participants. The challenge received 5 submissions from 3 teams, all of which outperformed our baselines. The best-performing submission achieved an absolute improvement in LID accuracy of 23% and a reduction in CER of 18% when compared to the best baseline on a general multilingual test set. On accented and dialectal data, the best submission obtained 30.2% lower CER and 15.7% higher LID accuracy, showing the importance of community challenges in making speech technologies more inclusive.

</details>


### [22] [Toward Purpose-oriented Topic Model Evaluation enabled by Large Language Models](https://arxiv.org/abs/2509.07142)

*Zhiyin Tan, Jennifer D'Souza*

**Main category:** cs.CL

**Keywords:** Topic Modeling, Large Language Models, Evaluation Framework, Digital Libraries, Topic Quality

**Relevance Score:** 8

**TL;DR:** This paper proposes a framework using Large Language Models for evaluating topic models, addressing limitations of traditional metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of topic models in digital libraries by addressing the shortcomings of conventional metrics.

**Method:** The framework employs nine LLM-based metrics across four dimensions of topic quality, validated through various protocols on diverse datasets.

**Key Contributions:**

	1. Introduction of a purpose-oriented evaluation framework for topic models using LLMs
	2. Identification of critical weaknesses in traditional topic model evaluations
	3. Accessibility of all code and data for further research

**Result:** The LLM-based metrics revealed critical weaknesses in topic models, such as redundancy and semantic drift, which traditional metrics missed.

**Limitations:** 

**Conclusion:** The framework offers robust, interpretable assessments and encourages the development of more effective evaluation tools for dynamic datasets.

**Abstract:** This study presents a framework for automated evaluation of dynamically evolving topic models using Large Language Models (LLMs). Topic modeling is essential for organizing and retrieving scholarly content in digital library systems, helping users navigate complex and evolving knowledge domains. However, widely used automated metrics, such as coherence and diversity, often capture only narrow statistical patterns and fail to explain semantic failures in practice. We introduce a purpose-oriented evaluation framework that employs nine LLM-based metrics spanning four key dimensions of topic quality: lexical validity, intra-topic semantic soundness, inter-topic structural soundness, and document-topic alignment soundness. The framework is validated through adversarial and sampling-based protocols, and is applied across datasets spanning news articles, scholarly publications, and social media posts, as well as multiple topic modeling methods and open-source LLMs. Our analysis shows that LLM-based metrics provide interpretable, robust, and task-relevant assessments, uncovering critical weaknesses in topic models such as redundancy and semantic drift, which are often missed by traditional metrics. These results support the development of scalable, fine-grained evaluation tools for maintaining topic relevance in dynamic datasets. All code and data supporting this work are accessible at https://github.com/zhiyintan/topic-model-LLMjudgment.

</details>


### [23] [Towards EnergyGPT: A Large Language Model Specialized for the Energy Sector](https://arxiv.org/abs/2509.07177)

*Amal Chebbi, Babajide Kolade*

**Main category:** cs.CL

**Keywords:** EnergyGPT, domain-specific language model, machine learning, energy sector, LLM

**Relevance Score:** 4

**TL;DR:** EnergyGPT is a specialized LLM for the energy sector, outperforming its base model in energy-related tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of language models in specialized fields like energy that require deep technical expertise.

**Method:** Fine-tuning LLaMA 3.1-8B model with a curated corpus of energy-related texts through a comprehensive development pipeline.

**Key Contributions:**

	1. Introduction of EnergyGPT, a specialized model for the energy sector
	2. Development pipeline from data curation to deployment
	3. Benchmarking results demonstrating superior performance over base model

**Result:** EnergyGPT shows improved performance on domain-specific question-answering benchmarks compared to the base model.

**Limitations:** 

**Conclusion:** The training strategy allows for improved domain relevance and performance without extensive infrastructure.

**Abstract:** Large Language Models have demonstrated impressive capabilities across various domains. However, their general-purpose nature often limits their effectiveness in specialized fields such as energy, where deep technical expertise and precise domain knowledge are essential. In this paper, we introduce EnergyGPT, a domain-specialized language model tailored for the energy sector, developed by fine-tuning LLaMA 3.1-8B model using Supervised Fine-Tuning on a high-quality, curated corpus of energy-related texts. We present a complete development pipeline, including data collection and curation, model fine-tuning, benchmark design and LLM-judge choice, evaluation and deployment. Through this work, we demonstrate that our training strategy enables improvements in domain relevance and performance without the need for large-scale infrastructure. By evaluating the performance of the model using domain-specific question-answering benchmarks, our results demonstrate that EnergyGPT outperforms the base model in most of the energy-related language understanding and generation tasks.

</details>


### [24] [DischargeSim: A Simulation Benchmark for Educational Doctor-Patient Communication at Discharge](https://arxiv.org/abs/2509.07188)

*Zonghai Yao, Michael Sun, Won Seok Jang, Sunjae Kwon, Soie Kwon, Hong Yu*

**Main category:** cs.CL

**Keywords:** discharge communication, large language models, patient education

**Relevance Score:** 9

**TL;DR:** DischargeSim benchmarks LLMs for post-visit patient education by simulating conversations between DoctorAgents and PatientAgents with various profiles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of assessment of LLMs in supporting patients after medical visits and improve discharge communication.

**Method:** DischargeSim simulates multi-turn dialogues between LLM-driven DoctorAgents and PatientAgents, structured around six discharge topics and evaluated on dialogue quality, document generation, and patient comprehension.

**Key Contributions:**

	1. Introduction of DischargeSim, a benchmark for LLMs
	2. Evaluation of patient education across diverse psychosocial profiles
	3. Highlighting performance variations unrelated to model size

**Result:** Experiments show significant gaps in LLM performance for discharge education, varying with patient profiles; larger models do not guarantee better outcomes.

**Limitations:** 

**Conclusion:** DischargeSim is a first step towards better evaluating LLMs for personalized post-visit clinical education.

**Abstract:** Discharge communication is a critical yet underexplored component of patient care, where the goal shifts from diagnosis to education. While recent large language model (LLM) benchmarks emphasize in-visit diagnostic reasoning, they fail to evaluate models' ability to support patients after the visit. We introduce DischargeSim, a novel benchmark that evaluates LLMs on their ability to act as personalized discharge educators. DischargeSim simulates post-visit, multi-turn conversations between LLM-driven DoctorAgents and PatientAgents with diverse psychosocial profiles (e.g., health literacy, education, emotion). Interactions are structured across six clinically grounded discharge topics and assessed along three axes: (1) dialogue quality via automatic and LLM-as-judge evaluation, (2) personalized document generation including free-text summaries and structured AHRQ checklists, and (3) patient comprehension through a downstream multiple-choice exam. Experiments across 18 LLMs reveal significant gaps in discharge education capability, with performance varying widely across patient profiles. Notably, model size does not always yield better education outcomes, highlighting trade-offs in strategy use and content prioritization. DischargeSim offers a first step toward benchmarking LLMs in post-visit clinical education and promoting equitable, personalized patient support.

</details>


### [25] [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)

*Zahra Atf, Peter R Lewis*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Management, Moral Reasoning

**Relevance Score:** 8

**TL;DR:** This paper proposes a framework incorporating rule-based moral principles to handle uncertainty in LLM-generated text, improving trust and interpretability in high-stakes settings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for transparency in uncertainty management for LLMs in high-stakes environments such as clinical and legal settings.

**Method:** A framework based on moral psychology and virtue ethics is implemented using a lightweight Prolog engine to guide response actions under uncertainty.

**Key Contributions:**

	1. Framework integrates moral principles for handling uncertainty
	2. Use of Prolog engine for rule encoding
	3. Illustration of use cases that improve trust and interpretability

**Result:** The approach demonstrates better rule coverage, fairness, and trust calibration in simulations, and highlights use cases where moral reasoning enhances interpretation.

**Limitations:** 

**Conclusion:** The proposed method offers a socially responsible and transparent alternative to existing probabilistic models for natural language generation.

**Abstract:** Large language models (LLMs) are increasingly used in high-stakes settings, where explaining uncertainty is both technical and ethical. Probabilistic methods are often opaque and misaligned with expectations of transparency. We propose a framework based on rule-based moral principles for handling uncertainty in LLM-generated text. Using insights from moral psychology and virtue ethics, we define rules such as precaution, deference, and responsibility to guide responses under epistemic or aleatoric uncertainty. These rules are encoded in a lightweight Prolog engine, where uncertainty levels (low, medium, high) trigger aligned system actions with plain-language rationales. Scenario-based simulations benchmark rule coverage, fairness, and trust calibration. Use cases in clinical and legal domains illustrate how moral reasoning can improve trust and interpretability. Our approach offers a transparent, lightweight alternative to probabilistic models for socially responsible natural language generation.

</details>


### [26] [LLM Analysis of 150+ years of German Parliamentary Debates on Migration Reveals Shift from Post-War Solidarity to Anti-Solidarity in the Last Decade](https://arxiv.org/abs/2509.07274)

*Aida Kostikova, Ole Pütz, Steffen Eger, Olga Sabelfeld, Benjamin Paassen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Political Text Analysis, Migration Studies

**Relevance Score:** 6

**TL;DR:** This paper evaluates the use of large language models (LLMs) in automating the annotation of political speech related to migration in Germany, comparing LLM-generated annotations with extensive human references. It unveils significant trends in (anti-)solidarity towards migrants in parliamentary debates over time.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of LLMs in automating the annotation process for analyzing political discourse on migration, an area traditionally relying on manual effort.

**Method:** The study evaluates multiple LLMs in annotating (anti-)solidarity subtypes in German parliamentary debates through comparison against thousands of human annotations, assessing factors like model size, prompting techniques, and fine-tuning.

**Key Contributions:**

	1. Evaluation of LLMs for political text annotation
	2. Insights into historical and contemporary solidarity trends
	3. Methodological framework for analyzing political speech using LLMs

**Result:** Findings highlight a significant presence of migrant-directed solidarity in the post-World War II period, contrasted by a rising trend of anti-solidarity in German parliament since 2015, providing valuable insights into evolving public sentiment.

**Limitations:** Potential systematic errors in model annotations and reliance on historical context could influence findings.

**Conclusion:** The research emphasizes the potential of LLMs for facilitating political text analysis while underlining the critical nature of migration debates in Germany amid demographic challenges and polarization.

**Abstract:** Migration has been a core topic in German political debate, from millions of expellees post World War II over labor migration to refugee movements in the recent past. Studying political speech regarding such wide-ranging phenomena in depth traditionally required extensive manual annotations, limiting the scope of analysis to small subsets of the data. Large language models (LLMs) have the potential to partially automate even complex annotation tasks. We provide an extensive evaluation of a multiple LLMs in annotating (anti-)solidarity subtypes in German parliamentary debates compared to a large set of thousands of human reference annotations (gathered over a year). We evaluate the influence of model size, prompting differences, fine-tuning, historical versus contemporary data; and we investigate systematic errors. Beyond methodological evaluation, we also interpret the resulting annotations from a social science lense, gaining deeper insight into (anti-)solidarity trends towards migrants in the German post-World War II period and recent past. Our data reveals a high degree of migrant-directed solidarity in the postwar period, as well as a strong trend towards anti-solidarity in the German parliament since 2015, motivating further research. These findings highlight the promise of LLMs for political text analysis and the importance of migration debates in Germany, where demographic decline and labor shortages coexist with rising polarization.

</details>


### [27] [Causal Attention with Lookahead Keys](https://arxiv.org/abs/2509.07301)

*Zhuoqing Song, Peng Sun, Huizhuo Yuan, Quanquan Gu*

**Main category:** cs.CL

**Keywords:** Causal attention, lookahead keys, language modeling, autoregressive models, machine learning

**Relevance Score:** 8

**TL;DR:** CASTLE introduces an adaptive attention mechanism that updates keys based on upcoming context while preserving autoregressive properties, leading to improved language modeling performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Standard causal attention uses static QKV encodings that limit the integration of future contextual information, necessitating a more dynamic approach to information processing in autoregressive models.

**Method:** CASTLE employs dynamically updated lookahead keys that leverage information from future tokens relative to their preceding positions without violating the sequential processing required in autoregressive settings.

**Key Contributions:**

	1. Introduction of lookahead keys in causal attention
	2. Mathematical framework for efficient parallel processing
	3. Demonstrated improvement in language modeling tasks and perplexity metrics

**Result:** CASTLE demonstrates superior performance over traditional causal attention across various language modeling benchmarks, indicated by reduced validation perplexity and enhanced downstream task outcomes.

**Limitations:** 

**Conclusion:** The proposed CASTLE mechanism provides an efficient alternative to standard causal attention, facilitating better parallel training and improved model performance in language tasks.

**Abstract:** In standard causal attention, each token's query, key, and value (QKV) are static and encode only preceding context. We introduce CAuSal aTtention with Lookahead kEys (CASTLE), an attention mechanism that continually updates each token's keys as the context unfolds. We term these updated keys lookahead keys because they belong to earlier positions yet integrate information from tokens that appear later relative to those positions, while strictly preserving the autoregressive property. Although the mechanism appears sequential, we derive a mathematical equivalence that avoids explicitly materializing lookahead keys at each position and enables efficient parallel training. On language modeling benchmarks, CASTLE consistently outperforms standard causal attention across model scales, reducing validation perplexity and improving performance on a range of downstream tasks.

</details>


### [28] [Basis Vector Metric: A Method for Robust Open-Ended State Change Detection](https://arxiv.org/abs/2509.07308)

*David Oprea, Sam Powers*

**Main category:** cs.CL

**Keywords:** Basis Vectors Method, image state classification, language embeddings

**Relevance Score:** 4

**TL;DR:** This paper introduces the Basis Vectors Method (BVM) for assessing image state changes using language embeddings, showing promising results in classifying nouns but inconclusive outcomes in differentiating adjectives compared to logistic regression.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve the classification of state changes in images through a new method called BVM, leveraging the MIT-States dataset.

**Method:** The authors utilized the MIT-States dataset and compared BVM against various metrics like cosine similarity, dot product, and logistic regression in two experiments: one for nouns and another for adjectives.

**Key Contributions:**

	1. Introduction of the Basis Vectors Method (BVM) for image state classification
	2. Performance comparison of BVM with existing metrics like cosine similarity and logistic regression
	3. Identification of areas for methodological improvements to increase accuracy

**Result:** BVM outperformed traditional metrics in classifying noun states but did not conclusively outperform logistic regression in distinguishing between adjectives.

**Limitations:** Inconclusive evidence of BVM's superiority in differentiating adjectives compared to logistic regression and need for methodological refinements.

**Conclusion:** While BVM shows potential for noun classification, further refinements are necessary to enhance its accuracy, especially in differentiating adjectives.

**Abstract:** We test a new method, which we will abbreviate using the acronym BVM (Basis Vectors Method), in its ability to judge the state changes in images through using language embeddings. We used the MIT-States dataset, containing about 53,000 images, to gather all of our data, which has 225 nouns and 115 adjectives, with each noun having about 9 different adjectives, forming approximately 1000 noun-adjective pairs. For our first experiment, we test our method's ability to determine the state of each noun class separately against other metrics for comparison. These metrics are cosine similarity, dot product, product quantization, binary index, Naive Bayes, and a custom neural network. Among these metrics, we found that our proposed BVM performs the best in classifying the states for each noun. We then perform a second experiment where we try using BVM to determine if it can differentiate adjectives from one another for each adjective separately. We compared the abilities of BVM to differentiate adjectives against the proposed method the MIT-States paper suggests: using a logistic regression model. In the end, we did not find conclusive evidence that our BVM metric could perform better than the logistic regression model at discerning adjectives. Yet, we were able to find evidence for possible improvements to our method; this leads to the chance of increasing our method's accuracy through certain changes in our methodologies.

</details>


### [29] [Instance-level Performance Prediction for Long-form Generation Tasks](https://arxiv.org/abs/2509.07309)

*Chi-Yang Hsu, Alexander Braylan, Yiheng Su, Omar Alonso, Matthew Lease*

**Main category:** cs.CL

**Keywords:** Performance prediction, Long-form generation, Quality metrics

**Relevance Score:** 7

**TL;DR:** This paper presents a benchmark for predicting performance in long-form generation tasks using a task-agnostic approach to evaluate model quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a standardized method to evaluate long-form generation tasks, particularly with regards to predicting quality metrics.

**Method:** The paper proposes a task-, model-, and metric-agnostic framework that predicts continuous evaluation metric scores based on black-box model inputs and outputs, while also inferring prediction intervals for uncertainty quantification.

**Key Contributions:**

	1. Introduction of a new benchmark for instance-level performance prediction in long-form generation tasks.
	2. A task-, model-, and metric-agnostic approach to predict evaluation scores.
	3. Demonstration of effective score prediction with minimal training examples.

**Result:** Demonstrates that model scores can be predicted effectively across various long-form tasks using only a small amount of training data (16 examples).

**Limitations:** 

**Conclusion:** The introduced benchmark is novel, valuable for advancing research, and provides ready-to-use baselines for practical application.

**Abstract:** We motivate and share a new benchmark for instance-level performance prediction of long-form generation tasks having multi-faceted, fine-grained quality metrics. Our task-, model- and metric-agnostic formulation predicts continuous evaluation metric scores given only black-box model inputs and outputs. Beyond predicting point estimates of metric scores, the benchmark also requires inferring prediction intervals to quantify uncertainty around point estimates. Evaluation spans 11 long-form datasets/tasks with multiple LLMs, baselines, and metrics per task. We show that scores can be effectively predicted across long-form generation tasks using as few as 16 training examples. Overall, we introduce a novel and useful task, a valuable benchmark to drive progress, and baselines ready for practical adoption today.

</details>


### [30] [Does This Look Familiar to You? Knowledge Analysis via Model Internal Representations](https://arxiv.org/abs/2509.07311)

*Sihyun Park*

**Main category:** cs.CL

**Keywords:** large language models, training data selection, machine learning, internal representations, generalization

**Relevance Score:** 9

**TL;DR:** This study introduces KAMIR, a method for selecting training data based on analyzing model internal representations to enhance performance in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effective training data selection is essential for improving the performance of language models, especially given the limitations of current methods reliant on prompt engineering.

**Method:** KAMIR analyzes similarities between the hidden states across model layers and the final hidden states to select training data based on the model's internal familiarity with inputs.

**Key Contributions:**

	1. Introduction of KAMIR for effective training data selection
	2. Ability to analyze model internal representations for various tasks
	3. Demonstrated improved generalization performance with less familiar data

**Result:** Experiments show that using less familiar data can improve generalization performance across various tasks such as machine reading comprehension and summarization.

**Limitations:** 

**Conclusion:** KAMIR provides a more efficient and effective approach to data selection, avoiding the pitfalls of traditional prompt engineering methods.

**Abstract:** Recent advances in large language models (LLMs) have been driven by pretraining, supervised fine tuning (SFT), and alignment tuning. Among these, SFT plays a crucial role in transforming a model 's general knowledge into structured responses tailored to specific tasks. However, there is no clearly established methodology for effective training data selection. Simply increasing the volume of data does not guarantee performance improvements, while preprocessing, sampling, and validation require substantial time and cost.   To address this issue, a variety of data selection methods have been proposed. Among them, knowledge based selection approaches identify suitable training data by analyzing the model 's responses. Nevertheless, these methods typically rely on prompt engineering, making them sensitive to variations and incurring additional costs for prompt design.   In this study, we propose Knowledge Analysis via Model Internal Representations (KAMIR), a novel approach that overcomes these limitations by analyzing data based on the model 's internal representations. KAMIR computes similarities between the hidden states of each layer (block) and the final hidden states for a given input to assess the data. Unlike prior methods that were largely limited to multiple choice tasks, KAMIR can be applied to a wide range of tasks such as machine reading comprehension and summarization. Moreover, it selects data useful for training based on the model 's familiarity with the input, even with a small dataset and a simple classifier architecture. Experiments across diverse task datasets demonstrate that training with less familiar data leads to better generalization performance.

</details>


### [31] [Mitigating Attention Localization in Small Scale: Self-Attention Refinement via One-step Belief Propagation](https://arxiv.org/abs/2509.07324)

*Nakyung Lee, Yeongoon Kim, Minhae Oh, Suhwan Kim, Jin Woo Koo, Hyewon Jo, Jungwoo Lee*

**Main category:** cs.CL

**Keywords:** Transformer, Self-attention, Belief Propagation, Global Token Dependency, Machine Learning

**Relevance Score:** 6

**TL;DR:** The paper proposes Self-Attention One-step Belief Propagation (SAOBP) to enhance transformer models by addressing localization issues in self-attention mechanisms.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the ability of transformer models to capture long-range dependencies and prevent attentions from collapsing onto a limited subset of tokens.

**Method:** The authors introduce SAOBP, which uses a belief propagation process to enhance multi-hop relationships, and they develop Global Token Dependency (GTD) to quantify these interactions within the attention graph.

**Key Contributions:**

	1. Introduction of Self-Attention One-step Belief Propagation (SAOBP).
	2. Development of Global Token Dependency (GTD) for quantifying multi-hop interactions.
	3. Empirical evidence showing competitive gains in small-scale models.

**Result:** SAOBP prevents entropy collapse in deeper layers and maintains GTD at appropriate levels for tasks, leading to improved performance, especially in small-scale models.

**Limitations:** 

**Conclusion:** The results demonstrate that SAOBP can enhance inference quality, especially in resource-constrained environments, making it a valuable approach for improving transformer models.

**Abstract:** Transformer-based self-attention mechanism serves as the core of modern language models, yet it often suffers from localization, where attentions collapse onto a limited subset of tokens and fail to capture long-range dependencies. To address this issue, we propose Self-Attention One-step Belief Propagation (SAOBP), a refinement framework that injects multi-hop relationships through a belief propagation process. To interpret and quantify these interactions, we introduce Global Token Dependency (GTD) that captures the relative contribution of multihop connections within the attention graph. Empirical results indicate that SAOBP helps prevent entropy collapse in deeper layers and adaptively maintains GTD at task-appropriate levels, thereby supporting improvements in model performance. Importantly, we observe competitive gains in small-scale models, highlighting its potential for improving inference quality in resource-constrained scenarios.

</details>


### [32] [PersonaFuse: A Personality Activation-Driven Framework for Enhancing Human-LLM Interactions](https://arxiv.org/abs/2509.07370)

*Yixuan Tang, Yi Yang, Ahmed Abbasi*

**Main category:** cs.CL

**Keywords:** Large Language Models, social-emotional intelligence, PersonaFuse, human-centered AI, mental health applications

**Relevance Score:** 9

**TL;DR:** PersonaFuse is a novel framework for adapting LLMs to express different personalities in varied contexts, improving social-emotional intelligence without losing general reasoning capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the emotional perception and social competence of LLMs for applications like mental health support and customer service.

**Method:** PersonaFuse utilizes a Mixture-of-Expert architecture, featuring persona adapters and a dynamic routing network to enable contextual expression of personality traits.

**Key Contributions:**

	1. Introduces a post-training framework for LLM personality adaptation.
	2. Implements Mixture-of-Expert architecture with a dynamic routing mechanism.
	3. Demonstrates significant improvements in emotional intelligence without compromising reasoning abilities.

**Result:** PersonaFuse outperforms baseline models in social-emotional intelligence and maintains general reasoning and model safety, yielding better performance in human-centered applications.

**Limitations:** 

**Conclusion:** The framework supports the development of social-emotional enhanced LLMs, advancing human-centric AI systems.

**Abstract:** Recent advancements in Large Language Models (LLMs) demonstrate remarkable capabilities across various fields. These developments have led to more direct communication between humans and LLMs in various situations, such as social companionship and psychological support. However, LLMs often exhibit limitations in emotional perception and social competence during real-world conversations. These limitations partly originate from their inability to adapt their communication style and emotional expression to different social and task contexts. In this work, we introduce PersonaFuse, a novel LLM post-training framework that enables LLMs to adapt and express different personalities for varying situations. Inspired by Trait Activation Theory and the Big Five personality model, PersonaFuse employs a Mixture-of-Expert architecture that combines persona adapters with a dynamic routing network, enabling contextual trait expression. Experimental results show that PersonaFuse substantially outperforms baseline models across multiple dimensions of social-emotional intelligence. Importantly, these gains are achieved without sacrificing general reasoning ability or model safety, which remain common limitations of direct prompting and supervised fine-tuning approaches. PersonaFuse also delivers consistent improvements in downstream human-centered applications, such as mental health counseling and review-based customer service. Finally, human preference evaluations against leading LLMs, including GPT-4o and DeepSeek, demonstrate that PersonaFuse achieves competitive response quality despite its comparatively smaller model size. These findings demonstrate that PersonaFuse~offers a theoretically grounded and practical approach for developing social-emotional enhanced LLMs, marking a significant advancement toward more human-centric AI systems.

</details>


### [33] [Rule-Based Moral Principles for Explaining Uncertainty in Natural Language Generation](https://arxiv.org/abs/2509.07190)

*Zahra Atf, Peter R Lewis*

**Main category:** cs.CL

**Keywords:** large language models, uncertainty, moral psychology, natural language generation, trust calibration

**Relevance Score:** 8

**TL;DR:** The paper proposes a framework using moral principles to handle uncertainty in LLM-generated text, offering a transparent alternative to probabilistic models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of explaining uncertainty in LLM outputs in high-stakes settings, ensuring transparency and ethical handling of information.

**Method:** A framework based on rule-based moral principles is developed, employing a lightweight Prolog engine to encode rules for different levels of uncertainty.

**Key Contributions:**

	1. Framework based on moral principles for LLM uncertainty
	2. Lightweight Prolog implementation for rule-based responses
	3. Application scenarios in clinical and legal domains

**Result:** The framework allows for scenario-based simulations that benchmark fairness, rule coverage, and trust calibration, demonstrating improved trust and interpretability in scenarios.

**Limitations:** 

**Conclusion:** The proposed method improves the transparency of LLM-generated outputs and provides a socially responsible approach to language generation in sensitive domains.

**Abstract:** Large language models (LLMs) are increasingly used in high-stakes settings, where explaining uncertainty is both technical and ethical. Probabilistic methods are often opaque and misaligned with expectations of transparency. We propose a framework based on rule-based moral principles for handling uncertainty in LLM-generated text. Using insights from moral psychology and virtue ethics, we define rules such as precaution, deference, and responsibility to guide responses under epistemic or aleatoric uncertainty. These rules are encoded in a lightweight Prolog engine, where uncertainty levels (low, medium, high) trigger aligned system actions with plain-language rationales. Scenario-based simulations benchmark rule coverage, fairness, and trust calibration. Use cases in clinical and legal domains illustrate how moral reasoning can improve trust and interpretability. Our approach offers a transparent, lightweight alternative to probabilistic models for socially responsible natural language generation.

</details>


### [34] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)

*Sankalp Tattwadarshi Swain, Anshika Krishnatray, Dhruv Kumar, Jagat Sesh Challa*

**Main category:** cs.CL

**Keywords:** language acquisition, large language models, interactive feedback

**Relevance Score:** 9

**TL;DR:** This paper examines whether large language models (LLMs) can learn a language through interaction and feedback, finding that while they struggled to converse in a new language, they employed strategies similar to human language learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the ability of LLM agents to acquire a new language through pattern recognition and interactive feedback, reflecting a key aspect of human language acquisition.

**Method:** An experimental framework was developed where an LLM agent attempts to learn and converse in a newly constructed language (Tinkatongue) with a bot that only understands Tinkatongue.

**Key Contributions:**

	1. Introduction of a new framework for evaluating LLM language acquisition
	2. Discovery of LLMs employing human-like learning strategies
	3. Recommendations for enhancing evaluation benchmarks in LLM research

**Result:** LLM agents failed to establish a conversation within 100 responses but demonstrated strategies that resemble human language learning approaches.

**Limitations:** The LLM agents did not successfully establish effective communication within the experiment, indicating potential areas for improvement in model design.

**Conclusion:** The study highlights the limitations of current LLM evaluation benchmarks and suggests the need for model designs that better incorporate interactive feedback mechanisms.

**Abstract:** Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.

</details>


### [35] [The Role of Exploration Modules in Small Language Models for Knowledge Graph Question Answering](https://arxiv.org/abs/2509.07399)

*Yi-Jie Cheng, Oscar Chew, Yun-Nung Chen*

**Main category:** cs.CL

**Keywords:** knowledge graphs, small language models, question answering, exploration modules, hallucination

**Relevance Score:** 8

**TL;DR:** This study explores the integration of knowledge graphs into small language models (SLMs) for improved question answering performance, proposing lightweight exploration modules to enhance their reasoning capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate hallucination in large language models by integrating knowledge graphs, while addressing the limitations of existing methods that rely on large or proprietary models.

**Method:** The study investigates existing integration methods for SLMs in knowledge graph question answering and proposes new exploration modules to facilitate knowledge graph traversal.

**Key Contributions:**

	1. Introduction of lightweight exploration modules for knowledge graph traversal
	2. Demonstration of improved performance in small language models using knowledge graphs
	3. Accessibility and scalability improvements compared to traditional methods

**Result:** Experiment results indicate that the proposed lightweight modules significantly enhance the performance of small language models in knowledge graph question answering tasks.

**Limitations:** The study primarily focuses on small language models and may not directly apply to larger models.

**Conclusion:** The incorporation of efficient exploration modules allows small language models to better utilize knowledge graphs, improving their question answering capabilities.

**Abstract:** Integrating knowledge graphs (KGs) into the reasoning processes of large language models (LLMs) has emerged as a promising approach to mitigate hallucination. However, existing work in this area often relies on proprietary or extremely large models, limiting accessibility and scalability. In this study, we investigate the capabilities of existing integration methods for small language models (SLMs) in KG-based question answering and observe that their performance is often constrained by their limited ability to traverse and reason over knowledge graphs. To address this limitation, we propose leveraging simple and efficient exploration modules to handle knowledge graph traversal in place of the language model itself. Experiment results demonstrate that these lightweight modules effectively improve the performance of small language models on knowledge graph question answering tasks. Source code: https://github.com/yijie-cheng/SLM-ToG/.

</details>


### [36] [Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents](https://arxiv.org/abs/2509.07389)

*Sankalp Tattwadarshi Swain, Anshika Krishnatray, Dhruv Kumar, Jagat Sesh Challa*

**Main category:** cs.CL

**Keywords:** large language models, language acquisition, interactive feedback, evaluation benchmarks, human language learning

**Relevance Score:** 8

**TL;DR:** This paper evaluates large language models (LLMs) for their ability to learn a new language through pattern recognition and feedback, a process akin to human language acquisition. The study finds LLMs struggle to maintain a conversation in the constructed language Tinkatongue but exhibit strategies similar to human learners.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, an approach central to human language learning, which has not been adequately explored in existing studies.

**Method:** A novel experimental framework was developed where an LLM agent was tasked with acquiring and using a constructed language (Tinkatongue) in interaction with a bot only capable of understanding Tinkatongue.

**Key Contributions:**

	1. Introduction of a new experimental framework for evaluating LLM agents
	2. Demonstration of LLM strategies that mirror human language learning
	3. Insights for future benchmarks and model design in language acquisition.

**Result:** The findings indicate that LLM agents fail to engage in a conversation within 100 responses. However, they utilize distinct strategies reflecting human approaches to language acquisition.

**Limitations:** The study is limited to a single constructed language and a specific framework, which may impact the generalizability of the findings.

**Conclusion:** The study suggests the need for new evaluation benchmarks and encourages future model designs that learn more efficiently from interactive feedback.

**Abstract:** Existing evaluation studies on linguistic competence of large language models (LLM agents) have focused primarily on vocabulary learning, morphological rule induction, syntactic generalization, pragmatic inference, and cross-linguistic transfer. However, none assess whether LLM agents can acquire a language through pattern recognition and interactive feedback, a central feature of human language acquisition. We propose a novel experimental framework in which an LLM agent is evaluated on its ability to acquire and use a newly constructed language (Tinkatongue) in conversation with a bot that understands only Tinkatongue. Our findings show that LLM agents fail to establish a conversation within 100 responses, yet they adopt distinct strategies that mirror human approaches to language learning. The results suggest a new direction for evaluation benchmarks and open pathways to model designs that learn more effectively from interactive feedback.

</details>


### [37] [LongEmotion: Measuring Emotional Intelligence of Large Language Models in Long-Context Interaction](https://arxiv.org/abs/2509.07403)

*Weichu Liu, Jing Xiong, Yuxuan Hu, Zixuan Li, Minghuan Tan, Ningning Mao, Chenyang Zhao, Zhongwei Wan, Chaofan Tao, Wendong Xu, Hui Shen, Chengming Li, Lingpeng Kong, Ngai Wong*

**Main category:** cs.CL

**Keywords:** Emotional Intelligence, Long-context, Large Language Models, Retrieval-Augmented Generation, Collaborative Emotional Modeling

**Relevance Score:** 9

**TL;DR:** Introduction of LongEmotion, a benchmark for long-context Emotional Intelligence tasks in large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks do not accurately assess Emotional Intelligence in long-context scenarios, which are common in practical applications.

**Method:** LongEmotion benchmark is created for various Emotional Intelligence tasks, using Retrieval-Augmented Generation and Collaborative Emotional Modeling to improve performance.

**Key Contributions:**

	1. Introduction of LongEmotion benchmark for long-context EI tasks
	2. Development of RAG and CoEM methods to enhance LLM performance
	3. Experimental validation highlighting model differences in EI capabilities

**Result:** RAG and CoEM methods demonstrate consistent enhancements in Emotional Intelligence performance across long-context tasks with LLMs.

**Limitations:** 

**Conclusion:** LongEmotion serves as a significant step toward practical applications of LLMs in Emotional Intelligence, with improved methods over traditional approaches.

**Abstract:** Large language models (LLMs) make significant progress in Emotional Intelligence (EI) and long-context understanding. However, existing benchmarks tend to overlook certain aspects of EI in long-context scenarios, especially under realistic, practical settings where interactions are lengthy, diverse, and often noisy. To move towards such realistic settings, we present LongEmotion, a benchmark specifically designed for long-context EI tasks. It covers a diverse set of tasks, including Emotion Classification, Emotion Detection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion Expression. On average, the input length for these tasks reaches 8,777 tokens, with long-form generation required for Emotion Expression. To enhance performance under realistic constraints, we incorporate Retrieval-Augmented Generation (RAG) and Collaborative Emotional Modeling (CoEM), and compare them with standard prompt-based methods. Unlike conventional approaches, our RAG method leverages both the conversation context and the large language model itself as retrieval sources, avoiding reliance on external knowledge bases. The CoEM method further improves performance by decomposing the task into five stages, integrating both retrieval augmentation and limited knowledge injection. Experimental results show that both RAG and CoEM consistently enhance EI-related performance across most long-context tasks, advancing LLMs toward more practical and real-world EI applications. Furthermore, we conducted a comparative case study experiment on the GPT series to demonstrate the differences among various models in terms of EI. Code is available on GitHub at https://github.com/LongEmotion/LongEmotion, and the project page can be found at https://longemotion.github.io/.

</details>


### [38] [AIxcellent Vibes at GermEval 2025 Shared Task on Candy Speech Detection: Improving Model Performance by Span-Level Training](https://arxiv.org/abs/2509.07459)

*Christian Rene Thelen, Patrick Gustav Blaneck, Tobias Bornheim, Niklas Grieger, Stephan Bialonski*

**Main category:** cs.CL

**Keywords:** candy speech, multilingual models, positive communication, social media, language detection

**Relevance Score:** 6

**TL;DR:** Automated detection of positive online communication (candy speech) is explored using monolingual and multilingual language models, revealing the effectiveness of XLM-RoBERTa-Large in detecting this type of language.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically analyze the impact of positive and supportive online communication, which is currently limited by the lack of automated detection methodologies.

**Method:** The study employs monolingual and multilingual language models, particularly focusing on the performance of XLM-RoBERTa-Large, GBERT, and Qwen3 Embedding on a large German YouTube comment corpus.

**Key Contributions:**

	1. Demonstration of the effectiveness of multilingual models in language detection tasks.
	2. Implementation of span-based training techniques for improved detection accuracy.
	3. Evaluation metrics for the automated detection of candy speech in a substantial YouTube comment dataset.

**Result:** The multilingual XLM-RoBERTa-Large model achieved the highest performance in detecting candy speech, with a binary positive F1 score of 0.8906 and a strict F1 score of 0.6307 for categorized span-based detection tasks.

**Limitations:** 

**Conclusion:** The findings highlight the potential of multilingual models, especially with span-based training and emoji-aware tokenizers, to effectively identify positive supportive language in online communications.

**Abstract:** Positive, supportive online communication in social media (candy speech) has the potential to foster civility, yet automated detection of such language remains underexplored, limiting systematic analysis of its impact. We investigate how candy speech can be reliably detected in a 46k-comment German YouTube corpus by monolingual and multilingual language models, including GBERT, Qwen3 Embedding, and XLM-RoBERTa. We find that a multilingual XLM-RoBERTa-Large model trained to detect candy speech at the span level outperforms other approaches, ranking first in both binary positive F1: 0.8906) and categorized span-based detection (strict F1: 0.6307) subtasks at the GermEval 2025 Shared Task on Candy Speech Detection. We speculate that span-based training, multilingual capabilities, and emoji-aware tokenizers improved detection performance. Our results demonstrate the effectiveness of multilingual models in identifying positive, supportive language.

</details>


### [39] [Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts](https://arxiv.org/abs/2509.07462)

*Yiliang Zhou, Di Hu, Tianchu Lyu, Jasmine Dhillon, Alexandra L. Beck, Gelareh Sadigh, Kai Zheng*

**Main category:** cs.CL

**Keywords:** stigmatizing language, healthcare inequities, sentiment analysis

**Relevance Score:** 9

**TL;DR:** This paper identifies and compares existing lexicons of stigmatizing language in healthcare, revealing moderate similarity among them and a predominance of negatively classified terms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of a standardized definition of stigmatizing language in healthcare, which contributes to healthcare inequities.

**Method:** A systematic literature search was conducted to identify existing lexicons of stigmatizing language, followed by a comparative analysis of these lexicons and a sentiment analysis of their terms.

**Key Contributions:**

	1. Identification of four existing stigmatizing language lexicons in healthcare
	2. Comparative analysis revealing moderate semantic similarity
	3. Highlighting the predominance of negatively classified terms

**Result:** The analysis revealed moderate semantic similarity among the identified lexicons and indicated that most stigmatizing terms are linked to judgmental clinician expressions. Sentiment analysis showed a predominance of negative terms, with variations across lexicons.

**Limitations:** 

**Conclusion:** There is a critical need for a standardized lexicon to define stigmatizing language in healthcare, alongside challenges in its characterization in clinical texts.

**Abstract:** Stigmatizing language results in healthcare inequities, yet there is no universally accepted or standardized lexicon defining which words, terms, or phrases constitute stigmatizing language in healthcare. We conducted a systematic search of the literature to identify existing stigmatizing language lexicons and then analyzed them comparatively to examine: 1) similarities and discrepancies between these lexicons, and 2) the distribution of positive, negative, or neutral terms based on an established sentiment dataset. Our search identified four lexicons. The analysis results revealed moderate semantic similarity among them, and that most stigmatizing terms are related to judgmental expressions by clinicians to describe perceived negative behaviors. Sentiment analysis showed a predominant proportion of negatively classified terms, though variations exist across lexicons. Our findings underscore the need for a standardized lexicon and highlight challenges in defining stigmatizing language in clinical texts.

</details>


### [40] [From Scarcity to Efficiency: Investigating the Effects of Data Augmentation on African Machine Translation](https://arxiv.org/abs/2509.07471)

*Mardiyyah Oduwole, Oluwatosin Olajide, Jamiu Suleiman, Faith Hunja, Busayo Awobade, Fatimo Adebanjo, Comfort Akanni, Chinonyelum Igwe, Peace Ododo, Promise Omoigui, Steven Kolawole, Abraham Owodunni*

**Main category:** cs.CL

**Keywords:** machine translation, data augmentation, African languages, low-resource languages, BLEU score

**Relevance Score:** 6

**TL;DR:** This study explores data augmentation techniques to enhance machine translation for low-resource African languages, achieving significant improvements in performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of machine translation for low-resource African languages and enhance translation systems' performance.

**Method:** Utilizing two data augmentation techniques: sentence concatenation with back translation and switch-out, and applying them to six African languages in experiments.

**Key Contributions:**

	1. Assessment of data augmentation techniques for African languages
	2. Demonstrated improvements in BLEU scores through experiments
	3. Comprehensive analysis of the impact on low-resource machine translation

**Result:** The application of data augmentation techniques resulted in a minimum increase of 25% in BLEU score across all six languages.

**Limitations:** 

**Conclusion:** Data augmentation techniques significantly improve machine translation performance, showcasing their potential to bolster translation systems for low-resource languages.

**Abstract:** The linguistic diversity across the African continent presents different challenges and opportunities for machine translation. This study explores the effects of data augmentation techniques in improving translation systems in low-resource African languages. We focus on two data augmentation techniques: sentence concatenation with back translation and switch-out, applying them across six African languages. Our experiments show significant improvements in machine translation performance, with a minimum increase of 25\% in BLEU score across all six languages.We provide a comprehensive analysis and highlight the potential of these techniques to improve machine translation systems for low-resource languages, contributing to the development of more robust translation systems for under-resourced languages.

</details>


### [41] [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)

*Jiahui Li, Sean Papay, Roman Klinger*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Brittleness, Human Annotation, Text Classification, Instruction Sensitivity

**Relevance Score:** 8

**TL;DR:** The study compares prompt brittleness in LLMs and human annotators during text classification tasks, revealing that both exhibit similar sensitivity to prompt modifications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate if prompt brittleness observed in large language models correlates with sensitivity in human annotations, particularly how instruction changes affect both.

**Method:** The research conducts systematic comparisons by prompting both human annotators and LLMs on text classification tasks with varied prompts.

**Key Contributions:**

	1. Comparison of LLMs and human annotators in terms of prompt brittleness
	2. Findings suggest shared characteristics in response to prompt variations
	3. Insights into the relationship between human sensitivity to instructions and LLM behavior.

**Result:** Both humans and LLMs show increased brittleness to specific prompt changes, especially those altering label sets, but human judgments are less impacted by typographical errors and label order reversals.

**Limitations:** 

**Conclusion:** Prompt brittleness may reflect genuine variances in human annotations rather than solely being a limitation of LLMs.

**Abstract:** The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.

</details>


### [42] [HALT-RAG: A Task-Adaptable Framework for Hallucination Detection with Calibrated NLI Ensembles and Abstention](https://arxiv.org/abs/2509.07475)

*Saumya Goswami, Siddharth Kurra*

**Main category:** cs.CL

**Keywords:** hallucination detection, Retrieval-Augmented Generation, NLI models

**Relevance Score:** 9

**TL;DR:** HALT-RAG is a verification system for identifying hallucinations in outputs of Retrieval-Augmented Generation pipelines, achieving strong performance across various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of detecting content that contradicts or is unsupported by source texts in generative language models.

**Method:** HALT-RAG employs a flexible framework using a universal feature set from two NLI models and lexical signals to train a calibrated meta-classifier, evaluated using a 5-fold OOF training protocol.

**Key Contributions:**

	1. Introduction of HALT-RAG for hallucination detection in RAG outputs
	2. Use of a universal feature set with NLI models
	3. Implementation of a precision-constrained decision policy for safety

**Result:** HALT-RAG achieves OOF F1-scores of 0.7756, 0.9786, and 0.7391 for summarization, QA, and dialogue tasks, respectively.

**Limitations:** 

**Conclusion:** The system provides a reliable tool balancing model performance with safety, featuring well-calibrated probabilities for practical application.

**Abstract:** Detecting content that contradicts or is unsupported by a given source text is a critical challenge for the safe deployment of generative language models. We introduce HALT-RAG, a post-hoc verification system designed to identify hallucinations in the outputs of Retrieval-Augmented Generation (RAG) pipelines. Our flexible and task-adaptable framework uses a universal feature set derived from an ensemble of two frozen, off-the-shelf Natural Language Inference (NLI) models and lightweight lexical signals. These features are used to train a simple, calibrated, and task-adapted meta-classifier. Using a rigorous 5-fold out-of-fold (OOF) training protocol to prevent data leakage and produce unbiased estimates, we evaluate our system on the HaluEval benchmark. By pairing our universal feature set with a lightweight, task-adapted classifier and a precision-constrained decision policy, HALT-RAG achieves strong OOF F1-scores of 0.7756, 0.9786, and 0.7391 on the summarization, QA, and dialogue tasks, respectively. The system's well-calibrated probabilities enable a practical abstention mechanism, providing a reliable tool for balancing model performance with safety requirements.

</details>


### [43] [ALLabel: Three-stage Active Learning for LLM-based Entity Recognition using Demonstration Retrieval](https://arxiv.org/abs/2509.07512)

*Zihan Chen, Lei Shi, Weize Wu, Qiji Zhou, Yue Zhang*

**Main category:** cs.CL

**Keywords:** entity recognition, large language models, active learning, annotation efficiency, natural sciences

**Relevance Score:** 4

**TL;DR:** ALLabel is a three-stage framework for efficient entity recognition in large datasets using active learning in LLMs, achieving high performance with minimal annotations.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for large-scale entity recognition in scientific datasets drove the development of a cost-effective framework as existing methods are expensive due to fine-tuning requirements.

**Method:** ALLabel utilizes a three-stage active learning process to select informative samples for LLM in-context learning, significantly reducing the required annotation budget.

**Key Contributions:**

	1. Introduction of ALLabel as a novel framework for entity recognition using LLMs
	2. Demonstration of reduced annotation needs with maintained performance
	3. Validating the effectiveness across multiple specialized domain datasets

**Result:** ALLabel outperforms baseline models while using only 5%-10% of the dataset for annotations, demonstrating high efficiency in entity recognition tasks.

**Limitations:** 

**Conclusion:** The approach allows for achieving high performance in entity recognition with a fraction of the typical annotation effort, confirming its effectiveness and generalizability.

**Abstract:** Many contemporary data-driven research efforts in the natural sciences, such as chemistry and materials science, require large-scale, high-performance entity recognition from scientific datasets. Large language models (LLMs) have increasingly been adopted to solve the entity recognition task, with the same trend being observed on all-spectrum NLP tasks. The prevailing entity recognition LLMs rely on fine-tuned technology, yet the fine-tuning process often incurs significant cost. To achieve a best performance-cost trade-off, we propose ALLabel, a three-stage framework designed to select the most informative and representative samples in preparing the demonstrations for LLM modeling. The annotated examples are used to construct a ground-truth retrieval corpus for LLM in-context learning. By sequentially employing three distinct active learning strategies, ALLabel consistently outperforms all baselines under the same annotation budget across three specialized domain datasets. Experimental results also demonstrate that selectively annotating only 5\%-10\% of the dataset with ALLabel can achieve performance comparable to the method annotating the entire dataset. Further analyses and ablation studies verify the effectiveness and generalizability of our proposal.

</details>


### [44] [VeriOS: Query-Driven Proactive Human-Agent-GUI Interaction for Trustworthy OS Agents](https://arxiv.org/abs/2509.07553)

*Zheng Wu, Heyuan Huang, Xingyu Lou, Xiangmou Qu, Pengzhou Cheng, Zongru Wu, Weiwen Liu, Weinan Zhang, Jun Wang, Zhaoxiang Wang, Zhuosheng Zhang*

**Main category:** cs.CL

**Keywords:** OS agents, human-agent interaction, multimodal large language models, trustworthiness, automation

**Relevance Score:** 7

**TL;DR:** Development of VeriOS-Agent, a human-agent-GUI interaction framework that enhances OS agent decision-making in untrustworthy environments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing OS agents that usually operate in idealized settings, which do not account for the complexities and uncertainties of real-world environments.

**Method:** A query-driven interaction framework allowing OS agents to determine when to ask humans for input, utilizing a two-stage learning paradigm for training the VeriOS-Agent.

**Key Contributions:**

	1. Introduction of a query-driven human-agent-GUI interaction framework.
	2. Development of the VeriOS-Agent with a two-stage training process.
	3. Demonstrated significant performance improvement in untrustworthy scenarios.

**Result:** VeriOS-Agent shows a 20.64% increase in average step-wise success rate in untrustworthy scenarios compared to state-of-the-art methods without degrading performance in normal conditions.

**Limitations:** 

**Conclusion:** VeriOS-Agent demonstrates improved rationality, generalizability, and scalability, making it a reliable tool for task automation in complex environments.

**Abstract:** With the rapid progress of multimodal large language models, operating system (OS) agents become increasingly capable of automating tasks through on-device graphical user interfaces (GUIs). However, most existing OS agents are designed for idealized settings, whereas real-world environments often present untrustworthy conditions. To mitigate risks of over-execution in such scenarios, we propose a query-driven human-agent-GUI interaction framework that enables OS agents to decide when to query humans for more reliable task completion. Built upon this framework, we introduce VeriOS-Agent, a trustworthy OS agent trained with a two-stage learning paradigm that falicitate the decoupling and utilization of meta-knowledge. Concretely, VeriOS-Agent autonomously executes actions in normal conditions while proactively querying humans in untrustworthy scenarios. Experiments show that VeriOS-Agent improves the average step-wise success rate by 20.64\% in untrustworthy scenarios over the state-of-the-art, without compromising normal performance. Analysis highlights VeriOS-Agent's rationality, generalizability, and scalability. The codes, datasets and models are available at https://github.com/Wuzheng02/VeriOS.

</details>


### [45] [Avoiding Knowledge Edit Skipping in Multi-hop Question Answering with Guided Decomposition](https://arxiv.org/abs/2509.07555)

*Yi Liu, Xiangrong Zhu, Xiangyu Liu, Wei Wei, Wei Hu*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, multi-hop question answering, retrieval-augmented generation, edit skipping

**Relevance Score:** 9

**TL;DR:** This paper presents IRAKE, a novel method for knowledge editing in large language models that addresses issues of edit skipping in multi-hop question answering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge editing in large language models, particularly for multi-hop question answering, which current methods struggle with due to edit skipping.

**Method:** The proposed method, IRAKE, utilizes guided decomposition through the guidance of single edited facts and entire edited cases, enhancing the capability of retrieval-augmented generation for knowledge editing.

**Key Contributions:**

	1. Introduction of IRAKE for guided decomposition in knowledge editing
	2. Reduction of edit skipping in multi-hop question answering
	3. Demonstrated superiority over existing methods for multi-hop question answering KE.

**Result:** Experimental results indicate that IRAKE significantly mitigates the failure of knowledge editing due to edit skipping and performs better than existing state-of-the-art methods in the realm of multi-hop question answering.

**Limitations:** 

**Conclusion:** IRAKE provides a robust solution for knowledge editing in LLMs, paving the way for more effective and efficient handling of updated information in multi-hop contexts.

**Abstract:** In a rapidly evolving world where information updates swiftly, knowledge in large language models (LLMs) becomes outdated quickly. Retraining LLMs is not a cost-effective option, making knowledge editing (KE) without modifying parameters particularly necessary. We find that although existing retrieval-augmented generation (RAG)-based KE methods excel at editing simple knowledge, they struggle with KE in multi-hop question answering due to the issue of "edit skipping", which refers to skipping the relevant edited fact in inference. In addition to the diversity of natural language expressions of knowledge, edit skipping also arises from the mismatch between the granularity of LLMs in problem-solving and the facts in the edited memory. To address this issue, we propose a novel Iterative Retrieval-Augmented Knowledge Editing method with guided decomposition (IRAKE) through the guidance from single edited facts and entire edited cases. Experimental results demonstrate that IRAKE mitigates the failure of editing caused by edit skipping and outperforms state-of-the-art methods for KE in multi-hop question answering.

</details>


### [46] [BALI: Enhancing Biomedical Language Representations through Knowledge Graph and Language Model Alignment](https://arxiv.org/abs/2509.07588)

*Andrey Sakhovskiy, Elena Tutubalina*

**Main category:** cs.CL

**Keywords:** Biomedical Language Models, Knowledge Graphs, BALI, UMLS, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents BALI, a method for aligning Language Models with Biomedical Knowledge Graphs to improve understanding of biomedical texts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the understanding of complex biomedical concepts and factual information through better alignment of Language Models with Knowledge Graphs.

**Method:** BALI proposes a joint pre-training approach that employs a KG encoder to augment a Language Model by linking biomedical concepts to the UMLS Knowledge Graph and using local subgraphs as positive samples.

**Key Contributions:**

	1. Introduction of BALI, a novel alignment method for LMs and KGs
	2. Demonstration of improved performance on several language understanding tasks
	3. Utilization of minimal pre-training data for effective results

**Result:** The method improves performance on tasks involving language understanding and enhances the quality of entity representations in leading biomedical Language Models like PubMedBERT and BioLinkBERT.

**Limitations:** 

**Conclusion:** BALI shows promise in enhancing biomedical LMs through effective alignment with Knowledge Graphs, demonstrating improvements even with limited pre-training data.

**Abstract:** In recent years, there has been substantial progress in using pretrained Language Models (LMs) on a range of tasks aimed at improving the understanding of biomedical texts. Nonetheless, existing biomedical LLMs show limited comprehension of complex, domain-specific concept structures and the factual information encoded in biomedical Knowledge Graphs (KGs). In this work, we propose BALI (Biomedical Knowledge Graph and Language Model Alignment), a novel joint LM and KG pre-training method that augments an LM with external knowledge by the simultaneous learning of a dedicated KG encoder and aligning the representations of both the LM and the graph. For a given textual sequence, we link biomedical concept mentions to the Unified Medical Language System (UMLS) KG and utilize local KG subgraphs as cross-modal positive samples for these mentions. Our empirical findings indicate that implementing our method on several leading biomedical LMs, such as PubMedBERT and BioLinkBERT, improves their performance on a range of language understanding tasks and the quality of entity representations, even with minimal pre-training on a small alignment dataset sourced from PubMed scientific abstracts.

</details>


### [47] [MaLei at MultiClinSUM: Summarisation of Clinical Documents using Perspective-Aware Iterative Self-Prompting with LLMs](https://arxiv.org/abs/2509.07622)

*Libo Ren, Yee Man Ng, Lifeng Han*

**Main category:** cs.CL

**Keywords:** clinical summarization, large language models, shared decision-making

**Relevance Score:** 9

**TL;DR:** This paper discusses a methodology for summarizing clinical case documents using large language models (LLMs), improving communication between patients and clinicians.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Efficient communication between patients and clinicians is critical for shared decision-making, yet clinical reports are often lengthy and complex, hindering efficient understanding.

**Method:** The authors applied an Iterative Self-Prompting technique to large language models, employing example-based few-shot learning along with lexical and embedding metrics to fine-tune the models across multiple epochs.

**Key Contributions:**

	1. Introduction of Iterative Self-Prompting for clinical summarization
	2. Use of multiple evaluation metrics for model fine-tuning
	3. High semantic alignment in summary outputs using large language models

**Result:** The approach yielded ROUGE scores of 46.53, 24.68, and 30.77, and BERT-scores of 87.84, 83.25, and 85.46, indicating effective production of semantically aligned summaries from clinical reports despite lower lexical overlap.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of perspective-aware ISP for clinical summarization, enhancing communication between patients and clinicians.

**Abstract:** Efficient communication between patients and clinicians plays an important role in shared decision-making. However, clinical reports are often lengthy and filled with clinical jargon, making it difficult for domain experts to identify important aspects in the document efficiently. This paper presents the methodology we applied in the MultiClinSUM shared task for summarising clinical case documents. We used an Iterative Self-Prompting technique on large language models (LLMs) by asking LLMs to generate task-specific prompts and refine them via example-based few-shot learning. Furthermore, we used lexical and embedding space metrics, ROUGE and BERT-score, to guide the model fine-tuning with epochs. Our submission using perspective-aware ISP on GPT-4 and GPT-4o achieved ROUGE scores (46.53, 24.68, 30.77) and BERTscores (87.84, 83.25, 85.46) for (P, R, F1) from the official evaluation on 3,396 clinical case reports from various specialties extracted from open journals. The high BERTscore indicates that the model produced semantically equivalent output summaries compared to the references, even though the overlap at the exact lexicon level is lower, as reflected in the lower ROUGE scores. This work sheds some light on how perspective-aware ISP (PA-ISP) can be deployed for clinical report summarisation and support better communication between patients and clinicians.

</details>


### [48] [MoLoRAG: Bootstrapping Document Understanding via Multi-modal Logic-aware Retrieval](https://arxiv.org/abs/2509.07666)

*Xixi Wu, Yanchao Tan, Nan Hou, Ruiyang Zhang, Hong Cheng*

**Main category:** cs.CL

**Keywords:** Document Understanding, Large Vision-Language Models, Retrieval-Augmented Generation, Multi-modal Processing

**Relevance Score:** 8

**TL;DR:** MoLoRAG is a logic-aware retrieval framework for improved multi-modal, multi-page document understanding in Document Question Answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional methods for Document Question Answering strip away critical multi-modal information, while existing Large Vision-Language Models face input size constraints that limit multi-page comprehension.

**Method:** MoLoRAG constructs a page graph to capture contextual relationships between pages. It employs a lightweight VLM for graph traversal to retrieve pages based on both semantic and logical relevance, allowing retrieval of top-$K$ pages for question answering with LVLMs.

**Key Contributions:**

	1. Development of MoLoRAG for logic-aware retrieval in document understanding
	2. Introduction of a page graph to capture relationships between pages
	3. Demonstrated significant improvements in accuracy and retrieval precision on DocQA datasets.

**Result:** Experiments on four DocQA datasets show a 9.68% accuracy improvement over LVLM direct inference and a 7.44% increase in retrieval precision compared to baselines.

**Limitations:** 

**Conclusion:** MoLoRAG provides a better approach for Document Question Answering by improving retrieval accuracy through logical relevance, with options for training-free deployment or fine-tuning.

**Abstract:** Document Understanding is a foundational AI capability with broad applications, and Document Question Answering (DocQA) is a key evaluation task. Traditional methods convert the document into text for processing by Large Language Models (LLMs), but this process strips away critical multi-modal information like figures. While Large Vision-Language Models (LVLMs) address this limitation, their constrained input size makes multi-page document comprehension infeasible. Retrieval-augmented generation (RAG) methods mitigate this by selecting relevant pages, but they rely solely on semantic relevance, ignoring logical connections between pages and the query, which is essential for reasoning.   To this end, we propose MoLoRAG, a logic-aware retrieval framework for multi-modal, multi-page document understanding. By constructing a page graph that captures contextual relationships between pages, a lightweight VLM performs graph traversal to retrieve relevant pages, including those with logical connections often overlooked. This approach combines semantic and logical relevance to deliver more accurate retrieval. After retrieval, the top-$K$ pages are fed into arbitrary LVLMs for question answering. To enhance flexibility, MoLoRAG offers two variants: a training-free solution for easy deployment and a fine-tuned version to improve logical relevance checking. Experiments on four DocQA datasets demonstrate average improvements of 9.68% in accuracy over LVLM direct inference and 7.44% in retrieval precision over baselines. Codes and datasets are released at https://github.com/WxxShirley/MoLoRAG.

</details>


### [49] [M-BRe: Discovering Training Samples for Relation Extraction from Unlabeled Texts with Large Language Models](https://arxiv.org/abs/2509.07730)

*Zexuan Li, Hongliang Dai, Piji Li*

**Main category:** cs.CL

**Keywords:** Relation Extraction, Large Language Models, Natural Language Processing, Unlabeled Texts, Training Data Extraction

**Relevance Score:** 8

**TL;DR:** The paper proposes M-BRe, a framework for Relation Extraction that automatically extracts training instances from unlabeled texts, addressing challenges in using LLMs for multi-class and binary classification.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Manual annotation for Relation Extraction is costly and challenging due to the scarcity of relevant sentences, necessitating an efficient method to extract training data automatically.

**Method:** M-BRe utilizes three modules: Relation Grouping, Relation Extraction, and Label Decision to enhance the extraction of training instances from unlabeled texts.

**Key Contributions:**

	1. Proposed a new framework (M-BRe) that automates training instance extraction for Relation Extraction.
	2. Introduced a novel combination of relation grouping and binary classification to improve classification efficiency.
	3. Provided empirical evidence of superior performance through extensive experiments.

**Result:** The framework demonstrates superior performance in finding high-quality training samples compared to traditional methods.

**Limitations:** 

**Conclusion:** The M-BRe framework effectively addresses the limitations of existing approaches in RE by leveraging unlabeled texts, thus providing a more efficient means of training RE models.

**Abstract:** For Relation Extraction (RE), the manual annotation of training data may be prohibitively expensive, since the sentences that contain the target relations in texts can be very scarce and difficult to find. It is therefore beneficial to develop an efficient method that can automatically extract training instances from unlabeled texts for training RE models. Recently, large language models (LLMs) have been adopted in various natural language processing tasks, with RE also benefiting from their advances. However, when leveraging LLMs for RE with predefined relation categories, two key challenges arise. First, in a multi-class classification setting, LLMs often struggle to comprehensively capture the semantics of every relation, leading to suboptimal results. Second, although employing binary classification for each relation individually can mitigate this issue, it introduces significant computational overhead, resulting in impractical time complexity for real-world applications. Therefore, this paper proposes a framework called M-BRe to extract training instances from unlabeled texts for RE. It utilizes three modules to combine the advantages of both of the above classification approaches: Relation Grouping, Relation Extraction, and Label Decision. Extensive experiments confirm its superior capability in discovering high-quality training samples from unlabeled texts for RE.

</details>


### [50] [Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts](https://arxiv.org/abs/2509.07755)

*Rochana Prih Hastuti, Rian Adam Rajagede, Mansour Al Ghanim, Mengxin Zheng, Qian Lou*

**Main category:** cs.CL

**Keywords:** large language models, watermarking, medical informatics, factual accuracy, safety risks

**Relevance Score:** 9

**TL;DR:** The paper evaluates watermarking methods for large language models in medical contexts, highlighting the need for approaches that maintain factual accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address safety risks associated with the application of large language models in sensitive medical domains, focusing on provenance and accountability.

**Method:** A medical-focused evaluation workflow was proposed, utilizing the Factuality-Weighted Score (FWS) to assess factual accuracy and coherence. This included the use of GPT-Judger and human validation.

**Key Contributions:**

	1. Introduction of a medical-focused evaluation workflow for watermarking in LLMs.
	2. Development of the Factuality-Weighted Score (FWS) to assess factual accuracy more rigorously.
	3. Highlighting serious shortcomings in existing watermarking methods related to medical content integrity.

**Result:** Current watermarking methods significantly compromise medical factuality, with entropy shifts leading to degraded representation of medical entities.

**Limitations:** The findings are based on a limited set of watermarking methods and may not encompass all potential risks in other sensitive domains.

**Conclusion:** There is a critical need for watermarking strategies that are aware of medical domain specifics to ensure the integrity of medical information.

**Abstract:** As large language models (LLMs) adapted to sensitive domains such as medicine, their fluency raises safety risks, particularly regarding provenance and accountability. Watermarking embeds detectable patterns to mitigate these risks, yet its reliability in medical contexts remains untested. Existing benchmarks focus on detection-quality tradeoffs, overlooking factual risks under low-entropy settings often exploited by watermarking's reweighting strategy. We propose a medical-focused evaluation workflow that jointly assesses factual accuracy and coherence. Using GPT-Judger and further human validation, we introduce the Factuality-Weighted Score (FWS), a composite metric prioritizing factual accuracy beyond coherence to guide watermarking deployment in medical domains. Our evaluation shows current watermarking methods substantially compromise medical factuality, with entropy shifts degrading medical entity representation. These findings underscore the need for domain-aware watermarking approaches that preserve the integrity of medical content.

</details>


### [51] [Are LLMs Enough for Hyperpartisan, Fake, Polarized and Harmful Content Detection? Evaluating In-Context Learning vs. Fine-Tuning](https://arxiv.org/abs/2509.07768)

*Michele Joshua Maggini, Dhia Merzougui, Rabiraj Bandyopadhyay, Gaël Dias, Fabrice Maurel, Pablo Gamallo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fake News Detection, Political Bias, Fine-Tuning, In-Context Learning

**Relevance Score:** 8

**TL;DR:** This study benchmarks different Large Language Models for detecting hyperpartisan, fake news, and political bias across multiple languages and datasets, highlighting the effectiveness of Fine-Tuning over In-Context Learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The prevalence of fake news and harmful content on online platforms necessitates the evaluation of LLMs for their effectiveness in detection tasks.

**Method:** The study tested various adaptation paradigms, including Fine-Tuning and In-Context Learning, across 10 datasets and 5 languages, using techniques like zero-shot prompts and few-shot learning.

**Key Contributions:**

	1. Comprehensive benchmarking of LLMs for fake news detection across multiple languages.
	2. Evaluation of Fine-Tuning vs. In-Context Learning methods.
	3. Identification of strategies for effective model adaptation in political bias detection.

**Result:** Fine-Tuning models outperformed In-Context Learning approaches in detecting hyperpartisan content, regardless of model size, indicating its critical importance for task-specific applications.

**Limitations:** The study primarily focuses on a limited number of languages and models, which may not generalize to all contexts.

**Conclusion:** Fine-Tuning smaller models is more effective than relying on larger models with In-Context Learning, particularly for specific detection tasks.

**Abstract:** The spread of fake news, polarizing, politically biased, and harmful content on online platforms has been a serious concern. With large language models becoming a promising approach, however, no study has properly benchmarked their performance across different models, usage methods, and languages. This study presents a comprehensive overview of different Large Language Models adaptation paradigms for the detection of hyperpartisan and fake news, harmful tweets, and political bias. Our experiments spanned 10 datasets and 5 different languages (English, Spanish, Portuguese, Arabic and Bulgarian), covering both binary and multiclass classification scenarios. We tested different strategies ranging from parameter efficient Fine-Tuning of language models to a variety of different In-Context Learning strategies and prompts. These included zero-shot prompts, codebooks, few-shot (with both randomly-selected and diversely-selected examples using Determinantal Point Processes), and Chain-of-Thought. We discovered that In-Context Learning often underperforms when compared to Fine-Tuning a model. This main finding highlights the importance of Fine-Tuning even smaller models on task-specific settings even when compared to the largest models evaluated in an In-Context Learning setup - in our case LlaMA3.1-8b-Instruct, Mistral-Nemo-Instruct-2407 and Qwen2.5-7B-Instruct.

</details>


### [52] [SciNLP: A Domain-Specific Benchmark for Full-Text Scientific Entity and Relation Extraction in NLP](https://arxiv.org/abs/2509.07801)

*Decheng Duan, Yingyi Zhang, Jitong Peng, Chengzhi Zhang*

**Main category:** cs.CL

**Keywords:** information extraction, NLP, knowledge graph, dataset, entity relation

**Relevance Score:** 7

**TL;DR:** Introducing SciNLP, a benchmark dataset for full-text entity and relation extraction in NLP, encompassing 60 annotated publications and demonstrating performance improvements over existing models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate effective extraction of core concepts and relationships in scientific literature by providing a dedicated dataset that encompasses full-text annotations.

**Method:** Development of the SciNLP dataset with manual annotations of entities and relations from 60 full-text NLP publications; comparative experiments with existing datasets to evaluate extraction capabilities of models.

**Key Contributions:**

	1. First dataset for full-text annotations of entities and relationships in NLP
	2. Publicly available dataset with substantial annotations
	3. Demonstration of improved model performance using SciNLP

**Result:** SciNLP shows significant performance improvements for certain baseline supervised models in extracting entities and relations compared to existing datasets.

**Limitations:** 

**Conclusion:** SciNLP is a valuable resource for enhancing the performance of NLP models and for constructing knowledge graphs in the domain.

**Abstract:** Structured information extraction from scientific literature is crucial for capturing core concepts and emerging trends in specialized fields. While existing datasets aid model development, most focus on specific publication sections due to domain complexity and the high cost of annotating scientific texts. To address this limitation, we introduce SciNLP - a specialized benchmark for full-text entity and relation extraction in the Natural Language Processing (NLP) domain. The dataset comprises 60 manually annotated full-text NLP publications, covering 7,072 entities and 1,826 relations. Compared to existing research, SciNLP is the first dataset providing full-text annotations of entities and their relationships in the NLP domain. To validate the effectiveness of SciNLP, we conducted comparative experiments with similar datasets and evaluated the performance of state-of-the-art supervised models on this dataset. Results reveal varying extraction capabilities of existing models across academic texts of different lengths. Cross-comparisons with existing datasets show that SciNLP achieves significant performance improvements on certain baseline models. Using models trained on SciNLP, we implemented automatic construction of a fine-grained knowledge graph for the NLP domain. Our KG has an average node degree of 3.2 per entity, indicating rich semantic topological information that enhances downstream applications. The dataset is publicly available at https://github.com/AKADDC/SciNLP.

</details>


### [53] [Dual Knowledge-Enhanced Two-Stage Reasoner for Multimodal Dialog Systems](https://arxiv.org/abs/2509.07817)

*Xiaolin Chen, Xuemeng Song, Haokun Wen, Weili Guan, Xiangyu Zhao, Liqiang Nie*

**Main category:** cs.CL

**Keywords:** multimodal dialog systems, large language models, dual knowledge, textual response generation

**Relevance Score:** 8

**TL;DR:** The paper introduces DK2R, a dual knowledge-enhanced reasoner that utilizes structured and unstructured knowledge to improve textual response generation in multimodal dialog systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve textual response generation in multimodal task-oriented dialog systems by addressing the limitations of neglecting unstructured review knowledge and underutilizing large language models (LLMs).

**Method:** The proposed DK2R employs a two-stage reasoner that extracts both structured and unstructured knowledge from an external knowledge base and evaluates their utility through LLM-generated probe responses.

**Key Contributions:**

	1. Introduction of a dual knowledge-enhanced reasoner (DK2R) for textual response generation.
	2. Addressing the challenges of dynamic knowledge type selection and intention-response decoupling in dialog systems.
	3. Demonstration of DK2R's effectiveness through extensive experiments.

**Result:** Experimental results on a public dataset demonstrate the superiority of DK2R over existing methods in generating responses.

**Limitations:** 

**Conclusion:** DK2R effectively enhances the response generation process by combining dual knowledge types and leveraging LLMs, indicating potential advancements in multimodal dialog systems.

**Abstract:** Textual response generation is pivotal for multimodal \mbox{task-oriented} dialog systems, which aims to generate proper textual responses based on the multimodal context. While existing efforts have demonstrated remarkable progress, there still exist the following limitations: 1) \textit{neglect of unstructured review knowledge} and 2) \textit{underutilization of large language models (LLMs)}. Inspired by this, we aim to fully utilize dual knowledge (\textit{i.e., } structured attribute and unstructured review knowledge) with LLMs to promote textual response generation in multimodal task-oriented dialog systems. However, this task is non-trivial due to two key challenges: 1) \textit{dynamic knowledge type selection} and 2) \textit{intention-response decoupling}. To address these challenges, we propose a novel dual knowledge-enhanced two-stage reasoner by adapting LLMs for multimodal dialog systems (named DK2R). To be specific, DK2R first extracts both structured attribute and unstructured review knowledge from external knowledge base given the dialog context. Thereafter, DK2R uses an LLM to evaluate each knowledge type's utility by analyzing LLM-generated provisional probe responses. Moreover, DK2R separately summarizes the intention-oriented key clues via dedicated reasoning, which are further used as auxiliary signals to enhance LLM-based textual response generation. Extensive experiments conducted on a public dataset verify the superiority of DK2R. We have released the codes and parameters.

</details>


### [54] [Small Open Models Achieve Near Parity with Large Models in Low Resource Literary Translation at a Fraction of the Cost](https://arxiv.org/abs/2509.07829)

*Mihai Nadas, Laura Diosan, Andreea Tomescu, Andrei Piscoran*

**Main category:** cs.CL

**Keywords:** literary translation, machine translation, low resource languages, language model, dataset creation

**Relevance Score:** 2

**TL;DR:** The paper introduces TINYFABULIST TRANSLATION FRAMEWORK (TF2), focusing on creating and fine-tuning a language model for English-Romanian literary translation, accompanied by the release of significant datasets.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in high-quality literary translation by small open models, particularly for low resource languages like Romanian.

**Method:** The framework includes dataset creation, a two-stage fine-tuning process for a 12B parameter model, and nuanced quality evaluation using BLEU and an LLM-based rubric.

**Key Contributions:**

	1. Introduction of the TF2 framework for dataset creation and model fine-tuning
	2. Development of the DS-TF2 datasets released for public use
	3. Evaluation methodology combining traditional metrics with LLM-based assessments

**Result:** TF2's fine-tuned model matches the fluency and adequacy of leading proprietary models while being open and cost-effective.

**Limitations:** 

**Conclusion:** TF2 represents a reproducible approach to enhance literary translations and supports the use of open models for low resource languages.

**Abstract:** Literary translation has recently gained attention as a distinct and complex task in machine translation research. However, the translation by small open models remains an open problem. We contribute to this ongoing research by introducing TINYFABULIST TRANSLATION FRAMEWORK (TF2), a unified framework for dataset creation, fine tuning, and evaluation in English-Romanian literary translations, centred on the creation and open release of both a compact, fine tuned language model (TF2-12B) and large scale synthetic parallel datasets (DS-TF2-EN-RO-3M and DS-TF2-EN-RO-15K). Building on DS-TF1-EN-3M (TF1), the largest collection of synthetic English fables to date, we address the need for rich, high quality literary datasets in low resource languages such as Romanian. Our pipeline first generates 15k high quality Romanian references from the TF1 pool using a high performing LLM. We then apply a two stage fine tuning process to a 12B parameter open weight model: (i) instruction tuning to capture genre specific narrative style, and (ii) adapter compression for efficient deployment. Evaluation combines corpus level BLEU and a five dimension LLM based rubric (accuracy, fluency, coherence, style, cultural adaptation) to provide a nuanced assessment of translation quality. Results show that our fine tuned model achieves fluency and adequacy competitive with top performing large proprietary models, while being open, accessible, and significantly more cost effective. Alongside the fine tuned model and both datasets, we publicly release all scripts and evaluation prompts. TF2 thus provides an end-to-end, reproducible pipeline for research on cost efficient translation, cross lingual narrative generation, and the broad adoption of open models for culturally significant literary content in low resource settings.

</details>


### [55] [Are Humans as Brittle as Large Language Models?](https://arxiv.org/abs/2509.07869)

*Jiahui Li, Sean Papay, Roman Klinger*

**Main category:** cs.CL

**Keywords:** large language models, human annotators, prompt brittleness, text classification, sensitivity analysis

**Relevance Score:** 7

**TL;DR:** This paper compares the sensitivity of human annotators and LLMs to prompt modifications in text classification tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether prompt brittleness seen in LLMs is also present in human annotations, thus understanding the implications for LLM reliability.

**Method:** The study systematically prompts both human annotators and LLMs with variations in instructions for text classification tasks to assess their sensitivity to changes.

**Key Contributions:**

	1. Systematic comparison of LLMs and human annotators regarding prompt sensitivity.
	2. Identified specific prompt modifications that affect both LLMs and human judgments.
	3. Provided insights into the implications of prompt brittleness in the context of human annotation variances.

**Result:** Both humans and LLMs showed increased brittleness with certain prompt alterations; however, human responses were less affected by typographical errors and label order changes than LLM outputs.

**Limitations:** The study focuses on particular text classification tasks, which may not generalize to other tasks or models.

**Conclusion:** Prompt brittleness is not unique to LLMs; humans also exhibit this behavior but with varying degrees of sensitivity to types of prompt changes.

**Abstract:** The output of large language models (LLM) is unstable, due to both non-determinism of the decoding process as well as to prompt brittleness. While the intrinsic non-determinism of LLM generation may mimic existing uncertainty in human annotations through distributional shifts in outputs, it is largely assumed, yet unexplored, that the prompt brittleness effect is unique to LLMs. This raises the question: do human annotators show similar sensitivity to instruction changes? If so, should prompt brittleness in LLMs be considered problematic? One may alternatively hypothesize that prompt brittleness correctly reflects human annotation variances. To fill this research gap, we systematically compare the effects of prompt modifications on LLMs and identical instruction modifications for human annotators, focusing on the question of whether humans are similarly sensitive to prompt perturbations. To study this, we prompt both humans and LLMs for a set of text classification tasks conditioned on prompt variations. Our findings indicate that both humans and LLMs exhibit increased brittleness in response to specific types of prompt modifications, particularly those involving the substitution of alternative label sets or label formats. However, the distribution of human judgments is less affected by typographical errors and reversed label order than that of LLMs.

</details>


### [56] [From Detection to Mitigation: Addressing Gender Bias in Chinese Texts via Efficient Tuning and Voting-Based Rebalancing](https://arxiv.org/abs/2509.07889)

*Chengyan Wu, Yiqiang Cai, Yufei Cheng, Yun Xue*

**Main category:** cs.CL

**Keywords:** gender bias, natural language generation, large language models, Low-Rank Adaptation, Chinese

**Relevance Score:** 7

**TL;DR:** The paper addresses gender bias detection and mitigation in Chinese using LLMs, achieving a competitive ranking in a shared task.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To promote fairness and controllability in natural language generation by addressing gender bias.

**Method:** A fine-tuning approach on LLMs using Low-Rank Adaptation (LoRA) and a balanced training set for effective bias detection and mitigation.

**Key Contributions:**

	1. Utilization of Low-Rank Adaptation (LoRA) for fine-tuning LLMs in gender bias tasks
	2. Construction of a balanced training set to address class imbalance
	3. Design of a multi-temperature sampling mechanism for bias expression variations.

**Result:** The method ranked fourth in the shared task with an average score of 47.90%, demonstrating effective bias detection and mitigation performance.

**Limitations:** 

**Conclusion:** The proposed methods improve effectiveness in detecting, classifying, and mitigating gender bias in text generation.

**Abstract:** This paper presents our team's solution to Shared Task 7 of NLPCC-2025, which focuses on sentence-level gender bias detection and mitigation in Chinese. The task aims to promote fairness and controllability in natural language generation by automatically detecting, classifying, and mitigating gender bias. To address this challenge, we adopt a fine-tuning approach based on large language models (LLMs), efficiently adapt to the bias detection task via Low-Rank Adaptation (LoRA). In terms of data processing, we construct a more balanced training set to alleviate class imbalance and introduce heterogeneous samples from multiple sources to enhance model generalization. For the detection and classification sub-tasks, we employ a majority voting strategy that integrates outputs from multiple expert models to boost performance. Additionally, to improve bias generation detection and mitigation, we design a multi-temperature sampling mechanism to capture potential variations in bias expression styles. Experimental results demonstrate the effectiveness of our approach in bias detection, classification, and mitigation. Our method ultimately achieves an average score of 47.90%, ranking fourth in the shared task.

</details>


### [57] [Biased Tales: Cultural and Topic Bias in Generating Children's Stories](https://arxiv.org/abs/2509.07908)

*Donya Rooein, Vilém Zouhar, Debora Nozza, Dirk Hovy*

**Main category:** cs.CL

**Keywords:** bias, large language models, storytelling, sociocultural bias, dataset

**Relevance Score:** 7

**TL;DR:** The paper presents 'Biased Tales', a dataset to analyze biases in LLM-generated stories, revealing significant disparities in attributes based on gender and cultural backgrounds.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and address concerns about cultural and gender stereotypes in bedtime stories generated by large language models as parents increasingly use them.

**Method:** Analysis of the 'Biased Tales' dataset to study the impact of sociocultural bias on protagonist attributes and story elements in narratives generated by LLMs.

**Key Contributions:**

	1. Introduction of the 'Biased Tales' dataset for analyzing LLM-generated stories
	2. Quantitative analysis showing bias in character attributes based on gender and culture
	3. Emphasis on the importance of understanding sociocultural influences in AI-generated content.

**Result:** The study found that stories with girl protagonists highlight appearance-related attributes significantly more than those with boy protagonists, and stories featuring non-Western children emphasize cultural themes disproportionately.

**Limitations:** The study may not cover all biases present in LLM-generated narratives and focuses primarily on specific attributes related to gender and culture.

**Conclusion:** Addressing sociocultural bias in LLM-generated stories is crucial for creating more equitable and diverse narratives.

**Abstract:** Stories play a pivotal role in human communication, shaping beliefs and morals, particularly in children. As parents increasingly rely on large language models (LLMs) to craft bedtime stories, the presence of cultural and gender stereotypes in these narratives raises significant concerns. To address this issue, we present Biased Tales, a comprehensive dataset designed to analyze how biases influence protagonists' attributes and story elements in LLM-generated stories. Our analysis uncovers striking disparities. When the protagonist is described as a girl (as compared to a boy), appearance-related attributes increase by 55.26%. Stories featuring non-Western children disproportionately emphasize cultural heritage, tradition, and family themes far more than those for Western children. Our findings highlight the role of sociocultural bias in making creative AI use more equitable and diverse.

</details>


### [58] [GENUINE: Graph Enhanced Multi-level Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2509.07925)

*Tuo Wang, Adithya Kulkarni, Tyler Cody, Peter A. Beling, Yujun Yan, Dawei Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Estimation, Graph Theory, Natural Language Processing, Supervised Learning

**Relevance Score:** 10

**TL;DR:** GENUINE is a framework for enhancing uncertainty estimation in Large Language Models using graph-based techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Uncertainty estimation is critical for the reliability of LLMs in high-stakes applications, as current methods fail to incorporate semantic dependencies and structural relationships within text.

**Method:** GENUINE utilizes dependency parse trees and hierarchical graph pooling combined with supervised learning to improve uncertainty quantification in LLM outputs.

**Key Contributions:**

	1. Introduces a graph-based approach for uncertainty estimation in LLMs.
	2. Demonstrates significant improvements in AUROC and calibration errors through extensive experimentation.
	3. Provides an open-source implementation for further research and application.

**Result:** GENUINE achieved up to 29% higher AUROC compared to existing semantic entropy-based methods and reduced calibration errors by over 15% across various NLP tasks.

**Limitations:** 

**Conclusion:** The results indicate that graph-based modeling significantly enhances uncertainty quantification in LLMs, making them more reliable for applications demanding high confidence assessments.

**Abstract:** Uncertainty estimation is essential for enhancing the reliability of Large Language Models (LLMs), particularly in high-stakes applications. Existing methods often overlook semantic dependencies, relying on token-level probability measures that fail to capture structural relationships within the generated text. We propose GENUINE: Graph ENhanced mUlti-level uncertaINty Estimation for Large Language Models, a structure-aware framework that leverages dependency parse trees and hierarchical graph pooling to refine uncertainty quantification. By incorporating supervised learning, GENUINE effectively models semantic and structural relationships, improving confidence assessments. Extensive experiments across NLP tasks show that GENUINE achieves up to 29% higher AUROC than semantic entropy-based approaches and reduces calibration errors by over 15%, demonstrating the effectiveness of graph-based uncertainty modeling. The code is available at https://github.com/ODYSSEYWT/GUQ.

</details>


### [59] [SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric Knowledge](https://arxiv.org/abs/2509.07968)

*Lukas Haas, Gal Yona, Giovanni D'Antonio, Sasha Goldshtein, Dipanjan Das*

**Main category:** cs.CL

**Keywords:** Large Language Models, factuality, benchmarking, machine learning, evaluation

**Relevance Score:** 9

**TL;DR:** Introduction of SimpleQA Verified, a 1,000-prompt benchmark for evaluating LLM factuality, improving upon OpenAI's existing benchmark.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in OpenAI's SimpleQA benchmark such as noisy labels, biases, and redundancy, providing a more reliable evaluation framework for LLMs.

**Method:** A rigorous multi-stage filtering process was employed to create the benchmark, ensuring de-duplication, topic balancing, and source reconciliation.

**Key Contributions:**

	1. Creation of SimpleQA Verified benchmark with 1,000 prompts
	2. Improvement over previous OpenAI benchmarks by addressing key limitations
	3. Providing tools for reliable evaluation of parametric model factuality

**Result:** Gemini 2.5 Pro achieved a state-of-the-art F1-score of 55.6 on the new benchmark, outperforming other models like GPT-5.

**Limitations:** 

**Conclusion:** The SimpleQA Verified benchmark offers a higher-fidelity tool for evaluating LLM factuality and tracking model improvements, with resources available for the research community.

**Abstract:** We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large Language Model (LLM) short-form factuality based on OpenAI's SimpleQA. It addresses critical limitations in OpenAI's benchmark, including noisy and incorrect labels, topical biases, and question redundancy. SimpleQA Verified was created through a rigorous multi-stage filtering process involving de-duplication, topic balancing, and source reconciliation to produce a more reliable and challenging evaluation set, alongside improvements in the autorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a state-of-the-art F1-score of 55.6, outperforming other frontier models, including GPT-5. This work provides the research community with a higher-fidelity tool to track genuine progress in parametric model factuality and to mitigate hallucinations. The benchmark dataset, evaluation code, and leaderboard are available at: https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.

</details>


### [60] [Parallel-R1: Towards Parallel Thinking via Reinforcement Learning](https://arxiv.org/abs/2509.07980)

*Tong Zheng, Hongming Zhang, Wenhao Yu, Xiaoyang Wang, Xinyu Yang, Runpeng Dai, Rui Liu, Huiwen Bao, Chengsong Huang, Heng Huang, Dong Yu*

**Main category:** cs.CL

**Keywords:** Parallel Thinking, Reinforcement Learning, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper introduces Parallel-R1, a reinforcement learning framework that enhances large language models' reasoning by promoting parallel thinking through a curriculum-based training method.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance reasoning abilities in large language models through a method that encourages exploration and generalization rather than imitation.

**Method:** The proposed Parallel-R1 framework employs a two-stage training process: first, supervised fine-tuning on simpler tasks followed by reinforcement learning on more complex problems to instill and generalize parallel thinking behaviors.

**Key Contributions:**

	1. Introduction of Parallel-R1, the first RL framework for parallel thinking in LLMs.
	2. Development of a progressive curriculum to address cold-start issues in RL training.
	3. Empirical validation showing significant performance improvements in math reasoning tasks.

**Result:** Experiments demonstrate that Parallel-R1 achieves an 8.4% accuracy improvement over models trained directly on difficult tasks, and a 42.9% improvement on specific benchmarks.

**Limitations:** 

**Conclusion:** Parallel thinking serves as an effective exploratory strategy in reinforcement learning, leading to higher performance ceilings for models on real-world reasoning tasks.

**Abstract:** Parallel thinking has emerged as a novel approach for enhancing the reasoning capabilities of large language models (LLMs) by exploring multiple reasoning paths concurrently. However, activating such capabilities through training remains challenging, as existing methods predominantly rely on supervised fine-tuning (SFT) over synthetic data, which encourages teacher-forced imitation rather than exploration and generalization. Different from them, we propose \textbf{Parallel-R1}, the first reinforcement learning (RL) framework that enables parallel thinking behaviors for complex real-world reasoning tasks. Our framework employs a progressive curriculum that explicitly addresses the cold-start problem in training parallel thinking with RL. We first use SFT on prompt-generated trajectories from easier tasks to instill the parallel thinking ability, then transition to RL to explore and generalize this skill on harder problems. Experiments on various math benchmarks, including MATH, AMC23, and AIME, show that Parallel-R1 successfully instills parallel thinking, leading to 8.4% accuracy improvements over the sequential thinking model trained directly on challenging tasks with RL. Further analysis reveals a clear shift in the model's thinking behavior: at an early stage, it uses parallel thinking as an exploration strategy, while in a later stage, it uses the same capability for multi-perspective verification. Most significantly, we validate parallel thinking as a \textbf{mid-training exploration scaffold}, where this temporary exploratory phase unlocks a higher performance ceiling after RL, yielding a 42.9% improvement over the baseline on AIME25. Our model, data, and code will be open-source at https://github.com/zhengkid/Parallel-R1.

</details>


### [61] [UPLex: Fine-Grained Personality Control in Large Language Models via Unsupervised Lexical Modulation](https://arxiv.org/abs/2310.16582)

*Tianlong Li, Wenhao Liu, Muling Wu, Shihan Dou, Zhenghua Wang, Changze Lv, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** large language models, personality manipulation, unsupervised learning, natural language processing, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper presents UPLex, a novel method for manipulating the personality traits of large language models (LLMs) using an unsupervised personalized lexicon during the decoding phase, allowing for fine-grained control over personality expression.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user experiences with LLMs by directly manipulating their personality traits, addressing the limitations of previous methods which were either costly or lacked precision.

**Method:** UPLex constructs an Unsupervisedly-Built Personalized Lexicon (UPL) from a situational judgment test dataset and uses it to modulate LLM personality during the decoding phase by altering predicted word probabilities.

**Key Contributions:**

	1. Introduction of an unsupervised method for building a personalized lexicon
	2. Demonstration of fine-grained control over LLM personality traits
	3. Validation of method effectiveness through extensive experiments

**Result:** Experimental results show that UPLex effectively allows for fine-grained manipulation of LLM personalities with high adaptability.

**Limitations:** 

**Conclusion:** UPLex demonstrates a robust and pluggable solution for improving LLM interactions through the controlled expression of personality traits.

**Abstract:** Personality is a crucial factor that shapes human communication patterns, thereby regulating the personalities of large language models (LLMs) holds significant potential in enhancing their user experiences. Previous approaches either relied on fine-tuning LLMs on specific corpora or required manually crafted prompts to evoke specific personalities from LLMs. However, the former is inefficient and costly, while the latter cannot precisely manipulate personality traits at a fine-grained level. To address these challenges, we propose UPLex, a method that uses an Unsupervisedly-Built Personalized Lexicon (UPL) during the decoding phase to manipulate LLM's personality traits. UPL can be constructed from a newly built situational judgment test dataset in an unsupervised fashion, and used to modulate the personality expression of LLMs by dynamically altering their predicted probability of upcoming words in a pluggable fashion. Extensive experimentation demonstrates the remarkable effectiveness and pluggability of our method for fine-grained manipulation of LLMs' personalities.

</details>


### [62] [JoPA:Explaining Large Language Model's Generation via Joint Prompt Attribution](https://arxiv.org/abs/2405.20404)

*Yurui Chang, Bochuan Cao, Yujia Wang, Jinghui Chen, Lu Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Attribution, Combinatorial Optimization, Language Generation, Counterfactual Explanations

**Relevance Score:** 8

**TL;DR:** This study presents a framework for understanding how input prompts collaboratively influence language generation in Large Language Models, using counterfactual explanations and probabilistic optimization.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the relationship between input prompts and generated text in LLMs is crucial, yet remains underexplored, particularly in explaining complex text generation.

**Method:** The authors introduce a framework called Joint Prompt Attribution (JoPA) which formulates prompt attribution as a combinatorial optimization problem and employs a probabilistic algorithm to identify influential input combinations.

**Key Contributions:**

	1. Introduction of a counterfactual explanation framework for LLMs.
	2. Formulation of prompt attribution as a combinatorial optimization problem.
	3. A probabilistic algorithm for identifying influential input combinations.

**Result:** The framework provides an explanation of input prompts' collaborative effects on a model's output, evaluated through various metrics for faithfulness and efficiency.

**Limitations:** 

**Conclusion:** The proposed framework improves understanding of LLM generation by addressing the combinatorial nature of prompt interactions, offering insights that existing methods may overlook.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive performances in complex text generation tasks. However, the contribution of the input prompt to the generated content still remains obscure to humans, underscoring the necessity of understanding the causality between input and output pairs. Existing works for providing prompt-specific explanation often confine model output to be classification or next-word prediction. Few initial attempts aiming to explain the entire language generation often treat input prompt texts independently, ignoring their combinatorial effects on the follow-up generation. In this study, we introduce a counterfactual explanation framework based on Joint Prompt Attribution, JoPA, which aims to explain how a few prompt texts collaboratively influences the LLM's complete generation. Particularly, we formulate the task of prompt attribution for generation interpretation as a combinatorial optimization problem, and introduce a probabilistic algorithm to search for the casual input combination in the discrete space. We define and utilize multiple metrics to evaluate the produced explanations, demonstrating both the faithfulness and efficiency of our framework.

</details>


### [63] [CTourLLM: Enhancing LLMs with Chinese Tourism Knowledge](https://arxiv.org/abs/2407.12791)

*Qikai Wei, Mingzhi Yang, Jinqiang Wang, Wenwei Mao, Jiabo Xu, Huansheng Ning*

**Main category:** cs.CL

**Keywords:** large language models, tourism, natural language processing, fine-tuning, chatbot performance

**Relevance Score:** 7

**TL;DR:** This paper introduces Cultour, a dataset for enhancing LLM performance in tourism applications, and CTourLLM, a model fine-tuned on Cultour, showing improved results over ChatGPT.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of large language models in tourist attraction presentations and travel planning by providing better tourism knowledge.

**Method:** A supervised fine-tuning dataset for the Chinese culture and tourism domain was created, consisting of tourism knowledge base data, travelogues data, and tourism QA data, along with the development of the CTourLLM model.

**Key Contributions:**

	1. Introduction of the Cultour dataset for tourism-related NLP tasks.
	2. Development of CTourLLM model fine-tuned on Cultour data supporting better travel planning.
	3. Introduction of a new evaluation criterion (RRA) for assessing responses.

**Result:** CTourLLM outperforms ChatGPT in various evaluations, improving BLEU-1 by 1.21 and Rouge-L by 1.54.

**Limitations:** 

**Conclusion:** The creation of the Cultour dataset and the development of the CTourLLM model effectively enhance the quality of information in tourism-related tasks.

**Abstract:** Recently, large language models (LLMs) have demonstrated their effectiveness in various natural language processing (NLP) tasks. However, the lack of tourism knowledge limits the performance of LLMs in tourist attraction presentations and travel planning. To address this challenge, we constructed a supervised fine-tuning dataset for the Chinese culture and tourism domain, named Cultour. This dataset consists of three parts: tourism knowledge base data, travelogues data, and tourism QA data. Additionally, we propose CTourLLM, a Qwen-based model supervised fine-tuned with Cultour, to improve the quality of information about attractions and travel planning. To evaluate the performance of CTourLLM, we proposed a human evaluation criterion named RRA (Relevance, Readability, Availability), and employed both automatic and human evaluation. The experimental results demonstrate that CTourLLM outperforms ChatGPT, achieving an improvement of 1.21 in BLEU-1 and 1.54 in Rouge-L, thereby validating the effectiveness of the response outcomes. Our proposed Cultour is accessible at https://github.com/mrweiqk/Cultour.

</details>


### [64] [TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection](https://arxiv.org/abs/2411.02886)

*Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Long-context inference, Attention mechanism

**Relevance Score:** 9

**TL;DR:** The paper introduces TokenSelect, a method for efficient long-context inference in LLMs that addresses performance degradation and long inference times.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of using Large Language Models for long-context scenarios due to degraded performance and long inference times.

**Method:** TokenSelect employs a training-free approach that utilizes non-contiguous attention sparsity and QK dot products to evaluate the criticality of KV cache tokens at a token level, optimizing attention computation.

**Key Contributions:**

	1. Introduction of Dynamic Token-Level KV Cache Selection (TokenSelect) for LLMs.
	2. Novel method that combines attention sparsity with a soft voting mechanism for improved efficiency.
	3. Empirical evaluation demonstrates significant performance gains over state-of-the-art methods.

**Result:** TokenSelect achieves up to 23.84 times speedup in attention computation and 2.28 times acceleration in end-to-end latency compared to existing long-context inference methods.

**Limitations:** 

**Conclusion:** The evaluation shows TokenSelect significantly improves the efficiency and performance of LLMs in long-context tasks without sacrificing accuracy.

**Abstract:** Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of TokenSelect demonstrates up to $23.84\times$ speedup in attention computation and up to $2.28\times$ acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.

</details>


### [65] [Dovetail: A CPU/GPU Heterogeneous Speculative Decoding for LLM inference](https://arxiv.org/abs/2412.18934)

*Libo Zhang, Zhaoning Zhang, Baizhou Xu, Rui Li, Zhiliang Tian, Songzhu Mei, Dongsheng Li*

**Main category:** cs.CL

**Keywords:** Inference acceleration, Large language models, Heterogeneous devices

**Relevance Score:** 8

**TL;DR:** Dovetail is a lossless inference acceleration method for large language models that optimally utilizes CPU and GPU resources to improve performance on consumer-grade devices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The performance of large language models has outpaced available computational resources on consumer-grade devices and legacy servers, necessitating efficient inference solutions.

**Method:** Dovetail deploys a draft model on the GPU for preliminary predictions and a target model on the CPU for validation, optimizing data transfer and reducing verification latency.

**Key Contributions:**

	1. Introduction of Dovetail for efficient inference on consumer-grade devices
	2. Optimization of draft model for heterogeneous hardware environments
	3. Dynamic Gating Fusion mechanism for better feature integration

**Result:** Dovetail achieves inference speedups ranging from 1.79x to 10.1x on various consumer-grade GPUs, showing consistent and stable output generation across different tasks and models.

**Limitations:** 

**Conclusion:** The proposed method demonstrates significant efficiency improvements for LLM inference in heterogeneous hardware environments without sacrificing output quality.

**Abstract:** With the continuous advancement in the performance of large language models (LLMs), their demand for computational resources and memory has significantly increased, which poses major challenges for efficient inference on consumer-grade devices and legacy servers. These devices typically feature relatively weaker GPUs and stronger CPUs. Although techniques such as parameter offloading and partial offloading can alleviate GPU memory pressure to some extent, their effectiveness is limited due to communication latency and suboptimal hardware resource utilization. To address this issue, we propose Dovetail, a lossless inference acceleration method that leverages the complementary characteristics of heterogeneous devices and the advantages of speculative decoding. Dovetail deploys a draft model on the GPU to perform preliminary predictions, while a target model running on the CPU validates these outputs. By reducing the granularity of data transfer, Dovetail significantly minimizes communication overhead. To further improve efficiency, we optimize the draft model specifically for heterogeneous hardware environments by reducing the number of draft tokens to lower parallel verification latency, increasing model depth to enhance predictive capabilities, and introducing a Dynamic Gating Fusion (DGF) mechanism to improve the integration of feature and embedding information. We conduct comprehensive evaluations of Dovetail across various consumer-grade GPUs, covering multiple tasks and mainstream models. Experimental results on 13B models demonstrate that Dovetail achieves inference speedups ranging from 1.79x to 10.1x across different devices, while maintaining consistency and stability in the distribution of generated texts.

</details>


### [66] [Cardiverse: Harnessing LLMs for Novel Card Game Prototyping](https://arxiv.org/abs/2502.07128)

*Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia*

**Main category:** cs.CL

**Keywords:** Card Games, Game Prototyping, Large Language Models, AI in Game Development, Gameplay AI

**Relevance Score:** 7

**TL;DR:** This paper presents an automated framework for card game prototyping using Large Language Models, addressing challenges in game design and AI development.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to automate and streamline the creative ideation and evaluation processes in card game design due to extensive human effort involved.

**Method:** The proposed framework includes a graph-based indexing method for generating new game mechanics, an LLM-driven system for creating consistent game code, and a method for developing gameplay AI that utilizes an ensemble of LLM-generated heuristics optimized through self-play.

**Key Contributions:**

	1. Graph-based method for generating novel game variations
	2. LLM-driven consistent game code generation
	3. Gameplay AI using ensemble of LLM-generated heuristic functions

**Result:** The framework successfully accelerates the prototyping process, reduces human labor, and lowers entry barriers for game developers.

**Limitations:** 

**Conclusion:** This innovative approach enhances the card game development process by leveraging advancements in LLMs to automate and optimize game design and evaluation tasks.

**Abstract:** The prototyping of computer games, particularly card games, requires extensive human effort in creative ideation and gameplay evaluation. Recent advances in Large Language Models (LLMs) offer opportunities to automate and streamline these processes. However, it remains challenging for LLMs to design novel game mechanics beyond existing databases, generate consistent gameplay environments, and develop scalable gameplay AI for large-scale evaluations. This paper addresses these challenges by introducing a comprehensive automated card game prototyping framework. The approach highlights a graph-based indexing method for generating novel game variations, an LLM-driven system for consistent game code generation validated by gameplay records, and a gameplay AI constructing method that uses an ensemble of LLM-generated heuristic functions optimized through self-play. These contributions aim to accelerate card game prototyping, reduce human labor, and lower barriers to entry for game developers. For code repo visit this http URL https://github.com/danruili/Cardiverse

</details>


### [67] [MEMIT-Merge: Addressing MEMIT's Key-Value Conflicts in Same-Subject Batch Editing for LLMs](https://arxiv.org/abs/2502.07322)

*Zilu Dong, Xiangqing Shen, Rui Xia*

**Main category:** cs.CL

**Keywords:** language models, knowledge editing, MEMIT-Merge, batch processing, update conflicts

**Relevance Score:** 8

**TL;DR:** MEMIT-Merge improves knowledge editing in language models by efficiently handling multiple edits of the same subject, showing higher success rates than MEMIT.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance decline of knowledge editing techniques, particularly MEMIT, when handling batches of edits concerning the same subject.

**Method:** Proposes MEMIT-Merge, which merges value computations for facts sharing the same subject to mitigate update conflicts in the editing process.

**Key Contributions:**

	1. Introduction of MEMIT-Merge to enhance MEMIT's performance in batch editing scenarios.
	2. Demonstration of improved success rates in knowledge editing tasks across larger batch sizes.
	3. Analysis of update conflicts due to identical keys in the original MEMIT framework.

**Result:** MEMIT-Merge achieves a success rate exceeding 90% for larger batch sizes, compared to MEMIT's approximate 50% success rate, demonstrating improved editing efficacy.

**Limitations:** 

**Conclusion:** The proposed MEMIT-Merge proves to be a robust solution for batch editing in language models, particularly when dealing with conflicts arising from multiple edits of the same subject.

**Abstract:** As large language models continue to scale up, knowledge editing techniques that modify models' internal knowledge without full retraining have gained significant attention. MEMIT, a prominent batch editing algorithm, stands out for its capability to perform mass knowledge modifications. However, we uncover that MEMIT's editing efficacy significantly deteriorates when processing batches containing multiple edits sharing the same subject. Our analysis reveals this stems from MEMIT's key value modeling framework: identical keys (derived from the shared subject) are forced to represent different values (corresponding to different knowledge), resulting in update conflicts during editing. Addressing this issue, we propose MEMIT-Merge, an enhanced approach that merges value computation processes for facts sharing the same subject, effectively resolving the performance degradation in samesubject batch editing scenarios. Experimental results demonstrate that when MEMIT's edit success rate drops to around 50% at larger batch sizes, MEMIT-Merge maintains a success rate exceeding 90%, showcasing remarkable robustness to subject entity collisions. The code is available at https://github.com/NUSTM/ MEMIT-Merge.

</details>


### [68] [MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering](https://arxiv.org/abs/2502.18993)

*Teng Lin, Yuyu Luo, Nan Tang*

**Main category:** cs.CL

**Keywords:** multi-entity question answering, large language models, benchmark, information extraction, entity-aware QA

**Relevance Score:** 9

**TL;DR:** MEBench is a benchmark for evaluating large language models in multi-entity question answering (MEQA) by focusing on their ability to retrieve and integrate information from multiple documents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods excel in single-document comprehension but falter in cross-document aggregation, particularly for entity-dense questions.

**Method:** We developed the MEBench benchmark comprising 4,780 multi-entity questions categorized into three primary categories with eight distinct types, aimed at testing LLMs and RAG systems.

**Key Contributions:**

	1. Introduction of the MEBench benchmark for MEQA tasks.
	2. Systematic categorization of questions into types for comprehensive evaluation.
	3. Granular evaluation metric (EA-F1) for assessing entity-level correctness.

**Result:** State-of-the-art LLMs, including GPT-4 and Llama-3, achieved only 59% accuracy on the MEBench, indicating significant limitations.

**Limitations:** Limited accuracy of advanced LLMs on the benchmark indicates potential areas for improvement in information extraction accuracy.

**Conclusion:** MEBench reveals weaknesses in current LLM frameworks and emphasizes the need for improved entity-aware QA architectures.

**Abstract:** Multi-entity question answering (MEQA) represents significant challenges for large language models (LLM) and retrieval-augmented generation (RAG) systems, which frequently struggle to consolidate scattered information across diverse documents. While existing methods excel at single-document comprehension, they often struggle with cross-document aggregation, particularly when resolving entity-dense questions like "What is the distribution of ACM Fellows among various fields of study?", which require integrating entity-centric insights from heterogeneous sources (e.g., Wikipedia pages). To address this gap, we introduce MEBench, a novel multi-document, multi-entity benchmark designed to systematically evaluate LLMs' capacity to retrieve, consolidate, and reason over fragmented information. Our benchmark comprises 4,780 questions which are systematically categorized into three primary categories, further divided into eight distinct types, ensuring broad coverage of real-world multi-entity reasoning scenarios. Our experiments on state-of-the-art LLMs (e.g., GPT-4, Llama-3) and RAG pipelines reveal critical limitations: even advanced models achieve only 59% accuracy on MEBench. Our benchmark emphasizes the importance of completeness and factual precision of information extraction in MEQA tasks, using Entity-Attributed F1 (EA-F1) metric for granular evaluation of entity-level correctness and attribution validity. MEBench not only highlights systemic weaknesses in current LLM frameworks but also provides a foundation for advancing robust, entity-aware QA architectures.

</details>


### [69] [When Large Language Models Meet Speech: A Survey on Integration Approaches](https://arxiv.org/abs/2502.19548)

*Zhengdong Yang, Shuichiro Shimizu, Yahan Yu, Chenhui Chu*

**Main category:** cs.CL

**Keywords:** large language models, speech integration, multimodal applications

**Relevance Score:** 9

**TL;DR:** The paper surveys the integration of speech modalities with large language models, categorizing methodologies into text-based, latent-representation-based, and audio-token-based approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Recent advancements in large language models have encouraged exploring their applications beyond text, particularly in integrating speech.

**Method:** The paper categorizes methodologies into three approaches: text-based, latent-representation-based, and audio-token-based integration and discusses their application in speech-related tasks.

**Key Contributions:**

	1. Categorizes methodologies for integrating speech with LLMs
	2. Explores applications in speech-related tasks
	3. Identifies key challenges for future research in the field

**Result:** It highlights various methodologies and their implementation in speech-related applications.

**Limitations:** 

**Conclusion:** The survey identifies challenges in the field and aims to inspire future research in multimodal applications of LLMs.

**Abstract:** Recent advancements in large language models (LLMs) have spurred interest in expanding their application beyond text-based tasks. A large number of studies have explored integrating other modalities with LLMs, notably speech modality, which is naturally related to text. This paper surveys the integration of speech with LLMs, categorizing the methodologies into three primary approaches: text-based, latent-representation-based, and audio-token-based integration. We also demonstrate how these methods are applied across various speech-related applications and highlight the challenges in this field to offer inspiration for

</details>


### [70] [Local Normalization Distortion and the Thermodynamic Formalism of Decoding Strategies for Large Language Models](https://arxiv.org/abs/2503.21929)

*Tom Kempton, Stuart Burrell*

**Main category:** cs.CL

**Keywords:** decoding strategies, language models, ergodic theory

**Relevance Score:** 8

**TL;DR:** This paper develops a theoretical framework for understanding and improving decoding strategies in autoregressive language models, revealing local normalization distortion as a key issue.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Decoding methods for language models are often heuristic-based and poorly understood, leading to difficulties in their application and improvement.

**Method:** The authors use ergodic theory to express various decoding algorithms as equilibrium states, analyzing their objective functions and the impacts of local normalization on sampling methods.

**Key Contributions:**

	1. Development of a theoretical framework for decoding strategies in language models
	2. Identification and quantification of local normalization distortion
	3. Implications for improving decoding algorithms and detecting machine-generated text.

**Result:** The research quantifies the distortion caused by local normalization in prominent decoding strategies and assesses its influence on text generation quality and diversity.

**Limitations:** 

**Conclusion:** To enhance decoding strategies, it's crucial to address local normalization distortion, which affects the performance of language models.

**Abstract:** Advances in hardware and language model architecture have spurred a revolution in natural language generation. However, autoregressive models compute probability distributions over next-token choices, and sampling from these distributions, known as decoding, has received significantly less attention than other design choices. Existing decoding strategies are largely based on heuristics, resulting in methods that are difficult to apply or improve in a principled manner. We develop the theory of decoding strategies for language models by expressing popular decoding algorithms as equilibrium states in the language of ergodic theory and stating the objective functions they optimize. Using this, we analyze the effect of the local normalization step required to make probabilities sum to one in top-k, nucleus, and temperature sampling. We argue that local normalization distortion is a fundamental defect of decoding strategies and quantify the size of this distortion and its effect on mathematical proxies for the quality and diversity of generated text. This yields conclusions for the design of decoding algorithms and the detection of machine-generated text.

</details>


### [71] [Register Always Matters: Analysis of LLM Pretraining Data Through the Lens of Language Variation](https://arxiv.org/abs/2504.01542)

*Amanda Myntti, Erik Henriksson, Veronika Laippala, Sampo Pyysalo*

**Main category:** cs.CL

**Keywords:** Large Language Models, pretraining data curation, registers, data selection, model performance

**Relevance Score:** 9

**TL;DR:** This study explores the impact of different text registers on the performance of Large Language Models (LLMs) during pretraining, revealing that certain registers, like Opinion, enhance performance while others, like News, detract from it.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand how different kinds of texts influence model performance and improve pretraining dataset curation for LLMs.

**Method:** The authors utilize registers or genres from corpus linguistics to classify pretraining data and evaluate small generative models trained on this classified data using standard benchmarks.

**Key Contributions:**

	1. First study applying registers to curate pretraining datasets for LLMs
	2. Demonstrated that some registers improve model performance while others hinder it
	3. Identified effective combinations of registers leading to improved outcomes.

**Result:** Models trained with register-classified datasets demonstrate varying performance, with combinations of beneficial registers yielding major improvements, while models trained solely on News register underperform.

**Limitations:** 

**Conclusion:** Register significantly affects LLM performance, suggesting that more nuanced data selection strategies could enhance model training outcomes.

**Abstract:** Pretraining data curation is a cornerstone in Large Language Model (LLM) development, leading to growing research on quality filtering of large web corpora. From statistical quality flags to LLM-based labelling systems, datasets are divided into categories, frequently reducing to a binary: those passing the filters are deemed as valuable examples, others are discarded as useless or detrimental. However, a more detailed understanding of the contribution of different kinds of texts to model performance is still largely lacking. In this article, we present the first study utilising registers or genres - a widely used standard in corpus linguistics to model linguistic variation - to curate pretraining datasets and investigate the effect of register on the performance of LLMs. We train small generative models with register classified data and evaluate them using standard benchmarks, and show that the register of pretraining data substantially affects model performance. We uncover surprising relationships between the pretraining material and the resulting models: using the News register results in subpar performance, and on the contrary, including the Opinion class, covering texts such as reviews and opinion blogs, is highly beneficial. While a model trained on the entire unfiltered dataset outperforms those trained on datasets limited to a single register, combining well-performing registers like How-to-Instructions, Informational Description, and Opinion leads to major improvements. Furthermore, analysis of individual benchmark results reveals key differences in the strengths and drawbacks of specific register classes as pretraining data. These findings show that register is an important explainer of model variation and can facilitate more deliberate future data selection practices.

</details>


### [72] [SemCAFE: When Named Entities make the Difference Assessing Web Source Reliability through Entity-level Analytics](https://arxiv.org/abs/2504.08776)

*Gautam Kishore Shahi, Oshani Seneviratne, Marc Spaniol*

**Main category:** cs.CL

**Keywords:** news reliability, NLP, entity relatedness, credibility assessment, misinformation

**Relevance Score:** 4

**TL;DR:** Introduction of SemCAFE, a system to assess news reliability through entity relatedness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of unreliable news in digital media and the challenge of distinguishing it from credible sources.

**Method:** SemCAFE uses NLP techniques, including boilerplate removal and tokenization, along with entity-level semantic analysis from the YAGO knowledge base, creating semantic fingerprints for articles.

**Key Contributions:**

	1. Development of SemCAFE for news reliability assessment
	2. Utilization of entity relatedness in news evaluation
	3. Improvement of 12% in macro F1 score over existing methods

**Result:** Achieved a 12% improvement in macro F1 score over state-of-the-art methods by assessing the reliability of 46,020 reliable and 3,407 unreliable articles.

**Limitations:** 

**Conclusion:** SemCAFE effectively detects news reliability, addressing the challenges posed by misinformation in the digital age.

**Abstract:** With the shift from traditional to digital media, the online landscape now hosts not only reliable news articles but also a significant amount of unreliable content. Digital media has faster reachability by significantly influencing public opinion and advancing political agendas. While newspaper readers may be familiar with their preferred outlets political leanings or credibility, determining unreliable news articles is much more challenging. The credibility of many online sources is often opaque, with AI generated content being easily disseminated at minimal cost. Unreliable news articles, particularly those that followed the Russian invasion of Ukraine in 2022, closely mimic the topics and writing styles of credible sources, making them difficult to distinguish. To address this, we introduce SemCAFE, a system designed to detect news reliability by incorporating entity relatedness into its assessment. SemCAFE employs standard Natural Language Processing techniques, such as boilerplate removal and tokenization, alongside entity level semantic analysis using the YAGO knowledge base. By creating a semantic fingerprint for each news article, SemCAFE could assess the credibility of 46,020 reliable and 3,407 unreliable articles on the 2022 Russian invasion of Ukraine. Our approach improved the macro F1 score by 12% over state of the art methods. The sample data and code are available on GitHub

</details>


### [73] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)

*Ke Wang, Yiming Qin, Nikolaos Dimitriadis, Alessandro Favero, Pascal Frossard*

**Main category:** cs.CL

**Keywords:** language models, model editing, knowledge injection, generalization, machine learning

**Relevance Score:** 8

**TL;DR:** MEMOIR is a scalable framework for efficiently updating language models with new knowledge while preserving existing capabilities, minimizing interference among edits, and enhancing generalization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Editing language models efficiently and reliably without retraining remains a significant challenge. Current methods either compromise generalization or interfere with previous edits.

**Method:** MEMOIR introduces a residual memory to inject knowledge, using sample-dependent masks to sparsify input activations and confine each edit to a specific subset of memory parameters.

**Key Contributions:**

	1. Introduction of MEMOIR framework for model editing
	2. Utilization of residual memory for knowledge injection
	3. Sample-dependent masks for minimizing edit interference

**Result:** Experiments show that MEMOIR outperforms existing methods on multiple benchmarks for reliability, generalization, and locality, and scales effectively to numerous sequential edits.

**Limitations:** 

**Conclusion:** The proposed framework enables effective model updates without forgetting previous information, demonstrating state-of-the-art performance in various tasks.

**Abstract:** Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably-without retraining or forgetting previous information-remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through sample-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks for LLaMA-3 and Mistral backbones demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting.

</details>
