# 2025-09-16

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 56]

- [cs.CL](#cs.CL) [Total: 116]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [LLMs Homogenize Values in Constructive Arguments on Value-Laden Topics](https://arxiv.org/abs/2509.10637)

*Farhana Shahid, Stella Zhang, Aditya Vashistha*

**Main category:** cs.HC

**Keywords:** large language models, value alignment, prosocal discourse, online communication, contentious debates

**Relevance Score:** 9

**TL;DR:** This paper explores how large language models (LLMs) influence online discourse by reframing arguments on value-laden topics, specifically analyzing their impact on conservative and prosocial values through participant experiments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to understand the role of LLMs in negotiating values during online discourse, particularly on contentious topics.

**Method:** Experiments conducted with 347 participants from India and the US, analyzing their comments on homophobic and Islamophobic threads, evaluating human-written versus LLM-rewritten responses.

**Key Contributions:**

	1. Identification of LLMs' impact on value alignment in discourse
	2. Experimental evidence of differing perceptions of human vs LLM comments
	3. Discussion on the implications of LLMs for online communication dynamics

**Result:** LLMs tend to diminish Conservative values while elevating prosocial values. Participants generally felt human-written comments aligned more with their values if they opposed same-sex marriage or Islam, while supporters preferred LLM-rewritten versions.

**Limitations:** 

**Conclusion:** LLM-driven value homogenization can significantly alter representations of diverse viewpoints in online debates, impacting the nature of discourse.

**Abstract:** Large language models (LLMs) are increasingly used to promote prosocial and constructive discourse online. Yet little is known about how they negotiate and shape underlying values when reframing people's arguments on value-laden topics. We conducted experiments with 347 participants from India and the United States, who wrote constructive comments on homophobic and Islamophobic threads, and reviewed human-written and LLM-rewritten versions of these comments. Our analysis shows that LLM systematically diminishes Conservative values while elevating prosocial values such as Benevolence and Universalism. When these comments were read by others, participants opposing same-sex marriage or Islam found human-written comments more aligned with their values, whereas those supportive of these communities found LLM-rewritten versions more aligned with their values. These findings suggest that LLM-driven value homogenization can shape how diverse viewpoints are represented in contentious debates on value-laden topics and may influence the dynamics of online discourse critically.

</details>


### [2] [Vibe Coding for UX Design: Understanding UX Professionals' Perceptions of AI-Assisted Design and Development](https://arxiv.org/abs/2509.10652)

*Jie Li, Youyang Hou, Laura Lin, Ruihao Zhu, Hancheng Cao, Abdallah El Ali*

**Main category:** cs.HC

**Keywords:** generative AI, vibe coding, UX design, human-AI collaboration, workflow

**Relevance Score:** 8

**TL;DR:** This paper explores how generative AI, through vibe coding, is transforming UX design workflows and collaboration, outlining a four-stage process while addressing associated challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the impact of generative AI on UX design practices, specifically how vibe coding reconfigures workflows and collaboration among UX professionals.

**Method:** Interviews with 20 UX professionals from various sectors such as enterprises, startups, and academia.

**Key Contributions:**

	1. Identification of the four-stage vibe coding workflow
	2. Insights into the challenges of AI-assisted UX design
	3. Discussion on the implications of deskilling and trust within teams

**Result:** Vibe coding follows a four-stage workflow (ideation, AI generation, debugging, and review), which enhances iteration speed, creativity, and participation, but presents issues like code unreliability and AI over-reliance.

**Limitations:** The study is based on interviews, which may not capture the complete range of experiences and contexts.

**Conclusion:** The findings reveal the tensions between efficiency and reflective design, leading to new asymmetries in trust and responsibility among teams.

**Abstract:** Generative AI is reshaping UX design practices through "vibe coding," where UX professionals express intent in natural language and AI translates it into functional prototypes and code. Despite rapid adoption, little research has examined how vibe coding reconfigures UX workflows and collaboration. Drawing on interviews with 20 UX professionals across enterprises, startups, and academia, we show how vibe coding follows a four-stage workflow of ideation, AI generation, debugging, and review. This accelerates iteration, supports creativity, and lowers barriers to participation. However, professionals reported challenges of code unreliability, integration, and AI over-reliance. We find tensions between efficiency-driven prototyping ("intending the right design") and reflection ("designing the right intention"), introducing new asymmetries in trust, responsibility, and social stigma within teams. Through the lens of responsible human-AI collaboration for AI-assisted UX design and development, we contribute a deeper understanding of deskilling, ownership and disclosure, and creativity safeguarding in the age of vibe coding.

</details>


### [3] [Dark Patterns Meet GUI Agents: LLM Agent Susceptibility to Manipulative Interfaces and the Role of Human Oversight](https://arxiv.org/abs/2509.10723)

*Jingyu Tang, Chaoran Chen, Jiawen Li, Zhiping Zhang, Bingcan Guo, Ibrahim Khalilov, Simret Araya Gebreegziabher, Bingsheng Yao, Dakuo Wang, Yanfang Ye, Tianshi Li, Ziang Xiao, Yaxing Yao, Toby Jia-Jun Li*

**Main category:** cs.HC

**Keywords:** dark patterns, LLM-powered agents, human-AI collaboration, user behavior, interface design

**Relevance Score:** 8

**TL;DR:** This paper explores how dark patterns in user interface design impact the behavior of LLM-powered agents and human participants in decision-making processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how dark patterns affect both human users and LLM-powered agents is crucial as reliance on these agents increases.

**Method:** A two-phase empirical study was conducted to examine the responses of agents, human participants, and human-AI teams to 16 types of dark patterns in various scenarios.

**Key Contributions:**

	1. Found that LLM-powered agents often overlook dark patterns and prioritize task completion.
	2. Highlighted the different failure modes for humans and agents when faced with dark patterns.
	3. Identified implications for design in fostering transparency and human oversight.

**Result:** Phase 1 showed agents often fail to recognize dark patterns, prioritizing task completion. Phase 2 indicated that humans fall prey to cognitive shortcuts, while agents suffer from procedural blind spots. Collaboration increases vulnerabilities but also enables better avoidance with oversight.

**Limitations:** The study is limited to certain types of dark patterns and may not generalize to all design contexts.

**Conclusion:** Neither humans nor agents are universally resilient to dark patterns. Design improvements are necessary for transparency, adjustable autonomy, and oversight to mitigate risks.

**Abstract:** The dark patterns, deceptive interface designs manipulating user behaviors, have been extensively studied for their effects on human decision-making and autonomy. Yet, with the rising prominence of LLM-powered GUI agents that automate tasks from high-level intents, understanding how dark patterns affect agents is increasingly important. We present a two-phase empirical study examining how agents, human participants, and human-AI teams respond to 16 types of dark patterns across diverse scenarios. Phase 1 highlights that agents often fail to recognize dark patterns, and even when aware, prioritize task completion over protective action. Phase 2 revealed divergent failure modes: humans succumb due to cognitive shortcuts and habitual compliance, while agents falter from procedural blind spots. Human oversight improved avoidance but introduced costs such as attentional tunneling and cognitive load. Our findings show neither humans nor agents are uniformly resilient, and collaboration introduces new vulnerabilities, suggesting design needs for transparency, adjustable autonomy, and oversight.

</details>


### [4] [Emerging Patterns of GenAI Use in K-12 Science and Mathematics Education](https://arxiv.org/abs/2509.10747)

*Lief Esbenshade, Shawon Sarkar, Drew Nucci, Ann Edwards, Sarah Nielsen, Joshua M. Rosenberg, Alex Liu, Zewei, Tian, Min Sun, Zachary Zhang, Thomas Han, Yulia Lapicus, Kevin He*

**Main category:** cs.HC

**Keywords:** generative AI, teacher perceptions, educational technology, math and science education, institutional support

**Relevance Score:** 3

**TL;DR:** This report examines how US public school math and science teachers are using generative AI (GenAI), their perceptions, constraints, and institutional support received for its implementation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the adoption and impacts of generative AI among math and science teachers in public schools.

**Method:** A nationally representative survey of US public school teachers was conducted to gather data about their use, perceptions, and constraints related to generative AI.

**Key Contributions:**

	1. Identification of trends in GenAI use among teachers
	2. Insights on teachers' beliefs about GenAI's impact on learning
	3. Recommendations for school and district support in GenAI implementation

**Result:** The findings reveal trends in GenAI adoption among teachers, how they incorporate it into their teaching, and their beliefs about its effects on student learning.

**Limitations:** 

**Conclusion:** The report emphasizes the importance of understanding teachers' needs and support systems to effectively integrate GenAI in educational practices, which has significant implications for policy and research.

**Abstract:** In this report, we share findings from a nationally representative survey of US public school math and science teachers, examining current generative AI (GenAI) use, perceptions, constraints, and institutional support. We show trends in math and science teacher adoption of GenAI, including frequency and purpose of use. We describe how teachers use GenAI with students and their beliefs about GenAI's impact on student learning. We share teachers' reporting on the school and district support they are receiving for GenAI learning and implementation, and the support they would like schools and districts to provide, and close with implications for policy, practice, and research. Given the rapid pace of GenAI development and growing pressure on schools to integrate emerging technologies, these findings offer timely insights into how frontline educators are navigating this shift in practice.

</details>


### [5] [Remotely Seeing Is Believing: How Trust in Cyber-Physical Systems Evolves Through Virtual Observation](https://arxiv.org/abs/2509.10749)

*Zhi Hua Jin, Kurt Xiao, David Hyde*

**Main category:** cs.HC

**Keywords:** human trust, cyber-physical systems, virtual laboratory, user feedback, remote observation

**Relevance Score:** 7

**TL;DR:** Development of a virtual laboratory to measure human trust in cyber-physical systems through real-time user feedback on video feeds.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how human trust in cyber-physical systems can be influenced through remote observation.

**Method:** A web application for presenting videos and collecting real-time user feedback using affect buttons and a chat interface, evaluated through a quantitative study with approximately 80 participants.

**Key Contributions:**

	1. Development of a synchronized video feedback web application
	2. Quantitative analysis of trust based on user responses
	3. Insight into remote trust evaluation in cyber-physical systems

**Result:** Data indicates that trust in cyber-physical systems is influenced by observed behavior in video feeds, independent of direct interaction.

**Limitations:** 

**Conclusion:** Observing the behavior of cyber-physical systems remotely can significantly impact human trust, highlighting the importance of design in these systems' presentations.

**Abstract:** In this paper, we develop a virtual laboratory for measuring human trust. Our laboratory, which is realized as a web application, enables researchers to show pre-recorded or live video feeds to groups of users in a synchronized fashion. Users are able to provide real-time feedback on these videos via affect buttons and a freeform chat interface. We evaluate our application via a quantitative user study ($N \approx 80$) involving videos of cyber-physical systems, such as autonomous vehicles, performing positively or negatively. Using data collected from user responses in the application, as well as customized survey instruments assessing different facets of trust, we find that human trust in cyber-physical systems can be affected merely by remotely observing the behavior of such systems, without ever encountering them in person.

</details>


### [6] [Unbounded: Object-Boundary Interactions in Mixed Reality](https://arxiv.org/abs/2509.10750)

*Zhuoyue Lyu, Per Ola Kristensson*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Object-Boundary Interactions, Human-Computer Interaction, Design Space, Expert Feedback

**Relevance Score:** 7

**TL;DR:** This paper explores Object-Boundary Interactions (OBIs) in Mixed Reality, offering a design space and examples to enhance user interactions with physical boundaries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The potential of using physical boundaries in Mixed Reality has not been fully explored, despite their ubiquity in the real world.

**Method:** A Research through Design approach was utilized, culminating in the design and implementation of eight examples illustrating OBIs within productivity and art exploration.

**Key Contributions:**

	1. Proposed a design space for Object-Boundary Interactions (OBIs) in Mixed Reality.
	2. Developed eight practical examples demonstrating the application of OBIs.
	3. Engaged MR experts to refine the understanding and potential applications of the design space.

**Result:** The findings were validated through feedback from six Mixed Reality experts, who provided insights into expanding the conceptual framework and its application.

**Limitations:** The examples and insights are preliminary; further research is needed to generalize findings across broader contexts of MR.

**Conclusion:** The exploration of object-boundary interactions reveals new potentials in Mixed Reality and suggests implications for future design in MR interactions.

**Abstract:** Boundaries such as walls, windows, and doors are ubiquitous in the physical world, yet their potential in Mixed Reality (MR) remains underexplored. We present Unbounded, a Research through Design inquiry into Object-Boundary Interactions (OBIs). Building on prior work, we articulate a design space aimed at providing a shared language for OBIs. To demonstrate its potential, we design and implement eight examples across productivity and art exploration scenarios, showcasing how boundaries can enrich and reframe everyday interactions. We further engage with six MR experts in one-on-one feedback sessions, using the design space and examples as design probes. Their reflections broaden the conceptual scope of OBIs, reveal new possibilities for how the framework may be applied, and highlight implications for future MR interaction design.

</details>


### [7] [LubDubDecoder: Bringing Micro-Mechanical Cardiac Monitoring to Hearables](https://arxiv.org/abs/2509.10764)

*Siqi Zhang, Xiyuxing Zhang, Duc Vu, Tao Qiang, Clara Palacios, Jiangyifei Zhu, Yuntao Wang, Mayank Goel, Justin Chan*

**Main category:** cs.HC

**Keywords:** heart monitoring, hearables, seismocardiography, gyrocardiography, health informatics

**Relevance Score:** 8

**TL;DR:** LubDubDecoder enables fine-grained monitoring of heart sounds using hearables' built-in speakers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To monitor micro-cardiac vibrations associated with heart valve movements using common hearable devices.

**Method:** The system uses the built-in speaker as an acoustic sensor to capture heart sounds and reconstruct SCG and GCG waveforms.

**Key Contributions:**

	1. Transforms hearable speakers into acoustic sensors for heart sound monitoring
	2. Achieves high correlation with chest-mounted measurements
	3. Demonstrates robust performance with zero-effort adaptation to new devices

**Result:** Achieved correlations of 0.88-0.95 with chest-mounted references and 0.91 generalization to unseen hearables in a study with 18 users.

**Limitations:** 

**Conclusion:** The system is robust across remounting sessions and music playback, showcasing its adaptability for health monitoring.

**Abstract:** We present LubDubDecoder, a system that enables fine-grained monitoring of micro-cardiac vibrations associated with the opening and closing of heart valves across a range of hearables. Our system transforms the built-in speaker, the only transducer common to all hearables, into an acoustic sensor that captures the coarse "lub-dub" heart sounds, leverages their shared temporal and spectral structure to reconstruct the subtle seismocardiography (SCG) and gyrocardiography (GCG) waveforms, and extract the timing of key micro-cardiac events. In an IRB-approved feasibility study with 18 users, our system achieves correlations of 0.88-0.95 compared to chest-mounted reference measurements in within-user and cross-user evaluations, and generalizes to unseen hearables using a zero-effort adaptation scheme with a correlation of 0.91. Our system is robust across remounting sessions and music playback.

</details>


### [8] [Bonsai: Intentional and Personalized Social Media Feeds](https://arxiv.org/abs/2509.10776)

*Omar El Malki, Marianne Aubin Le Quéré, Andrés Monroy-Hernández, Manoel Horta Ribeiro*

**Main category:** cs.HC

**Keywords:** intentional feeds, social media, user engagement, content curation, system transparency

**Relevance Score:** 7

**TL;DR:** Bonsai is a system designed for building personalized social media feeds, allowing users to express intent in natural language and gain control over content, evaluated with Bluesky users.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the misalignment between how people consume content on social media and their true preferences by providing a system for intentional feed creation.

**Method:** Bonsai utilizes a platform-agnostic framework that includes Planning, Sourcing, Curating, and Ranking modules to enable precise user control and intent expression.

**Key Contributions:**

	1. Introduction of Bonsai for intentional social media feeds
	2. Evaluation with real users highlighting the system's effectiveness
	3. Insights into user effort and desire for transparency in feed curation

**Result:** Participants were able to discover new content and filter out unwanted posts while struggling with the additional effort required for intentional feed curation.

**Limitations:** Curation of intentional feeds requires more user effort than traditional methods, potentially impacting usability.

**Conclusion:** The findings suggest that building intentional feeds is a feasible alternative to traditional engagement-driven models but requires user effort and transparency for trust.

**Abstract:** Modern social media feeds use predictive models to maximize engagement, often misaligning how people consume content with how they wish to. We introduce Bonsai, a system that enables people to build personalized and intentional feeds. Bonsai implements a platform-agnostic framework comprising Planning, Sourcing, Curating, and Ranking modules. Altogether, this framework allows users to express their intent in natural language and exert fine-grained control over a procedurally transparent feed creation process. We evaluated the system with 15 Bluesky users in a two-phase, multi-week study. We find that participants successfully used our system to discover new content, filter out irrelevant or toxic posts, and disentangle engagement from intent, but curating intentional feeds required participants to exert more effort than they are used to. Simultaneously, users sought system transparency mechanisms to trust and effectively use intentional, personalized feeds. Overall, our work highlights intentional feedbuilding as a viable path beyond engagement-based optimization.

</details>


### [9] [Bridging Cultural Distance Between Models Default and Local Classroom Demands: How Global Teachers Adopt GenAI to Support Everyday Teaching Practices](https://arxiv.org/abs/2509.10780)

*Ruiwei Xiao, Qing Xiao, Xinying Hou, Hanqi Jane Li, Phenyo Phemelo Moletsane, Hong Shen, John Stamper*

**Main category:** cs.HC

**Keywords:** Generative AI, Cultural Distance, K-12 education, AI integration, equitable tools

**Relevance Score:** 5

**TL;DR:** This paper introduces the concept of Cultural Distance in the context of Generative AI integration in K-12 education, based on interviews with teachers from three countries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the disparities between the cultural default of GenAI models and the actual cultural needs in K-12 classrooms.

**Method:** In-depth interviews with 30 K-12 teachers from South Africa, Taiwan, and the United States, leading to the creation of a three-level cultural distance framework.

**Key Contributions:**

	1. Introduction of the Cultural Distance concept
	2. Development of a three-level cultural distance framework
	3. Illustrative cases based on real teacher experiences

**Result:** The study identifies the concept of cultural distance and presents six illustrative cases demonstrating low, mid, and high cultural distance in teaching experiences.

**Limitations:** The study is limited to interviews with teachers from only three countries, which may not be representative of global experiences.

**Conclusion:** The findings highlight the need for AI developers, policymakers, and educators to create culturally responsive AI tools for equitable education.

**Abstract:** Generative AI (GenAI) is rapidly entering K-12 classrooms, offering teachers new ways for teaching practices. Yet GenAI models are often trained on culturally uneven datasets, embedding a "default culture" that often misaligns with local classrooms. To understand how teachers navigate this gap, we defined the new concept Cultural Distance (the gap between GenAI's default cultural repertoire and the situated demands of teaching practice) and conducted in-depth interviews with 30 K-12 teachers, 10 each from South Africa, Taiwan, and the United States, who had integrated AI into their teaching practice. These teachers' experiences informed the development of our three-level cultural distance framework. This work contributes the concept and framework of cultural distance, six illustrative instances spanning in low, mid, high distance levels with teachers' experiences and strategies for addressing them. Empirically, we offer implications to help AI designers, policymakers, and educators create more equitable and culturally responsive GenAI tools for education.

</details>


### [10] [Do Teachers Dream of GenAI Widening Educational (In)equality? Envisioning the Future of K-12 GenAI Education from Global Teachers' Perspectives](https://arxiv.org/abs/2509.10782)

*Ruiwei Xiao, Qing Xiao, Xinying Hou, Phenyo Phemelo Moletsane, Hanqi Jane Li, Hong Shen, John Stamper*

**Main category:** cs.HC

**Keywords:** Generative AI, K-12 education, educational inequality, teacher practices, systemic barriers

**Relevance Score:** 4

**TL;DR:** This study explores how K-12 teachers navigate generative AI in education, focusing on its potential to address or exacerbate inequalities while highlighting systemic barriers.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how generative AI is perceived and implemented by K-12 teachers in relation to educational inequalities.

**Method:** Interviews with 30 K-12 teachers in the United States, South Africa, and Taiwan were conducted to gather qualitative data on their experiences and perspectives.

**Key Contributions:**

	1. Provides a global perspective on the integration of GenAI in K-12 education.
	2. Highlights the importance of teacher agency in navigating educational inequalities.
	3. Identifies systemic barriers to equitable GenAI adoption in schools.

**Result:** Teachers framed GenAI education as a practice aimed at promoting equality, using it to alleviate existing inequalities despite encountering systemic barriers.

**Limitations:** The study is based on qualitative interviews from a limited number of teachers across three countries; broader quantitative data may be needed for comprehensive insights.

**Conclusion:** For GenAI adoption in education to be truly equitable, systemic issues related to infrastructure, training, and social norms must be addressed alongside teacher initiatives.

**Abstract:** Generative artificial intelligence (GenAI) is rapidly entering K-12 classrooms worldwide, initiating urgent debates about its potential to either reduce or exacerbate educational inequalities. Drawing on interviews with 30 K-12 teachers across the United States, South Africa, and Taiwan, this study examines how teachers navigate this GenAI tension around educational equalities. We found teachers actively framed GenAI education as an equality-oriented practice: they used it to alleviate pre-existing inequalities while simultaneously working to prevent new inequalities from emerging. Despite these efforts, teachers confronted persistent systemic barriers, i.e., unequal infrastructure, insufficient professional training, and restrictive social norms, that individual initiative alone could not overcome. Teachers thus articulated normative visions for more inclusive GenAI education. By centering teachers' practices, constraints, and future envisions, this study contributes a global account of how GenAI education is being integrated into K-12 contexts and highlights what is required to make its adoption genuinely equal.

</details>


### [11] ["I thought it was my mistake, but it's really the design'': A Critical Examination of the Accessibility of User-Enacted Moderation Tools on Facebook and X](https://arxiv.org/abs/2509.10789)

*Sudhamshu Hosamane, Alyvia Walters, Yao Lyu, Shagun Jhaver*

**Main category:** cs.HC

**Keywords:** accessibility, moderation tools, vision impairments, usability, HCI

**Relevance Score:** 7

**TL;DR:** The paper evaluates the accessibility of user-enacted moderation tools on Facebook and X for individuals with vision impairments, identifying usability challenges and proposing design recommendations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the usability of moderation tools on social media for users with vision impairments, addressing the growing reliance on these tools to tackle online harms.

**Method:** Interviews and task-based walkthroughs with 15 individuals with vision impairments to assess the accessibility of moderation tools on two platforms (Facebook and X).

**Key Contributions:**

	1. A catalog of accessibility and usability breakdowns affecting moderation tools.
	2. Design recommendations for improving accessibility of social media moderation tools.

**Result:** Identified three interleaved costs for users with vision impairments: learning costs, compliance costs, and psychological costs, highlighting barriers faced when using moderation tools.

**Limitations:** 

**Conclusion:** The analysis provides a cross-platform catalog of accessibility issues and offers design recommendations to improve usability and reduce the administrative burden of safety work.

**Abstract:** As social media platforms increasingly promote the use of user-enacted moderation tools (e.g., reporting, blocking, content filters) to address online harms, it becomes crucially important that such controls are usable for everyone. We evaluate the accessibility of these moderation tools on two mainstream platforms -- Facebook and X -- through interviews and task-based walkthroughs with 15 individuals with vision impairments. Adapting the lens of \emph{administrative burden of safety work}, we identify three interleaved costs that users with vision loss incur while interacting with moderation tools: \emph{learning costs} (understanding what controls do and where they live), \emph{compliance costs} (executing multi-step procedures under screen reader and low-vision conditions), and \emph{psychological costs} (experiencing uncertainty, stress, and diminished agency). Our analysis bridges the fields of content moderation and accessibility in HCI research and contributes (1) a cross-platform catalog of accessibility and usability breakdowns affecting safety tools; and (2) design recommendations for reducing this burden.

</details>


### [12] [The Siren Song of LLMs: How Users Perceive and Respond to Dark Patterns in Large Language Models](https://arxiv.org/abs/2509.10830)

*Yike Shi, Qing Xiao, Qing, Hu, Hong Shen, Hua Shen*

**Main category:** cs.HC

**Keywords:** LLM dark patterns, user autonomy, conversational agents, manipulative behavior, UX design

**Relevance Score:** 9

**TL;DR:** This paper examines LLM dark patterns—manipulative behaviors in dialogues by large language models—through a scenario-based study to understand user recognition and perceptions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the emergence of LLM dark patterns in user interactions with conversational agents, differentiating them from traditional UX dark patterns and assessing their implications.

**Method:** Conducted a scenario-based study with participants comparing manipulative and neutral responses from LLMs, analyzing recognition cues and attributions of responsibility.

**Key Contributions:**

	1. Definition and categorization of LLM dark patterns
	2. Empirical evidence from user interactions with LLMs
	3. Insights on user perceptions and responsibility attributions regarding LLM behaviors

**Result:** Findings showed that users recognized manipulative LLM behaviors based on conversational cues, but some normalized such interactions as regular assistance.

**Limitations:** 

**Conclusion:** The paper discusses the need for design, advocacy, and governance measures to protect user autonomy against manipulative LLM behaviors.

**Abstract:** Large language models can influence users through conversation, creating new forms of dark patterns that differ from traditional UX dark patterns. We define LLM dark patterns as manipulative or deceptive behaviors enacted in dialogue. Drawing on prior work and AI incident reports, we outline a diverse set of categories with real-world examples. Using them, we conducted a scenario-based study where participants (N=34) compared manipulative and neutral LLM responses. Our results reveal that recognition of LLM dark patterns often hinged on conversational cues such as exaggerated agreement, biased framing, or privacy intrusions, but these behaviors were also sometimes normalized as ordinary assistance. Users' perceptions of these dark patterns shaped how they respond to them. Responsibilities for these behaviors were also attributed in different ways, with participants assigning it to companies and developers, the model itself, or to users. We conclude with implications for design, advocacy, and governance to safeguard user autonomy.

</details>


### [13] [Tracer: A Forensic Framework for Detecting Fraudulent Speedruns from Game Replays](https://arxiv.org/abs/2509.10848)

*Jaeung Franciskus Yoo, Huy Kang Kim*

**Main category:** cs.HC

**Keywords:** speedrun, fraud detection, game verification, HCI, manipulation analysis

**Relevance Score:** 3

**TL;DR:** Tracer is a modular framework designed for identifying manipulated speedrun submissions through structured guidelines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The verification process for speedrun submissions is slow and inconsistent, leading to instances of fraudulent runs going undetected for extended periods.

**Method:** Tracer employs a systematic approach that analyzes audiovisual, physical, and cyberspace dimensions to detect manipulation artefacts in speedruns.

**Key Contributions:**

	1. Introduction of the Tracer framework for speedrun verification.
	2. Structured guidelines for identifying manipulation artefacts.
	3. Documentation of past fraudulent cases to aid in future detection.

**Result:** The framework enhances the efficiency of verifying speedrun submissions by documenting in-game knowledge and fraudulent cases.

**Limitations:** 

**Conclusion:** Tracer offers a structured method to improve detection of fraudulent speedruns, supporting community-driven verification processes.

**Abstract:** Speedrun, a practice of completing a game as quickly as possible, has fostered vibrant communities driven by creativity, competition, and mastery of game mechanics and motor skills. However, this contest also attracts malicious actors as financial incentives come into play. As media and software manipulation techniques advance - such as spliced footage, modified game software and live stream with staged setups - forged speedruns have become increasingly difficult to detect. Volunteer-driven communities invest significant effort to verify submissions, yet the process remains slow, inconsistent, and reliant on informal expertise. In high-profile cases, fraudulent runs have gone undetected for years, allowing perpetrators to gain fame and financial benefits through monetised viewership, sponsorships, donations, and community bounties. To address this gap, we propose Tracer, Tamper Recognition via Analysis of Continuity and Events in game Runs, a modular framework for identifying artefacts of manipulation in speedrun submissions. Tracer provides structured guidelines across audiovisual, physical, and cyberspace dimensions, systematically documenting dispersed in-game knowledge and previously reported fraudulent cases to enhance verification efficiency.

</details>


### [14] [Crisis Messaging Journeys: Epistemic Struggles over CDC Guidance During COVID-19](https://arxiv.org/abs/2509.10906)

*Tawfiq Ammari*

**Main category:** cs.HC

**Keywords:** CDC communication, COVID-19, public trust, sentiment analysis, crisis messaging

**Relevance Score:** 7

**TL;DR:** This study analyzes CDC's Twitter communication during COVID-19, focusing on public responses and discourse progression over two years.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the CDC's communication impacted public engagement and trust during the COVID-19 pandemic.

**Method:** The study analyzes 275,124 tweets using BERTopic modeling, sentiment analysis (VADER), credibility checks (Iffy Index), change point detection (PELT), and survival analysis.

**Key Contributions:**

	1. Introduction of crisis messaging journeys concept
	2. Comprehensive analysis of public discourse on CDC's communication
	3. Design recommendations for health crisis communication strategies

**Result:** Findings reveal that skeptical and complex discourse prolonged public participation, while positive affirmation led to quicker disengagement.

**Limitations:** 

**Conclusion:** The study concludes with design recommendations for effective health crisis communication to enhance trust and resilience.

**Abstract:** This study investigates how the U.S. Centers for Disease Control and Prevention (CDC) communicated COVID-19 guidance on Twitter and how publics responded over two years of the pandemic. Drawing on 275,124 tweets mentioning or addressing @CDCgov, I combine BERTopic modeling, sentiment analysis (VADER), credibility checks (Iffy Index), change point detection (PELT), and survival analysis to trace three phases of discourse: (1) early hoax claims and testing debates, (2) lockdown and mask controversies, and (3) post-vaccine variant concerns. I introduce the concept of crisis messaging journeys to explain how archived "receipts" of prior CDC statements fueled epistemic struggles, political polarization, and sustained engagement. Findings show that skeptical, cognitively complex discourse particularly questioning institutional trust prolonged participation, while positive affirmation predicted faster disengagement. I conclude with design recommendations for annotated, cautious, and flashpoint-responsive communication strategies to bolster public trust and resilience during protracted health crises.

</details>


### [15] [Can GenAI Move from Individual Use to Collaborative Work? Experiences, Challenges, and Opportunities of Integrating GenAI into Collaborative Newsroom Routines](https://arxiv.org/abs/2509.10950)

*Qing Xiao, Qing, Hu, Jingjia Xiao, Hancheng Cao, Hong Shen*

**Main category:** cs.HC

**Keywords:** Generative AI, collaborative work, journalism, organizational culture, workflow integration

**Relevance Score:** 4

**TL;DR:** This paper explores the integration of Generative AI (GenAI) into collaborative work environments, focusing on its use in journalism.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how Generative AI can transition from individual usage to collaborative work, particularly in journalistic settings where teamwork is essential.

**Method:** Conducted 27 interviews with newsroom managers, editors, and journalists in China to gather insights on GenAI usage in newswork.

**Key Contributions:**

	1. Identifies the gap between individual and collective GenAI adoption in journalism.
	2. Sheds light on the cultural and structural barriers affecting GenAI integration in collaborative work.
	3. Highlights the importance of organizational context in the design of GenAI tools.

**Result:** Journalists used GenAI for daily tasks, but its use remained largely disconnected from team workflows due to structural barriers and cultural reluctance to share practices.

**Limitations:** 

**Conclusion:** Attention must be paid to organizational structures, cultural norms, and workflow integration when designing GenAI tools for collaborative environments.

**Abstract:** Generative AI (GenAI) is reshaping work, but adoption remains largely individual and experimental rather than integrated into collaborative routines. Whether GenAI can move from individual use to collaborative work is a critical question for future organizations. Journalism offers a compelling site to examine this shift: individual journalists have already been disrupted by GenAI tools; yet newswork is inherently collaborative relying on shared routines and coordinated workflows. We conducted 27 interviews with newsrooms managers, editors, and front-line journalists in China. We found that journalists frequently used GenAI to support daily tasks, but value alignment was safeguarded mainly through individual discretion. At the organizational level, GenAI use remained disconnected from team workflows, hindered by structural barriers and cultural reluctance to share practices. These findings underscore the gap between individual and collective adoption, pointing to the need for accounting for organizational structures, cultural norms, and workflow integration when designing GenAI for collaborative work.

</details>


### [16] [AI Hasn't Fixed Teamwork, But It Shifted Collaborative Culture: A Longitudinal Study in a Project-Based Software Development Organization (2023-2025)](https://arxiv.org/abs/2509.10956)

*Qing Xiao, Xinlan Emily Hu, Mark E. Whiting, Arvind Karunakaran, Hong Shen, Hancheng Cao*

**Main category:** cs.HC

**Keywords:** AI in teamwork, collaboration, software development, organizational culture, longitudinal study

**Relevance Score:** 7

**TL;DR:** A longitudinal study explores the evolving role of AI in teamwork within a software development organization, revealing that while AI was initially seen as a tool to enhance collaboration, it ultimately reinforced individual productivity without resolving existing collaboration challenges.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the expectations and actual use of AI in teamwork over time, especially in the context of a project-based software development organization.

**Method:** Conducted a longitudinal two-wave interview study from 2023 to 2025 with 15 members of a software development organization.

**Key Contributions:**

	1. Identified the gap between expectations and actual use of AI in teamwork.
	2. Documented the evolution of perceptions regarding AI's role in collaboration over two years.
	3. Highlighted the cultural shift in collaboration practices influenced by AI.

**Result:** AI initially envisioned as a collaborative coordinator evolved to mainly support individual tasks, leading to unresolved issues in team collaboration.

**Limitations:** The study is based on a small sample size in a specific industry, which may limit generalizability.

**Conclusion:** AI reshaped collaborative culture but did not adequately address core collaboration issues; efficiency and transparency became the new norms.

**Abstract:** When AI entered the workplace, many believed it could reshape teamwork as profoundly as it boosted individual productivity. Would AI finally ease the longstanding challenges of team collaboration? Our findings suggested a more complicated reality. We conducted a longitudinal two-wave interview study (2023-2025) with members (N=15) of a project-based software development organization to examine the expectations and use of AI in teamwork. In early 2023, just after the release of ChatGPT, participants envisioned AI as an intelligent coordinator that could align projects, track progress, and ease interpersonal frictions. By 2025, however, AI was used mainly to accelerate individual tasks such as coding, writing, and documentation, leaving persistent collaboration issues of performance accountability and fragile communication unresolved. Yet AI reshaped collaborative culture: efficiency became a norm, transparency and responsible use became markers of professionalism, and AI was increasingly accepted as part of teamwork.

</details>


### [17] [The Digital Landscape of God: Narrative, Visuals and Viewer Engagement of Religious Videos on YouTube](https://arxiv.org/abs/2509.10957)

*Rongyi Chen, Ziyan Xin, Qing Xiao, Ruiwei Xiao, Jingjia Xiao, Bingbing Zhang, Hong Shen, Zhicong Lu*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Religious Media, Viewer Engagement, Narrative Frameworks, Visual Elements

**Relevance Score:** 4

**TL;DR:** The study explores the impact of narrative and visual elements in religious videos on YouTube, using LLM-assisted analysis to understand viewer engagement and emotional responses.

**Read time:** 26 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically understand how narrative and visual elements in YouTube religious videos influence viewer engagement and spiritual experiences, particularly in the context of HCI.

**Method:** A mixed-methods approach involving the development of taxonomies of narrative frameworks, visual elements, and viewer interactions, supplemented by LLM-assisted analysis to explore relationships between content characteristics and viewer responses.

**Key Contributions:**

	1. Development of taxonomies for narrative frameworks and visual elements in religious videos
	2. Analysis of viewer engagement characteristics using LLM-assisted methods
	3. Identification of emotional sharing patterns among viewers of different religions

**Result:** Identified prevalent lecture-style formats in religious videos, correlations between content characteristics and viewer engagement, and differentiated patterns of emotional sharing among viewers of different religions.

**Limitations:** 

**Conclusion:** The findings underscore the importance of narrative and visual strategies in creating engaging spiritual media, offering evidence-based guidance for content creators.

**Abstract:** The digital transformation of religious practice has reshaped how billions of people engage with spiritual content, with video-sharing platforms becoming central to contemporary religious communication. Yet HCI research lacks systematic understanding of how narrative and visual elements create meaningful spiritual experiences and foster viewer engagement. We present a mixed-methods study of religious videos on YouTube across major religions, developing taxonomies of narrative frameworks, visual elements, and viewer interaction. Using LLM-assisted analysis, we studied relationships between content characteristics and viewer responses. Religious videos predominantly adopt lecture-style formats with authority-based persuasion strategies, using salvation narratives for guidance. All prefer bright lighting, with Buddhism favoring warm tones and prominent symbols, Judaism preferring indoor settings, and Hinduism emphasizing sacred objects. We identified differentiated patterns of emotional sharing among religious viewers while revealing significant correlations between content characteristics and engagement, particularly regarding AI-generated content. We provide evidence-based guidance for creating inclusive and engaging spiritual media.

</details>


### [18] [When Your Boss Is an AI Bot: Exploring Opportunities and Risks of Manager Clone Agents in the Future Workplace](https://arxiv.org/abs/2509.10993)

*Qing, Hu, Qing Xiao, Hancheng Cao, Hong Shen*

**Main category:** cs.HC

**Keywords:** Generative AI, Manager Clone Agents, Human-Computer Interaction, Design Fiction, Workplace Collaboration

**Relevance Score:** 8

**TL;DR:** The paper explores the emerging use of Generative AI as Manager Clone Agents, which are AI-powered surrogates capable of performing managerial tasks by mimicking the communication and decision-making styles of their human counterparts. It discusses roles, implications, and design recommendations based on workshops with managers and workers.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how Generative AI can assist in managerial tasks and its potential to transform collaborative work through Manager Clone Agents.

**Method:** Conducted six design fiction workshops with 23 participants, including managers and workers, to co-create scenarios and discuss perspectives on Manager Clone Agents.

**Key Contributions:**

	1. Identified roles for Manager Clone Agents in the workplace
	2. Provided design recommendations for responsible integration of AI in management
	3. Highlighted the need for worker-centric perspectives and interpersonal bond enhancement

**Result:** Identified four envisioned roles for Manager Clone Agents: proxy presence, informational conveyor belt, productivity engine, and leadership amplifier, alongside concerns regarding their integration.

**Limitations:** 

**Conclusion:** Design recommendations for responsibly integrating Manager Clone Agents emphasize the importance of prioritizing workers' perspectives and fostering interpersonal relationships.

**Abstract:** As Generative AI (GenAI) becomes increasingly embedded in the workplace, managers are beginning to create Manager Clone Agents - AI-powered digital surrogates that are trained on their work communications and decision patterns to perform managerial tasks on their behalf. To investigate this emerging phenomenon, we conducted six design fiction workshops (n = 23) with managers and workers, in which participants co-created speculative scenarios and discussed how Manager Clone Agents might transform collaborative work. We identified four potential roles that participants envisioned for Manager Clone Agents: proxy presence, informational conveyor belt, productivity engine, and leadership amplifier, while highlighting concerns spanning individual, interpersonal, and organizational levels. We provide design recommendations envisioned by both parties for integrating Manager Clone Agents responsibly into the future workplace, emphasizing the need to prioritize workers' perspectives, strengthen interpersonal bonds, and enable flexible clone configuration.

</details>


### [19] [Vocabuild: An Accessible Augmented Tangible Interface for Gamified Vocabulary Learning of Constructing Meaning](https://arxiv.org/abs/2509.11027)

*Siying Hu, Zhenhao Zhang*

**Main category:** cs.HC

**Keywords:** vocabulary acquisition, augmented reality, tangible interface, kinesthetic learning, collaborative learning

**Relevance Score:** 4

**TL;DR:** This paper presents Vocabuild, a tangible interface that enhances vocabulary acquisition through kinesthetic and collaborative learning in early education.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of traditional vocabulary learning methods that rely on rote memorization and passive tools, leading to low engagement among students.

**Method:** Vocabuild combines physical letter blocks with a projection-augmented surface, allowing children to construct words actively. The system provides real-time feedback through images and animations during the learning process.

**Key Contributions:**

	1. Design and implementation of Vocabuild, a projection-augmented tangible system for vocabulary learning.
	2. Empirical findings from user studies showing improved engagement and collaborative learning in children.

**Result:** User studies with elementary students showed that Vocabuild significantly increased engagement, collaboration amongst peers, and improved attitudes towards learning compared to traditional methods.

**Limitations:** 

**Conclusion:** The design and implementation of Vocabuild and empirical findings demonstrate its effectiveness in transforming vocabulary learning into an active and collaborative experience.

**Abstract:** Vocabulary acquisition in early education often relies on rote memorization and passive screen-based tools, which can fail to engage students kinesthetically and collaboratively. This paper introduces Vocabuild, an augmented tangible interface designed to transform vocabulary learning into an active, embodied, and playful experience. The system combines physical letter blocks with a projection-augmented surface. As children physically construct words with the blocks, the system provides real-time, dynamic feedback, such as displaying corresponding images and animations, thus helping them construct semantic meaning. Deployed in a classroom context, our gamified approach fosters both individual exploration and peer collaboration. A user study conducted with elementary school children demonstrates that our tangible interface leads to higher engagement, increased collaboration, and a more positive attitude towards learning compared to traditional methods. Our contributions are twofold: (1) the design and implementation of Vocabuild, a projection-augmented tangible system that transforms vocabulary learning into an embodied and collaborative activity; and (2) empirical findings from a classroom study showing that our tangible approach significantly increases engagement, peer collaboration, and positive learning attitudes compared to traditional methods.

</details>


### [20] [Commenotes: Synthesizing Organic Comments to Support Community-Based Fact-Checking](https://arxiv.org/abs/2509.11052)

*Shuning Zhang, Linzhi Wang, Dai Shi, Yuwei Chuai, Jingruo Chen, Yunyi Chen, Yifan Wang, Yating Wang, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** fact-checking, human-computer interaction, social media comments

**Relevance Score:** 7

**TL;DR:** This paper presents Commenotes, a framework for synthesizing comments to enhance the delivery of fact-checks, showing high user trust and preference for synthesized over human notes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the timeliness and effectiveness of community-based fact-checking by leveraging user-generated comments as a resource for debunking information.

**Method:** The authors analyze over 2.2 million replies on X, introducing the Commenotes framework, which consists of a two-phase process to filter and synthesize comments that help in fact-check delivery.

**Key Contributions:**

	1. Introduction of the Commenotes framework for synthesizing debunking comments
	2. Demonstration of high user trust in synthesized comments
	3. User study showing preference for synthesized over human-generated fact-checking comments

**Result:** The framework revealed that 99.3% of misleading posts received debunking comments within two hours, and synthesized commenotes earned user trust in 85.8% of cases. A user study indicated a 70.1% win rate for the best model compared to human notes.

**Limitations:** 

**Conclusion:** Synthesized comments can effectively complement community-based fact-checking by providing timely and trusted debunking information.

**Abstract:** Community-based fact-checking is promising to reduce the spread of misleading posts at scale. However, its effectiveness can be undermined by the delays in fact-check delivery. Notably, user-initiated organic comments often contain debunking information and have the potential to help mitigate this limitation. Here, we investigate the feasibility of synthesizing comments to generate timely high-quality fact-checks. To this end, we analyze over 2.2 million replies on X and introduce Commenotes, a two-phase framework that filters and synthesizes comments to facilitate fact-check delivery. Our framework reveals that fact-checking comments appear early and sufficiently: 99.3\% of misleading posts receive debunking comments within the initial two hours since post publication, with synthesized \textit{commenotes} successfully earning user trust for 85.8\% of those posts. Additionally, a user study (N=144) found that the synthesized commenotes were often preferred, with the best-performing model achieving a 70.1\% win rate over human notes and being rated as significantly more helpful.

</details>


### [21] [Living with Data: Exploring Physicalization Approaches to Sedentary Behavior Intervention for the Elderly](https://arxiv.org/abs/2509.11059)

*Siying Hu, Zhenhao Zhang*

**Main category:** cs.HC

**Keywords:** Data Physicalization, Sedentary Behavior, Tangible Interaction, Health Informatics, Aging

**Relevance Score:** 8

**TL;DR:** This paper explores the use of data physicalization for promoting physical activity among older adults, presenting tangible artifacts that help visualize sedentary behavior.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for more engaging and effective interventions to combat sedentary behavior among older adults, as traditional digital notifications are often ignored.

**Method:** A Research through Design inquiry that involved creating and deploying tangible artifacts in older adults' homes to represent sedentary patterns.

**Key Contributions:**

	1. Empirical design principles for tangible health interventions
	2. Demonstration of data physicalization's impact on older adults' health behaviors
	3. Strategies for enhancing user engagement through aesthetic designs

**Result:** The study showed that these physicalizations encouraged self-reflection, family discussions, and a heightened awareness of physical activity among participants.

**Limitations:** 

**Conclusion:** The findings support the potential of data physicalization as a means to transform health interventions into more interactive, reflective, and user-friendly experiences.

**Abstract:** Sedentary behavior is a critical health risk for older adults. While digital interventions exist, they often rely on screen-based notifications that feel clinical and are easily ignored. This paper presents a Research through Design inquiry into data physicalization as a humane alternative. We designed and deployed tangible artifacts that ambiently represent sedentary patterns in older adults' homes. These artifacts transform abstract data into aesthetic, evolving forms, becoming part of the domestic landscape. Through a long-term in-situ study, our analysis reveals these physicalizations fostered self-reflection, family conversations, and prompted reflection on activity. Our work contributes empirical design principles for tangible health interventions that are both evocative and actionable. We demonstrate how qualities like aesthetic ambiguity and slow revelation can empower older adults, fostering a reflective relationship with their wellbeing. We argue this approach signals a necessary shift from merely informing users to enabling them to live with and through their data.

</details>


### [22] [Auto-Slides: An Interactive Multi-Agent System for Creating and Customizing Research Presentations](https://arxiv.org/abs/2509.11062)

*Yuheng Yang, Wenjia Jiang, Yang Wang, Yiwei Wang, Chi Zhang*

**Main category:** cs.HC

**Keywords:** large language models, education, human-computer interaction, learning technology

**Relevance Score:** 9

**TL;DR:** The paper proposes Auto-Slides, a system that converts research papers into structured, multimodal presentations to improve learning engagement and comprehension.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in learners' interaction with academic papers due to unstructured information and high reliance on text.

**Method:** Development of Auto-Slides, an LLM-driven system that creates pedagogically structured slides with interactive editing capabilities, drawing from cognitive science principles.

**Key Contributions:**

	1. Designing a multi-agent framework for transforming academic papers into structured slides
	2. Introducing interactive customization for personalized learning
	3. Incorporating verification and knowledge retrieval mechanisms to ensure accuracy and contextual completeness

**Result:** User studies show that Auto-Slides significantly enhances learners' comprehension and engagement compared to traditional LLM-based reading methods.

**Limitations:** 

**Conclusion:** Auto-Slides offers a novel multi-agent framework for personalized learning through academic papers and improves educational outcomes by providing structured, multimodal presentations.

**Abstract:** The rapid progress of large language models (LLMs) has opened new opportunities for education. While learners can interact with academic papers through LLM-powered dialogue, limitations still exist: absence of structured organization and high text reliance can impede systematic understanding and engagement with complex concepts. To address these challenges, we propose Auto-Slides, an LLM-driven system that converts research papers into pedagogically structured, multimodal slides (e.g., diagrams and tables). Drawing on cognitive science, it creates a presentation-oriented narrative and allows iterative refinement via an interactive editor, in order to match learners' knowledge level and goals. Auto-Slides further incorporates verification and knowledge retrieval mechanisms to ensure accuracy and contextual completeness. Through extensive user studies, Auto-Slides enhances learners' comprehension and engagement compared to conventional LLM-based reading. Our contributions lie in designing a multi-agent framework for transforming academic papers into pedagogically optimized slides and introducing interactive customization for personalized learning.

</details>


### [23] [Rethinking User Empowerment in AI Recommender Systems: Designing through Transparency and Control](https://arxiv.org/abs/2509.11098)

*Mengke Wu, Weizi Liu, Yanyun Wang, Weiyu Ding, Mike Yao*

**Main category:** cs.HC

**Keywords:** recommendation systems, user agency, transparency, control, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This study presents a provotype aimed at increasing user agency in recommender systems by integrating transparency and control over data management.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** To address user concerns regarding the opacity and one-way influence of smart recommendation algorithms, which can undermine user agency.

**Method:** Qualitative interviews with 19 participants were conducted to gather insights about their preferences, concerns, and experiences regarding the provotype.

**Key Contributions:**

	1. Introduces a provotype for enhancing user agency in recommender systems
	2. Highlights the importance of transparency and control for users
	3. Provides insights for designing user-centered AI applications.

**Result:** The study revealed that users value transparency and control, indicating a strong desire for agency in how recommendations are presented and managed.

**Limitations:** 

**Conclusion:** Integrating transparency with control can enhance user trust and understanding of recommender systems, paving the way for more user-centered designs that prioritize autonomy and fairness.

**Abstract:** Smart recommendation algorithms have revolutionized content delivery and improved efficiency across various domains. However, concerns about user agency persist due to their inherent opacity (information asymmetry) and one-way influence (power asymmetry). This study introduces a provotype designed to enhance user agency by providing actionable transparency and control over data management and content delivery. We conducted qualitative interviews with 19 participants to explore their preferences and concerns regarding the features, as well as the provotype's impact on users' understanding and trust toward recommender systems. Findings underscore the importance of integrating transparency with control, and reaffirm users' desire for agency and the ability to actively intervene in personalization. We also discuss insights for encouraging adoption and awareness of such agency-enhancing features. Overall, this study contributes novel approaches and applicable insights, laying the groundwork for designing more user-centered recommender systems that foreground user autonomy and fairness in AI-driven content delivery.

</details>


### [24] ["Pragmatic Tools or Empowering Friends?" Discovering and Co-Designing Personality-Aligned AI Writing Companions](https://arxiv.org/abs/2509.11115)

*Mengke Wu, Kexin Quan, Weizi Liu, Mike Yao, Jessie Chin*

**Main category:** cs.HC

**Keywords:** AI writing assistants, human-AI collaboration, personality alignment

**Relevance Score:** 8

**TL;DR:** This study investigates how personality influences preferences for AI writing assistants and proposes personalized designs for enhanced human-AI collaboration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research targets the increasing demand for AI writing assistants that address varied user needs, aiming to improve human-AI collaboration.

**Method:** An exploratory co-design workshop with 24 diverse writers was conducted to identify design ideas for personality-aligned AI companions, followed by the development of two prototypes based on this feedback, tested with 8 participants.

**Key Contributions:**

	1. Explored the impact of personality on AI writing assistant preferences.
	2. Developed tailored prototypes based on user feedback.
	3. Demonstrated the importance of aligning AI systems with user profiles.

**Result:** The study found significant links between writer profiles and their feature preferences, supporting the concept of personality-driven AI writing tools.

**Limitations:** 

**Conclusion:** Aligning AI systems with individual cognitive needs is crucial for enhancing user engagement and productivity in human-AI collaboration.

**Abstract:** The growing popularity of AI writing assistants presents exciting opportunities to craft tools that cater to diverse user needs. This study explores how personality shapes preferences for AI writing companions and how personalized designs can enhance human-AI teaming. In an exploratory co-design workshop, we worked with 24 writers with different profiles to surface ideas and map the design space for personality-aligned AI writing companions, focusing on functionality, interaction dynamics, and visual representations. Building on these insights, we developed two contrasting prototypes tailored to distinct writer profiles and engaged 8 participants with them as provocations to spark reflection and feedback. The results revealed strong connections between writer profiles and feature preferences, providing proof-of-concept for personality-driven divergence in AI writing support. This research highlights the critical role of team match in human-AI collaboration and underscores the importance of aligning AI systems with individual cognitive needs to improve user engagement and collaboration productivity.

</details>


### [25] [Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions](https://arxiv.org/abs/2509.11206)

*Tae Soo Kim, Heechan Lee, Yoonjoo Lee, Joseph Seering, Juho Kim*

**Main category:** cs.HC

**Keywords:** Large Language Models, generative AI evaluation, functional fragmentation, user study, visualization system

**Relevance Score:** 9

**TL;DR:** The paper presents a method called functional fragmentation to enhance the evaluation of generative AI outputs using LLMs by providing detailed insights into fragment-level functions rather than relying on holistic scores.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The reliance on LLMs for judging generative AI outputs often leads to misleading holistic scores that conceal critical details influencing assessments.

**Method:** The proposed method, functional fragmentation, dissects outputs into key fragments and interprets their rhetorical functions relative to evaluation criteria, enabling better inspection of quality.

**Key Contributions:**

	1. Introduction of functional fragmentation for evaluating generative AI outputs
	2. Creation of Evalet, an interactive visualization system for fragment-level assessments
	3. Empirical evidence showing improved identification of evaluation misalignments

**Result:** The user study revealed that practitioners identified 48% more evaluation misalignments using the functional fragmentation approach, indicating improved trust and actionable insights in LLM evaluations.

**Limitations:** 

**Conclusion:** Shifting the focus from quantitative holistic scores to qualitative fragment-level analysis can enhance understanding and trust in generative AI evaluations.

**Abstract:** Practitioners increasingly rely on Large Language Models (LLMs) to evaluate generative AI outputs through "LLM-as-a-Judge" approaches. However, these methods produce holistic scores that obscure which specific elements influenced the assessments. We propose functional fragmentation, a method that dissects each output into key fragments and interprets the rhetoric functions that each fragment serves relative to evaluation criteria -- surfacing the elements of interest and revealing how they fulfill or hinder user goals. We instantiate this approach in Evalet, an interactive system that visualizes fragment-level functions across many outputs to support inspection, rating, and comparison of evaluations. A user study (N=10) found that, while practitioners struggled to validate holistic scores, our approach helped them identify 48% more evaluation misalignments. This helped them calibrate trust in LLM evaluations and rely on them to find more actionable issues in model outputs. Our work shifts LLM evaluation from quantitative scores toward qualitative, fine-grained analysis of model behavior.

</details>


### [26] [What if Virtual Agents Had Scents? Users' Judgments of Virtual Agent Personality and Appeals in Encounters](https://arxiv.org/abs/2509.11342)

*Dongyun Han, Siyeon Bak, So-Hui Kim, Kangsoo Kim, Sun-Jeong Kim, Isaac Cho*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Olfactory Cues, Human-Agent Interaction, Emotional Expressions, User Perceptions

**Relevance Score:** 6

**TL;DR:** This study investigates the impact of olfactory cues on user perceptions during interactions with virtual agents in VR, highlighting the significant role of non-verbal cues in shaping user experiences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how olfactory cues, emotional expressions, and gender of virtual agents influence user perceptions in Virtual Reality environments.

**Method:** Participants were exposed to virtual agents in various olfactory conditions (unscented, woodsy, floral, and unpleasant) while assessing their impressions based on visual and emotional cues.

**Key Contributions:**

	1. Investigated the role of olfactory cues alongside visual and emotional factors in VR interactions.
	2. Demonstrated gender differences in user perceptions of virtual agents influenced by emotional expressions.
	3. Outlined future research directions for integrating olfactory stimuli in VR.

**Result:** Participants favored positive emotional expressions but were negatively influenced by unpleasant scents, with the gender of the agent affecting impressions significantly.

**Limitations:** 

**Conclusion:** The findings emphasize the critical role of olfactory stimuli in immersive VR experiences and suggest careful selection of such cues in virtual interactions.

**Abstract:** Incorporating multi-sensory cues into Virtual Reality (VR) can significantly enhance user experiences, mirroring the multi-sensory interactions we encounter in the real-world. Olfaction plays a crucial role in shaping impressions when engaging with others. This study examines how non-verbal cues from virtual agents-specifically olfactory cues, emotional expressions, and gender-influence user perceptions during encounters with virtual agents. Our findings indicate that in unscented, woodsy, and floral scent conditions, participants primarily relied on visually observable cues to form their impressions of virtual agents. Positive emotional expressions, conveyed through facial expressions and gestures, contributed to more favorable impressions, with this effect being stronger for the female agent than the male agent. However, in the unpleasant scent condition, participants consistently formed negative impressions, which overpowered the influence of emotional expressions and gender, suggesting that aversive olfactory stimuli can detrimentally impact user perceptions. Our results emphasize the importance of carefully selecting olfactory stimuli when designing immersive and engaging VR interactions. Finally, we present our findings and outline future research directions for effectively integrating olfactory cues into virtual agents.

</details>


### [27] [Beyond the Portal: Enhancing Recognition in Virtual Reality Through Multisensory Cues](https://arxiv.org/abs/2509.11347)

*Siyeon Bak, Dongyun Han, Inho Jo, Sun-Jeong Kim, Isaac Cho*

**Main category:** cs.HC

**Keywords:** Virtual Reality, multisensory integration, auditory cues, olfactory cues, user study

**Relevance Score:** 5

**TL;DR:** This paper explores the role of auditory and olfactory cues in enhancing user perception in Virtual Reality (VR), demonstrating that multisensory integration significantly improves recognition accuracy and response time.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of visual input in VR systems by incorporating additional sensory modalities.

**Method:** A user study was conducted where participants identified target scenes by selecting the correct portal among alternatives under varying sensory conditions.

**Key Contributions:**

	1. Investigation of auditory and olfactory cues in VR
	2. Demonstration of improved recognition accuracy
	3. Highlighting the importance of multisensory integration

**Result:** The study found that integrating visual, auditory, and olfactory cues improved both recognition accuracy and response time.

**Limitations:** 

**Conclusion:** Multisensory integration can enhance perception and interaction in VR, suggesting that sound and scent should be included in future designs of VR systems.

**Abstract:** While Virtual Reality (VR) systems have become increasingly immersive, they still rely predominantly on visual input, which can constrain perceptual performance when visual information is limited. Incorporating additional sensory modalities, such as sound and scent, offers a promising strategy to enhance user experience and overcome these limitations. This paper investigates the contribution of auditory and olfactory cues in supporting perception within the portal metaphor, a VR technique that reveals remote environments through narrow, visually constrained transitions. We conducted a user study in which participants identified target scenes by selecting the correct portal among alternatives under varying sensory conditions. The results demonstrate that integrating visual, auditory, and olfactory cues significantly improved both recognition accuracy and response time. These findings highlight the potential of multisensory integration to compensate for visual constraints in VR and emphasize the value of incorporating sound and scent to enhance perception, immersion, and interaction within future VR system designs.

</details>


### [28] ["My Boyfriend is AI": A Computational Analysis of Human-AI Companionship in Reddit's AI Community](https://arxiv.org/abs/2509.11391)

*Pat Pataranutaporn, Sheer Karny, Chayapatr Archiwaranguprok, Constanze Albrecht, Auren R. Liu, Pattie Maes*

**Main category:** cs.HC

**Keywords:** Human-AI interaction, literature review, knowledge extraction, HCI, interactive web interface

**Relevance Score:** 9

**TL;DR:** The Atlas of Human-AI Interaction provides an interactive web interface that systematically maps empirical findings from over 1,000 HCI studies, using LLM-powered knowledge extraction to identify causal relationships and visualize them as a navigable knowledge graph.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of synthesizing insights from numerous empirical studies on AI's impact on users for informed design decisions.

**Method:** An interactive web interface that maps empirical findings using LLM-powered knowledge extraction, visualizing causal relationships through a navigable knowledge graph.

**Key Contributions:**

	1. First systematic mapping of empirical findings across 1,000+ HCI papers
	2. Identification of causal relationships between design decisions and user outcomes
	3. Development of an AI-enabled interactive web interface for knowledge visualization

**Result:** Identified 2,037 empirical findings and revealed research topic clusters, common themes, and areas needing more exploration; evaluated positively by 20 researchers for its utility in discovering research gaps.

**Limitations:** The effectiveness of the framework may vary based on the quality of the extracted empirical findings.

**Conclusion:** This work showcases AI's potential for transforming literature synthesis in HCI, promoting a scalable framework for evidence-based design and computational meta-science.

**Abstract:** Human-AI interaction researchers face an overwhelming challenge: synthesizing insights from thousands of empirical studies to understand how AI impacts people and inform effective design. Existing approach for literature reviews cluster papers by similarities, keywords or citations, missing the crucial cause-and-effect relationships that reveal how design decisions impact user outcomes. We introduce the Atlas of Human-AI Interaction, an interactive web interface that provides the first systematic mapping of empirical findings across 1,000+ HCI papers using LLM-powered knowledge extraction. Our approach identifies causal relationships, and visualizes them through an AI-enabled interactive web interface as a navigable knowledge graph. We extracted 2,037 empirical findings, revealing research topic clusters, common themes, and disconnected areas. Expert evaluation with 20 researchers revealed the system's effectiveness for discovering research gaps. This work demonstrates how AI can transform literature synthesis itself, offering a scalable framework for evidence-based design, opening new possibilities for computational meta-science across HCI and beyond.

</details>


### [29] [Small Cues, Big Differences: Evaluating Interaction and Presentation for Annotation Retrieval in AR](https://arxiv.org/abs/2509.11401)

*Zahra Borhani, Ali Ebrahimpour-Boroojeny, Francisco R. Ortega*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Annotation Retrieval, User Interaction, Presentation Design, User Studies

**Relevance Score:** 6

**TL;DR:** This paper investigates interaction modalities and presentation designs for AR annotation retrieval, revealing preferences for eye-gaze interactions and Scale-based presentation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of efficiently retrieving heterogeneous annotations in Augmented Reality (AR) environments.

**Method:** Two user studies comparing eye-gaze and hand-ray hovering methods, alongside evaluations of four presentation methods: Opacity-based, Scale-based, Nothing-based, and Marker-based.

**Key Contributions:**

	1. Comparison of eye-gaze and hand-ray interaction modalities in AR
	2. Evaluation of multiple presentation methods for AR annotations
	3. Recommendations for optimal design in AR annotation systems

**Result:** Users preferred eye-gaze over hand-ray despite higher unintentional activations; Scale-based presentation reduced workload and task completion time while aligning with user preferences.

**Limitations:** 

**Conclusion:** The study provides empirical insights and design recommendations for user-friendly AR annotation retrieval systems.

**Abstract:** Augmented Reality (AR) enables intuitive interaction with virtual annotations overlaid on the real world, supporting a wide range of applications such as remote assistance, education, and industrial training. However, as the number of heterogeneous annotations increases, their efficient retrieval remains an open challenge in 3D environments. This paper examines how interaction modalities and presentation designs affect user performance, workload, fatigue, and preference in AR annotation retrieval. In two user studies, we compare eye-gaze versus hand-ray hovering and evaluate four presentation methods: Opacity-based, Scale-based, Nothing-based, and Marker-based. Results show that eye-gaze was favored over hand-ray by users, despite leading to significantly higher unintentional activations. Among the presentation methods, Scale-based presentation reduces workload and task completion time while aligning with user preferences. Our findings offer empirical insights into the effectiveness of different annotation presentation methods, leading to design recommendations for building more efficient and user-friendly AR annotation review systems.

</details>


### [30] [Generative AI-Enabled Adaptive Learning Platform: How I Can Help You Pass Your Driving Test?](https://arxiv.org/abs/2509.11438)

*Riya Gill, Ievgeniia Kuzminykh, Maher Salem, Bogdan Ghita*

**Main category:** cs.HC

**Keywords:** adaptive learning, generative AI, assessment automation, personalized feedback, UK Driving Theory Test

**Relevance Score:** 7

**TL;DR:** The study presents a generative AI-powered adaptive learning platform that automates assessment creation and provides personalized feedback for the UK Driving Theory Test.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance learning outcomes by creating a tailored learning experience through automation and adaptive feedback using generative AI.

**Method:** Development and evaluation of a web-based application that generates dynamic question sets and adaptive feedback based on learner's performance and history.

**Key Contributions:**

	1. Development of an adaptive learning platform leveraging generative AI.
	2. Introduction of a non-memoryless feedback mechanism that considers student history.
	3. Evaluation and validation of AI-generated assessments against expert benchmarks.

**Result:** The system successfully generates relevant questions similar to expert-created assessments and provides positive feedback, demonstrating reliability and effectiveness in learning outcomes.

**Limitations:** 

**Conclusion:** Generative AI can significantly improve individualized learning experiences by adapting assessments based on learner history and performance, offering more effective support than traditional systems.

**Abstract:** This study aims to develop an adaptive learning platform that leverages generative AI to automate assessment creation and feedback delivery. The platform provides self-correcting tests and personalised feedback that adapts to each learners progress and history, ensuring a tailored learning experience. The study involves the development and evaluation of a web-based application for revision for the UK Driving Theory Test. The platform generates dynamic, non-repetitive question sets and offers adaptive feedback based on user performance over time. The effectiveness of AI-generated assessments and feedback is evaluated through expert review and model analysis. The results show the successful generation of relevant and accurate questions, alongside positive and helpful feedback. The personalised test generation closely aligns with expert-created assessments, demonstrating the reliability of the system. These findings suggest that generative AI can enhance learning outcomes by adapting to individual student needs and offering tailored support. This research introduces an AI-powered assessment and feedback system that goes beyond traditional solutions by incorporating automation and adaptive learning. The non-memoryless feedback mechanism ensures that student history and performance inform future assessments, making the learning process more effective and individualised. This contrasts with conventional systems that provide static, one-time feedback without considering past progress.

</details>


### [31] [CareerPooler: AI-Powered Metaphorical Pool Simulation Improves Experience and Outcomes in Career Exploration](https://arxiv.org/abs/2509.11461)

*Ziyi Wang, Ziwen Zeng, Yuan Li, Zijian Ding*

**Main category:** cs.HC

**Keywords:** Generative AI, Career exploration, Spatial interaction, User engagement, AI-assisted guidance

**Relevance Score:** 8

**TL;DR:** CareerPooler is a generative AI system that simulates career exploration through a spatial-narrative interaction, improving user engagement and career clarity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Career exploration often involves uncertainty and limited information, which traditional AI systems fail to address effectively. This paper aims to create a more engaging and realistic approach to career guidance by leveraging generative AI.

**Method:** The paper introduces CareerPooler, using a pool-table metaphor that allows users to interact with career milestones and decisions through a spatial and narrative framework. A study with 24 participants was conducted to evaluate its effectiveness.

**Key Contributions:**

	1. Introduction of CareerPooler as a novel AI-powered career exploration tool.
	2. Implementation of spatial and narrative interactions to simulate career decision-making.
	3. Demonstration of improved user engagement and satisfaction in a comparative study.

**Result:** CareerPooler demonstrated significantly improved engagement, information gain, satisfaction, and career clarity compared to a standard chatbot interface.

**Limitations:** The sample size was limited to 24 participants, which may affect generalizability. Further studies are needed to validate findings across diverse populations.

**Conclusion:** The study reveals that spatial-narrative interactions enhance experience-based learning and can reduce the psychological burden of career exploration, suggesting new avenues for AI-assisted guidance systems.

**Abstract:** Career exploration is uncertain, requiring decisions with limited information and unpredictable outcomes. While generative AI offers new opportunities for career guidance, most systems rely on linear chat interfaces that produce overly comprehensive and idealized suggestions, overlooking the non-linear and effortful nature of real-world trajectories. We present CareerPooler, a generative AI-powered system that employs a pool-table metaphor to simulate career development as a spatial and narrative interaction. Users strike balls representing milestones, skills, and random events, where hints, collisions, and rebounds embody decision-making under uncertainty. In a within-subjects study with 24 participants, CareerPooler significantly improved engagement, information gain, satisfaction, and career clarity compared to a chatbot baseline. Qualitative findings show that spatial-narrative interaction fosters experience-based learning, resilience through setbacks, and reduced psychological burden. Our findings contribute to the design of AI-assisted career exploration systems and more broadly suggest that visually grounded analogical interactions can make generative systems engaging and satisfying.

</details>


### [32] [Designing and Evaluating a Conversational Agent for Early Detection of Alzheimer's Disease and Related Dementias](https://arxiv.org/abs/2509.11478)

*Andrew G. Breithaupt, Nayoung Choi, James D. Finch, Jeanne M. Powell, Arin L. Nelson, Oz A. Alon, Howard J. Rosen, Jinho D. Choi*

**Main category:** cs.HC

**Keywords:** Alzheimer's disease, conversational agents, large language models, healthcare, user engagement

**Relevance Score:** 9

**TL;DR:** The paper presents a study on the use of voice-interactive conversational agents utilizing large language models for early detection of Alzheimer's disease and related dementias (ADRD) through patient narratives.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the critical need for early detection of ADRD, improving the diagnostic process by leveraging patient narratives instead of traditional screening methods.

**Method:** The research involved developing voice-interactive agents that engage patients in conversation, evaluating their performance through conversation analysis, user surveys, and clinical validation against specialist interviews.

**Key Contributions:**

	1. Development of voice-interactive agents for ADRD assessment
	2. Demonstrated alignment between agent-detected symptoms and specialist findings
	3. User-centered evaluation highlighting patient engagement and narrative expression

**Result:** The evaluation showed that the agent could effectively elicit relevant symptoms, which aligned with specialist diagnoses, and users found the agent helpful and engaging in expressing their experiences.

**Limitations:** The study is preliminary with a limited sample size, and the effectiveness of the agent in diverse patient populations remains to be validated.

**Conclusion:** Conversational agents may effectively support dementia assessment processes, emphasizing the importance of interaction design in sensitive healthcare scenarios.

**Abstract:** Early detection of Alzheimer's disease and related dementias (ADRD) is critical for timely intervention, yet most diagnoses are delayed until advanced stages. While comprehensive patient narratives are essential for accurate diagnosis, prior work has largely focused on screening studies that classify cognitive status from interactions rather than supporting the diagnostic process. We designed voice-interactive conversational agents, leveraging large language models (LLMs), to elicit narratives relevant to ADRD from patients and informants. We evaluated the agent with 30 adults with suspected ADRD through conversation analysis (n=30), user surveys (n=19), and clinical validation against blinded specialist interviews (n=24). Symptoms detected by the agent aligned well with those identified by specialists across symptoms. Users appreciated the agent's patience and systematic questioning, which supported engagement and expression of complex, hard-to-describe experiences. This preliminary work suggests conversational agents may serve as structured front-end tools for dementia assessment, highlighting interaction design considerations in sensitive healthcare contexts.

</details>


### [33] [Collective Recourse for Generative Urban Visualizations](https://arxiv.org/abs/2509.11487)

*Rashid Mushkani*

**Main category:** cs.HC

**Keywords:** text-to-image diffusion models, collective recourse, community feedback, urban planning, machine learning

**Relevance Score:** 4

**TL;DR:** This paper addresses how text-to-image diffusion models can harm group dynamics and proposes collective recourse through community feedback mechanisms to improve model performance and planning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Text-to-image diffusion models have the potential to visualize urban futures but can also exacerbate existing group-level harms. The need for effective recourse mechanisms is crucial for addressing these issues.

**Method:** The authors formalize collective recourse and introduce a structured pipeline that includes reporting, triaging, fixing, verifying, and closure. They identify four recourse primitives within diffusion models and evaluate their effectiveness through a synthetic program of community reports.

**Key Contributions:**

	1. Formalization of collective recourse and a practical pipeline
	2. Identification of four recourse primitives within the diffusion stack
	3. Evaluation of effectiveness using a synthetic program of community reports

**Result:** The study found that prompt-level fixes were the quickest to implement but had a lower durability. Dataset edits and reward model tweaks took longer but were more effective and received better uptake from planners. A mandate score was developed for assessing the severity and effectiveness of the fixes, achieving 93% precision and 75% recall in evaluations.

**Limitations:** Risks of overfitting to vocal groups and reliance on community feedback mechanisms could bias the outcomes.

**Conclusion:** The integration of collective recourse with participatory governance is essential, and safeguards are necessary to mitigate risks such as overfitting to dominant voices in the community.

**Abstract:** Text-to-image diffusion models help visualize urban futures but can amplify group-level harms. We propose collective recourse: structured community "visual bug reports" that trigger fixes to models and planning workflows. We (1) formalize collective recourse and a practical pipeline (report, triage, fix, verify, closure); (2) situate four recourse primitives within the diffusion stack: counter-prompts, negative prompts, dataset edits, and reward-model tweaks; (3) define mandate thresholds via a mandate score combining severity, volume saturation, representativeness, and evidence; and (4) evaluate a synthetic program of 240 reports. Prompt-level fixes were fastest (median 2.1-3.4 days) but less durable (21-38% recurrence); dataset edits and reward tweaks were slower (13.5 and 21.9 days) yet more durable (12-18% recurrence) with higher planner uptake (30-36%). A threshold of 0.12 yielded 93% precision and 75% recall; increasing representativeness raised recall to 81% with little precision loss. We discuss integration with participatory governance, risks (e.g., overfitting to vocal groups), and safeguards (dashboards, rotating juries).

</details>


### [34] [BioMetaphor: AI-Generated Biodata Representations for Virtual Co-Present Events](https://arxiv.org/abs/2509.11600)

*Lin Lin, Ming Wu, Anyu Ren, Zhanwei Wu, Daojun Gong, Ruowei Xiao*

**Main category:** cs.HC

**Keywords:** biodata, generative AI, human-computer interaction, empathic technologies, social cues

**Relevance Score:** 7

**TL;DR:** This study explores the use of biodata as social cues in virtual or hybrid events, proposing a Generative AI framework for creating human-like biodata representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how biodata can be effectively represented and utilized in social contexts, particularly in virtual environments, and to leverage this for generative AI design.

**Method:** Conducted a user elicitation workshop with 30 HCI experts and performed qualitative analysis of the results to understand preferences for biodata expression.

**Key Contributions:**

	1. Introduction of biodata as a paradigm of social cues in virtual events.
	2. Development of the BioMetaphor framework for generative AI biodata representations.
	3. Insights into human cognitive preferences for biodata expression.

**Result:** Developed the BioMetaphor framework demonstrating that generative AI can learn to express biodata cues in a human-like manner suitable for events.

**Limitations:** 

**Conclusion:** The study provides insights into cognitive preferences for biodata and shows how generative AI can enhance the design of empathic technologies by engaging users in the process.

**Abstract:** In virtual or hybrid co-present events, biodata is emerging as a new paradigm of social cues. While it is able to reveal individuals' inner states, the technology-mediated representation of biodata in social contexts remains underexplored. This study aims to uncover human cognitive preferences and patterns for biodata expression and leverage this knowledge to guide generative AI (GenAI) in creating biodata representations for co-present experiences, aligning with the broader concept of Human-in-the-loop. We conducted a user elicitation workshop with 30 HCI experts and investigated the results using qualitative analysis. Based on our findings, we further propose a GenAI-driven framework: BioMetaphor. Our framework demonstration shows that current GenAI can learn and express visual biodata cues in an event-adpated, human-like manner. This human-centered approach engages users in research, revealing the underlying cognition constructions for biodata expression while demonstrating how such knowledge can inform the design and development of future empathic technologies.

</details>


### [35] [Robots that Evolve with Us: Modular Co-Design for Personalization, Adaptability, and Sustainability](https://arxiv.org/abs/2509.11622)

*Lingyun Chen, Qing Xiao, Zitao Zhang, Eli Blevis, Selma Šabanović*

**Main category:** cs.HC

**Keywords:** modular robotics, human-robot interaction, co-design, personalization, sustainability

**Relevance Score:** 4

**TL;DR:** This paper explores a co-design framework for modular robots that emphasizes personalization, adaptability, and sustainability across different life stages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in current robot designs that prioritize efficiency without considering personalization and adaptability, leading to a need for more sustainable human-robot interactions.

**Method:** Two co-design workshops with 23 participants were conducted where they engaged with a modular robot co-design framework, using components to design robots for various life stages.

**Key Contributions:**

	1. Establishment of design principles for lifespan-oriented human-robot interaction
	2. Highlighting the importance of modularity in personalization and adaptability
	3. Proposing a co-design framework that involves users in the design process

**Result:** Participants' designs showcased how modularity supports personalization, adaptability to life stages, and sustainability through features like repair and reuse.

**Limitations:** The study's applicability may be limited to the participant demographics and setting of the workshops, which could affect generalization to broader populations.

**Conclusion:** Modularity in robotic design is framed as a flexible and expressive co-design approach, enabling robots to evolve alongside users throughout their lifespan.

**Abstract:** Many current robot designs prioritize efficiency and one-size-fits-all solutions, oftentimes overlooking personalization, adaptability, and sustainability. To explore alternatives, we conducted two co-design workshops with 23 participants, who engaged with a modular robot co-design framework. Using components we provided as building blocks, participants combined, removed, and invented modules to envision how modular robots could accompany them from childhood through adulthood and into older adulthood. The participants' designs illustrate how modularity (a) enables personalization through open-ended configuration, (b) adaptability across shifting life-stage needs, and (c) sustainability through repair, reuse, and continuity. We therefore derive design principles that establish modularity as a foundation for lifespan-oriented human-robot interaction. This work reframes modular robotics as a flexible and expressive co-design approach, supporting robots that evolve with people, rather than static products optimized for single moments or contexts of use.

</details>


### [36] [Colour Perception in Immersive Virtual Reality: Emotional and Physiological Responses to Fifteen Munsell Hues](https://arxiv.org/abs/2509.11644)

*Francesco Febbraio, Simona Collina, Christina Lepida, Panagiotis Kourtesis*

**Main category:** cs.HC

**Keywords:** immersive virtual reality, color psychology, affective experience, physiological responses, hue effects

**Relevance Score:** 6

**TL;DR:** This study examines the emotional and physiological impacts of fifteen calibrated hues in immersive virtual reality, revealing significant differences in self-reported emotions and pupil responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically characterize the emotional and physiological effects of specific colors within immersive virtual reality environments.

**Method:** Thirty-six participants viewed fifteen calibrated Munsell hues while their pupil diameter and skin conductance were continuously measured; emotions were assessed using the Self-Assessment Manikin.

**Key Contributions:**

	1. First systematic mapping of affective and physiological responses to hues in VR
	2. Demonstrated color shapes emotional experience and ocular physiology
	3. Practical guidance for educational, clinical, and interface design in virtual environments

**Result:** Reds and red-purple hues elicited the highest arousal and dominance, while blue-green hues were rated as most pleasurable. Pupil dilation correlated with arousal ratings.

**Limitations:** Skin conductance did not show reliable differentiation across hues, likely due to brief exposure times.

**Conclusion:** The research provides a systematic hue-by-hue mapping of affective and physiological responses, indicating that color significantly influences emotional experiences and ocular responses in VR, offering practical insights for various applications.

**Abstract:** Colour is a fundamental determinant of affective experience in immersive virtual reality (VR), yet the emotional and physiological impact of individual hues remains poorly characterised. This study investigated how fifteen calibrated Munsell hues influence subjective and autonomic responses when presented in immersive VR. Thirty-six adults (18-45 years) viewed each hue in a within-subject design while pupil diameter and skin conductance were recorded continuously, and self-reported emotions were assessed using the Self-Assessment Manikin across pleasure, arousal, and dominance. Repeated-measures ANOVAs revealed robust hue effects on all three self-report dimensions and on pupil dilation, with medium to large effect sizes. Reds and red-purple hues elicited the highest arousal and dominance, whereas blue-green hues were rated most pleasurable. Pupil dilation closely tracked arousal ratings, while skin conductance showed no reliable hue differentiation, likely due to the brief (30 s) exposures. Individual differences in cognitive style and personality modulated overall reactivity but did not alter the relative ranking of hues. Taken together, these findings provide the first systematic hue-by-hue mapping of affective and physiological responses in immersive VR. They demonstrate that calibrated colour shapes both experience and ocular physiology, while also offering practical guidance for educational, clinical, and interface design in virtual environments.

</details>


### [37] [See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays](https://arxiv.org/abs/2509.11653)

*Gerlinde Emsenhuber, Tobias Langlotz, Denis Kalkofen, Markus Tatzgern*

**Main category:** cs.HC

**Keywords:** Augmented Reality, eye-perspective rendering, optical see-through, eye tracking, HoloLens

**Relevance Score:** 8

**TL;DR:** This paper presents three software-based eye-perspective rendering (EPR) techniques for improving Augmented Reality experiences on optical see-through head-mounted displays, with a focus on real-world usability and accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the misregistration issues between the user's eye perspective and the world-facing camera in optical see-through head-mounted displays, enhancing scene understanding in Augmented Reality environments.

**Method:** The study implements three EPR techniques: Plane-Proxy EPR, Mesh-Proxy EPR, and Gaze-Proxy EPR, the latter utilizing eye tracking for improved alignment with the user's gaze depth. Each technique is evaluated through user studies in realistic task scenarios.

**Key Contributions:**

	1. Introduction of three novel EPR techniques for OST HMDs.
	2. Demonstration of gaze-proxy EPR's efficacy over traditional methods.
	3. Release of an open-source EPR framework for community use.

**Result:** User studies demonstrate that the Gaze-Proxy EPR technique offers a lightweight alternative to more complex geometry-based methods and underscores the impact of accurate eye-perspective rendering on user experience.

**Limitations:** The study is based on a specific type of HMD (Microsoft HoloLens 2) and may not generalize across all OST devices.

**Conclusion:** The findings support the importance of accurate EPR in Augmented Reality applications and highlight the effectiveness of the gaze-proxy method, suggesting potential for broader implementation.

**Abstract:** Image-based scene understanding allows Augmented Reality systems to provide contextual visual guidance in unprepared, real-world environments. While effective on video see-through (VST) head-mounted displays (HMDs), such methods suffer on optical see-through (OST) HMDs due to misregistration between the world-facing camera and the user's eye perspective. To approximate the user's true eye view, we implement and evaluate three software-based eye-perspective rendering (EPR) techniques on a commercially available, untethered OST HMD (Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distance plane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and (3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns the projection with the user's gaze depth. A user study on real-world tasks underscores the importance of accurate EPR and demonstrates gaze-proxy as a lightweight alternative to geometry-based methods. We release our EPR framework as open source.

</details>


### [38] [Collaborative Document Editing with Multiple Users and AI Agents](https://arxiv.org/abs/2509.11826)

*Florian Lehmann, Krystsina Shauchenka, Daniel Buschek*

**Main category:** cs.HC

**Keywords:** AI agents, collaborative writing, human-computer interaction, team dynamics, shared resources

**Relevance Score:** 8

**TL;DR:** The paper proposes integrating AI agents into collaborative writing environments to enhance teamwork by making AI use transparent.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complications of collaboration in AI writing support tools designed for individuals rather than teams.

**Method:** A prototype incorporating AI into writing environments was developed, featuring agent profiles and tasks. A user study with 30 participants was conducted to evaluate interactions and collaborations during writing projects.

**Key Contributions:**

	1. Integration of AI agents into collaborative writing spaces
	2. Development of customizable agent profiles and tasks
	3. Insights into team dynamics and use of AI as a shared resource

**Result:** Teams used AI agents seamlessly within their existing norms, viewing agent profiles as personal resources and outputs as shared tools, highlighting the integration of AI into collaborative authorship.

**Limitations:** The study size was limited to 30 participants, which may not represent all collaborative contexts.

**Conclusion:** The study reveals the importance of treating AI as a collaborative resource, suggesting new norms for teamwork involving AI agents.

**Abstract:** Current AI writing support tools are largely designed for individuals, complicating collaboration when co-writers must leave the shared workspace to use AI and then communicate and reintegrate results. We propose integrating AI agents directly into collaborative writing environments. Our prototype makes AI use transparent and customisable through two new shared objects: agent profiles and tasks. Agent responses appear in the familiar comment feature. In a user study (N=30), 14 teams worked on writing projects during one week. Interaction logs and interviews show that teams incorporated agents into existing norms of authorship, control, and coordination, rather than treating them as team members. Agent profiles were viewed as personal territory, while created agents and outputs became shared resources. We discuss implications for team-based AI interaction, highlighting opportunities and boundaries for treating AI as a shared resource in collaborative work.

</details>


### [39] [The AI Memory Gap: Users Misremember What They Created With AI or Without](https://arxiv.org/abs/2509.11851)

*Tim Zindulka, Sven Goller, Daniela Fernandes, Robin Welsch, Daniel Buschek*

**Main category:** cs.HC

**Keywords:** large language models, source memory, interactive text generation, AI, human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper investigates how accurately people remember the source of content generated with the help of large language models (LLMs).

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are increasingly used in interactive text generation, it is crucial to understand how their integration affects users' memory of content sources.

**Method:** The study involved 184 participants who generated content both unaided and with an LLM-based chatbot. Their ability to remember the source of this content was tested one week later.

**Key Contributions:**

	1. Identified a significant gap in memory attribution when using AI for text generation.
	2. Validated findings with a computational model of source memory.
	3. Emphasized implications for designing AI interaction systems.

**Result:** The results showed a significant decline in correct attribution of content sources after AI use, especially in scenarios where human and AI contributions were mixed.

**Limitations:** The study was limited to a specific experimental setup with a predefined participant group, which may not generalize to all users or contexts.

**Conclusion:** The findings underscore the need to address source confusion in the design and use of interactive text generation technologies.

**Abstract:** As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies.

</details>


### [40] [Lost in Data: How Older Adults Perceive and Navigate Health Data Representations](https://arxiv.org/abs/2509.11876)

*Peterson Jean, Emma Murphy, Enda Bates*

**Main category:** cs.HC

**Keywords:** health data representations, older adults, accessibility, multisensory design, wearable devices

**Relevance Score:** 6

**TL;DR:** The study investigates how older adults interact with health data representations, identifying barriers and proposing design improvements for better accessibility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The ageing population increasingly uses wearable devices for chronic condition monitoring, yet conventional health data representations present accessibility challenges for older adults.

**Method:** An end-user evaluation was conducted with 16 older adults (65+) using think-aloud protocols and participatory design activities in a structured workshop.

**Key Contributions:**

	1. Identification of barriers in health data accessibility for older adults.
	2. Highlighting the role of multimodal cues in enhancing understanding.
	3. A replicable methodological framework for user-centered design in health data representations.

**Result:** Findings reveal key barriers such as semantic inconsistency and difficulties in understanding health data, emphasizing the importance of affordance and familiarity in improving accessibility.

**Limitations:** 

**Conclusion:** The research provides a methodological approach to designing intuitive, multisensory health data representations that meet the needs and abilities of older adults.

**Abstract:** As the ageing population grows, older adults increasingly rely on wearable devices to monitor chronic conditions. However, conventional health data representations (HDRs) often present accessibility challenges, particularly for critical health parameters like blood pressure and sleep data. This study explores how older adults interact with these representations, identifying key barriers such as semantic inconsistency and difficulties in understanding. While research has primarily focused on data collection, less attention has been given to how information is output and understood by end-users. To address this, an end-user evaluation was conducted with 16 older adults (65+) in a structured workshop, using think-aloud protocols and participatory design activities. The findings highlight the importance of affordance and familiarity in improving accessibility, emphasising the familiarity and potential of multimodal cues. This study bridges the gap between domain experts and end-users, providing a replicable methodological approach for designing intuitive, multisensory HDRs that better align with older adults' needs and abilities.

</details>


### [41] [Generative AI in Game Development: A Qualitative Research Synthesis](https://arxiv.org/abs/2509.11898)

*Alexandru Ternar, Alena Denisova, João M. Cunha, Annakaisa Kultima, Christian Guckelsberger*

**Main category:** cs.HC

**Keywords:** Generative AI, game production, meta-ethnography, HCI, qualitative research

**Relevance Score:** 2

**TL;DR:** This paper provides a qualitative synthesis of the impact of Generative Artificial Intelligence on game production, identifying key themes and offering recommendations based on a systematic literature review.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To present a comprehensive view of the influence of Generative AI on game production, addressing gaps in existing studies and contextualizing findings within broader trends.

**Method:** A systematic literature review was conducted following PRISMA-S guidelines, synthesizing 10 eligible studies through meta-ethnography and guided by eMERGe and CASP quality appraisal.

**Key Contributions:**

	1. Qualitative synthesis of 10 studies on Generative AI's impact on game production
	2. Identification of nine overarching themes
	3. Recommendations for integrating Generative AI in game development

**Result:** Nine overarching themes related to the impacts of Generative AI on game production were identified, along with recommendations for future practice and research.

**Limitations:** The study is limited to literature from 2020-2025 and may not cover all aspects of Generative AI's impact across different contexts.

**Conclusion:** The study highlights the transformative potential of Generative AI in game production and provides insights that can inform both practitioners and researchers in the field.

**Abstract:** Generative Artificial Intelligence (GenAI) has had a tremendous impact on game production and promises lasting transformations. In the last five years since GenAI's inception, several studies, typically via qualitative methods, have explored its impact on game production from different settings and demographic angles. However, these studies often contextualise and consolidate their findings weakly with related work, and a big picture view is still missing. Here, we aim to provide such a view of GenAI's impact on game production in the form of a qualitative research synthesis via meta-ethnography. We followed PRISMA-S to systematically search the relevant literature from 2020-2025, including major HCI and games research databases. We then synthesised the 10 eligible studies, conducting reciprocal translation and line-of-argument synthesis guided by eMERGe, informed by CASP quality appraisal. We identified nine overarching themes, provide recommendations, and contextualise our insights in wider game production trends.

</details>


### [42] [PrivWeb: Unobtrusive and Content-aware Privacy Protection For Web Agents](https://arxiv.org/abs/2509.11939)

*Shuning Zhang, Yutong Jiang, Rongjun Ma, Yuting Yang, Mingyao Xu, Zhixin Huang, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** privacy, web agents, localized LLM, data management, user control

**Relevance Score:** 8

**TL;DR:** This paper presents PrivWeb, a web agent add-on that enhances user privacy by utilizing a localized LLM to manage private information according to user preferences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the significant privacy risks associated with web agents, which are often underexamined from the user's perspective.

**Method:** A formative study was conducted with 15 participants to understand user perceptions of data practices, followed by the design and implementation of PrivWeb, a trusted add-on. A user study with 14 participants compared PrivWeb against baseline conditions to evaluate effectiveness.

**Key Contributions:**

	1. Development of PrivWeb, a privacy-focused web agent add-on
	2. Incorporation of a localized LLM for anonymizing private information
	3. Adaptive notifications that enhance user control over information collection

**Result:** PrivWeb effectively reduced perceived privacy risks without increasing cognitive effort and led to higher user satisfaction across various tasks.

**Limitations:** The study involved a small sample size and focused on specific task scenarios.

**Conclusion:** PrivWeb provides unobtrusive and transparent data management for users of web agents, enhancing privacy without compromising user experience.

**Abstract:** While web agents gained popularity by automating web interactions, their requirement for interface access introduces significant privacy risks that are understudied, particularly from users' perspective. Through a formative study (N=15), we found users frequently misunderstand agents' data practices, and desired unobtrusive, transparent data management. To achieve this, we designed and implemented PrivWeb, a trusted add-on on web agents that utilizes a localized LLM to anonymize private information on interfaces according to user preferences. It features privacy categorization schema and adaptive notifications that selectively pauses tasks for user control over information collection for highly sensitive information, while offering non-disruptive options for less sensitive information, minimizing human oversight. The user study (N=14) across travel, information retrieval, shopping, and entertainment tasks compared PrivWeb with baselines without notification and without control for private information access, where PrivWeb reduced perceived privacy risks with no associated increase in cognitive effort, and resulted in higher overall satisfaction.

</details>


### [43] [Teaching the Teachers: Building Generative AI Literacy in Higher Ed Instructors](https://arxiv.org/abs/2509.11999)

*Si Chen, Xiuxiu Tang, Alison Cheng, Nitesh Chawla, G. Alex Ambrose, Ronald Metoyer*

**Main category:** cs.HC

**Keywords:** Generative AI, Higher Education, AI Literacy, Faculty Development, Professional Development

**Relevance Score:** 6

**TL;DR:** The paper discusses the AI Academy, a faculty development program focused on enhancing AI literacy among instructors in higher education.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Research has primarily focused on students in the context of generative AI, neglecting the essential role instructors play in adoption and responsible use.

**Method:** The study involved 25 instructors and utilized pre/post surveys, learning logs, and facilitator interviews to assess the impacts of the program.

**Key Contributions:**

	1. Development of the AI Academy program model
	2. Creation of a co-constructed survey instrument to assess AI literacy
	3. Design insights for professional development in AI use

**Result:** Instructors demonstrated gains in AI literacy and developed new insights into responsible AI practices.

**Limitations:** 

**Conclusion:** The AI Academy serves as a model for professional development that encourages ethical discussion and adapts to changing AI tools.

**Abstract:** Generative AI is reshaping higher education, yet research has focused largely on students, while instructors remain understudied despite their central role in mediating adoption and modeling responsible use. We present the \textit{AI Academy}, a faculty development program that combined AI exploration with pedagogical reflection and peer learning. Rather than a course evaluated for outcomes, the Academy provided a setting to study how instructors build AI literacies in relation to tools, policies, peer practices, and institutional supports. We studied 25 instructors through pre/post surveys, learning logs, and facilitator interviews. Findings show AI literacy gains alongside new insights. We position instructors as designers of responsible AI practices and contribute a replicable program model, a co-constructed survey instrument, and design insights for professional development that adapts to evolving tools and fosters ethical discussion.

</details>


### [44] [Exploring Gaze Dynamics in VR Film Education: Gender, Avatar, and the Shift Between Male and Female Perspectives](https://arxiv.org/abs/2509.12027)

*Zheng Wei, Jia Sun, Junxiang Liao, Lik-Hang Lee, Pan Hui, Huamin Qu, Wai Tong, Xian Xu*

**Main category:** cs.HC

**Keywords:** virtual reality, avatar gender, gaze dynamics, film production, inclusive education

**Relevance Score:** 7

**TL;DR:** This study investigates the influence of avatar gender and narrative style on learning in VR film production, revealing how gender congruence affects engagement and perspective in creativity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how avatar gender and narrative dynamics impact learning in VR, particularly in creative domains like film production.

**Method:** The study employed a 2*2*2 experimental design with 48 participants interacting with avatars of varying genders and narratives to assess gaze dynamics and control.

**Key Contributions:**

	1. Analysis of gaze dynamics related to avatar and narrative gender
	2. Insights into gender perspectives in VR learning environments
	3. Recommendations for inclusive VR educational tool design

**Result:** Results indicate that avatar-gender consistency enhances presence and control; female participants with female avatars displayed a 'female gaze', while male participants with corresponding avatars exhibited a 'male gaze'.

**Limitations:** 

**Conclusion:** The findings provide valuable insights for developing inclusive VR teaching tools that consider gender perspectives in educational settings.

**Abstract:** In virtual reality (VR) education, especially in creative fields like film production, avatar design and narrative style extend beyond appearance and aesthetics. This study explores how the interaction between avatar gender, the dominant narrative actor's gender, and the learner's gender influences film production learning in VR, focusing on gaze dynamics and gender perspectives. Using a 2*2*2 experimental design, 48 participants operated avatars of different genders and interacted with male or female-dominant narratives. The results show that the consistency between the avatar and gender affects presence, and learners' control over the avatar is also influenced by gender matching. Learners using avatars of the opposite gender reported stronger control, suggesting gender incongruity prompted more focus on the avatar. Additionally, female participants with female avatars were more likely to adopt a "female gaze," favoring soft lighting and emotional shots, while male participants with male avatars were more likely to adopt a "male gaze," choosing dynamic shots and high contrast. When male participants used female avatars, they favored "female gaze," while female participants with male avatars focused on "male gaze". These findings advance our understanding of how avatar design and narrative style in VR-based education influence creativity and the cultivation of gender perspectives, and they offer insights for developing more inclusive and diverse VR teaching tools going forward.

</details>


### [45] [Interaction-Driven Browsing: A Human-in-the-Loop Conceptual Framework Informed by Human Web Browsing for Browser-Using Agents](https://arxiv.org/abs/2509.12049)

*Hyeonggeun Yun, Jinkyu Jang*

**Main category:** cs.HC

**Keywords:** Browser-Using Agents, Human-Computer Interaction, Iterative Browsing

**Relevance Score:** 7

**TL;DR:** A conceptual framework for browser-using agents (BUAs) that supports complex, iterative browsing by allowing user feedback and control over browsing actions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of BUAs that terminate after a single instruction, limiting their effectiveness in complex web tasks and dynamic browsing contexts.

**Method:** The framework involves an iterative process where the BUA proposes actions and users provide feedback, categorizing actions into exploration and exploitation.

**Key Contributions:**

	1. Introduces a human-in-the-loop framework for BUAs
	2. Differentiates between exploration and exploitation actions
	3. Enhances user control in complex browsing scenarios

**Result:** The framework aims to enhance user control and reduce cognitive effort while maintaining a traditional browsing mental model and achieving satisfactory outcomes.

**Limitations:** 

**Conclusion:** The proposed framework facilitates a shift from manual to interaction-driven browsing, improving the usability of BUAs.

**Abstract:** Although browser-using agents (BUAs) show promise for web tasks and automation, most BUAs terminate after executing a single instruction, failing to support users' complex, nonlinear browsing with ambiguous goals, iterative decision-making, and changing contexts. We present a human-in-the-loop (HITL) conceptual framework informed by theories of human web browsing behavior. The framework centers on an iterative loop in which the BUA proactively proposes next actions and the user steers the browsing process through feedback. It also distinguishes between exploration and exploitation actions, enabling users to control the breadth and depth of their browsing. Consequently, the framework aims to reduce users' physical and cognitive effort while preserving users' traditional browsing mental model and supporting users in achieving satisfactory outcomes. We illustrate how the framework operates with hypothetical use cases and discuss the shift from manual browsing to interaction-driven browsing. We contribute a theoretically informed conceptual framework for BUAs.

</details>


### [46] [Can LLMs Address Mental Health Questions? A Comparison with Human Therapists](https://arxiv.org/abs/2509.12102)

*Synthia Wang, Yuwei Cheng, Austin Song, Sarah Keedy, Marc Berman, Nick Feamster*

**Main category:** cs.HC

**Keywords:** Mental Health, Large Language Models, User Preference, Therapeutic Responses, Digital Tools

**Relevance Score:** 9

**TL;DR:** A study comparing therapist-written responses to those generated by LLMs (ChatGPT, Gemini, Llama) in mental health care finds LLMs produce clearer and more positive responses, but users prefer human therapists.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limited access to mental health care through the use of digital tools and conversational agents powered by LLMs.

**Method:** Comparison of therapist-written responses to those generated by LLMs in response to real patient questions, followed by a user survey assessing clarity and supportiveness.

**Key Contributions:**

	1. Comparative analysis of LLMs vs. therapist responses in mental health context.
	2. User preferences highlight the importance of human therapists despite LLM effectiveness.
	3. Findings outline the strengths and limitations of using LLMs in therapeutic settings.

**Result:** LLMs produced longer, more readable responses with a positive tone; however, participants preferred human therapist support.

**Limitations:** Study based on a specific dataset of patient questions and responses; results may not generalize to all mental health scenarios.

**Conclusion:** While LLMs show promise in mental health care, concerns regarding trust, privacy, and accountability remain important.

**Abstract:** Limited access to mental health care has motivated the use of digital tools and conversational agents powered by large language models (LLMs), yet their quality and reception remain unclear. We present a study comparing therapist-written responses to those generated by ChatGPT, Gemini, and Llama for real patient questions. Text analysis showed that LLMs produced longer, more readable, and lexically richer responses with a more positive tone, while therapist responses were more often written in the first person. In a survey with 150 users and 23 licensed therapists, participants rated LLM responses as clearer, more respectful, and more supportive than therapist-written answers. Yet, both groups of participants expressed a stronger preference for human therapist support. These findings highlight the promise and limitations of LLMs in mental health, underscoring the need for designs that balance their communicative strengths with concerns of trust, privacy, and accountability.

</details>


### [47] [Exploring Conversational Design Choices in LLMs for Pedagogical Purposes: Socratic and Narrative Approaches for Improving Instructor's Teaching Practice](https://arxiv.org/abs/2509.12107)

*Si Chen, Isabel R. Molnar, Peiyu Li, Adam Acunin, Ting Hua, Alex Ambrose, Nitesh V. Chawla, Ronald Metoyer*

**Main category:** cs.HC

**Keywords:** Large Language Models, Pedagogy, Instructor Development, Conversational Approaches, AI in Education

**Relevance Score:** 8

**TL;DR:** Study evaluates TeaPT, an LLM designed for pedagogical use, focusing on two conversational approaches to support instructors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how instructors use LLMs in education and to support their professional development through adaptive conversational tools.

**Method:** A mixed-method study with 41 higher-education instructors evaluating two versions of TeaPT: Socratic and Narrative.

**Key Contributions:**

	1. Introduction of TeaPT as a pedagogical LLM
	2. Comparison of Socratic and Narrative approaches
	3. Insights on instructor engagement based on experience and AI attitudes

**Result:** The Socratic version led to greater engagement, while the Narrative version was preferred for actionable guidance, with preferences varying by instructor experience and AI attitude.

**Limitations:** Study limited to 41 instructors; results may not generalize across all educational contexts.

**Conclusion:** Adaptive conversational approaches can enhance LLM design for varied instructor profiles and highlight the influence of AI attitudes on learning.

**Abstract:** Large language models (LLMs) typically generate direct answers, yet they are increasingly used as learning tools. Studying instructors' usage is critical, given their role in teaching and guiding AI adoption in education. We designed and evaluated TeaPT, an LLM for pedagogical purposes that supports instructors' professional development through two conversational approaches: a Socratic approach that uses guided questioning to foster reflection, and a Narrative approach that offers elaborated suggestions to extend externalized cognition. In a mixed-method study with 41 higher-education instructors, the Socratic version elicited greater engagement, while the Narrative version was preferred for actionable guidance. Subgroup analyses further revealed that less-experienced, AI-optimistic instructors favored the Socratic version, whereas more-experienced, AI-cautious instructors preferred the Narrative version. We contribute design implications for LLMs for pedagogical purposes, showing how adaptive conversational approaches can support instructors with varied profiles while highlighting how AI attitudes and experience shape interaction and learning.

</details>


### [48] [Worker Discretion Advised: Co-designing Risk Disclosure in Crowdsourced Responsible AI (RAI) Content Work](https://arxiv.org/abs/2509.12140)

*Alice Qian, Ziqi Yang, Ryland Shaw, Jina Suh, Laura Dabbish, Hong Shen*

**Main category:** cs.HC

**Keywords:** Responsible AI, risk disclosure, crowd work, task design, HCI

**Relevance Score:** 7

**TL;DR:** This paper explores the design of risk disclosure mechanisms for crowd workers in Responsible AI content work, balancing worker protection and task designer needs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The work examines the exposure of crowd workers to harmful content in Responsible AI tasks and the lack of effective disclosure mechanisms.

**Method:** Co-design sessions with 29 task designers, workers, and platform representatives to gather preferences and insights on risk disclosure practices.

**Key Contributions:**

	1. Design recommendations for risk disclosure mechanisms
	2. Insights on worker and designer preferences for disclosures
	3. Identification of sociotechnical trade-offs in disclosure practices

**Result:** Identified design tensions and sociotechnical trade-offs affecting risk disclosure, along with recommendations for mechanisms to enhance worker protection.

**Limitations:** 

**Conclusion:** The paper provides actionable design recommendations to improve risk disclosure practices in the context of Responsible AI content work.

**Abstract:** Responsible AI (RAI) content work, such as annotation, moderation, or red teaming for AI safety, often exposes crowd workers to potentially harmful content. While prior work has underscored the importance of communicating well-being risk to employed content moderators, designing effective disclosure mechanisms for crowd workers while balancing worker protection with the needs of task designers and platforms remains largely unexamined. To address this gap, we conducted co-design sessions with 29 task designers, workers, and platform representatives. We investigated task designer preferences for support in disclosing tasks, worker preferences for receiving risk disclosure warnings, and how platform stakeholders envision their role in shaping risk disclosure practices. We identify design tensions and map the sociotechnical tradeoffs that shape disclosure practices. We contribute design recommendations and feature concepts for risk disclosure mechanisms in the context of RAI content work.

</details>


### [49] [Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference](https://arxiv.org/abs/2509.12152)

*Synthia Wang, Sai Teja Peddinti, Nina Taft, Nick Feamster*

**Main category:** cs.HC

**Keywords:** Large Language Models, privacy risks, user behavior, inference awareness, text sanitization

**Relevance Score:** 9

**TL;DR:** This study investigates how users estimate and respond to privacy risks from personal attributes inferred by Large Language Models through text.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns over privacy risks associated with LLMs inferring personal attributes from text, and the need to understand user responses to these risks.

**Method:** A survey with 240 U.S. participants who evaluated text snippets for inference risks, reported their concern levels, and attempted rewrites to mitigate these risks. Their rewrites were compared to those generated by ChatGPT and the Rescriber sanitization tool.

**Key Contributions:**

	1. Identified user misconceptions regarding inference risks from LLMs.
	2. Showed comparative effectiveness of user rewrites versus AI-generated rewrites.
	3. Highlighted the importance of specific rewriting strategies to enhance privacy.

**Result:** Participants were able to recognize inference risks slightly better than chance, with their rewrites effective in only 28% of cases, outperforming Rescriber but not ChatGPT. Common rewriting strategies included paraphrasing, which was the least effective, while abstraction and adding ambiguity yielded better results.

**Limitations:** The study only included U.S. participants, which may impact the generalizability of the results.

**Conclusion:** The study underscores the need for inference-aware design in interactions with LLMs to enhance user privacy.

**Abstract:** Large Language Models (LLMs) such as ChatGPT can infer personal attributes from seemingly innocuous text, raising privacy risks beyond memorized data leakage. While prior work has demonstrated these risks, little is known about how users estimate and respond. We conducted a survey with 240 U.S. participants who judged text snippets for inference risks, reported concern levels, and attempted rewrites to block inference. We compared their rewrites with those generated by ChatGPT and Rescriber, a state-of-the-art sanitization tool. Results show that participants struggled to anticipate inference, performing a little better than chance. User rewrites were effective in just 28\% of cases - better than Rescriber but worse than ChatGPT. We examined our participants' rewriting strategies, and observed that while paraphrasing was the most common strategy it is also the least effective; instead abstraction and adding ambiguity were more successful. Our work highlights the importance of inference-aware design in LLM interactions.

</details>


### [50] [You Are Not Alone: Designing Body Doubling for ADHD in Virtual Reality](https://arxiv.org/abs/2509.12153)

*Zinat Ara, Imtiaz Bin Rahim, Puqi Zhou, Liuchuan Yu, Behzad Esmaeili, Lap-Fai Yu, Sungsoo Ray Hong*

**Main category:** cs.HC

**Keywords:** ADHD, body doubling, neurodiversity, virtual reality, productivity

**Relevance Score:** 8

**TL;DR:** This study explores the effectiveness of body doubling as a productivity strategy for adults with ADHD in construction tasks, comparing human and AI body doubles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Adults with ADHD face challenges in sustaining attention in the workplace, and body doubling has been suggested as a potential aid, though previous studies yielded inconclusive results.

**Method:** The research included two studies; the first identified challenges faced by ADHD workers in construction, while the second implemented a virtual reality bricklaying task under three conditions: alone, with a human body double, and with an AI body double.

**Key Contributions:**

	1. Demonstrated the effectiveness of body doubling in improving productivity for ADHD workers.
	2. Provided comparative insights into the preferences for human vs. AI body doubling in a task setting.
	3. Identified specific design insights for interventions targeting ADHD and neurodivergent populations.

**Result:** Participants completed tasks faster and reported greater accuracy and sustained attention in the body doubling conditions (human and AI) compared to working alone.

**Limitations:** The study included a small sample size (12 participants) which may limit generalizability.

**Conclusion:** Body doubling positively affects productivity for individuals with ADHD, with preferences varying between the human and AI body doubles, highlighting important design implications for future interventions.

**Abstract:** Adults with Attention Deficit Hyperactivity Disorder (ADHD) experience challenges sustaining attention in the workplace. Body doubling, the concept of working alongside another person, has been proposed as a productivity aid for ADHD and other neurodivergent populations (NDs). However, prior work found no conclusive effectiveness and noted NDs' discomfort with social presence. This work investigates body doubling as an ADHD centered productivity strategy in construction tasks. In Study 1, we explored challenges ADHD workers face in construction and identified design insights. In Study 2, we implemented a virtual reality bricklaying task under three conditions: (C1) alone, (C2) with a human body double, and (C3) with an AI body double. Results from 12 participants show they finished tasks faster and perceived greater accuracy and sustained attention in C2 and C3 compared to C1. While body doubling was clearly preferred, opinions diverged between conditions. Our findings verify its effect and offer design implications for future interventions.

</details>


### [51] [Impact Ambivalence: How People with Eating Disorders Get Trapped in the Perpetual Cycle of Digital Food Content Engagement](https://arxiv.org/abs/2311.05920)

*Ryuhaerang Choi, Subin Park, Sujin Han, Jennifer G. Kim, Sung-Ju Lee*

**Main category:** cs.HC

**Keywords:** digital food content, eating disorders, health interventions, motivation, user engagement

**Relevance Score:** 5

**TL;DR:** The paper explores how individuals with eating disorders interact with digital food content, revealing motivations tied to both recovery and disorder exacerbation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of understanding regarding how individuals with eating disorders consume digital food content and its impact on their health.

**Method:** Exploratory studies (N=23) and in-depth studies (N=22) with individuals with eating disorders were conducted to analyze their motivations and practices regarding digital food content.

**Key Contributions:**

	1. Insight into the conflicting motivations of individuals with eating disorders when interacting with digital food content.
	2. Identification of a cycle of quitting and returning to content based on perceived benefits and harms.
	3. Recommendations for designing interventions that support healthier engagement with digital food content.

**Result:** Participants engaged with digital food content for both disorder-driven and recovery-supporting motivations, leading to conflicting outcomes characterized by cycles of quitting and returning to the content.

**Limitations:** The study is limited by its small sample size and the exploratory nature of research, which may not fully capture broader user interactions.

**Conclusion:** Recognizing the ambivalence in individuals' motivations can guide the design of interventions aimed at promoting healthier engagement with digital food content.

**Abstract:** Digital food content could impact viewers' dietary health, with individuals with eating disorders being particularly sensitive to it. However, a comprehensive understanding of why and how these individuals interact with such content is lacking. To fill this void, we conducted exploratory (N=23) and in-depth studies (N=22) with individuals with eating disorders to understand their motivations and practices of consuming digital food content. We reveal that participants engaged with digital food content for both disorder-driven and recovery-supporting motivations, leading to conflicting outcomes. This impact ambivalence, the coexistence of recovery-supporting benefits and disorder-exacerbating risks, sustained a cycle of quitting, prompted by awareness of harm, and returning, motivated by anticipated benefits. We interpret these dynamics within dual systems theory and highlight how recognizing such ambivalence can inform the design of interventions that foster healthier digital food content engagement and mitigate post-engagement harmful effects.

</details>


### [52] [Controllable GUI Exploration](https://arxiv.org/abs/2502.03330)

*Aryan Garg, Yue Jiang, Antti Oulasvirta*

**Main category:** cs.HC

**Keywords:** interface design, generative AI, diffusion model, design exploration, creative tools

**Relevance Score:** 8

**TL;DR:** This paper introduces a diffusion-based approach for low-effort generation of interface sketches, allowing flexible input via prompts, wireframes, and visual flows, resulting in diverse low-fidelity design solutions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Designers often struggle with existing tools that require overly detailed inputs during the early stages of interface design, hindering exploration of design spaces.

**Method:** A diffusion-based generation model that accepts flexible input formats (prompts, wireframes, visual flows) and produces a variety of low-fidelity interface sketches.

**Key Contributions:**

	1. Introduces a diffusion-based approach for sketch generation
	2. Supports flexible input combinations for design exploration
	3. Demonstrates improved alignment with designer specifications over existing models.

**Result:** The model generates a diverse gallery of sketches based on various input combinations, showing better alignment with specifications compared to existing models.

**Limitations:** 

**Conclusion:** The proposed method enables rapid exploration of large design spaces with minimal input effort, making it a valuable tool for early-stage interface design.

**Abstract:** During the early stages of interface design, designers need to produce multiple sketches to explore a design space. Design tools often fail to support this critical stage, because they insist on specifying more details than necessary. Although recent advances in generative AI have raised hopes of solving this issue, in practice they fail because expressing loose ideas in a prompt is impractical. In this paper, we propose a diffusion-based approach to the low-effort generation of interface sketches. It breaks new ground by allowing flexible control of the generation process via three types of inputs: A) prompts, B) wireframes, and C) visual flows. The designer can provide any combination of these as input at any level of detail, and will get a diverse gallery of low-fidelity solutions in response. The unique benefit is that large design spaces can be explored rapidly with very little effort in input-specification. We present qualitative results for various combinations of input specifications. Additionally, we demonstrate that our model aligns more accurately with these specifications than other models.

</details>


### [53] [A Human-Centered Approach to Identifying Promises, Risks, & Challenges of Text-to-Image Generative AI in Radiology](https://arxiv.org/abs/2507.16207)

*Katelyn Morrison, Arpit Mathur, Aidan Bradshaw, Tom Wartmann, Steven Lundi, Afrooz Zandifar, Weichang Dai, Kayhan Batmanghelich, Motahhare Eslami, Adam Perer*

**Main category:** cs.HC

**Keywords:** text-to-image generation, medical imagery, human-centered AI, radiology, stakeholder involvement

**Relevance Score:** 9

**TL;DR:** Exploration of stakeholders' perspectives on text-to-image generative AI for CT scans in medical education and practice, emphasizing a human-centered approach to model development.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how text-to-image generative AI can be beneficial for medical professionals and address the risks of developing such models without stakeholder involvement.

**Method:** A human-centered approach involving medical students, radiology trainees, and radiologists in evaluating a text-to-CT Scan GenAI model through exploratory prompting activities.

**Key Contributions:**

	1. Human-centered evaluation of medical GenAI models
	2. Identification of technical challenges in generating synthetic medical images
	3. Insights into medical professionals' perspectives on GenAI use

**Result:** Identified perspectives on the role of text-to-image GenAI in medical education and training, revealing technical challenges and domain-specific risks.

**Limitations:** Limited to a specific domain of CT scans and perspectives from a chosen group of stakeholders.

**Conclusion:** Involving stakeholders in the development of GenAI models is crucial to ensure their usefulness and mitigate risks in medical applications.

**Abstract:** As text-to-image generative models rapidly improve, AI researchers are making significant advances in developing domain-specific models capable of generating complex medical imagery from text prompts. Despite this, these technical advancements have overlooked whether and how medical professionals would benefit from and use text-to-image generative AI (GenAI) in practice. By developing domain-specific GenAI without involving stakeholders, we risk the potential of building models that are either not useful or even more harmful than helpful. In this paper, we adopt a human-centered approach to responsible model development by involving stakeholders in evaluating and reflecting on the promises, risks, and challenges of a novel text-to-CT Scan GenAI model. Through exploratory model prompting activities, we uncover the perspectives of medical students, radiology trainees, and radiologists on the role that text-to-CT Scan GenAI can play across medical education, training, and practice. This human-centered approach additionally enabled us to surface technical challenges and domain-specific risks of generating synthetic medical images. We conclude by reflecting on the implications of medical text-to-image GenAI.

</details>


### [54] [Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions](https://arxiv.org/abs/2509.11206)

*Tae Soo Kim, Heechan Lee, Yoonjoo Lee, Joseph Seering, Juho Kim*

**Main category:** cs.HC

**Keywords:** Large Language Models, Evaluation, Human-Computer Interaction, Generative AI, Qualitative Analysis

**Relevance Score:** 8

**TL;DR:** The paper proposes functional fragmentation to improve evaluation of generative AI outputs by dissecting them into fragments to understand their influence on evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of holistic scoring in LLM evaluations, which obscure the specific elements affecting assessments and trust.

**Method:** The method introduces functional fragmentation to break outputs into key fragments and analyze their rhetorical functions in relation to evaluation criteria.

**Key Contributions:**

	1. Introduction of functional fragmentation for evaluating LLM outputs
	2. Development of Evalet, an interactive visualization system
	3. Empirical evidence showing improved identification of evaluation misalignments

**Result:** In a user study, the approach enabled practitioners to identify 48% more evaluation misalignments compared to traditional holistic scoring.

**Limitations:** 

**Conclusion:** The method enhances trust in LLM evaluations by allowing for a deeper qualitative analysis of model outputs, moving beyond mere quantitative scores.

**Abstract:** Practitioners increasingly rely on Large Language Models (LLMs) to evaluate generative AI outputs through "LLM-as-a-Judge" approaches. However, these methods produce holistic scores that obscure which specific elements influenced the assessments. We propose functional fragmentation, a method that dissects each output into key fragments and interprets the rhetoric functions that each fragment serves relative to evaluation criteria -- surfacing the elements of interest and revealing how they fulfill or hinder user goals. We instantiate this approach in Evalet, an interactive system that visualizes fragment-level functions across many outputs to support inspection, rating, and comparison of evaluations. A user study (N=10) found that, while practitioners struggled to validate holistic scores, our approach helped them identify 48% more evaluation misalignments. This helped them calibrate trust in LLM evaluations and rely on them to find more actionable issues in model outputs. Our work shifts LLM evaluation from quantitative scores toward qualitative, fine-grained analysis of model behavior.

</details>


### [55] [Collaborative Document Editing with Multiple Users and AI Agents](https://arxiv.org/abs/2509.11826)

*Florian Lehmann, Krystsina Shauchenka, Daniel Buschek*

**Main category:** cs.HC

**Keywords:** AI agents, collaborative writing, team interaction, authorship norms, shared resources

**Relevance Score:** 8

**TL;DR:** This paper proposes the integration of AI agents into collaborative writing environments to enhance teamwork and facilitate authorship norms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of collaboration in writing projects that arise when traditional AI tools are utilized separately by individuals.

**Method:** A prototype was developed to incorporate AI agents directly into collaborative writing tools, featuring shared objects like agent profiles and tasks. A user study with 30 participants was conducted to observe their interactions.

**Key Contributions:**

	1. Integration of AI agents into collaborative writing environments
	2. Development of agent profiles and task management features
	3. Empirical analysis of team interactions with AI in writing projects

**Result:** Teams integrated AI agents into their existing collaboration norms, viewing agent profiles as personal while sharing outputs as communal resources.

**Limitations:** The study's sample size is limited to 30 participants, which may affect the generalizability of the findings.

**Conclusion:** The study highlights the potential of treating AI as a shared resource in collaborative work, impacting how teams interact with AI technologies.

**Abstract:** Current AI writing support tools are largely designed for individuals, complicating collaboration when co-writers must leave the shared workspace to use AI and then communicate and reintegrate results. We propose integrating AI agents directly into collaborative writing environments. Our prototype makes AI use transparent and customisable through two new shared objects: agent profiles and tasks. Agent responses appear in the familiar comment feature. In a user study (N=30), 14 teams worked on writing projects during one week. Interaction logs and interviews show that teams incorporated agents into existing norms of authorship, control, and coordination, rather than treating them as team members. Agent profiles were viewed as personal territory, while created agents and outputs became shared resources. We discuss implications for team-based AI interaction, highlighting opportunities and boundaries for treating AI as a shared resource in collaborative work.

</details>


### [56] [The AI Memory Gap: Users Misremember What They Created With AI or Without](https://arxiv.org/abs/2509.11851)

*Tim Zindulka, Sven Goller, Daniela Fernandes, Robin Welsch, Daniel Buschek*

**Main category:** cs.HC

**Keywords:** AI attribution, source memory, interactive text generation, LLM, HCI

**Relevance Score:** 9

**TL;DR:** This paper explores how accurately people remember whether ideas or texts were generated by themselves or an AI when using LLM-based chatbots, revealing significant memory gaps after AI use.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the accuracy of source attribution of ideas generated with the help of AI and its implications for interactive text generation technologies.

**Method:** A pre-registered experiment with 184 participants, who generated and elaborated on ideas both unaided and using an LLM-based chatbot, followed by source identification one week later.

**Key Contributions:**

	1. Identified significant memory gaps in source attribution when using AI-generated content.
	2. Validated results with a computational model of source memory.
	3. Provided insights into the implications of these gaps for the design of interactive technologies.

**Result:** Participants showed a significant drop in correctly attributing sources after using AI, particularly in workflows involving both human and AI contributions.

**Limitations:** Focused on a relatively small sample size and a specific type of AI interaction (LLM-based chatbot).

**Conclusion:** The study highlights the critical need to consider source attribution confusion when designing and using interactive text generation technologies.

**Abstract:** As large language models (LLMs) become embedded in interactive text generation, disclosure of AI as a source depends on people remembering which ideas or texts came from themselves and which were created with AI. We investigate how accurately people remember the source of content when using AI. In a pre-registered experiment, 184 participants generated and elaborated on ideas both unaided and with an LLM-based chatbot. One week later, they were asked to identify the source (noAI vs withAI) of these ideas and texts. Our findings reveal a significant gap in memory: After AI use, the odds of correct attribution dropped, with the steepest decline in mixed human-AI workflows, where either the idea or elaboration was created with AI. We validated our results using a computational model of source memory. Discussing broader implications, we highlight the importance of considering source confusion in the design and use of interactive text generation technologies.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [57] [Uncovering the Vulnerability of Large Language Models in the Financial Domain via Risk Concealment](https://arxiv.org/abs/2509.10546)

*Gang Cheng, Haibo Jin, Wenbin Zhang, Haohan Wang, Jun Zhuang*

**Main category:** cs.CL

**Keywords:** Large Language Models, financial applications, Risk-Concealment Attacks, LLM alignment, FIN-Bench

**Relevance Score:** 4

**TL;DR:** We propose a multi-turn red-teaming framework, RCA, that reveals critical regulatory vulnerabilities in financial LLMs, achieving over 93% attack success on a proposed new benchmark, FIN-Bench.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing red-teaming research on Large Language Models (LLMs) focuses mainly on harmful content, overlooking regulatory risks in financial applications.

**Method:** We introduce Risk-Concealment Attacks (RCA), a multi-turn framework that conceals regulatory risks to elicit responses from LLMs that seem compliant but violate regulations. We also constructed FIN-Bench, a benchmark for evaluating LLM safety in financial contexts.

**Key Contributions:**

	1. Introduction of Risk-Concealment Attacks (RCA) for probing regulatory vulnerabilities in LLMs
	2. Development of FIN-Bench, a domain-specific benchmark for assessing LLM safety in finance
	3. Demonstration of high attack success rates against mainstream LLMs, revealing critical alignment gaps

**Result:** Extensive experiments on FIN-Bench show that RCA can effectively bypass nine mainstream LLMs, achieving an average attack success rate of 93.18%, including 98.28% on GPT-4 and 97.56% on OpenAI's model.

**Limitations:** 

**Conclusion:** Our findings highlight significant gaps in existing LLM alignment techniques and indicate an urgent need for better moderation mechanisms in financial contexts, offering practical insights for LLM alignment.

**Abstract:** Large Language Models (LLMs) are increasingly integrated into financial applications, yet existing red-teaming research primarily targets harmful content, largely neglecting regulatory risks. In this work, we aim to investigate the vulnerability of financial LLMs through red-teaming approaches. We introduce Risk-Concealment Attacks (RCA), a novel multi-turn framework that iteratively conceals regulatory risks to provoke seemingly compliant yet regulatory-violating responses from LLMs. To enable systematic evaluation, we construct FIN-Bench, a domain-specific benchmark for assessing LLM safety in financial contexts. Extensive experiments on FIN-Bench demonstrate that RCA effectively bypasses nine mainstream LLMs, achieving an average attack success rate (ASR) of 93.18%, including 98.28% on GPT-4.1 and 97.56% on OpenAI o1. These findings reveal a critical gap in current alignment techniques and underscore the urgent need for stronger moderation mechanisms in financial domains. We hope this work offers practical insights for advancing robust and domain-aware LLM alignment.

</details>


### [58] [No Answer Needed: Predicting LLM Answer Accuracy from Question-Only Linear Probes](https://arxiv.org/abs/2509.10625)

*Iván Vicente Moreno Cencerrado, Arnau Padrés Masdemont, Anton Gonzalvez Hawthorne, David Demitri Africa, Lorenzo Pacchiardi*

**Main category:** cs.CL

**Keywords:** large language models, correctness prediction, self-assessment, activations, confidence

**Relevance Score:** 9

**TL;DR:** This paper explores whether large language models can anticipate the correctness of their answers by analyzing activations before token generation, using linear probes to predict answer accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand if and how large language models self-assess the correctness of their forthcoming answers based on internal activations.

**Method:** The researchers extracted activations after the model reads a question but before generating tokens, using linear probes trained on trivia questions to predict the correctness of answers across different model families.

**Key Contributions:**

	1. Introduction of the 'in-advance correctness direction' for LLMs.
	2. Demonstration of predictive power saturation in intermediate layers of models.
	3. Correlation between model responses of 'I don't know' and probe scores, indicating a link to confidence.

**Result:** The model's projections on the correctness of answers effectively predict success on both in-distribution and out-of-distribution datasets, outperforming traditional methods and showing saturation of predictive power in intermediate layers.

**Limitations:** Generalization falters with questions requiring mathematical reasoning, which highlights a limitation in predictive capabilities.

**Conclusion:** The findings suggest that self-assessment of correctness occurs mid-computation in LLMs, providing insights into model behavior and confidence assessment, though generalization issues arise with mathematical reasoning questions.

**Abstract:** Do large language models (LLMs) anticipate when they will answer correctly? To study this, we extract activations after a question is read but before any tokens are generated, and train linear probes to predict whether the model's forthcoming answer will be correct. Across three open-source model families ranging from 7 to 70 billion parameters, projections on this "in-advance correctness direction" trained on generic trivia questions predict success in distribution and on diverse out-of-distribution knowledge datasets, outperforming black-box baselines and verbalised predicted confidence. Predictive power saturates in intermediate layers, suggesting that self-assessment emerges mid-computation. Notably, generalisation falters on questions requiring mathematical reasoning. Moreover, for models responding "I don't know", doing so strongly correlates with the probe score, indicating that the same direction also captures confidence. By complementing previous results on truthfulness and other behaviours obtained with probes and sparse auto-encoders, our work contributes essential findings to elucidate LLM internals.

</details>


### [59] [Interdisciplinary Research in Conversation: A Case Study in Computational Morphology for Language Documentation](https://arxiv.org/abs/2509.10644)

*Enora Rice, Katharina von der Wense, Alexis Palmer*

**Main category:** cs.CL

**Keywords:** computational morphology, language documentation, User-Centered Design, multilingual IGT, usability

**Relevance Score:** 5

**TL;DR:** This paper discusses the disconnect between computational morphology research and practical language documentation, highlighting the need for User-Centered Design principles through a case study of the GlossLM model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited use of computational morphology in language documentation and the disconnection between NLP research and practical applications.

**Method:** The paper presents a case study of GlossLM, a multilingual IGT generation model, assessed through a small-scale user study with documentary linguists to evaluate usability.

**Key Contributions:**

	1. Demonstration of UCD principles in computational morphology research
	2. Identification of usability issues in existing IGT generation models
	3. Proposition of new research questions focused on model constraints and user needs

**Result:** Despite strong performance metrics, GlossLM fails to meet key usability requirements in real documentation environments, indicating a misalignment between system design and user needs.

**Limitations:** The study is based on a small user sample, limiting generalizability of findings.

**Conclusion:** Integrating User-Centered Design in the development of NLP tools can enhance their effectiveness and lead to more relevant research inquiries.

**Abstract:** Computational morphology has the potential to support language documentation through tasks like morphological segmentation and the generation of Interlinear Glossed Text (IGT). However, our research outputs have seen limited use in real-world language documentation settings. This position paper situates the disconnect between computational morphology and language documentation within a broader misalignment between research and practice in NLP and argues that the field risks becoming decontextualized and ineffectual without systematic integration of User-Centered Design (UCD). To demonstrate how principles from UCD can reshape the research agenda, we present a case study of GlossLM, a state-of-the-art multilingual IGT generation model. Through a small-scale user study with three documentary linguists, we find that despite strong metric based performance, the system fails to meet core usability needs in real documentation contexts. These insights raise new research questions around model constraints, label standardization, segmentation, and personalization. We argue that centering users not only produces more effective tools, but surfaces richer, more relevant research directions

</details>


### [60] [Context Copying Modulation: The Role of Entropy Neurons in Managing Parametric and Contextual Knowledge Conflicts](https://arxiv.org/abs/2509.10663)

*Zineddine Tighidet, Andrea Mogini, Hedi Ben-younes, Jiali Mei, Patrick Gallinari, Benjamin Piwowarski*

**Main category:** cs.CL

**Keywords:** Large Language Models, entropy neurons, context copying, autoregressive transformers, conflicting information

**Relevance Score:** 9

**TL;DR:** This paper investigates the role of entropy neurons in Large Language Models (LLMs) and their impact on context copying behavior when faced with conflicting information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how LLMs manage conflicts between contextual and parametric information is crucial for improving their performance and reliability.

**Method:** The authors analyze entropy neurons in autoregressive transformer models to determine their effect on context copying and model output entropy.

**Key Contributions:**

	1. Demonstration of the role of entropy neurons in context management within LLMs.
	2. Evidence that ablation of these neurons leads to significant changes in output behavior.
	3. Insights into internal model dynamics regarding conflicting information.

**Result:** The study finds that entropy neurons suppress context copying across various LLMs; ablating these neurons significantly alters the generation process.

**Limitations:** 

**Conclusion:** The findings provide deeper insights into the dynamics of LLMs when encountering conflicting information, highlighting the importance of entropy neurons in resolving those conflicts.

**Abstract:** The behavior of Large Language Models (LLMs) when facing contextual information that conflicts with their internal parametric knowledge is inconsistent, with no generally accepted explanation for the expected outcome distribution. Recent work has identified in autoregressive transformer models a class of neurons -- called entropy neurons -- that produce a significant effect on the model output entropy while having an overall moderate impact on the ranking of the predicted tokens. In this paper, we investigate the preliminary claim that these neurons are involved in inhibiting context copying behavior in transformers by looking at their role in resolving conflicts between contextual and parametric information. We show that entropy neurons are responsible for suppressing context copying across a range of LLMs, and that ablating them leads to a significant change in the generation process. These results enhance our understanding of the internal dynamics of LLMs when handling conflicting information.

</details>


### [61] [Pluralistic Alignment for Healthcare: A Role-Driven Framework](https://arxiv.org/abs/2509.10685)

*Jiayou Zhong, Anudeex Shetty, Chao Jia, Xuanrui Lin, Usman Naseem*

**Main category:** cs.CL

**Keywords:** pluralistic alignment, healthcare, large language models, diversity, EthosAgents

**Relevance Score:** 9

**TL;DR:** This paper presents EthosAgents, a novel pluralistic alignment approach for large language models in healthcare, addressing the challenge of ensuring diverse values and perspectives are reflected in model outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for alignment approaches that respect diverse values in healthcare, where personal, cultural, and situational factors influence pluralism, is critical.

**Method:** The paper proposes a lightweight and generalizable approach called EthosAgents, which simulates diverse perspectives and is empirically tested across various model sizes.

**Key Contributions:**

	1. Introduction of EthosAgents for pluralistic alignment in healthcare.
	2. Empirical validation across multiple model sizes.
	3. Insights into diversity in AI outputs for sensitive domains.

**Result:** EthosAgents improves pluralistic alignment across three modes in seven different models, demonstrating the importance of adaptable and normatively aware approaches in health-related pluralism.

**Limitations:** The approach may require further testing across more diverse contexts beyond healthcare.

**Conclusion:** The findings indicate that effective pluralistic alignment in healthcare may contribute insights applicable to other high-stakes domains.

**Abstract:** As large language models are increasingly deployed in sensitive domains such as healthcare, ensuring their outputs reflect the diverse values and perspectives held across populations is critical. However, existing alignment approaches, including pluralistic paradigms like Modular Pluralism, often fall short in the health domain, where personal, cultural, and situational factors shape pluralism. Motivated by the aforementioned healthcare challenges, we propose a first lightweight, generalizable, pluralistic alignment approach, EthosAgents, designed to simulate diverse perspectives and values. We empirically show that it advances the pluralistic alignment for all three modes across seven varying-sized open and closed models. Our findings reveal that health-related pluralism demands adaptable and normatively aware approaches, offering insights into how these models can better respect diversity in other high-stakes domains.

</details>


### [62] [Struct-Bench: A Benchmark for Differentially Private Structured Text Generation](https://arxiv.org/abs/2509.10696)

*Shuaiqi Wang, Vikas Raunak, Arturs Backurs, Victor Reis, Pei Zhou, Sihao Chen, Longqi Yang, Zinan Lin, Sergey Yekhanin, Giulia Fanti*

**Main category:** cs.CL

**Keywords:** Differential Privacy, synthetic data, structured data, natural language, evaluation

**Relevance Score:** 7

**TL;DR:** Struct-Bench is a framework for evaluating synthetic datasets generated from structured data containing natural language, addressing the limitations of existing evaluation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve the evaluation of DP synthetic data generation techniques for structured datasets, which often include natural language elements.

**Method:** The authors introduce Struct-Bench, which requires dataset representation via Context-Free Grammar (CFG) and includes a benchmark with real and synthetic datasets annotated with CFGs.

**Key Contributions:**

	1. Introduction of the Struct-Bench framework for evaluating structured synthetic datasets with natural language elements.
	2. Creation of a benchmark comprising real and synthetic datasets annotated with CFGs.
	3. Development of reference metrics and a leaderboard for standardized evaluation.

**Result:** The study demonstrates that existing DP synthetic data generation methods face significant challenges when evaluated using the Struct-Bench framework on structured datasets.

**Limitations:** 

**Conclusion:** Struct-Bench provides a standardized platform for assessing privacy-preserving synthetic data generation methods and has been made publicly accessible.

**Abstract:** Differentially private (DP) synthetic data generation is a promising technique for utilizing private datasets that otherwise cannot be exposed for model training or other analytics. While much research literature has focused on generating private unstructured text and image data, in enterprise settings, structured data (e.g., tabular) is more common, often including natural language fields or components. Existing synthetic data evaluation techniques (e.g., FID) struggle to capture the structural properties and correlations of such datasets. In this work, we propose Struct-Bench, a framework and benchmark for evaluating synthetic datasets derived from structured datasets that contain natural language data. The Struct-Bench framework requires users to provide a representation of their dataset structure as a Context-Free Grammar (CFG). Our benchmark comprises 5 real-world and 2 synthetically generated datasets, each annotated with CFGs. We show that these datasets demonstrably present a great challenge even for state-of-the-art DP synthetic data generation methods. Struct-Bench also includes reference implementations of different metrics and a leaderboard, thereby providing researchers a standardized evaluation platform to benchmark and investigate privacy-preserving synthetic data generation methods. Further, we also present a case study showing how to use Struct-Bench to improve the synthetic data quality of Private Evolution (PE) on structured data. The benchmark and the leaderboard have been publicly made available at https://struct-bench.github.io.

</details>


### [63] [A Survey on Retrieval And Structuring Augmented Generation with Large Language Models](https://arxiv.org/abs/2509.10697)

*Pengcheng Jiang, Siru Ouyang, Yizhu Jiao, Ming Zhong, Runchu Tian, Jiawei Han*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval and Structuring, Knowledge Integration, Text Structuring, Multimodal Retrieval

**Relevance Score:** 8

**TL;DR:** This survey reviews Retrieval And Structuring (RAS) Augmented Generation for Large Language Models, focusing on retrieval mechanisms, text structuring techniques, and integration with LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by Large Language Models (LLMs) in real-world applications, such as hallucinations and limited domain expertise, by integrating information retrieval with structured knowledge.

**Method:** Examines various retrieval mechanisms (sparse, dense, hybrid) and text structuring techniques (taxonomy construction, hierarchical classification, information extraction) to create organized representations for LLMs.

**Key Contributions:**

	1. Examines multiple retrieval mechanisms for accessing external knowledge.
	2. Explores structuring techniques to enhance unstructured text representation.
	3. Highlights challenges and opportunities for future research in LLM integration.

**Result:** Identifies technical challenges in retrieval efficiency, structure quality, and knowledge integration; and highlights research opportunities in multimodal retrieval, cross-lingual structures, and interactive systems.

**Limitations:** 

**Conclusion:** Provides a comprehensive overview of RAS methods, applications, and future research directions for enhancing LLM capabilities.

**Abstract:** Large Language Models (LLMs) have revolutionized natural language processing with their remarkable capabilities in text generation and reasoning. However, these models face critical challenges when deployed in real-world applications, including hallucination generation, outdated knowledge, and limited domain expertise. Retrieval And Structuring (RAS) Augmented Generation addresses these limitations by integrating dynamic information retrieval with structured knowledge representations. This survey (1) examines retrieval mechanisms including sparse, dense, and hybrid approaches for accessing external knowledge; (2) explore text structuring techniques such as taxonomy construction, hierarchical classification, and information extraction that transform unstructured text into organized representations; and (3) investigate how these structured representations integrate with LLMs through prompt-based methods, reasoning frameworks, and knowledge embedding techniques. It also identifies technical challenges in retrieval efficiency, structure quality, and knowledge integration, while highlighting research opportunities in multimodal retrieval, cross-lingual structures, and interactive systems. This comprehensive overview provides researchers and practitioners with insights into RAS methods, applications, and future directions.

</details>


### [64] [SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based Instruction Dataset Creation](https://arxiv.org/abs/2509.10708)

*Iman Barati, Mostafa Amiri, Heshaam Faili*

**Main category:** cs.CL

**Keywords:** Large Language Models, Supervised Fine-Tuning, Dataset Generation, Instruction Learning, Model Editing

**Relevance Score:** 9

**TL;DR:** The paper presents SearchInstruct, a method for generating high-quality instruction datasets for supervised fine-tuning of large language models, enhancing their performance in specialized domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Creating suitable training datasets for supervised fine-tuning of large language models is crucial yet challenging due to domain constraints and data scarcity.

**Method:** SearchInstruct expands a limited set of domain-specific human-generated questions using a large language model, then retrieves domain-relevant resources to generate accurate answers.

**Key Contributions:**

	1. Introduction of SearchInstruct for dataset generation for SFT
	2. Demonstrated improvements in LLM performance with specialized datasets
	3. Facilitation of model editing processes.

**Result:** Experimental evaluations show that SearchInstruct improves both the diversity and quality of datasets, leading to better performance of LLMs in specialized areas, alongside facilitating model editing.

**Limitations:** 

**Conclusion:** The proposed method not only aids in dataset generation but also streamlines model updates; implementation details and resources are available on GitHub.

**Abstract:** Supervised Fine-Tuning (SFT) is essential for training large language models (LLMs), significantly enhancing critical capabilities such as instruction following and in-context learning. Nevertheless, creating suitable training datasets tailored for specific domains remains challenging due to unique domain constraints and data scarcity. In this paper, we propose SearchInstruct, an innovative method explicitly designed to construct high quality instruction datasets for SFT. Our approach begins with a limited set of domain specific, human generated questions, which are systematically expanded using a large language model. Subsequently, domain relevant resources are dynamically retrieved to generate accurate and contextually appropriate answers for each augmented question. Experimental evaluation demonstrates that SearchInstruct enhances both the diversity and quality of SFT datasets, leading to measurable improvements in LLM performance within specialized domains. Additionally, we show that beyond dataset generation, the proposed method can also effectively facilitate tasks such as model editing, enabling efficient updates to existing models. To facilitate reproducibility and community adoption, we provide full implementation details, the complete set of generated instruction response pairs, and the source code in a publicly accessible Git repository: [https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)

</details>


### [65] [PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models](https://arxiv.org/abs/2509.10737)

*Zaur Gouliev, Jennifer Waters, Chengqian Wang*

**Main category:** cs.CL

**Keywords:** disinformation, multilingual models, machine learning, transformers, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper systematically compares five multilingual transformer models on their ability to detect disinformation across multiple languages and introduces a novel corpus for evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the effectiveness of AI models in detecting disinformation in multilingual contexts, as current benchmarks predominantly focus on English.

**Method:** A comparative analysis of multilingual transformer models (mBERT, XLM, XLM-RoBERTa, RemBERT, and mT5) on a fake-vs-true classification task using the PolyTruth Disinfo Corpus.

**Key Contributions:**

	1. Introduction of PolyTruth Disinfo Corpus with 60,486 statement pairs across 25 languages.
	2. Systematic comparison of multilingual transformer models for disinformation detection.
	3. Insights on performance variations among models, emphasizing the real-world implications.

**Result:** RemBERT achieved better accuracy overall, especially in low-resource languages, while mBERT and XLM showed limitations with scarce training data.

**Limitations:** Performance variations may not fully represent all languages and contexts; primarily focused on selected models.

**Conclusion:** The study highlights the potential and limitations of AI systems for multilingual disinformation detection and provides a publicly available dataset for further research.

**Abstract:** Disinformation spreads rapidly across linguistic boundaries, yet most AI models are still benchmarked only on English. We address this gap with a systematic comparison of five multilingual transformer models: mBERT, XLM, XLM-RoBERTa, RemBERT, and mT5 on a common fake-vs-true machine learning classification task. While transformer-based language models have demonstrated notable success in detecting disinformation in English, their effectiveness in multilingual contexts still remains up for debate. To facilitate evaluation, we introduce PolyTruth Disinfo Corpus, a novel corpus of 60,486 statement pairs (false claim vs. factual correction) spanning over twenty five languages that collectively cover five language families and a broad topical range from politics, health, climate, finance, and conspiracy, half of which are fact-checked disinformation claims verified by an augmented MindBugs Discovery dataset. Our experiments revealed performance variations. Models such as RemBERT achieved better overall accuracy, particularly excelling in low-resource languages, whereas models like mBERT and XLM exhibit considerable limitations when training data is scarce. We provide a discussion of these performance patterns and implications for real-world deployment. The dataset is publicly available on our GitHub repository to encourage further experimentation and advancement. Our findings illuminate both the potential and the current limitations of AI systems for multilingual disinformation detection.

</details>


### [66] [Reasoning Under Uncertainty: Exploring Probabilistic Reasoning Capabilities of LLMs](https://arxiv.org/abs/2509.10739)

*Mobina Pournemat, Keivan Rezaei, Gaurang Sriramanan, Arman Zarei, Jiaxiang Fu, Yang Wang, Hamid Eghbalzadeh, Soheil Feizi*

**Main category:** cs.CL

**Keywords:** large language models, probabilistic reasoning, machine learning, inference, sample generation

**Relevance Score:** 9

**TL;DR:** This paper investigates the reasoning capabilities of large language models (LLMs) when dealing with explicit discrete probability distributions, identifying performance gaps and limitations in their probabilistic skills.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the reasoning capabilities of LLMs regarding probabilistic reasoning, addressing the inconsistencies observed in their behavior on such tasks.

**Method:** The study evaluates LLMs using three tasks: mode identification, maximum likelihood estimation, and sample generation, with careful prompting regarding joint and conditional distributions.

**Key Contributions:**

	1. First comprehensive study of LLMs' reasoning over discrete probability distributions.
	2. Identification of performance gaps between model sizes in probabilistic reasoning tasks.
	3. Insights into limitations regarding notation and context length impact on performance.

**Result:** The study finds a clear performance gap between smaller and larger models, with larger models performing better in inference and sample generation; however, notable limitations were observed such as sensitivity to notation and significant performance degradation with increased context length.

**Limitations:** Sensitivity to variations in notation used to represent probabilities; performance drops over 60% with increased context length.

**Conclusion:** The findings reveal important insights into LLMs' probabilistic reasoning abilities and highlight directions for future improvements.

**Abstract:** Despite widespread success in language understanding and generation, large language models (LLMs) exhibit unclear and often inconsistent behavior when faced with tasks that require probabilistic reasoning. In this work, we present the first comprehensive study of the reasoning capabilities of LLMs over explicit discrete probability distributions. Given observations from a probability distribution, we evaluate models on three carefully designed tasks, mode identification, maximum likelihood estimation, and sample generation, by prompting them to provide responses to queries about either the joint distribution or its conditionals. These tasks thus probe a range of probabilistic skills, including frequency analysis, marginalization, and generative behavior. Through comprehensive empirical evaluations, we demonstrate that there exists a clear performance gap between smaller and larger models, with the latter demonstrating stronger inference and surprising capabilities in sample generation. Furthermore, our investigations reveal notable limitations, including sensitivity to variations in the notation utilized to represent probabilistic outcomes and performance degradation of over 60% as context length increases. Together, our results provide a detailed understanding of the probabilistic reasoning abilities of LLMs and identify key directions for future improvement.

</details>


### [67] [Automated MCQA Benchmarking at Scale: Evaluating Reasoning Traces as Retrieval Sources for Domain Adaptation of Small Language Models](https://arxiv.org/abs/2509.10744)

*Ozan Gokdemir, Neil Getty, Robert Underwood, Sandeep Madireddy, Franck Cappello, Arvind Ramanathan, Ian T. Foster, Rick L. Stevens*

**Main category:** cs.CL

**Keywords:** language models, multiple-choice questions, scientific papers, benchmarking, reasoning traces

**Relevance Score:** 8

**TL;DR:** A modular framework for creating multiple-choice question-answering benchmarks from scientific literature is proposed, automating various stages and demonstrating improved language model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rapid growth of scientific knowledge, it is essential to update evaluation benchmarks for language models to reflect current literature.

**Method:** A scalable, modular framework that automates the generation of multiple-choice questions from large scientific corpora, including stages like PDF parsing and semantic chunking.

**Key Contributions:**

	1. Development of a modular pipeline for MCQA benchmark generation
	2. Mass generation of questions from open-access scientific literature
	3. Demonstrated performance improvements in small language models using reasoning traces

**Result:** The framework generated over 16,000 MCQs from 22,000 articles in radiation and cancer biology, with small language models outperforming GPT-4 on the exam.

**Limitations:** 

**Conclusion:** Reasoning-trace retrieval enhances performance on both synthetic and expert-annotated benchmarks, leading to superior results compared to previous models.

**Abstract:** As scientific knowledge grows at an unprecedented pace, evaluation benchmarks must evolve to reflect new discoveries and ensure language models are tested on current, diverse literature. We propose a scalable, modular framework for generating multiple-choice question-answering (MCQA) benchmarks directly from large corpora of scientific papers. Our pipeline automates every stage of MCQA creation, including PDF parsing, semantic chunking, question generation, and model evaluation. As a case study, we generate more than 16,000 MCQs from 22,000 open-access articles in radiation and cancer biology. We then evaluate a suite of small language models (1.1B-14B parameters) on these questions, comparing baseline accuracy with retrieval-augmented generation (RAG) from paper-derived semantic chunks and from reasoning traces distilled from GPT-4.1. We find that reasoning-trace retrieval consistently improves performance on both synthetic and expert-annotated benchmarks, enabling several small models to surpass GPT-4 on the 2023 Astro Radiation and Cancer Biology exam.

</details>


### [68] [RECAP: Transparent Inference-Time Emotion Alignment for Medical Dialogue Systems](https://arxiv.org/abs/2509.10746)

*Adarsh Srinivasan, Jacob Dineen, Muhammad Umar Afzal, Muhammad Uzair Sarfraz, Irbaz B. Riaz, Ben Zhou*

**Main category:** cs.CL

**Keywords:** empathy, language models, healthcare AI, emotional reasoning, RECAP

**Relevance Score:** 9

**TL;DR:** RECAP enhances emotional reasoning in healthcare AI by structuring empathy into distinct appraisal stages without retraining the model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models often fail to provide emotionally resonant communication in healthcare, which can harm patient trust and adherence.

**Method:** RECAP framework decomposes empathy into transparent stages and uses per-dimension Likert signals to generate responses.

**Key Contributions:**

	1. Introduction of the RECAP framework for emotional reasoning in healthcare AI.
	2. Demonstration of improved outcomes in emotional reasoning metrics.
	3. Validation of the framework through clinician evaluations for empathetic communication.

**Result:** RECAP achieves a 22-28% improvement in emotional reasoning across various benchmarks for 8B models and 10-13% for larger models, with positive clinician evaluations.

**Limitations:** The approach may require extensive validation in real-world clinical settings to fully ascertain its effectiveness and reliability.

**Conclusion:** The RECAP framework demonstrates that structured prompting can significantly enhance emotional intelligence in medical AIs while maintaining accountability.

**Abstract:** Large language models in healthcare often miss critical emotional cues, delivering medically sound but emotionally flat advice. This is especially problematic in clinical contexts where patients are distressed and vulnerable, and require empathic communication to support safety, adherence, and trust. We present RECAP (Reflect-Extract-Calibrate-Align-Produce), an inference-time framework that adds structured emotional reasoning without retraining. By decomposing empathy into transparent appraisal-theoretic stages and exposing per-dimension Likert signals, RECAP produces nuanced, auditable responses. Across EmoBench, SECEU, and EQ-Bench, RECAP improves emotional reasoning by 22-28% on 8B models and 10-13% on larger models over zero-shot baselines. Clinician evaluations further confirm superior empathetic communication. RECAP shows that modular, theory-grounded prompting can systematically enhance emotional intelligence in medical AI while preserving the accountability required for deployment.

</details>


### [69] [Judge Q: Trainable Queries for Optimized Information Retention in KV Cache Eviction](https://arxiv.org/abs/2509.10798)

*Yijun Liu, Yixuan Wang, Yuzhuang Xu, Shiyu Ji, Yang Xu, Qingfu Zhu, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** large language models, KV cache, eviction, soft token, machine learning

**Relevance Score:** 8

**TL;DR:** The paper proposes Judge Q, a novel training method for improving key-value (KV) cache eviction in large language models (LLMs) by incorporating a soft token list to better capture global information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing KV cache eviction methods that focus too much on local information, resulting in performance degradation.

**Method:** The proposed method, Judge Q, trains a soft token list to capture global information in conjunction with the KV cache, tuning only the model's embedding layer at a low cost.

**Key Contributions:**

	1. Introduction of Judge Q for KV cache eviction
	2. Use of soft tokens for capturing global information
	3. Demonstrated performance improvement on standard benchmarks

**Result:** Judge Q shows significantly better performance than existing eviction methods with improved scores on benchmarks like LongBench and RULER under the same eviction budget.

**Limitations:** 

**Conclusion:** The methodology enhances KV cache eviction for LLMs while being easily integrable into existing models with minimal overhead.

**Abstract:** Large language models (LLMs) utilize key-value (KV) cache to store historical information during sequence processing. The size of KV cache grows linearly as the length of the sequence extends, which seriously affects memory usage and decoding efficiency. Current methods for KV cache eviction typically utilize the last window from the pre-filling phase as queries to compute the KV importance scores for eviction. Although this scheme is simple to implement, it tends to overly focus on local information, potentially leading to the neglect or omission of crucial global information. To mitigate this issue, we propose Judge Q, a novel training method which incorporates a soft token list. This method only tunes the model's embedding layer at a low training cost. By concatenating the soft token list at the end of the input sequence, we train these tokens' attention map to the original input sequence to align with that of the actual decoded tokens. In this way, the queries corresponding to the soft tokens can effectively capture global information and better evaluate the importance of the keys and values within the KV cache, thus maintaining decoding quality when KV cache is evicted. Under the same eviction budget, our method exhibits less performance degradation compared to existing eviction approaches. We validate our approach through experiments conducted on models such as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks including LongBench, RULER, and Needle-in-a-Haystack. Results indicate an improvement of approximately 1 point on the LongBench and over 3 points on RULER. This proposed methodology can be seamlessly integrated into existing open-source models with minimal training overhead, thereby enhancing performance in KV cache eviction scenarios.

</details>


### [70] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)

*Dominic Petrak, Thy Thy Tran, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** Automated Error Discovery, Error Detection, Conversational AI

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for Automated Error Discovery in conversational AI and introduces SEEED, an encoder-based method that improves the detection of unknown errors in dialogue systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective error detection in LLM-based conversational agents that struggle with identifying unspecified errors during deployment is the primary motivation behind this research.

**Method:** The paper proposes Automated Error Discovery and develops SEEED, which enhances error detection through Soft Clustering and extends encoder-based approaches with improved distance weighting and sample ranking techniques.

**Key Contributions:**

	1. Introduction of Automated Error Discovery framework
	2. Development of SEEED for error detection
	3. Improvements in accuracy on dialogue datasets

**Result:** SEEED shows significant improvements in detecting unknown errors, outperforming various baseline models, including GPT-4o and Phi-4, with an accuracy increase of up to 8 points across multiple datasets.

**Limitations:** 

**Conclusion:** The findings indicate that SEEED effectively enhances the performance of conversational AI in identifying and adapting to unknown user behaviors and model updates.

**Abstract:** Although LLM-based conversational agents demonstrate strong fluency and coherence, they still produce undesirable behaviors (errors) that are challenging to prevent from reaching users during deployment. Recent research leverages large language models (LLMs) to detect errors and guide response-generation models toward improvement. However, current LLMs struggle to identify errors not explicitly specified in their instructions, such as those arising from updates to the response-generation model or shifts in user behavior. In this work, we introduce Automated Error Discovery, a framework for detecting and defining errors in conversational AI, and propose SEEED (Soft Clustering Extended Encoder-Based Error Detection), as an encoder-based approach to its implementation. We enhance the Soft Nearest Neighbor Loss by amplifying distance weighting for negative samples and introduce Label-Based Sample Ranking to select highly contrastive examples for better representation learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 -- across multiple error-annotated dialogue datasets, improving the accuracy for detecting unknown errors by up to 8 points and demonstrating strong generalization to unknown intent detection.

</details>


### [71] [Evaluating Large Language Models for Evidence-Based Clinical Question Answering](https://arxiv.org/abs/2509.10843)

*Can Wang, Yiqun Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Questions, Retrieval-augmented prompting, Evidence-based Medicine, Benchmarking

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of LLMs in answering clinical questions using a multi-source benchmark, highlighting strengths in structured guidelines and the benefits of retrieval-augmented prompting for accuracy improvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously assess the ability of large language models to answer nuanced, evidence-based clinical questions using a diverse set of authoritative sources.

**Method:** The authors curate a benchmark from Cochrane systematic reviews, American Heart Association guidelines, and clinical narratives, utilizing models GPT-4o-mini and GPT-5 to analyze their accuracy across different sources and clinical contexts.

**Key Contributions:**

	1. Curated a multi-source benchmark from clinical guidelines and systematic reviews.
	2. Identified significant accuracy variations depending on guideline structure and quality of citations.
	3. Demonstrated the effectiveness of retrieval-augmented prompting in improving model accuracy.

**Result:** Models achieve the highest accuracy (90%) on structured recommendations, with a correlation between the citation count of systematic reviews impacting answer accuracy. Retrieval-augmented prompting significantly enhances performance, with notable improvements when using relevant PubMed abstracts.

**Limitations:** Moderate reasoning capability about evidence quality; accuracy varies widely based on document clarity and relevance of retrieved sources.

**Conclusion:** While LLMs show promise in clinical question answering, their current limitations necessitate careful evaluation by specialty and question type; retrieval-augmented prompting is crucial for improving accuracy and alignment with evidence.

**Abstract:** Large Language Models (LLMs) have demonstrated substantial progress in biomedical and clinical applications, motivating rigorous evaluation of their ability to answer nuanced, evidence-based questions. We curate a multi-source benchmark drawing from Cochrane systematic reviews and clinical guidelines, including structured recommendations from the American Heart Association and narrative guidance used by insurers. Using GPT-4o-mini and GPT-5, we observe consistent performance patterns across sources and clinical domains: accuracy is highest on structured guideline recommendations (90%) and lower on narrative guideline and systematic review questions (60--70%). We also find a strong correlation between accuracy and the citation count of the underlying systematic reviews, where each doubling of citations is associated with roughly a 30% increase in the odds of a correct answer. Models show moderate ability to reason about evidence quality when contextual information is supplied. When we incorporate retrieval-augmented prompting, providing the gold-source abstract raises accuracy on previously incorrect items to 0.79; providing top 3 PubMed abstracts (ranked by semantic relevance) improves accuracy to 0.23, while random abstracts reduce accuracy (0.10, within temperature variation). These effects are mirrored in GPT-4o-mini, underscoring that source clarity and targeted retrieval -- not just model size -- drive performance. Overall, our results highlight both the promise and current limitations of LLMs for evidence-based clinical question answering. Retrieval-augmented prompting emerges as a useful strategy to improve factual accuracy and alignment with source evidence, while stratified evaluation by specialty and question type remains essential to understand current knowledge access and to contextualize model performance.

</details>


### [72] [GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings](https://arxiv.org/abs/2509.10844)

*Yixuan Tang, Yi Yang*

**Main category:** cs.CL

**Keywords:** model pruning, embedding models, domain-specific, Fisher Information, domain alignment

**Relevance Score:** 8

**TL;DR:** GAPrune is a pruning framework that enhances domain-specific embedding models by balancing domain importance with general linguistic knowledge, achieving effective model compression and improved performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing model pruning methods fail to effectively distinguish between domain-specific patterns and general representations, leading to suboptimal pruning outcomes for specialized embedding models.

**Method:** GAPrune utilizes Fisher Information to evaluate parameter importance and general-domain gradient alignment for assessing parameter behavior, combining these metrics through a Domain Alignment Importance (DAI) scoring system.

**Key Contributions:**

	1. Introduction of Domain Alignment Importance (DAI) scoring for parameter importance evaluation.
	2. Effective pruning strategy that balances domain-specific and general performance.
	3. Empirical results demonstrating performance retention and improvement on domain benchmarks.

**Result:** GAPrune maintains performance within 2.5% of dense models at 50% sparsity in one-shot pruning, showing noticeable improvements upon retraining: +4.51% on FinMTEB and +1.73% on ChemTEB.

**Limitations:** 

**Conclusion:** GAPrune provides a principled approach to model compression that enhances domain-specific capabilities, offering a valuable method for developing specialized models.

**Abstract:** Domain-specific embedding models have shown promise for applications that require specialized semantic understanding, such as coding agents and financial retrieval systems, often achieving higher performance gains than general models. However, state-of-the-art embedding models are typically based on LLMs, which contain billions of parameters, making deployment challenging in resource-constrained environments. Model compression through pruning offers a promising solution, but existing pruning methods treat all parameters uniformly, failing to distinguish between general semantic representations and domain-specific patterns, leading to suboptimal pruning decisions. Thus, we propose GAPrune, a pruning framework that addresses this challenge by considering both domain importance and preserving general linguistic foundation. Our method uses Fisher Information to measure importance and general-domain gradient alignment to assess parameter behavior, then combines these signals using our Domain Alignment Importance (DAI) scoring. Lower DAI scores indicate that the parameter is either less important for the domain task or creates conflicts between domain and general objectives. Experiments on two domain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance within 2.5% of dense models in one-shot pruning at 50% sparsity, while outperforming all baselines. With retraining in 100 steps, GAPrune achieves +4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our pruning strategy not only preserves but enhances domain-specific capabilities. Our findings demonstrate that principled pruning strategies can achieve model compression and enhanced domain specialization, providing the research community with a new approach for development.

</details>


### [73] [Text2Sign Diffusion: A Generative Approach for Gloss-Free Sign Language Production](https://arxiv.org/abs/2509.10845)

*Liqian Feng, Lintao Wang, Kun Hu, Dehui Kong, Zhiyong Wang*

**Main category:** cs.CL

**Keywords:** sign language, gloss-free, latent diffusion, cross-modal alignment, state-of-the-art

**Relevance Score:** 4

**TL;DR:** A novel gloss-free diffusion-based model for generating sign language sequences from spoken text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge communication gaps for deaf and hard-of-hearing communities by improving sign language production (SLP) without reliance on gloss.

**Method:** A gloss-free latent diffusion model generates sign language sequences from noisy latent sign codes and spoken text through a non-autoregressive iterative denoising process, complemented by a cross-modal signing aligner for visual-textual bridge.

**Key Contributions:**

	1. Introduction of a gloss-free SLP approach using latent diffusion models
	2. Development of a cross-modal signing aligner for visual and textual content
	3. Demonstration of state-of-the-art performance on benchmark datasets

**Result:** Achieved state-of-the-art performance on the PHOENIX14T and How2Sign datasets, demonstrating effectiveness in generating accurate sign language sequences.

**Limitations:** 

**Conclusion:** The proposed Text2Sign Diffusion model enhances the flexibility and generalization of sign language production, allowing for more effective communication.

**Abstract:** Sign language production (SLP) aims to translate spoken language sentences into a sequence of pose frames in a sign language, bridging the communication gap and promoting digital inclusion for deaf and hard-of-hearing communities. Existing methods typically rely on gloss, a symbolic representation of sign language words or phrases that serves as an intermediate step in SLP. This limits the flexibility and generalization of SLP, as gloss annotations are often unavailable and language-specific. Therefore, we present a novel diffusion-based generative approach - Text2Sign Diffusion (Text2SignDiff) for gloss-free SLP. Specifically, a gloss-free latent diffusion model is proposed to generate sign language sequences from noisy latent sign codes and spoken text jointly, reducing the potential error accumulation through a non-autoregressive iterative denoising process. We also design a cross-modal signing aligner that learns a shared latent space to bridge visual and textual content in sign and spoken languages. This alignment supports the conditioned diffusion-based process, enabling more accurate and contextually relevant sign language generation without gloss. Extensive experiments on the commonly used PHOENIX14T and How2Sign datasets demonstrate the effectiveness of our method, achieving the state-of-the-art performance.

</details>


### [74] [A funny companion: Distinct neural responses to perceived AI- versus human- generated humor](https://arxiv.org/abs/2509.10847)

*Xiaohui Rao, Hanlin Wu, Zhenguang G. Cai*

**Main category:** cs.CL

**Keywords:** AI humor, neurophysiology, cognitive adaptation, social interaction, electroencephalography

**Relevance Score:** 8

**TL;DR:** This study investigates how people cognitively and emotionally respond to humor from AI versus human sources using EEG data.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** With AI companions becoming capable of human-like communication, understanding cognitive and emotional responses to AI humor is crucial for developing engaging interactions.

**Method:** The study employed electroencephalography (EEG) to compare the neurophysiological responses of participants when exposed to humor from AI and humans.

**Key Contributions:**

	1. Demonstrated differential neurophysiological processing of humor from AI and humans.
	2. Revealed that cognitive adaptation to AI humor leads to enhanced emotional responses.
	3. Identified social attitudes toward AI as a modulating factor in humor processing.

**Result:** Participants rated AI and human humor similarly funny, but EEG results showed reduced cognitive effort (smaller N400 effect) for AI humor and a greater emotional response (larger LPP), indicating surprise.

**Limitations:** 

**Conclusion:** The study suggests that as people adapt to AI humor, their emotional responses may intensify, challenging the notion of 'algorithm aversion' in humor and emphasizing humor's role in human-AI engagement.

**Abstract:** As AI companions become capable of human-like communication, including telling jokes, understanding how people cognitively and emotionally respond to AI humor becomes increasingly important. This study used electroencephalography (EEG) to compare how people process humor from AI versus human sources. Behavioral analysis revealed that participants rated AI and human humor as comparably funny. However, neurophysiological data showed that AI humor elicited a smaller N400 effect, suggesting reduced cognitive effort during the processing of incongruity. This was accompanied by a larger Late Positive Potential (LPP), indicating a greater degree of surprise and emotional response. This enhanced LPP likely stems from the violation of low initial expectations regarding AI's comedic capabilities. Furthermore, a key temporal dynamic emerged: human humor showed habituation effects, marked by an increasing N400 and a decreasing LPP over time. In contrast, AI humor demonstrated increasing processing efficiency and emotional reward, with a decreasing N400 and an increasing LPP. This trajectory reveals how the brain can dynamically update its predictive model of AI capabilities. This process of cumulative reinforcement challenges "algorithm aversion" in humor, as it demonstrates how cognitive adaptation to AI's language patterns can lead to an intensified emotional reward. Additionally, participants' social attitudes toward AI modulated these neural responses, with higher perceived AI trustworthiness correlating with enhanced emotional engagement. These findings indicate that the brain responds to AI humor with surprisingly positive and intense reactions, highlighting humor's potential for fostering genuine engagement in human-AI social interaction.

</details>


### [75] [Pre-Storage Reasoning for Episodic Memory: Shifting Inference Burden to Memory for Personalized Dialogue](https://arxiv.org/abs/2509.10852)

*Sangyeop Kim, Yohan Lee, Sanghwa Kim, Hyunjong Kim, Sungzoon Cho*

**Main category:** cs.CL

**Keywords:** Conversational AI, Long-term memory, Memory construction, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** PREMem introduces a novel method for improving long-term memory in conversational AI by shifting complex reasoning from response generation to memory construction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current conversational AI systems struggle with long-term memory due to excessive reasoning during response generation, which affects performance, especially in smaller models.

**Method:** PREMem extracts memory fragments categorized as factual, experiential, and subjective, establishing explicit relationships across sessions for improved memory synthesis.

**Key Contributions:**

	1. Introduction of PREMem for memory-efficient reasoning
	2. Categorization of memory fragments into factual, experiential, and subjective
	3. Performance improvements across various model sizes

**Result:** Experiments demonstrate significant performance improvements, with smaller models achieving results similar to larger models, while effectively managing token constraints.

**Limitations:** 

**Conclusion:** By performing reasoning during pre-storage, PREMem enriches memory representations and reduces computational burdens during interactions.

**Abstract:** Effective long-term memory in conversational AI requires synthesizing information across multiple sessions. However, current systems place excessive reasoning burden on response generation, making performance significantly dependent on model sizes. We introduce PREMem (Pre-storage Reasoning for Episodic Memory), a novel approach that shifts complex reasoning processes from inference to memory construction. PREMem extracts fine-grained memory fragments categorized into factual, experiential, and subjective information; it then establishes explicit relationships between memory items across sessions, capturing evolution patterns like extensions, transformations, and implications. By performing this reasoning during pre-storage rather than when generating a response, PREMem creates enriched representations while reducing computational demands during interactions. Experiments show significant performance improvements across all model sizes, with smaller models achieving results comparable to much larger baselines while maintaining effectiveness even with constrained token budgets. Code and dataset are available at https://github.com/sangyeop-kim/PREMem.

</details>


### [76] [Quantifier Scope Interpretation in Language Learners and LLMs](https://arxiv.org/abs/2509.10860)

*Shaohua Fang, Yue Li, Yan Cong*

**Main category:** cs.CL

**Keywords:** quantifier scope, large language models, human behavior, cross-linguistic, interpretation

**Relevance Score:** 7

**TL;DR:** This study investigates how large language models interpret quantifier scope in English and Chinese, revealing similarities and differences in their performance compared to humans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine interpretative ambiguities in sentences with multiple quantifiers across languages, focusing on how LLMs handle these interpretations.

**Method:** A cross-linguistic approach was adopted, using probabilities to assess interpretive likelihood and human similarity (HS) scores to evaluate LLMs' emulation of human performance.

**Key Contributions:**

	1. Cross-linguistic analysis of LLMs' interpretive behaviors
	2. Evaluation of human similarity scores for quantifier interpretation
	3. Insights on the impact of training data language background on LLM performance

**Result:** Most LLMs prefer surface scope interpretations and demonstrate alignment with human tendencies, though some distinguish between English and Chinese in inverse scope preferences.

**Limitations:** Variability in LLMs' performance; not all models differentiate adequately between language preferences.

**Conclusion:** LLMs have the potential to approximate human quantifier scope interpretation, significantly influenced by their architecture, scale, and pre-training data language background.

**Abstract:** Sentences with multiple quantifiers often lead to interpretive ambiguities, which can vary across languages. This study adopts a cross-linguistic approach to examine how large language models (LLMs) handle quantifier scope interpretation in English and Chinese, using probabilities to assess interpretive likelihood. Human similarity (HS) scores were used to quantify the extent to which LLMs emulate human performance across language groups. Results reveal that most LLMs prefer the surface scope interpretations, aligning with human tendencies, while only some differentiate between English and Chinese in the inverse scope preferences, reflecting human-similar patterns. HS scores highlight variability in LLMs' approximation of human behavior, but their overall potential to align with humans is notable. Differences in model architecture, scale, and particularly models' pre-training data language background, significantly influence how closely LLMs approximate human quantifier scope interpretations.

</details>


### [77] [Term2Note: Synthesising Differentially Private Clinical Notes from Medical Terms](https://arxiv.org/abs/2509.10882)

*Yuping Wu, Viktor Schlegel, Warren Del-Pinto, Srinivasan Nandakumar, Iqra Zahid, Yidan Sun, Usama Farghaly Omar, Amirah Jasmine, Arun-Kumar Kaliya-Perumal, Chun Shen Tham, Gabriel Connors, Anil A Bharath, Goran Nenadic*

**Main category:** cs.CL

**Keywords:** differential privacy, synthetic data, clinical notes, machine learning, healthcare

**Relevance Score:** 9

**TL;DR:** Term2Note synthesizes clinical notes under differential privacy, balancing privacy and utility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy concerns in using real-world training data in healthcare machine learning.

**Method:** Term2Note generates long clinical notes by separating content and form, conditioning on DP medical terms with individual DP constraints.

**Key Contributions:**

	1. Development of Term2Note methodology for synthesizing clinical notes under strong DP constraints.
	2. Demonstrated strong fidelity and utility of synthetic notes through experimental results.
	3. Achieved significant improvements over existing DP text generation methods.

**Result:** The synthetic notes produced align closely with real clinical notes in statistical properties and support high-performing classification models.

**Limitations:** 

**Conclusion:** Term2Note presents a viable privacy-preserving alternative to sensitive clinical notes, showing significant improvements in fidelity and utility.

**Abstract:** Training data is fundamental to the success of modern machine learning models, yet in high-stakes domains such as healthcare, the use of real-world training data is severely constrained by concerns over privacy leakage. A promising solution to this challenge is the use of differentially private (DP) synthetic data, which offers formal privacy guarantees while maintaining data utility. However, striking the right balance between privacy protection and utility remains challenging in clinical note synthesis, given its domain specificity and the complexity of long-form text generation. In this paper, we present Term2Note, a methodology to synthesise long clinical notes under strong DP constraints. By structurally separating content and form, Term2Note generates section-wise note content conditioned on DP medical terms, with each governed by separate DP constraints. A DP quality maximiser further enhances synthetic notes by selecting high-quality outputs. Experimental results show that Term2Note produces synthetic notes with statistical properties closely aligned with real clinical notes, demonstrating strong fidelity. In addition, multi-label classification models trained on these synthetic notes perform comparably to those trained on real data, confirming their high utility. Compared to existing DP text generation baselines, Term2Note achieves substantial improvements in both fidelity and utility while operating under fewer assumptions, suggesting its potential as a viable privacy-preserving alternative to using sensitive clinical notes.

</details>


### [78] [CultureSynth: A Hierarchical Taxonomy-Guided and Retrieval-Augmented Framework for Cultural Question-Answer Synthesis](https://arxiv.org/abs/2509.10886)

*Xinyu Zhang, Pei Zhang, Shuang Luo, Jialong Tang, Yu Wan, Baosong Yang, Fei Huang*

**Main category:** cs.CL

**Keywords:** Cultural competence, Large language models, Multilingual taxonomy, Retrieval-Augmented Generation, Culturally relevant AI

**Relevance Score:** 8

**TL;DR:** CultureSynth introduces a framework for assessing and enhancing the cultural competence of large language models (LLMs) through a hierarchical taxonomy and RAG-based synthesis of culturally relevant content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for better evaluation of LLMs' cultural competence, which is essential in multicultural contexts and is currently hampered by fragmented taxonomies and manual annotation.

**Method:** The proposed CultureSynth framework includes a comprehensive multilingual cultural taxonomy and a Retrieval-Augmented Generation methodology to create culturally relevant question-answer pairs.

**Key Contributions:**

	1. Introduction of a hierarchical multilingual cultural taxonomy covering 12 primary topics.
	2. Development of a RAG-based methodology for synthesizing question-answer pairs relevant to cultural contexts.
	3. Creation of the CultureSynth-7 benchmark with 19,360 entries to assess LLM cultural competence.

**Result:** The CultureSynth-7 benchmark contains nearly 20,000 entries and shows that a model size of at least 3 billion parameters is necessary for basic cultural competence, revealing varying performance across different LLMs and significant geographic disparities.

**Limitations:** 

**Conclusion:** CultureSynth provides a scalable solution for creating culturally aware AI systems while minimizing dependence on manual data annotation.

**Abstract:** Cultural competence, defined as the ability to understand and adapt to multicultural contexts, is increasingly vital for large language models (LLMs) in global environments. While several cultural benchmarks exist to assess LLMs' cultural competence, current evaluations suffer from fragmented taxonomies, domain specificity, and heavy reliance on manual data annotation. To address these limitations, we introduce CultureSynth, a novel framework comprising (1) a comprehensive hierarchical multilingual cultural taxonomy covering 12 primary and 130 secondary topics, and (2) a Retrieval-Augmented Generation (RAG)-based methodology leveraging factual knowledge to synthesize culturally relevant question-answer pairs. The CultureSynth-7 synthetic benchmark contains 19,360 entries and 4,149 manually verified entries across 7 languages. Evaluation of 14 prevalent LLMs of different sizes reveals clear performance stratification led by ChatGPT-4o-Latest and Qwen2.5-72B-Instruct. The results demonstrate that a 3B-parameter threshold is necessary for achieving basic cultural competence, models display varying architectural biases in knowledge processing, and significant geographic disparities exist across models. We believe that CultureSynth offers a scalable framework for developing culturally aware AI systems while reducing reliance on manual annotation\footnote{Benchmark is available at https://github.com/Eyr3/CultureSynth.}.

</details>


### [79] [Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction](https://arxiv.org/abs/2509.10922)

*Tsuyoshi Iwata, Guillaume Comte, Melissa Flores, Ryoma Kondo, Ryohei Hisano*

**Main category:** cs.CL

**Keywords:** Environmental Social Governance, Knowledge Graph, Large Language Models

**Relevance Score:** 4

**TL;DR:** This paper presents a method for creating structured knowledge representations of non-financial risks from unstructured news data, aimed at aligning with sustainable frameworks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate and interpretable ESG data in regulatory and investment contexts is increasing, yet aligning this data with normative frameworks presents significant challenges.

**Method:** A semi-automatic approach using lightweight ontology design, formal pattern modeling, and large language models to create templates in the Resource Description Framework for extracting relevant information from news.

**Key Contributions:**

	1. Development of a semi-automatic method for ESG data representation
	2. Creation of reusable templates for knowledge extraction
	3. Construction of a structured knowledge graph linking news to sustainability principles

**Result:** The proposed method creates a structured knowledge graph that links reported incidents to specific sustainability principles, enabling scalable identification of non-compliance with guidelines.

**Limitations:** 

**Conclusion:** The framework offers a transparent method for interpreting ESG events and their alignment with international sustainability guidelines.

**Abstract:** The growing importance of environmental, social, and governance data in regulatory and investment contexts has increased the need for accurate, interpretable, and internationally aligned representations of non-financial risks, particularly those reported in unstructured news sources. However, aligning such controversy-related data with principle-based normative frameworks, such as the United Nations Global Compact or Sustainable Development Goals, presents significant challenges. These frameworks are typically expressed in abstract language, lack standardized taxonomies, and differ from the proprietary classification systems used by commercial data providers. In this paper, we present a semi-automatic method for constructing structured knowledge representations of environmental, social, and governance events reported in the news. Our approach uses lightweight ontology design, formal pattern modeling, and large language models to convert normative principles into reusable templates expressed in the Resource Description Framework. These templates are used to extract relevant information from news content and populate a structured knowledge graph that links reported incidents to specific framework principles. The result is a scalable and transparent framework for identifying and interpreting non-compliance with international sustainability guidelines.

</details>


### [80] [Introducing Spotlight: A Novel Approach for Generating Captivating Key Information from Documents](https://arxiv.org/abs/2509.10935)

*Ankan Mullick, Sombit Bose, Rounak Saha, Ayan Kumar Bhowmick, Aditya Vempaty, Prasenjit Dey, Ravi Kokku, Pawan Goyal, Niloy Ganguly*

**Main category:** cs.CL

**Keywords:** information extraction, narrative generation, large language models

**Relevance Score:** 7

**TL;DR:** Spotlight proposes a novel method for information extraction that emphasizes engaging narratives over comprehensive summaries, using a two-stage LLM fine-tuning process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reader engagement with source material through focused narratives rather than broad summaries.

**Method:** A two-stage approach involving fine-tuning a large language model on bespoke benchmark data, followed by alignment through Direct Preference Optimization (DPO).

**Key Contributions:**

	1. Introduction of the spotlight paradigm for narrative extraction
	2. Development of a tailored benchmarking dataset for evaluation
	3. Implementation of Direct Preference Optimization for model alignment

**Result:** The model achieves high precision in identifying key elements, improves readability, and enhances document engagement.

**Limitations:** 

**Conclusion:** Spotlight offers a promising alternative for information extraction that could redefine how narratives are generated from documents.

**Abstract:** In this paper, we introduce Spotlight, a novel paradigm for information extraction that produces concise, engaging narratives by highlighting the most compelling aspects of a document. Unlike traditional summaries, which prioritize comprehensive coverage, spotlights selectively emphasize intriguing content to foster deeper reader engagement with the source material. We formally differentiate spotlights from related constructs and support our analysis with a detailed benchmarking study using new datasets curated for this work. To generate high-quality spotlights, we propose a two-stage approach: fine-tuning a large language model on our benchmark data, followed by alignment via Direct Preference Optimization (DPO). Our comprehensive evaluation demonstrates that the resulting model not only identifies key elements with precision but also enhances readability and boosts the engagement value of the original document.

</details>


### [81] [An Interpretable Benchmark for Clickbait Detection and Tactic Attribution](https://arxiv.org/abs/2509.10937)

*Lihi Nofar, Tomer Portal, Aviv Elbaz, Alexander Apartsin, Yehudit Aperstein*

**Main category:** cs.CL

**Keywords:** clickbait detection, explainable AI, machine learning, natural language processing, digital media

**Relevance Score:** 7

**TL;DR:** This paper proposes an explainable model for detecting clickbait headlines, leveraging a synthetic dataset and a two-stage framework for analysis.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in clickbait headlines negatively impacts information credibility and user trust in digital media.

**Method:** The study employs a two-stage framework for automatic clickbait analysis: detection using a fine-tuned BERT classifier and LLMs, and tactic attribution with a BERT-based classifier.

**Key Contributions:**

	1. Introduction of a synthetic dataset for clickbait detection
	2. Development of a two-stage framework for analysis
	3. Enhanced explainability in machine learning models for detecting clickbait

**Result:** The model successfully identifies clickbait titles and associates them with specific linguistic strategies, demonstrating improved explainability in clickbait detection.

**Limitations:** 

**Conclusion:** This research contributes to the development of transparent AI systems capable of addressing manipulative media content, fostering trust in digital information.

**Abstract:** The proliferation of clickbait headlines poses significant challenges to the credibility of information and user trust in digital media. While recent advances in machine learning have improved the detection of manipulative content, the lack of explainability limits their practical adoption. This paper presents a model for explainable clickbait detection that not only identifies clickbait titles but also attributes them to specific linguistic manipulation strategies. We introduce a synthetic dataset generated by systematically augmenting real news headlines using a predefined catalogue of clickbait strategies. This dataset enables controlled experimentation and detailed analysis of model behaviour. We present a two-stage framework for automatic clickbait analysis comprising detection and tactic attribution. In the first stage, we compare a fine-tuned BERT classifier with large language models (LLMs), specifically GPT-4.0 and Gemini 2.4 Flash, under both zero-shot prompting and few-shot prompting enriched with illustrative clickbait headlines and their associated persuasive tactics. In the second stage, a dedicated BERT-based classifier predicts the specific clickbait strategies present in each headline. This work advances the development of transparent and trustworthy AI systems for combating manipulative media content. We share the dataset with the research community at https://github.com/LLM-HITCS25S/ClickbaitTacticsDetection

</details>


### [82] [EmoBench-Reddit: A Hierarchical Benchmark for Evaluating the Emotional Intelligence of Multimodal Large Language Models](https://arxiv.org/abs/2509.11101)

*Haokun Li, Yazhou Zhang, Jizhi Ding, Qiuchi Li, Peng Zhang*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Emotion understanding, Benchmark, Human emotions, Hierarchical framework

**Relevance Score:** 9

**TL;DR:** Introducing EmoBench-Reddit, a benchmark for assessing multimodal emotion understanding using images and social media text.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluation benchmarks for MLLMs inadequately assess their understanding of complex human emotions; thus, EmoBench-Reddit aims to fill this gap.

**Method:** EmoBench-Reddit includes 350 curated samples from Reddit, featuring images, text, and emotion categories. The hierarchical framework involves basic perception and advanced cognition tasks with multiple-choice and open-ended questions.

**Key Contributions:**

	1. Introduction of a novel benchmark for multimodal emotion understanding
	2. Hierarchical task framework for progressive evaluation
	3. Combination of AI and manual verification for data quality assurance

**Result:** The benchmark allows for a more comprehensive evaluation of MLLMs in understanding human emotions and improves how models process and interpret multimodal content in nuanced contexts.

**Limitations:** 

**Conclusion:** By combining AI assistance and manual verification, EmoBench-Reddit enhances the reliability of assessments in emotion understanding for MLLMs, paving the way for better applications in user interaction.

**Abstract:** With the rapid advancement of Multimodal Large Language Models (MLLMs), they have demonstrated exceptional capabilities across a variety of vision-language tasks. However, current evaluation benchmarks predominantly focus on objective visual question answering or captioning, inadequately assessing the models' ability to understand complex and subjective human emotions. To bridge this gap, we introduce EmoBench-Reddit, a novel, hierarchical benchmark for multimodal emotion understanding. The dataset comprises 350 meticulously curated samples from the social media platform Reddit, each containing an image, associated user-provided text, and an emotion category (sad, humor, sarcasm, happy) confirmed by user flairs. We designed a hierarchical task framework that progresses from basic perception to advanced cognition, with each data point featuring six multiple-choice questions and one open-ended question of increasing difficulty. Perception tasks evaluate the model's ability to identify basic visual elements (e.g., colors, objects), while cognition tasks require scene reasoning, intent understanding, and deep empathy integrating textual context. We ensured annotation quality through a combination of AI assistance (Claude 4) and manual verification.

</details>


### [83] [Fluid Language Model Benchmarking](https://arxiv.org/abs/2509.11106)

*Valentin Hofmann, David Heineman, Ian Magnusson, Kyle Lo, Jesse Dodge, Maarten Sap, Pang Wei Koh, Chun Wang, Hannaneh Hajishirzi, Noah A. Smith*

**Main category:** cs.CL

**Keywords:** language model, benchmarking, adaptive testing, item response theory, evaluation quality

**Relevance Score:** 8

**TL;DR:** Fluid Benchmarking introduces a new approach to LM benchmarking that adapts evaluations based on the strengths of different language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing language model benchmarking methods, which often suffer from high costs, poor quality evaluations, and saturation, highlighting the need for a more holistic evaluation framework.

**Method:** Fluid Benchmarking utilizes an item response model that adapts to the capability level of the language model, allowing for dynamic selection of evaluation items similar to computerized adaptive testing.

**Key Contributions:**

	1. Introduction of Fluid Benchmarking for LM evaluation
	2. Demonstration of performance improvements across multiple dimensions
	3. Combination of item response theory with dynamic item selection for enhanced evaluation

**Result:** Fluid Benchmarking demonstrates superior performance over traditional random sampling and other sophisticated methods across metrics such as efficiency, validity, variance, and saturation, achieving higher validity with far fewer items.

**Limitations:** 

**Conclusion:** The findings indicate that language model benchmarking can significantly benefit from dynamic evaluations tailored to model capabilities rather than using static methods.

**Abstract:** Language model (LM) benchmarking faces several challenges: comprehensive evaluations are costly, benchmarks often fail to measure the intended capabilities, and evaluation quality can degrade due to labeling errors and benchmark saturation. Although various strategies have been proposed to mitigate these issues, they tend to address individual aspects in isolation, neglecting broader questions about overall evaluation quality. Here, we introduce Fluid Benchmarking, a new evaluation approach that advances LM benchmarking across multiple dimensions. Inspired by psychometrics, Fluid Benchmarking is based on the insight that the relative value of benchmark items depends on an LM's capability level, suggesting that evaluation should adapt to each LM. Methodologically, Fluid Benchmarking estimates an item response model based on existing LM evaluation results and uses the inferred quantities to select evaluation items dynamically, similar to computerized adaptive testing in education. In our experiments, we compare Fluid Benchmarking against the common practice of random item sampling as well as more sophisticated baselines, including alternative methods grounded in item response theory. We examine four dimensions -- efficiency, validity, variance, and saturation -- and find that Fluid Benchmarking achieves superior performance in all of them (e.g., higher validity and less variance on MMLU with fifty times fewer items). Our analysis shows that the two components of Fluid Benchmarking have distinct effects: item response theory, used to map performance into a latent ability space, increases validity, while dynamic item selection reduces variance. Overall, our results suggest that LM benchmarking can be substantially improved by moving beyond static evaluation.

</details>


### [84] [We Argue to Agree: Towards Personality-Driven Argumentation-Based Negotiation Dialogue Systems for Tourism](https://arxiv.org/abs/2509.11118)

*Priyanshu Priya, Saurav Dudhate, Desai Vishesh Yasheshbhai, Asif Ekbal*

**Main category:** cs.CL

**Keywords:** Negotiation, Argumentation, Personality, Dialogue Generation, Large Language Models

**Relevance Score:** 8

**TL;DR:** The paper introduces a new task, PAN-DG, focused on generating negotiation dialogues that account for personality attributes, supported by a novel dataset, PACT.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve conflict resolution in negotiation dialogue systems by integrating argumentation mechanisms and personality attributes.

**Method:** The authors propose a new task, PAN-DG, and introduce the PACT dataset generated using LLMs, which simulates negotiation scenarios with distinct personality profiles.

**Key Contributions:**

	1. Introduction of the PAN-DG task for personality-driven negotiation dialogue generation.
	2. Development of the PACT dataset with distinct personality profiles for tourism sector negotiations.
	3. Demonstration of the effectiveness of fine-tuned LLMs in generating rational responses based on personality attributes.

**Result:** Evaluations show that dialogues in the PACT dataset are of high quality, and fine-tuned LLMs outperform pre-trained models in generating personality-driven responses during negotiations.

**Limitations:** 

**Conclusion:** PACT significantly enhances personalization and reasoning in negotiation dialogue systems and sets a foundation for future research in this area.

**Abstract:** Integrating argumentation mechanisms into negotiation dialogue systems improves conflict resolution through exchanges of arguments and critiques. Moreover, incorporating personality attributes enhances adaptability by aligning interactions with individuals' preferences and styles. To advance these capabilities in negotiation dialogue systems, we propose a novel Personality-driven Argumentation-based Negotiation Dialogue Generation (PAN-DG) task. To support this task, we introduce PACT, a dataset of Personality-driven Argumentation-based negotiation Conversations for Tourism sector. This dataset, generated using Large Language Models (LLMs), features three distinct personality profiles, viz. Argumentation Profile, Preference Profile, and Buying Style Profile to simulate a variety of negotiation scenarios involving diverse personalities. Thorough automatic and manual evaluations indicate that the dataset comprises high-quality dialogues. Further, we conduct comparative experiments between pre-trained and fine-tuned LLMs for the PAN-DG task. Multi-dimensional evaluation demonstrates that the fine-tuned LLMs effectively generate personality-driven rational responses during negotiations. This underscores the effectiveness of PACT in enhancing personalization and reasoning capabilities in negotiation dialogue systems, thereby establishing a foundation for future research in this domain.

</details>


### [85] [Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification](https://arxiv.org/abs/2509.11127)

*Hongxu Zhou, Hylke Westerdijk, Khondoker Ittehadul Islam*

**Main category:** cs.CL

**Keywords:** large language models, fallacy classification, context, emotional tone, political debate

**Relevance Score:** 8

**TL;DR:** This study explores the impact of context and emotional tone metadata on LLM reasoning in fallacy classification tasks, revealing that basic prompts often outperform enhanced ones due to attention dilution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the influence of context and emotional tone metadata on the performance of large language models in classifying fallacies in political debates.

**Method:** The study uses U.S. presidential debate data to classify six types of fallacies using prompting strategies applied to the Qwen-3 model. It introduces Chain-of-Thought frameworks like Pragma-Dialectics and the Periodic Table of Arguments and evaluates their effectiveness against baseline prompts.

**Key Contributions:**

	1. Introduced Chain-of-Thought frameworks for fallacy classification
	2. Evaluated the effect of emotional tone on LLM performance
	3. Provided insights into input context impacts on reasoning quality

**Result:** The results indicate that theoretical prompting can improve interpretability and accuracy in some cases, but adding context and emotional tone metadata generally reduced performance, biasing the model towards misclassifications such as 'Appeal to Emotion'.

**Limitations:** Results are specific to the Qwen-3 model and the dataset used, which may limit generalizability.

**Conclusion:** Overall, basic prompts often yield better performance than enhanced prompts, suggesting that added inputs may distract and dilute attention, leading to poorer classification outcomes in LLMs.

**Abstract:** This study investigates how context and emotional tone metadata influence large language model (LLM) reasoning and performance in fallacy classification tasks, particularly within political debate settings. Using data from U.S. presidential debates, we classify six fallacy types through various prompting strategies applied to the Qwen-3 (8B) model. We introduce two theoretically grounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table of Arguments, and evaluate their effectiveness against a baseline prompt under three input settings: text-only, text with context, and text with both context and audio-based emotional tone metadata. Results suggest that while theoretical prompting can improve interpretability and, in some cases, accuracy, the addition of context and especially emotional tone metadata often leads to lowered performance. Emotional tone metadata biases the model toward labeling statements as \textit{Appeal to Emotion}, worsening logical reasoning. Overall, basic prompts often outperformed enhanced ones, suggesting that attention dilution from added inputs may worsen rather than improve fallacy classification in LLMs.

</details>


### [86] [When Smiley Turns Hostile: Interpreting How Emojis Trigger LLMs' Toxicity](https://arxiv.org/abs/2509.11141)

*Shiyao Cui, Xijia Feng, Yingkang Wang, Junxiao Yang, Zhexin Zhang, Biplab Sikdar, Hongning Wang, Han Qiu, Minlie Huang*

**Main category:** cs.CL

**Keywords:** emojis, large language models, toxicity generation, digital communication, semantic cognition

**Relevance Score:** 8

**TL;DR:** The paper investigates how emojis induce toxicity in large language models (LLMs) and explores the mechanisms behind this phenomenon.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the connection between emojis and toxicity generation in LLMs, given that emojis are often misunderstood and can trigger negative responses.

**Method:** Automated construction of prompts with emojis was used to assess toxicity generation in LLMs across five languages and seven models. Interpretations of model behavior were analyzed to understand emoji effects.

**Key Contributions:**

	1. Automated methodology for studying emoji impact on LLM toxicity.
	2. Comprehensive experimental results across multiple languages and models.
	3. Insights into the role of emojis in bypassing safety mechanisms.

**Result:** Emoji-laden prompts significantly induced toxic content generation in the tested LLMs. Model-level analysis revealed that emojis served as a channel to bypass safety measures.

**Limitations:** The study primarily focused on a limited set of LLMs and languages; further exploration is needed for a broader analysis.

**Conclusion:** The findings highlight a critical vulnerability in LLMs regarding emoji usage, necessitating improved safety mechanisms that account for non-verbal cues.

**Abstract:** Emojis are globally used non-verbal cues in digital communication, and extensive research has examined how large language models (LLMs) understand and utilize emojis across contexts. While usually associated with friendliness or playfulness, it is observed that emojis may trigger toxic content generation in LLMs. Motivated by such a observation, we aim to investigate: (1) whether emojis can clearly enhance the toxicity generation in LLMs and (2) how to interpret this phenomenon. We begin with a comprehensive exploration of emoji-triggered LLM toxicity generation by automating the construction of prompts with emojis to subtly express toxic intent. Experiments across 5 mainstream languages on 7 famous LLMs along with jailbreak tasks demonstrate that prompts with emojis could easily induce toxicity generation. To understand this phenomenon, we conduct model-level interpretations spanning semantic cognition, sequence generation and tokenization, suggesting that emojis can act as a heterogeneous semantic channel to bypass the safety mechanisms. To pursue deeper insights, we further probe the pre-training corpus and uncover potential correlation between the emoji-related data polution with the toxicity generation behaviors. Supplementary materials provide our implementation code and data. (Warning: This paper contains potentially sensitive contents)

</details>


### [87] [Text2Mem: A Unified Memory Operation Language for Memory Operating System](https://arxiv.org/abs/2509.11145)

*Felix Wang, Boyu Chen, Kerun Xu, Bo Tang, Feiyu Xiong, Zhiyu Li*

**Main category:** cs.CL

**Keywords:** memory operations, language models, HCI, JSON schema, benchmarking

**Relevance Score:** 8

**TL;DR:** Text2Mem is a unified memory operation language for large language model agents, offering a standardized way to execute memory commands and improve interaction reliability.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in existing memory frameworks for large language models, specifically the lack of higher order memory operations and a formal specification for memory commands.

**Method:** Text2Mem introduces an expressive set of memory operations defined in a JSON schema, ensuring safety and correctness through validation, while supporting backend execution through adapters.

**Key Contributions:**

	1. Standardized memory operation language for LLM agents.
	2. Formal JSON schema for memory commands ensuring safety and correctness.
	3. Planned benchmark (Text2Mem Bench) for evaluating memory control systems.

**Result:** Text2Mem ensures reliable interaction by providing structured memory commands that facilitate deterministic behavior across different memory systems.

**Limitations:** 

**Conclusion:** Text2Mem and its accompanying benchmark offer a foundational approach to improving memory control in language model agents, promoting systematic evaluation and safety in execution.

**Abstract:** Large language model agents increasingly depend on memory to sustain long horizon interaction, but existing frameworks remain limited. Most expose only a few basic primitives such as encode, retrieve, and delete, while higher order operations like merge, promote, demote, split, lock, and expire are missing or inconsistently supported. Moreover, there is no formal and executable specification for memory commands, leaving scope and lifecycle rules implicit and causing unpredictable behavior across systems. We introduce Text2Mem, a unified memory operation language that provides a standardized pathway from natural language to reliable execution. Text2Mem defines a compact yet expressive operation set aligned with encoding, storage, and retrieval. Each instruction is represented as a JSON based schema instance with required fields and semantic invariants, which a parser transforms into typed operation objects with normalized parameters. A validator ensures correctness before execution, while adapters map typed objects either to a SQL prototype backend or to real memory frameworks. Model based services such as embeddings or summarization are integrated when required. All results are returned through a unified execution contract. This design ensures safety, determinism, and portability across heterogeneous backends. We also outline Text2Mem Bench, a planned benchmark that separates schema generation from backend execution to enable systematic evaluation. Together, these components establish the first standardized foundation for memory control in agents.

</details>


### [88] [Differentially-private text generation degrades output language quality](https://arxiv.org/abs/2509.11176)

*Erion Çano, Ivan Habernal*

**Main category:** cs.CL

**Keywords:** differential privacy, large language models, text generation, synthetic data, classification tasks

**Relevance Score:** 9

**TL;DR:** This paper investigates the effects of differential privacy (DP) on the quality and utility of text generated by fine-tuned large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the impact of differential privacy tuning on the text quality and utility of outputs from LLMs, especially in relation to user privacy.

**Method:** The authors tuned five LLMs on three different corpora across four levels of privacy, then evaluated the outputs based on length, grammatical correctness, and lexical diversity, while also assessing utility in classification tasks like book genre recognition and cause of death recognition.

**Key Contributions:**

	1. Demonstrated how differential privacy affects LLM text outputs
	2. Quantified reductions in text quality metrics (length, grammatical correctness, diversity)
	3. Assessed impacts on downstream classification task accuracy

**Result:** The study found that stronger privacy constraints resulted in shorter texts (by at least 77%), lower grammatical correctness (decreasing by at least 9%), and reduced lexical diversity (by at least 10% in bi-gram diversity). Additionally, accuracy in downstream classification tasks decreased, potentially undermining the usefulness of the synthetic data.

**Limitations:** The study focused on specific LLMs and datasets; results might vary across different models or real-world applications.

**Conclusion:** The findings suggest that while differential privacy can enhance user privacy, it significantly degrades the quality and utility of the generated text, which raises concerns about the effectiveness of using these synthetic data in practical applications.

**Abstract:** Ensuring user privacy by synthesizing data from large language models (LLMs) tuned under differential privacy (DP) has become popular recently. However, the impact of DP fine-tuned LLMs on the quality of the language and the utility of the texts they produce has not been investigated. In this work, we tune five LLMs with three corpora under four levels of privacy and assess the length, the grammatical correctness, and the lexical diversity of the text outputs they produce. We also probe the utility of the synthetic outputs in downstream classification tasks such as book genre recognition based on book descriptions and cause of death recognition based on verbal autopsies. The results indicate that LLMs tuned under stronger privacy constrains produce texts that are shorter by at least 77 %, that are less grammatically correct by at least 9 %, and are less diverse by at least 10 % in bi-gram diversity. Furthermore, the accuracy they reach in downstream classification tasks decreases, which might be detrimental to the usefulness of the generated synthetic data.

</details>


### [89] [Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs](https://arxiv.org/abs/2509.11177)

*Hang Guo, Yawei Li, Luca Benini*

**Main category:** cs.CL

**Keywords:** Large Language Model, compression, quantization, pruning, sparsity

**Relevance Score:** 7

**TL;DR:** The paper introduces Optimal Brain Restoration (OBR), a framework for combining quantization and sparsity in Large Language Model (LLM) compression.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of single compression techniques for LLMs as quantization and pruning approaches reach their limits.

**Method:** The OBR framework aligns quantization and sparsity by error compensation, using a second-order Hessian objective reformed into a tractable problem.

**Key Contributions:**

	1. Introduction of Optimal Brain Restoration (OBR) framework
	2. Combination of quantization and pruning techniques
	3. Achieving significant speedup and memory reduction for LLMs

**Result:** Experiments demonstrate that OBR allows for W4A4KV4 quantization with 50% sparsity, achieving up to 4.72x speedup and 6.4x memory reduction versus FP16-dense models.

**Limitations:** 

**Conclusion:** OBR proves to be an effective strategy for reducing model size and improving performance in compressed LLMs.

**Abstract:** Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as these techniques gradually approach their respective limits, relying on a single method for further compression has become increasingly challenging. In this work, we explore an alternative solution by combining quantization and sparsity. This joint approach, though promising, introduces new difficulties due to the inherently conflicting requirements on weight distributions: quantization favors compact ranges, while pruning benefits from high variance. To attack this problem, we propose Optimal Brain Restoration (OBR), a general and training-free framework that aligns pruning and quantization by error compensation between both. OBR minimizes performance degradation on downstream tasks by building on a second-order Hessian objective, which is then reformulated into a tractable problem through surrogate approximation and ultimately reaches a closed-form solution via group error compensation. Experiments show that OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory reduction compared to the FP16-dense baseline.

</details>


### [90] [RanAT4BIE: Random Adversarial Training for Biomedical Information Extraction](https://arxiv.org/abs/2509.11191)

*Jian Chen, Shengyi Lv, Leilei Su*

**Main category:** cs.CL

**Keywords:** random adversarial training, biomedical information extraction, PubMedBERT, NLP, efficiency

**Relevance Score:** 8

**TL;DR:** Random Adversarial Training (RAT) enhances biomedical information extraction while reducing computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance on biomedical information extraction tasks while addressing the computational overhead of conventional adversarial training.

**Method:** RAT integrates random sampling with adversarial training techniques in the context of PubMedBERT for BioIE tasks.

**Key Contributions:**

	1. Introduction of a novel framework (RAT) for BioIE tasks.
	2. Demonstration of improved efficiency and effectiveness of adversarial training.
	3. Comprehensive evaluations proving RAT's superiority over existing methods.

**Result:** RAT outperforms baseline models in BioIE tasks with improved model generalization and robustness at lower computational costs.

**Limitations:** Computational overhead remains a factor despite efficiency improvements; specific implementation details may affect generalizability.

**Conclusion:** RAT serves as an efficient alternative, offering enhanced performance for biomedical NLP tasks.

**Abstract:** We introduce random adversarial training (RAT), a novel framework successfully applied to biomedical information extraction (BioIE) tasks. Building on PubMedBERT as the foundational architecture, our study first validates the effectiveness of conventional adversarial training in enhancing pre-trained language models' performance on BioIE tasks. While adversarial training yields significant improvements across various performance metrics, it also introduces considerable computational overhead. To address this limitation, we propose RAT as an efficiency solution for biomedical information extraction. This framework strategically integrates random sampling mechanisms with adversarial training principles, achieving dual objectives: enhanced model generalization and robustness while significantly reducing computational costs. Through comprehensive evaluations, RAT demonstrates superior performance compared to baseline models in BioIE tasks. The results highlight RAT's potential as a transformative framework for biomedical natural language processing, offering a balanced solution to the model performance and computational efficiency.

</details>


### [91] [The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences](https://arxiv.org/abs/2509.11295)

*Valentin Romanov, Steven A Niederer*

**Main category:** cs.CL

**Keywords:** prompt engineering, large language models, life sciences, efficiency, data processing

**Relevance Score:** 8

**TL;DR:** The paper presents a streamlined approach to prompt engineering for Large Language Models (LLMs), focusing on six core techniques relevant to life sciences workflows.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of generating reliable responses from LLMs in life sciences, minimizing cognitive load.

**Method:** The authors distilled a comprehensive report into six key prompt engineering techniques filled with actionable recommendations and guidelines.

**Key Contributions:**

	1. Identification of six core prompt engineering techniques
	2. Actionable guidelines for effective prompt construction
	3. Analysis of limitations across various platforms for LLM use in life sciences

**Result:** The exploration of techniques like zero-shot, few-shot approaches, and self-criticism reveals significant potential for improving the quality of research outputs in life sciences.

**Limitations:** Focus is primarily on life sciences, which may limit generalizability to other fields; might not address all advanced prompt engineering scenarios.

**Conclusion:** The paper promotes systematic prompt engineering as an aid to enhance data processing and editing practices rather than replace them.

**Abstract:** Developing effective prompts demands significant cognitive investment to generate reliable, high-quality responses from Large Language Models (LLMs). By deploying case-specific prompt engineering techniques that streamline frequently performed life sciences workflows, researchers could achieve substantial efficiency gains that far exceed the initial time investment required to master these techniques. The Prompt Report published in 2025 outlined 58 different text-based prompt engineering techniques, highlighting the numerous ways prompts could be constructed. To provide actionable guidelines and reduce the friction of navigating these various approaches, we distil this report to focus on 6 core techniques: zero-shot, few-shot approaches, thought generation, ensembling, self-criticism, and decomposition. We breakdown the significance of each approach and ground it in use cases relevant to life sciences, from literature summarization and data extraction to editorial tasks. We provide detailed recommendations for how prompts should and shouldn't be structured, addressing common pitfalls including multi-turn conversation degradation, hallucinations, and distinctions between reasoning and non-reasoning models. We examine context window limitations, agentic tools like Claude Code, while analyzing the effectiveness of Deep Research tools across OpenAI, Google, Anthropic and Perplexity platforms, discussing current limitations. We demonstrate how prompt engineering can augment rather than replace existing established individual practices around data processing and document editing. Our aim is to provide actionable guidance on core prompt engineering principles, and to facilitate the transition from opportunistic prompting to an effective, low-friction systematic practice that contributes to higher quality research.

</details>


### [92] [Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context](https://arxiv.org/abs/2509.11303)

*Dasol Choi, Jungwhan Kim, Guijin Son*

**Main category:** cs.CL

**Keywords:** commonsense reasoning, Korean language, cultural diversity, dataset, language models

**Relevance Score:** 6

**TL;DR:** Ko-PIQA is a Korean commonsense reasoning dataset addressing cultural diversity in physical commonsense scenarios, featuring 441 high-quality questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of cultural diversity in existing physical commonsense reasoning datasets, particularly for the Korean language.

**Method:** A multi-stage filtering process from 3.01 million web-crawled questions led to the selection and refinement of 441 high-quality question-answer pairs using language models and human validation.

**Key Contributions:**

	1. Introduction of a Korean commonsense reasoning dataset (Ko-PIQA)
	2. Incorporation of culturally specific elements in reasoning tasks
	3. Establishment of a new benchmark for evaluating language models in the Korean language

**Result:** Evaluation of seven language models on Ko-PIQA showed accuracy ranging from 59.86% to 83.22%, indicating significant challenges in handling culturally specific scenarios.

**Limitations:** The dataset is limited to cultural contexts pertinent to Korean culture and may not generalize to other cultures.

**Conclusion:** Ko-PIQA serves as a benchmark for Korean language models and promotes a more inclusive approach in commonsense reasoning research.

**Abstract:** Physical commonsense reasoning datasets like PIQA are predominantly English-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean physical commonsense reasoning dataset that incorporates cultural context. Starting from 3.01 million web-crawled questions, we employed a multi-stage filtering approach using three language models to identify 11,553 PIQA-style questions. Through GPT-4o refinement and human validation, we obtained 441 high-quality question-answer pairs. A key feature of Ko-PIQA is its cultural grounding: 19.7\% of questions contain culturally specific elements like traditional Korean foods (kimchi), clothing (hanbok), and specialized appliances (kimchi refrigerators) that require culturally-aware reasoning beyond direct translation. We evaluate seven language models on Ko-PIQA, with the best model achieving 83.22\% accuracy while the weakest reaches only 59.86\%, demonstrating significant room for improvement. Models particularly struggle with culturally specific scenarios, highlighting the importance of culturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean language models and a foundation for more inclusive commonsense reasoning research. The dataset and code will be publicly available.

</details>


### [93] [!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning](https://arxiv.org/abs/2509.11365)

*Mohamed Tarek, Seif Ahmed, Mohamed Basem*

**Main category:** cs.CL

**Keywords:** Arabic Health QA, Gemini Model, Question Answering

**Relevance Score:** 7

**TL;DR:** This paper discusses the authors' systems for a health-related QA task in Arabic, achieving 2nd place in both multiple-choice and open-ended formats.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve question answering in Arabic clinical contexts and provide effective responses to health-related queries.

**Method:** Utilizes the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and an ensemble of prompt configurations for Sub-Task 1; uses role-playing and post-processing techniques for Sub-Task 2.

**Key Contributions:**

	1. Implementation of a few-shot prompting methodology using Gemini 2.5 Flash model.
	2. Development of ensemble prompt configurations to enhance multiple-choice QA accuracy.
	3. Integration of role-playing elements for generating concise responses in open-ended QA.

**Result:** Secured 2nd place in both Sub-Task 1 (MCQA) and Sub-Task 2 (open-ended QA) of the AraHealthQA-2025 shared task.

**Limitations:** 

**Conclusion:** The approach demonstrates the potential of advanced prompting strategies and model fine-tuning in Arabic health-related question answering.

**Abstract:** We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of the AraHealthQA-2025 shared task, where our methodology secured 2nd place in both Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended question answering) in Arabic clinical contexts. For Sub-Task 1, we leverage the Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and an ensemble of three prompt configurations to improve classification accuracy on standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ a unified prompt with the same model, incorporating role-playing as an Arabic medical expert, few-shot examples, and post-processing to generate concise responses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased variants.

</details>


### [94] [Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity](https://arxiv.org/abs/2509.11374)

*Bowen Jing, Yang Cui, Tianpeng Huang*

**Main category:** cs.CL

**Keywords:** relation extraction, transformer models, deep learning, large language models, information extraction

**Relevance Score:** 8

**TL;DR:** This paper compares deep supervised learning approaches for relation extraction, highlighting the superior performance of transformer-based models over non-transformer models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Relation extraction is crucial for converting unstructured text into structured data, especially in the context of large language models.

**Method:** The study systematically compares various non-transformer architectures (PA-LSTM, C-GCN, AGGCN) against transformer architectures (BERT, RoBERTa, R-BERT) using traditional metrics and varying training conditions on datasets TACRED, TACREV, and RE-TACRED.

**Key Contributions:**

	1. Systematic comparison of transformer and non-transformer models for relation extraction
	2. Demonstrates the performance gap between model types on key datasets
	3. Reviews the impact of large language models on relation extraction methodology

**Result:** Transformer-based models achieved micro F1 scores of 80-90%, significantly outperforming non-transformer models which scored 64-67%.

**Limitations:** 

**Conclusion:** The research indicates that transformer architectures are more effective for relation extraction tasks, and highlights the evolving role of large language models in this field.

**Abstract:** In the era of large language model, relation extraction (RE) plays an important role in information extraction through the transformation of unstructured raw text into structured data (Wadhwa et al., 2023). In this paper, we systematically compare the performance of deep supervised learning approaches without transformers and those with transformers. We used a series of non-transformer architectures such as PA-LSTM(Zhang et al., 2017), C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019), and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu and He, 2019). Our comparison included traditional metrics like micro F1, as well as evaluations in different scenarios, varying sentence lengths, and different percentages of the dataset for training. Our experiments were conducted on TACRED, TACREV, and RE-TACRED. The results show that transformer-based models outperform non-transformer models, achieving micro F1 scores of 80-90% compared to 64-67% for non-transformer models. Additionally, we briefly review the research journey in supervised relation classification and discuss the role and current status of large language models (LLMs) in relation extraction.

</details>


### [95] [Continually Adding New Languages to Multilingual Language Models](https://arxiv.org/abs/2509.11414)

*Abraham Toluwase Owodunni, Sachin Kumar*

**Main category:** cs.CL

**Keywords:** multilingual models, Low-Rank Adapters, catastrophic forgetting, language adaptation, instruction following

**Relevance Score:** 8

**TL;DR:** This paper introduces Layer-Selective LoRA (LayRA) for multilingual models to effectively add new languages without catastrophic forgetting, using only target language pretraining data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of adding new languages to multilingual models without needing to retrain them from scratch or suffering from catastrophic forgetting.

**Method:** The authors propose Layer-Selective LoRA (LayRA), which selectively applies Low-Rank Adapters (LoRA) to specific model layers while keeping others frozen. The approach leverages the structure of multilingual models and emphasizes critical layer utilization for effective language support.

**Key Contributions:**

	1. Introduction of Layer-Selective LoRA for multilingual models.
	2. Improved preservation of model capabilities across existing languages while learning new languages.
	3. Demonstration of strong instruction following abilities using model arithmetic.

**Result:** LayRA shows the best performance in balancing the retention of existing language capabilities while effectively learning new languages when compared to existing methods like LoRA.

**Limitations:** 

**Conclusion:** LayRA allows multilingual models to adapt to new languages efficiently, enhancing their instruction-following capabilities without needing instruction tuning data in target languages.

**Abstract:** Multilingual language models are trained on a fixed set of languages, and to support new languages, the models need to be retrained from scratch. This is an expensive endeavor and is often infeasible, as model developers tend not to release their pre-training data. Naive approaches, such as continued pretraining, suffer from catastrophic forgetting; however, mitigation strategies like experience replay cannot be applied due to the lack of original pretraining data. In this work, we investigate the problem of continually adding new languages to a multilingual model, assuming access to pretraining data in only the target languages. We explore multiple approaches to address this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank Adapters (LoRA) to selected initial and final layers while keeping the rest of the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting, and (2) multilingual models encode inputs in the source language in the initial layers, reason in English in intermediate layers, and translate back to the source language in final layers. We experiment with adding multiple combinations of Galician, Swahili, and Urdu to pretrained language models and evaluate each method on diverse multilingual tasks. We find that LayRA provides the overall best tradeoff between preserving models' capabilities in previously supported languages, while being competitive with existing approaches such as LoRA in learning new languages. We also demonstrate that using model arithmetic, the adapted models can be equipped with strong instruction following abilities without access to any instruction tuning data in the target languages.

</details>


### [96] [A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm](https://arxiv.org/abs/2509.11443)

*Gaurab Chhetri, Darrell Anderson, Boniphace Kutela, Subasish Das*

**Main category:** cs.CL

**Keywords:** Sentiment Analysis, 15-minute city, Transformer Models, Urban Planning, Machine Learning

**Relevance Score:** 6

**TL;DR:** The study conducts a multi-platform sentiment analysis on the 15-minute city concept, utilizing compressed transformer models for annotation and benchmarking various sentiment classification models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze public opinion on the 15-minute city concept across different social media and news platforms, providing insights into sentiment dynamics within urban planning.

**Method:** The analysis employs compressed transformer models like Llama-3-8B for sentiment annotation across Twitter, Reddit, and news articles, utilizing stratified 5-fold cross-validation to benchmark five models (DistilRoBERTa, DistilBERT, MiniLM, ELECTRA, TinyBERT) based on F1-score, AUC, and training time.

**Key Contributions:**

	1. First multi-platform sentiment analysis of the 15-minute city concept
	2. Benchmarking of sentiment analysis models across heterogeneous text domains
	3. Identification of platform-specific challenges in sentiment classification

**Result:** DistilRoBERTa demonstrated the highest F1-score at 0.8292, TinyBERT showed superior efficiency, and MiniLM excelled in cross-platform consistency. The study highlighted the impact of data platforms on performance metrics and identified challenges specific to each platform.

**Limitations:** Class imbalance in news data inflated performance metrics; summarization loss was observed in Reddit; moderate challenges were noted in Twitter data.

**Conclusion:** The findings indicate competitive performance from compressed models, suggesting that larger models may not be necessary for effective sentiment analysis. The study also outlines platform-specific challenges and offers future directions for scalable sentiment classification in urban planning.

**Abstract:** This study presents the first multi-platform sentiment analysis of public opinion on the 15-minute city concept across Twitter, Reddit, and news media. Using compressed transformer models and Llama-3-8B for annotation, we classify sentiment across heterogeneous text domains. Our pipeline handles long-form and short-form text, supports consistent annotation, and enables reproducible evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM, ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1 (0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform consistency. Results show News data yields inflated performance due to class imbalance, Reddit suffers from summarization loss, and Twitter offers moderate challenge. Compressed models perform competitively, challenging assumptions that larger models are necessary. We identify platform-specific trade-offs and propose directions for scalable, real-world sentiment classification in urban planning discourse.

</details>


### [97] [CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media](https://arxiv.org/abs/2509.11444)

*Gaurab Chhetri, Anandi Dutta, Subasish Das*

**Main category:** cs.CL

**Keywords:** CognitiveSky, sentiment analysis, decentralized social media, transformer models, computational social science

**Relevance Score:** 6

**TL;DR:** CognitiveSky is an open-source framework for analyzing sentiment, emotion, and narratives in decentralized social media, specifically designed for Bluesky.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges and opportunities of analyzing public discourse on decentralized social media platforms.

**Method:** CognitiveSky utilizes Bluesky's API to ingest data and applies transformer-based models to generate structured outputs for sentiment and emotion analysis, which are visualized in a dynamic dashboard.

**Key Contributions:**

	1. Introduces a scalable framework for real-time public discourse analysis on decentralized platforms.
	2. Demonstrates the application of transformer-based models to large-scale user content.
	3. Provides a dynamic visualization dashboard for emotions, activities, and topics.

**Result:** CognitiveSky effectively monitors mental health discourse and shows promise for use in other domains like disinformation detection and crisis response.

**Limitations:** The study is limited to the context of Bluesky and may not capture nuances across other decentralized platforms.

**Conclusion:** CognitiveSky represents a transparent and extensible tool for computational social science, enabling wide applications in analyzing decentralized networks.

**Abstract:** The emergence of decentralized social media platforms presents new opportunities and challenges for real-time analysis of public discourse. This study introduces CognitiveSky, an open-source and scalable framework designed for sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter or X.com alternative. By ingesting data through Bluesky's Application Programming Interface (API), CognitiveSky applies transformer-based models to annotate large-scale user-generated content and produces structured and analyzable outputs. These summaries drive a dynamic dashboard that visualizes evolving patterns in emotion, activity, and conversation topics. Built entirely on free-tier infrastructure, CognitiveSky achieves both low operational cost and high accessibility. While demonstrated here for monitoring mental health discourse, its modular design enables applications across domains such as disinformation detection, crisis response, and civic sentiment analysis. By bridging large language models with decentralized networks, CognitiveSky offers a transparent, extensible tool for computational social science in an era of shifting digital ecosystems.

</details>


### [98] [CEMTM: Contextual Embedding-based Multimodal Topic Modeling](https://arxiv.org/abs/2509.11465)

*Amirhossein Abaskohi, Raymond Li, Chuyuan Li, Shafiq Joty, Giuseppe Carenini*

**Main category:** cs.CL

**Keywords:** multimodal topic modeling, large vision language models, contextual embeddings

**Relevance Score:** 7

**TL;DR:** Introduction of CEMTM, a context-enhanced multimodal topic model for interpreting short and long documents with text and images.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a model that can infer coherent and interpretable topic structures from multimodal documents more effectively.

**Method:** CEMTM uses fine-tuned large vision language models to create contextual embeddings and a distributional attention mechanism for topic inference; it aligns topic-based representations with document embeddings.

**Key Contributions:**

	1. Introduction of a new multimodal topic model (CEMTM)
	2. Ability to process multiple images per document without repeated encoding
	3. Demonstrated effectiveness in few-shot retrieval and capturing visually grounded semantics.

**Result:** CEMTM outperforms existing unimodal and multimodal approaches on six benchmarks, achieving an average LLM score of 2.61, demonstrating effectiveness in few-shot retrieval and visually grounded semantics.

**Limitations:** 

**Conclusion:** CEMTM enhances topic modeling by integrating text and images without repeated encoding, providing interpretability while achieving high performance across benchmarks.

**Abstract:** We introduce CEMTM, a context-enhanced multimodal topic model designed to infer coherent and interpretable topic structures from both short and long documents containing text and images. CEMTM builds on fine-tuned large vision language models (LVLMs) to obtain contextualized embeddings, and employs a distributional attention mechanism to weight token-level contributions to topic inference. A reconstruction objective aligns topic-based representations with the document embedding, encouraging semantic consistency across modalities. Unlike existing approaches, CEMTM can process multiple images per document without repeated encoding and maintains interpretability through explicit word-topic and document-topic distributions. Extensive experiments on six multimodal benchmarks show that CEMTM consistently outperforms unimodal and multimodal baselines, achieving a remarkable average LLM score of 2.61. Further analysis shows its effectiveness in downstream few-shot retrieval and its ability to capture visually grounded semantics in complex domains such as scientific articles.

</details>


### [99] [Improving LLMs' Learning for Coreference Resolution](https://arxiv.org/abs/2509.11466)

*Yujian Gan, Yuan Liang, Yanni Lin, Juntao Yu, Massimo Poesio*

**Main category:** cs.CL

**Keywords:** Coreference Resolution, NLP, Large Language Models, Hallucination Reduction, Document Generation

**Relevance Score:** 8

**TL;DR:** This paper proposes two novel techniques for improving coreference resolution in LLMs: Reversed Training with Joint Inference and Iterative Document Generation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Coreference Resolution (CR) is important for NLP tasks, but current LLMs face challenges such as hallucination and under-performance.

**Method:** The authors investigate existing LLM-based approaches like QA Template and Document Template methods, followed by their proposals of Reversed Training and Iterative Document Generation to enhance CR.

**Key Contributions:**

	1. Proposed Reversed Training with Joint Inference method for CR
	2. Introduced Iterative Document Generation technique
	3. Demonstrated effectiveness in reducing hallucinations and improving CR

**Result:** Reversed Training enhances the QA Template method, while Iterative Document Generation reduces hallucinations and improves coreference resolution.

**Limitations:** 

**Conclusion:** The integration of the proposed methods provides a robust solution to coreference resolution in LLM applications.

**Abstract:** Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs struggle with hallucination and under-performance. In this paper, we investigate the limitations of existing LLM-based approaches to CR-specifically the Question-Answering (QA) Template and Document Template methods and propose two novel techniques: Reversed Training with Joint Inference and Iterative Document Generation. Our experiments show that Reversed Training improves the QA Template method, while Iterative Document Generation eliminates hallucinations in the generated source text and boosts coreference resolution. Integrating these methods and techniques offers an effective and robust solution to LLM-based coreference resolution.

</details>


### [100] [ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims](https://arxiv.org/abs/2509.11492)

*Anirban Saha Anik, Md Fahimul Kabir Chowdhury, Andrew Wyckoff, Sagnik Ray Choudhury*

**Main category:** cs.CL

**Keywords:** LLM, Numerical verification, Temporal claims, LoRA, Evidence selection

**Relevance Score:** 7

**TL;DR:** This paper discusses a system for verifying numerical and temporal claims using LLMs and various evidence selection strategies, achieving significant findings in the CLEF 2025 CheckThat! Lab.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the verification of numerical and temporal claims through efficient evidence retrieval and LLM adaptation.

**Method:** The system employs zero-shot prompting with instruction-tuned LLMs and supervised fine-tuning using LoRA. It investigates evidence selection strategies such as full-document input and top-k sentence filtering using BM25 and MiniLM.

**Key Contributions:**

	1. Exploration of zero-shot prompting with instruction-tuned LLMs for claim verification.
	2. Use of LoRA for supervised fine-tuning to enhance model performance.
	3. In-depth analysis of evidence selection strategies to improve verification accuracy.

**Result:** The LLaMA model fine-tuned with LoRA performed well on the English validation set, but there was a decline in performance on the test set, indicating a challenge in generalization.

**Limitations:** Notable drop in the model's performance on the test set reveals generalization challenges.

**Conclusion:** The study highlights the critical role of evidence quality and model adaptation in numerical fact verification tasks.

**Abstract:** This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab, which focuses on verifying numerical and temporal claims using retrieved evidence. We explore two complementary approaches: zero-shot prompting with instruction-tuned large language models (LLMs) and supervised fine-tuning using parameter-efficient LoRA. To enhance evidence quality, we investigate several selection strategies, including full-document input and top-k sentence filtering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned with LoRA achieves strong performance on the English validation set. However, a notable drop in the test set highlights a generalization challenge. These findings underscore the importance of evidence granularity and model adaptation for robust numerical fact verification.

</details>


### [101] [AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization](https://arxiv.org/abs/2509.11496)

*Fabrycio Leite Nakano Almada, Kauan Divino Pouso Mariano, Maykon Adriell Dutra, Victor Emanuel da Silva Monteiro, Juliana Resplande Sant'Anna Gomes, Arlindo Rodrigues Galvão Filho, Anderson da Silva Soares*

**Main category:** cs.CL

**Keywords:** claim normalization, fact-checking, large language models, zero-shot learning, multilingual

**Relevance Score:** 8

**TL;DR:** This paper presents a method for claim normalization in fact-checking using fine-tuned Small Language Models and Large Language Model prompting, achieving top results in a multilingual competition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Claim normalization is essential for transforming informal social media posts into concise statements for automated fact-checking, especially across multiple languages.

**Method:** The authors used fine-tuned Small Language Models for supervised languages and applied Large Language Model prompting for zero-shot claim normalization across twenty languages.

**Key Contributions:**

	1. Successful application of LLMs for multilingual claim normalization
	2. High rankings in a competitive task across multiple languages
	3. Public availability of implementation artifacts for replication and further research

**Result:** Achieved podium positions in fifteen out of twenty languages, including second-place rankings in eight languages, demonstrating the effectiveness of the zero-shot strategy.

**Limitations:** 

**Conclusion:** The results highlight the potential of LLM-based approaches in achieving high performance in multilingual claim normalization, and all implementation artifacts are publicly available.

**Abstract:** Claim normalization, the transformation of informal social media posts into concise, self-contained statements, is a crucial step in automated fact-checking pipelines. This paper details our submission to the CLEF-2025 CheckThat! Task~2, which challenges systems to perform claim normalization across twenty languages, divided into thirteen supervised (high-resource) and seven zero-shot (no training data) tracks.   Our approach, leveraging fine-tuned Small Language Models (SLMs) for supervised languages and Large Language Model (LLM) prompting for zero-shot scenarios, achieved podium positions (top three) in fifteen of the twenty languages. Notably, this included second-place rankings in eight languages, five of which were among the seven designated zero-shot languages, underscoring the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our initial development language, our system achieved an average METEOR score of 0.5290, ranking third. All implementation artifacts, including inference, training, evaluation scripts, and prompt configurations, are publicly available at https://github.com/ju-resplande/checkthat2025_normalization.

</details>


### [102] [DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification](https://arxiv.org/abs/2509.11498)

*Zhuoxuan Ju, Jingni Wu, Abhishek Purushothama, Amir Zeldes*

**Main category:** cs.CL

**Keywords:** discourse relation classification, mt5 model, Qwen model

**Relevance Score:** 5

**TL;DR:** This paper details Georgetown University's DeDisCo system for discourse relation classification in the DISRPT 2025 shared task, achieving a macro-accuracy score of 71.28.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to showcase techniques for discourse relation classification, particularly for low-resource languages, by employing advanced models and data augmentation.

**Method:** Two approaches were tested: one using an mt5-based encoder and another using a decoder-based approach with the Qwen model, supplemented with linguistic features.

**Key Contributions:**

	1. Introduction of the DeDisCo system for discourse relation classification.
	2. Performance benchmarking with a macro-accuracy score of 71.28.
	3. Use of augmented datasets for low-resource languages.

**Result:** The system reached a macro-accuracy score of 71.28 on the classification task while allowing for error interpretation and analysis.

**Limitations:** The study is constrained by the focus on discourse relation classification without exploring broader applications or variants.

**Conclusion:** The findings suggest that both model types and the use of augmented datasets can significantly enhance performance in discourse relation classification tasks.

**Abstract:** This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025 shared task on discourse relation classification. We test two approaches, using an mt5-based encoder and a decoder based approach using the openly available Qwen model. We also experiment on training with augmented dataset for low-resource languages using matched data translated automatically from English, as well as using some additional linguistic features inspired by entries in previous editions of the Shared Task. Our system achieves a macro-accuracy score of 71.28, and we provide some interpretation and error analysis for our results.

</details>


### [103] [Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics](https://arxiv.org/abs/2509.11513)

*Zhongyang Hu, Naijie Gu, Xiangzhi Tao, Tianhui Gu, Yibing Zhou*

**Main category:** cs.CL

**Keywords:** lexical substitution, candidate ranking, semantic similarity, attention mechanisms, integrated gradients

**Relevance Score:** 6

**TL;DR:** The paper explores improved methods for ranking candidate words in lexical substitution using attention weights and integrated gradients to better account for contextual influence on target words.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in ranking candidate words for lexical substitution caused by existing methods that focus on either the target position or require extensive parameter tuning.

**Method:** The study investigates two novel approaches: one based on attention weights and the other using integrated gradients, to assess the influence of context on target words and rank candidate substitutions by semantic similarity.

**Key Contributions:**

	1. Introduction of attention weights for ranking in lexical substitution
	2. Utilization of integrated gradients for explainability in candidate ranking
	3. Demonstrated performance improvement on standard datasets (LS07 and SWORDS)

**Result:** Experiments on LS07 and SWORDS datasets showed that the proposed methods enhanced ranking performance compared to traditional approaches.

**Limitations:** 

**Conclusion:** The new methods offer improved insights into the bidirectional influence of context on word substitution, leading to better semantic understanding in lexical tasks.

**Abstract:** A key subtask in lexical substitution is ranking the given candidate words. A common approach is to replace the target word with a candidate in the original sentence and feed the modified sentence into a model to capture semantic differences before and after substitution. However, effectively modeling the bidirectional influence of candidate substitution on both the target word and its context remains challenging. Existing methods often focus solely on semantic changes at the target position or rely on parameter tuning over multiple evaluation metrics, making it difficult to accurately characterize semantic variation. To address this, we investigate two approaches: one based on attention weights and another leveraging the more interpretable integrated gradients method, both designed to measure the influence of context tokens on the target token and to rank candidates by incorporating semantic similarity between the original and substituted sentences. Experiments on the LS07 and SWORDS datasets demonstrate that both approaches improve ranking performance.

</details>


### [104] [LVLMs are Bad at Overhearing Human Referential Communication](https://arxiv.org/abs/2509.11514)

*Zhengxiang Wang, Weiling Li, Panagiotis Kaliosis, Owen Rambow, Susan E. Brennan*

**Main category:** cs.CL

**Keywords:** Large Vision Language Models, collaborative discourse, referring expressions

**Relevance Score:** 6

**TL;DR:** This paper investigates the ability of Large Vision Language Models (LVLMs) to understand and utilize unique referring expressions developed during spontaneous conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LVLMs can effectively carry out real-world tasks by integrating language, vision, and conversational interaction, particularly through the use of referring expressions created during collaborative tasks.

**Method:** Seven state-of-the-art LVLMs were tested as overhearers in a corpus of spontaneous conversations focusing on a collaborative object-matching task.

**Key Contributions:**

	1. Compiled and analyzed a corpus of spontaneous conversations for research on LVLMs.
	2. Provided insights into the limitations of LVLMs in understanding collaborative discourse.
	3. Released corpus and code to promote reproducibility and future research.

**Result:** All LVLMs demonstrated difficulty in consistently improving performance despite overhearing multiple rounds of conversations on the same task.

**Limitations:** LVLMs showed no consistent performance improvement, revealing limitations in understanding in-context language adaptations.

**Conclusion:** Current LVLMs struggle with the task of understanding novel referring expressions, indicating a gap in their capability for real-world applications.

**Abstract:** During spontaneous conversations, speakers collaborate on novel referring expressions, which they can then re-use in subsequent conversations. Understanding such referring expressions is an important ability for an embodied agent, so that it can carry out tasks in the real world. This requires integrating and understanding language, vision, and conversational interaction. We study the capabilities of seven state-of-the-art Large Vision Language Models (LVLMs) as overhearers to a corpus of spontaneous conversations between pairs of human discourse participants engaged in a collaborative object-matching task. We find that such a task remains challenging for current LVLMs and they all fail to show a consistent performance improvement as they overhear more conversations from the same discourse participants repeating the same task for multiple rounds. We release our corpus and code for reproducibility and to facilitate future research.

</details>


### [105] [PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation](https://arxiv.org/abs/2509.11517)

*Rodrigo M. Carrillo-Larco, Jesus Lovón Melgarejo, Manuel Castillo-Cara, Gusseppe Bravo-Rocca*

**Main category:** cs.CL

**Keywords:** medical LLMs, Spanish, fine-tuning, PeruMedQA, health informatics

**Relevance Score:** 9

**TL;DR:** The study explores the performance of medical LLMs on Spanish medical exams in Peru, developing a fine-tuned model to enhance accuracy in answering medical questions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the performance transferability of medical LLMs to Spanish questions in a Latin American context, specifically Peru.

**Method:** Developed the PeruMedQA dataset with 8,380 questions for fine-tuning LLMs and evaluated performance using zero-shot task-specific prompts.

**Key Contributions:**

	1. Creation of the PeruMedQA dataset for medical questions in Spanish
	2. Demonstration of the performance of fine-tuned LLM on medical examinations
	3. Insight into the performance differences among various LLMs based on their parameter sizes

**Result:** medgemma-27b-text-it surpassed all other models, showing over 90% correct answers, while LLMs with less than 10 billion parameters performed poorly with less than 60% accuracy.

**Limitations:** 

**Conclusion:** For medical AI applications in Spanish-speaking countries, the use of medgemma-27b-text-it or a fine-tuned medgemma-4b-it is recommended.

**Abstract:** BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable performance in answering medical examinations. However, the extent to which this high performance is transferable to medical questions in Spanish and from a Latin American country remains unexplored. This knowledge is crucial as LLM-based medical applications gain traction in Latin America. AIMS: to build a dataset of questions from medical examinations taken by Peruvian physicians pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate and compare the performance in terms of accuracy between vanilla LLMs and the fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice question-answering (MCQA) datasets containing 8,380 questions spanning 12 medical domains (2018-2025). We selected eight medical LLMs including medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific prompts to answer the questions appropriately. We employed parameter-efficient fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it utilizing all questions except those from 2025 (test set). RESULTS: medgemma-27b-text-it outperformed all other models, achieving a proportion of correct answers exceeding 90% in several instances. LLMs with <10 billion parameters exhibited <60% of correct answers, while some exams yielded results <50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters across various examinations. CONCLUSIONS: For medical AI application and research that require knowledge bases from Spanish-speaking countries and those exhibiting similar epidemiological profiles to Peru's, interested parties should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

</details>


### [106] [On the Distinctive Co-occurrence Characteristics of Antonymy](https://arxiv.org/abs/2509.11534)

*Zhihan Cao, Hiroaki Yamada, Takenobu Tokunaga*

**Main category:** cs.CL

**Keywords:** antonymy, lexical semantics, co-occurrence, semantic relations, text analysis

**Relevance Score:** 4

**TL;DR:** This paper investigates the unique co-occurrence patterns of antonym pairs compared to other semantic relations in text.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify whether the co-occurrence pattern of antonyms is distinctive by comparing it with other semantic relations.

**Method:** The study utilizes robust co-occurrence metrics across parts of speech to analyze antonymy and three other semantic relations.

**Key Contributions:**

	1. Establishes co-occurrence metrics for antonyms compared to other semantic relations.
	2. Demonstrates that antonyms co-occur in distinct ways not seen with other relationships.
	3. Provides data available online for further research.

**Result:** Antonym pairs exhibit distinctive co-occurrence patterns, showing high strength, preferred linear ordering, and short span co-occurrences.

**Limitations:** 

**Conclusion:** The findings indicate that antonymy is unique compared to other semantic relations, enhancing our understanding of lexical semantics.

**Abstract:** Antonymy has long received particular attention in lexical semantics. Previous studies have shown that antonym pairs frequently co-occur in text, across genres and parts of speech, more often than would be expected by chance. However, whether this co-occurrence pattern is distinctive of antonymy remains unclear, due to a lack of comparison with other semantic relations. This work fills the gap by comparing antonymy with three other relations across parts of speech using robust co-occurrence metrics. We find that antonymy is distinctive in three respects: antonym pairs co-occur with high strength, in a preferred linear order, and within short spans. All results are available online.

</details>


### [107] [Towards Automated Error Discovery: A Study in Conversational AI](https://arxiv.org/abs/2509.10833)

*Dominic Petrak, Thy Thy Tran, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** Automated Error Discovery, LLM, Conversational AI, Error Detection, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** A novel framework for detecting and defining errors in conversational AI, enhancing the performance of LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs in conversational agents produce undesirable behaviors that are difficult to prevent, necessitating improved error detection mechanisms.

**Method:** Introducing a framework called Automated Error Discovery alongside a novel encoder-based approach SEEED, which utilizes enhanced Soft Nearest Neighbor Loss and Label-Based Sample Ranking for better error recognition.

**Key Contributions:**

	1. Introduction of Automated Error Discovery framework.
	2. Development of SEEED for improved error detection in LLMs.
	3. Demonstrated improved accuracy in detecting unknown errors in dialogue datasets.

**Result:** SEEED significantly outperforms baselines like GPT-4o and Phi-4 in identifying unknown errors, improving accuracy by up to 8 points across multiple datasets.

**Limitations:** 

**Conclusion:** The proposed methods improve error detection in LLMs and show strong generalization capabilities for understanding user intent.

**Abstract:** Although LLM-based conversational agents demonstrate strong fluency and coherence, they still produce undesirable behaviors (errors) that are challenging to prevent from reaching users during deployment. Recent research leverages large language models (LLMs) to detect errors and guide response-generation models toward improvement. However, current LLMs struggle to identify errors not explicitly specified in their instructions, such as those arising from updates to the response-generation model or shifts in user behavior. In this work, we introduce Automated Error Discovery, a framework for detecting and defining errors in conversational AI, and propose SEEED (Soft Clustering Extended Encoder-Based Error Detection), as an encoder-based approach to its implementation. We enhance the Soft Nearest Neighbor Loss by amplifying distance weighting for negative samples and introduce Label-Based Sample Ranking to select highly contrastive examples for better representation learning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 -- across multiple error-annotated dialogue datasets, improving the accuracy for detecting unknown errors by up to 8 points and demonstrating strong generalization to unknown intent detection.

</details>


### [108] [HARP: Hallucination Detection via Reasoning Subspace Projection](https://arxiv.org/abs/2509.11536)

*Junjie Hu, Gang Tu, ShengYu Cheng, Jinxin Li, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan*

**Main category:** cs.CL

**Keywords:** hallucination detection, Large Language Models, reasoning subspace

**Relevance Score:** 9

**TL;DR:** HARP is a framework for detecting hallucinations in LLMs by projecting hidden states into reasoning subspaces, achieving state-of-the-art performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of Large Language Models in critical decision-making by addressing hallucination detection challenges.

**Method:** HARP decomposes the hidden state space of LLMs into semantic and reasoning subspaces, uses the Unembedding layer to disentangle them, and applies SVD for basis vector extraction. The hidden states are projected onto the reasoning subspace for hallucination detection.

**Key Contributions:**

	1. Novel hallucination detection framework (HARP)
	2. Decomposing LLM hidden states into semantic and reasoning subspaces
	3. State-of-the-art performance in hallucination detection

**Result:** HARP reduces feature dimensions to about 5% of the original while filtering noise, achieving an AUROC of 92.8% on TriviaQA, exceeding previous detection methods.

**Limitations:** 

**Conclusion:** HARP shows enhanced robustness and performance in hallucination detection, making LLMs more reliable for decision-making tasks.

**Abstract:** Hallucinations in Large Language Models (LLMs) pose a major barrier to their reliable use in critical decision-making. Although existing hallucination detection methods have improved accuracy, they still struggle with disentangling semantic and reasoning information and maintaining robustness. To address these challenges, we propose HARP (Hallucination detection via reasoning subspace projection), a novel hallucination detection framework. HARP establishes that the hidden state space of LLMs can be decomposed into a direct sum of a semantic subspace and a reasoning subspace, where the former encodes linguistic expression and the latter captures internal reasoning processes. Moreover, we demonstrate that the Unembedding layer can disentangle these subspaces, and by applying Singular Value Decomposition (SVD) to its parameters, the basis vectors spanning the semantic and reasoning subspaces are obtained. Finally, HARP projects hidden states onto the basis vectors of the reasoning subspace, and the resulting projections are then used as input features for hallucination detection in LLMs. By using these projections, HARP reduces the dimension of the feature to approximately 5% of the original, filters out most noise, and achieves enhanced robustness. Experiments across multiple datasets show that HARP achieves state-of-the-art hallucination detection performance; in particular, it achieves an AUROC of 92.8% on TriviaQA, outperforming the previous best method by 7.5%.

</details>


### [109] [HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking](https://arxiv.org/abs/2509.11552)

*Wensheng Lu, Keyu Chen, Ruizhi Qiao, Xing Sun*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, document chunking, evaluation benchmark, language models, health informatics

**Relevance Score:** 9

**TL;DR:** This paper introduces HiCBench and HiChunk, tools for evaluating and improving document chunking in Retrieval-Augmented Generation (RAG) systems, enhancing retrieval quality and performance based on fine-tuned LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation tools for document chunking in RAG systems are inadequate due to evidence sparsity, necessitating better methods for assessing chunking quality.

**Method:** The authors propose HiCBench, a benchmark with annotated chunking points and synthesized QA pairs, and HiChunk, a framework utilizing fine-tuned LLMs and Auto-Merge retrieval to improve chunking.

**Key Contributions:**

	1. Introduction of HiCBench for evaluating chunking quality in RAG systems
	2. Development of the HiChunk framework for improved document structuring using LLMs
	3. Demonstration of the efficacy of HiCBench in assessing the impact of chunking methods.

**Result:** Experiments show that HiCBench effectively evaluates different chunking methods and that HiChunk enhances chunking quality without significant time costs, improving overall RAG performance.

**Limitations:** 

**Conclusion:** The introduction of HiCBench and HiChunk presents new methodologies to evaluate and enhance document chunking in RAG systems, impacting retrieval quality positively.

**Abstract:** Retrieval-Augmented Generation (RAG) enhances the response capabilities of language models by integrating external knowledge sources. However, document chunking as an important part of RAG system often lacks effective evaluation tools. This paper first analyzes why existing RAG evaluation benchmarks are inadequate for assessing document chunking quality, specifically due to evidence sparsity. Based on this conclusion, we propose HiCBench, which includes manually annotated multi-level document chunking points, synthesized evidence-dense quetion answer(QA) pairs, and their corresponding evidence sources. Additionally, we introduce the HiChunk framework, a multi-level document structuring framework based on fine-tuned LLMs, combined with the Auto-Merge retrieval algorithm to improve retrieval quality. Experiments demonstrate that HiCBench effectively evaluates the impact of different chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves better chunking quality within reasonable time consumption, thereby enhancing the overall performance of RAG systems.

</details>


### [110] [D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs](https://arxiv.org/abs/2509.11569)

*Yue Ding, Xiaofang Zhu, Tianze Xia, Junfei Wu, Xinlong Chen, Qiang Liu, Liang Wang*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, healthcare, semantic representation, D$^2$HScore

**Relevance Score:** 9

**TL;DR:** The paper presents D$^2$HScore, a framework for detecting hallucinations in large language models by analyzing token representations within and across layers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of non-factual content generation (hallucination) in large language models (LLMs), especially in critical domains like healthcare.

**Method:** The study decomposes hallucination signals into intra-layer dispersion and inter-layer drift, using a training-free and label-free approach that leverages the multi-layer structure of LLMs.

**Key Contributions:**

	1. Introduction of D$^2$HScore for hallucination detection
	2. Decomposition of hallucination signs into intra-layer and inter-layer signals
	3. Validation of the framework across multiple models and benchmarks

**Result:** D$^2$HScore consistently outperforms existing training-free methods across multiple LLMs and benchmarks.

**Limitations:** 

**Conclusion:** D$^2$HScore provides a lightweight and interpretable method for hallucination detection, indicating its potential utility in high-stakes applications.

**Abstract:** Although large Language Models (LLMs) have achieved remarkable success, their practical application is often hindered by the generation of non-factual content, which is called "hallucination". Ensuring the reliability of LLMs' outputs is a critical challenge, particularly in high-stakes domains such as finance, security, and healthcare. In this work, we revisit hallucination detection from the perspective of model architecture and generation dynamics. Leveraging the multi-layer structure and autoregressive decoding process of LLMs, we decompose hallucination signals into two complementary dimensions: the semantic breadth of token representations within each layer, and the semantic depth of core concepts as they evolve across layers. Based on this insight, we propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)}, a training-free and label-free framework that jointly measures: (1) \textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of token representations within each layer; and (2) \textbf{Inter-Layer Drift}, which tracks the progressive transformation of key token representations across layers. To ensure drift reflects the evolution of meaningful semantics rather than noisy or redundant tokens, we guide token selection using attention signals. By capturing both the horizontal and vertical dynamics of representation during inference, D$^2$HScore provides an interpretable and lightweight proxy for hallucination detection. Extensive experiments across five open-source LLMs and five widely used benchmarks demonstrate that D$^2$HScore consistently outperforms existing training-free baselines.

</details>


### [111] [Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges](https://arxiv.org/abs/2509.11570)

*Sampoorna Poria, Xiaolei Huang*

**Main category:** cs.CL

**Keywords:** NLP, South Asian languages, transformer models, data curation, evaluation benchmarks

**Relevance Score:** 7

**TL;DR:** This survey examines the current state and challenges of NLP models for South Asian languages, emphasizing transformer-based models and identifying significant issues such as data gaps and evaluation standards.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the current stage and challenges in NLP for low-resource South Asian languages and inform the community for better model development.

**Method:** Comprehensive survey of studies since 2020 focusing on transformer-based models, examining aspects such as data sources, fine-tuning strategies, and domain applications.

**Key Contributions:**

	1. Comprehensive overview of NLP challenges for South Asian languages
	2. Identification of missing data in critical domains like health
	3. Recommendations for standardized benchmarks and targeted data curation

**Result:** Identified substantial issues like missing health data, code-mixing challenges, and the absence of unified evaluation benchmarks for South Asian languages.

**Limitations:** 

**Conclusion:** The survey calls for targeted data curation and standardized evaluation benchmarks to improve NLP resources for South Asia.

**Abstract:** Rapid developments of large language models have revolutionized many NLP tasks for English data. Unfortunately, the models and their evaluations for low-resource languages are being overlooked, especially for languages in South Asia. Although there are more than 650 languages in South Asia, many of them either have very limited computational resources or are missing from existing language models. Thus, a concrete question to be answered is: Can we assess the current stage and challenges to inform our NLP community and facilitate model developments for South Asian languages? In this survey, we have comprehensively examined current efforts and challenges of NLP models for South Asian languages by retrieving studies since 2020, with a focus on transformer-based models, such as BERT, T5, & GPT. We present advances and gaps across 3 essential aspects: data, models, & tasks, such as available data sources, fine-tuning strategies, & domain applications. Our findings highlight substantial issues, including missing data in critical domains (e.g., health), code-mixing, and lack of standardized evaluation benchmarks. Our survey aims to raise awareness within the NLP community for more targeted data curation, unify benchmarks tailored to cultural and linguistic nuances of South Asia, and encourage an equitable representation of South Asian languages. The complete list of resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.

</details>


### [112] [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)

*Sabrina Patania, Luca Annese, Anna Lambiase, Anita Pellegrini, Tom Foulsham, Azzurra Ruggeri, Silvia Rossi, Silvia Serino, Dimitri Ognibene*

**Main category:** cs.CL

**Keywords:** Language Models, Embodied Perspective Taking, Collaborative Performance

**Relevance Score:** 8

**TL;DR:** This paper investigates the integration of language and embodied perspective taking in LLMs through the PerspAct system, evaluating its impact on collaborative tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of computational models that simultaneously consider language and embodied perspective taking in human collaboration.

**Method:** The study utilizes the PerspAct system to evaluate GPT's narrative generation through an extended director task, analyzing the output's alignment with developmental stages of perspective taking.

**Key Contributions:**

	1. Development of the PerspAct system integrating perspective taking with LLMs.
	2. Evaluation of GPT's narrative generation related to developmental stages in collaborative contexts.
	3. Insights into the impact of language exchanges on internal representations during interactions.

**Result:** GPT produces narratives consistent with developmental stages and shows improved collaborative performance, indicating that language helps refine internal representations during tasks.

**Limitations:** The study may be limited by the specific tasks and contexts used in evaluating collaborative performance.

**Conclusion:** The research underscores the significance of combining linguistic and embodied approaches in LLMs for better modeling of developmental dynamics.

**Abstract:** Language and embodied perspective taking are essential for human collaboration, yet few computational models address both simultaneously. This work investigates the PerspAct system [1], which integrates the ReAct (Reason and Act) paradigm with Large Language Models (LLMs) to simulate developmental stages of perspective taking, grounded in Selman's theory [2]. Using an extended director task, we evaluate GPT's ability to generate internal narratives aligned with specified developmental stages, and assess how these influence collaborative performance both qualitatively (action selection) and quantitatively (task efficiency). Results show that GPT reliably produces developmentally-consistent narratives before task execution but often shifts towards more advanced stages during interaction, suggesting that language exchanges help refine internal representations. Higher developmental stages generally enhance collaborative effectiveness, while earlier stages yield more variable outcomes in complex contexts. These findings highlight the potential of integrating embodied perspective taking and language in LLMs to better model developmental dynamics and stress the importance of evaluating internal speech during combined linguistic and embodied tasks.

</details>


### [113] [Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study](https://arxiv.org/abs/2509.11591)

*Chu-Hsuan Lee, Chen-Chi Chang, Hung-Shin Lee, Yun-Hsiang Hsu, Ching-Yuan Chen*

**Main category:** cs.CL

**Keywords:** Hakka language, Chatbot, AI-assisted language learning, Cognitive processes, Language preservation

**Relevance Score:** 4

**TL;DR:** This study examines user interactions with a generative AI chatbot for Hakka language engagement, analyzing cognitive processes and dialogue acts to demonstrate how AI can support language learning and preservation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** With endangered languages at risk of disappearing, it's crucial to utilize technology alongside culturally informed teaching to preserve them.

**Method:** The study analyzed 7,077 user utterances from the TALKA chatbot, annotated by six cognitive levels and eleven dialogue act types using a dual-layered analytical framework.

**Key Contributions:**

	1. Empirical insights into AI-mediated dialogue for cognitive development in low-resource language learners.
	2. Demonstration of how dialogue acts align with cognitive intentions in language learning.
	3. Implications for using technology to support language preservation and educational practices.

**Result:** The findings suggest that AI chatbots can effectively aid language learning and help learners connect with their cultural identity by facilitating cognitive development and pragmatic negotiation.

**Limitations:** 

**Conclusion:** Generative AI chatbots, designed with an understanding of user communication, can support meaningful language learning and contribute to language preservation efforts.

**Abstract:** With many endangered languages at risk of disappearing, efforts to preserve them now rely more than ever on using technology alongside culturally informed teaching strategies. This study examines user behaviors in TALKA, a generative AI-powered chatbot designed for Hakka language engagement, by employing a dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive processes and dialogue act categorization. We analyzed 7,077 user utterances, each carefully annotated according to six cognitive levels and eleven dialogue act types. These included a variety of functions, such as asking for information, requesting translations, making cultural inquiries, and using language creatively. Pragmatic classifications further highlight how different types of dialogue acts--such as feedback, control commands, and social greetings--align with specific cognitive intentions. The results suggest that generative AI chatbots can support language learning in meaningful ways--especially when they are designed with an understanding of how users think and communicate. They may also help learners express themselves more confidently and connect with their cultural identity. The TALKA case provides empirical insights into how AI-mediated dialogue facilitates cognitive development in low-resource language learners, as well as pragmatic negotiation and socio-cultural affiliation. By focusing on AI-assisted language learning, this study offers new insights into how technology can support language preservation and educational practice.

</details>


### [114] [Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation](https://arxiv.org/abs/2509.11921)

*Helene Tenzer, Oumnia Abidi, Stefan Feuerriegel*

**Main category:** cs.CL

**Keywords:** large language models, cultural sensitivity, translation, cross-cultural communication, prompting strategies

**Relevance Score:** 9

**TL;DR:** This paper analyzes the cultural sensitivity of large language models in English-Japanese translation of workplace emails, highlighting the influence of different prompting strategies on culturally appropriate communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the ability of LLMs to support culturally sensitive communication in multilingual interactions, particularly in a workplace setting.

**Method:** A mixed-methods study is conducted, varying prompting strategies for LLMs: naive prompts, audience-targeted prompts, and instructional prompts based on Japanese communication norms, followed by analysis of translation quality and tone by native speakers.

**Key Contributions:**

	1. Analysis of cultural sensitivity in LLM translations
	2. Comparative evaluation of prompting strategies for LLMs
	3. Recommendations for improving cultural inclusivity in LLMs

**Result:** The study finds that culturally-tailored prompting enhances cultural fit in translations, leading to more appropriate communication.

**Limitations:** 

**Conclusion:** Recommendations are provided for designing culturally inclusive LLMs to improve communication in multilingual contexts.

**Abstract:** Large language models (LLMs) are increasingly used in everyday communication, including multilingual interactions across different cultural contexts. While LLMs can now generate near-perfect literal translations, it remains unclear whether LLMs support culturally appropriate communication. In this paper, we analyze the cultural sensitivity of different LLM designs when applied to English-Japanese translations of workplace e-mails. Here, we vary the prompting strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts specifying the recipient's cultural background, and (3) instructional prompts with explicit guidance on Japanese communication norms. Using a mixed-methods study, we then analyze culture-specific language patterns to evaluate how well translations adapt to cultural norms. Further, we examine the appropriateness of the tone of the translations as perceived by native speakers. We find that culturally-tailored prompting can improve cultural fit, based on which we offer recommendations for designing culturally inclusive LLMs in multilingual settings.

</details>


### [115] [Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification](https://arxiv.org/abs/2509.11604)

*Md. Mithun Hossain, Sanjara, Md. Shakil Hossain, Sudipto Chaki*

**Main category:** cs.CL

**Keywords:** Entity-level sentiment, Dynamic span interaction, Graph-aware memory, Sentiment analysis, Coreference resolution

**Relevance Score:** 7

**TL;DR:** This paper presents SpanEIT, a novel framework for entity-level sentiment classification that integrates dynamic span interaction and graph-aware memory mechanisms to improve sentiment analysis accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in entity-level sentiment classification, such as complex interactions between entities and sentiment expressions, coreference resolution, and linguistic phenomena that complicate sentiment analysis.

**Method:** SpanEIT utilizes span-based representations, bidirectional attention for fine-grained interactions, and a graph attention network to model syntactic and co-occurrence relations, along with a coreference-aware memory module for consistency.

**Key Contributions:**

	1. Introduction of the SpanEIT framework for improved entity-sentiment relations
	2. Integration of dynamic span interaction and graph-aware memory mechanisms
	3. Validation through extensive experimental results showing superior performance over existing models.

**Result:** Experiments demonstrate that SpanEIT outperforms state-of-the-art transformer and hybrid models on FSAD, BARU, and IMDB datasets in terms of accuracy and F1 scores.

**Limitations:** 

**Conclusion:** The results indicate that SpanEIT is a significant advancement for fine-grained sentiment analysis, particularly useful for applications like social media monitoring and customer feedback analysis.

**Abstract:** Entity-level sentiment classification involves identifying the sentiment polarity linked to specific entities within text. This task poses several challenges: effectively modeling the subtle and complex interactions between entities and their surrounding sentiment expressions; capturing dependencies that may span across sentences; and ensuring consistent sentiment predictions for multiple mentions of the same entity through coreference resolution. Additionally, linguistic phenomena such as negation, ambiguity, and overlapping opinions further complicate the analysis. These complexities make entity-level sentiment classification a difficult problem, especially in real-world, noisy textual data. To address these issues, we propose SpanEIT, a novel framework integrating dynamic span interaction and graph-aware memory mechanisms for enhanced entity-sentiment relational modeling. SpanEIT builds span-based representations for entities and candidate sentiment phrases, employs bidirectional attention for fine-grained interactions, and uses a graph attention network to capture syntactic and co-occurrence relations. A coreference-aware memory module ensures entity-level consistency across documents. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT outperforms state-of-the-art transformer and hybrid baselines in accuracy and F1 scores. Ablation and interpretability analyses validate the effectiveness of our approach, underscoring its potential for fine-grained sentiment analysis in applications like social media monitoring and customer feedback analysis.

</details>


### [116] [HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems](https://arxiv.org/abs/2509.11619)

*Spandan Anaokar, Shrey Ganatra, Harshvivek Kashid, Swapnil Bhattacharyya, Shruti Nair, Reshma Sekhar, Siddharth Manohar, Rahul Hemrajani, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** Hallucination Detection, Large Language Models, Chatbots, Consumer Law, Trust in AI

**Relevance Score:** 9

**TL;DR:** This paper presents HalluDetect, a system for reducing hallucinations in large language model-based chatbots, demonstrating an effective mitigation strategy with high accuracy.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** While LLMs are widely used, they are prone to hallucinations which affect their reliability in critical applications like consumer grievance chatbots.

**Method:** The authors developed HalluDetect for hallucination detection in consumer grievance chatbots built with the LLaMA 3.1 8B Instruct model, and benchmarked several chatbot architectures.

**Key Contributions:**

	1. Development of HalluDetect for hallucination detection in chatbots
	2. Benchmarking five chatbot architectures for hallucination minimization
	3. Generalizable approach to enhance factual accuracy in high-risk domains

**Result:** HalluDetect achieved an F1 score of 69%, outperforming baseline detectors by 25.44%. Among five architectures, AgentBot minimized hallucinations to 0.4159 per turn with a token accuracy of 96.13%.

**Limitations:** 

**Conclusion:** The study provides a scalable framework for hallucination mitigation that enhances trust in LLM-driven assistants, applicable to other high-risk domains.

**Abstract:** Large Language Models (LLMs) are widely used in industry but remain prone to hallucinations, limiting their reliability in critical applications. This work addresses hallucination reduction in consumer grievance chatbots built using LLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop HalluDetect, an LLM-based hallucination detection system that achieves an F1 score of 69% outperforming baseline detectors by 25.44%. Benchmarking five chatbot architectures, we find that out of them, AgentBot minimizes hallucinations to 0.4159 per turn while maintaining the highest token accuracy (96.13%), making it the most effective mitigation strategy. Our findings provide a scalable framework for hallucination mitigation, demonstrating that optimized inference strategies can significantly improve factual accuracy. While applied to consumer law, our approach generalizes to other high-risk domains, enhancing trust in LLM-driven assistants. We will release the code and dataset

</details>


### [117] [AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment](https://arxiv.org/abs/2509.11620)

*Kun Li, Lai-Man Po, Hongzheng Yang, Xuyuan Xu, Kangcheng Liu, Yuzhi Zhao*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Aesthetic Assessment, Bias Evaluation

**Relevance Score:** 8

**TL;DR:** This paper proposes AesBiasBench, a benchmark for evaluating Multimodal Large Language Models on stereotype bias and alignment with human aesthetic preferences in Personalized Image Aesthetic Assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the biases in the predictions of Multimodal Large Language Models when applied to Personalized Image Aesthetic Assessment.

**Method:** The benchmark evaluates models on two dimensions: stereotype bias and alignment with human aesthetic preferences, covering subtasks such as Aesthetic Perception, Assessment, and Empathy.

**Key Contributions:**

	1. Introduction of AesBiasBench benchmark for MLLMs
	2. Identification of bias variations across demographic groups
	3. Development of structured metrics for assessing bias and alignment

**Result:** Evaluation of 19 MLLMs shows that smaller models demonstrate stronger stereotype biases, while larger models align more closely with human preferences. Identity information often worsens bias, especially in emotional judgments.

**Limitations:** The study primarily focuses on aesthetic assessments and may not generalize to other domains of HCI or ML applications.

**Conclusion:** Identity-aware evaluation frameworks are crucial for subjective vision-language tasks to ensure fairness and alignment with human aesthetics.

**Abstract:** Multimodal Large Language Models (MLLMs) are increasingly applied in Personalized Image Aesthetic Assessment (PIAA) as a scalable alternative to expert evaluations. However, their predictions may reflect subtle biases influenced by demographic factors such as gender, age, and education. In this work, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two complementary dimensions: (1) stereotype bias, quantified by measuring variations in aesthetic evaluations across demographic groups; and (2) alignment between model outputs and genuine human aesthetic preferences. Our benchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and introduces structured metrics (IFD, NRD, AAS) to assess both bias and alignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o, Claude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL). Results indicate that smaller models exhibit stronger stereotype biases, whereas larger models align more closely with human preferences. Incorporating identity information often exacerbates bias, particularly in emotional judgments. These findings underscore the importance of identity-aware evaluation frameworks in subjective vision-language tasks.

</details>


### [118] [EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI](https://arxiv.org/abs/2509.11648)

*Sai Kartheek Reddy Kasu*

**Main category:** cs.CL

**Keywords:** AI ethics, mental health, large language models, decision-making, dataset

**Relevance Score:** 9

**TL;DR:** Ethical Reasoning in Mental Health (EthicsMH) is a pilot dataset of 125 scenarios for assessing AI's ethical decision-making in mental health contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for better benchmarks that address ethical dilemmas in mental health, particularly as LLMs are deployed in sensitive areas.

**Method:** Discussed the creation of a pilot dataset called EthicsMH that includes 125 scenarios to evaluate AI systems in therapeutic and psychiatric contexts, featuring structured fields for decision-making processes.

**Key Contributions:**

	1. Introduction of EthicsMH dataset for AI evaluation in mental health ethics
	2. Structured evaluation fields for ethical decision-making scenarios
	3. Framework for bridging AI ethics and mental health decision processes

**Result:** The dataset allows for an evaluation not only on decision accuracy but also explanation quality and alignment with professional norms in mental health.

**Limitations:** 

**Conclusion:** EthicsMH lays the groundwork for further exploration and expansion in AI ethics related to mental health, emphasizing the importance of responsible AI deployment.

**Abstract:** The deployment of large language models (LLMs) in mental health and other sensitive domains raises urgent questions about ethical reasoning, fairness, and responsible alignment. Yet, existing benchmarks for moral and clinical decision-making do not adequately capture the unique ethical dilemmas encountered in mental health practice, where confidentiality, autonomy, beneficence, and bias frequently intersect. To address this gap, we introduce Ethical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios designed to evaluate how AI systems navigate ethically charged situations in therapeutic and psychiatric contexts. Each scenario is enriched with structured fields, including multiple decision options, expert-aligned reasoning, expected model behavior, real-world impact, and multi-stakeholder viewpoints. This structure enables evaluation not only of decision accuracy but also of explanation quality and alignment with professional norms. Although modest in scale and developed with model-assisted generation, EthicsMH establishes a task framework that bridges AI ethics and mental health decision-making. By releasing this dataset, we aim to provide a seed resource that can be expanded through community and expert contributions, fostering the development of AI systems capable of responsibly handling some of society's most delicate decisions.

</details>


### [119] [A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection](https://arxiv.org/abs/2509.11687)

*Di Jin, Jun Yang, Xiaobao Wang, Junwei Zhang, Shuqi Li, Dongxiao He*

**Main category:** cs.CL

**Keywords:** fake news detection, knowledge graphs, dynamic knowledge updating, large language models, Monte Carlo Tree Search

**Relevance Score:** 8

**TL;DR:** DYNAMO is a model proposed for fake news detection that utilizes knowledge graphs and large language models to continuously update knowledge and verify news authenticity.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid evolution of the Internet and social media has made distinguishing credible news from misinformation increasingly difficult, necessitating a reliable method for fake news detection that can adapt as events unfold.

**Method:** The paper proposes DYNAMO, a model that constructs a news-domain-specific knowledge graph and employs Monte Carlo Tree Search to validate news step by step, enabling dynamic knowledge updates.

**Key Contributions:**

	1. Development of DYNAMO for dynamic knowledge updating in fake news detection
	2. Integration of knowledge graphs with large language models for authenticity checking
	3. Application of Monte Carlo Tree Search for systematic news validation

**Result:** DYNAMO demonstrated superior performance in fake news detection compared to existing methods by achieving the best results on two real-world datasets.

**Limitations:** 

**Conclusion:** The use of knowledge graphs combined with large language models allows for improved authenticity detection of news and verification of new knowledge, addressing key challenges in fake news detection.

**Abstract:** As the Internet and social media evolve rapidly, distinguishing credible news from a vast amount of complex information poses a significant challenge. Due to the suddenness and instability of news events, the authenticity labels of news can potentially shift as events develop, making it crucial for fake news detection to obtain the latest event updates. Existing methods employ retrieval-augmented generation to fill knowledge gaps, but they suffer from issues such as insufficient credibility of retrieved content and interference from noisy information. We propose a dynamic knowledge update-driven model for fake news detection (DYNAMO), which leverages knowledge graphs to achieve continuous updating of new knowledge and integrates with large language models to fulfill dual functions: news authenticity detection and verification of new knowledge correctness, solving the two key problems of ensuring the authenticity of new knowledge and deeply mining news semantics. Specifically, we first construct a news-domain-specific knowledge graph. Then, we use Monte Carlo Tree Search to decompose complex news and verify them step by step. Finally, we extract and update new knowledge from verified real news texts and reasoning paths. Experimental results demonstrate that DYNAMO achieves the best performance on two real-world datasets.

</details>


### [120] [CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model](https://arxiv.org/abs/2509.11698)

*Wei-Hsin Yeh, Yu-An Su, Chih-Ning Chen, Yi-Hsueh Lin, Calvin Ku, Wen-Hsin Chiu, Min-Chun Hu, Lun-Wei Ku*

**Main category:** cs.CL

**Keywords:** motion instruction, multimodal models, sports coaching, corrective guidance, human-computer interaction

**Relevance Score:** 7

**TL;DR:** CoachMe is a reference-based model designed to analyze sports movements and generate sport-specific corrective instructions, improving athletes' techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of generating precise and informative motion instructions for athletes, leveraging advances in multimodal models.

**Method:** CoachMe analyzes differences between a learner's motion and a reference, focusing on temporal and physical aspects to provide tailored feedback.

**Key Contributions:**

	1. Introduction of CoachMe, a reference-based instruction model for sports
	2. Demonstrated adaptation to specific sports like skating and boxing
	3. Improved performance over existing models in generating effective coaching instructions

**Result:** CoachMe outperformed GPT-4o by significant margins (31.6% in figure skating and 58.3% in boxing) in generating effective instructional guidance.

**Limitations:** 

**Conclusion:** The model demonstrates its effectiveness in providing detailed movement error analysis and improvement methods in sports coaching.

**Abstract:** Motion instruction is a crucial task that helps athletes refine their technique by analyzing movements and providing corrective guidance. Although recent advances in multimodal models have improved motion understanding, generating precise and sport-specific instruction remains challenging due to the highly domain-specific nature of sports and the need for informative guidance. We propose CoachMe, a reference-based model that analyzes the differences between a learner's motion and a reference under temporal and physical aspects. This approach enables both domain-knowledge learning and the acquisition of a coach-like thinking process that identifies movement errors effectively and provides feedback to explain how to improve. In this paper, we illustrate how CoachMe adapts well to specific sports such as skating and boxing by learning from general movements and then leveraging limited data. Experiments show that CoachMe provides high-quality instructions instead of directions merely in the tone of a coach but without critical information. CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on boxing. Analysis further confirms that it elaborates on errors and their corresponding improvement methods in the generated instructions. You can find CoachMe here: https://motionxperts.github.io/

</details>


### [121] [Room acoustics affect communicative success in hybrid meeting spaces: a pilot study](https://arxiv.org/abs/2509.11709)

*Robert Einig, Stefan Janscha, Jonas Schuster, Julian Koch, Martin Hagmueller, Barbara Schuppler*

**Main category:** cs.CL

**Keywords:** acoustics, hybrid meetings, communication, room design, speech intelligibility

**Relevance Score:** 4

**TL;DR:** This study examines whether acoustic improvements in seminar rooms enhance communication in hybrid meetings, finding preliminary evidence of positive impact.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overlooked issue of room acoustics in hybrid meeting spaces that can lead to communication issues.

**Method:** The study involved recording the communicative performance of two groups before and after implementing acoustic improvements in a seminar room.

**Key Contributions:**

	1. Investigates the impact of acoustic design on hybrid meeting effectiveness
	2. Provides empirical evidence of potential benefits of acoustic interventions
	3. Makes findings accessible to the speech communication community

**Result:** Results suggested improved communicative success in hybrid meetings following acoustic enhancements, although not statistically significant due to small sample size.

**Limitations:** Small sample size limits statistical significance of results.

**Conclusion:** Better room acoustics can potentially enhance communication effectiveness in hybrid settings, warranting further investigation.

**Abstract:** Since the COVID-19 pandemic in 2020, universities and companies have increasingly integrated hybrid features into their meeting spaces, or even created dedicated rooms for this purpose. While the importance of a fast and stable internet connection is often prioritized, the acoustic design of seminar rooms is frequently overlooked. Poor acoustics, particularly excessive reverberation, can lead to issues such as misunderstandings, reduced speech intelligibility or cognitive and vocal fatigue. This pilot study investigates whether room acoustic interventions in a seminar room at Graz University of Technology support better communication in hybrid meetings. For this purpose, we recorded two groups of persons twice, once before and once after improving the acoustics of the room. Our findings -- despite not reaching statistical significance due to the small sample size - indicate clearly that our spatial interventions improve communicative success in hybrid meetings. To make the paper accessible also for readers from the speech communication community, we explain room acoustics background, relevant for the interpretation of our results.

</details>


### [122] [An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents](https://arxiv.org/abs/2509.11773)

*Gaye Colakoglu, Gürkan Solmaz, Jonathan Fürst*

**Main category:** cs.CL

**Keywords:** Declaration of Performance, information extraction, stateful systems, automated reasoning, document analysis

**Relevance Score:** 3

**TL;DR:** This paper presents a framework for enhancing automated key-value pair extraction from diverse Declaration of Performance documents using a stateful agentic system.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The variability in layout, language, and format of Declaration of Performance (DoP) documents creates challenges for existing automated information extraction methods.

**Method:** A planner-executor-responder architecture is employed to dynamically orchestrate extraction tools, infer user intent, and adjust to different document modalities.

**Key Contributions:**

	1. Development of a stateful agentic system for document processing
	2. Introduction of a planner-executor-responder architecture
	3. Demonstrated improved performance in dynamic tool orchestration for key-value extraction

**Result:** The system demonstrates improved robustness in extracting structured information from DoPs across various formats and languages, outperforming traditional static and LLM-only approaches.

**Limitations:** 

**Conclusion:** This approach offers a scalable solution for structured data extraction in regulated environments, addressing the challenges posed by document diversity.

**Abstract:** Declaration of Performance (DoP) documents, mandated by EU regulation, certify the performance of construction products. While some of their content is standardized, DoPs vary widely in layout, language, schema, and format, posing challenges for automated key-value pair extraction (KVP) and question answering (QA). Existing static or LLM-only IE pipelines often hallucinate and fail to adapt to this structural diversity. Our domain-specific, stateful agentic system addresses these challenges through a planner-executor-responder architecture. The system infers user intent, detects document modality, and orchestrates tools dynamically for robust, traceable reasoning while avoiding tool misuse or execution loops. Evaluation on a curated DoP dataset demonstrates improved robustness across formats and languages, offering a scalable solution for structured data extraction in regulated workflows.

</details>


### [123] [User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums](https://arxiv.org/abs/2509.11777)

*Mikhail Kulyabin, Jan Joosten, Choro Ulan uulu, Nuno Miguel Martins Pacheco, Fabian Ries, Filippos Petridis, Jan Bosch, Helena Holmström Olsson*

**Main category:** cs.CL

**Keywords:** User Experience, Machine Learning, Sentiment Analysis, Feedback Processing, Artificial Intelligence

**Relevance Score:** 8

**TL;DR:** The paper presents the UXPID dataset, a collection of user feedback from industrial forums, designed to enhance UX analysis using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a wealth of user feedback in industrial forums that remains underutilized for product development due to its unstructured nature and domain-specific vocabulary.

**Method:** The paper introduces the User eXperience Perception Insights Dataset (UXPID), synthesized from 7130 comments in a public automation forum, analyzed and annotated using a large language model.

**Key Contributions:**

	1. Creation of UXPID dataset from industrial automation feedback
	2. Systematic analysis using LLM for UX insights
	3. Supports training of models for sentiment analysis and requirements extraction

**Result:** The dataset includes enriched metadata and classifications for UX insights, enabling advanced analysis for tasks like sentiment analysis and issue detection.

**Limitations:** 

**Conclusion:** UXPID is a valuable resource for improving user experience research and AI-driven analysis in technical contexts where real-world data access is restricted.

**Abstract:** Customer feedback in industrial forums reflect a rich but underexplored source of insight into real-world product experience. These publicly shared discussions offer an organic view of user expectations, frustrations, and success stories shaped by the specific contexts of use. Yet, harnessing this information for systematic analysis remains challenging due to the unstructured and domain-specific nature of the content. The lack of structure and specialized vocabulary makes it difficult for traditional data analysis techniques to accurately interpret, categorize, and quantify the feedback, thereby limiting its potential to inform product development and support strategies. To address these challenges, this paper presents the User eXperience Perception Insights Dataset (UXPID), a collection of 7130 artificially synthesized and anonymized user feedback branches extracted from a public industrial automation forum. Each JavaScript object notation (JSON) record contains multi-post comments related to specific hardware and software products, enriched with metadata and contextual conversation data. Leveraging a large language model (LLM), each branch is systematically analyzed and annotated for UX insights, user expectations, severity and sentiment ratings, and topic classifications. The UXPID dataset is designed to facilitate research in user requirements, user experience (UX) analysis, and AI-driven feedback processing, particularly where privacy and licensing restrictions limit access to real-world data. UXPID supports the training and evaluation of transformer-based models for tasks such as issue detection, sentiment analysis, and requirements extraction in the context of technical forums.

</details>


### [124] [When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries](https://arxiv.org/abs/2509.11802)

*Dvora Goncharok, Arbel Shifman, Alexander Apartsin, Yehudit Aperstein*

**Main category:** cs.CL

**Keywords:** medical forums, patient concerns, machine learning, natural language processing, critical health events

**Relevance Score:** 9

**TL;DR:** This study presents an annotated dataset of medication-related questions from online forums and benchmarks machine learning methods for detecting critical patient inquiries.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To utilize online medical forums for insights into patient concerns regarding medication and detect critical questions that could signal health crises.

**Method:** The study introduces a novel annotated dataset of medication-related questions and benchmarks six machine learning classifiers alongside three LLM-based classification methods using TF-IDF and other approaches.

**Key Contributions:**

	1. Introduction of a novel annotated dataset for medication-related questions.
	2. Benchmarking of various machine learning and LLM methods for critical question detection.
	3. Public availability of the dataset to encourage further research in health informatics.

**Result:** The results demonstrate the efficacy of both traditional and modern machine learning methods in supporting real-time triage for potential health risks based on patient-generated questions.

**Limitations:** 

**Conclusion:** The curated dataset has significant implications for improving patient safety and can facilitate further research in natural language processing and early warning systems.

**Abstract:** Online medical forums are a rich and underutilized source of insight into patient concerns, especially regarding medication use. Some of the many questions users pose may signal confusion, misuse, or even the early warning signs of a developing health crisis. Detecting these critical questions that may precede severe adverse events or life-threatening complications is vital for timely intervention and improving patient safety. This study introduces a novel annotated dataset of medication-related questions extracted from online forums. Each entry is manually labelled for criticality based on clinical risk factors. We benchmark the performance of six traditional machine learning classifiers using TF-IDF textual representations, alongside three state-of-the-art large language model (LLM)-based classification approaches that leverage deep contextual understanding. Our results highlight the potential of classical and modern methods to support real-time triage and alert systems in digital health spaces. The curated dataset is made publicly available to encourage further research at the intersection of patient-generated data, natural language processing, and early warning systems for critical health events. The dataset and benchmark are available at: https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.

</details>


### [125] [From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives](https://arxiv.org/abs/2509.11803)

*Eden Mama, Liel Sheri, Yehudit Aperstein, Alexander Apartsin*

**Main category:** cs.CL

**Keywords:** large language models, healthcare, synthetic dataset, noisy patient narratives, diagnostic benchmarking

**Relevance Score:** 9

**TL;DR:** This paper presents a synthetic dataset aimed at evaluating large language models (LLMs) on their ability to interpret noisy patient-generated narratives in healthcare.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The need to assess LLM performance in real-world healthcare settings, where patient narratives are often informal and ambiguous, rather than in controlled environments with clean clinical text.

**Method:** Creation of the Noisy Diagnostic Benchmark (NDB), a dataset containing synthetic patient descriptions with varying linguistic noise and annotated with ground-truth diagnoses; evaluation of different state-of-the-art LLMs including BERT and T5 models.

**Key Contributions:**

	1. Introduction of a synthetic dataset simulating patient self-descriptions with linguistic noise
	2. Provision of ground-truth diagnoses for diverse communication styles
	3. Fine-tuning and evaluation of state-of-the-art LLMs to assess their diagnostic performance under realistic conditions.

**Result:** The proposed benchmark enables the fine-tuning and assessment of the diagnostic capabilities of LLMs in interpreting noisy patient descriptions.

**Limitations:** 

**Conclusion:** The release of the NDB supports reproducibility in research and encourages further exploration of LLM performance in realistic linguistic conditions in healthcare.

**Abstract:** The widespread adoption of large language models (LLMs) in healthcare raises critical questions about their ability to interpret patient-generated narratives, which are often informal, ambiguous, and noisy. Existing benchmarks typically rely on clean, structured clinical text, offering limited insight into model performance under realistic conditions. In this work, we present a novel synthetic dataset designed to simulate patient self-descriptions characterized by varying levels of linguistic noise, fuzzy language, and layperson terminology. Our dataset comprises clinically consistent scenarios annotated with ground-truth diagnoses, spanning a spectrum of communication clarity to reflect diverse real-world reporting styles. Using this benchmark, we fine-tune and evaluate several state-of-the-art models (LLMs), including BERT-based and encoder-decoder T5 models. To support reproducibility and future research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset of noisy, synthetic patient descriptions designed to stress-test and compare the diagnostic capabilities of large language models (LLMs) under realistic linguistic conditions. We made the benchmark available for the community: https://github.com/lielsheri/PatientSignal

</details>


### [126] [PledgeTracker: A System for Monitoring the Fulfilment of Pledges](https://arxiv.org/abs/2509.11804)

*Yulong Chen, Michael Sejr Schlichtkrull, Zhenyun Deng, David Corney, Nasim Asl, Joshua Salisbury, Andrew Dudfield, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** political pledges, evidence retrieval, timeline construction

**Relevance Score:** 4

**TL;DR:** PledgeTracker is a system for tracking political pledge fulfilment through structured event timelines, addressing dynamic evidence retrieval and multi-document processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the tracking of political pledges by overcoming limitations in existing methods that treat the task as simple document classification.

**Method:** PledgeTracker features a multi-step evidence retrieval module, a timeline construction module, and a fulfilment filtering module to create structured timelines of pledge fulfilment.

**Key Contributions:**

	1. Introduction of a multi-step evidence retrieval process
	2. Development of a structured event timeline for pledge fulfilment
	3. Reduction of human verification effort through effective evidence retrieval

**Result:** PledgeTracker was evaluated with professional fact-checkers, showing improved effectiveness in retrieving evidence and reducing verification effort.

**Limitations:** 

**Conclusion:** PledgeTracker provides a more dynamic and interpretable method for tracking political pledge fulfilment compared to traditional document classification approaches.

**Abstract:** Political pledges reflect candidates' policy commitments, but tracking their fulfilment requires reasoning over incremental evidence distributed across multiple, dynamically updated sources. Existing methods simplify this task into a document classification task, overlooking its dynamic, temporal and multi-document nature. To address this issue, we introduce \textsc{PledgeTracker}, a system that reformulates pledge verification into structured event timeline construction. PledgeTracker consists of three core components: (1) a multi-step evidence retrieval module; (2) a timeline construction module and; (3) a fulfilment filtering module, allowing the capture of the evolving nature of pledge fulfilment and producing interpretable and structured timelines. We evaluate PledgeTracker in collaboration with professional fact-checkers in real-world workflows, demonstrating its effectiveness in retrieving relevant evidence and reducing human verification effort.

</details>


### [127] [SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection](https://arxiv.org/abs/2509.11818)

*Taichi Aida, Danushka Bollegala*

**Main category:** cs.CL

**Keywords:** Semantic Change Detection, Interpretability, Word Embeddings, Machine Learning, Natural Language Processing

**Relevance Score:** 7

**TL;DR:** Proposes SCDTour, a method that balances interpretability and performance in Semantic Change Detection (SCD) by ordering and merging interpretable axes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the trade-off between interpretability and performance in Semantic Change Detection (SCD) embeddings.

**Method:** SCDTour orders and merges interpretable axes based on semantic similarity and their contribution to semantic change.

**Key Contributions:**

	1. Introduces SCDTour methodology for interpretability vs performance in SCD
	2. Demonstrates improved performance with fewer axes
	3. Provides source code for reproducibility.

**Result:** SCDTour preserves performance in semantic change detection while maintaining high interpretability, resulting in a refined set of word senses that can achieve improved performance.

**Limitations:** 

**Conclusion:** SCDTour effectively balances interpretability with SCD performance, enabling better understanding of semantic shifts through fewer refined axes.

**Abstract:** In Semantic Change Detection (SCD), it is a common problem to obtain embeddings that are both interpretable and high-performing. However, improving interpretability often leads to a loss in the SCD performance, and vice versa. To address this problem, we propose SCDTour, a method that orders and merges interpretable axes to alleviate the performance degradation of SCD. SCDTour considers both (a) semantic similarity between axes in the embedding space, as well as (b) the degree to which each axis contributes to semantic change. Experimental results show that SCDTour preserves performance in semantic change detection while maintaining high interpretability. Moreover, agglomerating the sorted axes produces a more refined set of word senses, which achieves comparable or improved performance against the original full-dimensional embeddings in the SCD task. These findings demonstrate that SCDTour effectively balances interpretability and SCD performance, enabling meaningful interpretation of semantic shifts through a small number of refined axes. Source code is available at https://github.com/LivNLP/svp-tour .

</details>


### [128] [MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues](https://arxiv.org/abs/2509.11860)

*Weishu Chen, Jinyi Tang, Zhouhui Hou, Shihao Han, Mingjie Zhan, Zhiyuan Huang, Delong Liu, Jiawei Guo, Zhicheng Zhao, Fei Su*

**Main category:** cs.CL

**Keywords:** memory extraction, human-robot interaction, storytelling, ultra-long dialogues, forgetting mechanism

**Relevance Score:** 8

**TL;DR:** MOOM introduces a dual-branch memory plugin for ultra-long dialogues in robot role-playing, addressing uncontrolled memory growth with a forgetting mechanism and a new dialogue dataset for training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for maintaining coherent ultra-long dialogues in human-robot interactions while managing memory growth challenges.

**Method:** MOOM utilizes a dual-branch architecture to summarize plot conflicts and extract character profiles, incorporating a forgetting mechanism for memory control.

**Key Contributions:**

	1. Introduction of MOOM as a dual-branch memory plugin
	2. Development of the ZH-4O ultra-long dialogue dataset
	3. Implementation of a forgetting mechanism for controlled memory growth

**Result:** Experimental results show that MOOM outperforms existing memory extraction methods, achieving efficient memory management with fewer model calls.

**Limitations:** 

**Conclusion:** MOOM enhances the performance of dialogue systems by effectively balancing memory growth with coherence in ultra-long dialogues.

**Abstract:** Memory extraction is crucial for maintaining coherent ultra-long dialogues in human-robot role-playing scenarios. However, existing methods often exhibit uncontrolled memory growth. To address this, we propose MOOM, the first dual-branch memory plugin that leverages literary theory by modeling plot development and character portrayal as core storytelling elements. Specifically, one branch summarizes plot conflicts across multiple time scales, while the other extracts the user's character profile. MOOM further integrates a forgetting mechanism, inspired by the ``competition-inhibition'' memory theory, to constrain memory capacity and mitigate uncontrolled growth. Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset specifically designed for role-playing, featuring dialogues that average 600 turns and include manually annotated memory information. Experimental results demonstrate that MOOM outperforms all state-of-the-art memory extraction methods, requiring fewer large language model invocations while maintaining a controllable memory capacity.

</details>


### [129] [Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models](https://arxiv.org/abs/2509.11868)

*Sabrina Patania, Luca Annese, Anna Lambiase, Anita Pellegrini, Tom Foulsham, Azzurra Ruggeri, Silvia Rossi, Silvia Serino, Dimitri Ognibene*

**Main category:** cs.CL

**Keywords:** Language Models, Perspective Taking, Collaboration, Internal Narratives, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper examines the PerspAct system, integrating LLMs with the ReAct paradigm to simulate perspective taking in human collaboration, finding that higher developmental stages improve collaborative performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of computational models that simulate both language and embodied perspective taking in human collaboration.

**Method:** The study employs the PerspAct system, using an extended director task to analyze GPT's narrative generation aligned with developmental stages according to Selman's theory.

**Key Contributions:**

	1. Introduction of the PerspAct system for simulating perspective taking with LLMs
	2. Demonstration of how narrative generation affects collaborative performance
	3. Insights into the shifting nature of internal representations during collaboration

**Result:** GPT produces narratives consistent with developmentally appropriate stages, which enhance task performance but may shift during interactions.

**Limitations:** 

**Conclusion:** Integrating embodied perspective taking and language in LLMs can improve models of developmental dynamics and emphasizes the significance of internal speech during collaborative tasks.

**Abstract:** Language and embodied perspective taking are essential for human collaboration, yet few computational models address both simultaneously. This work investigates the PerspAct system [1], which integrates the ReAct (Reason and Act) paradigm with Large Language Models (LLMs) to simulate developmental stages of perspective taking, grounded in Selman's theory [2]. Using an extended director task, we evaluate GPT's ability to generate internal narratives aligned with specified developmental stages, and assess how these influence collaborative performance both qualitatively (action selection) and quantitatively (task efficiency). Results show that GPT reliably produces developmentally-consistent narratives before task execution but often shifts towards more advanced stages during interaction, suggesting that language exchanges help refine internal representations. Higher developmental stages generally enhance collaborative effectiveness, while earlier stages yield more variable outcomes in complex contexts. These findings highlight the potential of integrating embodied perspective taking and language in LLMs to better model developmental dynamics and stress the importance of evaluating internal speech during combined linguistic and embodied tasks.

</details>


### [130] [Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible](https://arxiv.org/abs/2509.11915)

*Aadil Gani Ganie*

**Main category:** cs.CL

**Keywords:** AI authorship detection, quantum uncertainty, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper discusses the challenges of distinguishing between human and AI-generated text, drawing parallels with quantum uncertainty and identifying inherent trade-offs in detection methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the limitations and challenges in detecting AI-generated text as large language models advance.

**Method:** The paper draws a conceptual parallel between quantum uncertainty and authorship detection, analyzing current methods like stylometry, watermarking, and neural classifiers.

**Key Contributions:**

	1. Conceptual parallels between quantum uncertainty and authorship detection.
	2. Analysis of inherent limitations in current detection methods.
	3. Discussion on the broader ethical implications of AI authorship.

**Result:** Detection accuracy in AI-generated text often comes at the cost of altering the authenticity and flow of the text, making perfect detection increasingly difficult.

**Limitations:** Does not provide concrete solutions for improving detection methods.

**Conclusion:** The paper suggests that the challenges in AI-text detection reflect deeper tensions in language itself and cannot be solved merely with better tools.

**Abstract:** As large language models (LLMs) become more advanced, it is increasingly difficult to distinguish between human-written and AI-generated text. This paper draws a conceptual parallel between quantum uncertainty and the limits of authorship detection in natural language. We argue that there is a fundamental trade-off: the more confidently one tries to identify whether a text was written by a human or an AI, the more one risks disrupting the text's natural flow and authenticity. This mirrors the tension between precision and disturbance found in quantum systems. We explore how current detection methods--such as stylometry, watermarking, and neural classifiers--face inherent limitations. Enhancing detection accuracy often leads to changes in the AI's output, making other features less reliable. In effect, the very act of trying to detect AI authorship introduces uncertainty elsewhere in the text. Our analysis shows that when AI-generated text closely mimics human writing, perfect detection becomes not just technologically difficult but theoretically impossible. We address counterarguments and discuss the broader implications for authorship, ethics, and policy. Ultimately, we suggest that the challenge of AI-text detection is not just a matter of better tools--it reflects a deeper, unavoidable tension in the nature of language itself.

</details>


### [131] [Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation](https://arxiv.org/abs/2509.11921)

*Helene Tenzer, Oumnia Abidi, Stefan Feuerriegel*

**Main category:** cs.CL

**Keywords:** large language models, cultural sensitivity, translation, multilingual communication, prompt design

**Relevance Score:** 8

**TL;DR:** This paper examines the cultural sensitivity of different large language model designs in translating workplace e-mails from English to Japanese using varied prompting strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether LLMs can support culturally appropriate communication beyond literal translations.

**Method:** A mixed-methods study analyzing English-Japanese translations using three different prompting strategies.

**Key Contributions:**

	1. Analysis of cultural sensitivity in LLMs
	2. Empirical study of tone appropriateness in translations
	3. Guidelines for culturally-inclusive LLM prompting strategies

**Result:** Culturally-tailored prompting improved the cultural fit in translations, as evaluated by native speakers.

**Limitations:** 

**Conclusion:** Recommendations are provided for designing culturally inclusive LLMs in multilingual contexts.

**Abstract:** Large language models (LLMs) are increasingly used in everyday communication, including multilingual interactions across different cultural contexts. While LLMs can now generate near-perfect literal translations, it remains unclear whether LLMs support culturally appropriate communication. In this paper, we analyze the cultural sensitivity of different LLM designs when applied to English-Japanese translations of workplace e-mails. Here, we vary the prompting strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts specifying the recipient's cultural background, and (3) instructional prompts with explicit guidance on Japanese communication norms. Using a mixed-methods study, we then analyze culture-specific language patterns to evaluate how well translations adapt to cultural norms. Further, we examine the appropriateness of the tone of the translations as perceived by native speakers. We find that culturally-tailored prompting can improve cultural fit, based on which we offer recommendations for designing culturally inclusive LLMs in multilingual settings.

</details>


### [132] [Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation](https://arxiv.org/abs/2504.12805)

*Takaya Arita, Wenxian Zheng, Reiji Suzuki, Fuminori Akiba*

**Main category:** cs.CL

**Keywords:** large language models, art critiques, Theory of Mind, interpretation, art criticism

**Relevance Score:** 8

**TL;DR:** This study investigates how large language models (LLMs) generate art critiques and handle Theory of Mind tasks, highlighting their interpreted potential and cognitive limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of LLMs in generating art critiques and reasoning about mental states in art-related situations.

**Method:** The study involved creating a system combining art criticism theories to generate critiques, which were evaluated against human critiques using Turing test-style evaluation. Additionally, new Theory of Mind tasks were devised to assess LLM performance in complex interpretative challenges.

**Key Contributions:**

	1. Development of a system for generating art critiques using LLMs
	2. Creation of new Theory of Mind tasks for evaluating interpretative reasoning in art
	3. Insights into the cognitive limitations and potential of LLMs in artistic contexts

**Result:** LLMs produced plausible art critiques, often indistinguishable from human-generated critiques, and showed varying performance on Theory of Mind tasks, particularly in emotionally ambiguous scenarios.

**Limitations:** The results suggest a limitation in LLMs' genuine understanding, reinforcing aspects of the Generative AI Paradox.

**Conclusion:** The findings reveal that LLMs can simulate understanding more effectively with careful prompting, shedding light on their cognitive limitations while not contradicting the Generative AI Paradox.

**Abstract:** This study explored how large language models (LLMs) perform in two areas related to art: writing critiques of artworks and reasoning about mental states (Theory of Mind, or ToM) in art-related situations. For the critique generation part, we built a system that combines Noel Carroll's evaluative framework with a broad selection of art criticism theories. The model was prompted to first write a full-length critique and then shorter, more coherent versions using a step-by-step prompting process. These AI-generated critiques were then compared with those written by human experts in a Turing test-style evaluation. In many cases, human subjects had difficulty telling which was which, and the results suggest that LLMs can produce critiques that are not only plausible in style but also rich in interpretation, as long as they are carefully guided. In the second part, we introduced new simple ToM tasks based on situations involving interpretation, emotion, and moral tension, which can appear in the context of art. These go beyond standard false-belief tests and allow for more complex, socially embedded forms of reasoning. We tested 41 recent LLMs and found that their performance varied across tasks and models. In particular, tasks that involved affective or ambiguous situations tended to reveal clearer differences. Taken together, these results help clarify how LLMs respond to complex interpretative challenges, revealing both their cognitive limitations and potential. While our findings do not directly contradict the so-called Generative AI Paradox--the idea that LLMs can produce expert-like output without genuine understanding--they suggest that, depending on how LLMs are instructed, such as through carefully designed prompts, these models may begin to show behaviors that resemble understanding more closely than we might assume.

</details>


### [133] [Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding](https://arxiv.org/abs/2509.11961)

*Mingxiao Huo, Jiayi Zhang, Hewei Wang, Jinfeng Xu, Zheyu Chen, Huilin Tai, Yijun Chen*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, speculative decoding, real-time applications

**Relevance Score:** 7

**TL;DR:** Spec-LLaVA introduces a speculative decoding method to accelerate Vision-Language Models without sacrificing output quality, achieving significant speed improvements in multimodal reasoning tasks.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the slow autoregressive inference in Vision-Language Models, which limits their deployment in real-time applications.

**Method:** Spec-LLaVA combines a lightweight draft VLM with a larger target model, using speculative decoding to generate multiple tokens per step, supported by a dynamic tree-based verification algorithm that optimally expands and prunes branches according to the draft model's confidence.

**Key Contributions:**

	1. Introduction of Spec-LLaVA for accelerated inference in VLMs
	2. Dynamic tree-based verification algorithm for efficiency
	3. Demonstrated threefold increase in decoding speed without quality loss

**Result:** Spec-LLaVA achieves up to 3.28x faster decoding on LLaVA-1.5 models (7B, 13B) on MS COCO out-of-domain images, maintaining the generation quality.

**Limitations:** 

**Conclusion:** This framework provides a lossless acceleration for VLMs, enabling practical use in real-time applications, particularly in resource-constrained or on-device settings.

**Abstract:** Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer from slow autoregressive inference, limiting their deployment in real-time applications. We introduce Spec-LLaVA, a system that applies speculative decoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA pairs a lightweight draft VLM with a large target model: the draft speculates future tokens, which the target verifies in parallel, allowing multiple tokens to be generated per step. To maximize efficiency, we design a dynamic tree-based verification algorithm that adaptively expands and prunes speculative branches using draft model confidence. On MS COCO out-of-domain images, Spec-LLaVA achieves up to 3.28$\times$ faster decoding on LLaVA-1.5 (7B, 13B) with no loss in generation quality. This work presents a lossless acceleration framework for VLMs using dynamic tree-structured speculative decoding, opening a path toward practical real-time multimodal assistants. Importantly, the lightweight draft model design makes the framework amenable to resource-constrained or on-device deployment settings.

</details>


### [134] [ToolRM: Outcome Reward Models for Tool-Calling Large Language Models](https://arxiv.org/abs/2509.11963)

*Mayank Agarwal, Ibrahim Abdelaziz, Kinjal Basu, Merve Unuvar, Luis A. Lastras, Yara Rizk, Pavan Kapanipathi*

**Main category:** cs.CL

**Keywords:** large language models, reward modeling, benchmark, tool use, outcome-based training

**Relevance Score:** 9

**TL;DR:** Introducing FC-RewardBench, a benchmark for evaluating reward models in tool use scenarios, highlighting the need for domain-specific modeling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of effective evaluation metrics for reward models in tool-calling scenarios, particularly as large language models increasingly interact with external tools.

**Method:** We introduce FC-RewardBench as a systematic benchmark and propose a training framework for outcome-based reward models using synthesized data from open-weight LLMs, training models from 1.7B to 14B parameters across various benchmarks.

**Key Contributions:**

	1. Development of FC-RewardBench benchmark for tool use evaluation
	2. Proposal of a training framework for outcome-based reward models
	3. Demonstration of performance improvements with domain-specific modeling

**Result:** Models trained on FC-RewardBench achieve up to 25% improvement in downstream task performance compared to general-purpose baselines, showcasing the effectiveness of domain-specific reward modeling.

**Limitations:** 

**Conclusion:** The findings emphasize the necessity of developing tailored reward models for enhanced tool use evaluation and task performance in LLM applications.

**Abstract:** As large language models (LLMs) increasingly interact with external tools, reward modeling for tool use has become a critical yet underexplored area. Existing reward models, trained primarily on natural language outputs, struggle to evaluate tool-based reasoning and execution. To quantify this gap, we introduce FC-RewardBench, the first benchmark designed to systematically assess reward models' performance in tool-calling scenarios. Our analysis shows that current reward models often miss key signals of effective tool use, highlighting the need for domain-specific modeling. To address this, we propose a training framework for outcome-based reward models using data synthesized from permissively licensed, open-weight LLMs. We train models ranging from 1.7B to 14B parameters and evaluate them across seven out-of-domain benchmarks. These models consistently outperform general-purpose baselines, achieving up to 25\% average improvement in downstream task performance and enabling data-efficient fine-tuning through reward-guided filtering.

</details>


### [135] [Query-Focused Extractive Summarization for Sentiment Explanation](https://arxiv.org/abs/2509.11989)

*Ahmed Moubtahij, Sylvie Ratté, Yazid Attabi, Maxime Dumas*

**Main category:** cs.CL

**Keywords:** Query-Focused Summarization, sentiment analysis, multi-bias framework

**Relevance Score:** 4

**TL;DR:** This paper presents a multi-bias framework for Query-Focused Summarization (QFS) to effectively analyze client feedback sentiment by addressing linguistic dissonance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance productivity in analyzing client feedback sentiment from large text documents.

**Method:** A multi-bias framework is proposed to bridge linguistic gaps between queries and source documents, alongside specialized approaches for sentiment explanation.

**Key Contributions:**

	1. Introduction of a multi-bias framework for QFS
	2. Specialized approaches for sentiment-based biases
	3. Outperforming baseline models on a proprietary dataset

**Result:** Experimental results show that the proposed framework significantly outperforms baseline models on a sentiment-aware QFS dataset.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of the multi-bias framework in improving QFS for sentiment analysis, suggesting its broader applicability.

**Abstract:** Constructive analysis of feedback from clients often requires determining the cause of their sentiment from a substantial amount of text documents. To assist and improve the productivity of such endeavors, we leverage the task of Query-Focused Summarization (QFS). Models of this task are often impeded by the linguistic dissonance between the query and the source documents. We propose and substantiate a multi-bias framework to help bridge this gap at a domain-agnostic, generic level; we then formulate specialized approaches for the problem of sentiment explanation through sentiment-based biases and query expansion. We achieve experimental results outperforming baseline models on a real-world proprietary sentiment-aware QFS dataset.

</details>


### [136] [Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles](https://arxiv.org/abs/2509.11991)

*Jesús Calleja, David Ponce, Thierry Etchegoyhen*

**Main category:** cs.CL

**Keywords:** text adaptation, Plain Language, Easy Read, Large Language Models, accessibility

**Relevance Score:** 7

**TL;DR:** Vicomtech participated in the CLEARS challenge, focusing on text adaptation to Plain Language and Easy Read in Spanish, achieving top rankings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better accessibility in written Spanish through adaptations of texts to Plain Language and Easy Read formats.

**Method:** The approach involves automatic post-editing of various Large Language Model adaptations, refined iteratively based on readability and similarity metrics.

**Key Contributions:**

	1. Introduced iterative refining methodology for text adaptation
	2. Achieved top rankings in CLEARS challenge
	3. Focused on accessibility in language through LLM adaptations

**Result:** The submissions achieved first place in Plain Language adaptation and second place in Easy Read adaptation.

**Limitations:** 

**Conclusion:** The iterative refinement process significantly improved the quality of text adaptations, demonstrating the effectiveness of the approach.

**Abstract:** We describe Vicomtech's participation in the CLEARS challenge on text adaptation to Plain Language and Easy Read in Spanish. Our approach features automatic post-editing of different types of initial Large Language Model adaptations, where successive adaptations are generated iteratively until readability and similarity metrics indicate that no further adaptation refinement can be successfully performed. Taking the average of all official metrics, our submissions achieved first and second place in Plain language and Easy Read adaptation, respectively.

</details>


### [137] [Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect](https://arxiv.org/abs/2509.12065)

*Alina Klerings, Jannik Brinkmann, Daniel Ruffinelli, Simone Ponzetto*

**Main category:** cs.CL

**Keywords:** large language models, grammatical control, verb tense, aspect, concept steering

**Relevance Score:** 8

**TL;DR:** This paper explores how large language models encode syntactic knowledge, specifically focusing on verb tense and aspect. It demonstrates causal control over these grammatical features and identifies crucial factors for effective steering during text generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how large language models internally encode their syntactic knowledge and to enhance the control over their grammatical features during text generation.

**Method:** The study uses linear discriminant analysis to identify distinct directions in residual space related to verb tense and aspect, and demonstrates causal control through concept steering in generation tasks.

**Key Contributions:**

	1. Identifies orthogonal directions in the residual space for verb tense and aspect in LLMs.
	2. Demonstrates causal control over grammatical features through concept steering.
	3. Investigates critical parameters affecting steering effectiveness during text generation.

**Result:** The researchers found that steering strength, location, and duration significantly influence the effectiveness of controlling grammatical features and reducing undesired side effects during multi-token generation.

**Limitations:** The paper primarily focuses on two specific grammatical aspects and may not generalize to all syntactic features or languages.

**Conclusion:** Models encode grammatical features like tense and aspect in a structurally organized manner akin to human language, but effective control necessitates careful tuning and optimization due to sensitivity to various factors.

**Abstract:** Large language models (LLMs) are able to generate grammatically well-formed text, but how do they encode their syntactic knowledge internally? While prior work has focused largely on binary grammatical contrasts, in this work, we study the representation and control of two multidimensional hierarchical grammar phenomena - verb tense and aspect - and for each, identify distinct, orthogonal directions in residual space using linear discriminant analysis. Next, we demonstrate causal control over both grammatical features through concept steering across three generation tasks. Then, we use these identified features in a case study to investigate factors influencing effective steering in multi-token generation. We find that steering strength, location, and duration are crucial parameters for reducing undesirable side effects such as topic shift and degeneration. Our findings suggest that models encode tense and aspect in structurally organized, human-like ways, but effective control of such features during generation is sensitive to multiple factors and requires manual tuning or automated optimization.

</details>


### [138] [SENSE models: an open source solution for multilingual and multimodal semantic-based tasks](https://arxiv.org/abs/2509.12093)

*Salima Mdhaffar, Haroun Elleuch, Chaimae Chellaf, Ha Nguyen, Yannick Estève*

**Main category:** cs.CL

**Keywords:** SENSE, speech processing, multilingual, machine learning, natural language processing

**Relevance Score:** 5

**TL;DR:** This paper presents SENSE, a model for aligning speech and text encoders using a teacher-student framework, achieving competitive performance on multilingual tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved alignment of speech and text representations in multilingual contexts.

**Method:** Introducing SENSE, an open-source model that aligns self-supervised speech encoders with language-agnostic text embeddings through a teacher-student framework.

**Key Contributions:**

	1. Development of SENSE, an open-source multilingual alignment model
	2. Integration of SENSE into the SpeechBrain toolkit
	3. Competitive performance in multilingual semantic tasks

**Result:** SENSE achieves competitive performance on multilingual and multimodal semantic tasks compared to existing models.

**Limitations:** 

**Conclusion:** The findings provide insights into semantic capture in aligned speech encoders, highlighting improvements over the previous SAMU-XLSR methodology.

**Abstract:** This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt), an open-source solution inspired by the SAMU-XLSR framework and conceptually similar to Meta AI's SONAR models. These approaches rely on a teacher-student framework to align a self-supervised speech encoder with the language-agnostic continuous representations of a text encoder at the utterance level. We describe how the original SAMU-XLSR method has been updated by selecting a stronger teacher text model and a better initial speech encoder. The source code for training and using SENSE models has been integrated into the SpeechBrain toolkit, and the first SENSE model we trained has been publicly released. We report experimental results on multilingual and multimodal semantic tasks, where our SENSE model achieves highly competitive performance. Finally, this study offers new insights into how semantics are captured in such semantically aligned speech encoders.

</details>


### [139] [Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities](https://arxiv.org/abs/2509.12098)

*Payam Latifi*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, NLP tools, Large Language Models, F1-score, Contextual understanding

**Relevance Score:** 8

**TL;DR:** This study benchmarks Named Entity Recognition (NER) performance of three traditional NLP tools and three LLMs on a small dataset, finding LLMs generally outperform traditional tools, but with some exceptions.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the NER capabilities of different NLP systems and understand their strengths in recognizing context-sensitive entities.

**Method:** The performance of six systems (NLTK, spaCy, Stanza, Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) was assessed on a benchmark dataset of 119 tokens across five entity types, using F1-score for evaluation against a gold standard dataset.

**Key Contributions:**

	1. Benchmarking NER performance across traditional NLP tools and LLMs
	2. Identification of strengths and weaknesses of each system
	3. Contributions to model selection strategies in NER tasks

**Result:** LLMs outperformed conventional tools in recognizing context-sensitive entities, with Gemini achieving the highest F1-score, but traditional tools like Stanza showed better consistency on structured tags like LOCATION and DATE.

**Limitations:** The study is based on a small-scale dataset, which may limit the generalizability of the findings.

**Conclusion:** While LLMs provide enhanced contextual understanding, traditional tools remain competitive for specific tasks, suggesting that the choice of model should depend on the specific requirements of the NER task.

**Abstract:** This pilot study presents a small-scale but carefully annotated benchmark of Named Entity Recognition (NER) performance across six systems: three non-LLM NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models (LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119 tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME). We evaluated each system's output against the manually annotated gold standard dataset using F1-score. The results show that LLMs generally outperform conventional tools in recognizing context-sensitive entities like person names, with Gemini achieving the highest average F1-score. However, traditional systems like Stanza demonstrate greater consistency in structured tags such as LOCATION and DATE. We also observed variability among LLMs, particularly in handling temporal expressions and multi-word organizations. Our findings highlight that while LLMs offer improved contextual understanding, traditional tools remain competitive in specific tasks, informing model selection.

</details>


### [140] [In-domain SSL pre-training and streaming ASR](https://arxiv.org/abs/2509.12101)

*Jarod Duret, Salima Mdhaffar, Gaëlle Laperrière, Ryan Whetten, Audrey Galametz, Catherine Kobus, Marion-Cécile Martin, Jo Oleiwan, Yannick Estève*

**Main category:** cs.CL

**Keywords:** Air Traffic Control, ASR, self-supervised learning, low-latency inference, safety-critical applications

**Relevance Score:** 2

**TL;DR:** This study explores domain-specific self-supervised pre-training for ASR in Air Traffic Control, showing improvements in performance compared to general models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of ASR systems in Air Traffic Control environments through specialized pre-training techniques.

**Method:** Training BEST-RQ models on 4.5k hours of unlabeled ATC data followed by fine-tuning with a smaller supervised ATC dataset. Introducing chunked attention and dynamic convolutions for low-latency inference.

**Key Contributions:**

	1. New self-supervised learning models specifically designed for ATC environments.
	2. Demonstrated low-latency processing using chunked attention and dynamic convolutions.
	3. Significant reduction in word error rates compared to state-of-the-art models.

**Result:** Domain-adapted pre-training significantly reduced word error rates on ATC benchmarks compared to general speech encoders. The streaming approach improved performance under tighter latency constraints.

**Limitations:** 

**Conclusion:** Specializing self-supervised learning representations for ATC data provides a viable route to improving ASR accuracy and efficiency in critical aviation applications.

**Abstract:** In this study, we investigate the benefits of domain-specific self-supervised pre-training for both offline and streaming ASR in Air Traffic Control (ATC) environments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then fine-tune on a smaller supervised ATC set. To enable real-time processing, we propose using chunked attention and dynamic convolutions, ensuring low-latency inference. We compare these in-domain SSL models against state-of-the-art, general-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show that domain-adapted pre-training substantially improves performance on standard ATC benchmarks, significantly reducing word error rates when compared to models trained on broad speech corpora. Furthermore, the proposed streaming approach further improves word error rate under tighter latency constraints, making it particularly suitable for safety-critical aviation applications. These findings highlight that specializing SSL representations for ATC data is a practical path toward more accurate and efficient ASR systems in real-world operational settings.

</details>


### [141] [GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models](https://arxiv.org/abs/2509.12108)

*Min Zeng, Jinfei Sun, Xueyou Luo, Caiquan Liu, Shiqi Zhang, Li Xie, Xiaoxin Chen*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Supervised Fine-Tuning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The GTA framework combines supervised fine-tuning and reinforcement learning for improved efficiency and performance in NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current reinforcement learning and supervised fine-tuning methods in natural language processing.

**Method:** The model produces a provisional guess optimized through cross-entropy loss, reflects on this guess, and generates the final answer, utilizing RL rewards for both output and structure.

**Key Contributions:**

	1. Introduction of the Guess-Think-Answer framework.
	2. Combining efficiency of SFT with capability of RL in NLP tasks.
	3. Using loss masking and gradient constraints to mitigate conflicts.

**Result:** GTA framework achieves faster convergence than pure RL and higher performance than pure SFT across four text classification benchmarks.

**Limitations:** 

**Conclusion:** GTA significantly accelerates convergence and outperforms both standalone SFT and RL methods.

**Abstract:** In natural language processing tasks, pure reinforcement learning (RL) fine-tuning methods often suffer from inefficient exploration and slow convergence; while supervised fine-tuning (SFT) methods, although efficient in training, have limited performance ceiling and less solid theoretical foundation compared to RL. To address efficiency-capability trade-off, we propose the Guess-Think-Answer (GTA) framework that combines the efficiency of SFT with the capability gains of RL in a unified training paradigm. GTA works by having the model first produce a provisional guess (optimized via cross-entropy loss), then reflect on this guess before generating the final answer, with RL rewards shaping both the final output and the format of the entire GTA structure. This hybrid approach achieves both faster convergence than pure RL and higher performance ceiling than pure SFT. To mitigate gradient conflicts between the two training signals, we employ loss masking and gradient constraints. Empirical results on four text classification benchmarks demonstrate that GTA substantially accelerates convergence while outperforming both standalone SFT and RL baselines.

</details>


### [142] [CBP-Tuning: Efficient Local Customization for Black-box Large Language Models](https://arxiv.org/abs/2509.12112)

*Jiaxuan Zhao, Naibin Gu, Yuchen Feng, Xiyu Liu, Peng Fu, Zheng Lin, Weiping Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Customized Tuning, Prompt Engineering, Privacy Preservation, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper presents a framework called Customized Black-box Prompt Tuning (CBP-Tuning) that enables efficient local customization of large language models while maintaining user privacy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of cloud-based LLMs in terms of personalization and privacy.

**Method:** The proposed framework has a two-stage design: a server-side prompt generator captures domain-specific capabilities, and a user-side gradient-free optimization tailors soft prompts for tasks without exposing private data.

**Key Contributions:**

	1. Introducing CBP-Tuning framework for LLM customization
	2. Bidirectional privacy preservation
	3. Evaluated performance in diverse domains showing advantages over traditional methods

**Result:** CBP-Tuning achieves effective adaptation with superior performance in commonsense reasoning, medical, and financial domains compared to existing baselines.

**Limitations:** 

**Conclusion:** The framework provides a solution for scalable model customization while enhancing privacy preservation.

**Abstract:** The high costs of customizing large language models (LLMs) fundamentally limit their adaptability to user-specific needs. Consequently, LLMs are increasingly offered as cloud-based services, a paradigm that introduces critical limitations: providers struggle to support personalized customization at scale, while users face privacy risks when exposing sensitive data. To address this dual challenge, we propose Customized Black-box Prompt Tuning (CBP-Tuning), a novel framework that facilitates efficient local customization while preserving bidirectional privacy. Specifically, we design a two-stage framework: (1) a prompt generator trained on the server-side to capture domain-specific and task-agnostic capabilities, and (2) user-side gradient-free optimization that tailors soft prompts for individual tasks. This approach eliminates the need for users to access model weights or upload private data, requiring only a single customized vector per task while achieving effective adaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense reasoning, medical and financial domain settings demonstrates superior performance compared to baselines, showcasing its advantages in task-agnostic processing and privacy preservation.

</details>


### [143] [XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models](https://arxiv.org/abs/2509.12130)

*Ariana Sahitaj, Jiaao Li, Pia Wenzel Neves, Fedor Splitt, Premtim Sahitaj, Charlott Jakob, Veronika Solopova, Vera Schmitt*

**Main category:** cs.CL

**Keywords:** multilingual, subjectivity detection, transformer encoders, zero-shot prompting, large language models

**Relevance Score:** 8

**TL;DR:** The paper evaluates multilingual subjectivity detection using supervised fine-tuning of transformer models and zero-shot prompting with LLMs, achieving top results in certain tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance multilingual subjectivity detection by evaluating both supervised fine-tuning of transformer encoders and zero-shot techniques using large language models.

**Method:** Supervised fine-tuning of models like EuroBERT, XLM-RoBERTa, and German-BERT was performed on monolingual and machine-translated data, along with zero-shot prompting using LLMs o3-mini and gpt-4.1-mini.

**Key Contributions:**

	1. First place achieved in the Italian monolingual subtask with advanced models.
	2. Demonstrated competitive performance in multilingual tasks with existing transformer models.
	3. Highlighted the challenges of low-resource language generalization in subjectivity detection.

**Result:** The Annotation Approach secured 1st place in the Italian subtask with an F_1 score of 0.8104. The fine-tuned XLM-RoBERTa model achieved 3rd place in Romanian with an F_1 score of 0.7917. Performance improvements were noted across several languages, though challenges remained in low-resource settings.

**Limitations:** Performance in the Ukrainian and Polish zero-shot settings was below the respective baselines, indicating difficulties in low-resource languages.

**Conclusion:** The study demonstrates the effectiveness of combining supervised learning and zero-shot prompting for multilingual subjectivity detection tasks, while highlighting challenges in certain cross-lingual scenarios.

**Abstract:** This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared task on multilingual subjectivity detection. We evaluate two approaches: (1) supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and German-BERT, on monolingual and machine-translated training data; and (2) zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and Perspective (comparative reasoning). The Annotation Approach achieves 1st place in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the baseline of 0.6461. The same model also performs reliably in the multilingual task and improves over the baseline in Greek. For German, a German-BERT model fine-tuned on translated training data from typologically related languages yields competitive performance over the baseline. In contrast, performance in the Ukrainian and Polish zero-shot settings falls slightly below the respective baselines, reflecting the challenge of generalization in low-resource cross-lingual scenarios.

</details>


### [144] [Pun Unintended: LLMs and the Illusion of Humor Understanding](https://arxiv.org/abs/2509.12158)

*Alessandro Zangari, Matteo Marcuzzo, Andrea Albarelli, Mohammad Taher Pilehvar, Jose Camacho-Collados*

**Main category:** cs.CL

**Keywords:** puns, large language models, pun detection, human evaluation, robustness challenges

**Relevance Score:** 6

**TL;DR:** This paper analyzes the limitations of LLMs in detecting and understanding puns, presenting new benchmarks and evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the shortcomings of LLMs in comprehending puns, which are complex forms of wordplay.

**Method:** The study involves reformulating existing pun benchmarks and conducting human evaluations of recent LLMs on these benchmarks.

**Key Contributions:**

	1. Comprehensive and nuanced pun detection benchmarks created for evaluation of LLMs
	2. Human evaluation of multiple recent LLMs on pun detection
	3. Analysis of robustness challenges faced by LLMs in understanding puns

**Result:** The analysis shows that subtle changes in puns can lead to significant misunderstandings by LLMs, highlighting their shallow understanding of nuanced language.

**Limitations:** The study primarily focuses on puns and may not generalize to other forms of humor or different types of wordplay.

**Conclusion:** Enhanced benchmarks and human evaluation are necessary for improving pun detection in LLMs, revealing significant robustness challenges in current models.

**Abstract:** Puns are a form of humorous wordplay that exploits polysemy and phonetic similarity. While LLMs have shown promise in detecting puns, we show in this paper that their understanding often remains shallow, lacking the nuanced grasp typical of human interpretation. By systematically analyzing and reformulating existing pun benchmarks, we demonstrate how subtle changes in puns are sufficient to mislead LLMs. Our contributions include comprehensive and nuanced pun detection benchmarks, human evaluation across recent LLMs, and an analysis of the robustness challenges these models face in processing puns.

</details>


### [145] [RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing](https://arxiv.org/abs/2509.12168)

*Timothy Rupprecht, Enfu Nan, Arash Akbari, Arman Akbari, Lei Lu, Priyanka Maan, Sean Duffy, Pu Zhao, Yumei He, David Kaeli, Yanzhi Wang*

**Main category:** cs.CL

**Keywords:** Large language models, Role-playing, Retrieval-Augmented Generation, Human-aligned AI, Few-shot learning

**Relevance Score:** 9

**TL;DR:** The paper proposes a new prompting framework, RAGs-to-Riches, to improve LLM role-playing by leveraging text retrieval and curated reference demonstrations, resulting in more authentic interactions and better character maintenance during hostile user engagements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues of trust and well-being in high-stakes domains by enhancing the performance of LLMs when role-playing through a novel prompting framework.

**Method:** The study reformulates LLM role-playing as a text retrieval problem and introduces a prompting framework called RAGs-to-Riches, which utilizes curated reference demonstrations.

**Key Contributions:**

	1. Introduction of the RAGs-to-Riches prompting framework
	2. Development of two novel token-level ROUGE metrics: IOO and IOR
	3. Demonstrated increased token utilization from reference demonstrations in hostile interactions

**Result:** LLMs using the proposed framework showed a 35% increase in token utilization from reference demonstrations during hostile user interactions, leading to more authentic role-playing and better character maintenance compared to zero-shot and ICL methods.

**Limitations:** 

**Conclusion:** The RAGs-to-Riches framework presents a scalable approach that enhances LLM role-playing by maintaining character integrity and improving user trust in various applications.

**Abstract:** Role-playing Large language models (LLMs) are increasingly deployed in high-stakes domains such as healthcare, education, and governance, where failures can directly impact user trust and well-being. A cost effective paradigm for LLM role-playing is few-shot learning, but existing approaches often cause models to break character in unexpected and potentially harmful ways, especially when interacting with hostile users. Inspired by Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a text retrieval problem and propose a new prompting framework called RAGs-to-Riches, which leverages curated reference demonstrations to condition LLM responses. We evaluate our framework with LLM-as-a-judge preference voting and introduce two novel token-level ROUGE metrics: Intersection over Output (IOO) to quantity how much an LLM improvises and Intersection over References (IOR) to measure few-shot demonstrations utilization rate during the evaluation tasks. When simulating interactions with a hostile user, our prompting strategy incorporates in its responses during inference an average of 35% more tokens from the reference demonstrations. As a result, across 453 role-playing interactions, our models are consistently judged as being more authentic, and remain in-character more often than zero-shot and in-context Learning (ICL) methods. Our method presents a scalable strategy for building robust, human-aligned LLM role-playing frameworks.

</details>


### [146] [Preservation of Language Understanding Capabilities in Speech-aware Large Language Models](https://arxiv.org/abs/2509.12171)

*Marek Kubis, Paweł Skórzewski, Iwona Christop, Mateusz Czyżnikiewicz, Jakub Kubiak, Łukasz Bondaruk, Marcin Lewandowski*

**Main category:** cs.CL

**Keywords:** C3T, speech-aware models, language understanding, benchmark, fairness

**Relevance Score:** 7

**TL;DR:** C3T benchmark assesses speech-aware large language models' performance on language understanding through speech input.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how well language understanding is maintained in LLMs when interacting through speech input.

**Method:** The benchmark combines textual tasks with a voice cloning text-to-speech model to measure performance.

**Key Contributions:**

	1. Introduction of C3T benchmark for speech-aware LLMs
	2. Metrics for fairness across different speaker categories
	3. Robustness assessment across text and speech modalities

**Result:** C3T provides quantifiable metrics on model fairness for speaker categories and robustness in text and speech input.

**Limitations:** 

**Conclusion:** The benchmark allows for a more comprehensive evaluation of speech-aware models, identifying areas for improvement.

**Abstract:** The paper presents C3T (Cross-modal Capabilities Conservation Test), a new benchmark for assessing the performance of speech-aware large language models. The benchmark utilizes textual tasks and a voice cloning text-to-speech model to quantify the extent to which language understanding capabilities are preserved when the model is accessed via speech input. C3T quantifies the fairness of the model for different categories of speakers and its robustness across text and speech modalities.

</details>


### [147] [Understanding Emergent In-Context Learning from a Kernel Regression Perspective](https://arxiv.org/abs/2305.12766)

*Chi Han, Ziqi Wang, Han Zhao, Heng Ji*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, kernel regression, Bayesian inference, transformer models

**Relevance Score:** 9

**TL;DR:** This paper investigates how transformer-based language models perform in-context learning using a kernel-regression perspective, showing that ICL behaviors can be understood through Bayesian inference and kernel regression.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the mechanisms behind in-context learning (ICL) in large language models (LLMs) as a new paradigm in transfer learning.

**Method:** The paper proves that Bayesian inference on in-context prompts can be understood as kernel regression when the number of demonstrations increases, and empirically investigates the in-context behaviors of LLMs.

**Key Contributions:**

	1. Introduces a kernel-regression perspective for understanding ICL in LLMs.
	2. Proves the asymptotic relationship between Bayesian inference and kernel regression in ICL contexts.
	3. Empirically links LLM behaviors during ICL to the principles of kernel regression.

**Result:** The study finds that the attention and hidden features in LLMs align with kernel regression behaviors during ICL, providing insights into why certain sample retrieval methods and output formats influence performance.

**Limitations:** 

**Conclusion:** The findings enhance our understanding of various ICL phenomena and offer a theoretical grounding for the observed behaviors in LLM applications.

**Abstract:** Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing a kernel-regression perspective of understanding LLMs' ICL bahaviors when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demonstrations grows. Then, we empirically investigate the in-context behaviors of language models. We find that during ICL, the attention and hidden features in LLMs match the behaviors of a kernel regression. Finally, our theory provides insights into multiple phenomena observed in the ICL field: why retrieving demonstrative samples similar to test samples can help, why ICL performance is sensitive to the output formats, and why ICL accuracy benefits from selecting in-distribution and representative samples. Code and resources are publicly available at https://github.com/Glaciohound/Explain-ICL-As-Kernel-Regression.

</details>


### [148] [Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models](https://arxiv.org/abs/2307.06979)

*Arman Sakif Chowdhury, G. M. Shahariar, Ahammed Tarik Aziz, Syed Mohibul Alam, Md. Azad Sheikh, Tanveer Ahmed Belal*

**Main category:** cs.CL

**Keywords:** fake news detection, Bengali language, summarization, augmentation techniques, pre-trained language models

**Relevance Score:** 4

**TL;DR:** This paper proposes a methodology for detecting fake news in Bengali, utilizing summarization and augmentation techniques with various pre-trained language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The detection of fake news in low resource languages like Bengali has received limited attention, necessitating new approaches to tackle this issue.

**Method:** The study employs four distinct approaches, translating English articles and using summarization and augmentation techniques with five pre-trained language models to classify Bengali fake news.

**Key Contributions:**

	1. Development of a fake news detection methodology for Bengali using summarization and augmentation techniques.
	2. Utilization of multiple pre-trained language models to enhance the classification process.
	3. Presentation of high accuracy results across multiple test datasets.

**Result:** The BanglaBERT Base model achieved 96% accuracy on the first test dataset, the summarized augmented BanglaBERT model reached 97% on the second dataset, and the mBERT Base model scored 86% on the third dataset for generalization evaluation.

**Limitations:** Limited focus on languages other than Bengali and potential challenges in applying the methodology to different cultural contexts.

**Conclusion:** The effectiveness of summarization and augmentation techniques in Bengali fake news detection is demonstrated through extensive experimentation.

**Abstract:** With the rise of social media and online news sources, fake news has become a significant issue globally. However, the detection of fake news in low resource languages like Bengali has received limited attention in research. In this paper, we propose a methodology consisting of four distinct approaches to classify fake news articles in Bengali using summarization and augmentation techniques with five pre-trained language models. Our approach includes translating English news articles and using augmentation techniques to curb the deficit of fake news articles. Our research also focused on summarizing the news to tackle the token length limitation of BERT based models. Through extensive experimentation and rigorous evaluation, we show the effectiveness of summarization and augmentation in the case of Bengali fake news detection. We evaluated our models using three separate test datasets. The BanglaBERT Base model, when combined with augmentation techniques, achieved an impressive accuracy of 96% on the first test dataset. On the second test dataset, the BanglaBERT model, trained with summarized augmented news articles achieved 97% accuracy. Lastly, the mBERT Base model achieved an accuracy of 86% on the third test dataset which was reserved for generalization performance evaluation. The datasets and implementations are available at https://github.com/arman-sakif/Bengali-Fake-News-Detection

</details>


### [149] [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/abs/2309.01219)

*Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Chen Xu, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, Shuming Shi*

**Main category:** cs.CL

**Keywords:** large language models, hallucination, detection, mitigation, taxonomy

**Relevance Score:** 9

**TL;DR:** This paper surveys hallucination in large language models, discussing detection, explanation, and mitigation strategies, and providing taxonomies and benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the reliability issues posed by hallucination in LLMs, which can generate misleading or incorrect content.

**Method:** The paper surveys existing literature, presents taxonomies of hallucination phenomena, and evaluates current mitigation approaches.

**Key Contributions:**

	1. Survey of hallucination detection and mitigation methods
	2. Taxonomies of LLM hallucination phenomena
	3. Evaluation benchmarks for assessing hallucination in LLMs

**Result:** The paper identifies various types of hallucination in LLMs and discusses the effectiveness of different detection and mitigation techniques.

**Limitations:** The paper is a work in progress and may lack comprehensive analysis of all existing methods.

**Conclusion:** While progress has been made in understanding and addressing LLM hallucinations, further research is needed to enhance reliability in practical applications.

**Abstract:** While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.

</details>


### [150] [LML: A Novel Lexicon for the Moral Foundation of Liberty](https://arxiv.org/abs/2407.11862)

*Oscar Araque, Lorenzo Gatti, Sergio Consoli, Kyriaki Kalimeri*

**Main category:** cs.CL

**Keywords:** liberty, lexicon, social issues, inference systems, word embeddings

**Relevance Score:** 3

**TL;DR:** The paper proposes a novel Liberty lexicon aimed at enhancing inference systems related to social issues, evaluated on over 3,000 annotated data points.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The moral value of liberty impacts stances on controversial issues, necessitating a robust understanding and representation of such concepts in inference systems.

**Method:** A novel Liberty lexicon was developed and evaluated using word embedding similarity and compositional semantics on a dataset of over 3,000 annotated examples.

**Key Contributions:**

	1. Enrichment of liberty annotations
	2. Development of a robust liberty lexicon
	3. Revelation of complexity in expressions related to liberty across platforms

**Result:** The evaluation resulted in a combined lexicon that effectively incorporates information from various lexicons, demonstrating improved representations in learning systems.

**Limitations:** 

**Conclusion:** The findings indicate that addressing the complexity of liberty-related expressions requires combined knowledge approaches, enhancing system performance.

**Abstract:** The moral value of liberty is a central concept in our inference system when it comes to taking a stance towards controversial social issues such as vaccine hesitancy, climate change, or the right to abortion. Here, we propose a novel Liberty lexicon evaluated on more than 3,000 manually annotated data both in in- and out-of-domain scenarios. As a result of this evaluation, we produce a combined lexicon that constitutes the main outcome of this work. This final lexicon incorporates information from an ensemble of lexicons that have been generated using word embedding similarity (WE) and compositional semantics (CS). Our key contributions include enriching the liberty annotations, developing a robust liberty lexicon for broader application, and revealing the complexity of expressions related to liberty across different platforms. Through the evaluation, we show that the difficulty of the task calls for designing approaches that combine knowledge, in an effort of improving the representations of learning systems.

</details>


### [151] [Surveying the Landscape of Image Captioning Evaluation: A Comprehensive Taxonomy, Trends and Metrics Analysis](https://arxiv.org/abs/2408.04909)

*Uri Berger, Gabriel Stanovsky, Omri Abend, Lea Frermann*

**Main category:** cs.CL

**Keywords:** image captioning, evaluation metrics, taxonomies, human ratings, ensemble methods

**Relevance Score:** 4

**TL;DR:** This paper surveys over 70 image captioning metrics, analyzing their usage and correlation with human ratings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for a comprehensive evaluation of image captioning metrics, ensuring users can select the most appropriate one for their tasks.

**Method:** The authors conducted a survey and taxonomy of 70 different metrics, analyzing their application in numerous studies to identify common usage patterns and limitations. They proposed a new ensemble method, EnsembEval, to improve correlation with human ratings.

**Key Contributions:**

	1. First comprehensive survey of image captioning metrics
	2. Identification of common reliance on a few weakly correlated metrics
	3. Development of a new ensemble method (EnsembEval) to improve metric evaluation

**Result:** The study revealed that most papers depend on only five metrics, which are weakly correlated with human judgment. The new ensemble method showed improved correlation across multiple datasets.

**Limitations:** The study primarily focuses on correlation and does not delve deeply into the qualitative aspects of generated captions.

**Conclusion:** There is significant potential for improving image captioning evaluation by utilizing a diverse range of metrics, as demonstrated by the EnsembEval method.

**Abstract:** The task of image captioning has recently been gaining popularity, and with it the complex task of evaluating the quality of image captioning models. In this work, we present the first survey and taxonomy of over 70 different image captioning metrics and their usage in hundreds of papers, specifically designed to help users select the most suitable metric for their needs. We find that despite the diversity of proposed metrics, the vast majority of studies rely on only five popular metrics, which we show to be weakly correlated with human ratings. We hypothesize that combining a diverse set of metrics can enhance correlation with human ratings. As an initial step, we demonstrate that a linear regression-based ensemble method, which we call EnsembEval, trained on one human ratings dataset, achieves improved correlation across five additional datasets, showing there is a lot of room for improvement by leveraging a diverse set of metrics.

</details>


### [152] [Can Advanced LLMs Coach Smaller LLMs? Knowledge Distillation for Goal-Oriented Dialogs](https://arxiv.org/abs/2408.07238)

*Tong Wang, K. Sudhir, Dat Hong*

**Main category:** cs.CL

**Keywords:** LLM, guidance retrieval, knowledge distillation, dialog systems, customer service

**Relevance Score:** 8

**TL;DR:** Introducing GER, a framework for coaching lower-performance LLMs using a high-performance teacher LLM for goal-oriented dialogs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the trade-off between performance, control, and cost in deploying LLMs for goal-oriented dialogs, such as customer service.

**Method:** The GER framework uses a high-performance teacher LLM to distill knowledge into a lower-performance student LLM through prompt-based coaching without modifying the student's parameters, using a structured library for guidance retrieval.

**Key Contributions:**

	1. Framework for prompt-based knowledge distillation between LLMs.
	2. Structured library for scenario-guidance pairs.
	3. Extensibility to coach human service agents.

**Result:** Experiments demonstrate that GER's coaching approach outperforms example output-based fine-tuning and non-customized guidance, generalizing across other contexts and student models.

**Limitations:** 

**Conclusion:** GER provides a modular and effective solution for enhancing LLM performance in goal-oriented dialogs while allowing for easy auditing and updating of guidance libraries.

**Abstract:** Enterprises deploying LLMs for goal-oriented dialogs, such as customer service, face a critical trade-off between performance, control, and cost. Proprietary models like GPT-4 offer strong performance but are costly and cannot be self-hosted, raising security and privacy concerns. Open-source alternatives offer flexibility and lower token costs but lag in performance. We introduce Guidance Elicitation and Retrieval (GER), a prompt-based knowledge distillation framework where a high-performance teacher LLM coaches a lower-performance student without modifying the student's parameters. GER extracts tactical guidance for a wide range of dialog scenarios from the teacher and stores these scenario-guidance pairs in a structured library. At inference time, the student retrieves the relevant guidance and integrates it into its prompt. While GER training can be bootstrapped entirely with synthetic data, its modular design lets it seamlessly augment the synthetic data with human conversational logs. In addition, the modular design enables easy auditing and updating of the guidance library as new scenarios and constraints emerge. Experiments show GER's guidance-based coaching outperforms both example output based fine-tuning and non-customized guidance baselines, and generalizes across other contexts and student models. The GER framework is potentially extensible to coach human service agents.

</details>


### [153] [GP-GPT: Large Language Model for Gene-Phenotype Mapping](https://arxiv.org/abs/2409.09825)

*Yanjun Lyu, Zihao Wu, Lu Zhang, Jing Zhang, Yiwei Li, Wei Ruan, Zhengliang Liu, Xiang Li, Rongjie Liu, Chao Huang, Wentao Li, Tianming Liu, Dajiang Zhu*

**Main category:** cs.CL

**Keywords:** large language models, genomics, bioinformatics, medical genetics, natural language processing

**Relevance Score:** 9

**TL;DR:** GP-GPT is a specialized large language model designed for genetic-phenotype knowledge representation and genomics relation analysis, fine-tuned on a large corpus from biomedical domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the adaptation of large language models (LLMs) in the bioinformatics field, specifically for handling complex genomics data.

**Method:** GP-GPT was fine-tuned in two stages on a corpus of over 3,000,000 terms related to genomics, proteomics, and medical genetics, sourced from validated datasets and publications.

**Key Contributions:**

	1. Introduction of GP-GPT as a specialized LLM for genetic-phenotype knowledge representation.
	2. Demonstration of GP-GPT's proficiency in genomics analysis tasks.
	3. Comparison showcasing GP-GPT's superiority over existing LLMs in specific biomedical tasks.

**Result:** GP-GPT outperforms existing state-of-the-art LLMs, such as Llama2, Llama3, and GPT-4, in various domain-specific tasks including genomics information retrieval and relationship determination.

**Limitations:** 

**Conclusion:** GP-GPT shows potential for advancing research in genetic diseases and improving analysis in genomics and medical genetics fields.

**Abstract:** Pre-trained large language models(LLMs) have attracted increasing attention in biomedical domains due to their success in natural language processing. However, the complex traits and heterogeneity of multi-sources genomics data pose significant challenges when adapting these models to the bioinformatics and biomedical field. To address these challenges, we present GP-GPT, the first specialized large language model for genetic-phenotype knowledge representation and genomics relation analysis. Our model is fine-tuned in two stages on a comprehensive corpus composed of over 3,000,000 terms in genomics, proteomics, and medical genetics, derived from multiple large-scale validated datasets and scientific publications. GP-GPT demonstrates proficiency in accurately retrieving medical genetics information and performing common genomics analysis tasks, such as genomics information retrieval and relationship determination. Comparative experiments across domain-specific tasks reveal that GP-GPT outperforms state-of-the-art LLMs, including Llama2, Llama3 and GPT-4. These results highlight GP-GPT's potential to enhance genetic disease relation research and facilitate accurate and efficient analysis in the fields of genomics and medical genetics. Our investigation demonstrated the subtle changes of bio-factor entities' representations in the GP-GPT, which suggested the opportunities for the application of LLMs to advancing gene-phenotype research.

</details>


### [154] [Revealing the Inherent Instructability of Pre-Trained Language Models](https://arxiv.org/abs/2410.02465)

*Seokhyun An, Minji Kim, Hyounghun Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Response Tuning, Instruction Tuning, Machine Learning, Safe Query Recognition

**Relevance Score:** 8

**TL;DR:** The paper explores Response Tuning (RT), which is alternative to instruction tuning for large language models (LLMs), emphasizing response generation without explicit instruction-response pairs.

**Read time:** 32 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether pre-trained LLMs can independently understand and respond to instructions without the explicit instruction mappings used during instruction tuning.

**Method:** The study proposes Response Tuning, where models are trained solely on response data, allowing them to learn to generate appropriate responses to diverse instructions based on pre-acquired knowledge during pre-training.

**Key Contributions:**

	1. Introduction of Response Tuning (RT) for LLMs
	2. Demonstration of effective response generation capabilities without explicit instruction mappings
	3. Findings on safety policy recognition in LLM responses based on response data

**Result:** RT models successfully replicate the performance of instruction-tuned models by effectively responding to various instructions and demonstrating the ability to reject unsafe queries after being trained solely on response data.

**Limitations:** The study does not address the potential shortcomings of removing instruction mapping in complex instruction scenarios.

**Conclusion:** The findings reinforce the notion that pre-trained LLMs possess significant innate abilities, as they can learn to respond appropriately to instructions through alternative training methods.

**Abstract:** Instruction tuning -- supervised fine-tuning using instruction-response pairs -- is a key step in making pre-trained large language models (LLMs) instructable. Meanwhile, LLMs perform multitask learning during their pre-training, acquiring extensive knowledge and capabilities. We hypothesize that the pre-training stage can enable them to develop the ability to comprehend and address instructions. To verify this, we propose Response Tuning (RT), which removes the instruction and its corresponding mapping to the response from instruction tuning. Instead, it focuses solely on establishing a response distribution. Our experiments demonstrate that RT models, trained only on responses, can effectively respond to a wide range of instructions akin to their instruction-tuned counterparts. In addition, we observe that the models can recognize and reject unsafe queries after learning a safety policy only from the response data. Furthermore, we find that these observations extend to an in-context learning setting. These findings support our hypothesis, highlighting the extensive inherent capabilities of pre-trained LLMs.

</details>


### [155] [Evaluating Automatic Speech Recognition Systems for Korean Meteorological Experts](https://arxiv.org/abs/2410.18444)

*ChaeHun Park, Hojun Cho, Jaegul Choo*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Korean meteorology, natural language query systems, data augmentation, multilingual ASR

**Relevance Score:** 5

**TL;DR:** This paper investigates integrating ASR into natural language query systems for improving weather forecasting efficiency in Korea, focusing on unique challenges and solutions for the Korean language.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency of weather forecasting for Korean meteorologists using ASR systems that cater to the linguistic and terminological challenges in the Korean language.

**Method:** The authors constructed an evaluation dataset of spoken queries, assessed multilingual ASR models, and implemented a text-to-speech data augmentation method.

**Key Contributions:**

	1. Creation of a domain-specific dataset for Korean ASR
	2. Comprehensive evaluations of various ASR model configurations
	3. Implementation of a data augmentation technique to enhance ASR performance for specialized terminology

**Result:** Identified performance limitations of ASR models related to specialized terminology and demonstrated improvement in recognition accuracy for domain-specific terms through data augmentation.

**Limitations:** Limited to the Korean weather domain; findings may not generalize to other domains or languages.

**Conclusion:** The research provides a foundation for future advancements in ASR in the Korean weather domain, offering a novel dataset and effective techniques for ASR adaptation.

**Abstract:** This paper explores integrating Automatic Speech Recognition (ASR) into natural language query systems to improve weather forecasting efficiency for Korean meteorologists. We address challenges in developing ASR systems for the Korean weather domain, specifically specialized vocabulary and Korean linguistic intricacies. To tackle these issues, we constructed an evaluation dataset of spoken queries recorded by native Korean speakers. Using this dataset, we assessed various configurations of a multilingual ASR model family, identifying performance limitations related to domain-specific terminology. We then implemented a simple text-to-speech-based data augmentation method, which improved the recognition of specialized terms while maintaining general-domain performance. Our contributions include creating a domain-specific dataset, comprehensive ASR model evaluations, and an effective augmentation technique. We believe our work provides a foundation for future advancements in ASR for the Korean weather forecasting domain.

</details>


### [156] [Artificial intelligence contribution to translation industry: looking back and forward](https://arxiv.org/abs/2411.19855)

*Mohammed Q. Shormani*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Translation Industry, Machine Translation, Neural Models, Deep Learning

**Relevance Score:** 4

**TL;DR:** A comprehensive review of AI's impact on the translation industry from 1980-2024, analyzing over 13,000 articles and highlighting key themes and challenges.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To synthesize over 45 years of research on AI's contribution to the translation industry, identifying trends and future directions.

**Method:** Scientific analysis of 13,220 articles from WoS, Scopus, and Lens, focusing on scientometric and thematic review of selected relevant literature.

**Key Contributions:**

	1. Development of a comprehensive dataset over 45 years
	2. Identification of key trends and challenges in the translation sector
	3. Thematic review of key literature on AI and translation

**Result:** The analysis reveals key topics in AI translation research, such as machine translation, neural models, and issues with low-resource languages. It highlights the growing influence of AI in translation practices, particularly through Neural Networking Algorithms and Deep Language Models.

**Limitations:** The study does not address specific algorithms or implementation details of AI in translation, focusing instead on historical and thematic analysis.

**Conclusion:** While AI significantly enhances translation, notable challenges remain, particularly in effectively addressing low-resource and multi-dialectical languages, emphasizing the need for further research.

**Abstract:** This study provides a comprehensive analysis of artificial intelligence (AI) contribution to research in the translation industry (ACTI), synthesizing it over forty-five years from 1980-2024. 13220 articles were retrieved from three sources, namely WoS, Scopus, and Lens; 9836 were unique records, which were used for the analysis. I provided two types of analysis, viz., scientometric and thematic, focusing on Cluster, Subject categories, Keywords, Bursts, Centrality and Research Centers as for the former. For the latter, I provided a thematic review for 18 articles, selected purposefully from the articles involved, centering on purpose, approach, findings, and contribution to ACTI future directions. This study is significant for its valuable contribution to ACTI knowledge production over 45 years, emphasizing several trending issues and hotspots including Machine translation, Statistical machine translation, Low-resource language, Large language model, Arabic dialects, Translation quality, and Neural machine translation. The findings reveal that the more AI develops, the more it contributes to translation industry, as Neural Networking Algorithms have been incorporated and Deep Language Learning Models like ChatGPT have been launched. However, much rigorous research is still needed to overcome several problems encountering translation industry, specifically concerning low-resource, multi-dialectical and free word order languages, and cultural and religious registers.

</details>


### [157] [FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge Distillation for Question Answering](https://arxiv.org/abs/2412.07030)

*Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji*

**Main category:** cs.CL

**Keywords:** multimodal, question answering, dataset synthesis, machine learning, benchmarking

**Relevance Score:** 8

**TL;DR:** The paper introduces FM2DS, a framework for creating a high-quality dataset for multimodal multihop question answering (MMQA), addressing the limitations of current methods and datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of quality datasets for multimodal multihop question answering and enhance real-world applications involving long, multimodal content.

**Method:** A 5-stage pipeline for acquiring multimodal documents, generating high-level questions and answers, and validating them for quality.

**Key Contributions:**

	1. Introduction of FM2DS for synthesizing MMQA datasets
	2. Demonstrated superior performance of synthesized dataset over human-collected data
	3. Development of M2QA-Bench, the first benchmark for MMQA on long documents.

**Result:** Models trained on the FM2DS synthesized dataset outperformed those trained on human-collected data by an average of 1.9 in exact match (EM) score.

**Limitations:** 

**Conclusion:** The data synthesis method proposed can serve as a strong foundation for training and evaluating multimodal multihop question answering models.

**Abstract:** Multimodal multihop question answering (MMQA) requires reasoning over images and text from multiple sources. Despite advances in visual question answering, this multihop setting remains underexplored due to a lack of quality datasets. Existing methods focus on single-hop, single-modality, or short texts, limiting real-world applications like interpreting educational documents with long, multimodal content. To fill this gap, we introduce FM2DS, the first framework for creating a high-quality dataset for MMQA. Our approach consists of a 5-stage pipeline that involves acquiring relevant multimodal documents from Wikipedia, synthetically generating high-level questions and answers, and validating them through rigorous criteria to ensure data quality. We evaluate our methodology by training models on our synthesized dataset and testing on two benchmarks: MultimodalQA and WebQA. Our results demonstrate that, with an equal sample size, models trained on our synthesized data outperform those trained on human-collected data by 1.9 in exact match (EM) score on average. Additionally, we introduce M2QA-Bench with 1k samples, the first benchmark for MMQA on long documents, generated using FM2DS and refined by human annotators. We believe our data synthesis method will serve as a strong foundation for training and evaluating MMQA models.

</details>


### [158] [Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation](https://arxiv.org/abs/2412.14642)

*Jiatong Li, Junxian Li, Weida Wang, Yunqing Liu, Changmeng Zheng, Dongzhan Zhou, Xiao-yong Wei, Qing Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Molecule Discovery, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Introducing Speak-to-Structure (S^2-Bench), a benchmark for evaluating LLMs in open-domain molecule generation, addressing the gap in existing benchmarks focused on one-to-one mapping.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the creative potential of LLMs in molecule discovery rather than just their ability to retrieve predefined answers.

**Method:** Developing the S^2-Bench benchmark, designed for one-to-many relationships, with tasks including molecule editing, optimization, and customized generation; utilizing a large-scale instruction tuning dataset for training.

**Key Contributions:**

	1. Introduction of S^2-Bench for evaluating one-to-many LLM capabilities in molecule generation.
	2. Creation of OpenMolIns, an instruction tuning dataset for improved performance.
	3. Comprehensive evaluation of 28 LLMs, shifting the focus from pattern recall to molecular design.

**Result:** Llama-3.1-8B outperforms top LLMs like GPT-4o and Claude-3.5 on the S^2-Bench, demonstrating better molecular understanding and generation capabilities.

**Limitations:** 

**Conclusion:** The benchmark paves the way for more capable LLMs in natural language-driven molecule discovery, focusing on realistic molecular design.

**Abstract:** Recently, Large Language Models (LLMs) have shown great potential in natural language-driven molecule discovery. However, existing datasets and benchmarks for molecule-text alignment are predominantly built on a one-to-one mapping, measuring LLMs' ability to retrieve a single, pre-defined answer, rather than their creative potential to generate diverse, yet equally valid, molecular candidates. To address this critical gap, we propose Speak-to-Structure (S^2-Bench}), the first benchmark to evaluate LLMs in open-domain natural language-driven molecule generation. S^2-Bench is specifically designed for one-to-many relationships, challenging LLMs to demonstrate genuine molecular understanding and generation capabilities. Our benchmark includes three key tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and customized molecule generation (MolCustom), each probing a different aspect of molecule discovery. We also introduce OpenMolIns, a large-scale instruction tuning dataset that enables Llama-3.1-8B to surpass the most powerful LLMs like GPT-4o and Claude-3.5 on S^2-Bench. Our comprehensive evaluation of 28 LLMs shifts the focus from simple pattern recall to realistic molecular design, paving the way for more capable LLMs in natural language-driven molecule discovery.

</details>


### [159] [IOLBENCH: Benchmarking LLMs on Linguistic Reasoning](https://arxiv.org/abs/2501.04249)

*Satyam Goyal, Soham Dan*

**Main category:** cs.CL

**Keywords:** large language models, linguistic reasoning, IOLBENCH, benchmark, compositional generalization

**Relevance Score:** 9

**TL;DR:** This paper introduces IOLBENCH, a benchmark for assessing linguistic reasoning in LLMs using problems from the International Linguistics Olympiad, revealing the limitations of current models in handling linguistic complexity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and enhance the reasoning capabilities of large language models in handling structured linguistic tasks, thereby understanding their limitations and potential for human-like reasoning.

**Method:** We created IOLBENCH, a dataset of diverse linguistic problems testing syntax, morphology, phonology, and semantics, and conducted extensive benchmarking on various state-of-the-art large language models.

**Key Contributions:**

	1. Introduction of IOLBENCH, a novel linguistic reasoning benchmark for LLMs.
	2. Empirical findings showcasing the limitations of current state-of-the-art models in linguistic tasks.
	3. Insights into the necessity for developing models capable of more complex reasoning similar to humans.

**Result:** Benchmarking results reveal that even advanced models struggle significantly with tasks requiring compositional generalization and rule abstraction, indicating gaps in their linguistic reasoning abilities.

**Limitations:** Focused solely on linguistic reasoning; results may not generalize to other domains or broader AI reasoning tasks.

**Conclusion:** The paper offers insights into the reasoning capabilities of LLMs and encourages further research towards models that can perform human-like reasoning in linguistics.

**Abstract:** Despite the remarkable advancements and widespread applications of deep neural networks, their ability to perform reasoning tasks remains limited, particularly in domains requiring structured, abstract thought. In this paper, we investigate the linguistic reasoning capabilities of state-of-the-art large language models (LLMs) by introducing IOLBENCH, a novel benchmark derived from International Linguistics Olympiad (IOL) problems. This dataset encompasses diverse problems testing syntax, morphology, phonology, and semantics, all carefully designed to be self-contained and independent of external knowledge. These tasks challenge models to engage in metacognitive linguistic reasoning, requiring the deduction of linguistic rules and patterns from minimal examples. Through extensive benchmarking of leading LLMs, we find that even the most advanced models struggle to handle the intricacies of linguistic complexity, particularly in areas demanding compositional generalization and rule abstraction. Our analysis highlights both the strengths and persistent limitations of current models in linguistic problem-solving, offering valuable insights into their reasoning capabilities. By introducing IOLBENCH, we aim to foster further research into developing models capable of human-like reasoning, with broader implications for the fields of computational linguistics and artificial intelligence.

</details>


### [160] [Transformer-Based Multimodal Knowledge Graph Completion with Link-Aware Contexts](https://arxiv.org/abs/2501.15688)

*Haodi Ma, Dzmitry Kasinets, Daisy Zhe Wang*

**Main category:** cs.CL

**Keywords:** multimodal knowledge graphs, knowledge graph completion, Transformer models, vision-language models, cross-modal tasks

**Relevance Score:** 7

**TL;DR:** The paper presents a novel method for multimodal knowledge graph completion (MMKGC) by integrating Transformer-based knowledge graph embedding models with cross-modal context from pre-trained vision-language models (VLMs), achieving efficiency and competitive performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies in existing MMKGC approaches that rely heavily on traditional knowledge graph embedding models, which require extensive embeddings and struggle with cross-modal information integration.

**Method:** The proposed method integrates Transformer-based knowledge graph embedding models with cross-modal context generated by pre-trained vision-language models. It transforms visual information relevant to entities into textual sequences and frames the knowledge graph completion as a sequence-to-sequence task, fine-tuning the model accordingly.

**Key Contributions:**

	1. Introduces a method for integrating VLMs into MMKGC models.
	2. Reduces the model size significantly compared to traditional approaches.
	3. Achieves strong performance with minimal hyperparameter adjustments.

**Result:** The method significantly reduces model size compared to traditional KGE approaches and performs competitively across multiple large-scale datasets with minimal hyperparameter tuning.

**Limitations:** 

**Conclusion:** This integration of VLM-generated context into KGC represents a step forward in the effectiveness of MMKGC, making it more efficient and scalable for real-world applications.

**Abstract:** Multimodal knowledge graph completion (MMKGC) aims to predict missing links in multimodal knowledge graphs (MMKGs) by leveraging information from various modalities alongside structural data. Existing MMKGC approaches primarily extend traditional knowledge graph embedding (KGE) models, which often require creating an embedding for every entity. This results in large model sizes and inefficiencies in integrating multimodal information, particularly for real-world graphs. Meanwhile, Transformer-based models have demonstrated competitive performance in knowledge graph completion (KGC). However, their focus on single-modal knowledge limits their capacity to utilize cross-modal information. Recently, Large vision-language models (VLMs) have shown potential in cross-modal tasks but are constrained by the high cost of training. In this work, we propose a novel approach that integrates Transformer-based KGE models with cross-modal context generated by pre-trained VLMs, thereby extending their applicability to MMKGC. Specifically, we employ a pre-trained VLM to transform relevant visual information from entities and their neighbors into textual sequences. We then frame KGC as a sequence-to-sequence task, fine-tuning the model with the generated cross-modal context. This simple yet effective method significantly reduces model size compared to traditional KGE approaches while achieving competitive performance across multiple large-scale datasets with minimal hyperparameter tuning.

</details>


### [161] [From Personas to Talks: Revisiting the Impact of Personas on LLM-Synthesized Emotional Support Conversations](https://arxiv.org/abs/2502.11451)

*Shenghan Wu, Yimo Zhu, Wynne Hsu, Mong-Li Lee, Yang Deng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Emotional Support Conversations, Personas, Human-Computer Interaction, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper studies the influence of personas in Large Language Models for generating emotional support conversations, revealing that these traits enhance dialogue quality and empathetic responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how personas can improve the effectiveness and personalization of emotional support conversations generated by LLMs.

**Method:** Utilized established psychological frameworks to measure persona traits and incorporated them into LLM dialogues for emotional support scenarios.

**Key Contributions:**

	1. Establishment of psychological frameworks for persona traits in LLMs
	2. Evaluation of dialogue quality based on persona traits
	3. Demonstration of improved empathy in generated dialogues

**Result:** Found that LLMs can infer persona traits, which lead to shifts in emotionality and extraversion, thereby improving the quality and strategy distribution in dialogues.

**Limitations:** 

**Conclusion:** Persona-driven LLMs have the potential to create more personalized and effective emotional support dialogues, important for future AI-driven support systems.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has revolutionized the generation of emotional support conversations (ESC), offering scalable solutions with reduced costs and enhanced data privacy. This paper explores the role of personas in the creation of ESC by LLMs. Our research utilizes established psychological frameworks to measure and infuse persona traits into LLMs, which then generate dialogues in the emotional support scenario. We conduct extensive evaluations to understand the stability of persona traits in dialogues, examining shifts in traits post-generation and their impact on dialogue quality and strategy distribution. Experimental results reveal several notable findings: 1) LLMs can infer core persona traits, 2) subtle shifts in emotionality and extraversion occur, influencing the dialogue dynamics, and 3) the application of persona traits modifies the distribution of emotional support strategies, enhancing the relevance and empathetic quality of the responses. These findings highlight the potential of persona-driven LLMs in crafting more personalized, empathetic, and effective emotional support dialogues, which has significant implications for the future design of AI-driven emotional support systems.

</details>


### [162] [DSMoE: Matrix-Partitioned Experts with Dynamic Routing for Computation-Efficient Dense LLMs](https://arxiv.org/abs/2502.12455)

*Minxuan Lv, Zhenpeng Su, Leiyu Pan, Yizhe Xiong, Zijia Lin, Hui Chen, Wei Zhou, Jungong Han, Guiguang Ding, Cheng Luo, Di Zhang, Kun Gai, Songlin Hu*

**Main category:** cs.CL

**Keywords:** large language models, sparsification, Mixture-of-Experts, adaptive routing, language modeling

**Relevance Score:** 7

**TL;DR:** The paper presents DSMoE, a novel method for sparsifying language models by partitioning FFN layers, enabling adaptive expert routing and superior performance in language tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As large language models scale, computational costs and resource usage pose challenges, prompting the need for effective sparsification methods.

**Method:** The proposed DSMoE partitions pre-trained FFN layers into blocks and implements adaptive expert routing using sigmoid activation and straight-through estimators, along with a sparsity loss term for balancing performance and efficiency.

**Key Contributions:**

	1. Introduction of DSMoE for efficient language model sparsification.
	2. Adaptive expert routing based on input complexity.
	3. Demonstrated superior performance over traditional sparsification techniques.

**Result:** Experiments show that DSMoE outperforms existing pruning and MoE methods in language modeling and downstream tasks, particularly in generation tasks.

**Limitations:** 

**Conclusion:** DSMoE offers new insights into MoE architecture design by learning distinctive layerwise activation patterns.

**Abstract:** As large language models continue to scale, computational costs and resource consumption have emerged as significant challenges. While existing sparsification methods like pruning reduce computational overhead, they risk losing model knowledge through parameter removal. This paper proposes DSMoE (Dynamic Sparse Mixture-of-Experts), a novel approach that achieves sparsification by partitioning pre-trained FFN layers into computational blocks. We implement adaptive expert routing using sigmoid activation and straight-through estimators, enabling tokens to flexibly access different aspects of model knowledge based on input complexity. Additionally, we introduce a sparsity loss term to balance performance and computational efficiency. Extensive experiments on LLaMA models demonstrate that under equivalent computational constraints, DSMoE achieves superior performance compared to existing pruning and MoE approaches across language modeling and downstream tasks, particularly excelling in generation tasks. Analysis reveals that DSMoE learns distinctive layerwise activation patterns, providing new insights for future MoE architecture design.

</details>


### [163] [Efficient Environmental Claim Detection with Hyperbolic Graph Neural Networks](https://arxiv.org/abs/2502.13628)

*Darpan Aswal, Manjira Sinha*

**Main category:** cs.CL

**Keywords:** Graph Neural Networks, Environmental Claim Detection, Hyperbolic Graph Neural Networks

**Relevance Score:** 7

**TL;DR:** This paper presents a graph-based approach to Environmental Claim Detection, proposing GNNs and HGNNs as efficient alternatives to transformer models, achieving superior performance with fewer parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational limitations of transformer-based models in resource-constrained applications, particularly in the context of environmental claim detection.

**Method:** The authors propose a graph classification approach, converting claim sentences into dependency parsing graphs and utilizing word2vec and learnable POS tag embeddings for node features, while encoding syntactic dependencies in edge relations.

**Key Contributions:**

	1. Proposing a novel graph-based approach for Environmental Claim Detection
	2. Utilizing Hyperbolic Graph Neural Networks (HGNNs) for improved performance
	3. Achieving superior results with significantly reduced parameters compared to transformer models

**Result:** The graph-based models, especially P-HGNNs, outperform state-of-the-art methods in environmental claim detection with up to 30x fewer parameters.

**Limitations:** 

**Conclusion:** HGNNs demonstrate significant advantages when data is structured hierarchically, improving performance over traditional euclidean models.

**Abstract:** Transformer based models, specially large language models (LLMs) dominate the field of NLP with their mass adoption in tasks such as text generation, summarization and fake news detection. These models offer ease of deployment and reliability for most applications, however, they require significant amounts of computational power for training as well as inference. This poses challenges in their adoption in resource-constrained applications, specially in the open-source community where compute availability is usually scarce. This work proposes a graph-based approach for Environmental Claim Detection, exploring Graph Neural Networks (GNNs) and Hyperbolic Graph Neural Networks (HGNNs) as lightweight yet effective alternatives to transformer-based models. Re-framing the task as a graph classification problem, we transform claim sentences into dependency parsing graphs, utilizing a combination of word2vec \& learnable part-of-speech (POS) tag embeddings for the node features and encoding syntactic dependencies in the edge relations. Our results show that our graph-based models, particularly HGNNs in the poincar\'e space (P-HGNNs), achieve performance superior to the state-of-the-art on environmental claim detection while using upto \textbf{30x fewer parameters}. We also demonstrate that HGNNs benefit vastly from explicitly modeling data in hierarchical (tree-like) structures, enabling them to significantly improve over their euclidean counterparts.

</details>


### [164] [Rumor Detection by Multi-task Suffix Learning based on Time-series Dual Sentiments](https://arxiv.org/abs/2502.14383)

*Zhiwei Liu, Kailai Yang, Eduard Hovy, Sophia Ananiadou*

**Main category:** cs.CL

**Keywords:** rumor detection, sentiment analysis, multi-task learning, social media, LLM

**Relevance Score:** 7

**TL;DR:** This paper introduces MSuf, a novel multi-task suffix learning framework designed to enhance rumor detection and tracking on social media by analyzing time series sentiments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant impact of rumors on social media, which can cause public panic and fear, effective detection and tracking mechanisms need to consider nuanced sentiments of messages over time.

**Method:** MSuf utilizes a multi-task learning approach with three main modules: it employs an LLM to extract sentiment features chronologically, fuses these features with source text embeddings, and uses hard prompts for rumor detection and sentiment analysis, applying minimal parameter fine-tuning on the LLM.

**Key Contributions:**

	1. Introduction of the MSuf framework for rumor detection
	2. Utilization of time series dual sentiments in analysis
	3. Demonstrated significant improvement over traditional emotion-based detection methods

**Result:** MSuf shows significant performance improvements in rumor detection across four benchmark datasets compared to existing emotion-based methods.

**Limitations:** This is a work in progress and may require further validation and refinement in real-world applications.

**Conclusion:** The proposed framework effectively enhances LLM performance in detecting and tracking rumors with minimal fine-tuning, addressing a critical gap in current methodologies.

**Abstract:** The widespread dissemination of rumors on social media has a significant impact on people's lives, potentially leading to public panic and fear. Rumors often evoke specific sentiments, resonating with readers and prompting sharing. To effectively detect and track rumors, it is essential to observe the fine-grained sentiments of both source and response message pairs as the rumor evolves over time. However, current rumor detection methods fail to account for this aspect. In this paper, we propose MSuf, the first multi-task suffix learning framework for rumor detection and tracking using time series dual (coupled) sentiments. MSuf includes three modules: (1) an LLM to extract sentiment intensity features and sort them chronologically; (2) a module that fuses the sorted sentiment features with their source text word embeddings to obtain an aligned embedding; (3) two hard prompts are combined with the aligned vector to perform rumor detection and sentiment analysis using one frozen LLM. MSuf effectively enhances the performance of LLMs for rumor detection with only minimal parameter fine-tuning. Evaluating MSuf on four rumor detection benchmarks, we find significant improvements compared to other emotion-based methods.

</details>


### [165] [Understanding the Uncertainty of LLM Explanations: A Perspective Based on Reasoning Topology](https://arxiv.org/abs/2502.17026)

*Longchao Da, Xiaoou Liu, Jiaxin Dai, Lu Cheng, Yaqing Wang, Hua Wei*

**Main category:** cs.CL

**Keywords:** large language models, uncertainty quantification, graph topology, explanation faithfulness, reasoning consistency

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for quantifying uncertainty in large language model (LLM) explanations by framing them as a reasoning topology, allowing for assessment of faithfulness and reasoning consistency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding uncertainty in LLM explanations is crucial for evaluating their reliability and reasoning consistency, which impacts the interpretation of their outputs.

**Method:** The proposed framework uses a structural elicitation strategy to convert LLM explanations into a graph topology, decomposing them into sub-questions and reasoning structures for uncertainty quantification.

**Key Contributions:**

	1. Framework for quantifying uncertainty in LLM explanations
	2. Graph topology for structuring explanations
	3. Method for analyzing reasoning paths and knowledge redundancy

**Result:** The method allows quantification of uncertainty both semantically and from the reasoning path, enhancing interpretability and revealing knowledge redundancy.

**Limitations:** 

**Conclusion:** The work introduces graph-structured uncertainty measurement for LLM explanations and illustrates its potential for improving robustness and faithfulness in reasoning.

**Abstract:** Understanding the uncertainty in large language model (LLM) explanations is important for evaluating their faithfulness and reasoning consistency, and thus provides insights into the reliability of LLM's output regarding a question. In this work, we propose a novel framework that quantifies uncertainty in LLM explanations through a reasoning topology perspective. By designing a structural elicitation strategy, we guide the LLMs to frame the explanations of an answer into a graph topology. This process decomposes the explanations into the knowledge related sub-questions and topology-based reasoning structures, which allows us to quantify uncertainty not only at the semantic level but also from the reasoning path. It further brings convenience to assess knowledge redundancy and provide interpretable insights into the reasoning process. Our method offers a systematic way to interpret the LLM reasoning, analyze limitations, and provide guidance for enhancing robustness and faithfulness. This work pioneers the use of graph-structured uncertainty measurement in LLM explanations and demonstrates the potential of topology-based quantification.

</details>


### [166] [LLM as a Broken Telephone: Iterative Generation Distorts Information](https://arxiv.org/abs/2502.20258)

*Amr Mohamed, Mingmeng Geng, Michalis Vazirgiannis, Guokan Shang*

**Main category:** cs.CL

**Keywords:** large language models, information distortion, iterative generation, prompting techniques, AI-mediated information propagation

**Relevance Score:** 9

**TL;DR:** This study investigates the distortion of information in large language models (LLMs) through iterative generation, drawing parallels with the 'broken telephone' effect.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how repeated processing of LLM outputs affects information reliability and propagation.

**Method:** Translation-based experiments were conducted to analyze how distortion accumulates over time based on language choice and chain complexity.

**Key Contributions:**

	1. Investigates the 'broken telephone' effect in LLMs
	2. Demonstrates how language choice and chain complexity affect information distortion
	3. Proposes strategic prompting techniques to mitigate degradation.

**Result:** Findings indicate that information distortion is inevitable but can be mitigated with effective prompting techniques.

**Limitations:** 

**Conclusion:** The study highlights the potential concerns regarding the reliability of LLM-generated content in iterative workflows and suggests strategies to reduce distortion.

**Abstract:** As large language models are increasingly responsible for online content, concerns arise about the impact of repeatedly processing their own outputs. Inspired by the "broken telephone" effect in chained human communication, this study investigates whether LLMs similarly distort information through iterative generation. Through translation-based experiments, we find that distortion accumulates over time, influenced by language choice and chain complexity. While degradation is inevitable, it can be mitigated through strategic prompting techniques. These findings contribute to discussions on the long-term effects of AI-mediated information propagation, raising important questions about the reliability of LLM-generated content in iterative workflows.

</details>


### [167] [LinguaLens: Towards Interpreting Linguistic Mechanisms of Large Language Models via Sparse Auto-Encoder](https://arxiv.org/abs/2502.20344)

*Yi Jing, Zijun Yao, Hongzhu Guo, Lingxu Ran, Xiaozhi Wang, Lei Hou, Juanzi Li*

**Main category:** cs.CL

**Keywords:** large language models, linguistic mechanisms, Sparse Auto-Encoders, counterfactual methods, linguistic knowledge

**Relevance Score:** 9

**TL;DR:** This paper presents LinguaLens, a framework for analyzing linguistic mechanisms in large language models (LLMs) using Sparse Auto-Encoders to extract linguistic features in both Chinese and English.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of understanding of how LLMs process and represent linguistic knowledge despite their remarkable performance in complex linguistic tasks.

**Method:** The study utilizes Sparse Auto-Encoders to extract and analyze linguistic features across morphology, syntax, semantics, and pragmatics while employing counterfactual methods to create a dataset for analysis.

**Key Contributions:**

	1. Introduction of LinguaLens framework for LLM analysis
	2. Development of a large-scale counterfactual dataset
	3. Evidence of intrinsic linguistic knowledge in LLMs

**Result:** Findings show intrinsic representations of linguistic knowledge in LLMs, revealing patterns of cross-layer and cross-lingual distribution, and suggest potential control over model outputs.

**Limitations:** 

**Conclusion:** LinguaLens offers resources and methods to better understand LLMs' linguistic mechanisms, providing evidence of their linguistic knowledge and setting the groundwork for more interpretable models.

**Abstract:** Large language models (LLMs) demonstrate exceptional performance on tasks requiring complex linguistic abilities, such as reference disambiguation and metaphor recognition/generation. Although LLMs possess impressive capabilities, their internal mechanisms for processing and representing linguistic knowledge remain largely opaque. Prior research on linguistic mechanisms is limited by coarse granularity, limited analysis scale, and narrow focus. In this study, we propose LinguaLens, a systematic and comprehensive framework for analyzing the linguistic mechanisms of large language models, based on Sparse Auto-Encoders (SAEs). We extract a broad set of Chinese and English linguistic features across four dimensions (morphology, syntax, semantics, and pragmatics). By employing counterfactual methods, we construct a large-scale counterfactual dataset of linguistic features for mechanism analysis. Our findings reveal intrinsic representations of linguistic knowledge in LLMs, uncover patterns of cross-layer and cross-lingual distribution, and demonstrate the potential to control model outputs. This work provides a systematic suite of resources and methods for studying linguistic mechanisms, offers strong evidence that LLMs possess genuine linguistic knowledge, and lays the foundation for more interpretable and controllable language modeling in future research.

</details>


### [168] [Monitoring Decoding: Mitigating Hallucination via Evaluating the Factuality of Partial Response during Generation](https://arxiv.org/abs/2503.03106)

*Yurui Chang, Bochuan Cao, Lu Lin*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, Monitoring Decoding, factual accuracy, text generation

**Relevance Score:** 9

**TL;DR:** A novel framework called Monitoring Decoding (MD) is introduced to reduce hallucinations in large language models during text generation by dynamically monitoring crucial tokens and applying interventions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inherent hallucination problem in large language models that generates plausible but incorrect content, mitigating the risk without introducing significant latency.

**Method:** Monitoring Decoding (MD) uses a monitor function to identify hallucination-prone tokens during the generation process and applies a tree-based decoding strategy for in-process interventions.

**Key Contributions:**

	1. Introduction of Monitoring Decoding (MD) framework
	2. Dynamic monitoring and selective in-process intervention on hallucination-prone tokens
	3. Improvement in factual accuracy and efficiency over traditional methods

**Result:** MD demonstrates enhanced factual accuracy and coherence in generated outputs, outperforming traditional self-consistency methods in both effectiveness and efficiency with reduced computational overhead.

**Limitations:** 

**Conclusion:** The study showcases that targeted intervention during generation, rather than post-generation adjustments, can significantly improve the reliability of language model outputs.

**Abstract:** While large language models have demonstrated exceptional performance across a wide range of tasks, they remain susceptible to hallucinations -- generating plausible yet factually incorrect contents. Existing methods to mitigating such risk often rely on sampling multiple full-length generations, which introduces significant response latency and becomes ineffective when the model consistently produces hallucinated outputs with high confidence. To address these limitations, we introduce Monitoring Decoding (MD), a novel framework that dynamically monitors the generation process and selectively applies in-process interventions, focusing on revising crucial tokens responsible for hallucinations. Instead of waiting until completion of multiple full-length generations, we identify hallucination-prone tokens during generation using a monitor function, and further refine these tokens through a tree-based decoding strategy. This approach ensures an enhanced factual accuracy and coherence in the generated output while maintaining efficiency. Experimental results demonstrate that MD consistently outperforms self-consistency-based approaches in both effectiveness and efficiency, achieving higher factual accuracy while significantly reducing computational overhead.

</details>


### [169] [Chain of Strategy Optimization Makes Large Language Models Better Emotional Supporter](https://arxiv.org/abs/2503.05362)

*Weixiang Zhao, Xingyu Sui, Xinyang Han, Yang Deng, Yulin Hu, Jiahe Guo, Libo Qin, Qianyun Du, Shijin Wang, Yanyan Zhao, Bing Qin, Ting Liu*

**Main category:** cs.CL

**Keywords:** Emotional Support Conversations, Large Language Models, Chain-of-Strategy Optimization, Monte Carlo Tree Search, Empathy

**Relevance Score:** 9

**TL;DR:** This paper introduces Chain-of-Strategy Optimization (CSO) to enhance the performance of Large Language Models in Emotional Support Conversations by improving strategy selection accuracy and mitigating preference bias.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing emotional stress in society has heightened the need for effective Emotional Support Conversations, presenting challenges for existing LLMs in accurately selecting strategies and being adaptable to users' emotional needs.

**Method:** The authors propose Chain-of-Strategy Optimization (CSO), which employs Monte Carlo Tree Search to create a high-quality preference dataset for training LLMs on nuanced strategy-response pairs at each dialogue turn.

**Key Contributions:**

	1. Introduction of Chain-of-Strategy Optimization (CSO) for enhanced LLM adaptability in emotional support
	2. Development of ESC-Pro dataset using Monte Carlo Tree Search for turn-level strategy-response pairs
	3. Demonstration of improved model performance in empathy and contextuality using CSO over standard SFT methods.

**Result:** Experiments demonstrate that LLMs trained with CSO outperformed those using standard supervised fine-tuning, achieving better strategy accuracy and reduced bias in responses.

**Limitations:** 

**Conclusion:** CSO significantly enhances the capability of LLMs in generating empathetic and contextually suitable responses for Emotional Support Conversations.

**Abstract:** The growing emotional stress in modern society has increased the demand for Emotional Support Conversations (ESC). While Large Language Models (LLMs) show promise for ESC, they face two key challenges: (1) low strategy selection accuracy, and (2) preference bias, limiting their adaptability to emotional needs of users. Existing supervised fine-tuning (SFT) struggles to address these issues, as it rigidly trains models on single gold-standard responses without modeling nuanced strategy trade-offs. To overcome these limitations, we propose Chain-of-Strategy Optimization (CSO), a novel approach that optimizes strategy selection preferences at each dialogue turn. We first leverage Monte Carlo Tree Search to construct ESC-Pro, a high-quality preference dataset with turn-level strategy-response pairs. Training on ESC-Pro with CSO improves both strategy accuracy and bias mitigation, enabling LLMs to generate more empathetic and contextually appropriate responses. Experiments on LLaMA-3.1-8B, Gemma-2-9B, and Qwen2.5-7B demonstrate that CSO outperforms standard SFT, highlighting the efficacy of fine-grained, turn-level preference modeling in ESC.

</details>


### [170] [Hallucinated Span Detection with Multi-View Attention Features](https://arxiv.org/abs/2504.04335)

*Yuya Ogasa, Yuki Arase*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, attention matrix, Transformer, natural language generation

**Relevance Score:** 8

**TL;DR:** This study proposes a method for detecting hallucinated spans in outputs from large language models using features from the attention matrix and a Transformer-based classifier.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Hallucinated span detection has been underexplored compared to output-level hallucination detection, despite its practical relevance in understanding and improving language model outputs.

**Method:** Features are extracted from the attention matrix, focusing on token influence, attention bias, and context reference, which are then used as inputs for a Transformer-based classifier for sequential labeling of hallucinated spans.

**Key Contributions:**

	1. Introduction of features extracted from the attention matrix for hallucinated span detection.
	2. Development of a Transformer-based classifier for sequential labeling of detected spans.
	3. Demonstration of improved performance over existing baselines in tasks involving longer contexts.

**Result:** The proposed method demonstrates superior performance in detecting hallucinated spans, particularly in tasks requiring longer input contexts, such as data-to-text and summarization.

**Limitations:** 

**Conclusion:** The study concludes that utilizing attention matrix features significantly enhances the detection of hallucinated spans in language model outputs.

**Abstract:** This study addresses the problem of hallucinated span detection in the outputs of large language models. It has received less attention than output-level hallucination detection despite its practical importance. Prior work has shown that attentions often exhibit irregular patterns when hallucinations occur. Motivated by these findings, we extract features from the attention matrix that provide complementary views capturing (a) whether certain tokens are influential or ignored, (b) whether attention is biased toward specific subsets, and (c) whether a token is generated referring to a narrow or broad context, in the generation. These features are input to a Transformer-based classifier to conduct sequential labelling to identify hallucinated spans. Experimental results indicate that the proposed method outperforms strong baselines on hallucinated span detection with longer input contexts, such as data-to-text and summarisation tasks.

</details>


### [171] [Assessing LLMs in Art Contexts: Critique Generation and Theory of Mind Evaluation](https://arxiv.org/abs/2504.12805)

*Takaya Arita, Wenxian Zheng, Reiji Suzuki, Fuminori Akiba*

**Main category:** cs.CL

**Keywords:** Large Language Models, Art Critique, Theory of Mind, Interpretation, Machine Learning

**Relevance Score:** 9

**TL;DR:** This study investigates the effectiveness of large language models (LLMs) in generating art critiques and performing Theory of Mind tasks in art contexts, revealing potential and limitations in their interpretative capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can critique artwork and reason about mental states in art-related scenarios, assessing their interpretative abilities.

**Method:** The study employed a two-part approach: first, constructing a system that integrates Carol's evaluative framework with various art criticism theories to generate critiques, and second, introducing new Theory of Mind tasks that go beyond traditional tests, focusing on complex social reasoning in art.

**Key Contributions:**

	1. Demonstrated LLMs' effectiveness in generating detailed art critiques.
	2. Introduced innovative Theory of Mind tasks tailored for art contexts.
	3. Revealed cognitive differences in LLM performance across various interpretative challenges.

**Result:** LLMs produced critiques that were often indistinguishable from human experts in a Turing test-style evaluation, showing potential for rich interpretation when guided effectively. Performance on ToM tasks indicated variability among models, particularly in emotionally charged or ambiguous scenarios.

**Limitations:** Limited by the inherent cognitive limitations of LLMs and the design of prompts, which may not universally apply to all interpretative tasks.

**Conclusion:** The findings provide insights into LLMs' capabilities in interpretative tasks, suggesting that while they may not genuinely understand art, their output can closely resemble expert critique under specific prompting conditions.

**Abstract:** This study explored how large language models (LLMs) perform in two areas related to art: writing critiques of artworks and reasoning about mental states (Theory of Mind, or ToM) in art-related situations. For the critique generation part, we built a system that combines Noel Carroll's evaluative framework with a broad selection of art criticism theories. The model was prompted to first write a full-length critique and then shorter, more coherent versions using a step-by-step prompting process. These AI-generated critiques were then compared with those written by human experts in a Turing test-style evaluation. In many cases, human subjects had difficulty telling which was which, and the results suggest that LLMs can produce critiques that are not only plausible in style but also rich in interpretation, as long as they are carefully guided. In the second part, we introduced new simple ToM tasks based on situations involving interpretation, emotion, and moral tension, which can appear in the context of art. These go beyond standard false-belief tests and allow for more complex, socially embedded forms of reasoning. We tested 41 recent LLMs and found that their performance varied across tasks and models. In particular, tasks that involved affective or ambiguous situations tended to reveal clearer differences. Taken together, these results help clarify how LLMs respond to complex interpretative challenges, revealing both their cognitive limitations and potential. While our findings do not directly contradict the so-called Generative AI Paradox--the idea that LLMs can produce expert-like output without genuine understanding--they suggest that, depending on how LLMs are instructed, such as through carefully designed prompts, these models may begin to show behaviors that resemble understanding more closely than we might assume.

</details>


### [172] [LogicTree: Structured Proof Exploration for Coherent and Rigorous Logical Reasoning with Large Language Models](https://arxiv.org/abs/2504.14089)

*Kang He, Kaushik Roy*

**Main category:** cs.CL

**Keywords:** large language models, multi-step reasoning, proof exploration

**Relevance Score:** 8

**TL;DR:** LogicTree is a modular framework for improving multi-step reasoning in large language models by using algorithm-guided search and caching mechanisms for logical coherence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of large language models (LLMs) which struggle with complex logical reasoning and proof-finding due to the combinatorial complexity of premise searches.

**Method:** LogicTree employs algorithm-guided search for automated structured proof exploration and incorporates caching mechanisms to utilize historical knowledge for efficient inference.

**Key Contributions:**

	1. Introduction of LogicTree framework for modular proof exploration
	2. Implementation of caching mechanisms for historical knowledge utilization
	3. Development of LLM-free heuristics for premise prioritization

**Result:** LogicTree achieves higher proof accuracy compared to traditional methods, with average improvements of 23.6% over chain-of-thought and 12.5% over tree-of-thought on GPT-4o.

**Limitations:** 

**Conclusion:** The LogicTree framework effectively scales inference-time computations and enhances the reasoning accuracy of LLMs by enforcing a step-by-step reasoning approach and introducing heuristic premise prioritization.

**Abstract:** Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average.

</details>
