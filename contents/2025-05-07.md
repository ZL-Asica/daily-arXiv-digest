# 2025-05-07

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 19]

- [cs.CL](#cs.CL) [Total: 57]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Revisiting Performance Models of Distal Pointing Tasks in Virtual Reality](https://arxiv.org/abs/2505.03027)

*Logan Lane, Feiyu Lu, Shakiba Davari, Rob Teather, Doug A. Bowman*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Distal Pointing, Fitts Law, Virtual Reality, Performance Models

**Relevance Score:** 8

**TL;DR:** This study examines the efficacy of existing models for predicting distal pointing performance in 3D interaction, suggesting a need for new methodologies and model forms due to current VR technology limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the lack of consensus on computing the index of difficulty for distal pointing tasks amidst evolving VR technologies.

**Method:** A new methodology was employed to collect empirical data for distal pointing tasks, evaluating traditional models, ballistic models, and two-part models.

**Key Contributions:**

	1. Introduced a new methodology for collecting distal pointing data.
	2. Evaluated various models of interaction performance.
	3. Proposed an improved model for distal pointing using angular measures.

**Result:** The study concluded that a modified Fitts-Law-style index of difficulty with angular measures of amplitude and width is superior for modeling distal pointing performance.

**Limitations:** The study is preliminary and may require further testing in diverse contexts and technologies.

**Conclusion:** Existing models may not suffice for current virtual reality applications, necessitating methodological changes for better modeling.

**Abstract:** Performance models of interaction, such as Fitts Law, are important tools for predicting and explaining human motor performance and for designing high-performance user interfaces. Extensive prior work has proposed such models for the 3D interaction task of distal pointing, in which the user points their hand or a device at a distant target in order to select it. However, there is no consensus on how to compute the index of difficulty for distal pointing tasks. We present a preliminary study suggesting that existing models may not be sufficient to model distal pointing performance with current virtual reality technologies. Based on these results, we hypothesized that both the form of the model and the standard method for collecting empirical data for pointing tasks might need to change in order to achieve a more accurate and valid distal pointing model. In our main study, we used a new methodology to collect distal pointing data and evaluated traditional models, purely ballistic models, and two-part models. Ultimately, we found that the best model used a simple Fitts-Law-style index of difficulty with angular measures of amplitude and width.

</details>


### [2] [Coupling the Heart to Musical Machines](https://arxiv.org/abs/2505.03073)

*Eric Easthope*

**Main category:** cs.HC

**Keywords:** biofeedback, human-computer interaction, music technology, heart rate, real-time audio processing

**Relevance Score:** 8

**TL;DR:** The paper presents a novel musical interface controlled by heart rate data via a photoplethysmogram (PPG), exploring the relationship between sound and heart rate in real-time to enhance musical expression.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of heart rate as a control parameter in musical interfaces, complementing existing biofeedback mechanisms utilizing breath.

**Method:** The author developed a PPG-based musical interface that transforms audio in real-time based on heart rate changes, implementing a feedback loop between sound and physiological responses.

**Key Contributions:**

	1. Introduction of heart rate as a control parameter in musical interfaces
	2. Implementation of a real-time audio processing technique using PPG data
	3. Discussion of generative latent spaces for extending control parameters

**Result:** The PPG-based controller dynamically alters sound characteristics based on the user's heart rate, demonstrating how biofeedback can affect sound perception and performance ambience.

**Limitations:** The study is limited by the complexity of building a robust real-time system and the need for more empirical validation in varied musical contexts.

**Conclusion:** The findings suggest the heart's role in musical interaction could offer a new dimension to human-computer interaction in music, potentially enriching performance experiences through a feedback loop.

**Abstract:** Biofeedback is being used more recently as a general control paradigm for human-computer interfaces (HCIs). While biofeedback especially from breath has seen increasing uptake as a controller for novel musical interfaces, new interfaces for musical expression (NIMEs), the community has not given as much attention to the heart. The heart is just as intimate a part of music as breath and it is argued that the heart determines our perception of time and so indirectly our perception of music. Inspired by this I demonstrate a photoplethysmogram (PPG)-based NIME controller using heart rate as a 1D control parameter to transform the qualities of sounds in real-time over a Bluetooth wireless HCI. I apply time scaling to "warp" audio buffers inbound to the sound card, and play these transformed audio buffers back to the listener wearing the PPG sensor, creating a hypothetical perceptual biofeedback loop: changes in sound change heart rate to change PPG measurements to change sound. I discuss how a sound-heart-PPG biofeedback loop possibly affords greater control and/or variety of movements with a 1D controller, how controlling the space and/or time scale of sound playback with biofeedback makes for possibilities in performance ambience, and I briefly discuss generative latent spaces as a possible way to extend a 1D PPG control space.

</details>


### [3] [Cognitio Emergens: Agency, Dimensions, and Dynamics in Human-AI Knowledge Co-Creation](https://arxiv.org/abs/2505.03105)

*Xule Lin*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, Epistemic partnerships, Cognitio Emergens, Scientific knowledge creation, Epistemic alienation

**Relevance Score:** 6

**TL;DR:** The article introduces Cognitio Emergens (CE), a framework for understanding human-AI collaboration in scientific knowledge creation, emphasizing dynamic roles, capabilities, and the evolution of partnerships.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** This paper addresses the limitations of existing models in capturing the dynamic and co-evolutionary nature of human-AI partnerships in scientific inquiry.

**Method:** The CE framework integrates Agency Configurations, Epistemic Dimensions, and Partnership Dynamics to analyze and categorize the evolving relationships between humans and AI during knowledge creation.

**Key Contributions:**

	1. Introduction of the Cognitio Emergens framework for human-AI collaboration in science.
	2. Identification of dynamic Agency Configurations and Epistemic Dimensions in partnerships.
	3. Exploration of Partnership Dynamics and the concept of epistemic alienation.

**Result:** CE reveals that scientific understanding emerges through ongoing negotiations of roles, values, and structures in human-AI collaborations, highlighting the risk of epistemic alienation.

**Limitations:** The framework may not address all possible dimensions of human-AI interaction in diverse scientific fields.

**Conclusion:** By framing human-AI scientific collaboration as co-evolutionary, the paper provides tools to foster meaningful human participation amidst transformative AI abilities in research.

**Abstract:** Scientific knowledge creation is fundamentally transforming as humans and AI systems evolve beyond tool-user relationships into co-evolutionary epistemic partnerships. When AlphaFold revolutionized protein structure prediction, researchers described engaging with an epistemic partner that reshaped how they conceptualized fundamental relationships. This article introduces Cognitio Emergens (CE), a framework addressing critical limitations in existing models that focus on static roles or narrow metrics while failing to capture how scientific understanding emerges through recursive human-AI interaction over time. CE integrates three components addressing these limitations: Agency Configurations describing how authority distributes between humans and AI (Directed, Contributory, Partnership), with partnerships dynamically oscillating between configurations rather than following linear progression; Epistemic Dimensions capturing six specific capabilities emerging through collaboration across Discovery, Integration, and Projection axes, creating distinctive "capability signatures" that guide development; and Partnership Dynamics identifying forces shaping how these relationships evolve, particularly the risk of epistemic alienation where researchers lose interpretive control over knowledge they formally endorse. Drawing from autopoiesis theory, social systems theory, and organizational modularity, CE reveals how knowledge co-creation emerges through continuous negotiation of roles, values, and organizational structures. By reconceptualizing human-AI scientific collaboration as fundamentally co-evolutionary, CE offers a balanced perspective that neither uncritically celebrates nor unnecessarily fears AI's evolving role, instead providing conceptual tools for cultivating partnerships that maintain meaningful human participation while enabling transformative scientific breakthroughs.

</details>


### [4] [Do ATCOs Need Explanations, and Why? Towards ATCO-Centered Explainable AI for Conflict Resolution Advisories](https://arxiv.org/abs/2505.03117)

*Katherine Fennedy, Brian Hilburn, Thaivalappil N. M. Nadirsha, Sameer Alam, Khanh-Duy Le, Hua Li*

**Main category:** cs.HC

**Keywords:** explainable AI, human-centered design, air traffic management, decision-making, human-computer interaction

**Relevance Score:** 6

**TL;DR:** This paper emphasizes a human-centered approach to explainable AI (XAI) in air traffic management, focusing on the needs of air traffic controllers (ATCOs) for explanations when making decisions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in XAI research which often overlooks end-user needs, specifically for air traffic controllers.

**Method:** Synthesized insights from air traffic management, human-computer interaction, and social sciences, evaluating the need for explanations based on 11 operational goals in ATM.

**Key Contributions:**

	1. Introduction of a human-centered approach to XAI in ATM
	2. Evaluation of ATCOs' needs for explanations
	3. Identification of circumstances under which ATCOs seek explanations.

**Result:** The study found that ATCOs need explanations for documenting decisions but are less inclined to seek them when their decision-making aligns with AI recommendations.

**Limitations:** This is a preliminary study with limited scope and more extensive research is required.

**Conclusion:** These preliminary findings support the idea that further investigations into ATCO-centric XAI systems are necessary for improved human-AI interactions in air traffic management.

**Abstract:** Interest in explainable artificial intelligence (XAI) is surging. Prior research has primarily focused on systems' ability to generate explanations, often guided by researchers' intuitions rather than end-users' needs. Unfortunately, such approaches have not yielded favorable outcomes when compared to a black-box baseline (i.e., no explanation). To address this gap, this paper advocates a human-centered approach that shifts focus to air traffic controllers (ATCOs) by asking a fundamental yet overlooked question: Do ATCOs need explanations, and if so, why? Insights from air traffic management (ATM), human-computer interaction, and the social sciences were synthesized to provide a holistic understanding of XAI challenges and opportunities in ATM. Evaluating 11 ATM operational goals revealed a clear need for explanations when ATCOs aim to document decisions and rationales for future reference or report generation. Conversely, ATCOs are less likely to seek them when their conflict resolution approach align with the artificial intelligence (AI) advisory. While this is a preliminary study, the findings are expected to inspire broader and deeper inquiries into the design of ATCO-centric XAI systems, paving the way for more effective human-AI interaction in ATM.

</details>


### [5] [InfoVids: Reimagining the Viewer Experience with Alternative Visualization-Presenter Relationships](https://arxiv.org/abs/2505.03164)

*Ji Won Chung, Tongyu Zhou, Ivy Chen, Kevin Hsu, Ryan A. Rossi, Alexa Siu, Shunan Guo, Franck Dernoncourt, James Tompkin, Jeff Huang*

**Main category:** cs.HC

**Keywords:** InfoVids, data presentation, viewer engagement, HCI, mixed methods

**Relevance Score:** 4

**TL;DR:** This paper introduces InfoVids, a new format for presenting data that integrates the presenter and visualization in a human-centric way, enhancing viewer engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a more equitable relationship between presenters and visualizations and improve the viewer experience during data presentations.

**Method:** A mixed methods approach with comparative analysis conducted with 30 participants across 9 metrics to evaluate InfoVids against traditional 2D slides.

**Key Contributions:**

	1. Introduction of InfoVids as a new format for data presentation
	2. Empirical comparison of InfoVids and traditional 2D slides
	3. Insights into how layout and interactions influence viewer experience

**Result:** InfoVids reduced viewer attention splitting, shifted focus from visualization to presenter, and led to more engaging full-body performances.

**Limitations:** The study is based on a limited participant pool and specific presentation contexts.

**Conclusion:** InfoVids can redefine presenter-visualization dynamics, leading to a more natural and interactive experiencing of data.

**Abstract:** Traditional data presentations typically separate the presenter and visualization into two separate spaces--the 3D world and a 2D screen--enforcing visualization-centric stories. To create a more human-centric viewing experience, we establish a more equitable relationship between the visualization and the presenter through our InfoVids. These infographics-inspired informational videos are crafted to redefine relationships between the presenter and visualizations. As we design InfoVids, we explore how the use of layout, form, and interactions affects the viewer experience. We compare InfoVids against their baseline 2D `slides' equivalents across 9 metrics with 30 participants and provide practical, long-term insights from an autobiographical perspective. Our mixed methods analyses reveal that this paradigm reduced viewer attention splitting, shifted the focus from the visualization to the presenter, and led to more interactive, natural, and engaging full-body data performances for viewers. Ultimately, InfoVids helped viewers re-imagine traditional dynamics between the presenter and visualizations.

</details>


### [6] [Behavioral Sensing and Intervention Paradigm: A Review of Closed-Loop Approaches for Ingestion Health](https://arxiv.org/abs/2505.03185)

*Jun Fang, Yanuo Zhou, Ka I Chan, Jiajin Li, Zeyi Sun, Zhengnan Li, Zicong Fu, Hongjing Piao, Haodong Xu, Yuanchun Shi, Yuntao Wang*

**Main category:** cs.HC

**Keywords:** ingestive behavior, health interventions, sensing modalities, feedback strategies, adaptive systems

**Relevance Score:** 7

**TL;DR:** This survey reviews 136 studies on sensor-enabled interventions for eating behavior, proposing a behavioral closed-loop paradigm to enhance health interventions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of static guidance and manual tracking in health interventions related to ingestive behavior.

**Method:** Review and analysis of 136 studies on sensor-enabled and interaction-mediated approaches to influence eating behavior, proposing a taxonomy and evaluating design trends.

**Key Contributions:**

	1. Proposed a behavioral closed-loop paradigm for eating behavior interventions.
	2. Developed a taxonomy of sensing and intervention modalities.
	3. Evaluated design trends and gaps in current research on ingestive behavior interventions.

**Result:** Identification of patterns and gaps in existing studies, providing insights into effective sensing modalities and feedback strategies for creating adaptive ingestion health interventions.

**Limitations:** The review primarily focuses on existing studies and may not cover all innovative interventions, particularly outside the sensor-enabled realm.

**Conclusion:** The review highlights the need for more context-aware interventions and suggests design principles for future systems that leverage closed-loop feedback.

**Abstract:** Ingestive behavior plays a critical role in health, yet many existing interventions remain limited to static guidance or manual self-tracking. With the increasing integration of sensors and perceptual computing, recent systems have begun to support closed-loop interventions that dynamically sense user behavior and provide feedback during or around ingestion episodes. In this survey, we review 136 studies that leverage sensor-enabled or interaction-mediated approaches to influence eating behavior. We propose a behavioral closed-loop paradigm comprising three core components: target behaviors, sensing modalities, and feedback strategies. A taxonomy of sensing and intervention modalities is presented, organized along human- and environment-based dimensions. Our analysis also examines evaluation methods and design trends across different modality-behavior pairings. This review reveals prevailing patterns and critical gaps, offering design insights for future adaptive and context-aware ingestion health interventions.

</details>


### [7] [DroidRetriever: An Autonomous Navigation and Information Integration System Facilitating Mobile Sensemaking](https://arxiv.org/abs/2505.03364)

*Yiheng Bian, Yunpeng Song, Guiyu Ma, Rongrong Zhu, Zhongmin Cai*

**Main category:** cs.HC

**Keywords:** mobile applications, information retrieval, natural language processing, cross-application integration, mobile sensemaking

**Relevance Score:** 8

**TL;DR:** DroidRetriever is a proposed system for cross-application information retrieval aimed at enhancing mobile sensemaking by automating the access to relevant information across diverse mobile applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The prevalence of mobile applications for daily information needs and the challenges posed by scattered data in closed ecosystems necessitate an effective tool for real-time information retrieval and integration.

**Method:** DroidRetriever utilizes natural language commands to navigate mobile interfaces, capture screenshots, extract pertinent information, and present integrated results with minimal user intervention.

**Key Contributions:**

	1. Development of DroidRetriever for cross-application retrieval
	2. Demonstration of near-human accuracy in information integration
	3. Reduction of user workload through automation of tasks

**Result:** DroidRetriever achieves near-human accuracy in information extraction and integration, significantly reducing processing time and user workload in comparison to traditional methods.

**Limitations:** 

**Conclusion:** The introduction of DroidRetriever represents a significant advancement in facilitating mobile sensemaking by improving the efficiency of cross-application information retrieval.

**Abstract:** Users regularly rely on mobile applications for their daily information needs, and mobile sensemaking is prevalent in various domains such as education, healthcare, business intelligence, and emergency response, where timely and context-aware information-processing and decision-making is critical. However, valuable information is often scattered across the closed ecosystems within various applications, posing challenges for traditional search engines to retrieve data openly and in real-time. Additionally, due to limitations such as mobile device screen sizes, language differences, and unfamiliarity with specific applications and domain knowledge, users have to frequently switch between multiple applications and spend substantial time locating and integrating the information. To address these challenges, we present DroidRetriever, a system for cross-application information retrieval to facilitate mobile sensemaking. DroidRetriever can automatically navigate to relevant interfaces based on users' natural language commands, capture screenshots, extract and integrate information, and finally present the results. Our experimental results demonstrate that DroidRetriever can extract and integrate information with near-human accuracy while significantly reducing processing time. Furthermore, with minimal user intervention, DroidRetriever effectively corrects and completes various information retrieval tasks, substantially reducing the user's workload. Our summary of the motivations for intervention and the discussion of their necessity provide valuable implications for future research. We will open-source our code upon acceptance of the paper.

</details>


### [8] [AI-Based Feedback in Counselling Competence Training of Prospective Teachers](https://arxiv.org/abs/2505.03423)

*Tobias Hallmen, Kathrin Gietl, Karoline Hillesheim, Moritz Bauermann, Annemarie Friedrich, Elisabeth André*

**Main category:** cs.HC

**Keywords:** AI in education, teacher training, communication skills, feedback systems, nonverbal communication

**Relevance Score:** 4

**TL;DR:** The study examines AI-based feedback's effectiveness in improving counseling skills in teacher training through seminars and simulations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the counseling competence of prospective teachers using AI tools for feedback on communication skills.

**Method:** An iterative seminar design involving theoretical foundations, practical applications, and AI tools for analyzing communication. It included recorded teacher-parent conversations and qualitative interviews to assess the impact of AI feedback.

**Key Contributions:**

	1. Integration of AI feedback within teacher training.
	2. Identification of key communication features that correlate with conversation quality.
	3. Positive student perceptions of AI feedback in teacher training.

**Result:** Significant correlations were found between nonverbal and paraverbal communication features and the quality of conversations. Students reported a positive perception of the AI feedback.

**Limitations:** Future work is needed to refine verbal skill annotations and expand the dataset.

**Conclusion:** AI-based feedback offers objective insights that can enhance teacher training programs, with plans for further refinement and expansion of the feedback system.

**Abstract:** This study explores the use of AI-based feedback to enhance the counselling competence of prospective teachers. An iterative block seminar was designed, incorporating theoretical foundations, practical applications, and AI tools for analysing verbal, paraverbal, and nonverbal communication. The seminar included recorded simulated teacher-parent conversations, followed by AI-based feedback and qualitative interviews with students. The study investigated correlations between communication characteristics and conversation quality, student perceptions of AI-based feedback, and the training of AI models to identify conversation phases and techniques. Results indicated significant correlations between nonverbal and paraverbal features and conversation quality, and students positively perceived the AI feedback. The findings suggest that AI-based feedback can provide objective, actionable insights to improve teacher training programs. Future work will focus on refining verbal skill annotations, expanding the dataset, and exploring additional features to enhance the feedback system.

</details>


### [9] [manvr3d: A Platform for Human-in-the-loop Cell Tracking in Virtual Reality](https://arxiv.org/abs/2505.03440)

*Samuel Pantze, Jean-Yves Tinevez, Matthew McGinity, Ulrik Günther*

**Main category:** cs.HC

**Keywords:** VR, cell tracking, deep learning, human-in-the-loop, 3D visualization

**Relevance Score:** 4

**TL;DR:** manvr3d is a VR-ready platform for enhancing human-in-the-loop cell tracking via 3D visualization and interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate and efficient cell tracking in life sciences is hindered by the limitations of traditional 2D visualization and manual annotation processes.

**Method:** The platform integrates deep learning for cell tracking with VR technologies, allowing users to interactively generate ground truth and proofread annotations using VR controllers and eye tracking.

**Key Contributions:**

	1. Integration of VR technology into cell tracking workflows
	2. Human-in-the-loop approach enhances data annotation
	3. Utilization of natural user interfaces for interaction

**Result:** manvr3d improves the workflow for cell tracking, enabling faster and more intuitive data annotations, thus enhancing the quality of training data for deep learning algorithms.

**Limitations:** 

**Conclusion:** By creating a 3D interactive system, manvr3d significantly speeds up the cell tracking process, making it easier for life scientists to visualize and analyze complex cellular data.

**Abstract:** We propose manvr3d, a novel VR-ready platform for interactive human-in-the-loop cell tracking. We utilize VR controllers and eye-tracking hardware to facilitate rapid ground truth generation and proofreading for deep learning-based cell tracking models. Life scientists reconstruct the developmental history of organisms on the cellular level by analyzing 3D time-lapse microscopy images acquired at high spatio-temporal resolution. The reconstruction of such cell lineage trees traditionally involves tracking individual cells through all recorded time points, manually annotating their positions, and then linking them over time to create complete trajectories. Deep learning-based algorithms accelerate this process, yet depend heavily on manually-annotated high-quality ground truth data and curation. Visual representation of the image data in this process still relies primarily on 2D renderings, which greatly limits spatial understanding and navigation. In this work, we bridge the gap between deep learning-based cell tracking software and 3D/VR visualization to create a human-in-the-loop cell tracking system. We lift the incremental annotation, training and proofreading loop of the deep learning model into the 3rd dimension and apply natural user interfaces like hand gestures and eye tracking to accelerate the cell tracking workflow for life scientists.

</details>


### [10] [Augmenting Human Cognition through Everyday AR](https://arxiv.org/abs/2505.03492)

*Xiaoan Liu*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Multimodal LLMs, Human-Computer Interaction, Context-aware Computing

**Relevance Score:** 8

**TL;DR:** The paper discusses the evolution of AR into a cognitive tool that integrates AI to foster better human interactions with the environment.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the potential of always-on AR systems in enhancing cognitive capabilities and task performance through context-aware intelligence.

**Method:** The paper presents a conceptual exploration of integrating multimodal LLMs with AR technologies to create immersive and intuitive user experiences.

**Key Contributions:**

	1. Integration of multimodal LLMs with AR for improved cognitive performance.
	2. Exploration of proactive context-sensitive interactions in AR.
	3. Proposition of AR as an intuitive tool for everyday tasks.

**Result:** The paper argues that a seamless integration of digital and physical elements through AR can significantly improve interaction quality and task efficiency.

**Limitations:** 

**Conclusion:** Always-on AR can transform how humans engage with digital information within their physical environments, suggesting a need for more research in this area.

**Abstract:** As spatial computing and multimodal LLMs mature, AR is tending to become an intuitive "thinking tool," embedding semantic and context-aware intelligence directly into everyday environments. This paper explores how always-on AR can seamlessly bridge digital cognition and physical affordances, enabling proactive, context-sensitive interactions that enhance human task performance and understanding.

</details>


### [11] [BCause: Human-AI collaboration to improve hybrid mapping and ideation in argumentation-grounded deliberation](https://arxiv.org/abs/2505.03584)

*Lucas Anastasiou, Anna De Liddo*

**Main category:** cs.HC

**Keywords:** public deliberation, generative AI, human-machine collaboration, democratic processes, actionable policy

**Relevance Score:** 7

**TL;DR:** This paper presents BCause, a generative AI-based system that enhances public deliberation by transforming unstructured dialogue into structured democratic processes, promoting actionable outcomes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues in public deliberation such as scattered discourse and a lack of actionable policy outcomes.

**Method:** Introducing BCause, a system that utilizes generative AI to organize public discussions through innovations like transforming transcripts into argumentative formats, geo-deliberation via a Telegram bot, and customizable reporting widgets.

**Key Contributions:**

	1. Introduction of BCause for structured public deliberation
	2. Innovative use of AI to transform unstructured dialogue into meaningful discussions
	3. Development of geo-deliberation tools for localized issue reporting

**Result:** BCause effectively converts unstructured discussions into structured, actionable insights into public issues, supporting better decision-making processes.

**Limitations:** 

**Conclusion:** The proposed system enhances democratic engagement by ensuring human participation and ethical oversight in conjunction with AI capabilities.

**Abstract:** Public deliberation, as in open discussion of issues of public concern, often suffers from scattered and shallow discourse, poor sensemaking, and a disconnect from actionable policy outcomes. This paper introduces BCause, a discussion system leveraging generative AI and human-machine collaboration to transform unstructured dialogue around public issues (such as urban living, policy changes, and current socio-economic transformations) into structured, actionable democratic processes. We present three innovations: (i) importing and transforming unstructured transcripts into argumentative discussions, (ii) geo-deliberated problem-sensing via a Telegram bot for local issue reporting, and (iii) smart reporting with customizable widgets (e.g., summaries, topic modelling, policy recommendations, clustered arguments). The system's human-AI partnership preserves critical human participation to ensure ethical oversight, contextual relevance, and creative synthesis.

</details>


### [12] [Scalable Class-Centric Visual Interactive Labeling](https://arxiv.org/abs/2505.03618)

*Matthias Matt, Jana Sedlakova, Jürgen Bernard, Matthias Zeppelzauer, Manuela Waldner*

**Main category:** cs.HC

**Keywords:** data labeling, visual analytics, human-computer interaction

**Relevance Score:** 7

**TL;DR:** Introduction of cVIL, a Class-Centric Visual Interactive Labeling methodology aimed at improving labeling efficiency in large datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large unlabeled datasets necessitate scalable data labeling solutions, particularly when facing significant visual scalability challenges and user cognitive load.

**Method:** cVIL shifts the labeling focus from assigning classes to instances to assigning instances to classes, enhancing user interaction and efficiency through a novel visual analytics labeling interface.

**Key Contributions:**

	1. Introduction of cVIL methodology for visual data labeling.
	2. Development of a novel visual analytics interface that enhances scalability.
	3. Evidence from user study supporting improved efficiency and satisfaction.

**Result:** User studies show that cVIL improves labeling efficiency and user satisfaction compared to traditional instance-centric interfaces.

**Limitations:** 

**Conclusion:** cVIL alleviates cognitive load and supports efficient management of extensive labeling tasks, demonstrating significant advantages over existing methods.

**Abstract:** Large unlabeled datasets demand efficient and scalable data labeling solutions, in particular when the number of instances and classes is large. This leads to significant visual scalability challenges and imposes a high cognitive load on the users. Traditional instance-centric labeling methods, where (single) instances are labeled in each iteration struggle to scale effectively in these scenarios. To address these challenges, we introduce cVIL, a Class-Centric Visual Interactive Labeling methodology designed for interactive visual data labeling. By shifting the paradigm from assigning-classes-to-instances to assigning-instances-to-classes, cVIL reduces labeling effort and enhances efficiency for annotators working with large, complex and class-rich datasets. We propose a novel visual analytics labeling interface built on top of the conceptual cVIL workflow, enabling improved scalability over traditional visual labeling. In a user study, we demonstrate that cVIL can improve labeling efficiency and user satisfaction over instance-centric interfaces. The effectiveness of cVIL is further demonstrated through a usage scenario, showcasing its potential to alleviate cognitive load and support experts in managing extensive labeling tasks efficiently.

</details>


### [13] [Evaluating Foveated Frame Rate Reduction in Virtual Reality for Head-Mounted Displays](https://arxiv.org/abs/2505.03682)

*Christopher Flöter, Sergej Geringer, Guido Reina, Daniel Weiskopf, Timo Ropinski*

**Main category:** cs.HC

**Keywords:** temporal resolution reduction, frame rate reduction, foveated rendering, virtual reality, perception

**Relevance Score:** 7

**TL;DR:** This study explores reducing frame rates in peripheral vision using foveated rendering in virtual reality and assesses the perception of temporal artifacts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the perceptual effects of reducing temporal resolution (frame rates) in foveated rendering, an area less explored compared to spatial resolution reduction.

**Method:** A user study was conducted with 15 participants in a virtual reality setting, utilizing a head-mounted display to evaluate perception of temporal artifacts based on gaze eccentricity and frame rate reductions.

**Key Contributions:**

	1. First exploration of foveated temporal resolution in VR
	2. Demonstrated user tolerance to reduced frame rates in periphery
	3. Provided insights for optimizing rendering costs in virtual environments

**Result:** The study found that significant reductions in rendering costs were achievable before temporal artifacts were consistently perceived by participants.

**Limitations:** Only 15 participants; results may not generalize to all user groups or VR contexts.

**Conclusion:** Foveated temporal resolution reduction can successfully lower rendering demands while maintaining spatial quality without noticeable artifacts, suggesting its potential in optimizing virtual reality applications.

**Abstract:** Foveated rendering methods usually reduce spatial resolution in the periphery of the users' view. However, using foveated rendering to reduce temporal resolution, i.e., rendering frame rate, seems less explored. In this work, we present the results of a user study investigating the perceptual effects of foveated temporal resolution reduction, where only the temporal resolution (frame rate) is reduced in the periphery without affecting spatial quality (pixel density). In particular, we investigated the perception of temporal resolution artifacts caused by reducing the frame rate dependent on the eccentricity of the user's gaze. Our user study with 15 participants was conducted in a virtual reality setting using a head-mounted display. Our results indicate that it was possible to reduce average rendering costs, i.e., the number of rendered pixels, to a large degree before participants consistently reported perceiving temporal artifacts.

</details>


### [14] [ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers](https://arxiv.org/abs/2409.10913)

*Pragnya Ramjee, Mehak Chhokar, Bhuvan Sachdeva, Mahendra Meena, Hamid Abdullah, Aditya Vashistha, Ruchit Nagar, Mohit Jain*

**Main category:** cs.HC

**Keywords:** Community health workers, Health informatics, Chatbot, LLM, Supervision

**Relevance Score:** 8

**TL;DR:** ASHABot, a WhatsApp-based chatbot powered by LLMs, was designed to assist community health workers (CHWs) in India by addressing their information needs and offering a private channel for inquiries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capacity of community health workers in India who face challenges with limited training and medical knowledge.

**Method:** Design and deployment of ASHABot, followed by interviews with CHWs and their supervisors and log analysis to evaluate engagement and information needs.

**Key Contributions:**

	1. Development and implementation of ASHABot for CHWs
	2. Demonstration of trust in LLMs as authoritative resources
	3. Identification of engagement factors and knowledge sharing dynamics

**Result:** ASHABot enabled CHWs to ask sensitive questions confidentially, built trust in the information provided, and supported knowledge sharing between CHWs and their supervisors.

**Limitations:** Concerns about increased workload and accountability for supervisors.

**Conclusion:** Positioning LLMs as complementary resources can provide essential support in community healthcare without replacing human supervisors.

**Abstract:** Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training. This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India. Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot's role in addressing CHWs' informational needs. We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors. CHWs trusted the information they received on ASHABot and treated it as an authoritative resource. CHWs' supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability. We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.

</details>


### [15] [Motion Design Principles for Accessible Video-based Learning: Addressing Cognitive Challenges for Deaf and Hard of Hearing Learners](https://arxiv.org/abs/2410.00196)

*Si Cheng, Haocong Cheng, Suzy Su, Lu Ming, Sarah Masud, Qi Wang, Yun Huang*

**Main category:** cs.HC

**Keywords:** Deaf and Hard-of-Hearing, video learning, Motion design, cognitive challenges, inclusive education

**Relevance Score:** 8

**TL;DR:** This paper presents Motion design guidelines to enhance video learning for Deaf and Hard-of-Hearing (DHH) learners by addressing cognitive challenges in processing visual and auditory information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve video-based learning experiences for Deaf and Hard-of-Hearing learners by addressing the cognitive overload that arises from traditional captioning methods.

**Method:** The paper identifies challenges through two phases of research and proposes Motion design principles based on user studies with DHH participants.

**Key Contributions:**

	1. Introduction of Motion design guidelines for DHH learners
	2. Identification of key cognitive challenges in video learning
	3. Demonstration of improved learning outcomes through user studies

**Result:** User studies revealed that improved visual-audio relevance and guidance in visual attention significantly enhance learning satisfaction and reduce cognitive load for DHH learners.

**Limitations:** Limited sample size in user study with only 16 participants.

**Conclusion:** Motion design has the potential to significantly improve educational content accessibility for DHH learners, suggesting a new direction for inclusive video learning tools.

**Abstract:** Deaf and Hard-of-Hearing (DHH) learners face unique challenges in video-based learning due to the complex interplay between visual and auditory information in videos. Traditional approaches to making video content accessible primarily focus on captioning, but these solutions often neglect the cognitive demands of processing both visual and textual information simultaneously. This paper introduces a set of \textit{Motion} design guidelines, aimed at mitigating these cognitive challenges and improving video learning experiences for DHH learners. Through a two-phase research, we identified five key challenges, including misaligned content and visual overload. We proposed five design principles accordingly. User study with 16 DHH participants showed that improving visual-audio relevance and guiding visual attention significantly enhances the learning experience by reducing physical demand, alleviating temporal pressure, and improving learning satisfaction. Our findings highlight the potential of Motion design to transform educational content for DHH learners, and we discuss implications for inclusive video learning tools.

</details>


### [16] [CounterQuill: Investigating the Potential of Human-AI Collaboration in Online Counterspeech Writing](https://arxiv.org/abs/2410.03032)

*Xiaohan Ding, Kaike Ping, Uma Sushmitha Gunturi, Buse Carik, Sophia Stil, Lance T Wilhelm, Taufiq Daryanto, James Hawdon, Sang Won Lee, Eugenia H Rho*

**Main category:** cs.HC

**Keywords:** Hate speech, Counterspeech, AI mediation, User engagement, Social media

**Relevance Score:** 6

**TL;DR:** Introducing CounterQuill, an AI-mediated system to assist users in creating effective counterspeech against online hate speech.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The prevalence of online hate speech on social media platforms and the inadequacy of current counterspeech methods.

**Method:** CounterQuill employs a three-step process: learning about hate speech, brainstorming counterspeech strategies, and co-writing the counterspeech with users.

**Key Contributions:**

	1. Introduces CounterQuill, an AI-assisted tool for counterspeech.
	2. Demonstrates the benefits of a collaborative writing process.
	3. Highlights the educational aspect of understanding hate speech and counterspeech.

**Result:** Users found that CounterQuill provided them with a stronger sense of ownership over their counterspeech than ChatGPT, making them more willing to post their work online.

**Limitations:** The study involved only 20 participants, which may limit the generalizability of the findings.

**Conclusion:** CounterQuill effectively enhances user engagement in counterspeech through its collaborative approach and educational component.

**Abstract:** Online hate speech has become increasingly prevalent on social media platforms, causing harm to individuals and society. While efforts have been made to combat this issue through content moderation, the potential of user-driven counterspeech as an alternative solution remains underexplored. Existing counterspeech methods often face challenges such as fear of retaliation and skill-related barriers. To address these challenges, we introduce CounterQuill, an AI-mediated system that assists users in composing effective and empathetic counterspeech. CounterQuill provides a three-step process: (1) a learning session to help users understand hate speech and counterspeech; (2) a brainstorming session that guides users in identifying key elements of hate speech and exploring counterspeech strategies; and (3) a co-writing session that enables users to draft and refine their counterspeech with CounterQuill. We conducted a within-subjects user study with 20 participants to evaluate CounterQuill in comparison to ChatGPT. Results show that CounterQuill's guidance and collaborative writing process provided users a stronger sense of ownership over their co-authored counterspeech. Users perceived CounterQuill as a writing partner and thus were more willing to post the co-written counterspeech online compared to the one written with ChatGPT.

</details>


### [17] [Advancing Human-Machine Teaming: Concepts, Challenges, and Applications](https://arxiv.org/abs/2503.16518)

*Dian Chen, Han Jun Yoon, Zelin Wan, Nithin Alluru, Sang Won Lee, Richard He, Terrence J. Moore, Frederica F. Nelson, Sunghyun Yoon, Hyuk Lim, Dan Dongseong Kim, Jin-Hee Cho*

**Main category:** cs.HC

**Keywords:** Human-Machine Teaming, AI-driven decision-making, ethical AI, team cognition, reinforcement learning

**Relevance Score:** 8

**TL;DR:** This survey analyzes Human-Machine Teaming (HMT) across several domains, presenting a taxonomy and discussing theoretical models and methodologies.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of AI-driven decision-making in collaborative environments like defense and healthcare, addressing key challenges and ethical considerations.

**Method:** A comprehensive survey examining theoretical models such as reinforcement learning and interdependence theory, alongside methodologies in team cognition and ethical AI.

**Key Contributions:**

	1. Comprehensive taxonomy of Human-Machine Teaming
	2. In-depth analysis of theoretical models including reinforcement learning
	3. Proposals for future research in cross-domain adaptation and ethical AI

**Result:** Identification of key challenges in HMT, including explainability and role allocation, with proposed directions for future research like trust-aware AI.

**Limitations:** 

**Conclusion:** The work provides a foundational framework for developing resilient and ethical Human-Machine Teaming systems across various domains.

**Abstract:** Human-Machine Teaming (HMT) is revolutionizing collaboration across domains such as defense, healthcare, and autonomous systems by integrating AI-driven decision-making, trust calibration, and adaptive teaming. This survey presents a comprehensive taxonomy of HMT, analyzing theoretical models, including reinforcement learning, instance-based learning, and interdependence theory, alongside interdisciplinary methodologies. Unlike prior reviews, we examine team cognition, ethical AI, multi-modal interactions, and real-world evaluation frameworks. Key challenges include explainability, role allocation, and scalable benchmarking. We propose future research in cross-domain adaptation, trust-aware AI, and standardized testbeds. By bridging computational and social sciences, this work lays a foundation for resilient, ethical, and scalable HMT systems.

</details>


### [18] [Catch Me if You Search: When Contextual Web Search Results Affect the Detection of Hallucinations](https://arxiv.org/abs/2504.01153)

*Mahjabin Nahar, Eun-Ju Lee, Jin Won Park, Dongwon Lee*

**Main category:** cs.HC

**Keywords:** Large Language Models, Web Search, Hallucinations, Human-Computer Interaction, User Confidence

**Relevance Score:** 9

**TL;DR:** An experiment studying the effect of static and dynamic web search results on the perceived accuracy of LLM-generated content and users' confidence in their assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether integrating web search results into LLMs helps users detect inaccuracies or hallucinations in the generated content.

**Method:** An online experiment with 560 participants, comparing three conditions: static search results, dynamic participant-led searches, and a control group with no web search.

**Key Contributions:**

	1. Investigates how web search results influence user perceptions of LLM accuracy.
	2. Demonstrates differences in user confidence between static and dynamic searching.
	3. Provides practical implications for LLM design in real-world applications.

**Result:** Participants exposed to web search results rated hallucinated LLM content as less accurate and had a more negative view of the LLM. Those in the dynamic search condition judged genuine content as more accurate and were more self-assured in their evaluations.

**Limitations:** The study relies on self-reported measures, which may introduce bias.

**Conclusion:** Incorporating web search functionality in LLMs can enhance user accuracy assessments and confidence, particularly in dynamic search scenarios.

**Abstract:** While we increasingly rely on large language models (LLMs) for various tasks, these models are known to produce inaccurate content or `hallucinations' with potentially disastrous consequences. The recent integration of web search results into LLMs prompts the question of whether people utilize them to verify the generated content, thereby accurately detecting hallucinations. An online experiment (N = 560) investigated how the provision of search results, either static (i.e., fixed search results provided by LLM) or dynamic (i.e., participant-led searches), affects participants' perceived accuracy of LLM-generated content (i.e., genuine, minor hallucination, major hallucination), self-confidence in accuracy ratings, as well as their overall evaluation of the LLM, as compared to the control condition (i.e., no search results). Results showed that participants in both static and dynamic conditions (vs. control) rated hallucinated content to be less accurate and perceived the LLM more negatively. However, those in the dynamic condition rated genuine content as more accurate and demonstrated greater overall self-confidence in their assessments than those in the static search or control conditions. We highlighted practical implications of incorporating web search functionality into LLMs in real-world contexts.

</details>


### [19] [Exploring Families' Use and Mediation of Generative AI: A Multi-User Perspective](https://arxiv.org/abs/2504.09004)

*Shirley Zhang, Bengisu Cagiltay, Jennica Li, Dakota Sullivan, Bilge Mutlu, Heather Kirkorian, Kassem Fawaz*

**Main category:** cs.HC

**Keywords:** Generative AI, family mediation, ChatGPT, parental control, safeguards

**Relevance Score:** 6

**TL;DR:** This study investigates families' interactions with Generative AI, particularly ChatGPT, exploring perceived risks and parental mediation strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the unique risks and opportunities presented by Generative AI for families and highlight the need for protective measures.

**Method:** Semi-structured interviews with 12 families were conducted to gather insights on how they use and mediate the use of GenAI.

**Key Contributions:**

	1. Identification of family mediation strategies for GenAI use
	2. Insights into parental perceptions of GenAI risks and opportunities
	3. Recommendations for improving protective measures on GenAI platforms

**Result:** Identified various ways families engaged with GenAI and the factors influencing parental mediation strategies.

**Limitations:** 

**Conclusion:** The findings emphasize the need for improved safeguards tailored for families and suggest a model for future research on family interactions with GenAI.

**Abstract:** Applications of Generative AI (GenAI), such as ChatGPT, have gained popularity among the public due to their ease of access, use, and support of educational and creative activities. Despite these benefits, GenAI poses unique risks for families, such as lacking sufficient safeguards tailored to protect children under 16 years of age and not offering parental control features. This study explores families' use and co-use of GenAI, the perceived risks and opportunities of ChatGPT, and how parents mediate their children's use of GenAI. Through semi-structured interviews with 12 families, we identified ways families used and mediated GenAI and factors that influenced parents' GenAI mediation strategies. We contextualize our findings with a modified model of family mediation strategies, drawing from previous family media and mediation frameworks. We provide insights for future research on family-GenAI interactions and highlight the need for more robust protective measures on GenAI platforms for families.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [20] [Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models](https://arxiv.org/abs/2505.02847)

*Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li*

**Main category:** cs.CL

**Keywords:** large language models, human emotion, social cognition, evaluative framework, empathetic agents

**Relevance Score:** 8

**TL;DR:** SAGE is an evaluation framework that assesses LLMs' understanding of human social cognition by simulating emotional changes and thoughts during conversations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of evaluating how well large language models understand human emotions and social interactions.

**Method:** SAGE simulates a Sentient Agent that reasons about its emotions and replies during multi-turn conversations, resulting in numerical emotion trajectories and interpretable thoughts.

**Key Contributions:**

	1. Introduction of SAGE framework for evaluating LLMs' social cognition.
	2. Development of a public Sentient Leaderboard for comparing empathy metrics across models.
	3. Demonstration of SAGE's ability to uncover performance gaps not seen in traditional evaluation methods.

**Result:** Experiments indicate a strong correlation between the final Sentient emotion score and established psychological metrics, validating the framework's effectiveness in measuring social cognition.

**Limitations:** 

**Conclusion:** SAGE offers a scalable tool for evaluating empathetic capabilities of language models, revealing significant gaps in performance among current systems.

**Abstract:** Assessing how well a large language model (LLM) understands human, rather than merely text, remains an open challenge. To bridge the gap, we introduce Sentient Agent as a Judge (SAGE), an automated evaluation framework that measures an LLM's higher-order social cognition. SAGE instantiates a Sentient Agent that simulates human-like emotional changes and inner thoughts during interaction, providing a more realistic evaluation of the tested model in multi-turn conversations. At every turn, the agent reasons about (i) how its emotion changes, (ii) how it feels, and (iii) how it should reply, yielding a numerical emotion trajectory and interpretable inner thoughts. Experiments on 100 supportive-dialogue scenarios show that the final Sentient emotion score correlates strongly with Barrett-Lennard Relationship Inventory (BLRI) ratings and utterance-level empathy metrics, validating psychological fidelity. We also build a public Sentient Leaderboard covering 18 commercial and open-source models that uncovers substantial gaps (up to 4x) between frontier systems (GPT-4o-Latest, Gemini2.5-Pro) and earlier baselines, gaps not reflected in conventional leaderboards (e.g., Arena). SAGE thus provides a principled, scalable and interpretable tool for tracking progress toward genuinely empathetic and socially adept language agents.

</details>


### [21] [Harnessing Structured Knowledge: A Concept Map-Based Approach for High-Quality Multiple Choice Question Generation with Effective Distractors](https://arxiv.org/abs/2505.02850)

*Nicy Scaria, Silvester John Joseph Kennedy, Diksha Seth, Ananya Thakur, Deepak Subramani*

**Main category:** cs.CL

**Keywords:** Multiple-Choice Questions, Cognitive Levels, Common Misconceptions, Physics Education, Machine Learning

**Relevance Score:** 7

**TL;DR:** This paper introduces a hierarchical concept map-based framework to automate the generation of high-quality multiple-choice questions (MCQs) that address misconceptions in high-school physics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Generating high-quality MCQs is challenging and often requires extensive expertise and time, making manual creation impractical at scale. Existing automated approaches lack the ability to address cognitive diversity and common misconceptions.

**Method:** The framework utilizes a hierarchical concept map to structure knowledge, guiding LLMs in generating MCQs and distractors that focus on common misconceptions. An automated pipeline retrieves relevant sections of the concept map, ensuring contextually relevant question generation, followed by validation to meet quality criteria.

**Key Contributions:**

	1. Introduction of a structured knowledge framework for MCQ generation
	2. Effective incorporation of common misconceptions into distractor design
	3. Significant improvement in assessment quality over existing methods

**Result:** The proposed method achieved a success rate of 75.20% in meeting quality criteria during expert evaluations, significantly outperforming baseline methods (approximately 37%). Student assessments showed a guess success rate of 28.05% for the new approach compared to 37.10% for baselines.

**Limitations:** 

**Conclusion:** The concept map-based framework enables effective assessment across various cognitive levels, quickly identifying conceptual gaps and allowing for targeted educational interventions at scale.

**Abstract:** Generating high-quality MCQs, especially those targeting diverse cognitive levels and incorporating common misconceptions into distractor design, is time-consuming and expertise-intensive, making manual creation impractical at scale. Current automated approaches typically generate questions at lower cognitive levels and fail to incorporate domain-specific misconceptions. This paper presents a hierarchical concept map-based framework that provides structured knowledge to guide LLMs in generating MCQs with distractors. We chose high-school physics as our test domain and began by developing a hierarchical concept map covering major Physics topics and their interconnections with an efficient database design. Next, through an automated pipeline, topic-relevant sections of these concept maps are retrieved to serve as a structured context for the LLM to generate questions and distractors that specifically target common misconceptions. Lastly, an automated validation is completed to ensure that the generated MCQs meet the requirements provided. We evaluate our framework against two baseline approaches: a base LLM and a RAG-based generation. We conducted expert evaluations and student assessments of the generated MCQs. Expert evaluation shows that our method significantly outperforms the baseline approaches, achieving a success rate of 75.20% in meeting all quality criteria compared to approximately 37% for both baseline methods. Student assessment data reveal that our concept map-driven approach achieved a significantly lower guess success rate of 28.05% compared to 37.10% for the baselines, indicating a more effective assessment of conceptual understanding. The results demonstrate that our concept map-based approach enables robust assessment across cognitive levels and instant identification of conceptual gaps, facilitating faster feedback loops and targeted interventions at scale.

</details>


### [22] [30DayGen: Leveraging LLMs to Create a Content Corpus for Habit Formation](https://arxiv.org/abs/2505.02851)

*Franklin Zhang, Sonya Zhang, Alon Halevy*

**Main category:** cs.CL

**Keywords:** habit formation, large language models, content generation, educational technology, behavioral change

**Relevance Score:** 9

**TL;DR:** The paper presents 30 Day Me, a habit formation app utilizing LLMs to help users set and track actionable 30-day challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist users in forming new habits by breaking down larger goals into smaller, actionable steps using LLM technology.

**Method:** The 30DAYGEN system generates unique challenges and allows runtime search for ideas aligned with user goals, sourcing from a vast web corpus.

**Key Contributions:**

	1. Introduction of the 30DAYGEN system for generating unique challenges.
	2. Demonstration of LLM capabilities in content generation for behavioral changes.
	3. Proposal of a practical pipeline for semantic deduplication in challenge creation.

**Result:** The app generates over 3,500 unique challenges and effectively uses LLMs for content creation and semantic deduplication.

**Limitations:** 

**Conclusion:** The proposed pipeline showcases how LLMs can enhance habit formation applications by generating tailored content

**Abstract:** In this paper, we present 30 Day Me, a habit formation application that leverages Large Language Models (LLMs) to help users break down their goals into manageable, actionable steps and track their progress. Central to the app is the 30DAYGEN system, which generates 3,531 unique 30-day challenges sourced from over 15K webpages, and enables runtime search of challenge ideas aligned with user-defined goals. We showcase how LLMs can be harnessed to rapidly construct domain specific content corpora for behavioral and educational purposes, and propose a practical pipeline that incorporates effective LLM enhanced approaches for content generation and semantic deduplication.

</details>


### [23] [Ensuring Reproducibility in Generative AI Systems for General Use Cases: A Framework for Regression Testing and Open Datasets](https://arxiv.org/abs/2505.02854)

*Masumi Morishige, Ryo Koshihara*

**Main category:** cs.CL

**Keywords:** Generative AI, Reproducibility, Benchmarking, Prompt Engineering, Language Models

**Relevance Score:** 8

**TL;DR:** GPR-bench is a benchmark for regression testing generative AI systems using bilingual datasets and automated evaluation, addressing reproducibility and reliability challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the pressing challenge of reproducibility and reliability in generative AI systems as model behavior can change with updates or prompt modifications.

**Method:** GPR-bench combines an open dataset in English and Japanese with an automated evaluation system that rates correctness and conciseness using LLM-as-a-Judge.

**Key Contributions:**

	1. Introduction of a lightweight and extensible benchmark for regression testing in generative AI.
	2. Bilingual dataset covering multiple task categories and scenarios.
	3. Insights into the effectiveness of prompt engineering on model performance.

**Result:** Experiments showed that while newer models generally improve correctness, these differences are modest and not statistically significant. Concise writing instructions improved conciseness significantly with minimal impact on accuracy.

**Limitations:** GPR-bench may not sufficiently challenge to differentiate between recent model versions due to modest improvement in correctness.

**Conclusion:** GPR-bench aids reproducibility monitoring and offers a framework for future extensions, while also presenting challenges for benchmark design of evolving language models.

**Abstract:** Reproducibility and reliability remain pressing challenges for generative AI systems whose behavior can drift with each model update or prompt revision. We introduce GPR-bench, a lightweight, extensible benchmark that operationalizes regression testing for general purpose use cases. GPR-bench couples an open, bilingual (English and Japanese) dataset covering eight task categories (e.g., text generation, code generation, and information retrieval) and 10 scenarios in each task categories (80 total test cases for each language) with an automated evaluation pipeline that employs "LLM-as-a-Judge" scoring of correctness and conciseness. Experiments across three recent model versions - gpt-4o-mini, o3-mini, and o4-mini - and two prompt configurations (default versus concise-writing instruction) reveal heterogeneous quality. Our results show that newer models generally improve correctness, but the differences are modest and not statistically significant, suggesting that GPR-bench may not be sufficiently challenging to differentiate between recent model versions. In contrast, the concise-writing instruction significantly enhances conciseness (+12.37 pp, Mann-Whitney U test: p < 0.001, effect size r = 0.2995) with minimal degradations on accuracy (-1.7 pp), demonstrating the effectiveness of prompt engineering. Released under the MIT License, GPR- bench lowers the barrier to initiating reproducibility monitoring and provides a foundation for community-driven extensions, while also raising important considerations about benchmark design for rapidly evolving language models.

</details>


### [24] [Towards High-Fidelity Synthetic Multi-platform Social Media Datasets via Large Language Models](https://arxiv.org/abs/2505.02858)

*Henry Tari, Nojus Sereiva, Rishabh Kaushal, Thales Bertaglia, Adriana Iamnitchi*

**Main category:** cs.CL

**Keywords:** Large language models, Synthetic datasets, Social media research

**Relevance Score:** 6

**TL;DR:** This paper discusses how large language models can be used to create synthetic social media datasets across multiple platforms, highlighting their potential to match the fidelity of real data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper seeks to address the challenges in accessing diverse social media datasets needed for various research areas due to costs and platform limitations.

**Method:** The authors propose a methodology based on multi-platform topic-based prompting and leverage different language models to generate synthetic datasets derived from real social media posts across three platforms.

**Key Contributions:**

	1. Methodology for generating synthetic social media datasets using large language models
	2. Empirical evaluation of fidelity of synthetic data versus real data
	3. Introduction of new fidelity metrics for multi-platform datasets

**Result:** The study reveals that synthetic data generated by large language models can maintain lexical and semantic relevance compared to real data, with variability in performance across different models, suggesting the need for post-processing to enhance fidelity.

**Limitations:** The need for post-processing for high-fidelity dataset generation and variability in language model performance.

**Conclusion:** Large language models offer a promising approach for generating multi-platform social media datasets, though specific metrics for fidelity need to be developed for better assessments.

**Abstract:** Social media datasets are essential for research on a variety of topics, such as disinformation, influence operations, hate speech detection, or influencer marketing practices. However, access to social media datasets is often constrained due to costs and platform restrictions. Acquiring datasets that span multiple platforms, which is crucial for understanding the digital ecosystem, is particularly challenging. This paper explores the potential of large language models to create lexically and semantically relevant social media datasets across multiple platforms, aiming to match the quality of real data. We propose multi-platform topic-based prompting and employ various language models to generate synthetic data from two real datasets, each consisting of posts from three different social media platforms. We assess the lexical and semantic properties of the synthetic data and compare them with those of the real data. Our empirical findings show that using large language models to generate synthetic multi-platform social media data is promising, different language models perform differently in terms of fidelity, and a post-processing approach might be needed for generating high-fidelity synthetic datasets for research. In addition to the empirical evaluation of three state of the art large language models, our contributions include new fidelity metrics specific to multi-platform social media datasets.

</details>


### [25] [Enhancing ML Model Interpretability: Leveraging Fine-Tuned Large Language Models for Better Understanding of AI](https://arxiv.org/abs/2505.02859)

*Jonas Bokstaller, Julia Altheimer, Julian Dormehl, Alina Buss, Jasper Wiltfang, Johannes Schneider, Maximilian Röglinger*

**Main category:** cs.CL

**Keywords:** Explainable AI, Large Language Models, State-of-Health prediction, Machine Learning, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper presents a novel architecture combining Explainable AI (XAI) and Large Language Models (LLMs) via an interactive chatbot to improve human interpretability of machine learning models, specifically in battery State-of-Health (SoH) prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of black-box ML models and improve interpretability for users with less experience with XAI.

**Method:** A reference architecture is designed and instantiated using a fine-tuned LLM, implemented as an interactive chatbot for interpreting XAI in battery SoH prediction.

**Key Contributions:**

	1. Development of a novel reference architecture for XAI interpretation using LLMs
	2. Implementation of an interactive chatbot to facilitate user comprehension
	3. Validation of the approach in the context of battery health prediction

**Result:** The evaluation shows that the prototype significantly enhances human interpretability of ML, especially for users not well-versed in XAI.

**Limitations:** 

**Conclusion:** The proposed architecture effectively bridges the gap between complex ML models and user understanding, particularly for non-expert users.

**Abstract:** Across various sectors applications of eXplainableAI (XAI) gained momentum as the increasing black-boxedness of prevailing Machine Learning (ML) models became apparent. In parallel, Large Language Models (LLMs) significantly developed in their abilities to understand human language and complex patterns. By combining both, this paper presents a novel reference architecture for the interpretation of XAI through an interactive chatbot powered by a fine-tuned LLM. We instantiate the reference architecture in the context of State-of-Health (SoH) prediction for batteries and validate its design in multiple evaluation and demonstration rounds. The evaluation indicates that the implemented prototype enhances the human interpretability of ML, especially for users with less experience with XAI.

</details>


### [26] [Cannot See the Forest for the Trees: Invoking Heuristics and Biases to Elicit Irrational Choices of LLMs](https://arxiv.org/abs/2505.02862)

*Haoming Yang, Ke Ma, Xiaojun Jia, Yingfei Sun, Qianqian Xu, Qingming Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Jailbreak attacks, Cognitive biases, Harmfulness evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel framework for jailbreak attacks on LLMs, employing cognitive biases to enhance attack effectiveness and propose new evaluation metrics for harmful outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To expose the vulnerabilities of LLMs to jailbreak attacks that can undermine their safety mechanisms.

**Method:** A novel framework called ICRT is developed, utilizing cognitive decomposition and relevance bias to craft malicious prompts, along with a new ranking-based harmfulness evaluation metric.

**Key Contributions:**

	1. Introduction of the ICRT framework for jailbreak attacks
	2. Utilization of cognitive biases for prompt optimization
	3. Development of a ranking-based harmfulness evaluation metric

**Result:** Experimental results demonstrate that the proposed ICRT framework consistently bypasses the safety mechanisms of mainstream LLMs and generates high-risk outputs.

**Limitations:** 

**Conclusion:** The insights gained from the study highlight the risks of jailbreak attacks and contribute to the development of better defense strategies for LLMs.

**Abstract:** Despite the remarkable performance of Large Language Models (LLMs), they remain vulnerable to jailbreak attacks, which can compromise their safety mechanisms. Existing studies often rely on brute-force optimization or manual design, failing to uncover potential risks in real-world scenarios. To address this, we propose a novel jailbreak attack framework, ICRT, inspired by heuristics and biases in human cognition. Leveraging the simplicity effect, we employ cognitive decomposition to reduce the complexity of malicious prompts. Simultaneously, relevance bias is utilized to reorganize prompts, enhancing semantic alignment and inducing harmful outputs effectively. Furthermore, we introduce a ranking-based harmfulness evaluation metric that surpasses the traditional binary success-or-failure paradigm by employing ranking aggregation methods such as Elo, HodgeRank, and Rank Centrality to comprehensively quantify the harmfulness of generated content. Experimental results show that our approach consistently bypasses mainstream LLMs' safety mechanisms and generates high-risk content, providing insights into jailbreak attack risks and contributing to stronger defense strategies.

</details>


### [27] [Accelerating Large Language Model Reasoning via Speculative Search](https://arxiv.org/abs/2505.02865)

*Zhihai Wang, Jie Wang, Jilai Pan, Xilin Xia, Huiling Zhen, Mingxuan Yuan, Jianye Hao, Feng Wu*

**Main category:** cs.CL

**Keywords:** Speculative Search, LLM reasoning, thought generation

**Relevance Score:** 8

**TL;DR:** A new framework, Speculative Search (SpecSearch), accelerates reasoning in large language models (LLMs) by optimizing thought generation while maintaining quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce inference latency in tree-search-based reasoning methods for LLMs, which involves generating numerous intermediate thoughts.

**Method:** SpecSearch uses a small model to collaborate with a large model for generating reasoning thoughts, featuring a quality-preserving rejection mechanism to filter out low-quality thoughts.

**Key Contributions:**

	1. Introduction of the SpecSearch framework for optimizing LLM reasoning
	2. Implementation of a quality-preserving rejection mechanism
	3. Demonstrated state-of-the-art speedup and quality preservation

**Result:** SpecSearch achieves up to 2.12x speedup in reasoning speed while preserving comparable reasoning quality to that of large models.

**Limitations:** 

**Conclusion:** The SpecSearch framework provides an effective solution for faster LLM reasoning without compromising output quality.

**Abstract:** Tree-search-based reasoning methods have significantly enhanced the reasoning capability of large language models (LLMs) by facilitating the exploration of multiple intermediate reasoning steps, i.e., thoughts. However, these methods suffer from substantial inference latency, as they have to generate numerous reasoning thoughts, severely limiting LLM applicability. To address this challenge, we propose a novel Speculative Search (SpecSearch) framework that significantly accelerates LLM reasoning by optimizing thought generation. Specifically, SpecSearch utilizes a small model to strategically collaborate with a large model at both thought and token levels, efficiently generating high-quality reasoning thoughts. The major pillar of SpecSearch is a novel quality-preserving rejection mechanism, which effectively filters out thoughts whose quality falls below that of the large model's outputs. Moreover, we show that SpecSearch preserves comparable reasoning quality to the large model. Experiments on both the Qwen and Llama models demonstrate that SpecSearch significantly outperforms state-of-the-art approaches, achieving up to 2.12$\times$ speedup with comparable reasoning quality.

</details>


### [28] [Decoding Open-Ended Information Seeking Goals from Eye Movements in Reading](https://arxiv.org/abs/2505.02872)

*Cfir Avraham Hadar, Omer Shubi, Yoav Meiri, Yevgeni Berzak*

**Main category:** cs.CL

**Keywords:** eye movements, reading goals, LLMs, goal classification, multimodal

**Relevance Score:** 8

**TL;DR:** The paper explores whether open-ended reading goals can be decoded from eye movements using LLMs, introducing new tasks and evaluation frameworks for goal classification and reconstruction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how readers' text-specific goals influence reading behavior and whether these goals can be inferred from eye movement data.

**Method:** The authors introduced goal classification and goal reconstruction tasks, employing large-scale eye tracking data and several multimodal LLMs to analyze eye movements in conjunction with text.

**Key Contributions:**

	1. Introduction of goal classification and goal reconstruction tasks for reading research
	2. Development of new evaluation frameworks for these tasks
	3. Demonstration of LLMs' capability to decode text-specific reading goals from eye movements.

**Result:** Experiments demonstrated significant success in both goal classification and goal reconstruction, indicating that LLMs can effectively decode reading goals from eye movements.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs are capable of extracting meaningful information about a reader's intentions from their eye movement patterns, enhancing our understanding of reading behaviors.

**Abstract:** When reading, we often have specific information that interests us in a text. For example, you might be reading this paper because you are curious about LLMs for eye movements in reading, the experimental design, or perhaps you only care about the question ``but does it work?''. More broadly, in daily life, people approach texts with any number of text-specific goals that guide their reading behavior. In this work, we ask, for the first time, whether open-ended reading goals can be automatically decoded from eye movements in reading. To address this question, we introduce goal classification and goal reconstruction tasks and evaluation frameworks, and use large-scale eye tracking for reading data in English with hundreds of text-specific information seeking tasks. We develop and compare several discriminative and generative multimodal LLMs that combine eye movements and text for goal classification and goal reconstruction. Our experiments show considerable success on both tasks, suggesting that LLMs can extract valuable information about the readers' text-specific goals from eye movements.

</details>


### [29] [Logits-Constrained Framework with RoBERTa for Ancient Chinese NER](https://arxiv.org/abs/2505.02983)

*Wenjie Hua, Shenghan Xu*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, GujiRoBERTa, Ancient Chinese NLP

**Relevance Score:** 2

**TL;DR:** A Logits-Constrained framework for Ancient Chinese Named Entity Recognition using GujiRoBERTa shows improved performance over traditional methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance Named Entity Recognition for Ancient Chinese by integrating modern techniques and improving upon existing methods.

**Method:** A two-stage model utilizing GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions.

**Key Contributions:**

	1. Introduction of Logits-Constrained framework for NER
	2. Integration of GujiRoBERTa into the NER process
	3. Development of a model selection criterion for Ancient Chinese NLP tasks.

**Result:** The LC framework outperforms traditional CRF and BiLSTM approaches, particularly in scenarios with high label complexity and large datasets.

**Limitations:** 

**Conclusion:** The proposed model selection criterion aids in balancing between label complexity and dataset size, offering practical solutions for Ancient Chinese NLP tasks.

**Abstract:** This paper presents a Logits-Constrained (LC) framework for Ancient Chinese Named Entity Recognition (NER), evaluated on the EvaHan 2025 benchmark. Our two-stage model integrates GujiRoBERTa for contextual encoding and a differentiable decoding mechanism to enforce valid BMES label transitions. Experiments demonstrate that LC improves performance over traditional CRF and BiLSTM-based approaches, especially in high-label or large-data settings. We also propose a model selection criterion balancing label complexity and dataset size, providing practical guidance for real-world Ancient Chinese NLP tasks.

</details>


### [30] [RADLADS: Rapid Attention Distillation to Linear Attention Decoders at Scale](https://arxiv.org/abs/2505.03005)

*Daniel Goldstein, Eric Alcaide, Janna Lu, Eugene Cheah*

**Main category:** cs.CL

**Keywords:** linear attention, transformers, RWKV, HuggingFace

**Relevance Score:** 6

**TL;DR:** RADLADS converts softmax attention transformers into efficient linear attention models with low training requirements, achieving competitive performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies of softmax attention transformers while maintaining high performance in downstream tasks.

**Method:** Introduced RADLADS, a protocol for converting transformers to linear attention decoders, requiring significantly fewer tokens for training compared to standard methods.

**Key Contributions:**

	1. Introduction of RADLADS protocol
	2. Development of new RWKV-variant architectures
	3. Models released on HuggingFace under open licenses

**Result:** Achieved state-of-the-art performance for linear attention models at various sizes with significantly reduced training costs and minimal token requirement.

**Limitations:** 

**Conclusion:** The RADLADS method demonstrates an efficient pathway for transitioning from traditional transformers to linear attention models without substantial loss in performance.

**Abstract:** We present Rapid Attention Distillation to Linear Attention Decoders at Scale (RADLADS), a protocol for rapidly converting softmax attention transformers into linear attention decoder models, along with two new RWKV-variant architectures, and models converted from popular Qwen2.5 open source models in 7B, 32B, and 72B sizes. Our conversion process requires only 350-700M tokens, less than 0.005% of the token count used to train the original teacher models. Converting to our 72B linear attention model costs less than \$2,000 USD at today's prices, yet quality at inference remains close to the original transformer. These models achieve state-of-the-art downstream performance across a set of standard benchmarks for linear attention models of their size. We release all our models on HuggingFace under the Apache 2.0 license, with the exception of our 72B models which are also governed by the Qwen License Agreement.   Models at https://huggingface.co/collections/recursal/radlads-6818ee69e99e729ba8a87102 Training Code at https://github.com/recursal/RADLADS-paper

</details>


### [31] [Memorization or Interpolation ? Detecting LLM Memorization through Input Perturbation Analysis](https://arxiv.org/abs/2505.03019)

*Albérick Euraste Djiré, Abdoul Kader Kaboré, Earl T. Barr, Jacques Klein, Tegawendé F. Bissyandé*

**Main category:** cs.CL

**Keywords:** Large Language Models, Memorization Detection, PEARL, Input Perturbations, Data Privacy

**Relevance Score:** 9

**TL;DR:** PEARL is a new framework for detecting memorization in large language models (LLMs) through input perturbations, addressing privacy and generalization concerns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to tackle the memorization phenomenon in LLMs, which can lead to issues related to data privacy and the reliability of model evaluations.

**Method:** PEARL assesses sensitivity of LLM performance to input perturbations to detect memorization without needing access to model internals.

**Key Contributions:**

	1. Introduction of PEARL for memorization detection
	2. Framework operates without model internals access
	3. Identification of memorization instances in various texts

**Result:** Experiments on the Pythia model and GPT 4o demonstrated that PEARL can identify instances of memorization across various texts and provide evidence of training data inclusion.

**Limitations:** 

**Conclusion:** The PEARL framework facilitates the detection of memorization, enhancing understanding of LLM behavior and aiding in concerns regarding data usage and privacy.

**Abstract:** While Large Language Models (LLMs) achieve remarkable performance through training on massive datasets, they can exhibit concerning behaviors such as verbatim reproduction of training data rather than true generalization. This memorization phenomenon raises significant concerns about data privacy, intellectual property rights, and the reliability of model evaluations. This paper introduces PEARL, a novel approach for detecting memorization in LLMs. PEARL assesses how sensitive an LLM's performance is to input perturbations, enabling memorization detection without requiring access to the model's internals. We investigate how input perturbations affect the consistency of outputs, enabling us to distinguish between true generalization and memorization. Our findings, following extensive experiments on the Pythia open model, provide a robust framework for identifying when the model simply regurgitates learned information. Applied on the GPT 4o models, the PEARL framework not only identified cases of memorization of classic texts from the Bible or common code from HumanEval but also demonstrated that it can provide supporting evidence that some data, such as from the New York Times news articles, were likely part of the training data of a given model.

</details>


### [32] [A Typology of Synthetic Datasets for Dialogue Processing in Clinical Contexts](https://arxiv.org/abs/2505.03025)

*Steven Bedrick, A. Seza Doğruöz, Sergiu Nisioi*

**Main category:** cs.CL

**Keywords:** synthetic data, clinical dialogues, data synthesis, NLP, healthcare

**Relevance Score:** 8

**TL;DR:** Overview of synthetic data sets in clinical dialogues, their creation, evaluation, and a proposed typology for effective usage.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges related to the limited availability of authentic clinical dialogue data due to privacy and data governance issues.

**Method:** The paper reviews existing methods for creating and evaluating synthetic datasets in medical dialogues and introduces a typology for classifying data synthesis types.

**Key Contributions:**

	1. Comprehensive overview of synthetic datasets in clinical dialogue
	2. Introduction of a novel typology for data synthesis
	3. Discussion on the evaluation methods for synthetic datasets in healthcare

**Result:** Synthetic datasets have shown sufficient performance in some dialogue tasks but lack theoretical guidance for optimal use across various applications.

**Limitations:** The theoretical framework for optimizing the usage of synthetic datasets remains underdeveloped.

**Conclusion:** The proposed typology aims to enhance the understanding and evaluation of synthetic datasets in clinical contexts, promoting better application in real-world medical dialogue tasks.

**Abstract:** Synthetic data sets are used across linguistic domains and NLP tasks, particularly in scenarios where authentic data is limited (or even non-existent). One such domain is that of clinical (healthcare) contexts, where there exist significant and long-standing challenges (e.g., privacy, anonymization, and data governance) which have led to the development of an increasing number of synthetic datasets. One increasingly important category of clinical dataset is that of clinical dialogues which are especially sensitive and difficult to collect, and as such are commonly synthesized.   While such synthetic datasets have been shown to be sufficient in some situations, little theory exists to inform how they may be best used and generalized to new applications. In this paper, we provide an overview of how synthetic datasets are created, evaluated and being used for dialogue related tasks in the medical domain. Additionally, we propose a novel typology for use in classifying types and degrees of data synthesis, to facilitate comparison and evaluation.

</details>


### [33] [UCSC at SemEval-2025 Task 3: Context, Models and Prompt Optimization for Automated Hallucination Detection in LLM Output](https://arxiv.org/abs/2505.03030)

*Sicong Huang, Jincheng He, Shiyuan Huang, Karthik Raja Anandan, Arkajyoti Chakraborty, Ian Lane*

**Main category:** cs.CL

**Keywords:** hallucinations, large language models, Mu-SHROOM, multilingual, prompt optimization

**Relevance Score:** 8

**TL;DR:** The paper presents a framework for detecting and mapping hallucinations in LLM outputs, achieving top performance in the Mu-SHROOM multilingual task.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To effectively detect and locate hallucinations in large language model outputs, as their use grows in knowledge-intensive applications.

**Method:** The framework retrieves relevant context, identifies erroneous content in the LLM output, and maps it back to specific spans, with an optimization of prompts to enhance the process.

**Key Contributions:**

	1. Introduction of a framework for hallucination detection in LLM outputs.
	2. Achieving the highest performance in the Mu-SHROOM task across all languages.
	3. Release of code and experimental results for community use.

**Result:** The proposed system outperformed all other submissions, ranking #1 in average position across multiple languages during the SemEval 2025 Task 3, Mu-SHROOM.

**Limitations:** 

**Conclusion:** The implementation and results of our system demonstrate the effectiveness of our approach for hallucination detection in LLMs, and we provide our code for further research.

**Abstract:** Hallucinations pose a significant challenge for large language models when answering knowledge-intensive queries. As LLMs become more widely adopted, it is crucial not only to detect if hallucinations occur but also to pinpoint exactly where in the LLM output they occur. SemEval 2025 Task 3, Mu-SHROOM: Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes, is a recent effort in this direction. This paper describes the UCSC system submission to the shared Mu-SHROOM task. We introduce a framework that first retrieves relevant context, next identifies false content from the answer, and finally maps them back to spans in the LLM output. The process is further enhanced by automatically optimizing prompts. Our system achieves the highest overall performance, ranking #1 in average position across all languages. We release our code and experiment results.

</details>


### [34] [Teaching Models to Understand (but not Generate) High-risk Data](https://arxiv.org/abs/2505.03052)

*Ryan Wang, Matthew Finlayson, Luca Soldaini, Swabha Swayamdipta, Robin Jia*

**Main category:** cs.CL

**Keywords:** language models, high-risk content, selective loss, toxicity recognition, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The SLUNG paradigm allows language models to understand high-risk content without generating it, improving recognition of such content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance models' ability to recognize harmful or sensitive content without generating undesirable outputs by filtering out high-risk data from pre-training.

**Method:** Introduces Selective Loss to Understand but Not Generate (SLUNG), which selectively avoids incentivizing the generation of high-risk tokens while retaining them in the context for recognition tasks during pre-training.

**Key Contributions:**

	1. Introduces a novel pre-training paradigm (SLUNG) for language models
	2. Demonstrates improved understanding of high-risk data without harmful generation
	3. Provides experimental validation of the effectiveness of SLUNG in HCI applications.

**Result:** SLUNG improves models' understanding of high-risk data significantly (e.g., toxicity recognition) without increasing the generation of harmful responses.

**Limitations:** 

**Conclusion:** The SLUNG framework allows language models to benefit from high-risk text that would otherwise be discarded, enhancing their contextual understanding without compromising output safety.

**Abstract:** Language model developers typically filter out high-risk content -- such as toxic or copyrighted text -- from their pre-training data to prevent models from generating similar outputs. However, removing such data altogether limits models' ability to recognize and appropriately respond to harmful or sensitive content. In this paper, we introduce Selective Loss to Understand but Not Generate (SLUNG), a pre-training paradigm through which models learn to understand high-risk data without learning to generate it. Instead of uniformly applying the next-token prediction loss, SLUNG selectively avoids incentivizing the generation of high-risk tokens while ensuring they remain within the model's context window. As the model learns to predict low-risk tokens that follow high-risk ones, it is forced to understand the high-risk content. Through our experiments, we show that SLUNG consistently improves models' understanding of high-risk data (e.g., ability to recognize toxic content) without increasing its generation (e.g., toxicity of model responses). Overall, our SLUNG paradigm enables models to benefit from high-risk text that would otherwise be filtered out.

</details>


### [35] [Developing A Framework to Support Human Evaluation of Bias in Generated Free Response Text](https://arxiv.org/abs/2505.03053)

*Jennifer Healey, Laurie Byrum, Md Nadeem Akhtar, Surabhi Bhargava, Moumita Sinha*

**Main category:** cs.CL

**Keywords:** LLM evaluation, Bias evaluation, Human evaluation, Bias classification, Language models

**Relevance Score:** 9

**TL;DR:** This paper presents a semi-automated framework for evaluating bias in LLM free text responses, incorporating human insights and a refined methodology for bias classification.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of evaluating LLM bias in real-world deployments where context and prompts can significantly vary.

**Method:** The authors developed an operational definition of bias to automate their evaluation pipeline and established a methodology that extends bias classification beyond fixed-choice benchmarks.

**Key Contributions:**

	1. Development of a semi-automated bias evaluation framework for LLMs
	2. Operational definition of bias for automating evaluation
	3. Identification of problematic templates in bias benchmarks through human insights

**Result:** The framework highlighted problematic templates in existing bias benchmarks, enhancing the evaluation process of language models beyond limited conditions.

**Limitations:** 

**Conclusion:** Human insights proved essential in the development of a new methodology for bias classification, demonstrating the importance of a nuanced approach in LLM evaluation.

**Abstract:** LLM evaluation is challenging even the case of base models. In real world deployments, evaluation is further complicated by the interplay of task specific prompts and experiential context. At scale, bias evaluation is often based on short context, fixed choice benchmarks that can be rapidly evaluated, however, these can lose validity when the LLMs' deployed context differs. Large scale human evaluation is often seen as too intractable and costly. Here we present our journey towards developing a semi-automated bias evaluation framework for free text responses that has human insights at its core. We discuss how we developed an operational definition of bias that helped us automate our pipeline and a methodology for classifying bias beyond multiple choice. We additionally comment on how human evaluation helped us uncover problematic templates in a bias benchmark.

</details>


### [36] [Improving Model Alignment Through Collective Intelligence of Open-Source LLMS](https://arxiv.org/abs/2505.03059)

*Junlin Wang, Roy Xie, Shang Zhu, Jue Wang, Ben Athiwaratkun, Bhuwan Dhingra, Shuaiwen Leon Song, Ce Zhang, James Zou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Model Alignment, Mixture of Agents, Data Generation, Synthetic Datasets

**Relevance Score:** 9

**TL;DR:** Introduction of Mixture of Agents Alignment (MoAA) to generate high-quality datasets for LLM model alignment using multiple language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Building effective large language models (LLMs) requires high-quality human-labeled data, which is expensive and difficult to scale due to limitations on diversity and generalization.

**Method:** Mixture of Agents Alignment (MoAA) leverages the strengths of various language models to generate high-quality alignment data, enhancing both supervised fine-tuning and preference optimization.

**Key Contributions:**

	1. Introduction of MoAA for improved dataset generation.
	2. Demonstrated enhancement of LLAma model performance.
	3. Evidence of models exceeding initial capabilities through self-improvement.

**Result:** MoAA significantly improved the win rate of LLaMA-3.1-8B-Instruct on evaluation tasks, suggesting improved performance in alignment tasks compared to single-model approaches.

**Limitations:** 

**Conclusion:** MoAA facilitates a self-improvement pipeline for language models and lays a foundation for scalable and diverse synthetic data generation for model alignment.

**Abstract:** Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback, which necessitates high-quality human-labeled data. Constructing such datasets is often expensive and hard to scale, and may face potential limitations on diversity and generalization. To address these challenges, we introduce Mixture of Agents Alignment (MoAA), that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning and preference optimization, leading to improved performance compared to using a single model alone to generate alignment data (e.g. using GPT-4o alone). Evaluation results show that our approach can improve win rate of LLaMA-3.1-8B-Instruct from 19.5 to 48.3 on Arena-Hard and from 22.33 to 57.23 on AlpacaEval2, highlighting a promising direction for model alignment through this new scalable and diverse synthetic data recipe. Furthermore, we demonstrate that MoAA enables a self-improvement pipeline, where models finetuned on MoA-generated data surpass their own initial capabilities, providing evidence that our approach can push the frontier of open-source LLMs without reliance on stronger external supervision. Data and code will be released.

</details>


### [37] [Survey of Abstract Meaning Representation: Then, Now, Future](https://arxiv.org/abs/2505.03229)

*Behrooz Mansouri*

**Main category:** cs.CL

**Keywords:** Abstract Meaning Representation, graph-based structure, NLP applications, semantic representation, information extraction

**Relevance Score:** 6

**TL;DR:** This survey discusses Abstract Meaning Representation (AMR), a graph-based semantic representation framework capturing sentence meanings and its implications on various NLP tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze and summarize the capabilities, parsing, and generation tasks of Abstract Meaning Representation (AMR) and its applications.

**Method:** The paper presents a survey of AMR, focusing on the graph-based structure of sentences and reviewing traditional and current approaches to text-to-AMR parsing and AMR-to-text generation.

**Key Contributions:**

	1. Comprehensive review of AMR framework and its capabilities
	2. Exploration of AMR applications in NLP tasks
	3. Identification of challenges and future research directions in AMR.

**Result:** The survey showcases various applications of AMR in text generation, classification, and information extraction, while also identifying recent developments and challenges in the field.

**Limitations:** 

**Conclusion:** Insights provided may help shape future research directions and enhance machine understanding of human language through AMR.

**Abstract:** This paper presents a survey of Abstract Meaning Representation (AMR), a semantic representation framework that captures the meaning of sentences through a graph-based structure. AMR represents sentences as rooted, directed acyclic graphs, where nodes correspond to concepts and edges denote relationships, effectively encoding the meaning of complex sentences. This survey investigates AMR and its extensions, focusing on AMR capabilities. It then explores the parsing (text-to-AMR) and generation (AMR-to-text) tasks by showing traditional, current, and possible futures approaches. It also reviews various applications of AMR including text generation, text classification, and information extraction and information seeking. By analyzing recent developments and challenges in the field, this survey provides insights into future directions for research and the potential impact of AMR on enhancing machine understanding of human language.

</details>


### [38] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)

*Mouath Abu Daoud, Chaimae Abouzahir, Leen Kharouf, Walid Al-Eisawi, Nizar Habash, Farah E. Shamout*

**Main category:** cs.CL

**Keywords:** Large Language Models, healthcare, Arabic medical domain, benchmark, MedArabiQ

**Relevance Score:** 9

**TL;DR:** This study presents MedArabiQ, a benchmark dataset for evaluating Large Language Models in Arabic medical tasks, addressing data scarcity for effective healthcare applications.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the promise of Large Language Models in healthcare, their effectiveness in the Arabic medical domain is uncharted due to insufficient high-quality datasets.

**Method:** The dataset was created from past medical exams and public datasets, encompassing various tasks like multiple choice questions and patient-doctor Q&A. Several LLMs were evaluated including GPT-4o and Claude 3.5-Sonnet, with modifications for bias assessment.

**Key Contributions:**

	1. Introduction of MedArabiQ, the first Arabic medical benchmark dataset
	2. Comprehensive evaluation of multiple state-of-the-art LLMs in the Arabic medical context
	3. Insights into the need for multilingual benchmarks in healthcare AI applications

**Result:** Evaluation of five advanced LLMs showed the necessity for high-quality multilingual benchmarks to support LLM deployment in healthcare.

**Limitations:** The study is limited by the current implementations of LLMs and the inherent challenges in dataset creation for specific tasks.

**Conclusion:** The creation and release of MedArabiQ provides a crucial foundation for future research on improving the multilingual capabilities of LLMs for fair AI usage in healthcare.

**Abstract:** Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.

</details>


### [39] [Ψ-Arena: Interactive Assessment and Optimization of LLM-based Psychological Counselors with Tripartite Feedback](https://arxiv.org/abs/2505.03293)

*Shijing Zhu, Zhuang Chen, Guanqun Bi, Binghang Li, Yaxi Deng, Dazhen Wan, Libiao Peng, Xiyao Xiao, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, FangFang Li, Minlie Huang*

**Main category:** cs.CL

**Keywords:** Large language models, mental health, counseling, evaluation, optimization

**Relevance Score:** 9

**TL;DR:** The paper introduces {}Arena, an interactive framework for assessing and optimizing LLM-based counselors in mental health, addressing limitations in existing evaluation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficacy and safety of LLMs in providing mental health support through effective evaluation mechanisms.

**Method:** The proposed framework includes realistic dialogues with psychologically profiled NPC clients, employs a tripartite evaluation perspective, and applies a closed-loop optimization method.

**Key Contributions:**

	1. Introduction of realistic arena interactions for LLM evaluation
	2. Tripartite evaluation integrating various perspectives
	3. Closed-loop optimization for iterative improvement of LLM counselors

**Result:** Experiments reveal substantial performance differences among LLMs and reflection-based optimizations yield up to a 141% improvement in counseling performance.

**Limitations:** 

**Conclusion:** {}Arena aims to serve as a foundational resource for improving LLM applications in mental healthcare, promoting reliability and human alignment.

**Abstract:** Large language models (LLMs) have shown promise in providing scalable mental health support, while evaluating their counseling capability remains crucial to ensure both efficacy and safety. Existing evaluations are limited by the static assessment that focuses on knowledge tests, the single perspective that centers on user experience, and the open-loop framework that lacks actionable feedback. To address these issues, we propose {\Psi}-Arena, an interactive framework for comprehensive assessment and optimization of LLM-based counselors, featuring three key characteristics: (1) Realistic arena interactions that simulate real-world counseling through multi-stage dialogues with psychologically profiled NPC clients, (2) Tripartite evaluation that integrates assessments from the client, counselor, and supervisor perspectives, and (3) Closed-loop optimization that iteratively improves LLM counselors using diagnostic feedback. Experiments across eight state-of-the-art LLMs show significant performance variations in different real-world scenarios and evaluation perspectives. Moreover, reflection-based optimization results in up to a 141% improvement in counseling performance. We hope PsychoArena provides a foundational resource for advancing reliable and human-aligned LLM applications in mental healthcare.

</details>


### [40] [Recall with Reasoning: Chain-of-Thought Distillation for Mamba's Long-Context Memory and Extrapolation](https://arxiv.org/abs/2505.03320)

*Junyu Ma, Tianqing Fang, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu*

**Main category:** cs.CL

**Keywords:** Mamba, long-context memory, chain-of-thought, fine-tuning, Recall with Reasoning

**Relevance Score:** 6

**TL;DR:** The paper presents a method called Recall with Reasoning (RwR) to enhance Mamba's ability to handle long-context memory by utilizing chain-of-thought prompts during fine-tuning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address Mamba's limitations in processing sequences that exceed training lengths effectively.

**Method:** Recall with Reasoning (RwR) involves distilling chain-of-thought summarization from a teacher model and using these as prompts during fine-tuning.

**Key Contributions:**

	1. Introduction of Recall with Reasoning (RwR) for long-context memory enhancement
	2. Demonstration of performance improvements on specific benchmarks
	3. Preservation of short-context efficacy alongside long-context improvements

**Result:** RwR significantly improves Mamba's long-context performance on LONGMEMEVAL and HELMET datasets, outperforming Transformer/hybrid baselines while maintaining short-context efficiency.

**Limitations:** 

**Conclusion:** RwR provides a straightforward solution to enhance the long-context capabilities of Mamba without requiring changes to its architecture.

**Abstract:** Mamba's theoretical infinite-context potential is limited in practice when sequences far exceed training lengths. This work explores unlocking Mamba's long-context memory ability by a simple-yet-effective method, Recall with Reasoning (RwR), by distilling chain-of-thought (CoT) summarization from a teacher model. Specifically, RwR prepends these summarization as CoT prompts during fine-tuning, teaching Mamba to actively recall and reason over long contexts. Experiments on LONGMEMEVAL and HELMET show RwR boosts Mamba's long-context performance against comparable Transformer/hybrid baselines under similar pretraining conditions, while preserving short-context capabilities, all without architectural changes.

</details>


### [41] [Lightweight Clinical Decision Support System using QLoRA-Fine-Tuned LLMs and Retrieval-Augmented Generation](https://arxiv.org/abs/2505.03406)

*Mohammad Shoaib Ansari, Mohd Sohail Ali Khan, Shubham Revankar, Aditya Varma, Anil S. Mokhade*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare, Retrieval-Augmented Generation, Quantized Low-Rank Adaptation, Medical Decision Support

**Relevance Score:** 10

**TL;DR:** This paper explores the use of Large Language Models (LLMs) for enhancing medical decision support via Retrieval-Augmented Generation (RAG) and QLoRA, focusing on scalability and practical challenges in healthcare applications.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve medical decision support and enhance response accuracy using LLMs integrated with hospital-specific data and fine-tuned for efficiency.

**Method:** The study develops a system based on Llama 3.2-3B-Instruct, employing RAG to retrieve healthcare information and optimize memory and parameters through QLoRA.

**Key Contributions:**

	1. Development of a robust LLM-based medical decision support system
	2. Implementation of QLoRA for parameter and memory efficiency
	3. Discussion of ethical and practical implications for healthcare integration

**Result:** The model shows improved performance on medical benchmarks, able to provide medical suggestions and better disease predictions from patients' data.

**Limitations:** Integration challenges into current healthcare systems, need for clinical validation, and considerations for patient privacy and data security.

**Conclusion:** While promising, the integration of LLMs in healthcare raises ethical considerations, requires clinical validation, and faces practical deployment challenges, indicating a need for future research in this domain.

**Abstract:** This research paper investigates the application of Large Language Models (LLMs) in healthcare, specifically focusing on enhancing medical decision support through Retrieval-Augmented Generation (RAG) integrated with hospital-specific data and fine-tuning using Quantized Low-Rank Adaptation (QLoRA). The system utilizes Llama 3.2-3B-Instruct as its foundation model. By embedding and retrieving context-relevant healthcare information, the system significantly improves response accuracy. QLoRA facilitates notable parameter efficiency and memory optimization, preserving the integrity of medical information through specialized quantization techniques. Our research also shows that our model performs relatively well on various medical benchmarks, indicating that it can be used to make basic medical suggestions. This paper details the system's technical components, including its architecture, quantization methods, and key healthcare applications such as enhanced disease prediction from patient symptoms and medical history, treatment suggestions, and efficient summarization of complex medical reports. We touch on the ethical considerations-patient privacy, data security, and the need for rigorous clinical validation-as well as the practical challenges of integrating such systems into real-world healthcare workflows. Furthermore, the lightweight quantized weights ensure scalability and ease of deployment even in low-resource hospital environments. Finally, the paper concludes with an analysis of the broader impact of LLMs on healthcare and outlines future directions for LLMs in medical settings.

</details>


### [42] [MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks](https://arxiv.org/abs/2505.03427)

*Mouath Abu Daoud, Chaimae Abouzahir, Leen Kharouf, Walid Al-Eisawi, Nizar Habash, Farah E. Shamout*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare, Arabic Medical Domain, Benchmark Dataset, Multilingual Capabilities

**Relevance Score:** 9

**TL;DR:** This paper introduces MedArabiQ, a benchmark dataset for evaluating LLMs in the Arabic medical domain, addressing the lack of high-quality datasets.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the efficacy of LLMs in the Arabic medical domain, which has been overlooked due to the absence of suitable datasets.

**Method:** The dataset was constructed from medical exams and public datasets, covering seven Arabic medical tasks, and evaluated using five LLMs including GPT-4o and Claude 3.5-Sonnet.

**Key Contributions:**

	1. Introduction of MedArabiQ dataset for Arabic medical tasks
	2. Evaluation of bias mitigation techniques in LLMs
	3. Highlighting the need for multilingual benchmarks in healthcare applications

**Result:** Evaluation revealed the necessity for high-quality multilingual benchmarks to ensure fair deployment of LLMs in healthcare applications.

**Limitations:** 

**Conclusion:** The creation of MedArabiQ provides a foundation for future research to improve LLM capabilities in the healthcare sector.

**Abstract:** Large Language Models (LLMs) have demonstrated significant promise for various applications in healthcare. However, their efficacy in the Arabic medical domain remains unexplored due to the lack of high-quality domain-specific datasets and benchmarks. This study introduces MedArabiQ, a novel benchmark dataset consisting of seven Arabic medical tasks, covering multiple specialties and including multiple choice questions, fill-in-the-blank, and patient-doctor question answering. We first constructed the dataset using past medical exams and publicly available datasets. We then introduced different modifications to evaluate various LLM capabilities, including bias mitigation. We conducted an extensive evaluation with five state-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude 3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of new high-quality benchmarks that span different languages to ensure fair deployment and scalability of LLMs in healthcare. By establishing this benchmark and releasing the dataset, we provide a foundation for future research aimed at evaluating and enhancing the multilingual capabilities of LLMs for the equitable use of generative AI in healthcare.

</details>


### [43] [An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation](https://arxiv.org/abs/2505.03452)

*Matan Orbach, Ohad Eytan, Benjamin Sznajder, Ariel Gera, Odellia Boni, Yoav Kantor, Gal Bloch, Omri Levy, Hadas Abraham, Nitzan Barzilay, Eyal Shnarch, Michael E. Factor, Shila Ofek-Koifman, Paula Ta-Shma, Assaf Toledo*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Hyper-parameter optimization, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper benchmarks various hyper-parameter optimization (HPO) algorithms for Retrieval-Augmented Generation (RAG) across diverse datasets, revealing effective strategies for enhancing RAG performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the complexity and cost of finding optimal RAG configurations, and to address the lack of rigorous benchmarking of HPO frameworks for RAG.

**Method:** A comprehensive study was conducted using 5 different HPO algorithms across 5 datasets, including a newly collected dataset for real-world product documentation, analyzing the largest HPO search space with two optimized evaluation metrics.

**Key Contributions:**

	1. Benchmarking of 5 HPO algorithms for RAG
	2. Inclusion of a new dataset on product documentation
	3. Identification of effective optimization strategies for RAG performance enhancement.

**Result:** The study finds that RAG HPO can be performed efficiently with either greedy or iterative random search strategies, significantly boosting the performance of RAG models across all datasets analyzed.

**Limitations:** 

**Conclusion:** The findings suggest that optimizing models first is more effective than the traditional sequential optimization according to the RAG pipeline order.

**Abstract:** Finding the optimal Retrieval-Augmented Generation (RAG) configuration for a given use case can be complex and expensive. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To address this gap, we present a comprehensive study involving 5 HPO algorithms over 5 datasets from diverse domains, including a new one collected for this work on real-world product documentation. Our study explores the largest HPO search space considered to date, with two optimized evaluation metrics. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with iterative random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing models first is preferable to the prevalent practice of optimizing sequentially according to the RAG pipeline order.

</details>


### [44] [Uncertainty-Aware Large Language Models for Explainable Disease Diagnosis](https://arxiv.org/abs/2505.03467)

*Shuang Zhou, Jiashuo Wang, Zidu Xu, Song Wang, David Brauer, Lindsay Welton, Jacob Cogan, Yuen-Hei Chung, Lei Tian, Zaifu Zhan, Yu Hou, Mingquan Lin, Genevieve B. Melton, Rui Zhang*

**Main category:** cs.CL

**Keywords:** explainable AI, diagnostic uncertainty, large language model, health informatics, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces ConfiDx, an uncertainty-aware LLM for explainable disease diagnosis that recognizes and explains diagnostic uncertainties to improve reliability in automatic diagnostic systems.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical need for trustworthy diagnostic systems that can explicitly identify and explain diagnostic uncertainties arising from insufficient clinical evidence.

**Method:** ConfiDx was developed by fine-tuning open-source LLMs using diagnostic criteria and evaluated through richly annotated datasets that reflect various degrees of diagnostic ambiguity.

**Key Contributions:**

	1. Introduction of ConfiDx, an uncertainty-aware LLM for diagnostics
	2. Development of annotated datasets capturing diagnostic ambiguity
	3. Demonstration of improved reliability in automatic diagnostic explanations

**Result:** ConfiDx demonstrated superior performance in identifying diagnostic uncertainties and generating trustworthy explanations, outperforming existing systems on real-world datasets.

**Limitations:** 

**Conclusion:** This study is the first to jointly address the recognition and explanation of diagnostic uncertainty, contributing significantly to the enhancement of automatic diagnostic systems.

**Abstract:** Explainable disease diagnosis, which leverages patient information (e.g., signs and symptoms) and computational models to generate probable diagnoses and reasonings, offers clear clinical values. However, when clinical notes encompass insufficient evidence for a definite diagnosis, such as the absence of definitive symptoms, diagnostic uncertainty usually arises, increasing the risk of misdiagnosis and adverse outcomes. Although explicitly identifying and explaining diagnostic uncertainties is essential for trustworthy diagnostic systems, it remains under-explored. To fill this gap, we introduce ConfiDx, an uncertainty-aware large language model (LLM) created by fine-tuning open-source LLMs with diagnostic criteria. We formalized the task and assembled richly annotated datasets that capture varying degrees of diagnostic ambiguity. Evaluating ConfiDx on real-world datasets demonstrated that it excelled in identifying diagnostic uncertainties, achieving superior diagnostic performance, and generating trustworthy explanations for diagnoses and uncertainties. To our knowledge, this is the first study to jointly address diagnostic uncertainty recognition and explanation, substantially enhancing the reliability of automatic diagnostic systems.

</details>


### [45] [Long-Short Chain-of-Thought Mixture Supervised Fine-Tuning Eliciting Efficient Reasoning in Large Language Models](https://arxiv.org/abs/2505.03469)

*Bin Yu, Hang Yuan, Yuliang Wei, Bailing Wang, Weizhen Qi, Kai Chen*

**Main category:** cs.CL

**Keywords:** Supervised Fine-Tuning, Chain-of-Thought, Reasoning Models, Human-Computer Interaction, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper presents LS-Mixture SFT, a method to fine-tune non-reasoning models using a mix of long and short Chain-of-Thought data to improve reasoning capabilities while minimizing verbosity.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address the 'overthinking' problem in models fine-tuned with Chain-of-Thought reasoning data, which results in verbose responses.

**Method:** The proposed LS-Mixture SFT combines long CoT reasoning datasets with short versions obtained through structure-preserved rewriting to fine-tune non-reasoning models.

**Key Contributions:**

	1. Proposes a novel LS-Mixture SFT approach for fine-tuning
	2. Demonstrates improvements in accuracy and response length
	3. Addresses the overthinking problem in reasoning models

**Result:** LS-Mixture SFT improved model accuracy by 2.3% and reduced response length by 47.61% compared to traditional SFT methods.

**Limitations:** 

**Conclusion:** LS-Mixture SFT effectively enhances reasoning capabilities in non-reasoning models while mitigating verbosity, making it a viable approach for efficient fine-tuning.

**Abstract:** Recent advances in large language models have demonstrated that Supervised Fine-Tuning (SFT) with Chain-of-Thought (CoT) reasoning data distilled from large reasoning models (e.g., DeepSeek R1) can effectively transfer reasoning capabilities to non-reasoning models. However, models fine-tuned with this approach inherit the "overthinking" problem from teacher models, producing verbose and redundant reasoning chains during inference. To address this challenge, we propose \textbf{L}ong-\textbf{S}hort Chain-of-Thought \textbf{Mixture} \textbf{S}upervised \textbf{F}ine-\textbf{T}uning (\textbf{LS-Mixture SFT}), which combines long CoT reasoning dataset with their short counterparts obtained through structure-preserved rewriting. Our experiments demonstrate that models trained using the LS-Mixture SFT method, compared to those trained with direct SFT, achieved an average accuracy improvement of 2.3\% across various benchmarks while substantially reducing model response length by approximately 47.61\%. This work offers an approach to endow non-reasoning models with reasoning capabilities through supervised fine-tuning while avoiding the inherent overthinking problems inherited from teacher models, thereby enabling efficient reasoning in the fine-tuned models.

</details>


### [46] [Evaluation of LLMs on Long-tail Entity Linking in Historical Documents](https://arxiv.org/abs/2505.03473)

*Marta Boscariol, Luana Bulla, Lia Draetta, Beatrice Fiumanò, Emanuele Lenzi, Leonardo Piano*

**Main category:** cs.CL

**Keywords:** Entity Linking, Long-tail Entities, Natural Language Processing, LLMs, Knowledge Bases

**Relevance Score:** 8

**TL;DR:** This paper evaluates the performance of LLMs (GPT and LLama3) in long-tail entity linking tasks, highlighting their effectiveness compared to traditional methods.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Entity Linking is critical for NLP applications, but linking long-tail entities is particularly challenging due to their underrepresentation in models and data.

**Method:** The study assesses LLM performance in long-tail entity linking using MHERCL v0.1 benchmark, comparing results against ReLiK framework.

**Key Contributions:**

	1. Evaluation of LLMs in long-tail entity linking
	2. Introduction of MHERCL v0.1 benchmark
	3. Comparison with state-of-the-art methods like ReLiK

**Result:** LLMs showed promising results in identifying and linking long-tail entities, indicating their potential to enhance traditional EL methods.

**Limitations:** The study may have limitations in terms of dataset size and diversity of historical texts used for evaluation.

**Conclusion:** LLMs can effectively assist in long-tail entity linking, bridging the gap between head and long-tail entities in NLP tasks.

**Abstract:** Entity Linking (EL) plays a crucial role in Natural Language Processing (NLP) applications, enabling the disambiguation of entity mentions by linking them to their corresponding entries in a reference knowledge base (KB). Thanks to their deep contextual understanding capabilities, LLMs offer a new perspective to tackle EL, promising better results than traditional methods. Despite the impressive generalization capabilities of LLMs, linking less popular, long-tail entities remains challenging as these entities are often underrepresented in training data and knowledge bases. Furthermore, the long-tail EL task is an understudied problem, and limited studies address it with LLMs. In the present work, we assess the performance of two popular LLMs, GPT and LLama3, in a long-tail entity linking scenario. Using MHERCL v0.1, a manually annotated benchmark of sentences from domain-specific historical texts, we quantitatively compare the performance of LLMs in identifying and linking entities to their corresponding Wikidata entries against that of ReLiK, a state-of-the-art Entity Linking and Relation Extraction framework. Our preliminary experiments reveal that LLMs perform encouragingly well in long-tail EL, indicating that this technology can be a valuable adjunct in filling the gap between head and long-tail EL.

</details>


### [47] [Sentence Embeddings as an intermediate target in end-to-end summarisation](https://arxiv.org/abs/2505.03481)

*Maciej Zembrzuski, Saad Mahamood*

**Main category:** cs.CL

**Keywords:** document summarisation, neural networks, sentence embeddings

**Relevance Score:** 6

**TL;DR:** Proposes a new approach for document summarisation that combines extractive and abstractive techniques to improve summarisation of large input datasets, specifically for user reviews.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods struggle with large datasets in document summarisation, particularly in user reviews of accommodations.

**Method:** Combines an extractive approach with externally pre-trained sentence level embeddings alongside an abstractive summarisation model.

**Key Contributions:**

	1. Introduction of a combined extractive and abstractive summarisation model
	2. Use of externally pre-trained sentence embeddings
	3. Improvement in summarisation quality for large datasets

**Result:** The new method outperforms existing techniques in summarising large datasets and enhances quality by predicting sentence level embeddings instead of sentence selection probabilities.

**Limitations:** 

**Conclusion:** The proposed approach significantly improves the end-to-end summation quality for loosely aligned source to target corpora.

**Abstract:** Current neural network-based methods to the problem of document summarisation struggle when applied to datasets containing large inputs. In this paper we propose a new approach to the challenge of content-selection when dealing with end-to-end summarisation of user reviews of accommodations. We show that by combining an extractive approach with externally pre-trained sentence level embeddings in an addition to an abstractive summarisation model we can outperform existing methods when this is applied to the task of summarising a large input dataset. We also prove that predicting sentence level embedding of a summary increases the quality of an end-to-end system for loosely aligned source to target corpora, than compared to commonly predicting probability distributions of sentence selection.

</details>


### [48] [Faster MoE LLM Inference for Extremely Large Models](https://arxiv.org/abs/2505.03531)

*Haoqi Yang, Luohe Shi, Qiwei Li, Zuchao Li, Ping Wang, Bo Du, Mengjia Shen, Hai Zhao*

**Main category:** cs.CL

**Keywords:** Mixture of Experts, fine-grained models, efficiency, performance, large language models

**Relevance Score:** 9

**TL;DR:** This paper explores the efficiency dynamics of fine-grained Sparse Mixture of Experts (MoE) models, demonstrating how reducing activated experts can enhance efficiency with minor performance loss.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of fine-grained Mixture of Experts models in large language applications necessitates understanding their efficiency under different loads, as existing research is limited.

**Method:** Analysis of efficiency dynamics in fine-grained MoE models under varying service loads, with experiments on the impact of different numbers of activated and total experts.

**Key Contributions:**

	1. Identification of the efficiency trade-offs in fine-grained MoE models
	2. Proposal of methods leading to increased throughput
	3. Insight into performance implications of expert reduction strategies

**Result:** The study finds that reducing the number of activated experts enhances efficiency significantly (10% throughput increase), while reducing total experts leads to performance degradation.

**Limitations:** The results are primarily focused on specific scenarios and may not generalize to all applications of MoE models.

**Conclusion:** MoE inference optimization offers substantial potential for improvements, particularly in the development of fine-grained models.

**Abstract:** Sparse Mixture of Experts (MoE) large language models (LLMs) are gradually becoming the mainstream approach for ultra-large-scale models. Existing optimization efforts for MoE models have focused primarily on coarse-grained MoE architectures. With the emergence of DeepSeek Models, fine-grained MoE models are gaining popularity, yet research on them remains limited. Therefore, we want to discuss the efficiency dynamic under different service loads. Additionally, fine-grained models allow deployers to reduce the number of routed experts, both activated counts and total counts, raising the question of how this reduction affects the trade-off between MoE efficiency and performance. Our findings indicate that while deploying MoE models presents greater challenges, it also offers significant optimization opportunities. Reducing the number of activated experts can lead to substantial efficiency improvements in certain scenarios, with only minor performance degradation. Reducing the total number of experts provides limited efficiency gains but results in severe performance degradation. Our method can increase throughput by at least 10\% without any performance degradation. Overall, we conclude that MoE inference optimization remains an area with substantial potential for exploration and improvement.

</details>


### [49] [Say It Another Way: A Framework for User-Grounded Paraphrasing](https://arxiv.org/abs/2505.03563)

*Cléa Chataigner, Rebecca Ma, Prakhar Ganesh, Afaf Taïk, Elliot Creager, Golnoosh Farnadi*

**Main category:** cs.CL

**Keywords:** paraphrasing, large language models, evaluations, prompt variations, natural language

**Relevance Score:** 8

**TL;DR:** This paper presents a controlled paraphrasing framework for generating natural variations of prompts to evaluate the behavior of LLMs, demonstrating significant effects of prompt wording on model responses.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the stability and reliability of language model evaluations due to variations in prompt wording.

**Method:** A controlled paraphrasing framework based on minimal linguistic transformations to create natural prompt variations, validated using the BBQ dataset through human annotations and automated checks.

**Key Contributions:**

	1. Development of a controlled paraphrasing framework.
	2. Demonstration of the impact of prompt variations on LLM responses.
	3. Call for improved evaluation protocols considering paraphrasing.

**Result:** Analysis reveals that subtle prompt modifications can lead to significant changes in LLM behavior, particularly in stereotype evaluation tasks.

**Limitations:** 

**Conclusion:** The findings underscore the necessity for robust, paraphrase-aware protocols when evaluating LLMs.

**Abstract:** Small changes in how a prompt is worded can lead to meaningful differences in the behavior of large language models (LLMs), raising concerns about the stability and reliability of their evaluations. While prior work has explored simple formatting changes, these rarely capture the kinds of natural variation seen in real-world language use. We propose a controlled paraphrasing framework based on a taxonomy of minimal linguistic transformations to systematically generate natural prompt variations. Using the BBQ dataset, we validate our method with both human annotations and automated checks, then use it to study how LLMs respond to paraphrased prompts in stereotype evaluation tasks. Our analysis shows that even subtle prompt modifications can lead to substantial changes in model behavior. These results highlight the need for robust, paraphrase-aware evaluation protocols.

</details>


### [50] [Self-reflecting Large Language Models: A Hegelian Dialectical Approach](https://arxiv.org/abs/2501.14917)

*Sara Abdali, Can Goksen, Saeed Amizadeh, Julie E. Maybee, Kazuhito Koishida*

**Main category:** cs.CL

**Keywords:** NLP, Hegelian Dialectic, LLM, creativity, self-reflection

**Relevance Score:** 6

**TL;DR:** This paper presents a philosophical perspective on NLP, leveraging the Hegelian Dialectic for LLM self-reflection and proposing dynamic temperature strategies for generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To connect computational methods with classical philosophy and improve the creativity and reasoning of LLMs.

**Method:** Introduces Hegelian Dialectic for self-reflection in LLMs, explores dynamic and fixed-temperature strategies for idea generation, and utilizes a Multi-Agent Majority Voting (MAMV) strategy for idea validation.

**Key Contributions:**

	1. Philosophical approach using Hegelian Dialectic for LLM self-reflection.
	2. Dynamic annealing for temperature adjustment in idea generation.
	3. Implementation of Multi-Agent Majority Voting to assess idea validity.

**Result:** The proposed methods show promise in generating novel ideas and enhancing reasoning capabilities in LLMs during problem-solving.

**Limitations:** 

**Conclusion:** The integration of philosophical approaches and temperature strategies can significantly improve LLMs' performance in creative generation and reasoning tasks.

**Abstract:** Investigating NLP through a philosophical lens has recently caught researcher's eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by the \textit{Hegelian Dialectic} for LLMs' \textit{self-reflection}, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the opposing points of view. Moreover, this paper investigates the effect of LLMs' temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed-temperature strategy for generation. We assess the effectiveness of our proposed method in generating novel ideas and in improving the reasoning abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent Majority Voting (MAMV) strategy to assess the validity and novelty of the generated ideas, which proves useful in the absence of domain experts. Our experiments demonstrate promising results in generating ideas and enhancing problem-solving performance.

</details>


### [51] [Towards conversational assistants for health applications: using ChatGPT to generate conversations about heart failure](https://arxiv.org/abs/2505.03675)

*Anuja Tayal, Devika Salunke, Barbara Di Eugenio, Paula G Allen-Meares, Eulalia P Abril, Olga Garcia-Bedoya, Carolyn A Dickens, Andrew D. Boyd*

**Main category:** cs.CL

**Keywords:** ChatGPT, self-care, heart failure, health communication, African American

**Relevance Score:** 8

**TL;DR:** This paper investigates the use of ChatGPT to generate self-care strategy conversations for African-American heart failure patients, focusing on the role of prompt design in enhancing dialogue quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of specialized datasets for creating effective health communication strategies for African-American heart failure patients.

**Method:** Four prompting strategies were employed to simulate dialogues: domain-focused, AAVE, SDOH, and SDOH-informed reasoning, covering key self-care topics such as food, exercise, and fluid intake.

**Key Contributions:**

	1. Exploration of ChatGPT's capabilities in health communication
	2. Identification of effective prompting strategies
	3. Findings indicating the importance of empathy in health dialogues

**Result:** Effective prompt design significantly impacts dialogue quality, with SDOH incorporation enhancing conversations; however, ChatGPT struggles with empathy and engagement levels necessary for healthcare interactions.

**Limitations:** ChatGPT lacks the necessary empathy and engagement for meaningful healthcare conversations.

**Conclusion:** Although ChatGPT can improve dialogue in specific contexts, further refinements are needed to enhance emotional and engagement aspects of healthcare communication.

**Abstract:** We explore the potential of ChatGPT (3.5-turbo and 4) to generate conversations focused on self-care strategies for African-American heart failure patients -- a domain with limited specialized datasets. To simulate patient-health educator dialogues, we employed four prompting strategies: domain, African American Vernacular English (AAVE), Social Determinants of Health (SDOH), and SDOH-informed reasoning. Conversations were generated across key self-care domains of food, exercise, and fluid intake, with varying turn lengths (5, 10, 15) and incorporated patient-specific SDOH attributes such as age, gender, neighborhood, and socioeconomic status. Our findings show that effective prompt design is essential. While incorporating SDOH and reasoning improves dialogue quality, ChatGPT still lacks the empathy and engagement needed for meaningful healthcare communication.

</details>


### [52] [CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models](https://arxiv.org/abs/2503.10707)

*Zhiyuan Wang, Katharine E. Daniel, Laura E. Barnes, Philip I. Chow*

**Main category:** cs.CL

**Keywords:** Cancer survivors, Mobile diaries, Emotion analysis, Large Language Models, Context-Aware

**Relevance Score:** 7

**TL;DR:** This paper presents CALLM, a Context-Aware framework using LLMs and RAG to analyze mobile diary entries of cancer survivors for improved emotional state tracking and intervention opportunities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Cancer survivors encounter distinct emotional challenges that influence their quality of life, necessitating effective tracking of emotional states for timely interventions.

**Method:** The study analyzes mobile diary entries from 407 cancer survivors, establishing connections between context and emotional states, and employs CALLM to enhance emotion analysis using LLM and RAG techniques.

**Key Contributions:**

	1. Development of the CALLM framework for emotional analysis
	2. Identification of context-emotion relationships in cancer survivors
	3. Achievement of high prediction accuracies for emotional states

**Result:** CALLM achieved balanced accuracies of 72.96% for positive affect, 73.29% for negative affect, 73.72% for emotion regulation desire, and 60.09% for intervention availability, outperforming existing models.

**Limitations:** 

**Conclusion:** The framework shows that contextual data from mobile diaries can be utilized to gain insights into emotional experiences and optimize intervention timing for personalized support.

**Abstract:** Cancer survivors face unique emotional challenges that impact their quality of life. Mobile diary entries provide a promising method for tracking emotional states, improving self-awareness, and promoting well-being outcome. This paper aims to, through mobile diaries, understand cancer survivors' emotional states and key variables related to just-in-time intervention opportunities, including the desire to regulate emotions and the availability to engage in interventions. Although emotion analysis tools show potential for recognizing emotions from text, current methods lack the contextual understanding necessary to interpret brief mobile diary narratives. Our analysis of diary entries from cancer survivors (N=407) reveals systematic relationships between described contexts and emotional states, with administrative and health-related contexts associated with negative affect and regulation needs, while leisure activities promote positive emotions. We propose CALLM, a Context-Aware framework leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to analyze these brief entries by integrating retrieved peer experiences and personal diary history. CALLM demonstrates strong performance with balanced accuracies reaching 72.96% for positive affect, 73.29% for negative affect, 73.72% for emotion regulation desire, and 60.09% for intervention availability, outperforming language model baselines. Post-hoc analysis reveals that model confidence strongly predicts accuracy, with longer diary entries generally enhancing performance, and brief personalization periods yielding meaningful improvements. Our findings demonstrate how contextual information in mobile diaries can be effectively leveraged to understand emotional experiences, predict key states, and identify optimal intervention moments for personalized just-in-time support.

</details>


### [53] [IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages](https://arxiv.org/abs/2505.03688)

*Sharvi Endait, Ruturaj Ghatage, Aditya Kulkarni, Rajlaxmi Patil, Raviraj Joshi*

**Main category:** cs.CL

**Keywords:** Indic languages, question-answering, machine learning, NLP, dataset

**Relevance Score:** 7

**TL;DR:** IndicSQuAD is a new extractive QA dataset for nine major Indic languages, enhancing question-answering resources for low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of Indic languages in question-answering systems despite their large native speaker base.

**Method:** We adapted and extended translation techniques from the SQuAD dataset to create a comprehensive multi-lingual QA dataset, IndicSQuAD, while ensuring high linguistic fidelity and accurate answer-span alignment.

**Key Contributions:**

	1. Introduction of IndicSQuAD, a multi-lingual QA dataset for nine Indic languages.
	2. Utilization of translation techniques to ensure linguistic fidelity in dataset creation.
	3. Evaluation of baseline performance highlights challenges in low-resource language QA systems.

**Result:** Baseline performance evaluations using language-specific BERT models show challenges faced in low-resource settings.

**Limitations:** Challenges inherent in low-resource settings that affect QA system performance.

**Conclusion:** IndicSQuAD provides a valuable resource for developing QA systems in Indic languages and suggests future directions for dataset expansion and multimodal integration.

**Abstract:** The rapid progress in question-answering (QA) systems has predominantly benefited high-resource languages, leaving Indic languages largely underrepresented despite their vast native speaker base. In this paper, we present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset covering nine major Indic languages, systematically derived from the SQuAD dataset. Building on previous work with MahaSQuAD for Marathi, our approach adapts and extends translation techniques to maintain high linguistic fidelity and accurate answer-span alignment across diverse languages. IndicSQuAD comprises extensive training, validation, and test sets for each language, providing a robust foundation for model development. We evaluate baseline performances using language-specific monolingual BERT models and the multilingual MuRIL-BERT. The results indicate some challenges inherent in low-resource settings. Moreover, our experiments suggest potential directions for future work, including expanding to additional languages, developing domain-specific datasets, and incorporating multimodal data. The dataset and models are publicly shared at https://github.com/l3cube-pune/indic-nlp

</details>


### [54] [NBF at SemEval-2025 Task 5: Light-Burst Attention Enhanced System for Multilingual Subject Recommendation](https://arxiv.org/abs/2505.03711)

*Baharul Islam, Nasim Ahmad, Ferdous Ahmed Barbhuiya, Kuntal Dey*

**Main category:** cs.CL

**Keywords:** Cross-lingual classification, Subject retrieval, Self-attention mechanism, Bilingual data, Academic domains

**Relevance Score:** 4

**TL;DR:** This paper presents a system for cross-lingual subject classification in English and German using a dimension-as-token self-attention mechanism and bilingual data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to improve subject classification in academic domains across languages with limited resources.

**Method:** The system leverages bilingual data with negative sampling and a margin-based retrieval objective, using a self-attention mechanism with reduced internal dimensions for effective sentence embedding encoding.

**Key Contributions:**

	1. Dimension-as-token self-attention mechanism for sentence embeddings
	2. Leveraging bilingual data for effective cross-lingual classification
	3. Competitive performance metrics with minimal resource usage

**Result:** Achieved an average recall rate of 32.24% in general quantitative settings, with 43.16% and 31.53% in qualitative evaluations, demonstrating competitive performance with minimal GPU usage.

**Limitations:** Still room for improvement in capturing subject information more effectively.

**Conclusion:** The approach effectively captures relevant subject information under resource constraints, although there is potential for further improvement.

**Abstract:** We present our system submission for SemEval 2025 Task 5, which focuses on cross-lingual subject classification in the English and German academic domains. Our approach leverages bilingual data during training, employing negative sampling and a margin-based retrieval objective. We demonstrate that a dimension-as-token self-attention mechanism designed with significantly reduced internal dimensions can effectively encode sentence embeddings for subject retrieval. In quantitative evaluation, our system achieved an average recall rate of 32.24% in the general quantitative setting (all subjects), 43.16% and 31.53% of the general qualitative evaluation methods with minimal GPU usage, highlighting their competitive performance. Our results demonstrate that our approach is effective in capturing relevant subject information under resource constraints, although there is still room for improvement.

</details>


### [55] [WebGen-Bench: Evaluating LLMs on Generating Interactive and Functional Websites from Scratch](https://arxiv.org/abs/2505.03733)

*Zimu Lu, Yunqiao Yang, Houxing Ren, Haotian Hou, Han Xiao, Ke Wang, Weikang Shi, Aojun Zhou, Mingjie Zhan, Hongsheng Li*

**Main category:** cs.CL

**Keywords:** LLM, web code generation, benchmark, test cases, website development

**Relevance Score:** 8

**TL;DR:** WebGen-Bench is a benchmark for evaluating LLMs in generating code for multi-file websites, revealing challenges in accuracy through extensive testing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To measure the capability of LLM-based agents in generating complex website codebases from scratch, addressing gaps in existing benchmarks.

**Method:** Development of WebGen-Bench with diverse instructions for code generation, followed by creating test cases that assess the generated websites' functionality and automating the testing process using web-navigation agents.

**Key Contributions:**

	1. Introduction of WebGen-Bench as a new benchmark for LLM-based web code generation.
	2. Creation of 6,667 website-generation instructions for training LLMs.
	3. Evaluation of multiple LLM frameworks, highlighting varying performances.

**Result:** The best LLM-based framework achieved only 27.8% accuracy on test cases, suggesting the benchmark's difficulty, while a modified model trained on specific instructions reached 38.2% accuracy.

**Limitations:** The benchmark reveals that current LLMs have limited accuracy, indicating potential overfitting and underperformance in real-world applications.

**Conclusion:** Despite advancements, LLM-based agents struggle with website code generation accuracy, indicating significant room for improvement in AI-assisted coding tasks.

**Abstract:** LLM-based agents have demonstrated great potential in generating and managing code within complex codebases. In this paper, we introduce WebGen-Bench, a novel benchmark designed to measure an LLM-based agent's ability to create multi-file website codebases from scratch. It contains diverse instructions for website generation, created through the combined efforts of human annotators and GPT-4o. These instructions span three major categories and thirteen minor categories, encompassing nearly all important types of web applications. To assess the quality of the generated websites, we use GPT-4o to generate test cases targeting each functionality described in the instructions, and then manually filter, adjust, and organize them to ensure accuracy, resulting in 647 test cases. Each test case specifies an operation to be performed on the website and the expected result after the operation. To automate testing and improve reproducibility, we employ a powerful web-navigation agent to execute tests on the generated websites and determine whether the observed responses align with the expected results. We evaluate three high-performance code-agent frameworks, Bolt.diy, OpenHands, and Aider, using multiple proprietary and open-source LLMs as engines. The best-performing combination, Bolt.diy powered by DeepSeek-R1, achieves only 27.8\% accuracy on the test cases, highlighting the challenging nature of our benchmark. Additionally, we construct WebGen-Instruct, a training set consisting of 6,667 website-generation instructions. Training Qwen2.5-Coder-32B-Instruct on Bolt.diy trajectories generated from a subset of this training set achieves an accuracy of 38.2\%, surpassing the performance of the best proprietary model.

</details>


### [56] [VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model](https://arxiv.org/abs/2505.03739)

*Zuwei Long, Yunhang Shen, Chaoyou Fu, Heting Gao, Lijiang Li, Peixian Chen, Mengdan Zhang, Hang Shao, Jian Li, Jinlong Peng, Haoyu Cao, Ke Li, Rongrong Ji, Xing Sun*

**Main category:** cs.CL

**Keywords:** speech models, human-computer interaction, latency reduction

**Relevance Score:** 8

**TL;DR:** VITA-Audio is a large speech model that enhances real-time audio generation with minimal latency during streaming through a novel token prediction approach and progressive training techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for natural human-computer interaction has increased the focus on speech-based systems, which currently face high latency in generating the first audio token during streaming.

**Method:** VITA-Audio employs a Multiple Cross-modal Token Prediction (MCTP) module to generate multiple audio tokens in a single forward pass and introduces a four-stage progressive training strategy to optimize model performance and speed.

**Key Contributions:**

	1. Introduction of the Multiple Cross-modal Token Prediction (MCTP) module for faster audio generation.
	2. Four-stage progressive training strategy to minimize quality loss while accelerating the model.
	3. Demonstration of 3-5x inference speedup without sacrificing performance on ASR, TTS, and SQA tasks.

**Result:** The model achieves a 3-5x speedup at the 7B parameter scale and outperforms similar sized models on benchmarks for automatic speech recognition, text-to-speech, and spoken question answering tasks.

**Limitations:** 

**Conclusion:** VITA-Audio is a reproducible, open-source model that enables real-time conversational capabilities with notably reduced latency, outperforming existing models in various speech tasks.

**Abstract:** With the growing requirement for natural human-computer interaction, speech-based systems receive increasing attention as speech is one of the most common forms of daily communication. However, the existing speech models still experience high latency when generating the first audio token during streaming, which poses a significant bottleneck for deployment. To address this issue, we propose VITA-Audio, an end-to-end large speech model with fast audio-text token generation. Specifically, we introduce a lightweight Multiple Cross-modal Token Prediction (MCTP) module that efficiently generates multiple audio tokens within a single model forward pass, which not only accelerates the inference but also significantly reduces the latency for generating the first audio in streaming scenarios. In addition, a four-stage progressive training strategy is explored to achieve model acceleration with minimal loss of speech quality. To our knowledge, VITA-Audio is the first multi-modal large language model capable of generating audio output during the first forward pass, enabling real-time conversational capabilities with minimal latency. VITA-Audio is fully reproducible and is trained on open-source data only. Experimental results demonstrate that our model achieves an inference speedup of 3~5x at the 7B parameter scale, but also significantly outperforms open-source models of similar model size on multiple benchmarks for automatic speech recognition (ASR), text-to-speech (TTS), and spoken question answering (SQA) tasks.

</details>


### [57] [Incoherent Probability Judgments in Large Language Models](https://arxiv.org/abs/2401.16646)

*Jian-Qiao Zhu, Thomas L. Griffiths*

**Main category:** cs.CL

**Keywords:** Large Language Models, Probability Judgments, Bayesian Inference

**Relevance Score:** 8

**TL;DR:** The paper investigates the coherence of probability judgments by autoregressive LLMs, revealing systematic deviations from probability theory similar to human judgment errors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if LLMs are capable of coherent probability judgments, as they are with text generation.

**Method:** Probabilistic identities and repeated judgments were used to analyze the coherence of LLMs' probability assessments.

**Key Contributions:**

	1. Assessment of LLMs' probability judgment coherence
	2. Comparison of LLMs' behaviors to human probability judgment errors
	3. Introduction of a Bayesian framework to explain LLM deviations

**Result:** LLMs often produce incoherent probability judgments, showing human-like systematic deviations and an inverted-U-shaped mean-variance relationship.

**Limitations:** 

**Conclusion:** These deviations can be linked to implicit Bayesian inference, offering parallels with human probability judgments through the Bayesian Sampler model.

**Abstract:** Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.

</details>


### [58] [LLaSA: A Multimodal LLM for Human Activity Analysis Through Wearable and Smartphone Sensors](https://arxiv.org/abs/2406.14498)

*Sheikh Asif Imran, Mohammad Nur Hossain Khan, Subrata Biswas, Bashima Islam*

**Main category:** cs.CL

**Keywords:** Wearable Technology, Question Answering, Motion Data

**Relevance Score:** 8

**TL;DR:** LLaSA is a new model for open-ended question answering based on IMU data, enabling explanations of sensor-detected behaviors in real-world scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current wearable systems only classify what happened, lacking the capability to explain why it happened or its implications.

**Method:** LLaSA, a 13B model, is tuned for conversational reasoning and grounded in raw IMU data, providing context-aware answers to free-form questions.

**Key Contributions:**

	1. Introduction of LLaSA for open-ended sensor-based question answering
	2. Release of three large-scale datasets: SensorCaps, OpenSQA, Tune-OpenSQA
	3. Benchmark for evaluating sensor-language models

**Result:** LLaSA consistently produces causal, interpretable answers and outperforms commercial LLMs, demonstrating scientific accuracy and response reliability.

**Limitations:** 

**Conclusion:** LLaSA sets a new benchmark for sensor-language models with three large-scale datasets, promoting advancements in sensor-based question answering.

**Abstract:** Wearables generate rich motion data, yet current systems only classify what happened - failing to support natural questions about why it happened or what it means. We introduce LLaSA (Large Language and Sensor Assistant), a compact 13B model that enables ask-anything, open-ended question answering grounded in raw IMU data. LLaSA supports conversational, context-aware reasoning - explaining the causes of sensor-detected behaviors and answering free-form questions in real-world scenarios. It is tuned for scientific accuracy, coherence, and response reliability. To advance this new task of sensor-based QA, we release three large-scale datasets: SensorCaps, OpenSQA, and Tune-OpenSQA. Together, these resources define a new benchmark for sensor-language models. LLaSA consistently produces interpretable, causal answers and outperforms commercial LLMs across both public and real-world settings. Our code repository and datasets can be found at https://github.com/BASHLab/LLaSA.

</details>


### [59] [CFBench: A Comprehensive Constraints-Following Benchmark for LLMs](https://arxiv.org/abs/2408.01122)

*Tao Zhang, Chenglin Zhu, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Tao Zhang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, MillBench, natural language instructions, constraint following, NLP tasks

**Relevance Score:** 9

**TL;DR:** CFBench is a benchmark for evaluating Large Language Models (LLMs) on their ability to understand and follow natural language instructions through comprehensive constraints derived from real-world scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing evaluations that focus on narrow aspects of constraints, CFBench aims to provide a more holistic assessment of LLMs' performance in real-world applications by covering a wider range of constraints and scenarios.

**Method:** CFBench features 1,000 curated samples across more than 200 real-life scenarios and over 50 NLP tasks, organized into 10 main constraint categories and over 25 subcategories. It uses a multi-dimensional methodology for evaluation that prioritizes user perception of requirement fulfillment.

**Key Contributions:**

	1. Introduction of CFBench for comprehensive constraint evaluation of LLMs
	2. Creation of a systematic framework for 10 primary constraint categories and subcategories
	3. Public availability of data and code for further research and benchmark applications.

**Result:** Leading LLMs show significant limitations in following constraints accurately, highlighting areas for improvement and further research on influencing factors and enhancement strategies.

**Limitations:** The benchmark may not cover all possible real-world scenarios and constraints; further enhancements may be needed as LLM capabilities evolve.

**Conclusion:** CFBench sets a new standard for evaluating LLM performance in real-world contexts, aiming to enhance their usability in practical applications by better understanding constraints as perceived by users.

**Abstract:** The adeptness of Large Language Models (LLMs) in comprehending and following natural language instructions is critical for their deployment in sophisticated real-world applications. Existing evaluations mainly focus on fragmented constraints or narrow scenarios, but they overlook the comprehensiveness and authenticity of constraints from the user's perspective. To bridge this gap, we propose CFBench, a large-scale Comprehensive Constraints Following Benchmark for LLMs, featuring 1,000 curated samples that cover more than 200 real-life scenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from real-world instructions and constructs an innovative systematic framework for constraint types, which includes 10 primary categories and over 25 subcategories, and ensures each constraint is seamlessly integrated within the instructions. To make certain that the evaluation of LLM outputs aligns with user perceptions, we propose an advanced methodology that integrates multi-dimensional assessment criteria with requirement prioritization, covering various perspectives of constraints, instructions, and requirement fulfillment. Evaluating current leading LLMs on CFBench reveals substantial room for improvement in constraints following, and we further investigate influencing factors and enhancement strategies. The data and code are publicly available at https://github.com/PKU-Baichuan-MLSystemLab/CFBench

</details>


### [60] [LLM-3D Print: Large Language Models To Monitor and Control 3D Printing](https://arxiv.org/abs/2408.14307)

*Yayati Jadhav, Peter Pak, Amir Barati Farimani*

**Main category:** cs.CL

**Keywords:** Large Language Models, 3D Printing, Additive Manufacturing, Defect Detection, Automation

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework that utilizes Large Language Models (LLMs) for monitoring and controlling 3D printing processes, enabling automated defect detection and correction without human intervention.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing complexity of 3D printing technologies, particularly in Fused Deposition Modeling (FDM), necessitates a robust solution for detecting and correcting defects to maintain product quality and reduce reliance on expert intervention.

**Method:** The framework integrates pre-trained LLMs to evaluate print quality through image analysis of each printed layer, identifying defects and querying printer parameters to execute corrective actions.

**Key Contributions:**

	1. Introduction of an LLM-based monitoring framework for 3D printing
	2. Validation against human expert performance
	3. Demonstration of autonomy in defect correction

**Result:** The framework effectively identifies and corrects common 3D printing errors, surpassing traditional methods where human experts were involved, demonstrating improved accuracy and autonomy in quality control.

**Limitations:** Requires integration with specific 3D printer setups and may depend on the quality of the input data for image analysis.

**Conclusion:** The proposed LLM-based system can significantly enhance the efficiency and reliability of additive manufacturing processes by automating defect detection and correction.

**Abstract:** Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.

</details>


### [61] [Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution](https://arxiv.org/abs/2410.00153)

*Haiyan Zhao, Heng Zhao, Bo Shen, Ali Payani, Fan Yang, Mengnan Du*

**Main category:** cs.CL

**Keywords:** Large Language Models, Probing, Gaussian Concept Subspace, Emotion Steering, Natural Language Generation

**Relevance Score:** 9

**TL;DR:** This paper introduces the Gaussian Concept Subspace (GCS) to improve the robustness of concept representation in large language models (LLMs) for probing tasks, demonstrating its effectiveness in emotion steering applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how semantic knowledge is encoded in LLMs requires effective probing of learned concepts, but existing methods suffer from variability and lack robustness.

**Method:** The approach involves extending concept vectors into a Gaussian Concept Subspace (GCS) using linear probing classifiers, aiming to create a more stable representation of concepts.

**Key Contributions:**

	1. Introduction of Gaussian Concept Subspace (GCS) for LLMs
	2. Demonstration of robustness in concept representation
	3. Application of GCS in emotion steering tasks

**Result:** GCS shows improved faithfulness and plausibility in representations across various LLMs, and its efficacy is validated through representation intervention tasks in applications like emotion steering.

**Limitations:** 

**Conclusion:** GCS concept vectors can balance improved steering performance while maintaining fluency in natural language generation tasks, providing a more reliable tool for probing LLMs.

**Abstract:** Probing learned concepts in large language models (LLMs) is crucial for understanding how semantic knowledge is encoded internally. Training linear classifiers on probing tasks is a principle approach to denote the vector of a certain concept in the representation space. However, the single vector identified for a concept varies with both data and training, making it less robust and weakening its effectiveness in real-world applications. To address this challenge, we propose an approach to approximate the subspace representing a specific concept. Built on linear probing classifiers, we extend the concept vectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's effectiveness through measuring its faithfulness and plausibility across multiple LLMs with different sizes and architectures. Additionally, we use representation intervention tasks to showcase its efficacy in real-world applications such as emotion steering. Experimental results indicate that GCS concept vectors have the potential to balance steering performance and maintaining the fluency in natural language generation tasks.

</details>


### [62] [SAPIENT: Mastering Multi-turn Conversational Recommendation with Strategic Planning and Monte Carlo Tree Search](https://arxiv.org/abs/2410.09580)

*Hanwen Du, Bo Peng, Xia Ning*

**Main category:** cs.CL

**Keywords:** Conversational Recommender Systems, Reinforcement Learning, Monte Carlo Tree Search, Conversational Planning, Personalized Recommendations

**Relevance Score:** 7

**TL;DR:** SAPIENT is a novel Conversational Recommender System using Monte Carlo Tree Search for improved conversational planning and personalized responses.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address suboptimal conversational planning in existing Reinforcement Learning-based systems by enhancing the dialogue capabilities through structured search methods.

**Method:** Introduces a two-component system: S-agent (the conversational agent) and S-planner (the conversational planner using MCTS) to build a conversational search tree for optimal dialogue strategies.

**Key Contributions:**

	1. Development of SAPIENT framework for CRS using MCTS.
	2. Introduction of a self-training loop for iterative improvement of conversation quality.
	3. Efficiency enhancements in SAPIENT-e variant for better training performance.

**Result:** SAPIENT shows significant improvements over state-of-the-art methods in terms of conversational planning and user engagement across multiple benchmark datasets.

**Limitations:** 

**Conclusion:** The proposed framework and its variant demonstrate effective self-training and enhanced performance, making SAPIENT a robust solution for conversational recommender systems.

**Abstract:** Conversational Recommender Systems (CRS) proactively engage users in interactive dialogues to elicit user preferences and provide personalized recommendations. Existing methods train Reinforcement Learning (RL)-based agent with greedy action selection or sampling strategy, and may suffer from suboptimal conversational planning. To address this, we present a novel Monte Carlo Tree Search (MCTS)-based CRS framework SAPIENT. SAPIENT consists of a conversational agent (S-agent) and a conversational planner (S-planner). S-planner builds a conversational search tree with MCTS based on the initial actions proposed by S-agent to find conversation plans. The best conversation plans from S-planner are used to guide the training of S-agent, creating a self-training loop where S-agent can iteratively improve its capability for conversational planning. Furthermore, we propose an efficient variant SAPIENT-e for trade-off between training efficiency and performance. Extensive experiments on four benchmark datasets validate the effectiveness of our approach, showing that SAPIENT outperforms the state-of-the-art baselines.

</details>


### [63] [Personalization of Large Language Models: A Survey](https://arxiv.org/abs/2411.00027)

*Zhehao Zhang, Ryan A. Rossi, Branislav Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernoncourt, Joe Barrow, Tong Yu, Sungchul Kim, Ruiyi Zhang, Jiuxiang Gu, Tyler Derr, Hongjie Chen, Junda Wu, Xiang Chen, Zichao Wang, Subrata Mitra, Nedim Lipka, Nesreen Ahmed, Yu Wang*

**Main category:** cs.CL

**Keywords:** personalized language models, taxonomy, human-computer interaction, machine learning, health informatics

**Relevance Score:** 9

**TL;DR:** This paper introduces a taxonomy for personalized Large Language Models (LLMs) and bridges the gap between personalization in text generation and applications like recommendation systems, while addressing key challenges and open problems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To unify existing research on personalized LLMs, which has been fragmented into text generation and application-focused segments.

**Method:** The paper introduces a taxonomy for personalized LLM usage and systematically organizes the literature on personalization techniques, datasets, evaluation methods, and applications of personalized LLMs.

**Key Contributions:**

	1. Introduction of a taxonomy for personalized LLMs
	2. Formalization of personalization foundations and facets for LLMs
	3. Identification of challenges and open problems in the field.

**Result:** The authors consolidate and categorize research on personalization aspects of LLMs, providing a structured overview of techniques, challenges, and gaps in the literature.

**Limitations:** 

**Conclusion:** The proposed taxonomies and categorization aim to clarify the landscape of personalized LLMs, empowering researchers and practitioners with a comprehensive guide.

**Abstract:** Personalization of Large Language Models (LLMs) has recently become increasingly important with a wide range of applications. Despite the importance and recent progress, most existing works on personalized LLMs have focused either entirely on (a) personalized text generation or (b) leveraging LLMs for personalization-related downstream applications, such as recommendation systems. In this work, we bridge the gap between these two separate main directions for the first time by introducing a taxonomy for personalized LLM usage and summarizing the key differences and challenges. We provide a formalization of the foundations of personalized LLMs that consolidates and expands notions of personalization of LLMs, defining and discussing novel facets of personalization, usage, and desiderata of personalized LLMs. We then unify the literature across these diverse fields and usage scenarios by proposing systematic taxonomies for the granularity of personalization, personalization techniques, datasets, evaluation methods, and applications of personalized LLMs. Finally, we highlight challenges and important open problems that remain to be addressed. By unifying and surveying recent research using the proposed taxonomies, we aim to provide a clear guide to the existing literature and different facets of personalization in LLMs, empowering both researchers and practitioners.

</details>


### [64] [LSAQ: Layer-Specific Adaptive Quantization for Large Language Model Deployment](https://arxiv.org/abs/2412.18135)

*Binrui Zeng, Bin Ji, Xiaodong Liu, Jie Yu, Shasha Li, Jun Ma, Xiaopeng Li, Shangwen Wang, Xinran Hong, Yongtao Tang*

**Main category:** cs.CL

**Keywords:** Large Language Models, quantization, edge devices, adaptive systems, layer importance

**Relevance Score:** 8

**TL;DR:** The paper introduces Layer-Specific Adaptive Quantization (LSAQ), a technique that dynamically adapts the quantization of Large Language Models (LLMs) for deployment on edge devices based on layer importance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing use of Large Language Models on resource-constrained edge devices, existing quantization methods have limitations that prevent efficient deployment. There is a need for adaptive methods that can adjust to different computational resources.

**Method:** Layer-Specific Adaptive Quantization (LSAQ) evaluates the importance of layers in LLMs by analyzing token sets from the inputs and outputs of these layers and calculating their Jaccard similarity, which informs real-time adjustments to quantization strategies.

**Key Contributions:**

	1. Introduction of Layer-Specific Adaptive Quantization (LSAQ) for LLMs.
	2. Real-time adaptation of quantization based on layer importance.
	3. Demonstrated performance improvements over traditional quantization baselines.

**Result:** Experimental results show that LSAQ outperforms existing quantization methods in handling perplexity and zero-shot tasks while enabling tailored quantization schemes for diverse scenarios.

**Limitations:** 

**Conclusion:** LSAQ presents a promising approach for the dynamic deployment of LLMs on edge devices, enhancing their applicability across various computational environments.

**Abstract:** As Large Language Models (LLMs) demonstrate exceptional performance across various domains, deploying LLMs on edge devices has emerged as a new trend. Quantization techniques, which reduce the size and memory requirements of LLMs, are effective for deploying LLMs on resource-limited edge devices. However, existing one-size-fits-all quantization methods often fail to dynamically adjust the memory requirements of LLMs, limiting their applications to practical edge devices with various computation resources. To tackle this issue, we propose Layer-Specific Adaptive Quantization (LSAQ), a system for adaptive quantization and dynamic deployment of LLMs based on layer importance. Specifically, LSAQ evaluates the importance of LLMs' neural layers by constructing top-k token sets from the inputs and outputs of each layer and calculating their Jaccard similarity. Based on layer importance, our system adaptively adjusts quantization strategies in real time according to the computation resource of edge devices, which applies higher quantization precision to layers with higher importance, and vice versa. {Experimental results show that LSAQ consistently outperforms the selected quantization baselines in terms of perplexity and zero-shot tasks. Additionally, it can devise appropriate quantization schemes for different usage scenarios to facilitate the deployment of LLMs.

</details>


### [65] [Self-reflecting Large Language Models: A Hegelian Dialectical Approach](https://arxiv.org/abs/2501.14917)

*Sara Abdali, Can Goksen, Saeed Amizadeh, Julie E. Maybee, Kazuhito Koishida*

**Main category:** cs.CL

**Keywords:** NLP, Hegelian Dialectic, LLMs, creativity, self-reflection

**Relevance Score:** 6

**TL;DR:** This paper introduces a philosophical approach inspired by the Hegelian Dialectic for enabling self-reflection in LLMs to enhance creativity and reasoning abilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To connect computational methods in NLP with classical philosophy and improve LLMs' problem-solving performance.

**Method:** The paper employs a self-dialectical approach for LLMs' self-reflection, a dynamic annealing method for generation temperature, and a Multi-Agent Majority Voting strategy to evaluate generated ideas.

**Key Contributions:**

	1. Introduction of a Hegelian Dialectic inspired approach for LLM self-reflection
	2. Dynamic annealing approach for generation temperature
	3. Multi-Agent Majority Voting strategy for evaluating generated ideas

**Result:** The proposed methods show promising results in generating novel ideas and enhancing the reasoning abilities of LLMs during problem-solving.

**Limitations:** 

**Conclusion:** The philosophical approach combined with innovative techniques leads to improved creativity and reasoning in LLMs.

**Abstract:** Investigating NLP through a philosophical lens has recently caught researcher's eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by the \textit{Hegelian Dialectic} for LLMs' \textit{self-reflection}, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the opposing points of view. Moreover, this paper investigates the effect of LLMs' temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed-temperature strategy for generation. We assess the effectiveness of our proposed method in generating novel ideas and in improving the reasoning abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent Majority Voting (MAMV) strategy to assess the validity and novelty of the generated ideas, which proves useful in the absence of domain experts. Our experiments demonstrate promising results in generating ideas and enhancing problem-solving performance.

</details>


### [66] [Applications of Artificial Intelligence for Cross-language Intelligibility Assessment of Dysarthric Speech](https://arxiv.org/abs/2501.15858)

*Eunjung Yeo, Julie Liss, Visar Berisha, David Mortensen*

**Main category:** cs.CL

**Keywords:** Dysarthria, Speech Intelligibility, Artificial Intelligence, Cross-Language Assessment, Linguistics

**Relevance Score:** 5

**TL;DR:** This paper presents a framework for improving speech intelligibility assessment of dysarthric speech across languages using AI.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective dysarthria assessment tools that are applicable across various languages due to a focus on English in existing research.

**Method:** A two-tiered conceptual framework using a universal speech model for encoding dysarthric speech and a language-specific model for intelligibility assessment.

**Key Contributions:**

	1. Introduces a universal speech model for dysarthric speech.
	2. Proposes a language-specific intelligibility assessment model.
	3. Identifies barriers and suggests AI-driven solutions for cross-language assessment.

**Result:** Identified common barriers such as data scarcity and proposed AI-driven solutions to facilitate cross-language intelligibility assessment.

**Limitations:** Challenges include data scarcity, annotation complexity, and the need for comprehensive linguistic insights.

**Conclusion:** Effective assessment requires scalable models that adhere to linguistic rules, leveraging advances in AI for better outcomes.

**Abstract:** Purpose: Speech intelligibility is a critical outcome in the assessment and management of dysarthria, yet most research and clinical practices have focused on English, limiting their applicability across languages. This commentary introduces a conceptual framework--and a demonstration of how it can be implemented--leveraging artificial intelligence (AI) to advance cross-language intelligibility assessment of dysarthric speech. Method: We propose a two-tiered conceptual framework consisting of a universal speech model that encodes dysarthric speech into acoustic-phonetic representations, followed by a language-specific intelligibility assessment model that interprets these representations within the phonological or prosodic structures of the target language. We further identify barriers to cross-language intelligibility assessment of dysarthric speech, including data scarcity, annotation complexity, and limited linguistic insights into dysarthric speech, and outline potential AI-driven solutions to overcome these challenges. Conclusion: Advancing cross-language intelligibility assessment of dysarthric speech necessitates models that are both efficient and scalable, yet constrained by linguistic rules to ensure accurate and language-sensitive assessment. Recent advances in AI provide the foundational tools to support this integration, shaping future directions toward generalizable and linguistically informed assessment frameworks.

</details>


### [67] [Predicting potentially abusive clauses in Chilean terms of services with natural language processing](https://arxiv.org/abs/2502.00865)

*Christoffer Loeffler, Andrea Martínez Freile, Tomás Rey Pizarro*

**Main category:** cs.CL

**Keywords:** Machine Learning, Legal analysis, Terms of Service, Spanish-language models, Consumer rights

**Relevance Score:** 4

**TL;DR:** This study introduces a new methodology and dataset for analyzing online Terms of Service in Spanish, focusing on detecting and classifying abusive clauses under Chilean law using transformer-based models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address information asymmetry in consumer contracts exacerbated by complex online services and to provide insights beyond English-language models focusing on major jurisdictions.

**Method:** A novel annotation scheme categorizing legal clauses applied to 50 Terms of Service in Chile, with evaluation of transformer-based models considering language-specific pre-training and other factors.

**Key Contributions:**

	1. Introduction of a methodology for analyzing Spanish-language legal documents
	2. Creation of a comprehensive dataset for Terms of Service under Chilean law
	3. Evaluation of transformer-based models in a rarely considered legal domain

**Result:** The evaluation shows high macro-F1 scores ranging from 79% to 89% for detection tasks and 60% to 70% for classification tasks, indicating variability in performance across tasks and models.

**Limitations:** Focus on Spanish language and Chilean context may limit applicability to other jurisdictions or languages.

**Conclusion:** This work establishes the first Spanish-language multi-label classification dataset for legal clauses, paving the way for further research and practical applications in consumer rights in Chile and Latin America.

**Abstract:** This study addresses the growing concern of information asymmetry in consumer contracts, exacerbated by the proliferation of online services with complex Terms of Service that are rarely even read. Even though research on automatic analysis methods is conducted, the problem is aggravated by the general focus on English-language Machine Learning approaches and on major jurisdictions, such as the European Union. We introduce a new methodology and a substantial dataset addressing this gap. We propose a novel annotation scheme with four categories and a total of 20 classes, and apply it on 50 online Terms of Service used in Chile. Our evaluation of transformer-based models highlights how factors like language- and/or domain-specific pre-training, few-shot sample size, and model architecture affect the detection and classification of potentially abusive clauses. Results show a large variability in performance for the different tasks and models, with the highest macro-F1 scores for the detection task ranging from 79% to 89% and micro-F1 scores up to 96%, while macro-F1 scores for the classification task range from 60% to 70% and micro-F1 scores from 64% to 80%. Notably, this is the first Spanish-language multi-label classification dataset for legal clauses, applying Chilean law and offering a comprehensive evaluation of Spanish-language models in the legal domain. Our work lays the ground for future research in method development for rarely considered legal analysis and potentially leads to practical applications to support consumers in Chile and Latin America as a whole.

</details>


### [68] [MoM: Linear Sequence Modeling with Mixture-of-Memories](https://arxiv.org/abs/2502.13685)

*Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, Yu Cheng*

**Main category:** cs.CL

**Keywords:** linear sequence modeling, memory interference, recall-intensive tasks, Mixture-of-Memories, neuroscience-inspired design

**Relevance Score:** 6

**TL;DR:** The paper introduces Mixture-of-Memories (MoM), a linear sequence modeling architecture that enhances memory capacity and performance on recall-intensive tasks.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance in recall-intensive downstream tasks by addressing limitations of existing linear sequence modeling methods that use a single fixed-size memory state.

**Method:** MoM employs multiple independent memory states, directed by a router network, to manage input tokens and enhance memory capacity while maintaining linear complexity.

**Key Contributions:**

	1. Introduction of the Mixture-of-Memories architecture.
	2. Demonstrates enhanced performance on recall-intensive tasks compared to existing linear models.
	3. Maintains linear complexity in memory state computation for efficiency.

**Result:** MoM significantly outperforms current linear sequence models in recall-intensive tasks and achieves performance comparable to Transformer models.

**Limitations:** 

**Conclusion:** MoM retains linear-complexity advantages in training and constant-complexity in inference, showing promise for better memory efficiency in sequence modeling.

**Abstract:** Linear sequence modeling methods, such as linear attention, state space modeling, and linear RNNs, offer significant efficiency improvements by reducing the complexity of training and inference. However, these methods typically compress the entire input sequence into a single fixed-size memory state, which leads to suboptimal performance on recall-intensive downstream tasks. Drawing inspiration from neuroscience, particularly the brain's ability to maintain robust long-term memory while mitigating "memory interference", we introduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes multiple independent memory states, with a router network directing input tokens to specific memory states. This approach greatly enhances the overall memory capacity while minimizing memory interference. As a result, MoM performs exceptionally well on recall-intensive tasks, surpassing existing linear sequence modeling techniques. Despite incorporating multiple memory states, the computation of each memory state remains linear in complexity, allowing MoM to retain the linear-complexity advantage during training, while constant-complexity during inference. Our experimental results show that MoM significantly outperforms current linear sequence models on downstream language tasks, particularly recall-intensive tasks, and even achieves performance comparable to Transformer models. The code is released at https://github.com/OpenSparseLLMs/MoM and is also released as a part of https://github.com/OpenSparseLLMs/Linear-MoE.

</details>


### [69] [English Please: Evaluating Machine Translation with Large Language Models for Multilingual Bug Reports](https://arxiv.org/abs/2502.14338)

*Avinash Patil, Aryan Jadon*

**Main category:** cs.CL

**Keywords:** Machine Translation, Bug Reports, Natural Language Processing, Large Language Models, Software Development

**Relevance Score:** 6

**TL;DR:** This study evaluates machine translation performance on bug reports in global software development, comparing models like ChatGPT and AWS Translate.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve collaboration in global software development through accurate translation of bug reports.

**Method:** Comprehensive evaluation of machine translation models including DeepL, AWS Translate, and large language models, using various MT evaluation metrics and classification metrics.

**Key Contributions:**

	1. First comprehensive evaluation of MT on bug reports
	2. Insights for integrating MT into bug triaging workflows
	3. Code and dataset shared for further research

**Result:** ChatGPT excels in translation quality, while Almeida and Mistral achieve high F1-scores. AWS Translate leads in source language identification accuracy.

**Limitations:** Focuses only on specific datasets (Visual Studio Code GitHub repository) and evaluation metrics; may not generalize to all bug reporting scenarios.

**Conclusion:** No single system outperforms others in all tasks, indicating the need for task-specific evaluations and domain adaptation for translating technical content.

**Abstract:** Accurate translation of bug reports is critical for efficient collaboration in global software development. In this study, we conduct the first comprehensive evaluation of machine translation (MT) performance on bug reports, analyzing the capabilities of DeepL, AWS Translate, and large language models such as ChatGPT, Claude, Gemini, LLaMA, and Mistral using data from the Visual Studio Code GitHub repository, specifically focusing on reports labeled with the english-please tag. To assess both translation quality and source language identification accuracy, we employ a range of MT evaluation metrics-including BLEU, BERTScore, COMET, METEOR, and ROUGE-alongside classification metrics such as accuracy, precision, recall, and F1-score. Our findings reveal that while ChatGPT (gpt-4o) excels in semantic and lexical translation quality, it does not lead in source language identification. Claude and Mistral achieve the highest F1-scores (0.7182 and 0.7142, respectively), and Gemini records the best precision (0.7414). AWS Translate shows the highest accuracy (0.4717) in identifying source languages. These results highlight that no single system dominates across all tasks, reinforcing the importance of task-specific evaluations. This study underscores the need for domain adaptation when translating technical content and provides actionable insights for integrating MT into bug-triaging workflows. The code and dataset for this paper are available at GitHub-https://github.com/av9ash/English-Please

</details>


### [70] [BIG-Bench Extra Hard](https://arxiv.org/abs/2502.19187)

*Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, Orhan Firat*

**Main category:** cs.CL

**Keywords:** LLM, reasoning benchmarks, BIG-Bench Extra Hard

**Relevance Score:** 8

**TL;DR:** Introduction of a new benchmark, BIG-Bench Extra Hard (BBEH), for evaluating reasoning capabilities of LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the saturation of existing benchmarks (like BIG-Bench and BIG-Bench Hard) in evaluating LLM reasoning capabilities.

**Method:** Development of the BIG-Bench Extra Hard benchmark by replacing tasks in BIG-Bench Hard with novel, significantly more difficult tasks while probing similar reasoning capabilities.

**Key Contributions:**

	1. Introduction of BBEH as a new and challenging benchmark for LLM reasoning.
	2. Replacement of existing tasks with more difficult alternatives to better evaluate reasoning capabilities.
	3. Public release of BBEH for community use.

**Result:** The best general-purpose model achieved a harmonic average accuracy of 9.8%, and the best reasoning-specialized model achieved 44.8%, indicating significant room for improvement in LLM reasoning.

**Limitations:** 

**Conclusion:** BBEH highlights the ongoing challenges in attaining robust general reasoning in LLMs and has been made publicly available.

**Abstract:** Large language models (LLMs) are increasingly deployed in everyday applications, demanding robust general reasoning capabilities and diverse reasoning skillset. However, current LLM reasoning benchmarks predominantly focus on mathematical and coding abilities, leaving a gap in evaluating broader reasoning proficiencies. One particular exception is the BIG-Bench dataset, which has served as a crucial benchmark for evaluating the general reasoning capabilities of LLMs, thanks to its diverse set of challenging tasks that allowed for a comprehensive assessment of general reasoning across various skills within a unified framework. However, recent advances in LLMs have led to saturation on BIG-Bench, and its harder version BIG-Bench Hard (BBH). State-of-the-art models achieve near-perfect scores on many tasks in BBH, thus diminishing its utility. To address this limitation, we introduce BIG-Bench Extra Hard (BBEH), a new benchmark designed to push the boundaries of LLM reasoning evaluation. BBEH replaces each task in BBH with a novel task that probes a similar reasoning capability but exhibits significantly increased difficulty. We evaluate various models on BBEH and observe a (harmonic) average accuracy of 9.8\% for the best general-purpose model and 44.8\% for the best reasoning-specialized model, indicating substantial room for improvement and highlighting the ongoing challenge of achieving robust general reasoning in LLMs. We release BBEH publicly at: https://github.com/google-deepmind/bbeh.

</details>


### [71] [CALLM: Understanding Cancer Survivors' Emotions and Intervention Opportunities via Mobile Diaries and Context-Aware Language Models](https://arxiv.org/abs/2503.10707)

*Zhiyuan Wang, Katharine E. Daniel, Laura E. Barnes, Philip I. Chow*

**Main category:** cs.CL

**Keywords:** Cancer Survivors, Emotional Challenges, Mobile Diaries, Context-Aware Framework, Large Language Models

**Relevance Score:** 8

**TL;DR:** The paper explores the emotional challenges faced by cancer survivors and introduces CALLM, a Context-Aware framework using LLMs to analyze mobile diary entries, aiming to improve emotional well-being through better understanding and intervention opportunities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Cancer survivors often experience emotional challenges that impact their quality of life; tracking their emotional states can help improve self-awareness and well-being.

**Method:** The study analyzes mobile diary entries from 407 cancer survivors, using CALLM, a Context-Aware framework that leverages LLMs and RAG to comprehend the context of entries and predict emotional states.

**Key Contributions:**

	1. Development of CALLM for analyzing mobile diary entries
	2. Demonstration of strong accuracy in predicting emotional states
	3. Insights into the impact of context on emotional well-being

**Result:** CALLM shows strong performance in recognizing emotional states and predicting factors relating to just-in-time interventions, achieving balanced accuracies of around 72-74% for various emotional dimensions.

**Limitations:** The current model may still struggle with brief diary entries and lacks comprehensive contextual understanding for deeper emotional insights.

**Conclusion:** The research highlights the importance of contextual factors in diary entries for understanding emotions in cancer survivors and suggests that LLMs can significantly improve emotional analysis and intervention strategies.

**Abstract:** Cancer survivors face unique emotional challenges that impact their quality of life. Mobile diary entries provide a promising method for tracking emotional states, improving self-awareness, and promoting well-being outcome. This paper aims to, through mobile diaries, understand cancer survivors' emotional states and key variables related to just-in-time intervention opportunities, including the desire to regulate emotions and the availability to engage in interventions. Although emotion analysis tools show potential for recognizing emotions from text, current methods lack the contextual understanding necessary to interpret brief mobile diary narratives. Our analysis of diary entries from cancer survivors (N=407) reveals systematic relationships between described contexts and emotional states, with administrative and health-related contexts associated with negative affect and regulation needs, while leisure activities promote positive emotions. We propose CALLM, a Context-Aware framework leveraging Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) to analyze these brief entries by integrating retrieved peer experiences and personal diary history. CALLM demonstrates strong performance with balanced accuracies reaching 72.96% for positive affect, 73.29% for negative affect, 73.72% for emotion regulation desire, and 60.09% for intervention availability, outperforming language model baselines. Post-hoc analysis reveals that model confidence strongly predicts accuracy, with longer diary entries generally enhancing performance, and brief personalization periods yielding meaningful improvements. Our findings demonstrate how contextual information in mobile diaries can be effectively leveraged to understand emotional experiences, predict key states, and identify optimal intervention moments for personalized just-in-time support.

</details>


### [72] [Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models](https://arxiv.org/abs/2503.13551)

*Teng Wang, Zhangyi Jiang, Zhenqi He, Shenyang Tong, Wenhan Yang, Yanan Zheng, Zeyu Li, Zifan He, Hailei Gong*

**Main category:** cs.CL

**Keywords:** Large Language Models, reward modeling, reasoning tasks, data augmentation, human-computer interaction

**Relevance Score:** 8

**TL;DR:** The paper introduces the Hierarchical Reward Model (HRM) to improve the reliability of reward modeling in reasoning tasks, addressing limitations of current models like the Process Reward Model (PRM).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models face issues with reward hacking and high costs associated with annotating reasoning processes. This limits their effectiveness in training and evaluating reasoning capabilities.

**Method:** The Hierarchical Reward Model (HRM) evaluates reasoning steps at different granularity levels and utilizes Hierarchical Node Compression (HNC) for data augmentation to enhance training data diversity and robustness.

**Key Contributions:**

	1. Introduction of the Hierarchical Reward Model (HRM) for better evaluation of reasoning steps.
	2. Hierarchical Node Compression (HNC) to reduce training data generation costs.
	3. Demonstrated improved stability and robustness in evaluations across multiple reasoning tasks.

**Result:** Empirical evaluations indicate that HRM shows more stable and reliable evaluations compared to PRM on the PRM800K dataset, and demonstrates strong generalization across MATH500 and GSM8K datasets.

**Limitations:** The focus on multi-step reasoning might limit applicability in single-step contexts; further validation required on additional datasets.

**Conclusion:** HRM, enhanced with HNC, provides a more effective alternative for reward modeling, improving the overall assessment of multi-step reasoning tasks in LLMs.

**Abstract:** Recent studies show that Large Language Models (LLMs) achieve strong reasoning capabilities through supervised fine-tuning or reinforcement learning. However, a key approach, the Process Reward Model (PRM), suffers from reward hacking, making it unreliable in identifying the best intermediate step. In addition, the cost of annotating reasoning processes for reward modeling is high, making large-scale collection of high-quality data challenging. To address this, we propose a novel reward model approach called the Hierarchical Reward Model (HRM), which evaluates both individual and consecutive reasoning steps at both fine-grained and coarse-grained levels. HRM excels at assessing multi-step reasoning coherence, especially when flawed steps are later corrected through self-reflection. To further reduce the cost of generating training data, we introduce a lightweight and effective data augmentation strategy called Hierarchical Node Compression (HNC), which merges two consecutive reasoning steps into one within the tree structure. By applying HNC to MCTS-generated reasoning trajectories, we enhance the diversity and robustness of HRM training data while introducing controlled noise with minimal computational overhead. Empirical results on the PRM800K dataset show that HRM, together with HNC, provides more stable and reliable evaluations than PRM. Furthermore, cross-domain evaluations on the MATH500 and GSM8K datasets demonstrate HRM's strong generalization and robustness across a variety of reasoning tasks.

</details>


### [73] [CASE -- Condition-Aware Sentence Embeddings for Conditional Semantic Textual Similarity Measurement](https://arxiv.org/abs/2503.17279)

*Gaifan Zhang, Yi Zhou, Danushka Bollegala*

**Main category:** cs.CL

**Keywords:** sentence embeddings, contextual embeddings, semantic textual similarity, dimensionality reduction, large language models

**Relevance Score:** 7

**TL;DR:** This paper proposes Condition-Aware Sentence Embeddings (CASE), a method for generating sentence embeddings conditioned on context, showing improved performance over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve sentence embeddings by considering context, which is crucial for understanding meaning and improving performance on semantic similarity tasks.

**Method:** The proposed method, CASE, utilizes a Large Language Model to create embeddings conditioned on a sentence, learning a supervised nonlinear projection for dimensionality reduction.

**Key Contributions:**

	1. Introduction of Condition-Aware Sentence Embeddings (CASE)
	2. Demonstrated improvement over existing C-STS methods
	3. Proposed a novel supervised dimensionality reduction technique

**Result:** CASE significantly outperforms previous C-STS methods on standard benchmark datasets and improves performance by subtracting the condition embedding from LLM-based embeddings.

**Limitations:** 

**Conclusion:** The study establishes that context-aware embeddings enhance the accuracy of semantic similarity measures in sentences, aided by an effective dimensionality reduction approach.

**Abstract:** The meaning conveyed by a sentence often depends on the context in which it appears. Despite the progress of sentence embedding methods, it remains unclear how to best modify a sentence embedding conditioned on its context. To address this problem, we propose Condition-Aware Sentence Embeddings (CASE), an efficient and accurate method to create an embedding for a sentence under a given condition. First, CASE creates an embedding for the condition using a Large Language Model (LLM), where the sentence influences the attention scores computed for the tokens in the condition during pooling. Next, a supervised nonlinear projection is learned to reduce the dimensionality of the LLM-based text embeddings. We show that CASE significantly outperforms previously proposed Conditional Semantic Textual Similarity (C-STS) methods on an existing standard benchmark dataset. We find that subtracting the condition embedding consistently improves the C-STS performance of LLM-based text embeddings. Moreover, we propose a supervised dimensionality reduction method that not only reduces the dimensionality of LLM-based embeddings but also significantly improves their performance.

</details>


### [74] [HAIR: Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning for LLM Alignment](https://arxiv.org/abs/2503.18991)

*Ruoxi Cheng, Haoxuan Ma, Weixin Wang*

**Main category:** cs.CL

**Keywords:** large language models, alignment, reinforcement learning, safety, introspective reasoning

**Relevance Score:** 9

**TL;DR:** HAIR is a novel approach for aligning large language models (LLMs) with human values, addressing key challenges related to safety datasets, alignment tax, jailbreak vulnerabilities, and reward adaptability.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The alignment of LLMs with human values is critical, yet it is hindered by challenges such as the scarcity of balanced safety datasets and other alignment issues.

**Method:** HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning) involves creating a balanced safety Chain-of-Draft dataset using structured prompts and training category-specific reward models with Group Relative Policy Optimization to dynamically adjust to task difficulty.

**Key Contributions:**

	1. Introduction of HAIR, a novel alignment approach for LLMs
	2. Construction of a balanced safety Chain-of-Draft dataset
	3. Dynamic tuning of reward models based on task difficulty

**Result:** HAIR outperforms all baseline methods in terms of safety while maintaining high usefulness levels across four harmlessness and four usefulness benchmarks.

**Limitations:** 

**Conclusion:** The introduction of HAIR provides a promising direction for improving the alignment of LLMs with human values.

**Abstract:** The alignment of large language models (LLMs) with human values remains critical yet hindered by four key challenges: (1) scarcity of balanced safety datasets, (2) alignment tax, (3) vulnerability to jailbreak attacks due to shallow alignment, and (4) inability to dynamically adapt rewards according to task difficulty. To address these limitations, we introduce HAIR (Hardness-Aware Inverse Reinforcement Learning with Introspective Reasoning), a novel alignment approach inspired by shadow models in membership inference attacks. Our approach consists of two main components: (1) construction of a balanced safety Chain-of-Draft (CoD) dataset for seven harmful categories using structured prompts that leverage the introspective reasoning capabilities of LLMs; and (2) training of category-specific reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization to task difficulty at both the data and model levels. Comprehensive experiments across four harmlessness and four usefulness benchmarks demonstrate that HAIR achieves state-of-the-art performance, outperforming all baseline methods in safety while maintaining high levels of usefulness.

</details>


### [75] [Clean & Clear: Feasibility of Safe LLM Clinical Guidance](https://arxiv.org/abs/2503.20953)

*Julia Ive, Felix Jozsa, Nick Jackson, Paulina Bondaronek, Ciaran Scott Hill, Richard Dobson*

**Main category:** cs.CL

**Keywords:** LLM, healthcare, chatbot, clinical guidelines, information retrieval

**Relevance Score:** 9

**TL;DR:** Development and assessment of an LLM-empowered chatbot for answering clinical guideline questions, demonstrating relevance and efficiency in providing medical information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the speed and accuracy of information retrieval for clinical guidelines in healthcare using LLM-empowered chatbots.

**Method:** Utilized the open-weight Llama-3.1-8B LLM to extract information from UCLH clinical guidelines, with performance assessed by seven doctors comparing chatbot answers to a gold standard.

**Key Contributions:**

	1. Development of a chatbot specifically trained on clinical guidelines
	2. Assessment of chatbot performance in a clinical context
	3. Demonstration of high recall and response efficiency compared to humans

**Result:** The chatbot achieved ~73% relevant responses, a recall of 1.00 for guideline lines, and a satisfaction rate of approximately 78%, with an efficiency advantage over human respondents.

**Limitations:** Occasional minor lapses in precision with ~14.5% of responses containing unnecessary information.

**Conclusion:** The chatbot shows significant potential to improve access to clinical information for healthcare professionals, albeit with some minor precision issues.

**Abstract:** Background:   Clinical guidelines are central to safe evidence-based medicine in modern healthcare, providing diagnostic criteria, treatment options and monitoring advice for a wide range of illnesses. LLM-empowered chatbots have shown great promise in Healthcare Q&A tasks, offering the potential to provide quick and accurate responses to medical inquiries.   Our main objective was the development and preliminary assessment of an LLM-empowered chatbot software capable of reliably answering clinical guideline questions using University College London Hospital (UCLH) clinical guidelines.   Methods: We used the open-weight Llama-3.1-8B LLM to extract relevant information from the UCLH guidelines to answer questions. Our approach highlights the safety and reliability of referencing information over its interpretation and response generation. Seven doctors from the ward assessed the chatbot's performance by comparing its answers to the gold standard.   Results: Our chatbot demonstrates promising performance in terms of relevance, with ~73% of its responses rated as very relevant, showcasing a strong understanding of the clinical context. Importantly, our chatbot achieves a recall of 1.00 for extracted guideline lines, substantially minimising the risk of missing critical information. Approximately 78% of responses were rated satisfactory in terms of completeness. A small portion (~14.5%) contained minor unnecessary information, indicating occasional lapses in precision. The chatbot' showed high efficiency, with an average completion time of 10 seconds, compared to 30 seconds for human respondents. Evaluation of clinical reasoning showed that 72% of the chatbot's responses were without flaws. Our chatbot demonstrates significant potential to speed up and improve the process of accessing locally relevant clinical information for healthcare professionals.

</details>


### [76] [SEAL: Steerable Reasoning Calibration of Large Language Models for Free](https://arxiv.org/abs/2504.07986)

*Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, Zhangyang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought, Reasoning efficiency, AI training, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper investigates redundancy in chain-of-thought (CoT) reasoning in large language models (LLMs) and introduces SEAL, a training-free approach that improves reasoning efficiency and accuracy.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the redundancy in CoT reasoning that increases inference latency and negatively impacts model performance.

**Method:** The authors categorize internal reasoning structures into execution, reflection, and transition thoughts, and introduce a training-free method called SEAL, which calibrates the CoT process using a steering vector extracted from latent space.

**Key Contributions:**

	1. Introduction of the SEAL approach for CoT calibration
	2. Identification of thought types impacting model performance
	3. Empirical validation of SEAL with substantial efficiency improvements

**Result:** SEAL improves accuracy by up to 11% and reduces reasoning tokens by 11.8% to 50.4% across multiple LLMs and benchmarks.

**Limitations:** 

**Conclusion:** The SEAL method enhances LLM performance by effectively calibrating reasoning processes and shows strong transferability across tasks.

**Abstract:** Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (Steerable reasoning calibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11% improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our code is publicly available at https://github.com/VITA-Group/SEAL.

</details>
