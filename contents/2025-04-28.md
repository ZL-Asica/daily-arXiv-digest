# 2025-04-28

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 18]

- [cs.CL](#cs.CL) [Total: 53]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data](https://arxiv.org/abs/2504.17792)

*Hauke Sandhaus, Angel Hsing-Chi Hwang, Wendy Ju, Qian Yang*

**Main category:** cs.HC

**Keywords:** autonomous vehicles, data sharing, safety-critical data, human-computer interaction, competitive knowledge

**Relevance Score:** 6

**TL;DR:** This paper investigates barriers to data sharing among autonomous vehicle (AV) companies regarding safety-critical data, emphasizing implications for enhancing sharing practices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing importance of safety-critical data in improving autonomous vehicle safety necessitates understanding the reluctance of AV companies to share such data.

**Method:** The authors conducted interviews with twelve employees from AV companies who handle safety-critical data in their work.

**Key Contributions:** Identification of unique barriers to safety-critical data sharing in AV companies, Insights into the politics of data ownership and confidentiality, Recommendations for fostering data sharing through innovative tools and incentives

**Result:** Two major barriers to safety-critical data sharing were identified: (1) the inherent value and resource intensity of datasets creating internal politics, and (2) the perception of AV safety knowledge as proprietary rather than communal.

**Limitations:** 

**Future Work:** Future research could explore specific tools and strategies to facilitate safer data sharing among AV stakeholders.

**Conclusion:** Addressing these barriers through innovative data sharing tools, cost-offset strategies, and public-private knowledge debates could improve AV safety.

**Abstract:** Safety-critical data, such as crash and near-crash records, are crucial to improving autonomous vehicle (AV) design and development. Sharing such data across AV companies, academic researchers, regulators, and the public can help make all AVs safer. However, AV companies rarely share safety-critical data externally. This paper aims to pinpoint why AV companies are reluctant to share safety-critical data, with an eye on how these barriers can inform new approaches to promote sharing. We interviewed twelve AV company employees who actively work with such data in their day-to-day work. Findings suggest two key, previously unknown barriers to data sharing: (1) Datasets inherently embed salient knowledge that is key to improving AV safety and are resource-intensive. Therefore, data sharing, even within a company, is fraught with politics. (2) Interviewees believed AV safety knowledge is private knowledge that brings competitive edges to their companies, rather than public knowledge for social good. We discuss the implications of these findings for incentivizing and enabling safety-critical AV data sharing, specifically, implications for new approaches to (1) debating and stratifying public and private AV safety knowledge, (2) innovating data tools and data sharing pipelines that enable easier sharing of public AV safety data and knowledge; (3) offsetting costs of curating safety-critical data and incentivizing data sharing.

</details>


### [2] [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)

*Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li*

**Main category:** cs.HC

**Keywords:** Large Language Models, GUI automation, privacy, security, human-centered evaluation

**Relevance Score:** 9

**TL;DR:** This position paper discusses the privacy and security risks of LLM-powered GUI agents and advocates for a human-centered evaluation framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses growing concerns over privacy and security risks associated with LLM-powered GUI agents, highlighting a gap in existing evaluations that prioritize performance over these critical factors.

**Method:** The authors review existing evaluation metrics and identify challenges in integrating human evaluators for GUI agent assessments, advocating for a comprehensive human-centered framework.

**Key Contributions:** Identification of three key risks associated with LLM-powered GUI agents, Outline of five challenges in the current assessment process, Proposal of a human-centered evaluation framework emphasizing privacy and security

**Result:** Identifies three key risks of GUI agents compared to traditional automation and outlines five challenges in current evaluation practices.

**Limitations:** The paper primarily focuses on identifying risks and suggesting a framework without empirical evaluation or case studies to support the proposed changes.

**Future Work:** Future research should explore the implementation and effectiveness of the proposed human-centered evaluation framework in real-world scenarios.

**Conclusion:** A shift towards a human-centered evaluation framework is essential for ensuring that privacy and security considerations are integrated into the design and assessment of GUI agents.

**Abstract:** The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.

</details>


### [3] [VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics](https://arxiv.org/abs/2504.17960)

*Kazi Shahrukh Omar, Shuaijie Wang, Ridhuparan Kungumaraju, Tanvi Bhatt, Fabio Miranda*

**Main category:** cs.HC

**Keywords:** gait analysis, visual analytics, mobility, rehabilitation, open-access

**Relevance Score:** 8

**TL;DR:** VIGMA is an open-access visual analytics framework designed for gait analysis in various patient populations, addressing the complexity of gait data interpretation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Gait disorders affect various populations, including older adults and children, and effective analysis is necessary for improving mobility and developing tailored treatment plans.

**Method:** We conducted a requirements assessment with gait practitioners through surveys to identify workflow components and designed VIGMA, integrating computational notebooks and a Python library.

**Key Contributions:** Development of an open-access visual analytics framework for gait analysis, Integration with computational notebooks and a Python library, Capability to assess disease progression and compare patient groups

**Result:** VIGMA supports analytical capabilities for assessing disease progression and comparing multiple patient groups, validated through expert usage scenarios.

**Limitations:** Existing tools are limited to specific patient groups and tasks, and may require specialized technical skills for use.

**Future Work:** Future research may explore further enhancements to VIGMA for broader applications in gait analysis.

**Conclusion:** VIGMA addresses limitations of existing gait analysis tools by providing an open-access framework that meets the needs of practitioners in gait and mobility rehabilitation.

**Abstract:** Gait disorders are commonly observed in older adults, who frequently experience various issues related to walking. Additionally, researchers and clinicians extensively investigate mobility related to gait in typically and atypically developing children, athletes, and individuals with orthopedic and neurological disorders. Effective gait analysis enables the understanding of the causal mechanisms of mobility and balance control of patients, the development of tailored treatment plans to improve mobility, the reduction of fall risk, and the tracking of rehabilitation progress. However, analyzing gait data is a complex task due to the multivariate nature of the data, the large volume of information to be interpreted, and the technical skills required. Existing tools for gait analysis are often limited to specific patient groups (e.g., cerebral palsy), only handle a specific subset of tasks in the entire workflow, and are not openly accessible. To address these shortcomings, we conducted a requirements assessment with gait practitioners (e.g., researchers, clinicians) via surveys and identified key components of the workflow, including (1) data processing and (2) data analysis and visualization. Based on the findings, we designed VIGMA, an open-access visual analytics framework integrated with computational notebooks and a Python library, to meet the identified requirements. Notably, the framework supports analytical capabilities for assessing disease progression and for comparing multiple patient groups. We validated the framework through usage scenarios with experts specializing in gait and mobility rehabilitation. VIGMA is available at https://github.com/komar41/VIGMA.

</details>


### [4] [Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content](https://arxiv.org/abs/2504.17964)

*Celia Chen, Alex Leitch*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Large Language Models, AI content evaluation, Graduate students, Web interactions

**Relevance Score:** 8

**TL;DR:** Graduate students develop frameworks for evaluating AI-generated expertise in web interactions with LLMs, influenced by identity, verification capabilities, and system navigation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how graduate students assess machine-generated content in the context of large language models.

**Method:** Qualitative study using surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students.

**Key Contributions:** Identification of evaluation framework factors for AI content, Insights into student interaction with LLMs, Recommendations for platform improvements in AI content evaluation

**Result:** Students construct evaluation frameworks based on identity, verification, and system navigation, showing patterns of protecting certain domains of expertise while engaging with LLM outputs.

**Limitations:** Limited to graduate students and their specific contexts; may not generalize to all user demographics.

**Future Work:** Exploration of diverse user groups and development of platform features to support evaluation frameworks.

**Conclusion:** The research sheds light on human-genAI interaction patterns and suggests ways to improve support for users evaluating AI-generated content.

**Abstract:** This paper examines how graduate students develop frameworks for evaluating machine-generated expertise in web-based interactions with large language models (LLMs). Through a qualitative study combining surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students, we identify patterns in how these emerging professionals assess and engage with AI-generated content. Our findings reveal that students construct evaluation frameworks shaped by three main factors: professional identity, verification capabilities, and system navigation experience. Rather than uniformly accepting or rejecting LLM outputs, students protect domains central to their professional identities while delegating others--with managers preserving conceptual work, designers safeguarding creative processes, and programmers maintaining control over core technical expertise. These evaluation frameworks are further influenced by students' ability to verify different types of content and their experience navigating complex systems. This research contributes to web science by highlighting emerging human-genAI interaction patterns and suggesting how platforms might better support users in developing effective frameworks for evaluating machine-generated expertise signals in AI-mediated web environments.

</details>


### [5] [Chatperone: An LLM-Based Negotiable Scaffolding System for Mediating Adolescent Mobile Interactions](https://arxiv.org/abs/2504.17997)

*Suwon Yoon, Seungwon Yang, Jeongwon Choi, Wonjeong Park, Inseok Hwang*

**Main category:** cs.HC

**Keywords:** adolescents, digital content, adaptive scaffolding, mobile interactions, moderation

**Relevance Score:** 4

**TL;DR:** Chatperone is a proposed system that supports adolescents' healthy mobile interactions through adaptive scaffolding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the negative impact of uncontrolled digital content exposure on adolescents and the limitations of traditional regulatory methods.

**Method:** The paper outlines a system with three key modules: Perception, Negotiation, and Moderation, designed to provide adaptive support for adolescents.

**Key Contributions:** Introduction of the Chatperone system for adolescent digital interactions, Development of three modules (Perception, Negotiation, Moderation) for adaptive scaffolding, Discussion of real-world implementation considerations

**Result:** Chatperone's modules aim to foster healthier mobile interactions by adapting to adolescents' decision-making.

**Limitations:** 

**Future Work:** 

**Conclusion:** The proposed system is expected to enhance the way adolescents engage with digital content in a supportive manner.

**Abstract:** Adolescents' uncontrolled exposure to digital content can negatively impact their development. Traditional regulatory methods, such as time limits or app restrictions, often take a rigid approach, ignoring adolescents' decision-making abilities. Another issue is the lack of content and services tailored for adolescents. To address this, we propose Chatperone, a concept of a system that provides adaptive scaffolding to support adolescents. Chatperone fosters healthy mobile interactions through three key modules: Perception, Negotiation, and Moderation. This paper outlines these modules' functionalities and discusses considerations for real-world implementation.

</details>


### [6] [Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving](https://arxiv.org/abs/2504.17999)

*Chang Xiao, Brenda Yang*

**Main category:** cs.HC

**Keywords:** Generative conversational interfaces, Large language models, Cognitive load, Adaptive streaming, Computational efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents an adaptive streaming method for conversational interfaces powered by LLMs that optimizes output pacing based on users' cognitive load, resulting in substantial computational savings and improved user experience.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inefficiency in computational resource use when streaming outputs from LLMs faster than users can read, particularly during peak usage in cloud-based services.

**Method:** An adaptive streaming method is proposed that dynamically adjusts the speed of LLM output streaming based on inferred cognitive load, slowing down during complex segments to enhance efficiency.

**Key Contributions:** Development of an adaptive streaming method for LLMs based on cognitive load., Quantitative analysis demonstrating up to 16.8% reduction in computational resources., User studies indicating improved service efficiency without sacrificing user satisfaction.

**Result:** The method significantly reduces computational consumption by up to 16.8%, as demonstrated through statistical analysis and user studies that examine the balance between efficiency and user satisfaction.

**Limitations:** The method relies on accurate inference of cognitive load, which may vary between users and contexts.

**Future Work:** Future research may involve refining cognitive load estimations and exploring integration with various conversational AI systems.

**Conclusion:** The proposed strategy offers a practical framework for improving system efficiency in cloud-based conversational AI interfaces while maintaining a positive user experience.

**Abstract:** Generative conversational interfaces powered by large language models (LLMs) typically stream output token-by-token at a rate determined by computational budget, often neglecting actual human reading speeds and the cognitive load associated with the content. This mismatch frequently leads to inefficient use of computational resources. For example, in cloud-based services, streaming content faster than users can read appears unnecessary, resulting in wasted computational resources and potential delays for other users, particularly during peak usage periods. To address this issue, we propose an adaptive streaming method that dynamically adjusts the pacing of LLM streaming output in real-time based on inferred cognitive load. Our approach estimates the cognitive load associated with streaming content and strategically slows down the stream during complex or information-rich segments, thereby freeing computational resources for other users. Our statistical analysis of computational savings, combined with crowdsourced user studies, provides insights into the trade-offs between service efficiency and user satisfaction, demonstrating that our method can significantly reduce computational consumption up to 16.8\%. This context-aware computational resource management strategy presents a practical framework for enhancing system efficiency in cloud-based conversational AI interfaces without compromising user experience.

</details>


### [7] [ClassComet: Exploring and Designing AI-generated Danmaku in Educational Videos to Enhance Online Learning](https://arxiv.org/abs/2504.18189)

*Zipeng Ji, Pengcheng An, Jian Zhao*

**Main category:** cs.HC

**Keywords:** danmaku, educational videos, large multimodal models, user engagement, learning outcomes

**Relevance Score:** 8

**TL;DR:** This paper explores the use of large multimodal models to automatically generate high-quality danmaku comments for educational videos, enhancing viewer engagement and learning outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity and unpredictable quality of user-generated danmaku in educational videos limit their potential in promoting effective online learning.

**Method:** A formative study was conducted to identify the desirable characteristics of danmaku, leading to the development of ClassComet, a platform utilizing LMM techniques to generate relevant danmaku.

**Key Contributions:** Development of ClassComet platform for generating danmaku, Insights on desirable characteristics of educational danmaku, Demonstration of improved engagement and learning outcomes through generated content

**Result:** User studies demonstrated that the quality of the LMM-generated danmaku was comparable to human-created comments, significantly improving viewer engagement and learning outcomes in videos featuring both content- and emotion-related danmaku.

**Limitations:** 

**Future Work:** Further exploration of scaling danmaku generation across various platforms and different educational contexts.

**Conclusion:** The integration of LMM-driven danmaku generation can enhance the educational impact of video-based learning environments.

**Abstract:** Danmaku, users' live comments synchronized with, and overlaying on videos, has recently shown potential in promoting online video-based learning. However, user-generated danmaku can be scarce-especially in newer or less viewed videos and its quality is unpredictable, limiting its educational impact. This paper explores how large multimodal models (LMM) can be leveraged to automatically generate effective, high-quality danmaku. We first conducted a formative study to identify the desirable characteristics of content- and emotion-related danmaku in educational videos. Based on the obtained insights, we developed ClassComet, an educational video platform with novel LMM-driven techniques for generating relevant types of danmaku to enhance video-based learning. Through user studies, we examined the quality of generated danmaku and their influence on learning experiences. The results indicate that our generated danmaku is comparable to human-created ones, and videos with both content- and emotion-related danmaku showed significant improvement in viewers' engagement and learning outcome.

</details>


### [8] [SecCityVR: Visualization and Collaborative Exploration of Software Vulnerabilities in Virtual Reality](https://arxiv.org/abs/2504.18238)

*Dennis WÃ¼ppelman, Enes Yigitbas*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Software Security, Vulnerability Visualization, Collaborative Software Engineering, Immersive Technologies

**Relevance Score:** 6

**TL;DR:** Introduction of SecCityVR, a VR environment for visualizing software security vulnerabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of traditional 2D/3D visualizations in displaying software security vulnerabilities and enable real-time, collaborative exploration.

**Method:** Development of SecCityVR using the code city metaphor to visualize vulnerabilities as colored building floors in a virtual city, supporting multi-user collaboration and providing context through call graphs.

**Key Contributions:** Introduction of VR-based visualization for software security vulnerabilities, Support for multi-user collaboration in a virtual environment, Contextualization of vulnerabilities within related software components using call graphs.

**Result:** User study showed that SecCityVR offers better usability and lower frustration compared to traditional dashboards, despite longer task completion times.

**Limitations:** Focus on a proof-of-concept implementation; effectiveness in larger or more complex codebases is yet to be validated.

**Future Work:** Investigate scalability of the VR approach and explore further applications in software engineering.

**Conclusion:** SecCityVR enhances collaborative software security analysis by providing immersive visualization of vulnerabilities.

**Abstract:** Security vulnerabilities in software systems represent significant risks as potential entry points for malicious attacks. Traditional dashboards that display the results of static analysis security testing often use 2D or 3D visualizations, which tend to lack the spatial details required to effectively reveal issues such as the propagation of vulnerabilities across the codebase or the appearance of concurrent vulnerabilities. Additionally, most reporting solutions only treat the analysis results as an artifact that can be reviewed or edited asynchronously by developers, limiting real-time, collaborative exploration. To the best of our knowledge, no VR-based approach exists for the visualization and interactive exploration of software security vulnerabilities. Addressing these challenges, the virtual reality (VR) environment SecCityVR was developed as a proof-of-concept implementation that employs the code city metaphor within VR to visualize software security vulnerabilities as colored building floors inside the surrounding virtual city. By integrating the application's call graph, vulnerabilities are contextualized within related software components. SecCityVR supports multi-user collaboration and interactive exploration. It provides explanations and mitigations for detected issues. A user study comparing SecCityVR with the traditional dashboard find-sec-bugs showed the VR approach provided a favorable experience, with higher usability, lower temporal demand, and significantly lower frustration despite having longer task completion times. This paper and its results contribute to the fields of collaborative and secure software engineering, as well as software visualization. It provides a new application of VR code cities to visualize security vulnerabilities, as well as a novel environment for security audits using collaborative and immersive technologies.

</details>


### [9] [Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection on Parental Verbal Abuse](https://arxiv.org/abs/2504.18410)

*Jiaying Fu, Jialin Gu, Tianyue Gong, Tiange Zhou*

**Main category:** cs.HC

**Keywords:** virtual reality, LLMs, parental verbal abuse, emotional support, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper presents a VR experience powered by LLMs to encourage reflection on parental verbal abuse, showing varied emotional impacts based on personal histories.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To provide immersive self-reflection opportunities for individuals affected by parental verbal abuse, addressing gaps in current therapeutic approaches.

**Method:** A dual-phase VR experience where participants first role-play as a verbally abusive parent and then observe an LLM reframing abusive dialogue positively.

**Key Contributions:** Development of an innovative VR experience using LLMs for emotional reflection, Highlighting the need for personalization in AI emotional support, Empirical insights from a qualitative study on participant experiences.

**Result:** Qualitative analysis of 12 participants indicated that the experience fosters emotional reflection and supportive feelings, although effectiveness varies among participants based on personal experiences.

**Limitations:** The study's small sample size and the variability of outcomes based on individual histories limit generalizability.

**Future Work:** Suggests further exploration of personalizing AI-driven emotional support systems and expanding participant diversity in future studies.

**Conclusion:** The study suggests that LLMs can be effectively utilized in immersive environments to promote emotional reflection, highlighting the importance of personalization in AI-driven support systems.

**Abstract:** Parental verbal abuse leaves lasting emotional impacts, yet current therapeutic approaches often lack immersive self-reflection opportunities. To address this, we developed a VR experience powered by LLMs to foster reflection on parental verbal abuse. Participants with relevant experiences engage in a dual-phase VR experience: first assuming the role of a verbally abusive parent, interacting with an LLM portraying a child, then observing the LLM reframing abusive dialogue into warm, supportive expressions as a nurturing parent. A qualitative study with 12 participants showed that the experience encourages reflection on their past experiences and fosters supportive emotions. However, these effects vary with participants' personal histories, emphasizing the need for greater personalization in AI-driven emotional support. This study explores the use of LLMs in immersive environment to promote emotional reflection, offering insights into the design of AI-driven emotional support systems.

</details>


### [10] [Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review](https://arxiv.org/abs/2504.18496)

*Raymond Fok, Joseph Chee Chang, Marissa Radensky, Pao Siangliulue, Jonathan Bragg, Amy X. Zhang, Daniel S. Weld*

**Main category:** cs.HC

**Keywords:** Literature Review, Natural Language Processing, Interactive Systems

**Relevance Score:** 8

**TL;DR:** DimInd is an interactive system designed to assist researchers in conducting literature reviews by utilizing LLM-generated structured representations, reducing cognitive load and effort in synthesizing large collections of research papers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Comprehensive literature review involves synthesizing vast research, which is a labor-intensive process that has seen little support for deep analysis of large collections.

**Method:** DimInd employs multiple levels of information compression, providing faceted literature comparison tables, taxonomies, and narrative syntheses while ensuring provenance of source texts.

**Key Contributions:** Introduction of DimInd to scaffold literature reviews through LLM-generated representations, Multiple levels of information transformation for better understanding and organization of research papers, Evaluation showing improved efficiency in information extraction compared to traditional methods

**Result:** In an evaluation with 23 researchers, DimInd enabled users to extract information and organize papers more efficiently compared to a ChatGPT-assisted workflow.

**Limitations:** Evaluation limited to 23 researchers; broader applicability and effectiveness across diverse research domains need further exploration.

**Future Work:** Explore additional functionalities and enhancements for DimInd based on user feedback and extend evaluation to larger and more varied groups of researchers.

**Conclusion:** DimInd demonstrates the potential of using interactive LLM-generated structures to enhance the literature review process.

**Abstract:** Comprehensive literature review requires synthesizing vast amounts of research -- a labor intensive and cognitively demanding process. Most prior work focuses either on helping researchers deeply understand a few papers (e.g., for triaging or reading), or retrieving from and visualizing a vast corpus. Deep analysis and synthesis of large paper collections (e.g., to produce a survey paper) is largely conducted manually with little support. We present DimInd, an interactive system that scaffolds literature review across large paper collections through LLM-generated structured representations. DimInd scaffolds literature understanding with multiple levels of compression, from papers, to faceted literature comparison tables with information extracted from individual papers, to taxonomies of concepts, to narrative syntheses. Users are guided through these successive information transformations while maintaining provenance to source text. In an evaluation with 23 researchers, DimInd supported participants in extracting information and conceptually organizing papers with less effort compared to a ChatGPT-assisted baseline workflow.

</details>


### [11] [Papers-to-Posts: Supporting Detailed Long-Document Summarization with an Interactive LLM-Powered Source Outline](https://arxiv.org/abs/2406.10370)

*Marissa Radensky, Daniel S. Weld, Joseph Chee Chang, Pao Siangliulue, Jonathan Bragg*

**Main category:** cs.HC

**Keywords:** long-form summarization, human-computer interaction, large language models

**Relevance Score:** 9

**TL;DR:** The paper introduces Papers-to-Posts, an LLM-powered system that enhances the process of summarizing long, technical documents into shorter articles using interactive reverse source outlines for better content control.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for effective communication of long technical documents to varied audiences, which traditional LLM interfaces do not sufficiently address, especially in detail-oriented fields.

**Method:** The proposed method involves interactive reverse source outlines that allow users to adjust outline bullet points iteratively for better summarization of content.

**Key Contributions:** Introduction of interactive reverse source outlines for summarization, Implementation of Papers-to-Posts for authoring blog posts from research papers, Demonstrated improved user satisfaction and content coverage in summarization

**Result:** Papers-to-Posts significantly improves writer satisfaction with blog post quality and increases editing power compared to a baseline tool.

**Limitations:** Limited to the effectiveness of LLMs and the specific domain studied (blog posts from research papers).

**Future Work:** Explore further enhancements to the interactive features and test additional user groups in various domains.

**Conclusion:** The system makes it easier for users to incorporate critical insights from research papers into their blog posts, providing a more controlled summarization experience.

**Abstract:** Compressing long and technical documents (e.g., >10 pages) into shorter-form articles (e.g., <2 pages) is critical for communicating information to different audiences, for example, blog posts of scientific research paper or legal briefs of dense court proceedings. While large language models (LLMs) are powerful tools for condensing large amounts of text, current interfaces to these models lack support for understanding and controlling what content is included in a detailed summarizing article. Such capability is especially important for detail- and technical-oriented domains, in which tactical selection and coherent synthesis of key details is critical for effective communication to the target audience. For this, we present interactive reverse source outlines, a novel mechanism for controllable long-form summarization featuring outline bullet points with automatic point selections that the user can iteratively adjust to obtain an article with the desired content coverage. We implement this mechanism in Papers-to-Posts, a new LLM-powered system for authoring research-paper blog posts. Through a within-subjects lab study (n=20) and a between-subjects deployment study (n=37 blog posts, 26 participants), we compare Papers-to-Posts to a strong baseline tool that provides an LLM-generated draft and access to free-form prompting. Under time constraints, Papers-to-Posts significantly increases writer satisfaction with blog post quality, particularly with respect to content coverage. Furthermore, quantitative results showed an increase in editing power (change in text for an amount of time or writing actions) while using Papers-to-Posts, and qualitative results showed that participants found incorporating key research-paper insights in their blog posts easier while using Papers-to-Posts.

</details>


### [12] [Misty: UI Prototyping Through Interactive Conceptual Blending](https://arxiv.org/abs/2409.13900)

*Yuwen Lu, Alan Leung, Amanda Swearngin, Jeffrey Nichols, Titus Barik*

**Main category:** cs.HC

**Keywords:** UI Prototyping, Conceptual Blending, HCI, Frontend Development, Design Tools

**Relevance Score:** 8

**TL;DR:** Misty is a novel UI workflow that aids developers in rapidly incorporating elements from design examples into UI prototyping through conceptual blending.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current UI prototyping tools lack support for blending design examples, which is essential for creative iterations.

**Method:** An exploratory first-use study with 14 frontend developers was conducted to assess the effectiveness of Misty.

**Key Contributions:** Introduces a novel workflow for UI prototyping that uses conceptual blending., Demonstrates the effectiveness of Misty through user feedback and studies., Encourages creative exploration and flexibility in design intent.

**Result:** Misty facilitates creative exploration, enabling flexible intent specification during prototyping and inspiring developers with new UI blends.

**Limitations:** 

**Future Work:** Explore the integration of Misty with existing prototyping tools and further investigate its impact on various design processes.

**Conclusion:** Misty shows promise in improving the prototyping process by bridging the gap between developers and designers.

**Abstract:** UI prototyping often involves iterating and blending elements from examples such as screenshots and sketches, but current tools offer limited support for incorporating these examples. Inspired by the cognitive process of conceptual blending, we introduce a novel UI workflow that allows developers to rapidly incorporate diverse aspects from design examples into work-in-progress UIs. We prototyped this workflow as Misty. Through an exploratory first-use study with 14 frontend developers, we assessed Misty's effectiveness and gathered feedback on this workflow. Our findings suggest that Misty's conceptual blending workflow helps developers kickstart creative explorations, flexibly specify intent in different stages of prototyping, and inspires developers through serendipitous UI blends. Misty demonstrates the potential for tools that blur the boundaries between developers and designers.

</details>


### [13] [Co-Designing with Algorithms: Unpacking the Complex Role of GenAI in Interactive System Design Education](https://arxiv.org/abs/2410.14048)

*Hauke Sandhaus, Quiquan Gu, Maria Teresa Parreira, Wendy Ju*

**Main category:** cs.HC

**Keywords:** Generative AI, Human-Computer Interaction, HCI education, device design, creativity

**Relevance Score:** 9

**TL;DR:** This study investigates the integration of Generative AI (GenAI) into Human-Computer Interaction (HCI) education, focusing on graduate students' use of GenAI tools in device design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of Generative AI on Human-Computer Interaction education and technology design.

**Method:** Qualitative analysis through 12 post-class group interviews with graduate students from an applied HCI course.

**Key Contributions:** Insights into the benefits and risks of using GenAI in HCI education., A taxonomy of student usage patterns regarding GenAI in design processes., Recommendations for integrating GenAI into HCI curricula.

**Result:** The study found that all groups incorporated GenAI into their workflows, highlighting benefits like increased creativity and faster iterations, but also risks such as shallow learning.

**Limitations:** The study is based on a specific course and may not generalize to all educational contexts.

**Future Work:** Further research is needed to explore how HCI education can adapt nationwide as GenAI continues to evolve.

**Conclusion:** HCI education must evolve to account for GenAI's influence, with recommendations for curricula to better prepare future designers.

**Abstract:** Generative Artificial Intelligence (GenAI) is transforming Human-Computer Interaction (HCI) education and technology design, yet its impact remains poorly understood. This study explores how graduate students in an applied HCI course used GenAI tools during interactive device design. Despite no encouragement, all groups integrated GenAI into their workflows. Through 12 post-class group interviews, we identified how GenAI co-design behaviors present both benefits, such as enhanced creativity and faster design iterations, and risks, including shallow learning and reflection. Benefits were most evident during the execution phases, while the discovery and reflection phases showed limited gains. A taxonomy of usage patterns revealed that students' outcomes depended more on how they used GenAI than the specific tasks performed. These findings highlight the need for HCI education to adapt to GenAI's role and offer recommendations for curricula to better prepare future designers for effective creative co-design.

</details>


### [14] [The GenUI Study: Exploring the Design of Generative UI Tools to Support UX Practitioners and Beyond](https://arxiv.org/abs/2501.13145)

*Xiang 'Anthony' Chen, Tiffany Knearem, Yang Li*

**Main category:** cs.HC

**Keywords:** Generative UI, UX design, human-computer interaction, user experience, professional workflow

**Relevance Score:** 8

**TL;DR:** The paper explores how UX practitioners adopt Generative UI (GenUI) tools by conducting a study with 37 professionals and reporting their experiences and needs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the integration of Generative UI tools in UX work is essential for improving their design and effectiveness.

**Method:** A formative study was conducted involving 37 UX professionals who engaged in a week-long individual project using a GenUI tool, documenting their experiences through journals and interviews.

**Key Contributions:** Insights into the workflow of UX practitioners using GenUI tools, Identification of role-specific needs and gaps in GenUI functionalities, Design implications to enhance future GenUI tools

**Result:** The study provides insights into participant workflows, role-specific support from GenUI, and identifies gaps between GenUI capabilities and user expectations.

**Limitations:** The study is limited by its sample size and the specific GenUI tool used, which may not generalize across all contexts.

**Future Work:** Future research should explore diverse GenUI tools, involve more participants, and address identified gaps to enhance adoption.

**Conclusion:** The findings highlight crucial design implications for future GenUI development to better meet the needs of UX practitioners.

**Abstract:** AI can now generate high-fidelity UI mock-up screens from a high-level textual description, promising to support UX practitioners' work. However, it remains unclear how UX practitioners would adopt such Generative UI (GenUI) models in a way that is integral and beneficial to their work. To answer this question, we conducted a formative study with 37 UX-related professionals that consisted of four roles: UX designers, UX researchers, software engineers, and product managers. Using a state-of-the-art GenUI tool, each participant went through a week-long, individual mini-project exercise with role-specific tasks, keeping a daily journal of their usage and experiences with GenUI, followed by a semi-structured interview. We report findings on participants' workflow using the GenUI tool, how GenUI can support all and each specific roles, and existing gaps between GenUI and users' needs and expectations, which lead to design implications to inform future work on GenUI development.

</details>


### [15] [Control Center Framework for Teleoperation Support of Automated Vehicles on Public Roads](https://arxiv.org/abs/2503.24249)

*Maria-Magdalena Wolf, Niklas Krauss, Arwed Schmidt, Frank Diermeyer*

**Main category:** cs.HC

**Keywords:** teleoperation, automated vehicles, remote operator, state diagram, control center framework

**Relevance Score:** 6

**TL;DR:** This paper presents a control center framework for remote operation of automated vehicle fleets, detailing the tasks required for effective teleoperation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of implementing a teleoperation system for automated vehicles and to provide a structured overview of necessary functions.

**Method:** The authors collected tasks from literature and categorized them into roles of Remote Operator and Fleet Manager, focusing on driving-related tasks for the Remote Operator. A state diagram was developed to illustrate the sequence of tasks and vehicle states.

**Key Contributions:** Development of a control center framework for remote operation of automated vehicle fleets, Creation of a state diagram for remote operator tasks and vehicle states, Provision of a structure that can be aligned with legislation for teleoperations.

**Result:** The state diagram effectively maps out the actions available to the remote operator to manage automated vehicle disengagements and can be adapted to align with legislation.

**Limitations:** 

**Future Work:** Further validation of the framework on public roads and potential adaptations based on real-world scenarios.

**Conclusion:** The developed framework and state diagram lay the groundwork for implementing remote support systems for automated vehicles, aiming for validation on public roads.

**Abstract:** Implementing a teleoperation system with its various actors and interactions is challenging and requires an overview of the necessary functions. This work collects all tasks that arise in a control center for an automated vehicle fleet from literature and assigns them to the two roles Remote Operator and Fleet Manager. Focusing on the driving-related tasks of the remote operator, a process is derived that contains the sequence of tasks, associated vehicle states, and transitions between the states. The resulting state diagram shows all remote operator actions available to effectively resolve automated vehicle disengagements. Thus, the state diagram can be applied to existing legislation or modified based on prohibitions of specific interactions. The developed control center framework and included state diagram should serve as a basis for implementing and testing remote support for automated vehicles to be validated on public roads.

</details>


### [16] [Learning by gaming, coding and making with EDUMING: A new approach to utilising atypical digital games for learning](https://arxiv.org/abs/2504.13878)

*Stefan Pietrusky*

**Main category:** cs.HC

**Keywords:** EDUMING, digital learning games, usability, acceptance, constructivism

**Relevance Score:** 5

**TL;DR:** This paper explores the EDUMING concept, which facilitates the development and active user modification of digital learning games, focusing on an empirical study of the game 'Professor Chip's Learning Quest' regarding its usability and acceptance in educational settings.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how digital learning games can be effectively integrated into educational contexts by allowing users to modify and engage with them, following the principles of Papert's constructionism.

**Method:** An empirical study was conducted to evaluate the usability, acceptance, and motivation associated with using the digital learning game 'Professor Chip's Learning Quest', developed according to the EDUMING concept.

**Key Contributions:** Introduction of the EDUMING concept for active game modification, Empirical evaluation of 'Professor Chip's Learning Quest' game, Insights into user acceptance and motivation for digital learning games

**Result:** The study recorded the usability of 'Professor Chip's Learning Quest' as well as users' previous experiences with digital learning games, highlighting acceptance and motivation metrics.

**Limitations:** The study is explorative and may require further research for generalizable results across varied educational settings.

**Future Work:** Future research could expand on different educational contexts and include longitudinal studies to assess the long-term effectiveness of games developed under the EDUMING concept.

**Conclusion:** The exploratory study provides initial empirical evidence for the PCLQ game as a viable educational tool that aligns with constructivist learning principles, suggesting potential for further integration into school curricula.

**Abstract:** Papert's constructionism makes it clear that learning is particularly effective when learners create tangible artifacts and share and discuss them in social contexts. Technological progress in recent decades has created numerous opportunities for learners to not only passively consume media, but to actively shape it through construction. This article uses the EDUMING concept to present a new method to simplify the development of digital learning games and thus support their integration into learning situations. A key difference between the concept and established ideas such as game-based learning, gamification, serious games, etc. is that games are not closed and are consumed passively, but can also be actively developed by users individually by modifying the source code with the help of an IDE. As part of an empirical study, the usability of the game "Professor Chip's Learning Quest" (PCLQ) is recorded, as well as previous experience with digital learning games and the acceptance and motivation to use new technologies. The purpose of this article is to test the PCLQ digital learning game, developed according to the EDUMING concept, as part of an exploratory study regarding its usability, acceptance and suitability for use in schools. The study is intended as a first empirical approach to practical testing of the concept.

</details>


### [17] [Using customized GPT to develop prompting proficiency in architectural AI-generated images](https://arxiv.org/abs/2504.13948)

*Juan David Salazar Rodriguez, Sam Conrad Joyce, Julfendi*

**Main category:** cs.HC

**Keywords:** generative AI, prompt engineering, architectural education, GPT models, mixed-methods research

**Relevance Score:** 6

**TL;DR:** This research explores how customized GPT models can improve prompting skills in architecture students for generating AI-driven images.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of generative AI tools necessitates effective prompt engineering in architectural education.

**Method:** A mixed-methods experimental design with three student groups: one with no support, one with structured prompts, and one with AI personas and structured prompts. Students engaged in reverse engineering tasks to enhance their prompting skills, with various quantitative metrics analyzed.

**Key Contributions:** Demonstrated the effectiveness of structured prompting guides in architectural education., Showed the benefits of interactive AI personas in enhancing students' critical thinking and prompting skills., Provided quantitative and qualitative evidence of improvements in prompting proficiency.

**Result:** Statistically significant improvements in word count, similarity, and concreteness were found in the group using AI personas and structured guides.

**Limitations:** Correlations observed were not all statistically significant, suggesting variability in the effectiveness of the methods used.

**Future Work:** Further exploration of AI-assisted pedagogical methods in different educational contexts and disciplines is suggested.

**Conclusion:** Tailored GPT interactions can significantly enhance students' communication of architectural concepts.

**Abstract:** This research investigates the use of customized GPT models to enhance prompting proficiency among architecture students when generating AI-driven images. Prompt engineering is increasingly essential in architectural education due to the widespread adoption of generative AI tools. This study utilized a mixed-methods experimental design involving architecture students divided into three distinct groups: a control group receiving no structured support, a second group provided with structured prompting guides, and a third group supported by both structured guides and interactive AI personas. Students engaged in reverse engineering tasks, first guessing provided image prompts and then generating their own prompts, aiming to boost critical thinking and prompting skills. Variables examined included time spent prompting, word count, prompt similarity, and concreteness. Quantitative analysis involved correlation assessments between these variables and a one-way ANOVA to evaluate differences across groups. While several correlations showed meaningful relationships, not all were statistically significant. ANOVA results indicated statistically significant improvements in word count, similarity, and concreteness, especially in the group supported by AI personas and structured prompting guides. Qualitative feedback complemented these findings, revealing enhanced confidence and critical thinking skills in students. These results suggest tailored GPT interactions substantially improve students' ability to communicate architectural concepts clearly and effectively.

</details>


### [18] [Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents](https://arxiv.org/abs/2504.17934)

*Chaoran Chen, Zhiping Zhang, Ibrahim Khalilov, Bingcan Guo, Simret A Gebreegziabher, Yanfang Ye, Ziang Xiao, Yaxing Yao, Tianshi Li, Toby Jia-Jun Li*

**Main category:** cs.HC

**Keywords:** Large Language Models, GUI Automation, Privacy, Security, Human-Centered Design

**Relevance Score:** 9

**TL;DR:** This position paper discusses the privacy and security risks of LLM-powered GUI agents and advocates for a human-centered evaluation framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The advent of LLMs has transformed GUI automation, but their handling of sensitive data poses privacy and security concerns that have not been adequately addressed.

**Method:** The paper identifies three key risks of GUI agents, reviews existing evaluation metrics for GUI and LLM agents, and outlines challenges in human evaluator integration for assessments.

**Key Contributions:** Identification of privacy and security risks unique to LLM-powered GUI agents., Review of current evaluation metrics and their limitations., Proposal for a human-centered evaluation framework for GUI agents.

**Result:** It reveals a significant focus on performance in evaluations, neglecting privacy and security considerations; this gap necessitates a new evaluation framework.

**Limitations:** Limited exploration of existing data on GUI agent privacy and security assessments.

**Future Work:** Further development and testing of the proposed human-centered evaluation framework and integration of user privacy factors in GUI agents.

**Conclusion:** The paper calls for incorporating risk assessments, in-context consent, and enhanced privacy considerations in the design and evaluation of GUI agents.

**Abstract:** The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [19] [Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English](https://arxiv.org/abs/2504.17974)

*Sabur Butt, Fazlourrahman Balouchzahi, Ahmad Imam Amjad, Maaz Amjad, Hector G. Ceballos, Salud Maria Jimenez-Zafra*

**Main category:** cs.CL

**Keywords:** hope, Natural Language Processing, emotion recognition, dataset, transformer models

**Relevance Score:** 8

**TL;DR:** The study introduces PolyHope V2, a multilingual fine-grained hope speech dataset and benchmarks various models to detect nuanced categories of hope in tweets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the complex emotional state of hope, which is underexplored yet significant in education, mental health, and social interaction, and to improve natural language processing (NLP) detection of this emotion.

**Method:** The methodology involves the creation of the PolyHope V2 dataset, which includes over 30,000 annotated tweets in English and Spanish categorized into four hope subtypes. Multiple pretrained transformer models and large language models (LLMs) were benchmarked under zero-shot and few-shot conditions.

**Key Contributions:** Introduction of the PolyHope V2 dataset with nuanced hope categories, Benchmarking performance of transformers vs LLMs in detecting hope, Highlighting challenges in NLP emotion recognition, particularly for sarcasm

**Result:** Fine-tuned transformers showed superior performance compared to prompt-based LLMs in distinguishing nuanced hope categories and identifying sarcasm, revealing systematic challenges in separating closely related hope subtypes.

**Limitations:** The study primarily focuses on tweets, which may limit generalizability to other contexts of emotional expression.

**Future Work:** Future research directions include enhancing datasets for other emotions and improving model performance in nuanced emotion detection across diverse contexts.

**Conclusion:** The study provides a foundational resource for future emotion recognition tasks, emphasizing the need for greater semantic and contextual sensitivity across languages.

**Abstract:** Hope is a complex and underexplored emotional state that plays a significant role in education, mental health, and social interaction. Unlike basic emotions, hope manifests in nuanced forms ranging from grounded optimism to exaggerated wishfulness or sarcasm, making it difficult for Natural Language Processing systems to detect accurately. This study introduces PolyHope V2, a multilingual, fine-grained hope speech dataset comprising over 30,000 annotated tweets in English and Spanish. This resource distinguishes between four hope subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances existing datasets by explicitly labeling sarcastic instances. We benchmark multiple pretrained transformer models and compare them with large language models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes. Our findings show that fine-tuned transformers outperform prompt-based LLMs, especially in distinguishing nuanced hope categories and sarcasm. Through qualitative analysis and confusion matrices, we highlight systematic challenges in separating closely related hope subtypes. The dataset and results provide a robust foundation for future emotion recognition tasks that demand greater semantic and contextual sensitivity across languages.

</details>


### [20] [Improving LLM Personas via Rationalization with Psychological Scaffolds](https://arxiv.org/abs/2504.17993)

*Brihi Joshi, Xiang Ren, Swabha Swayamdipta, Rik Koncel-Kedziorski, Tim Paek*

**Main category:** cs.CL

**Keywords:** Language Model, User Preferences, Psychological Rationales, Personality Traits, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** The paper introduces PB&J, a framework that enhances language model personas by incorporating psychological rationales for user preferences, leading to improved predictions of user behavior.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing persona-building methods for language models fail to account for the reasoning behind user judgments, which limits their effectiveness in predicting user preferences.

**Method:** The PB&J framework integrates user rationales derived from their experiences, personality traits, and beliefs, utilizing psychological scaffolds rooted in established theories like the Big 5 Personality Traits.

**Key Contributions:** Introduction of the PB&J framework for enhancing LLM personas, Demonstration of the effectiveness of incorporating psychological rationales, Comparison of PB&J with traditional demographic-based approaches showing superior performance.

**Result:** Experiments reveal that LLM personas enhanced with PB&J rationales consistently outperform traditional methods that rely only on demographics or prior judgments for user preference predictions.

**Limitations:** 

**Future Work:** Further exploration of psychological theories that can enhance LLM persona development and testing on diverse user preference tasks.

**Conclusion:** The integration of psychological rationales significantly improves the predictive accuracy of language model personas, suggesting a need to move beyond simplistic demographic approaches.

**Abstract:** Language models prompted with a user description or persona can predict a user's preferences and opinions, but existing approaches to building personas -- based solely on a user's demographic attributes and/or prior judgments -- fail to capture the underlying reasoning behind said user judgments. We introduce PB&J (Psychology of Behavior and Judgments), a framework that improves LLM personas by incorporating rationales of why a user might make specific judgments. These rationales are LLM-generated, and aim to reason about a user's behavior on the basis of their experiences, personality traits or beliefs. This is done using psychological scaffolds -- structured frameworks grounded in theories such as the Big 5 Personality Traits and Primal World Beliefs -- that help provide structure to the generated rationales. Experiments on public opinion and movie preference prediction tasks demonstrate that LLM personas augmented with PB&J rationales consistently outperform methods using only a user's demographics and/or judgments. Additionally, LLM personas constructed using scaffolds describing user beliefs perform competitively with those using human-written rationales.

</details>


### [21] [Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation](https://arxiv.org/abs/2504.18012)

*Zhuang Yu, Shiliang Sun, Jing Zhao, Tengfei Song, Hao Yang*

**Main category:** cs.CL

**Keywords:** Multimodal Machine Translation, pre-trained models, visual-text alignment, translation performance, modality fusion

**Relevance Score:** 6

**TL;DR:** This study explores the role of pre-trained encoders and decoders in Multimodal Machine Translation, demonstrating that pre-trained decoders enhance translation fluency and accuracy, while encoders' effectiveness varies with visual-text alignment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the underexplored effectiveness of pre-trained language and vision models in Multimodal Machine Translation (MMT).

**Method:** We systematically analyze different training strategies from scratch to pre-trained and partially frozen components in a unified MMT framework, conducting experiments on Multi30K and CoMMuTE datasets for English-German and English-French translation tasks.

**Key Contributions:** Analysis of pre-trained encoders and decoders in MMT, Insights on modality fusion and pre-trained components, Guidance for future architecture design in multimodal translation systems.

**Result:** Pre-trained decoders yield more fluent and accurate outputs, while pre-trained encoders show varied effects based on visual-text alignment quality.

**Limitations:** 

**Future Work:** Further exploration of architecture designs that incorporate effective modality fusion and pre-trained components.

**Conclusion:** Pre-training is crucial in multimodal settings, and effective modality fusion alongside pre-trained components must be considered in future MMT architecture designs.

**Abstract:** Multimodal Machine Translation (MMT) aims to improve translation quality by leveraging auxiliary modalities such as images alongside textual input. While recent advances in large-scale pre-trained language and vision models have significantly benefited unimodal natural language processing tasks, their effectiveness and role in MMT remain underexplored. In this work, we conduct a systematic study on the impact of pre-trained encoders and decoders in multimodal translation models. Specifically, we analyze how different training strategies, from training from scratch to using pre-trained and partially frozen components, affect translation performance under a unified MMT framework. Experiments are carried out on the Multi30K and CoMMuTE dataset across English-German and English-French translation tasks. Our results reveal that pre-training plays a crucial yet asymmetrical role in multimodal settings: pre-trained decoders consistently yield more fluent and accurate outputs, while pre-trained encoders show varied effects depending on the quality of visual-text alignment. Furthermore, we provide insights into the interplay between modality fusion and pre-trained components, offering guidance for future architecture design in multimodal translation systems.

</details>


### [22] [RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models](https://arxiv.org/abs/2504.18041)

*Bang An, Shiyue Zhang, Mark Dredze*

**Main category:** cs.CL

**Keywords:** Large language models, Retrieval-Augmented Generation, Safety, Red teaming, AI safety

**Relevance Score:** 9

**TL;DR:** The paper analyzes the safety implications of the Retrieval-Augmented Generation (RAG) framework in relation to large language models (LLMs), demonstrating that RAG can reduce safety and alter models' safety profiles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how RAG frameworks differ from standard LLMs in terms of safety, as current safety research largely overlooks RAG use cases.

**Method:** Conducted a comparative analysis of eleven LLMs, examining their safety profiles in both RAG and non-RAG contexts.

**Key Contributions:** Detailed comparative analysis of safety profiles for RAG vs non-RAG LLMs, Demonstration of RAG's potential to decrease model safety, Evaluation of red teaming methods' effectiveness in RAG settings

**Result:** RAG can reduce model safety and alter safety profiles, with unsafe generations possible even from safe combinations of models and documents; existing red teaming methods are less effective for RAG settings.

**Limitations:** The study examines only eleven LLMs, which may not represent the full spectrum of models.

**Future Work:** Develop safety evaluation frameworks and red teaming methods tailored for RAG LLMs.

**Conclusion:** There is a critical need for safety research and red teaming methods specifically adapted for RAG LLMs.

**Abstract:** Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.

</details>


### [23] [DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models](https://arxiv.org/abs/2504.18053)

*Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Risk Disentanglement, Safety Alignment, Reinforcement Learning, DREAM

**Relevance Score:** 8

**TL;DR:** This paper introduces DREAM, a novel approach for enhancing safety alignment in Multimodal Large Language Models through risk disentanglement and reinforcement learning, significantly improving safety during inference and training phases without performance loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of visual and textual data in MLLMs creates new safety challenges and potential risk combinations that need to be disentangled systematically.

**Method:** The paper employs a step-by-step reasoning analysis to disentangle risks associated with multimodal inputs and develops the DREAM framework which utilizes supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF).

**Key Contributions:** Introduction of DREAM framework for risk disentanglement in MLLMs, Demonstrated significant safety improvements without sacrificing performance, Provided code and data for further research

**Result:** DREAM achieved a 16.17% improvement in the SIUO safe&effective score compared to GPT-4V, enhancing safety during both inference and training.

**Limitations:** 

**Future Work:** Further explorations on enhancing safety alignment in more diverse multimodal contexts and improving the scalability of the DREAM framework.

**Conclusion:** The systematic multimodal risk disentanglement was found to significantly improve risk awareness in MLLMs, leading to enhanced performance in terms of safety alignment.

**Abstract:** Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \textbf{DREAM} (\textit{\textbf{D}isentangling \textbf{R}isks to \textbf{E}nhance Safety \textbf{A}lignment in \textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\% improvement in the SIUO safe\&effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM.

</details>


### [24] [Exploring Personality-Aware Interactions in Salesperson Dialogue Agents](https://arxiv.org/abs/2504.18058)

*Sijia Cheng, Wen-Yu Chang, Yun-Nung Chen*

**Main category:** cs.CL

**Keywords:** dialogue agents, user personas, Myers-Briggs Type Indicator, sales, personalization

**Relevance Score:** 7

**TL;DR:** This study investigates how user personas, defined by the Myers-Briggs Type Indicator, affect the performance of dialogue agents in sales scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the quality of interactions between dialogue agents and users with diverse personalities.

**Method:** The study involves large-scale testing of a pre-trained dialogue agent, analyzing its adaptability and personalization capabilities based on MBTI user types.

**Key Contributions:** Development of persona-defined user simulators for research, Insights into interaction dynamics with varying MBTI-defined user types, Recommendations for building adaptive dialogue systems in sales

**Result:** The findings indicate significant patterns in interaction quality, task completion rates, and dialogue naturalness based on user personas.

**Limitations:** The study is confined to the sales domain and may require further exploration in other contexts.

**Future Work:** Extending the framework to other domains and further improving personalization techniques for dialogue agents.

**Conclusion:** Dialogue agents can improve their strategies to be more effective by adapting to varying personality traits, with implications for broader applications in conversational systems.

**Abstract:** The integration of dialogue agents into the sales domain requires a deep understanding of how these systems interact with users possessing diverse personas. This study explores the influence of user personas, defined using the Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance of sales-oriented dialogue agents. Through large-scale testing and analysis, we assess the pre-trained agent's effectiveness, adaptability, and personalization capabilities across a wide range of MBTI-defined user types. Our findings reveal significant patterns in interaction dynamics, task completion rates, and dialogue naturalness, underscoring the future potential for dialogue agents to refine their strategies to better align with varying personality traits. This work not only provides actionable insights for building more adaptive and user-centric conversational systems in the sales domain but also contributes broadly to the field by releasing persona-defined user simulators. These simulators, unconstrained by domain, offer valuable tools for future research and demonstrate the potential for scaling personalized dialogue systems across diverse applications.

</details>


### [25] [PropRAG: Guiding Retrieval with Beam Search over Proposition Paths](https://arxiv.org/abs/2504.18070)

*Jingjin Wang*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, Large Language Models, Multi-step reasoning, Graph traversal, Non-parametric continual learning

**Relevance Score:** 9

**TL;DR:** PropRAG is a new framework enhancing Retrieval Augmented Generation (RAG) for Large Language Models (LLMs) by utilizing contextual propositions and a novel beam search algorithm for improved multi-step reasoning and retrieval without online LLM invocation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of standard RAG in capturing human memory's interconnected nature crucial for complex reasoning and contextual understanding.

**Method:** PropRAG employs contextually rich propositions and a beam search algorithm for discovering multi-step reasoning chains, operating through efficient graph traversal and pre-computed embeddings, avoiding online LLM inference costs.

**Key Contributions:** Introduction of PropRAG framework leveraging contextually rich propositions., Development of a novel beam search algorithm for reasoning chain discovery., Demonstration of state-of-the-art performance on multiple QA datasets.

**Result:** PropRAG achieves state-of-the-art performance on various QA benchmarks, with zero-shot Recall@5 results showing significant improvements over existing methods.

**Limitations:** The framework's reliance on pre-computed embeddings may limit adaptability to rapidly changing knowledge bases.

**Future Work:** Exploring further enhancements to the reasoning capabilities and adapting PropRAG for more dynamic retrieval scenarios.

**Conclusion:** By enhancing evidence retrieval and representation, PropRAG represents a leap forward in non-parametric continual learning, offering improved efficiency and effectiveness for LLMs in reasoning tasks.

**Abstract:** Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to capture the interconnected nature of human memory crucial for complex reasoning (associativity) and contextual understanding (sense-making). While structured RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples, the inherent context loss limits fidelity. We introduce PropRAG, a framework leveraging contextually rich propositions and a novel beam search algorithm over proposition paths to explicitly discover multi-step reasoning chains. Crucially, PropRAG's online retrieval process operates entirely without invoking generative LLMs, relying instead on efficient graph traversal and pre-computed embeddings. This avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are used effectively offline for high-quality proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.

</details>


### [26] [Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization](https://arxiv.org/abs/2504.18080)

*Wataru Kawakami, Keita Suzuki, Junichiro Iwasawa*

**Main category:** cs.CL

**Keywords:** Large Language Models, Japanese medical domain, Reasoning Preference Optimization

**Relevance Score:** 9

**TL;DR:** Introduction of Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for Japanese medicine, achieving high accuracy and reliable reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of factual accuracy and reasoning reliability in clinical adoption of LLMs in the medical field, particularly focusing on the Japanese context.

**Method:** A two-stage fine-tuning process on Qwen2.5-72B: Continued Pretraining (CPT) on a Japanese medical corpus followed by Reasoning Preference Optimization (RPO) for enhancing reliable reasoning pathways.

**Key Contributions:** Introduction of a model specifically optimized for the Japanese medical domain, Demonstration of effective reasoning generation stabilization through RPO, Release of model weights to aid further research in trustworthy LLMs for high-stakes applications.

**Result:** Preferred-MedLLM-Qwen-72B achieves a state-of-the-art performance of 0.868 accuracy on the IgakuQA benchmark, outperforming models like GPT-4o, and maintains accuracy even when generating reasoning explanations.

**Limitations:** 

**Future Work:** Further exploration into trustworthy LLMs for specialized applications in medicine and possibly other languages.

**Conclusion:** Optimizing for reliable explanations alongside accuracy is crucial for clinical LLM adoption, and the model's weights are released to support further research in this area.

**Abstract:** Large Language Models (LLMs) show potential in medicine, yet clinical adoption is hindered by concerns over factual accuracy, language-specific limitations (e.g., Japanese), and critically, their reliability when required to generate reasoning explanations -- a prerequisite for trust. This paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to achieve both high accuracy and stable reasoning. We employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first, Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a preference-based method, enhances the generation of reliable reasoning pathways while preserving high answer accuracy. Evaluations on the Japanese Medical Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves state-of-the-art performance (0.868 accuracy), surpassing strong proprietary models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which exhibit significant accuracy degradation (up to 11.5\% and 3.8\% respectively on IgakuQA) when prompted for explanations, our model maintains its high accuracy (0.868) under such conditions. This highlights RPO's effectiveness in stabilizing reasoning generation. This work underscores the importance of optimizing for reliable explanations alongside accuracy. We release the Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy LLMs for specialized, high-stakes applications.

</details>


### [27] [Random-Set Large Language Models](https://arxiv.org/abs/2504.18085)

*Muhammad Mubashar, Shireen Kudukkil Manchingal, Fabio Cuzzolin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Quantification, Random-Set, Hierarchical Clustering, Token Space

**Relevance Score:** 8

**TL;DR:** This paper studies uncertainty quantification in Large Language Models (LLMs) and introduces a novel Random-Set LLM (RSLLM) approach for predicting belief functions over tokens, enhancing scalability and effectiveness in assessing model uncertainty.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of trustworthiness in the text generated by LLMs by quantifying uncertainty in their predictions.

**Method:** The authors propose the Random-Set Large Language Model (RSLLM), which predicts finite random sets over the token space and uses hierarchical clustering to define a budget of 'focal' subsets of tokens for scalable belief prediction.

**Key Contributions:** Introduction of Random-Set Large Language Model (RSLLM) for better uncertainty quantification in LLMs., Use of hierarchical clustering to efficiently define focal subsets of tokens., Demonstration of RSLLM's superiority in correctness and uncertainty estimation over standard models.

**Result:** RSLLM outperforms classical models on CoQA and OBQA datasets in terms of correctness and shows potential for estimating second-level uncertainty and detecting hallucinations in predictions.

**Limitations:** 

**Future Work:** Exploration of further enhancements in quantifying and reducing model hallucinations and expanding dataset evaluations.

**Conclusion:** RSLLMs provide a more reliable framework for understanding and quantifying uncertainty in LLM outputs, enhancing their utility in real-world applications.

**Abstract:** Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which predicts finite random sets (belief functions) over the token space, rather than probability vectors as in classical LLMs. In order to allow so efficiently, we also present a methodology based on hierarchical clustering to extract and use a budget of "focal" subsets of tokens upon which the belief prediction is defined, rather than using all possible collections of tokens, making the method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions. The proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b and Phi-2 models and is shown to outperform the standard model in both datasets in terms of correctness of answer while also showing potential in estimating the second level uncertainty in its predictions and providing the capability to detect when its hallucinating.

</details>


### [28] [Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation](https://arxiv.org/abs/2504.18104)

*Yinglong Yu, Hao Shen, Zhengyi Lyu, Qi He*

**Main category:** cs.CL

**Keywords:** Fact-checking, Prompt tuning, Misinformation, Large language models, Classification

**Relevance Score:** 6

**TL;DR:** This paper proposes a prompt tuning method for estimating fact-check-worthiness, showing improved accuracy in classification tasks compared to various baseline models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing problem of misinformation due to globalization and informatization.

**Method:** A classification method for fact-check-worthiness estimation using prompt tuning applied to large language models with designed prompt templates.

**Key Contributions:** Development of a novel prompt tuning approach for fact-check-worthiness estimation., Demonstration of advantages over classical and recent large model baselines., Validation of effectiveness in handling limited datasets.

**Result:** The proposed method outperforms or matches multiple baseline methods, including classical models like BERT and large models like GPT-3.5 and GPT-4, in terms of evaluation metrics such as F1 score and accuracy.

**Limitations:** 

**Future Work:** Exploration of additional applications of prompt tuning in other domains and further improvements in estimation accuracy.

**Conclusion:** The prompt tuning-based method effectively enhances fact-check-worthiness estimation, especially in cases with limited or unlabeled data.

**Abstract:** In response to the growing problem of misinformation in the context of globalization and informatization, this paper proposes a classification method for fact-check-worthiness estimation based on prompt tuning. We construct a model for fact-check-worthiness estimation at the methodological level using prompt tuning. By applying designed prompt templates to large language models, we establish in-context learning and leverage prompt tuning technology to improve the accuracy of determining whether claims have fact-check-worthiness, particularly when dealing with limited or unlabeled data. Through extensive experiments on public datasets, we demonstrate that the proposed method surpasses or matches multiple baseline methods in the classification task of fact-check-worthiness estimation assessment, including classical pre-trained models such as BERT, as well as recent popular large models like GPT-3.5 and GPT-4. Experiments show that the prompt tuning-based method proposed in this study exhibits certain advantages in evaluation metrics such as F1 score and accuracy, thereby effectively validating its effectiveness and advancement in the task of fact-check-worthiness estimation.

</details>


### [29] [Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering](https://arxiv.org/abs/2504.18106)

*Yinglong Yu, Zhaopu Yao, Fang Yuan*

**Main category:** cs.CL

**Keywords:** topic modeling, Large Language Models, media analysis, Paris Olympics, attitudinal meanings

**Relevance Score:** 6

**TL;DR:** This study investigates Chinese and English media reports on the Paris Olympics using LLM prompt engineering and topic modeling to identify discourse differences and similarities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how media discourse differs between Chinese and English reports on the Paris Olympics, looking at attitudinal meanings and thematic focus.

**Method:** The study employs topic modeling and LLM prompt engineering alongside corpus phraseology methods to analyze media reports.

**Key Contributions:** Use of LLM in analyzing media reports, Identification of discourse similarities and differences, Insights into cultural perspectives on the Olympics

**Result:** The analysis reveals that Chinese media emphasizes specific sports and positive sentiments, while English media highlights female athletes and exhibits mixed sentiments.

**Limitations:** The analysis is limited to media reports and may not represent broader public opinion.

**Future Work:** Future research could explore public response or social media sentiments surrounding the Olympics.

**Conclusion:** The findings showcase distinct focus areas and sentiments in Chinese versus English media, reflecting cultural and regional perspectives on the Olympics.

**Abstract:** This study analyzes Chinese and English media reports on the Paris Olympics using topic modeling, Large Language Model (LLM) prompt engineering, and corpus phraseology methods to explore similarities and differences in discourse construction and attitudinal meanings. Common topics include the opening ceremony, athlete performance, and sponsorship brands. Chinese media focus on specific sports, sports spirit, doping controversies, and new technologies, while English media focus on female athletes, medal wins, and eligibility controversies. Chinese reports show more frequent prepositional co-occurrences and positive semantic prosody in describing the opening ceremony and sports spirit. English reports exhibit positive semantic prosody when covering female athletes but negative prosody in predicting opening ceremony reactions and discussing women's boxing controversies.

</details>


### [30] [Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection](https://arxiv.org/abs/2504.18114)

*Atharva Kulkarni, Yuan Zhang, Joel Ruben Antony Moniz, Xiou Ge, Bo-Hsiang Tseng, Dhivya Piraviperumal, Swabha Swayamdipta, Hong Yu*

**Main category:** cs.CL

**Keywords:** hallucinations, language models, LLM evaluation, GPT-4, decoding methods

**Relevance Score:** 8

**TL;DR:** This paper evaluates the effectiveness of various hallucination detection metrics in language models and highlights significant limitations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in measuring hallucinations in language models, which hinder their reliability and adoption.

**Method:** The authors conducted a large-scale empirical evaluation of 6 sets of hallucination detection metrics across 4 datasets, 37 language models, and 5 decoding methods.

**Key Contributions:** Large-scale empirical evaluation of hallucination metrics, Identification of gaps in current evaluation methods, Discovery that mode-seeking decoding methods help reduce hallucinations.

**Result:** The study found that many metrics do not align with human judgments and show inconsistent performance as parameters scale, though LLM-based evaluation with GPT-4 performed best.

**Limitations:** Metrics often do not align with human judgments and are myopic; performance varies with parameter scaling.

**Future Work:** Develop more robust metrics and strategies for hallucination mitigation.

**Conclusion:** The results indicate the need for more robust evaluation metrics for hallucinations and better methods to mitigate them.

**Abstract:** Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.

</details>


### [31] [Temporal Entailment Pretraining for Clinical Language Models over EHR Data](https://arxiv.org/abs/2504.18128)

*Tatsunori Tanaka, Fi Zheng, Kai Sato, Zhifeng Li, Yuanyun Zhang, Shi Li*

**Main category:** cs.CL

**Keywords:** clinical language models, temporal entailment, electronic health records, pretraining, MIMIC IV

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel temporal entailment pretraining objective for clinical language models to enhance their ability to understand evolving patient trajectories in electronic health records (EHRs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current clinical language models fail to address the dynamic and temporal aspects of patient data in electronic health records, which limits their understanding of patient trajectories.

**Method:** The authors introduce a pretraining task that formulates EHR segments as ordered sentence pairs, training models to assess entailment, contradiction, or neutrality between states over time.

**Key Contributions:** Introduction of a temporal entailment pretraining objective for clinical language models, Improved model generalization across clinical tasks, Achievement of state-of-the-art results on several key tasks in the clinical domain.

**Result:** The proposed method leads to state-of-the-art performance in temporal clinical question answering, early warning prediction, and disease progression modeling.

**Limitations:** 

**Future Work:** Exploration of further applications of the temporal framework in other areas of clinical research and refining the model for better performance.

**Conclusion:** Training language models with a focus on temporal relations improves their reasoning capabilities, enhancing their performance on forecasting and diagnosis tasks.

**Abstract:** Clinical language models have achieved strong performance on downstream tasks by pretraining on domain specific corpora such as discharge summaries and medical notes. However, most approaches treat the electronic health record as a static document, neglecting the temporally-evolving and causally entwined nature of patient trajectories. In this paper, we introduce a novel temporal entailment pretraining objective for language models in the clinical domain. Our method formulates EHR segments as temporally ordered sentence pairs and trains the model to determine whether a later state is entailed by, contradictory to, or neutral with respect to an earlier state. Through this temporally structured pretraining task, models learn to perform latent clinical reasoning over time, improving their ability to generalize across forecasting and diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and demonstrate state of the art results on temporal clinical QA, early warning prediction, and disease progression modeling.

</details>


### [32] [EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)](https://arxiv.org/abs/2504.18142)

*Fida Ullah, Muhammad Ahmad, Muhammad Tayyab Zamir, Muhammad Arif, Grigori sidorov, Edgardo Manuel Felipe RiverÃ³n, Alexander Gelbukh*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Urdu Language, Dataset Creation, Education, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** This paper presents a dataset for Named Entity Recognition (NER) in the Urdu language focused on the education domain, addressing the lack of annotated resources.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant underexploration of Named Entity Recognition (NER) in Urdu, particularly in education, due to the absence of annotated datasets.

**Method:** Creation of a manually annotated dataset named EDU-NER-2025, which includes 13 unique educational entities, along with a detailed description of the annotation process and the challenges encountered.

**Key Contributions:** Creation of EDU-NER-2025, the first annotated dataset for NER in Urdu education, Detailed annotation process and guidelines, Analysis of linguistic challenges in Urdu texts

**Result:** The EDU-NER-2025 dataset facilitates improved NER in Urdu educational texts by providing targeted resources for academic roles and course names, which lack representation in existing datasets.

**Limitations:** The study is limited to the education domain and may not be applicable to other fields, potentially affecting generalizability.

**Future Work:** Future research may expand the dataset to other domains and improve the NER models using this dataset.

**Conclusion:** This work lays the foundation for enhanced NER applications in Urdu education, highlighting the necessity for targeted datasets in underrepresented languages and domains.

**Abstract:** Named Entity Recognition (NER) plays a pivotal role in various Natural Language Processing (NLP) tasks by identifying and classifying named entities (NEs) from unstructured data into predefined categories such as person, organization, location, date, and time. While extensive research exists for high-resource languages and general domains, NER in Urdu particularly within domain-specific contexts like education remains significantly underexplored. This is Due to lack of annotated datasets for educational content which limits the ability of existing models to accurately identify entities such as academic roles, course names, and institutional terms, underscoring the urgent need for targeted resources in this domain. To the best of our knowledge, no dataset exists in the domain of the Urdu language for this purpose. To achieve this objective this study makes three key contributions. Firstly, we created a manually annotated dataset in the education domain, named EDU-NER-2025, which contains 13 unique most important entities related to education domain. Second, we describe our annotation process and guidelines in detail and discuss the challenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed key linguistic challenges, such as morphological complexity and ambiguity, which are prevalent in formal Urdu texts.

</details>


### [33] [Aligning Language Models for Icelandic Legal Text Summarization](https://arxiv.org/abs/2504.18180)

*ÃÃ³rir Hrafn HarÃ°arson, Hrafn Loftsson, StefÃ¡n Ãlafsson*

**Main category:** cs.CL

**Keywords:** language models, legal summaries, preference training, Reinforcement Learning, Icelandic

**Relevance Score:** 4

**TL;DR:** This study investigates the use of preference-based training techniques to improve language model performance in generating legal summaries in Icelandic.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of language models in the legal field aims to improve efficiency in managing extensive workloads but faces challenges due to specialized legal language.

**Method:** The study compares preference-based training methods, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, with conventional supervised learning for generating Icelandic legal summaries.

**Key Contributions:** Introduces preference-based training techniques to the legal domain, Demonstrates improved legal accuracy in summary generation, Highlights the importance of qualitative assessment over automated metrics

**Result:** Preference training enhances legal accuracy in summaries compared to standard fine-tuning but does not significantly improve overall language quality, highlighting discrepancies between automated metrics and human evaluations.

**Limitations:** Limited improvement in overall language quality.

**Future Work:** Further research is needed to enhance the qualitative aspects of language models in the legal context.

**Conclusion:** Qualitative assessments are crucial in the development of language models for the legal domain despite improvements in accuracy with preference training.

**Abstract:** The integration of language models in the legal domain holds considerable promise for streamlining processes and improving efficiency in managing extensive workloads. However, the specialized terminology, nuanced language, and formal style of legal texts can present substantial challenges. This study examines whether preference-based training techniques, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, can enhance models' performance in generating Icelandic legal summaries that align with domain-specific language standards and user preferences. We compare models fine-tuned with preference training to those using conventional supervised learning. Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations further underscore the importance of qualitative assessment in developing language models for the legal domain.

</details>


### [34] [Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish](https://arxiv.org/abs/2504.18221)

*Shuxiang Du, Ana Guerberof Arenas, Antonio Toral, Kyo Gerrits, Josep Marco Borillo*

**Main category:** cs.CL

**Keywords:** Chat-GPT, machine translation, creativity, literary text, temperature settings

**Relevance Score:** 7

**TL;DR:** This paper explores the variability of Chat-GPT's machine translation outputs across different configurations and languages, with an emphasis on creativity.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how differing settings in Chat-GPT influence the creativity of machine translations, especially in literary contexts.

**Method:** The study evaluates translations across six configurations and four languages, using a Creativity Score to assess outcomes with varying text granularity, temperature settings, and prompting strategies.

**Key Contributions:** Examines the impact of different configurations on Chat-GPT's creative machine translations., Demonstrates that minimal instruction prompting yields superior creative output over other settings., Provides a comparative analysis of Chat-GPT and DeepL performance in various languages.

**Result:** The findings indicate that minimal instructions lead to the most creative translations, particularly at a temperature setting of 1.0, outperforming DeepL in Spanish, Dutch, and Chinese, though not surpassing human translation.

**Limitations:** The study primarily focuses on literary texts and may not generalize across other types of text or contexts.

**Future Work:** Further research could explore machine translation creativity in diverse genres and contexts, as well as improvements in ChatGPT's performance relative to human translation.

**Conclusion:** Prompting ChatGPT effectively can enhance creativity in translations, but it still falls short of human translation quality.

**Abstract:** This study examines the variability of Chat-GPT machine translation (MT) outputs across six different configurations in four languages,with a focus on creativity in a literary text. We evaluate GPT translations in different text granularity levels, temperature settings and prompting strategies with a Creativity Score formula. We found that prompting ChatGPT with a minimal instruction yields the best creative translations, with "Translate the following text into [TG] creatively" at the temperature of 1.0 outperforming other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless, ChatGPT consistently underperforms compared to human translation (HT).

</details>


### [35] [Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family](https://arxiv.org/abs/2504.18225)

*Pierre-Carl Langlais, Pavel Chizhov, Mattia Nee, Carlos Rosas Hinostroza, Matthieu Delsart, IrÃ¨ne Girard, Othman Hicheur, Anastasia Stasenko, Ivan P. Yamshchikov*

**Main category:** cs.CL

**Keywords:** RAG, multilingual models, AI applications

**Relevance Score:** 9

**TL;DR:** Introduction of new small reasoning models, Pleias-RAG-350m and Pleias-RAG-1B, for RAG, search, and source summarization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve retrieval-augmented generation (RAG) with small models while maintaining performance in multilingual contexts and ensuring reference grounding.

**Method:** Train mid-sized models on a large synthetic dataset emulating retrieval from multilingual open sources and integrate capabilities like query routing and source reranking.

**Key Contributions:** Introduction of Pleias-RAG-350m and Pleias-RAG-1B models., Demonstration of improved performance on standardized RAG benchmarks., Native support for citation and grounding with literal quotes.

**Result:** Pleias-RAG models outperform smaller SLMs on RAG benchmarks and show competitive performance against larger models, ensuring consistent results across leading European languages.

**Limitations:** 

**Future Work:** Exploration of additional use cases and refinement of model capabilities in live applications.

**Conclusion:** The small size and design of these models facilitate deployment on limited infrastructure, enhancing their usability for various generative AI applications.

**Abstract:** We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.

</details>


### [36] [Efficient Single-Pass Training for Multi-Turn Reasoning](https://arxiv.org/abs/2504.18246)

*Ritesh Goru, Shanay Mehta, Prateek Jain*

**Main category:** cs.CL

**Keywords:** Large Language Models, multi-turn reasoning, fine-tuning, attention mask, token duplication

**Relevance Score:** 8

**TL;DR:** This paper introduces a method to fine-tune Large Language Models (LLMs) for multi-turn reasoning by employing response token duplication and a custom attention mask, addressing challenges in conventional training methodologies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of LLMs in generating explicit reasoning for tasks requiring multi-turn interactions, particularly in scenarios where conventional training methods hinder efficiency.

**Method:** The proposed method utilizes response token duplication alongside a custom attention mask that adjusts visibility constraints, enabling LLMs to handle multi-turn reasoning effectively in a single forward pass.

**Key Contributions:** Introduction of response token duplication for LLM fine-tuning, Development of a custom attention mask for visibility constraints, Demonstration of improved training efficiency for multi-turn reasoning tasks

**Result:** This approach leads to a significant reduction in training time while allowing for efficient fine-tuning on multi-turn reasoning datasets, improving overall LLM performance in tasks requiring explicit reasoning.

**Limitations:** The specific implications of the custom attention mask on model interpretability and potential trade-offs in performance across different tasks are not fully explored.

**Future Work:** Further exploration of the generalization capabilities of this approach across diverse reasoning tasks and its impact on model interpretability is suggested.

**Conclusion:** By addressing the limitations of previous fine-tuning methodologies, this paper presents a viable solution for training LLMs to better handle complex reasoning tasks over multiple turns.

**Abstract:** Training Large Language Models ( LLMs) to generate explicit reasoning before they produce an answer has been shown to improve their performance across various tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets presents a unique challenge: LLMs must generate reasoning tokens that are excluded from subsequent inputs to the LLM. This discrepancy prevents us from processing an entire conversation in a single forward pass-an optimization readily available when we fine-tune on a multi-turn non-reasoning dataset. This paper proposes a novel approach that overcomes this limitation through response token duplication and a custom attention mask that enforces appropriate visibility constraints. Our approach significantly reduces the training time and allows efficient fine-tuning on multi-turn reasoning datasets.

</details>


### [37] [MAGI: Multi-Agent Guided Interview for Psychiatric Assessment](https://arxiv.org/abs/2504.18260)

*Guanqun Bi, Zhuang Chen, Zhoufu Liu, Hongkai Wang, Xiyao Xiao, Yuqiang Xie, Wen Zhang, Yongkang Huang, Yuxuan Chen, Libiao Peng, Yi Feng, Minlie Huang*

**Main category:** cs.CL

**Keywords:** automated clinical interviews, large language models, mental health, psychometric assessment, multi-agent framework

**Relevance Score:** 9

**TL;DR:** MAGI is a framework that automates structured clinical interviews for mental healthcare by using coordinated multi-agent collaboration based on the MINI protocol.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance accessibility to mental healthcare through automation that aligns with psychiatric diagnostic protocols.

**Method:** MAGI utilizes four specialized agents to navigate clinical logic and conduct interviews: a navigation agent, an adaptive question agent, a judgment agent, and a diagnosis agent that generates PsyCoT traces.

**Key Contributions:** Introduction of a multi-agent framework for clinical interviews, Integration of conversational adaptability with psychiatric protocols, Generation of explainable PsyCoT traces for symptom mapping

**Result:** Experimental results show MAGI effectively combines clinical rigor with conversational adaptability, demonstrating improvements in mental health assessment.

**Limitations:** Limited to the Mini International Neuropsychiatric Interview protocol; requires validation across diverse populations and conditions.

**Future Work:** Exploration of additional diagnostic protocols and enhancement of agent capabilities to cover broader mental health conditions.

**Conclusion:** MAGI represents a significant advancement in LLM-assisted mental health assessments by ensuring compliance with established clinical protocols.

**Abstract:** Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.

</details>


### [38] [TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation](https://arxiv.org/abs/2504.18269)

*Shintaro Ozaki, Kazuki Hayashi, Yusuke Sakai, Jingun Kwon, Hidetaka Kamigaito, Katsuhiko Hayashi, Manabu Okumura, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** Image Generation, Entity Prompt Refinement, Large Language Models, Human-Computer Interaction, Knowledge Augmentation

**Relevance Score:** 6

**TL;DR:** The paper presents TextTIGER, a method for enhancing image generation from entity-based prompts using entity knowledge augmentation and summarization with LLMs, leading to improved performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective image generation from prompts containing specific entities while managing the vast and ever-growing knowledge of these entities.

**Method:** TextTIGER augments entity knowledge in prompts and summarizes descriptions using LLMs to reduce performance degradation with longer inputs.

**Key Contributions:** Introduction of Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER), Creation of the WiT-Cub dataset for evaluating image generation based on entity knowledge, Demonstrated improvements in image generation metrics using LLMs and entity augmentation.

**Result:** TextTIGER shows improved image generation performance evaluated on four models and five LLMs, outperforming caption-only prompts in standard metrics and through human assessment of description quality.

**Limitations:** The effectiveness may vary with different entity complexities and types, and the method's reliance on the quality of the initial prompts.

**Future Work:** Exploration of more diverse entity characteristics and extending the approach to other generative tasks beyond image generation.

**Conclusion:** The results affirm that refining prompts with enriched and summarized entity-related details significantly boosts image generation capabilities.

**Abstract:** Generating images from prompts containing specific entities requires models to retain as much entity-specific knowledge as possible. However, fully memorizing such knowledge is impractical due to the vast number of entities and their continuous emergence. To address this, we propose Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER), which augments knowledge on entities included in the prompts and then summarizes the augmented descriptions using Large Language Models (LLMs) to mitigate performance degradation from longer inputs. To evaluate our method, we introduce WiT-Cub (WiT with Captions and Uncomplicated Background-explanations), a dataset comprising captions, images, and an entity list. Experiments on four image generation models and five LLMs show that TextTIGER improves image generation performance in standard metrics (IS, FID, and CLIPScore) compared to caption-only prompts. Additionally, multiple annotators' evaluation confirms that the summarized descriptions are more informative, validating LLMs' ability to generate concise yet rich descriptions. These findings demonstrate that refining prompts with augmented and summarized entity-related descriptions enhances image generation capabilities. The code and dataset will be available upon acceptance.

</details>


### [39] [Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review](https://arxiv.org/abs/2504.18346)

*Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif Ali, Yukai Miao, Dan Li, Qingsong Wei*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Quantification, Calibration Techniques, Hallucination in AI, Reliability Datasets

**Relevance Score:** 9

**TL;DR:** This paper reviews Uncertainty Quantification (UQ) and calibration methods for Large Language Models (LLMs), providing a benchmark and evaluating the effectiveness of existing techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant challenge of hallucination in LLMs by assessing and quantifying their uncertainty, a comprehensive analysis of existing UQ methods and their calibration is needed.

**Method:** A systematic survey of prior works related to UQ and calibration for LLMs, along with an empirical evaluation of six related methods using reliability datasets.

**Key Contributions:** Systematic survey of UQ and calibration methods for LLMs, Introduction of a rigorous benchmark for evaluating existing methods, Empirical evaluation of six UQ methods using reliability datasets

**Result:** The study introduces a benchmark to analyze calibration methods and presents significant findings regarding the effectiveness of these techniques in measuring uncertainty in LLMs.

**Limitations:** The survey may not cover every possible UQ method or calibration technique due to the rapidly evolving nature of the field.

**Future Work:** Outlooks include addressing open challenges in UQ and calibration for LLMs, with suggestions for further empirical evaluations and method improvements.

**Conclusion:** The paper concludes that their review is the first dedicated assessment of calibration methods specifically for LLMs and outlines future research directions in this critical area.

**Abstract:** Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.

</details>


### [40] [Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant](https://arxiv.org/abs/2504.18373)

*Lei Shen, Xiaoyu Shen*

**Main category:** cs.CL

**Keywords:** multi-agent systems, large language models, benchmark dataset, intelligent personal assistants, performance evaluation

**Relevance Score:** 9

**TL;DR:** Auto-SLURP is a benchmark dataset for evaluating LLM-based multi-agent frameworks focusing on intelligent personal assistants, enhancing the original SLURP dataset.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of benchmark datasets for evaluating the performance of multi-agent frameworks powered by large language models (LLMs).

**Method:** The original SLURP dataset was extended by relabeling data and integrating simulated servers and external services, allowing for a comprehensive evaluation of language understanding, task execution, and response generation.

**Key Contributions:** Introduction of Auto-SLURP as a benchmark dataset for evaluating LLM-powered multi-agent systems., Enhancement of the original SLURP dataset through relabeling and integration of external services., Presentation of a comprehensive end-to-end evaluation pipeline for personal assistants.

**Result:** Experiments show that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, revealing that dependable multi-agent personal assistants are still under development.

**Limitations:** Limited to the context of intelligent personal assistants and may not cover other applications of multi-agent frameworks.

**Future Work:** Encouragement of further research to develop more reliable and intelligent multi-agent personal assistants.

**Conclusion:** The dataset and code are publicly available for further research and evaluation of LLM-based systems.

**Abstract:** In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.

</details>


### [41] [Pushing the boundary on Natural Language Inference](https://arxiv.org/abs/2504.18376)

*Pablo Miralles-GonzÃ¡lez, Javier Huertas-Tato, Alejandro MartÃ­n, David Camacho*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Reinforcement Learning, Chain-of-Thought, Language Models, Adversarial Benchmarks

**Relevance Score:** 8

**TL;DR:** This paper presents a reinforcement learning-based approach for Natural Language Inference (NLI) using Group Relative Policy Optimization, eliminating labeled rationale needs and allowing training on challenging datasets, achieving state-of-the-art results with parameter-efficient techniques for language models.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current NLI systems that rely on supervised learning and are affected by annotation artifacts and biases, thereby enhancing generalization and applicability in real-world scenarios.

**Method:** A reinforcement learning framework using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, enabling the elimination of labeled rationales and fine-tuning of large language models.

**Key Contributions:** Introduced a reinforcement learning-based approach for NLI using GRPO and CoT learning., Eliminated the dependency on labeled rationales for training., Achieved state-of-the-art performance on various NLI benchmark sets with large language models.

**Result:** The proposed approach demonstrates strong performance on standard and adversarial NLI benchmarks, with the 32B AWQ-quantized model outperforming state-of-the-art results on 7 out of 11 adversarial sets.

**Limitations:** 

**Future Work:** 

**Conclusion:** The work presents a scalable, practical framework for developing robust NLI systems while maintaining high inference quality, even under aggressive quantization.

**Abstract:** Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\unicode{x2013}$or on all of them considering our replication$\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality.

</details>


### [42] [A UD Treebank for Bohairic Coptic](https://arxiv.org/abs/2504.18386)

*Amir Zeldes, Nina Speransky, Nicholas Wagner, Caroline T. Schroeder*

**Main category:** cs.CL

**Keywords:** Bohairic Coptic, Sahidic Coptic, syntactically annotated corpus, natural language processing, cross-dialect parsing

**Relevance Score:** 2

**TL;DR:** This paper presents the first syntactically annotated corpus of Bohairic Coptic and evaluates its distinct syntactic characteristics compared to Sahidic Coptic.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of digital resources and syntactically annotated corpora for Bohairic Coptic, which is essential for understanding pre-Mamluk, late Byzantine Egypt and the contemporary Coptic Church language.

**Method:** The authors compiled a corpus from various Bohairic texts, including Biblical works and Christian literature, and conducted parsing experiments to analyze syntactic differences between Bohairic and Sahidic Coptic.

**Key Contributions:** First syntactically annotated corpus of Bohairic Coptic, Comparison and analysis of Bohairic and Sahidic Coptic, Parsing experiments highlighting syntactic differences

**Result:** The study unveils unique syntactic features of Bohairic Coptic and highlights key differences compared to the Sahidic dialect, providing insights into cross-dialect relationships.

**Limitations:** 

**Future Work:** Future research could expand the corpus further and explore additional dialectical differences and their implications.

**Conclusion:** The first Bohairic Coptic corpus lays the groundwork for further research and resource development in under-studied Coptic dialects.

**Abstract:** Despite recent advances in digital resources for other Coptic dialects, especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk, late Byzantine Egypt, and the contemporary language of the Coptic Church, remains critically under-resourced. This paper presents and evaluates the first syntactically annotated corpus of Bohairic Coptic, sampling data from a range of works, including Biblical text, saints' lives and Christian ascetic writing. We also explore some of the main differences we observe compared to the existing UD treebank of Sahidic Coptic, the classical dialect of the language, and conduct joint and cross-dialect parsing experiments, revealing the unique nature of Bohairic as a related, but distinct variety from the more often studied Sahidic.

</details>


### [43] [HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?](https://arxiv.org/abs/2504.18406)

*Yusen Zhang, Wenliang Zheng, Aashrith Madasu, Peng Shi, Ryo Kamoi, Hao Zhou, Zhuoyang Zou, Shu Zhao, Sarkar Snigdha Sarathi Das, Vipul Gupta, Xiaoxin Lu, Nan Zhang, Ranran Haoran Zhang, Avitej Iyer, Renze Lou, Wenpeng Yin, Rui Zhang*

**Main category:** cs.CL

**Keywords:** High-Resolution Image Understanding, Vision Large Language Models, HRScene Benchmark

**Relevance Score:** 4

**TL;DR:** Introducing HRScene, a benchmark for evaluating Vision Large Language Models (VLMs) in high-resolution image understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of comprehensive benchmarks for evaluating the HRI understanding capabilities of Vision Large Language Models (VLMs).

**Method:** HRScene is a unified benchmark incorporating 25 real-world datasets and 2 synthetic diagnostic datasets, with detailed annotations provided by graduate students. It encompasses various scenarios including pathological and agricultural images.

**Key Contributions:** Introduction of the HRScene benchmark for high-resolution image understanding., Comprehensive evaluation of 28 VLMs demonstrating their current limitations., Identification of significant regional divergence in VLM performance.

**Result:** Current VLMs exhibit average accuracy of around 50% in understanding real-world high-resolution images, with significant gaps identified in their effectiveness to utilize regions in the images.

**Limitations:** The performance of VLMs is currently limited, with average accuracy suggesting room for significant improvement.

**Future Work:** The study highlights the need for improved VLM models, specifically in effectively utilizing regions in HRI and addressing the gaps shown in the experiments.

**Conclusion:** The findings indicate that there are considerable challenges in the ability of VLMs to understand high-resolution images, which points to essential areas for future research.

**Abstract:** High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\times$ 1,024 to 35,503 $\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research.

</details>


### [44] [Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers](https://arxiv.org/abs/2504.18412)

*Jared Moore, Declan Grabb, William Agnew, Kevin Klyman, Stevie Chancellor, Desmond C. Ong, Nick Haber*

**Main category:** cs.CL

**Keywords:** Large Language Models, Therapy, Mental Health, Therapeutic Alliance, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper evaluates the feasibility of using large language models (LLMs) as therapists, highlighting significant shortcomings and recommending against their replacement of human therapists.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the suitability of LLMs as mental health providers, addressing concerns raised in the tech startup and research space about their potential replacement of traditional therapists.

**Method:** Conducted a mapping review of therapy guides and executed experiments assessing LLM responses to determine their adherence to critical therapeutic relationship aspects.

**Key Contributions:** Assessment of LLMs' appropriateness in therapeutic contexts, Identification of critical therapeutic alliance characteristics, Highlighting current safety practices' inadequacy in LLM responses

**Result:** LLMs demonstrated stigma and provided inappropriate responses in therapy scenarios, actively undermining therapeutic relationships and encouraging harmful thought patterns.

**Limitations:** LLMs lack essential human characteristics required for therapeutic alliances, affecting their suitability as replacements for therapists.

**Future Work:** Explore alternative roles for LLMs in therapy that do not involve direct replacement of human therapists.

**Conclusion:** LLMs should not replace human therapists due to inherent limitations in replicating therapeutic alliances and human characteristics necessary for effective therapy.

**Abstract:** Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy.

</details>


### [45] [BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs](https://arxiv.org/abs/2504.18415)

*Hongyu Wang, Shuming Ma, Furu Wei*

**Main category:** cs.CL

**Keywords:** Large Language Models, Quantization, Activation Outliers, H-BitLinear, Efficiency

**Relevance Score:** 7

**TL;DR:** BitNet v2 enables efficient deployment of 1-bit LLMs through 4-bit activation quantization, minimizing performance degradation and resource usage.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The deployment of 1-bit Large Language Models is hampered by activation outliers that complicate low bit-width quantization.

**Method:** BitNet v2 utilizes H-BitLinear, which applies an online Hadamard transformation before activation quantization to create Gaussian-like distributions from sharp activations.

**Key Contributions:** Introduction of BitNet v2 for 4-bit quantization of 1-bit LLMs, Development of H-BitLinear for mitigating activation outliers, Demonstration of minimal performance degradation with low-bit activations

**Result:** BitNet v2 trained from scratch with 8-bit activations performs similarly to BitNet b1.58, maintaining performance with native 4-bit activations while reducing memory and computational costs during batched inference.

**Limitations:** 

**Future Work:** 

**Conclusion:** The proposed framework demonstrates significant efficiency gains in deploying 1-bit LLMs with improved activation quantization.

**Abstract:** Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.

</details>


### [46] [PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts](https://arxiv.org/abs/2504.18428)

*Yiming Wang, Pei Zhang, Jialong Tang, Haoran Wei, Baosong Yang, Rui Wang, Chenshu Sun, Feitong Sun, Jiran Zhang, Junxuan Wu, Qiqian Cang, Yichang Zhang, Fei Huang, Junyang Lin, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** multilingual benchmark, mathematical reasoning, LLMs, language diversity, output language control

**Relevance Score:** 5

**TL;DR:** Introduction of PolyMath, a multilingual mathematical reasoning benchmark.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a comprehensive, multilingual benchmark for evaluating reasoning LLMs across various languages and difficulty levels.

**Method:** Development of a benchmark covering 18 languages and 4 difficulty levels, with a focus on difficulty comprehensiveness, language diversity, and quality of translation.

**Key Contributions:** Introduction of a diverse multilingual benchmark for mathematical reasoning., Identification of key challenges faced by LLMs in multilingual contexts., Demonstration of how output language control can influence LLM reasoning performance.

**Result:** Evaluation shows that advanced LLMs score low on the benchmark, highlighting significant challenges in multilingual reasoning.

**Limitations:** Benchmark scores show low accuracy for current state-of-the-art LLMs across different languages.

**Future Work:** Exploration of methods to improve reasoning performance in LLMs for low-resource languages.

**Conclusion:** Controlling output language in instructions can enhance reasoning performance in LLMs, especially for low-resource languages.

**Abstract:** In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Deepseek-R1-671B and Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30% accuracy under the highest level. From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs.

</details>


### [47] [Fast-Slow Thinking for Large Vision-Language Model Reasoning](https://arxiv.org/abs/2504.18458)

*Wenyi Xiao, Leilei Gan, Weilong Dai, Wanggui He, Ziwei Huang, Haoyuan Li, Fangxun Shu, Zhelun Yu, Peng Zhang, Hao Jiang, Fei Wu*

**Main category:** cs.CL

**Keywords:** vision-language models, FAST framework, adaptive reasoning, machine learning, token usage

**Relevance Score:** 8

**TL;DR:** Introduction of the FAST framework for dynamic reasoning in large vision-language models (LVLMs) that reduces verbosity while improving accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the overthinking phenomenon in LVLMs that causes verbose reasoning across tasks.

**Method:** Development of the FAST framework that employs model-based metrics for question characterization, an adaptive reward mechanism, and difficulty-aware KL regularization.

**Key Contributions:** Introduction of the FAST framework for adaptive reasoning, Demonstration of significant improvements in accuracy, Reduction in verbosity and token usage across benchmarks.

**Result:** FAST shows state-of-the-art accuracy with over 10% improvement vs. base model and reduces token usage by 32.7-67.3%.

**Limitations:** 

**Future Work:** Exploration of further applications of the FAST framework and its adaptations to other models.

**Conclusion:** FAST effectively balances reasoning length and accuracy in LVLMs.

**Abstract:** Recent advances in large vision-language models (LVLMs) have revealed an \textit{overthinking} phenomenon, where models generate verbose reasoning across all tasks regardless of questions. To address this issue, we present \textbf{FAST}, a novel \textbf{Fa}st-\textbf{S}low \textbf{T}hinking framework that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. We develop FAST-GRPO with three components: model-based metrics for question characterization, an adaptive thinking reward mechanism, and difficulty-aware KL regularization. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10\% relative improvement compared to the base model, while reducing token usage by 32.7-67.3\% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy.

</details>


### [48] [Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions](https://arxiv.org/abs/2504.18474)

*James D. Finch, Yasasvi Josyula, Jinho D. Choi*

**Main category:** cs.CL

**Keywords:** Slot Schema Induction, task-oriented dialogue, language models, evaluation metrics, dialogue understanding

**Relevance Score:** 6

**TL;DR:** Presenting a novel method for Slot Schema Induction in task-oriented dialogue systems using language models to improve schema identification and evaluation metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the automation of identifying key information slots in task-oriented dialogue systems without requiring manual labeling, addressing existing challenges in evaluation methods.

**Method:** This study formulates Slot Schema Induction as a text generation task and develops a fully automatic LLM-based dialogue simulation method that generates high-quality slot schemas from dialogue data.

**Key Contributions:** New approach for Slot Schema Induction formulated as a text generation task using language models., Development of a fully automatic LLM-based TOD simulation method for generating schema data., Improved evaluation metrics and data addressing existing SSI evaluation challenges.

**Result:** The proposed approach improves the state-of-the-art in Slot Schema Induction by effectively creating reliable evaluation metrics and data for task-oriented dialogue systems.

**Limitations:** The method's reliance on the quality of generated dialogue data and potential biases in human-guided evaluations.

**Future Work:** Further exploration of SSI applications in varied domains and refining evaluation techniques based on real-world dialogue applications.

**Conclusion:** The contributions lay a foundation for future research in Slot Schema Induction, improving dialogue understanding and system development.

**Abstract:** In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is essential for automatically identifying key information slots from dialogue data without manual intervention. This paper presents a novel state-of-the-art (SoTA) approach that formulates SSI as a text generation task, where a language model incrementally constructs and refines a slot schema over a stream of dialogue data. To develop this approach, we present a fully automatic LLM-based TOD simulation method that creates data with high-quality state labels for novel task domains. Furthermore, we identify issues in SSI evaluation due to data leakage and poor metric alignment with human judgment. We resolve these by creating new evaluation data using our simulation method with human guidance and correction, as well as designing improved evaluation metrics. These contributions establish a foundation for future SSI research and advance the SoTA in dialogue understanding and system development.

</details>


### [49] [Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues](https://arxiv.org/abs/2504.18483)

*Leandra Fichtel, Maximilian SpliethÃ¶ver, Eyke HÃ¼llermeier, Patricia Jimenez, Nils Klowait, Stefan Kopp, Axel-Cyrille Ngonga Ngomo, Amelie Robrecht, Ingrid Scharlau, Lutz Terfloth, Anna-Lisa Vollmer, Henning Wachsmuth*

**Main category:** cs.CL

**Keywords:** explainable AI, co-constructive explanations, large language models, user study, understanding enhancement

**Relevance Score:** 9

**TL;DR:** The paper examines the role of large language models in co-constructive explanation dialogues to enhance understanding among explainees.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can be used to create explanations that are adapted to the needs of explainees, thus improving the field of explainable AI.

**Method:** A user study was conducted where explainees interacted with LLMs tasked with providing co-constructive explanations on a predefined topic. The study evaluated changes in understanding and perceptions of the LLMs' behavior.

**Key Contributions:** Evaluation of LLMs in co-constructive explanations, User study on explainee engagement, Identification of limitations in LLMs' monitoring abilities

**Result:** The study found that LLMs exhibit some co-constructive behaviors, such as asking verification questions, which can engage explainees and improve their understanding, although their overall effectiveness in monitoring understanding remains limited.

**Limitations:** LLMs' ability to monitor explainees' understanding and adapt explanations in real-time is currently limited.

**Future Work:** Future research should focus on improving LLMs' capabilities in understanding and dynamically adjusting to explainees' comprehension levels.

**Conclusion:** While LLMs show potential in enhancing dialogues through co-constructive explanations, further advancements are needed for more effective understanding tracking and adaptive explanations.

**Abstract:** The ability to generate explanations that are understood by explainees is the quintessence of explainable artificial intelligence. Since understanding depends on the explainee's background and needs, recent research has focused on co-constructive explanation dialogues, where the explainer continuously monitors the explainee's understanding and adapts explanations dynamically. We investigate the ability of large language models (LLMs) to engage as explainers in co-constructive explanation dialogues. In particular, we present a user study in which explainees interact with LLMs, of which some have been instructed to explain a predefined topic co-constructively. We evaluate the explainees' understanding before and after the dialogue, as well as their perception of the LLMs' co-constructive behavior. Our results indicate that current LLMs show some co-constructive behaviors, such as asking verification questions, that foster the explainees' engagement and can improve understanding of a topic. However, their ability to effectively monitor the current understanding and scaffold the explanations accordingly remains limited.

</details>


### [50] [TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation](https://arxiv.org/abs/2504.18535)

*Gwen Yidou Weng, Benjie Wang, Guy Van den Broeck*

**Main category:** cs.CL

**Keywords:** large language models, controllable generation, Hidden Markov Model, Expected Attribute Probability, detoxification

**Relevance Score:** 9

**TL;DR:** TRACE is a new framework for controlling large language model outputs through tractable probabilistic reasoning, allowing for efficient computation of expected attributes and global compliance with low overhead.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of aligning large language models with human values and desired attributes, highlighting the limitations of current tuning methods and the need for efficient control mechanisms.

**Method:** TRACE distills a Hidden Markov Model from a large language model and pairs it with a classifier to compute Expected Attribute Probability (EAP) efficiently, allowing for quick adaptation to new attributes and maintaining global properties.

**Key Contributions:** Introduction of the TRACE framework for controlled generation., Efficient computation of Expected Attribute Probability using Hidden Markov Models and classifiers., Demonstrated capability to adapt quickly to new attributes and composite characteristics.

**Result:** TRACE demonstrates state-of-the-art performance in detoxification tasks, achieving this with only a 10% increase in decoding time, and can adapt to 76 low-resource personalized LLMs in seconds.

**Limitations:** 

**Future Work:** Future research could explore further extensions of TRACE to additional attributes and other natural language processing tasks, as well as potential improvements in computational efficiency.

**Conclusion:** The TRACE framework provides an efficient method for controllable generation in language models, enabling more consistent alignment with desired attributes while maintaining performance.

**Abstract:** As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute - expensive and inflexible - or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable probabilistic reasoning and lightweight control. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMM's predicted futures. This EAP is then used to reweigh the LM's next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with only 10% decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes.

</details>


### [51] [PRobELM: Plausibility Ranking Evaluation for Language Models](https://arxiv.org/abs/2404.03818)

*Zhangdie Yuan, Eric Chamoun, Rami Aly, Chenxi Whitehouse, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** language models, plausibility ranking, benchmark, world knowledge, model evaluation

**Relevance Score:** 8

**TL;DR:** This paper presents PRobELM, a benchmark for evaluating language models' capability to discern plausible scenarios using world knowledge, bridging gaps left by existing benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate language models' abilities to prioritize plausible scenarios that leverage world knowledge, addressing limitations in existing benchmarks focused on truthfulness and factual accuracy.

**Method:** The benchmark is constructed from a dataset curated from Wikidata edit histories, allowing evaluation across multiple prompting types like statement, text completion, and question-answering.

**Key Contributions:** Introduces PRobELM benchmark for plausibility ranking in language models., Demonstrates that factual accuracy does not equate to plausibility performance., Shows that training data recency improves plausibility assessment in models.

**Result:** Experiments reveal that there is no direct correlation between factual accuracy and plausibility performance, while up-to-date training data significantly enhances plausibility assessment across different model architectures.

**Limitations:** The benchmark may require extensive refinement to encompass all aspects of plausibility in language understanding and its practical applications are yet to be fully explored.

**Future Work:** Future directions include expanding the dataset for richer evaluation and exploring the implications of plausibility in various downstream applications.

**Conclusion:** PRobELM can facilitate literature-based discovery by enabling language models to identify likely but not yet known information, showcasing the importance of plausibility in language understanding.

**Abstract:** This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models' capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.

</details>


### [52] [Nearest Neighbor Speculative Decoding for LLM Generation and Attribution](https://arxiv.org/abs/2405.19325)

*Minghan Li, Xilun Chen, Ari Holtzman, Beidi Chen, Jimmy Lin, Wen-tau Yih, Xi Victoria Lin*

**Main category:** cs.CL

**Keywords:** Language Models, NLP, Semi-parametric Models, Speculative Decoding, Attribution

**Relevance Score:** 9

**TL;DR:** NEST is a novel semi-parametric language modeling approach that improves inference speed and generation quality while providing attribution for language model outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address hallucination and attribution limitations in large language models (LLMs), while improving generation speed and fluency.

**Method:** NEST employs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations, using approximate speculative decoding.

**Key Contributions:** Introduction of Nearest Neighbor Speculative Decoding (NEST), Significant speedup in inference time, Improved generation quality and attribution capabilities

**Result:** NEST enhances generation quality and attribution rate, with a significant improvement in inference speed (1.8x faster) compared to traditional kNN-LM methods.

**Limitations:** 

**Future Work:** Exploration of further enhancements in retrieval techniques and broader application scenarios in language modeling.

**Conclusion:** NEST outperforms conventional kNN-LM approaches and is competitive with in-context retrieval augmentation, making it an effective solution for knowledge-intensive tasks.

**Abstract:** Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B. Code will be released at https://github.com/facebookresearch/NEST/tree/main.

</details>


### [53] [AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction](https://arxiv.org/abs/2406.10432)

*Peitao Han, Lis Kanashiro Pereira, Fei Cheng, Wan Jou She, Eiji Aramaki*

**Main category:** cs.CL

**Keywords:** Relation Extraction, In-Context Learning, AMR Enhancement

**Relevance Score:** 6

**TL;DR:** This paper presents an AMR-enhanced retrieval-based in-context learning method for relation extraction that focuses on semantic structure similarity, outperforming existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve relation extraction by addressing the limitations of existing in-context learning methods which often focus on language similarity rather than structural similarity, potentially overlooking entity relationships.

**Method:** The proposed method retrieves examples based on semantic structure similarity between the inputs and training samples for relation extraction tasks.

**Key Contributions:** Introduction of an AMR-enhanced model for relation extraction, Demonstration of improved performance in unsupervised and supervised settings, Focus on semantic structure similarity in in-context learning

**Result:** Evaluations demonstrated that the model outperforms existing baselines in unsupervised settings across four datasets and achieves state-of-the-art results in supervised settings on three datasets.

**Limitations:** 

**Future Work:** 

**Conclusion:** The AMR-enhanced retrieval-based model significantly improves relation extraction performance by leveraging semantic structural similarities.

**Abstract:** Existing in-context learning (ICL) methods for relation extraction (RE) often prioritize language similarity over structural similarity, which can lead to overlooking entity relationships. To address this, we propose an AMR-enhanced retrieval-based ICL method for RE. Our model retrieves in-context examples based on semantic structure similarity between task inputs and training samples. Evaluations on four standard English RE datasets show that our model outperforms baselines in the unsupervised setting across all datasets. In the supervised setting, it achieves state-of-the-art results on three datasets and competitive results on the fourth.

</details>


### [54] [Multilingual Large Language Models and Curse of Multilinguality](https://arxiv.org/abs/2406.10602)

*Daniil Gurgurov, Tanja BÃ¤umel, Tatiana Anikina*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, Natural Language Processing, Model architectures, Pre-training data, Tokenization

**Relevance Score:** 8

**TL;DR:** An overview of multilingual Large Language Models (LLMs), discussing their architectures, training techniques, and limitations, particularly the challenges posed by multilinguality.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an introductory overview of multilingual LLMs and their technical details to NLP researchers and practitioners.

**Method:** The paper analyzes the underlying architectures of different model types (encoder-only, decoder-only, and encoder-decoder) and their related components such as pre-training data and tokenization methods.

**Key Contributions:** Introductory overview of multilingual LLM architectures and characteristics., Discussion of the curse of multilinguality and ongoing efforts to mitigate its effects., Detailed analysis of various model types and their respective methodologies.

**Result:** The exploration of multilingual LLMs reveals their strengths in various languages and tasks, while also highlighting the challenge of managing the curse of multilinguality.

**Limitations:** The paper mainly focuses on the technical aspects and may not delve into application-specific use cases or practical implementation.

**Future Work:** Future research directions include exploring more effective strategies for addressing multilinguality and improving the training of multilingual models for better performance.

**Conclusion:** Addressing the limitations and exploring current solutions to the challenges faced by multilingual LLMs could enhance their effectiveness in diverse applications.

**Abstract:** Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.

</details>


### [55] [Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation](https://arxiv.org/abs/2408.06276)

*Jieyong Kim, Hyunseo Kim, Hyunjin Cho, SeongKu Kang, Buru Chang, Jinyoung Yeo, Dongha Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Recommendation Systems, Explainability, User Preferences, Rating Prediction

**Relevance Score:** 9

**TL;DR:** The paper introduces EXP3RT, an LLM-based recommender that enhances recommendation systems by leveraging user and item review information to improve rating prediction and explainability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current recommendation systems have not fully utilized the capabilities of Large Language Models, often limited by input information and reasoning capabilities.

**Method:** EXP3RT is fine-tuned from a teacher LLM to perform three tasks: extracting subjective preferences from reviews, creating user and item profiles, and generating reasoning followed by predicted ratings.

**Key Contributions:** Introduction of EXP3RT, a novel LLM-based recommender system., Improved accuracy in rating prediction and item reranking through enhanced reasoning., Increased explainability in recommendations based on user and item profiles.

**Result:** EXP3RT outperforms existing methods in rating prediction and item reranking while improving explainability in recommendation systems.

**Limitations:** 

**Future Work:** Future research may explore further enhancements in recommendation accuracy and additional applications of LLMs in various domains.

**Conclusion:** The personalized reasoning approach of EXP3RT enhances recommendation accuracy and provides clear explanations for decisions made by the system.

**Abstract:** Recent advancements in Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, generating significant interest in their application to recommendation systems. However, existing methods have not fully capitalized on the potential of LLMs, often constrained by limited input information or failing to fully utilize their advanced reasoning capabilities. To address these limitations, we introduce EXP3RT, a novel LLM-based recommender designed to leverage rich preference information contained in user and item reviews. EXP3RT is basically fine-tuned through distillation from a teacher LLM to perform three key tasks in order: EXP3RT first extracts and encapsulates essential subjective preferences from raw reviews, aggregates and summarizes them according to specific criteria to create user and item profiles. It then generates detailed step-by-step reasoning followed by predicted rating, i.e., reasoning-enhanced rating prediction, by considering both subjective and objective information from user/item profiles and item descriptions. This personalized preference reasoning from EXP3RT enhances rating prediction accuracy and also provides faithful and reasonable explanations for recommendation. Extensive experiments show that EXP3RT outperforms existing methods on both rating prediction and candidate item reranking for top-k recommendation, while significantly enhancing the explainability of recommendation systems.

</details>


### [56] [Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings](https://arxiv.org/abs/2408.16073)

*Leo Yeykelis, Kaavya Pichai, James J. Cummings, Byron Reeves*

**Main category:** cs.CL

**Keywords:** Large Language Models, Replication Research, Marketing Psychology, AI in Social Science, Message Testing

**Relevance Score:** 8

**TL;DR:** This report examines how large language models (LLMs) can replicate and generalize published marketing research findings, showing a 76% success rate in replicating original experimental results.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the capability of large language models (LLMs) in replicating and generalizing marketing research findings to address the replication and generalizability crises in social science.

**Method:** The study utilized LLM-powered personas to replicate 133 findings from 14 marketing papers, generating prompts based on original study specifications to create datasets and perform statistical analyses.

**Key Contributions:** Demonstrated the potential of LLMs in research replication and generalization., Provided empirical evidence supporting LLMs for rapid message testing in consumer marketing., Addressed implications for marketing psychology and replicability challenges in social science.

**Result:** LLM replications reproduced 76% of the original main effects and 68% of overall replication rates, indicating strong potential for AI-assisted replication in marketing research.

**Limitations:** Limitations include challenges with complex interaction effects, biases present in AI models, and the need for establishing benchmarks for AI metrics in marketing research.

**Future Work:** Future research should explore refining AI models for better accuracy in complex interactions and expanding applications to other fields of social science.

**Conclusion:** The findings suggest that LLMs can be effective tools in accelerating message testing and theory building in marketing psychology, although limitations exist regarding complex effects and biases.

**Abstract:** This report analyzes the potential for large language models (LLMs) to expedite accurate replication and generalization of published research about message effects in marketing. LLM-powered participants (personas) were tested by replicating 133 experimental findings from 14 papers containing 45 recent studies published in the Journal of Marketing. For each study, the measures, stimuli, and sampling specifications were used to generate prompts for LLMs to act as unique personas. The AI personas, 19,447 in total across all of the studies, generated complete datasets and statistical analyses were then compared with the original human study results. The LLM replications successfully reproduced 76% of the original main effects (84 out of 111), demonstrating strong potential for AI-assisted replication. The overall replication rate including interaction effects was 68% (90 out of 133). Furthermore, a test of how human results generalized to different participant samples, media stimuli, and measures showed that replication results can change when tests go beyond the parameters of the original human studies. Implications are discussed for the replication and generalizability crises in social science, the acceleration of theory building in media and marketing psychology, and the practical advantages of rapid message testing for consumer products. Limitations of AI replications are addressed with respect to complex interaction effects, biases in AI models, and establishing benchmarks for AI metrics in marketing research.

</details>


### [57] [Your Weak LLM is Secretly a Strong Teacher for Alignment](https://arxiv.org/abs/2409.08813)

*Leitian Tao, Yixuan Li*

**Main category:** cs.CL

**Keywords:** large language models, model alignment, feedback generation, human values, weak LLMs

**Relevance Score:** 9

**TL;DR:** This paper studies the use of weak large language models (LLMs) for feedback in alignment tasks, demonstrating their efficacy in generating high-quality feedback comparable to human annotation at lower resource costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high resource demands of traditional alignment frameworks for large language models while ensuring they align with human values and intentions.

**Method:** The authors conducted a systematic study involving qualitative and quantitative analyses to evaluate the feedback generated by weak LLMs compared to fully human-annotated data.

**Key Contributions:** Demonstrated the effectiveness of weak LLMs for alignment feedback., Provided insights into the quality of feedback relative to human feedback., Established a framework that reduces the need for extensive human effort and computational resources.

**Result:** Empirical findings show that feedback from weak LLMs can match or surpass the quality of human feedback, with limited dependence on model size affecting efficacy.

**Limitations:** The study focuses solely on weak LLMs and does not explore other potential alignment methods.

**Future Work:** Further research could investigate the integration of weak LLMs with other techniques and the long-term implications of using such models for ongoing alignment processes.

**Conclusion:** Weak LLMs offer a scalable and sustainable approach to model alignment, mitigating resource constraints associated with traditional methods.

**Abstract:** The burgeoning capabilities of large language models (LLMs) have underscored the need for alignment to ensure these models act in accordance with human values and intentions. Existing alignment frameworks present constraints either in the form of expensive human effort or high computational costs. This paper explores a promising middle ground, where we employ a weak LLM that is significantly less resource-intensive than top-tier models, yet offers more automation than purely human feedback. We present a systematic study to evaluate and understand weak LLM's ability to generate feedback for alignment. Our empirical findings demonstrate that weak LLMs can provide feedback that rivals or even exceeds that of fully human-annotated data. Our study indicates a minimized impact of model size on feedback efficacy, shedding light on a scalable and sustainable alignment strategy. To deepen our understanding of alignment under weak LLM feedback, we conduct a series of qualitative and quantitative analyses, offering novel insights into the quality discrepancies between human feedback vs. weak LLM feedback.

</details>


### [58] [MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning](https://arxiv.org/abs/2409.12059)

*Ningyuan Xi, Xiaoyu Wang, Yetao Wu, Teng Chen, Qingqing Gu, Yue Zhao, Jinxian Qu, Zhonglin Jiang, Yong Chen, Luo Ji*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cognitive Mechanism, Thinking Layer, Natural Language Processing, Reasoning

**Relevance Score:** 9

**TL;DR:** This paper presents a novel model architecture called TaS that enhances Large Language Models by integrating a thinking layer to improve reasoning in responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of Large Language Models in reasoning and thorough thinking, proposing a data-driven approach inspired by cognitive mechanisms.

**Method:** The authors designed a model architecture called TaS, which incorporates a middle layer that functions as a thinking layer, allowing for the generation of thought contents based on prompt-response samples.

**Key Contributions:** Introduction of the TaS model architecture, Integration of a thinking layer into language models, Empirical validation of enhanced reasoning capabilities

**Result:** The studies show that TaS can successfully generate reasonable thoughts and produce improved responses, supported by qualitative and quantitative validation.

**Limitations:** The model's effectiveness may vary based on the types of queries and the dataset used for training.

**Future Work:** Exploration of broader applications of the TaS model and potential improvements in reasoning techniques for language models.

**Conclusion:** TaS enhances the reasoning capabilities of language models, leading to better quality responses, with the implementation details and code made available for further research.

**Abstract:** Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE.

</details>


### [59] [FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"](https://arxiv.org/abs/2410.03727)

*Yifei Ming, Senthil Purushwalkam, Shrey Pandit, Zixuan Ke, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty*

**Main category:** cs.CL

**Keywords:** faithfulness, large language models, evaluation benchmark, contextual fidelity, retrieval-augmented generation

**Relevance Score:** 9

**TL;DR:** The paper introduces FaithEval, a benchmark for evaluating faithfulness in LLMs, revealing that even the best models often struggle with context fidelity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of faithfulness in LLMs and RAG systems, which is critical for user trust and effective real-world deployment.

**Method:** The paper presents FaithEval, a benchmark comprising three tasks (unanswerable, inconsistent, and counterfactual contexts) with 4.9K problems validated through a four-stage construction and evaluation process.

**Key Contributions:** Introduction of FaithEval benchmark for evaluating contextual faithfulness in LLMs, Identification of significant faithfulness challenges in state-of-the-art models, Development of a rigorous validation framework for benchmarks.

**Result:** The study found that state-of-the-art models frequently fail to maintain faithfulness to context, and larger models do not necessarily lead to better performance in this regard.

**Limitations:** The benchmark may not cover all possible contextual scenarios relevant to real-world applications.

**Future Work:** Further refinement of the benchmark and exploration of methods to enhance the faithfulness of LLMs.

**Conclusion:** The FaithEval benchmark can help evaluate and improve the faithfulness of LLMs in real-world applications.

**Abstract:** Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness.Project is available at: https://github.com/SalesforceAIResearch/FaithEval.

</details>


### [60] [Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models](https://arxiv.org/abs/2411.07611)

*Shuai Niu, Jing Ma, Hongzhan Lin, Liang Bai, Zhihua Wang, Yida Xu, Yunya Song, Xian Yang*

**Main category:** cs.CL

**Keywords:** disease diagnosis, rationale generation, multimodal reasoning, small language models, large language models

**Relevance Score:** 9

**TL;DR:** ClinRaGen enhances small language models by integrating large language model reasoning and domain knowledge for improved disease diagnosis interpretability.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The need to balance predictive accuracy with human-understandable rationales in disease diagnosis.

**Method:** ClinRaGen utilizes a sequential rationale distillation framework to enhance small language models' reasoning abilities, alongside a knowledge-augmented attention mechanism for multimodal data integration.

**Key Contributions:** Proposes ClinRaGen for enhanced rationale generation in disease diagnosis., Introduces a framework for distilling reasoning from LLMs to SLMs., Develops a knowledge-augmented attention mechanism for multimodal representation.

**Result:** ClinRaGen demonstrates state-of-the-art performance in disease diagnosis and rationale generation across real-world medical datasets.

**Limitations:** Limited to the current available medical datasets; generalization to other domains not tested.

**Future Work:** Exploration of broader applications of ClinRaGen in varied medical contexts and other domains.

**Conclusion:** ClinRaGen effectively combines LLM-driven reasoning and knowledge augmentation to improve interpretability in medical settings.

**Abstract:** Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable mutlimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in a same encoding space, enabling it naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.

</details>


### [61] [ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data](https://arxiv.org/abs/2412.11704)

*Atsuki Yamaguchi, Terufumi Morishita, Aline Villavicencio, Nikolaos Aletras*

**Main category:** cs.CL

**Keywords:** chat models, language adaptation, large language models, machine translation, instruction following

**Relevance Score:** 8

**TL;DR:** ElChat is a method for adapting chat LLMs directly on target unlabeled data, enhancing language and instruction-following abilities without a base model.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in adapting chat LLMs under low-resource language settings where direct access to target chat data is often unavailable or too costly.

**Method:** ElChat directly adapts a chat model on target unlabeled data by injecting information from an existing source chat model, eliminating the need for a base model.

**Key Contributions:** Introduces a novel method for direct adaptation of chat models on unlabeled data without a base model., Demonstrates enhanced performance in target language adaptation and safety measures., Shows significant improvements in English and instruction-following capabilities.

**Result:** ElChat shows improved target language performance and safety, as well as superior English, chat, and instruction-following abilities compared to previous methods that relied on a chat vector from base models.

**Limitations:** 

**Future Work:** Exploration of further optimizations for ElChat and its applicability to additional low-resource languages.

**Conclusion:** ElChat provides a more effective alternative for adapting chat models in low-resource scenarios without necessitating a base model.

**Abstract:** Vocabulary expansion (VE) is the de-facto approach to language adaptation of large language models (LLMs) by adding new tokens and continuing pre-training on target data. While this is effective for base models trained on unlabeled data, it poses challenges for chat models trained to follow instructions through labeled conversation data. Directly adapting the latter with VE on target unlabeled data may result in forgetting chat abilities. While ideal, target chat data is often unavailable or costly to create for low-resource languages, and machine-translated alternatives are not always effective. To address this issue, previous work proposed using a base and chat model from the same family. This method first adapts the base LLM with VE on target unlabeled data and then converts it to a chat model by adding a chat vector (CV) derived from the weight difference between the source base and chat models. We propose ElChat, a new language adaptation method for chat LLMs that adapts a chat model directly on target unlabeled data, without a base model. It elicits chat abilities by injecting information from the source chat model. ElChat offers more robust and competitive target language and safety performance while achieving superior English, chat, and instruction-following abilities compared to CV.

</details>


### [62] [Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations](https://arxiv.org/abs/2502.01220)

*Hichem Ammar Khodja, FrÃ©dÃ©ric BÃ©chet, Quentin Brabant, Alexis Nasr, GwÃ©nolÃ© LecorvÃ©*

**Main category:** cs.CL

**Keywords:** temporal context, language models, factual knowledge, TimeStress dataset, machine learning

**Relevance Score:** 8

**TL;DR:** This paper evaluates the robustness of language models in correctly associating temporal contexts with factual knowledge using the newly introduced TimeStress dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how language models associate temporal contexts with past facts and to identify their limitations in handling temporal representations.

**Method:** The study introduces a dataset called TimeStress to analyze 18 diverse language models based on their ability to differentiate correct from incorrect temporal contexts related to factual knowledge.

**Key Contributions:** Introduction of the TimeStress dataset for analyzing temporal context in language models., Evaluation of 18 diverse language models on their accuracy with temporal knowledge., Highlighting significant limitations in language models' abilities compared to human understanding.

**Result:** The best-performing language model achieves perfect accuracy on only 6% of the studied facts, demonstrating significant limitations compared to human performance.

**Limitations:** The study primarily focuses on language models' performance in temporal context associations without offering insights into improvements for these models.

**Future Work:** Further research directions include improving the temporal representation capabilities of language models and expanding the dataset for broader evaluation.

**Conclusion:** The findings indicate critical errors in current language models' handling of temporal information, underscoring the need for further research in this area.

**Abstract:** This paper explores the robustness of language models (LMs) to variations in the temporal context within factual knowledge. It examines whether LMs can correctly associate a temporal context with a past fact valid over a defined period, by asking them to differentiate correct from incorrect contexts. The accuracy of LMs is analyzed along two dimensions: the distance of the incorrect context from the validity period and the granularity of the context. To this end, a dataset called TimeStress is introduced, enabling the evaluation of 18 diverse LMs. Results reveal that the best LM achieves perfect accuracy for only 6% of the studied facts, with critical errors that humans would not make. This work highlights the limitations of current LMs in temporal representation. We provide all data and code for further research.

</details>


### [63] [EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning](https://arxiv.org/abs/2502.12486)

*Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, strategic reasoning, reinforcement learning, policy optimization, goal-directed behavior

**Relevance Score:** 8

**TL;DR:** Proposes explicit policy optimization (EPO) for improving strategic reasoning in Large Language Models, focusing on dynamic environments and long-term goal alignment.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the strategic reasoning capabilities of LLMs in complex real-world scenarios, addressing challenges in adaptability and scalability.

**Method:** The model is trained using multi-turn reinforcement learning with process rewards and iterative self-play, avoiding supervised fine-tuning.

**Key Contributions:** Introduction of explicit policy optimization for LLMs, Improvement in strategic reasoning through multi-turn reinforcement learning, State-of-the-art performance in real-world tasks without supervised fine-tuning

**Result:** EPO achieves state-of-the-art performance in social dialogue and web navigation tasks, showing effective long-term goal alignment.

**Limitations:** 

**Future Work:** Exploration of further applications in various dynamic environments and enhancements in adaptability.

**Conclusion:** EPO demonstrates improved strategic reasoning and collaborative mechanisms, indicating its potential for real-world applications.

**Abstract:** Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.

</details>


### [64] [Machine-generated text detection prevents language model collapse](https://arxiv.org/abs/2502.15654)

*George Drayson, Emine Yilmaz, Vasileios Lampos*

**Main category:** cs.CL

**Keywords:** Large Language Models, model collapse, decoding strategies, machine-generated text detection, text generation

**Relevance Score:** 8

**TL;DR:** The paper investigates the impact of decoding strategies on model collapse in Large Language Models (LLMs) and proposes a method to alleviate this issue through a machine-generated text detector.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of LLMs and their outputs risks diluting human-authored content, potentially leading to model collapse where errors are reinforced and performance declines.

**Method:** The study analyzes the impact of different decoding strategies on model generation, measuring text characteristics, similarity to human references, and model performance. It also proposes a machine-generated text detector with importance sampling to mitigate model collapse.

**Key Contributions:** Identification of the impact of decoding strategies on model collapse in LLMs., Development of a machine-generated text detector to address model collapse., Validation of approach on GPT-2 and SmolLM2, showing improved performance.

**Result:** The approach proves effective in preventing model collapse and enhances performance when adequate human-generated samples are available, validated through experiments with GPT-2 and SmolLM2.

**Limitations:** Potential limitations in generalizability to all types of LLMs and scenarios.

**Future Work:** Future research could explore broader applications of machine-generated text detection and refinement of decoding strategies across different models.

**Conclusion:** Implementing an importance sampling strategy can significantly improve the reliability of LLM outputs and counteract the challenges posed by model collapse.

**Abstract:** As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the characteristics of text at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present. We release our code at https://github.com/GeorgeDrayson/model_collapse.

</details>


### [65] [LRAGE: Legal Retrieval Augmented Generation Evaluation Tool](https://arxiv.org/abs/2504.01840)

*Minhu Park, Hongseok Oh, Eunkyung Choi, Wonseok Hwang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Legal Domain, Evaluation Tool, Large Language Models, Open-source

**Relevance Score:** 8

**TL;DR:** LRAGE is an open-source tool designed for the holistic evaluation of RAG systems in the legal domain, examining the impacts of various components on accuracy.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need to evaluate RAG systems in the legal domain due to the importance of previous judicial decisions in legal contexts.

**Method:** LRAGE provides both GUI and CLI interfaces for conducting experiments, facilitating the evaluation of five components of RAG systems: retrieval corpora, retrieval algorithms, rerankers, LLM backbones, and evaluation metrics.

**Key Contributions:** Introduction of LRAGE, a comprehensive evaluation tool for RAG systems in legal contexts., Validation of the tool with multilingual legal datasets to demonstrate its effectiveness., Facilitation of seamless experimentation through GUI and CLI interfaces.

**Result:** Validation of LRAGE was done using multilingual legal benches (KBL, LegalBench, LawBench) to show how accuracy varies with changes in the five evaluated components.

**Limitations:** 

**Future Work:** Future improvements may include expanding the tool's capabilities to other domains beyond law and enhancing the user experience.

**Conclusion:** LRAGE significantly aids in assessing the effectiveness of various components in retrieval-augmented generation systems for legal applications.

**Abstract:** Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at https://github.com/hoorangyee/LRAGE.

</details>


### [66] [Generative Evaluation of Complex Reasoning in Large Language Models](https://arxiv.org/abs/2504.02810)

*Haowei Lin, Xiangyu Wang, Ruilin Yan, Baizhou Huang, Haotian Ye, Jianhua Zhu, Zihao Wang, James Zou, Jianzhu Ma, Yitao Liang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Assessment, Generative Evaluation Framework, KUMO, Symbolic Engines

**Relevance Score:** 9

**TL;DR:** KUMO is a generative evaluation framework for assessing reasoning in LLMs, showing that many LLMs outperform university students on reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if LLMs genuinely reason or simply recall information from training data, highlighting the need for reliable assessment benchmarks.

**Method:** KUMO combines LLMs with symbolic engines to create multi-turn reasoning tasks that adapt in difficulty, producing 5,000 tasks across 100 domains.

**Key Contributions:** Introduction of KUMO as a tool for LLM reasoning assessment., Generation of diverse reasoning tasks that adapt in difficulty., Benchmarking LLMs against university-level performance.

**Result:** Evaluation of 23 LLMs indicated many outperformed university students on easier tasks, with reasoning-scaled LLMs achieving university-level performance on complex challenges.

**Limitations:** The framework may still encounter biases in task generation and assessment, depending on the LLMs used.

**Future Work:** Explore further refinements of KUMO to reduce bias and expand the types of reasoning tasks generated.

**Conclusion:** KUMO serves as a valuable tool for assessing the genuine reasoning capabilities of LLMs, showing a strong correlation with real-world reasoning benchmarks.

**Abstract:** With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.

</details>


### [67] [Can Reasoning LLMs Enhance Clinical Document Classification?](https://arxiv.org/abs/2504.08040)

*Akram Mustafa, Usman Naseem, Mostafa Rahimi Azghadi*

**Main category:** cs.CL

**Keywords:** Clinical Document Classification, Large Language Models, ICD-10, Medical Text, Health Informatics

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of eight Large Language Models (LLMs) in classifying clinical discharge summaries into ICD-10 diagnoses, revealing a trade-off between accuracy and consistency.

**Read time:** 27 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate classification of clinical documents into standardized ICD-10 diagnoses due to challenges posed by complex medical language and limited datasets.

**Method:** The performance of four reasoning and four non-reasoning LLMs was assessed using the MIMIC-IV dataset, employing cTAKES to structure clinical narratives and majority voting for predictions across three experimental runs.

**Key Contributions:** Evaluation of performance of eight LLMs for clinical document classification, Demonstration of a trade-off between accuracy and consistency, Recommendations for future research directions including multi-label classification and ensemble methods

**Result:** Reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with the best performance by Gemini 2.0 Flash Thinking, yet non-reasoning models showed higher consistency (91% vs 84%).

**Limitations:** The performance varied significantly across different ICD-10 codes, particularly with reasoning models struggling with abstract categories.

**Future Work:** Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in clinical applications.

**Conclusion:** There is a critical trade-off between accuracy and consistency in model performance suggesting a hybrid approach might be more effective for clinical coding tasks.

**Abstract:** Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications.

</details>


### [68] [BitNet b1.58 2B4T Technical Report](https://arxiv.org/abs/2504.12285)

*Shuming Ma, Hongyu Wang, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting Song, Yan Xia, Furu Wei*

**Main category:** cs.CL

**Keywords:** Large Language Model, 1-bit model, open-source, computational efficiency, Hugging Face

**Relevance Score:** 8

**TL;DR:** Introduction of BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at 2 billion parameters, achieving competitive performance with lower resource requirements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a computationally efficient LLM that minimizes memory usage and energy consumption while maintaining performance comparable to leading models.

**Method:** BitNet b1.58 2B4T was trained on a 4 trillion token corpus and evaluated on language understanding, reasoning, coding, and conversational benchmarks.

**Key Contributions:** First open-source, native 1-bit LLM at 2 billion parameters., Significant improvements in computational efficiency and reduced resource requirements., Model weights and inference implementations made publicly available.

**Result:** BitNet b1.58 2B4T achieves performance similar to full-precision LLMs with significant reductions in memory, energy, and decoding latency.

**Limitations:** Details on the specific methodologies and comprehensive evaluations of the model's limitations are still in progress revealing the work's ongoing status.

**Future Work:** Encouragement for continued research on optimizing 1-bit model architectures and exploring various applications in real-world scenarios.

**Conclusion:** BitNet b1.58 2B4T offers an efficient alternative for LLM deployment, facilitating easier research and application through open-source distribution.

**Abstract:** We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.

</details>


### [69] [A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs](https://arxiv.org/abs/2504.14657)

*Yihan Lin, Zhirong Bella Yu, Simon Lee*

**Main category:** cs.CL

**Keywords:** Synthetic EHRs, Large Language Models, Healthcare, Data Generation, Privacy Preservation

**Relevance Score:** 9

**TL;DR:** This paper evaluates the capability of commercial LLMs in generating synthetic electronic health records (EHRs), highlighting their strengths and limitations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of synthetic EHRs in healthcare and the role of LLMs in generating such data without compromising real individuals' privacy.

**Method:** The paper assesses the performance of various commercial LLMs in generating synthetic health records, with a focus on the generalizability across different hospitals.

**Key Contributions:** Evaluation of LLMs for synthetic EHR generation, Identification of strengths and weaknesses in data generation, Insights into the challenges of dimensionality in healthcare data

**Result:** LLMs can generate reliable synthetic health records for smaller subsets, but they struggle with preserving realistic distributions and correlations in higher-dimensional data.

**Limitations:** The study is limited by its focus on commercial LLMs and may not account for all factors affecting synthetic data quality.

**Future Work:** Further research is needed to develop methods that enhance the generalization capabilities of synthetic health records across various hospital environments.

**Conclusion:** There is a need for improved strategies to ensure that synthetic health records can generalize better across diverse healthcare settings.

**Abstract:** Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to create privacy preserving and harmonized structured data, supporting numerous applications in healthcare. Key benefits of synthetic data include precise control over the data schema, improved fairness and representation of patient populations, and the ability to share datasets without concerns about compromising real individuals privacy. Consequently, the AI community has increasingly turned to Large Language Models (LLMs) to generate synthetic data across various domains. However, a significant challenge in healthcare is ensuring that synthetic health records reliably generalize across different hospitals, a long standing issue in the field. In this work, we evaluate the current state of commercial LLMs for generating synthetic data and investigate multiple aspects of the generation process to identify areas where these models excel and where they fall short. Our main finding from this work is that while LLMs can reliably generate synthetic health records for smaller subsets of features, they struggle to preserve realistic distributions and correlations as the dimensionality of the data increases, ultimately limiting their ability to generalize across diverse hospital settings.

</details>


### [70] [Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model](https://arxiv.org/abs/2504.15843)

*Junshu Pan, Wei Shen, Shulin Huang, Qiji Zhou, Yue Zhang*

**Main category:** cs.CL

**Keywords:** Direct Preference Optimization, reinforcement learning, human feedback, large language models, training paradigm

**Relevance Score:** 9

**TL;DR:** Pre-DPO enhances Direct Preference Optimization (DPO) by using a guiding reference model to improve performance in reinforcement learning from human feedback for LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to optimize human preferences in reinforcement learning without a reward model while overcoming inefficiencies in the initialization of policy and reference models.

**Method:** Proposes a new training paradigm called Pre-DPO that utilizes a reference model to improve weight assignment during training, guiding the model to focus on more suitable samples.

**Key Contributions:** Introduction of Pre-DPO, a novel training paradigm for LLMs., Demonstrating the effectiveness of using a guiding reference model in optimizing training data weights., Showing consistent improvement in benchmark performance over existing methods.

**Result:** Pre-DPO consistently improves performance on benchmarks like AlpacaEval 2.0 and Arena-Hard v0.1 compared to traditional DPO and Simple Preference Optimization without requiring extra models or data.

**Limitations:** Potential dependence on the quality and training of the reference model, which may affect performance.

**Future Work:** Investigate further modifications to the Pre-DPO framework and explore its applicability across different model architectures and data types.

**Conclusion:** Pre-DPO provides a robust solution for preference optimization in LLMs by enhancing training performance and data utilization.

**Abstract:** Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.

</details>


### [71] [CAPO: Cost-Aware Prompt Optimization](https://arxiv.org/abs/2504.16005)

*Tom Zehle, Moritz Schlager, Timo HeiÃ, Matthias Feurer*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Optimization, AutoML Techniques, Evolutionary Approach, Cost-Efficiency

**Relevance Score:** 8

**TL;DR:** CAPO is an algorithm for cost-aware prompt optimization in large language models, improving efficiency by integrating AutoML techniques and outperforming existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The performance of large language models is sensitive to prompt formulation, and current automated methods for prompt optimization are expensive in terms of LLM calls and input tokens.

**Method:** CAPO combines evolutionary techniques with LLMs, utilizing racing to reduce evaluations and multi-objective optimization to optimize prompt performance and length simultaneously.

**Key Contributions:** Introduction of CAPO, an efficient prompt optimization algorithm., Incorporation of AutoML techniques to reduce costs without sacrificing performance., Demonstrated improvements across diverse datasets and better overall robustness compared to existing methods.

**Result:** CAPO outperformed state-of-the-art discrete prompt optimization methods in 11 out of 15 cases, with performance improvements of up to 21% while being more cost-efficient.

**Limitations:** The paper does not deeply investigate the impact of various initial prompt forms on final performance.

**Future Work:** Future work could explore further refinements in prompt optimization techniques and evaluate CAPO across additional tasks and scenarios.

**Conclusion:** CAPO enhances the accessibility and efficiency of prompt optimization for large language models by balancing performance with cost.

**Abstract:** Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.

</details>
