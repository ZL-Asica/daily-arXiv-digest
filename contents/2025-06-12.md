# 2025-06-12

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 15]

- [cs.CL](#cs.CL) [Total: 100]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT](https://arxiv.org/abs/2506.09089)

*Xia Li*

**Main category:** cs.HC

**Keywords:** ChatGPT, language teaching, oral expression, interactivity, AI in education

**Relevance Score:** 3

**TL;DR:** This paper explores the integration of ChatGPT in developing oral expression tasks for teaching Chinese as a foreign language, examining the dynamics of teacher-ChatGPT interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the teaching of oral expression in Chinese by using communicative tasks that encourage learner engagement.

**Method:** The paper analyzes the interactions between a teacher and ChatGPT while designing a course program, focusing on specific communicative tasks based on conflicts.

**Key Contributions:**

	1. Examination of teacher-ChatGPT interaction dynamics.
	2. Insights into using AI in language teaching contexts.
	3. Impact assessment of AI tools on course program development.

**Result:** The study highlights how ChatGPT can assist teachers in finalizing teaching programs and the nature of interactions that take place during this process.

**Limitations:** 

**Conclusion:** Integrating ChatGPT into course design can significantly impact the development of teaching strategies and learner participation.

**Abstract:** In developing the teaching program for a course in Oral Expression in Teaching Chinese as a Foreign Language at the university level, the teacher designs communicative tasks based on conflicts to encourage learners to engage in interactive dynamics and develop their oral interaction skills. During the design of these tasks, the teacher uses ChatGPT to assist in finalizing the program. This article aims to present the key characteristics of the interactions between the teacher and ChatGPT during this program development process, as well as to examine the use of ChatGPT and its impacts in this specific context.

</details>


### [2] [Real-Time Confidence Detection through Facial Expressions and Hand Gestures](https://arxiv.org/abs/2506.09153)

*Tanjil Hasan Sakib, Samia Jahan Mojumder, Rajan Das Gupta, Md Imrul Hasan Showmick, Md. Yeasin Rahat, Md. Jakir Hossen*

**Main category:** cs.HC

**Keywords:** face orientation recognition, virtual environments, media pipe, head tracking, user engagement

**Relevance Score:** 7

**TL;DR:** This paper presents a novel real-time face orientation recognition system utilizing the Media Pipe Face Mesh framework to enhance engagement in virtual environments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The growing demand for virtual interactions necessitates effective measures of participant engagement, attention, and interaction.

**Method:** The system employs the Media Pipe Face Mesh framework to track 3D facial landmarks and calculates Euler angles to determine real-time head orientation with high accuracy.

**Key Contributions:**

	1. First implementation of real-time head orientation detection using Euler angles in virtual environments.
	2. Integration of Media Pipe Face Mesh for accurate facial landmark tracking.
	3. Potential applications for improving user engagement in online assessments and meetings.

**Result:** The system achieves 90% accuracy in identifying head orientation at distances of up to four feet, significantly improving the monitoring of user interaction in virtual settings.

**Limitations:** 

**Conclusion:** The proposed method enhances the quality of virtual communication, fostering better understanding and interaction among participants in digital spaces.

**Abstract:** Real-time face orientation recognition is a cutting-edge technology meant to track and analyze facial movements in virtual environments such as online interviews, remote meetings, and virtual classrooms. As the demand for virtual interactions grows, it becomes increasingly important to measure participant engagement, attention, and overall interaction. This research presents a novel solution that leverages the Media Pipe Face Mesh framework to identify facial landmarks and extract geometric data for calculating Euler angles, which determine head orientation in real time. The system tracks 3D facial landmarks and uses this data to compute head movements with a focus on accuracy and responsiveness. By studying Euler angles, the system can identify a user's head orientation with an accuracy of 90\%, even at a distance of up to four feet. This capability offers significant enhancements for monitoring user interaction, allowing for more immersive and interactive virtual ex-periences. The proposed method shows its reliability in evaluating participant attentiveness during online assessments and meetings. Its application goes beyond engagement analysis, potentially providing a means for improving the quality of virtual communication, fostering better understanding between participants, and ensuring a higher level of interaction in digital spaces. This study offers a basis for future developments in enhancing virtual user experiences by integrating real-time facial tracking technologies, paving the way for more adaptive and interactive web-based platform.

</details>


### [3] [Show Me Your Best Side: Characteristics of User-Preferred Perspectives for 3D Graph Drawings](https://arxiv.org/abs/2506.09212)

*Lucas Joos, Gavin J. Mooney, Maximilian T. Fischer, Daniel A. Keim, Falk Schreiber, Helen C. Purchase, Karsten Klein*

**Main category:** cs.HC

**Keywords:** 3D visualization, User experience, Virtual reality, Graph aesthetics

**Relevance Score:** 4

**TL;DR:** The paper investigates user-preferred viewpoints in 3D graph visualizations through a controlled study in a virtual reality environment, revealing key factors influencing viewpoint preference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The importance of viewpoint selection in 3D graph visualizations and the lack of empirical evidence on user-preferred perspectives.

**Method:** A controlled study with 23 participants in a VR environment where users selected preferred and least preferred viewpoints for various graph layouts.

**Key Contributions:**

	1. Systematic investigation of user-preferred viewpoints in 3D graphs
	2. Introduction of new measures for viewpoint evaluation
	3. Publicly available dataset for future research

**Result:** User preferences correlated with classical 2D aesthetic criteria and new measures, leading to the identification of key indicators influencing viewpoint choice.

**Limitations:** 

**Conclusion:** The findings contribute to understanding viewpoint preference in 3D graph visualizations and provide a dataset for further research.

**Abstract:** The visual analysis of graphs in 3D has become increasingly popular, accelerated by the rise of immersive technology, such as augmented and virtual reality. Unlike 2D drawings, 3D graph layouts are highly viewpoint-dependent, making perspective selection critical for revealing structural and relational patterns. Despite its importance, there is limited empirical evidence guiding what constitutes an effective or preferred viewpoint from the user's perspective. In this paper, we present a systematic investigation into user-preferred viewpoints in 3D graph visualisations. We conducted a controlled study with 23 participants in a virtual reality environment, where users selected their most and least preferred viewpoints for 36 different graphs varying in size and layout. From this data, enriched by qualitative feedback, we distil common strategies underlying viewpoint choice. We further analyse the alignment of user preferences with classical 2D aesthetic criteria (e.g., Crossings), 3D-specific measures (e.g., Node-Node Occlusion), and introduce a novel measure capturing the perceivability of a graph's principal axes (Isometric Viewpoint Deviation). Our data-driven analysis indicates that Stress, Crossings, Gabriel Ratio, Edge-Node Overlap, and Isometric Viewpoint Deviation are key indicators of viewpoint preference. Beyond our findings, we contribute a publicly available dataset consisting of the graphs and computed aesthetic measures, supporting further research and the development of viewpoint evaluation measures for 3D graph drawing.

</details>


### [4] ["How do you even know that stuff?": Barriers to expertise sharing among spreadsheet users](https://arxiv.org/abs/2506.09216)

*Qing, Xia, Advait Sarkar, Duncan Brumby, Anna Cox*

**Main category:** cs.HC

**Keywords:** spreadsheet collaboration, social norms, knowledge sharing, user engagement, collaborative learning

**Relevance Score:** 5

**TL;DR:** This paper explores how social norms and beliefs affect knowledge sharing among spreadsheet users, revealing challenges in adapting personalized strategies and self-evaluations of expertise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the under-explored area of knowledge sharing in spreadsheet collaboration, which is crucial for retaining technical skillsets in organizations.

**Method:** The research involved conducting 31 semi-structured interviews with professional spreadsheet users from two different samples to understand their collaboration behaviors.

**Key Contributions:**

	1. Identified social norms affecting sharing behaviors in spreadsheet use.
	2. Highlighted challenges in adapting personalized sharing strategies and self-evaluation.
	3. Provided design implications to enhance collaborative learning in feature-rich software.

**Result:** Findings indicate that users struggle with sharing strategies due to personalized approaches, conflicted self-assessments of their skills, and dismissive views on the importance of collaborating, leading to a decrease in knowledge dissemination.

**Limitations:** 

**Conclusion:** The results highlight the interplay between technology design and social factors in promoting collaborative learning, suggesting that current software does not adequately support the long-term sharing of knowledge.

**Abstract:** Spreadsheet collaboration provides valuable opportunities for learning and expertise sharing between colleagues. Sharing expertise is essential for the retention of important technical skillsets within organisations, but previous studies suggest that spreadsheet experts often fail to disseminate their knowledge to others. We suggest that social norms and beliefs surrounding the value of spreadsheet use significantly influence user engagement in sharing behaviours. To explore this, we conducted 31 semi-structured interviews with professional spreadsheet users from two separate samples. We found that spreadsheet providers face challenges in adapting highly personalised strategies to often subjective standards and evaluating the appropriate social timing of sharing. In addition, conflicted self-evaluations of one's spreadsheet expertise, dismissive normative beliefs about the value of this knowledge, and concerns about the potential disruptions associated with collaboration can further deter sharing. We suggest these observations reflect the challenges of long-term learning in feature-rich software designed primarily with initial learnability in mind. We therefore provide implications for design to navigate this tension. Overall, our findings demonstrate how the complex interaction between technology design and social dynamics can shape collaborative learning behaviours in the context of feature-rich software.

</details>


### [5] [Beyond the Hype: Mapping Uncertainty and Gratification in AI Assistant Use](https://arxiv.org/abs/2506.09220)

*Karen Joy, Tawfiq Ammari, Alyssa Sheehan*

**Main category:** cs.HC

**Keywords:** AI personal assistants, user experience, Uncertainty Reduction Theory

**Relevance Score:** 8

**TL;DR:** The paper analyzes user experiences with emerging AI personal assistants, focusing on unmet expectations and the types of user uncertainty they face.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap between the expectations set by marketing and the actual performance of AI personal assistants by understanding user experiences.

**Method:** Conducted interviews with early adopters of various AI personal assistants and analyzed their experiences using Uses and Gratifications and Uncertainty Reduction Theory.

**Key Contributions:**

	1. Identified core types of user uncertainty in AI assistant usage.
	2. Proposed design and policy recommendations for ethical AI integration.
	3. Highlighted the importance of user control over contextual memory.

**Result:** Identified three types of user uncertainty (functional, interactional, and social) that lead to unmet expectations and subsequently frustration or abandonment of AI assistants.

**Limitations:** 

**Conclusion:** Recommendations for improving AI personal assistants include enhancing transparency, user control, and design that aligns with user needs and expectations.

**Abstract:** This paper examines the gap between the promises and real-world performance of emerging AI personal assistants. Drawing on interviews with early adopters of devices like Rabbit R1 and Humane AI Pin, as well as services like Ohai and Docus, we map user experiences through the lens of Uses and Gratifications and Uncertainty Reduction Theory. We identify three core types of user uncertainty, functional, interactional, and social, and explore how each disrupts different user gratifications. We show that while marketing hype fuels initial adoption, unmet expectations often result in frustration or abandonment. Our findings highlight the importance of transparency, task-specific design, and user control over contextual memory and personalization. We provide design and policy recommendations, including user-facing explainability tools and calls for regulatory benchmarks such as CI Bench, to guide ethical and interpretable AI integration. Our study offers actionable insights for creating more usable, trustworthy, and socially aligned AI assistants.

</details>


### [6] [Augmented Reality User Interfaces for First Responders: A Scoping Literature Review](https://arxiv.org/abs/2506.09236)

*Erin Argo, Tanim Ahmed, Sarah Gable, Callie Hampton, Jeronimo Grandi, Regis Kopper*

**Main category:** cs.HC

**Keywords:** Augmented Reality, User Interfaces, Public Safety, Emergency Services, First Responders

**Relevance Score:** 4

**TL;DR:** This paper reviews the application of AR user interfaces in public safety, analyzing current research and identifying trends, challenges, and gaps.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of AR user interfaces in public safety applications for first responders and to provide a comprehensive analysis of the current research landscape.

**Method:** A scoping review using a systematic review methodology, analyzing 1,751 publications to identify 90 relevant works, followed by an in-depth literature analysis to create a taxonomy.

**Key Contributions:**

	1. Comprehensive taxonomy of AR user interfaces for public safety
	2. Identification of key challenges and gaps in the research
	3. Insights for future research directions

**Result:** Development of a faceted taxonomy categorizing AR user interfaces for public safety and identification of design considerations, challenges, and literature gaps.

**Limitations:** Limited to publications indexed until April 2025, may not cover emerging trends post-review.

**Conclusion:** The review serves as a resource for researchers and developers, offering insights to drive future advances in AR applications in public safety.

**Abstract:** During the past decade, there has been a significant increase in research focused on integrating AR User Interfaces into public safety applications, particularly for first responders in the domains of Emergency Medical Services, Firefighting, and Law Enforcement. This paper presents the results of a scoping review involving the application of AR user interfaces in the public safety domain and applies an established systematic review methodology to provide a comprehensive analysis of the current research landscape, identifying key trends, challenges, and gaps in the literature. This review includes peer-reviewed publications indexed by the major scientific databases up to April 2025. A basic keyword search retrieved 1,751 papers, of which 90 were deemed relevant for this review. An in-depth analysis of the literature allowed the development of a faceted taxonomy that categorizes AR user interfaces for public safety. This classification lays a solid foundation for future research, while also highlighting key design considerations, challenges, and gaps in the literature. This review serves as a valuable resource for researchers and developers, offering insights that can drive further advances in the field.

</details>


### [7] [AI Tutors vs. Tenacious Myths: Evidence from Personalised Dialogue Interventions in Education](https://arxiv.org/abs/2506.09292)

*Brooklyn J. Corbett, Jason M. Tangen*

**Main category:** cs.HC

**Keywords:** AI dialogue, misconceptions, educational intervention, personalisation, belief correction

**Relevance Score:** 6

**TL;DR:** The study explores the effectiveness of personalised AI dialogue in correcting psychology misconceptions and finds it significantly reduces belief in false ideas compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Misconceptions in psychology and education persist despite evidence against them, creating a need for innovative correction methods.

**Method:** A preregistered experiment with 375 participants tested three interventions: personalised AI dialogue, generic textbook refutation, and neutral AI dialogue.

**Key Contributions:**

	1. Demonstrated the effectiveness of personalised AI dialogue in belief correction.
	2. Showed the motivational benefits of conversational interaction compared to traditional methods.
	3. Highlighted the need for reinforcement to sustain the effectiveness of interventions over time.

**Result:** Personalised AI dialogue significantly reduced misconceptions immediately and maintained engagement, but effects lessened at 2 months without reinforcement.

**Limitations:** The effects of personalised dialogue diminished over time, indicating that additional support is necessary for long-lasting change.

**Conclusion:** AI dialogue can effectively correct misconceptions but requires ongoing intervention for lasting change; future methods should include spaced reinforcement in educational frameworks.

**Abstract:** Misconceptions in psychology and education persist despite clear contradictory evidence, resisting traditional correction methods. This study investigated whether personalised AI dialogue could effectively correct these stubborn beliefs. In a preregistered experiment (N = 375), participants holding strong psychology misconceptions engaged in one of three interventions: (1) personalised AI dialogue targeting their specific misconception, (2) generic textbook-style refutation, or (3) neutral AI dialogue (control). Results showed that personalised AI dialogue produced significantly larger immediate belief reductions compared to both textbook reading and neutral dialogue. This advantage persisted at 10-day follow-up but diminished by 2 months, where AI dialogue and textbook conditions converged while both remained superior to control. Both AI conditions generated significantly higher engagement and confidence than textbook reading, demonstrating the motivational benefits of conversational interaction. These findings demonstrate that AI dialogue can accelerate initial belief correction through personalised, interactive engagement that disrupts the cognitive processes maintaining misconceptions. However, the convergence of effects over time suggests brief interventions require reinforcement for lasting change. Future applications should integrate AI tutoring into structured educational programs with spaced reinforcement to sustain the initial advantages of personalised dialogue.

</details>


### [8] ["Is This Really a Human Peer Supporter?": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions](https://arxiv.org/abs/2506.09354)

*Kellie Yu Hui Sim, Roy Ka-Wei Lee, Kenny Tsu Wei Choo*

**Main category:** cs.HC

**Keywords:** AI in mental health, peer support, Large Language Models, psychosocial support, training standards

**Relevance Score:** 9

**TL;DR:** This paper presents an AI-supported system that enhances peer support interactions for mental health through LLMs, evaluates its effectiveness, and emphasizes the need for better training standards in peer support.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Mental health is a critical global issue, and AI-driven solutions can improve access to psychosocial support. Peer support offers valuable assistance but faces quality and safety challenges.

**Method:** Two mixed-methods studies were conducted involving 12 peer supporters and 5 mental health professionals to evaluate the AI system's effectiveness and implications for practice.

**Key Contributions:**

	1. Development of an AI-supported peer support system leveraging LLMs
	2. Insights from evaluating the system's effectiveness in real-time interactions
	3. Identification of training gaps in current peer support practices

**Result:** Both peer supporters and mental health experts recognized the system's potential to enhance training and improve interaction quality, though experts noted critical issues in peer responses.

**Limitations:** Misalignment in responses between peer supporters and mental health experts, especially concerning distress cues and advice-giving.

**Conclusion:** The findings indicate a need for standardized training in peer support that aligns with best practices, highlighting LLMs' role in supporting this development under expert guidance.

**Abstract:** Mental health is a growing global concern, prompting interest in AI-driven solutions to expand access to psychosocial support. Peer support, grounded in lived experience, offers a valuable complement to professional care. However, variability in training, effectiveness, and definitions raises concerns about quality, consistency, and safety. Large Language Models (LLMs) present new opportunities to enhance peer support interactions, particularly in real-time, text-based interactions. We present and evaluate an AI-supported system with an LLM-simulated distressed client, context-sensitive LLM-generated suggestions, and real-time emotion visualisations. 2 mixed-methods studies with 12 peer supporters and 5 mental health professionals (i.e., experts) examined the system's effectiveness and implications for practice. Both groups recognised its potential to enhance training and improve interaction quality. However, we found a key tension emerged: while peer supporters engaged meaningfully, experts consistently flagged critical issues in peer supporter responses, such as missed distress cues and premature advice-giving. This misalignment highlights potential limitations in current peer support training, especially in emotionally charged contexts where safety and fidelity to best practices are essential. Our findings underscore the need for standardised, psychologically grounded training, especially as peer support scales globally. They also demonstrate how LLM-supported systems can scaffold this development--if designed with care and guided by expert oversight. This work contributes to emerging conversations on responsible AI integration in mental health and the evolving role of LLMs in augmenting peer-delivered care.

</details>


### [9] ["I Said Things I Needed to Hear Myself": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore](https://arxiv.org/abs/2506.09362)

*Kellie Yu Hui Sim, Kenny Tsu Wei Choo*

**Main category:** cs.HC

**Keywords:** peer support, mental health, digital platforms, AI, cultural responsiveness

**Relevance Score:** 8

**TL;DR:** This paper explores digital peer support for mental health in Singapore, highlighting the practices and needs of peer supporters and proposing design implications for AI-enhanced tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of digital platforms in peer support for mental health and provide design implications for culturally responsive technologies in Asian contexts.

**Method:** An interview study with 20 peer supporters in Singapore, utilizing thematic analysis to understand their practices and motivations.

**Key Contributions:**

	1. Insights into the lived realities of peer supporters in Singapore.
	2. Proposed design directions for AI-enhanced peer support tools.
	3. Thematic analysis of peer support practices and motivations.

**Result:** The study reveals the emotional labor and sociocultural factors affecting peer support practices, and offers design directions for AI tools that complement relational care.

**Limitations:** 

**Conclusion:** The research underscores the importance of culturally responsive design in digital mental health supports, emphasizing trust and context sensitivity in AI applications.

**Abstract:** Peer support plays a vital role in expanding access to mental health care by providing empathetic, community-based support outside formal clinical systems. As digital platforms increasingly mediate such support, the design and impact of these technologies remain under-examined, particularly in Asian contexts. This paper presents findings from an interview study with 20 peer supporters in Singapore, who operate across diverse online, offline, and hybrid environments. Through a thematic analysis, we unpack how participants start, conduct, and sustain peer support, highlighting their motivations, emotional labour, and the sociocultural dimensions shaping their practices. Building on this grounded understanding, we surface design directions for culturally responsive digital tools that scaffold rather than supplant relational care. Drawing insights from qualitative accounts, we offer a situated perspective on how AI might responsibly augment peer support. This research contributes to human-centred computing by articulating the lived realities of peer supporters and proposing design implications for trustworthy and context-sensitive AI in mental health.

</details>


### [10] [Patterns of Patterns III](https://arxiv.org/abs/2506.09696)

*Joseph Corneli, Charles J. Danoff, Raymond S. Puzio, Sridevi Ayloo, Serge Belich, Mary Tedeschi*

**Main category:** cs.HC

**Keywords:** PLACARD pattern, collaborative design, AI agents, institutional governance

**Relevance Score:** 6

**TL;DR:** This paper re-examines the PLACARD pattern through workshops and AI chatbots to enhance collaborative design and reflection.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To explore effective methods for supporting collaborative design and reflection using the PLACARD pattern and AI agents.

**Method:** The research involved organizing workshops to utilize the PLACARD pattern and compare outcomes with those from virtual workshops featuring AI-based chatbots.

**Key Contributions:**

	1. Re-evaluates the PLACARD pattern in collaborative design contexts.
	2. Compares human-facilitated workshops with AI chatbot-led sessions.
	3. Identifies limitations and offers insights for future AI and design integration.

**Result:** Findings reveal insights into collaborative reflection and design pattern generation, highlighting the effectiveness of PLACARD and the potential of AI chatbots.

**Limitations:** The study primarily reflects on workshops and may not encompass broader applications beyond the specific settings.

**Conclusion:** A future development strategy is proposed that merges AI agents with design patterns and governance.

**Abstract:** Building on earlier installments, this paper re-examines the PLACARD pattern. We report on a series of workshops where PLACARD was used to scaffold collaborative reflection, speculative inquiry, and stimulate design pattern generation. These accounts are enriched by a comparison case: virtual workshops carried out with simple AI-based chatbots. We discuss limitations and lessons learned from both the human and multi-agent settings. We conclude by outlining a future development strategy at the intersection of AI agents, design patterns, and institutional governance.

</details>


### [11] [Investigating the Perception of Translational Shape-Changing Haptic Interfaces](https://arxiv.org/abs/2506.09801)

*Qihan Yang, Xin Zhou, Adam J. Spiers*

**Main category:** cs.HC

**Keywords:** shape-changing haptic interfaces, perception evaluation, psychophysical study, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper investigates the perception of shape-changing haptic interfaces (SCHIs) through a psychophysical user study, evaluating the effects of grasp types and displacement on user experience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To formally evaluate the perception of dynamic shapes in SCHIs, an area with limited existing literature compared to established modalities like vibration.

**Method:** A psychophysical user study with a 1-DOF translational shape-changing interface, where participants completed a Method of Constant Stimulus study while using three different grasps.

**Key Contributions:**

	1. Initiation of formal evaluation on dynamic shape perception in SCHIs
	2. Comparison of grasp types and displacement effects on user perception
	3. Demonstration of improved user interaction through non-linear mapping in applications

**Result:** Results suggest maximizing translation magnitude over the number of fingers in contact. The non-linear mapping derived from the study improved user experience in a practical application compared to conventional linear mapping.

**Limitations:** 

**Conclusion:** The study's findings can inform the design of more effective shape-changing haptic interfaces and encourage further research into other SCHI morphologies.

**Abstract:** Shape-changing haptic interfaces (SCHIs) are a promising and emerging field. However, compared to more established stimulus modalities, such as vibration, there is sparse literature on the perception of dynamic shapes. Furthermore, the influence of properties such as grasp types and displacement magnitude/direction has not been formally evaluated. This work attempts to initiate a formal perceptual evaluation of SCHIs via a psychophysical user study involving a 1-DOF translational shape-changing interface that can move its body with 1.25-micrometer resolution. Participants completed a Method of Constant Stimulus study while holding the device with three different grasps. Stimuli direction occurred both toward and away from the thumb, while the standard stimuli varied between small (0.48 mm) and large (6 mm). Our results indicate that translational SCHIs should maximize the translation magnitude rather than the number of fingers in contact. We also demonstrated how to apply our findings to real-world applications via a simple 'paddle game', where we compared conventional linear mapping with non-linear mapping derived from our perceptual experiment outcomes between the device position and its represented value. Results indicate that the non-linear mapping was more effective, with improved error distribution. We hope this work inspires further formal perceptual investigation into other SCHI morphologies.

</details>


### [12] [SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification and LLM Assistance](https://arxiv.org/abs/2506.09968)

*Wentao Ge, Yuqing Sun, Ziyan Wang, Haoyue Zheng, Weiyang He, Piaohong Wang, Qianyu Zhu, Benyou Wang*

**Main category:** cs.HC

**Keywords:** self-regulated learning, LLM-assisted, gamification, educational technology, metacognitive skills

**Relevance Score:** 7

**TL;DR:** This paper presents SRLAgent, an LLM-assisted system designed to enhance self-regulated learning (SRL) skills in college students through gamification and real-time feedback.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges faced by college students in developing self-regulated learning skills, which are essential for academic success.

**Method:** A formative study involving 59 college students was conducted to identify challenges in SRL, leading to the development and evaluation of SRLAgent against baseline systems.

**Key Contributions:**

	1. Development of SRLAgent, an innovative LLM-assisted learning tool
	2. Demonstrated significant improvements in SRL capabilities among users
	3. Provided insights into gamification and AI support in educational contexts

**Result:** Significant improvements in SRL skills and higher engagement were observed in students using SRLAgent compared to traditional approaches.

**Limitations:** 

**Conclusion:** Integrating LLMs into gamified learning environments can effectively support the development of SRL skills and provide valuable design implications for educational technologies.

**Abstract:** Self-regulated learning (SRL) is crucial for college students navigating increased academic demands and independence. Insufficient SRL skills can lead to disorganized study habits, low motivation, and poor time management, undermining learners ability to thrive in challenging environments. Through a formative study involving 59 college students, we identified key challenges students face in developing SRL skills, including difficulties with goal-setting, time management, and reflective learning. To address these challenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL skills through gamification and adaptive support from large language models (LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables students to engage in goal-setting, strategy execution, and self-reflection within an interactive game-based environment. The system offers real-time feedback and scaffolding powered by LLMs to support students independent study efforts. We evaluated SRLAgent using a between-subjects design, comparing it to a baseline system (SRL without Agent features) and a traditional multimedia learning condition. Results showed significant improvements in SRL skills within the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement compared to the baselines. This work highlights the value of embedding SRL scaffolding and real-time AI support within gamified environments, offering design implications for educational technologies that aim to promote deeper learning and metacognitive skill development.

</details>


### [13] [Mixed Reality Tele-Ultrasound over 750 km: A Feasibility Study](https://arxiv.org/abs/2409.13058)

*Ryan Yeung, David Black, Patrick B. Chen, Victoria Lessoway, Janice Reid, Sergio Rangel-Suarez, Silvia D. Chang, Septimiu E. Salcudean*

**Main category:** cs.HC

**Keywords:** tele-ultrasound, mixed reality, haptic feedback, remote healthcare, feasibility study

**Relevance Score:** 7

**TL;DR:** This paper presents advancements in a mixed reality tele-ultrasound system that enables remote novice operators to perform abdominal ultrasound scans under expert supervision, demonstrating high image quality and positive user feedback.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve access to ultrasound in remote areas through teleoperation, leveraging mixed reality and haptic technologies for effective training and execution of ultrasound examinations.

**Method:** A mixed reality tele-ultrasound system was assessed in a feasibility study involving 10 novices and 2 sonographers, with remote experts guiding the process. The system included haptic feedback and was tested over a distance of 754 km.

**Key Contributions:**

	1. Development of a mixed reality and haptics-based tele-ultrasound system for remote operation
	2. Successful feasibility study demonstrating high image acquisition quality from novice operators
	3. User feedback indicating positive usability and low task load for the teleoperation system.

**Result:** 92% of the ultrasound images obtained had sufficient quality for interpretation by radiologists, with novices reporting low task load and high usability. No correlation was found between image quality and alignment error of the virtual transducer.

**Limitations:** The study was limited to a small number of scans and a specific geographical location, which may affect generalizability.

**Conclusion:** Human teleoperation can effectively enable novice sonographers to perform abdominal ultrasound imaging at a distance, presenting a viable solution for remote healthcare delivery.

**Abstract:** To address the lack of access to ultrasound in remote communities, previous work introduced human teleoperation, a mixed reality and haptics-based tele-ultrasound system. In this approach, a novice takes the role of a cognitive robot controlled remotely by an expert through mixed reality. In this manuscript we summarize new developments to this system and describe a feasibility study assessing its use for long-distance remote abdominal ultrasound examinations. To provide simple but effective haptic feedback, we used an ellipsoid model of the patient with its parameters calibrated using our system's position and force sensors. We tested the system in Skidegate, Haida Gwaii, Canada, with the experts positioned 754 km away in Vancouver, Canada. We performed 11 total scans with 10 novices and 2 sonographers. The sonographers were tasked with acquiring 5 target images in the epigastric region. The image acquisition quality was assessed by 2 radiologists. We collected alignment data and the novices completed task load and usability questionnaires. Both the novices and sonographers provided written and verbal feedback to inform future design iterations. 92% of the acquired images had sufficient quality for interpretation by both radiologists. The mean task load reported by the novices was below reference values reported in literature and the usability was unanimously positive. No correlation was found between image quality and the follower's alignment error with the virtual transducer. Overall, we show that human teleoperation enables sonographers to perform remote abdominal ultrasound imaging with high performance, even across large distances and with novice followers. Future work will compare human teleoperation to conventional, robotic and tele-mentored ultrasound.

</details>


### [14] [From Simple Sensors to Complex Context: Insights for HabiTech](https://arxiv.org/abs/2412.06085)

*Albrecht Kurze, Karola Köpferl*

**Main category:** cs.HC

**Keywords:** HabiTech, smart homes, participatory research, sensor data, community

**Relevance Score:** 7

**TL;DR:** This paper discusses the concept of HabiTech in smart homes, emphasizing the importance of context in data capture and interpretation, and suggesting a participatory approach to address stakeholder interests and privacy concerns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to explore how smart homes can leverage HabiTech and existing research to enhance community living through better data context and stakeholder involvement.

**Method:** The paper adopts a participatory research approach, examining the role of proximity in defining communities and addressing the complexities of interpreting sensor data in smart environments.

**Key Contributions:**

	1. Introduces the concept of HabiTech in relation to smart housing.
	2.  highlights the social and spatial dimensions of community for data context.
	3. Discusses privacy issues in shared data management.

**Result:** Findings indicate that understanding both spatial and social dimensions of community is critical for effective data interpretation in smart home contexts, while participatory approaches help in addressing the privacy concerns of shared data.

**Limitations:** The study may require further empirical validation and examines the challenges of diverse stakeholder engagement.

**Conclusion:** The paper concludes that integrating HabiTech with community-focused strategies can improve the management of sensor data in smart homes, balancing stakeholder interests and privacy.

**Abstract:** We relate our previous as well as ongoing research in the domain of smart homes to the concept of HabiTech. HabiTech can benefit from existing approaches and findings in a broader context of whole buildings or communities within. Along with data comes context of data capture and data interpretation in different dimensions (spatial, temporal, social). For defining what is 'community' proximity plays a crucial role in context, both spatially as well as socially. A participatory approach for research in living in sensing environments is promising to address complexity as well as interests of different stakeholders. Often it is the complex context that makes even simple sensor data sensitive, i.e. in terms of privacy. When it comes to handle shared data then concepts from the physical world for shared spaces might be related back to the data domain.

</details>


### [15] [Bridging the Gap Between LLMs and Human Intentions: Progresses and Challenges in Instruction Understanding, Intention Reasoning, and Reliable Generation](https://arxiv.org/abs/2502.09101)

*Zongyu Chang, Feihong Lu, Ziqin Zhu, Qian Li, Cheng Ji, Zhuo Chen, Hao Peng, Yang Liu, Ruifeng Xu, Yangqiu Song, Shangguang Wang, Jianxin Li*

**Main category:** cs.HC

**Keywords:** large language models, instruction understanding, intention reasoning, dialog generation, benchmarking

**Relevance Score:** 9

**TL;DR:** This paper discusses significant challenges faced by large language models (LLMs) in understanding and generating text according to human instructions, focusing on instruction understanding, intention reasoning, and reliable dialog generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in comprehending human instructions and improving their reliability in real-world applications.

**Method:** The paper classifies and analyzes the performance of LLMs in complex instruction scenarios, conducting a comprehensive evaluation of current solutions and introducing benchmarks based on core challenges identified.

**Key Contributions:**

	1. Identification of challenges in LLMs related to instruction understanding, intention reasoning, and dialog generation.
	2. Classification and analysis of LLM performance in challenging scenarios.
	3. Introduction of benchmarks for future research direction.

**Result:** The evaluation reveals deficiencies in LLMs regarding understanding long contexts, intention reasoning, and generating reliable dialog, highlighting the need for improved models.

**Limitations:** 

**Conclusion:** The paper calls for targeted research on enhancing LLMs' capabilities in real-world settings, particularly around instruction comprehension and dialog reliability.

**Abstract:** Large language models (LLMs) have demonstrated exceptional capabilities in understanding and generation. However, when interacting with human instructions in real-world scenarios, LLMs still face significant challenges, particularly in accurately capturing and comprehending human instructions and intentions. This paper focuses on three challenges in LLM-based text generation tasks: instruction understanding, intention reasoning, and Reliable Dialog Generation. Regarding human complex instruction, LLMs have deficiencies in understanding long contexts and instructions in multi-round conversations. For intention reasoning, LLMs may have inconsistent command reasoning, difficulty reasoning about commands containing incorrect information, difficulty understanding user ambiguous language commands, and a weak understanding of user intention in commands. Besides, In terms of Reliable Dialog Generation, LLMs may have unstable generated content and unethical generation. To this end, we classify and analyze the performance of LLMs in challenging scenarios and conduct a comprehensive evaluation of existing solutions. Furthermore, we introduce benchmarks and categorize them based on the aforementioned three core challenges. Finally, we explore potential directions for future research to enhance the reliability and adaptability of LLMs in real-world applications.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [16] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)

*Nadezhda Chirkova, Tunde Oluwaseyi Ajayi, Seth Aycock, Zain Muhammad Mujahid, Vladana Perlić, Ekaterina Borisova, Markarit Vartampetian*

**Main category:** cs.CL

**Keywords:** large language models, natural language generation, qualitative evaluation

**Relevance Score:** 9

**TL;DR:** The paper proposes an LLM-based evaluation method for natural language generation that provides qualitative insights into generated texts rather than just quantitative scores.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of natural language generation systems by providing developers with structured qualitative reports on issues found in generated texts.

**Method:** The proposed method involves two steps: open-ended issue analysis for each instance and clustering the identified issues using a cumulative algorithm, alongside a strategy for evaluating the approach.

**Key Contributions:**

	1. Introduced LLM-as-a-qualitative-judge for NLG evaluation
	2. Developed an intuitive cumulative algorithm for issue clustering
	3. Provided an evaluation strategy with annotated instances from multiple datasets.

**Result:** The approach successfully identifies instance-specific issues in approximately two-thirds of cases and produces issue reports similar to those generated by human annotators.

**Limitations:** 

**Conclusion:** The developed method offers a new way for developers to gain meaningful insights into the performance of NLG systems, fostering improvement through qualitative feedback.

**Abstract:** Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3 cases and is capable of producing error type reports resembling the reports composed by human annotators. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [17] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)

*Peidong Wang, Jian Xue, Rui Zhao, Junkun Chen, Aswin Shanmugam Subramanian, Jinyu Li*

**Main category:** cs.CL

**Keywords:** speech translation, phrase dictionary, multimodal large language models, translation biasing, phrase recall

**Relevance Score:** 7

**TL;DR:** The paper introduces a phrase dictionary biasing method that enhances speech translation by utilizing pairs of phrases from different languages, leading to significant improvements in translation performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Correct translation of phrases is challenging due to their infrequency in training data, which hampers speech translation tasks.

**Method:** The authors propose a phrase dictionary biasing method applied to a transducer-based streaming speech translation model and a multimodal large language model.

**Key Contributions:**

	1. Introduction of a phrase dictionary biasing method for speech translation
	2. Demonstrated substantial improvements in both transducer-based models and multimodal LLMs
	3. Enhanced phrase recall utilizing external phrase information

**Result:** The proposed method shows a 21% relative improvement over traditional phrase list biasing for the streaming speech translation model and achieves an 85% relative improvement in phrase recall for multimodal large language models.

**Limitations:** 

**Conclusion:** The phrase dictionary biasing method significantly improves translation quality in speech tasks by effectively utilizing phrase mapping.

**Abstract:** Phrases are essential to understand the core concepts in conversations. However, due to their rare occurrence in training data, correct translation of phrases is challenging in speech translation tasks. In this paper, we propose a phrase dictionary biasing method to leverage pairs of phrases mapping from the source language to the target language. We apply the phrase dictionary biasing method to two types of widely adopted models, a transducer-based streaming speech translation model and a multimodal large language model. Experimental results show that the phrase dictionary biasing method outperforms phrase list biasing by 21% relatively for the streaming speech translation model. In addition, phrase dictionary biasing enables multimodal large language models to use external phrase information, achieving 85% relative improvement in phrase recall.

</details>


### [18] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)

*Bruno Ferenc Šegedin*

**Main category:** cs.CL

**Keywords:** deep neural networks, phonotactic generalization, convolutional neural networks, lexical learning, audio processing

**Relevance Score:** 4

**TL;DR:** This study examines the ability of convolutional neural networks to generalize phonotactic rules from raw audio waveforms, particularly focusing on the impact of reducing the fully-connected layer size.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how deep neural networks can learn and generalize phonotactic structures while being trained on lexical items.

**Method:** The paper employs generative convolutional neural networks trained on raw audio, testing the effects of a dramatically reduced fully-connected layer size on the models' generalization abilities.

**Key Contributions:**

	1. Proposed a novel technique for probing lexically-independent generalizations
	2. Investigated the impact of reducing fully-connected layer size on phonotactic generalization
	3. Demonstrated that convolutional layers can dynamically generalize phonetic dependencies.

**Result:** The novel probing technique revealed that convolutional layers can achieve phonetic generalizations independent of the fully-connected layer constraints, showing similar biases in outputs whether processed through FC or not.

**Limitations:** 

**Conclusion:** The findings suggest that the architecture of neural networks can influence their capacity for phonotactic generalization, with implications for understanding lexical learning in DNNs.

**Abstract:** The ability of deep neural networks (DNNs) to represent phonotactic generalizations derived from lexical learning remains an open question. This study (1) investigates the lexically-invariant generalization capacity of generative convolutional neural networks (CNNs) trained on raw audio waveforms of lexical items and (2) explores the consequences of shrinking the fully-connected layer (FC) bottleneck from 1024 channels to 8 before training. Ultimately, a novel technique for probing a model's lexically-independent generalizations is proposed that works only under the narrow FC bottleneck: generating audio outputs by bypassing the FC and inputting randomized feature maps into the convolutional block. These outputs are equally biased by a phonotactic restriction in training as are outputs generated with the FC. This result shows that the convolutional layers can dynamically generalize phonetic dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [19] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)

*Ziyang Cai, Nayoung Lee, Avi Schwarzschild, Samet Oymak, Dimitris Papailiopoulos*

**Main category:** cs.CL

**Keywords:** Length Generalization, Transformer Models, Task Association, Natural Language Processing, Attention Mechanisms

**Relevance Score:** 8

**TL;DR:** This paper explores length generalization in transformer models, showing that models can transfer generalization capabilities from related auxiliary tasks to target tasks, enhancing their ability to extrapolate to longer inputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the mechanisms behind generalization in transformer language models, specifically focusing on length generalization and its transferability across tasks.

**Method:** The authors examined length generalization through tasks that involve arithmetic operations, string transformations, and maze navigation, assessing the transfer of generalization from auxiliary to target tasks.

**Key Contributions:**

	1. Investigates length generalization transfer across tasks
	2. Shows the influence of task association on model generalization
	3. Provides evidence linking attention head reuse to generalization capabilities

**Result:** The paper demonstrates that training models on longer auxiliary tasks enhances their performance on unseen longer inputs in related target tasks, with similar effects noted in pretrained models where attention heads are reused.

**Limitations:** 

**Conclusion:** The findings indicate that transformer models can leverage learned generalization from related tasks, emphasizing the importance of compositionality and inductive structure in task performance beyond training data.

**Abstract:** Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization--the ability to extrapolate from shorter to longer inputs--through the lens of \textit{task association}. We find that length generalization can be \textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead it to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across diverse algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks.

</details>


### [20] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)

*Zhuofang Li, Rafal Kocielnik, Fereshteh Soltani, Penphob, Boonyarungsrit, Animashree Anandkumar, R. Michael Alvarez*

**Main category:** cs.CL

**Keywords:** prosocial behavior, Natural Language Processing, game chat, Self-Anchored Attention Model, toxicity detection

**Relevance Score:** 7

**TL;DR:** This research proposes a novel approach using unsupervised discovery and a Self-Anchored Attention Model (SAAM) to classify prosocial behaviors in online game chat, improving model performance in low-resource settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of resources and focus on identifying prosocial behaviors in online gaming communications, which is as important as detecting toxicity.

**Method:** The study employs unsupervised discovery methods along with expert collaboration to identify and categorize prosocial behaviors from game chat, leveraging the Self-Anchored Attention Model for improved classification.

**Key Contributions:**

	1. Development of the first automated system for identifying prosocial behaviors in game chat
	2. Introduction of the Self-Anchored Attention Model (SAAM)
	3. Demonstration of effective classification in low-resource settings

**Result:** The proposed SAAM achieved a 7.9% improvement in performance compared to existing methods and established the first automated system for classifying prosocial interactions in game-chat text.

**Limitations:** 

**Conclusion:** The research highlights the feasibility of identifying prosocial communication in online gaming, suggesting a shift in moderation focus towards encouraging positive interactions.

**Abstract:** Millions of players engage daily in competitive online games, communicating through in-game chat. Prior research has focused on detecting relatively small volumes of toxic content using various Natural Language Processing (NLP) techniques for the purpose of moderation. However, recent studies emphasize the importance of detecting prosocial communication, which can be as crucial as identifying toxic interactions. Recognizing prosocial behavior allows for its analysis, rewarding, and promotion. Unlike toxicity, there are limited datasets, models, and resources for identifying prosocial behaviors in game-chat text. In this work, we employed unsupervised discovery combined with game domain expert collaboration to identify and categorize prosocial player behaviors from game chat. We further propose a novel Self-Anchored Attention Model (SAAM) which gives 7.9% improvement compared to the best existing technique. The approach utilizes the entire training set as "anchors" to help improve model performance under the scarcity of training data. This approach led to the development of the first automated system for classifying prosocial behaviors in in-game chats, particularly given the low-resource settings where large-scale labeled data is not available. Our methodology was applied to one of the most popular online gaming titles - Call of Duty(R): Modern Warfare(R)II, showcasing its effectiveness. This research is novel in applying NLP techniques to discover and classify prosocial behaviors in player in-game chat communication. It can help shift the focus of moderation from solely penalizing toxicity to actively encouraging positive interactions on online platforms.

</details>


### [21] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)

*Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, Sarath Chandar, Marie-Jeanne Lesot*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Explanation, faithfulness, neural activity, explanation generation

**Relevance Score:** 9

**TL;DR:** This paper introduces a framework to measure the faithfulness of Large Language Model-generated explanations by comparing them with the model's internal reasoning processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unfaithfulness of self-Natural Language Explanations generated by LLMs, which do not reliably reflect their decision-making processes.

**Method:** The paper presents a novel flexible framework that quantitatively measures the faithfulness of self-NLE by comparing it with interpretations of the model's internal hidden states.

**Key Contributions:**

	1. Introduction of a novel framework for measuring self-NLE faithfulness
	2. Direct comparison between self-NLE and neural interpretations
	3. Insights into the model's reasoning process

**Result:** The framework provides insights into self-NLE faithfulness and establishes a connection between generated explanations and model reasoning.

**Limitations:** 

**Conclusion:** This work enhances the understanding of self-NLE faithfulness and offers foundational elements for more faithful explanation generation.

**Abstract:** Large Language Models (LLM) have demonstrated the capability of generating free text self Natural Language Explanation (self-NLE) to justify their answers. Despite their logical appearance, self-NLE do not necessarily reflect the LLM actual decision-making process, making such explanations unfaithful. While existing methods for measuring self-NLE faithfulness mostly rely on behavioral tests or computational block identification, none of them examines the neural activity underlying the model's reasoning. This work introduces a novel flexible framework for quantitatively measuring the faithfulness of LLM-generated self-NLE by directly comparing the latter with interpretations of the model's internal hidden states. The proposed framework is versatile and provides deep insights into self-NLE faithfulness by establishing a direct connection between self-NLE and model reasoning. This approach advances the understanding of self-NLE faithfulness and provides building blocks for generating more faithful self-NLE.

</details>


### [22] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)

*Cesare Spinoso-Di Piano, David Austin, Pablo Piantanida, Jackie Chi Kit Cheung*

**Main category:** cs.CL

**Keywords:** figurative language, rhetorical strategies, Rational Speech Act, irony interpretation, large language models

**Relevance Score:** 8

**TL;DR:** This paper introduces the Rhetorical-Strategy-Aware Rational Speech Act framework to enhance understanding of figurative language by modeling speakers' rhetorical strategies without relying on their implicit motivations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve how figurative language, such as irony, is interpreted in communication, leveraging a probabilistic framework that is compatible with human understanding.

**Method:** Introducing the Rhetorical-Strategy-Aware RSA framework, denoted as $(RSA)^2$, which focuses on the rhetorical strategies used by speakers rather than their implicit motivations.

**Key Contributions:**

	1. Development of the $(RSA)^2$ framework for interpreting figurative language.
	2. Demonstrated success on the PragMega+ dataset for irony interpretation.
	3. Compatibility with large language models for enhanced performance.

**Result:** $(RSA)^2$ achieves state-of-the-art results on the PragMega+ dataset specifically for irony interpretation when combined with large language models.

**Limitations:** 

**Conclusion:** The proposed framework allows for more accurate and human-compatible interpretations of figurative language while simplifying the modeling process by not needing to understand the speaker's motivations.

**Abstract:** Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in human communication, resulting in utterances where the literal and the intended meanings do not match. The Rational Speech Act (RSA) framework, which explicitly models speaker intentions, is the most widespread theory of probabilistic pragmatics, but existing implementations are either unable to account for figurative expressions or require modeling the implicit motivations for using figurative language (e.g., to express joy or annoyance) in a setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware RSA $(RSA)^2$ framework which models figurative language use by considering a speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables human-compatible interpretations of non-literal utterances without modeling a speaker's motivations for being non-literal. Combined with LLMs, it achieves state-of-the-art performance on the ironic split of PragMega+, a new irony interpretation dataset introduced in this study.

</details>


### [23] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)

*Yao Xiao, Heidi Christensen, Stefan Goetze*

**Main category:** cs.CL

**Keywords:** Alzheimer's dementia, large language models, paired perplexity, language analysis, model interpretability

**Relevance Score:** 9

**TL;DR:** This paper improves Alzheimer's detection using a fine-tuned LLM, Mistral-7B, achieving better accuracy and interpretability than existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance detection methods for Alzheimer's dementia by leveraging large language models to analyze language patterns in patients.

**Method:** The authors utilize the instruction-following version of the Mistral-7B LLM to extend the paired perplexity approach for AD detection.

**Key Contributions:**

	1. Improved accuracy in AD detection using Mistral-7B LLM
	2. Clear and interpretable decision boundary for language analysis
	3. Demonstration of LLM understanding specific language patterns of AD speakers

**Result:** The proposed method shows improvements in accuracy by 3.33% over current paired perplexity approaches and 6.35% over top-ranked methods from the ADReSS 2020 challenge.

**Limitations:** 

**Conclusion:** The findings suggest that the fine-tuned LLM not only enhances AD detection accuracy but also offers interpretability, which can inform future techniques in model transparency and data augmentation.

**Abstract:** Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive decline that commonly impacts language ability. This work extends the paired perplexity approach to detecting AD by using a recent large language model (LLM), the instruction-following version of Mistral-7B. We improve accuracy by an average of 3.33% over the best current paired perplexity method and by 6.35% over the top-ranked method from the ADReSS 2020 challenge benchmark. Our further analysis demonstrates that the proposed approach can effectively detect AD with a clear and interpretable decision boundary in contrast to other methods that suffer from opaque decision-making processes. Finally, by prompting the fine-tuned LLMs and comparing the model-generated responses to human responses, we illustrate that the LLMs have learned the special language patterns of AD speakers, which opens up possibilities for novel methods of model interpretation and data augmentation.

</details>


### [24] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)

*Yuxin Jiang*

**Main category:** cs.CL

**Keywords:** Large language models, LLM alignment, Data collection, Training methodologies, Evaluation frameworks

**Relevance Score:** 9

**TL;DR:** This thesis focuses on advancing alignment methodologies for large language models via novel data collection, training, and evaluation techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning large language models (LLMs) with human expectations remains a critical challenge, necessitating improved methodologies for data collection, training, and evaluation.

**Method:** The thesis presents Lion, an adversarial distillation framework for refining training data, Web Reconstruction for automated instruction-tuning data synthesis, Learning to Edit for knowledge integration, and Bridging and Modeling Correlations for enhancing preference data alignment, culminating in FollowBench for evaluating constraint adherence.

**Key Contributions:**

	1. Introduction of Lion for dynamic training data refinement
	2. Development of Learning to Edit for knowledge integration
	3. Creation of FollowBench for assessing adherence to complex constraints

**Result:** The methods proposed significantly improve data diversity, scalability, and alignment of LLMs in reasoning tasks, exposing weaknesses in current evaluation benchmarks.

**Limitations:** 

**Conclusion:** The advancements made in LLM alignment can guide future improvements and better model evaluations in line with human expectations.

**Abstract:** Large language models (LLMs) exhibit remarkable capabilities across diverse tasks, yet aligning them efficiently and effectively with human expectations remains a critical challenge. This thesis advances LLM alignment by introducing novel methodologies in data collection, training, and evaluation. We first address alignment data collection. Existing approaches rely heavily on manually curated datasets or proprietary models. To overcome these limitations, we propose Lion, an adversarial distillation framework that iteratively refines training data by identifying and generating challenging instructions, enabling state-of-the-art zero-shot reasoning. Additionally, we introduce Web Reconstruction (WebR), a fully automated framework that synthesizes instruction-tuning data directly from raw web documents, significantly improving data diversity and scalability over existing synthetic data methods. Next, we enhance alignment training through novel optimization techniques. We develop Learning to Edit (LTE), a framework that enables LLMs to efficiently integrate new knowledge while preserving existing information. LTE leverages meta-learning to improve both real-time and batch knowledge updates. Furthermore, we introduce Bridging and Modeling Correlations (BMC), a refinement of Direct Preference Optimization (DPO) that explicitly captures token-level correlations in preference data, leading to superior alignment across QA and mathematical reasoning tasks. Finally, we tackle the challenge of evaluating alignment. Existing benchmarks emphasize response quality but overlook adherence to specific constraints. To bridge this gap, we introduce FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to follow complex constraints across diverse instruction types. Our results expose key weaknesses in current models' constraint adherence, offering insights for future improvements.

</details>


### [25] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)

*Arjun Vaithilingam Sudhakar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Theory of Mind, Multi-Agent Reinforcement Learning, Human-AI Collaboration, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper investigates if Large Language Models (LLMs) can model and reason about the intentions of others, akin to a theory of mind, through cooperative multi-agent reinforcement learning, aiming to enhance collaboration between artificial agents and humans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore if LLMs possess a form of theory of mind, which is crucial for effective collaboration in human-AI interaction.

**Method:** The study utilizes cooperative multi-agent reinforcement learning (MARL) where agents learn to collaborate through repeated interactions, simulating human social reasoning.

**Key Contributions:**

	1. Investigation of theory of mind in LLMs
	2. Framework for cooperative multi-agent reinforcement learning
	3. Enhancement of human-AI collaboration capabilities

**Result:** The approach seeks to improve artificial agents' adaptability and cooperation with both human and artificial partners, leveraging LLM-based agents for natural language interaction.

**Limitations:** 

**Conclusion:** The research aims to create hybrid human-AI systems that facilitate seamless collaboration, impacting the future of human-artificial interaction.

**Abstract:** Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot generalization capabilities across complex natural language tasks, enabling their widespread use as virtual assistants for diverse applications such as translation and summarization. Despite being trained solely on large corpora of text without explicit supervision on author intent, LLMs appear to infer the underlying meaning of textual interactions. This raises a fundamental question: can LLMs model and reason about the intentions of others, i.e., do they possess a form of theory of mind? Understanding other's intentions is crucial for effective collaboration, which underpins human societal success and is essential for cooperative interactions among multiple agents, including humans and autonomous systems. In this work, we investigate the theory of mind in LLMs through the lens of cooperative multi-agent reinforcement learning (MARL), where agents learn to collaborate via repeated interactions, mirroring human social reasoning. Our approach aims to enhance artificial agent's ability to adapt and cooperate with both artificial and human partners. By leveraging LLM-based agents capable of natural language interaction, we move towards creating hybrid human-AI systems that can foster seamless collaboration, with broad implications for the future of human-artificial interaction.

</details>


### [26] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)

*Siheng Li, Zhanhui Zhou, Wai Lam, Chao Yang, Chaochao Lu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Policy Optimization

**Relevance Score:** 8

**TL;DR:** Introducing Replay-Enhanced Policy Optimization (RePO) to improve efficiency in training large language models by utilizing diverse replay strategies for better policy optimization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance data efficiency and reduce computational costs in the reinforcement learning (RL) optimization of large language models (LLMs).

**Method:** RePO employs diverse replay strategies to retrieve off-policy samples from a replay buffer, optimizing policy based on a larger variety of samples.

**Key Contributions:**

	1. Introduces Replay-Enhanced Policy Optimization (RePO)
	2. Achieves significant performance gains over existing methods
	3. Enhances policy optimization utilizing off-policy samples

**Result:** RePO shows significant performance improvements of 18.4 and 4.1 points over the previous Group Relative Policy Optimization (GRPO) method on different LLMs, while increasing effective optimization steps by 48%.

**Limitations:** 

**Conclusion:** RePO optimizes training in LLMs by balancing computational costs and data efficiency, demonstrating better results across various benchmarks.

**Abstract:** Reinforcement learning (RL) is vital for optimizing large language models (LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages using multiple on-policy outputs per prompt, leading to high computational costs and low data efficiency. To address this, we introduce Replay-Enhanced Policy Optimization (RePO), which leverages diverse replay strategies to retrieve off-policy samples from a replay buffer, allowing policy optimization based on a broader and more diverse set of samples for each prompt. Experiments on five LLMs across seven mathematical reasoning benchmarks demonstrate that RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further analysis indicates that RePO increases computational cost by $15\%$ while raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B, with both on-policy and off-policy sample numbers set to $8$. The repository can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [27] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)

*Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat*

**Main category:** cs.CL

**Keywords:** latent multi-head attention, small language models, rotary positional embeddings, efficiency quality trade-offs, GPT models

**Relevance Score:** 9

**TL;DR:** This paper study examines latent multi-head attention (MLA) in small language models, demonstrating significant efficiency improvements in terms of memory usage and inference speed with minimal loss in quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the efficiency-quality trade-offs of latent multi-head attention in small language models, aiming for improved performance in memory-constrained environments.

**Method:** The authors trained 30M-parameter GPT models on 100,000 synthetic stories and compared three architectural variants: standard multi-head attention, MLA, and MLA with rotary positional embeddings (MLA+RoPE).

**Key Contributions:**

	1. First comprehensive study on latent multi-head attention for small language models.
	2. Demonstrated significant memory savings and speed improvements with MLA+RoPE.
	3. Insights on the importance of rotary positional embeddings in achieving better model performance.

**Result:** MLA+RoPE with half-rank latent dimensions achieves a 45% reduction in KV-cache memory with only a 0.3% increase in validation loss, showcasing a Pareto improvement. It offers a 1.4 times speedup in inference while maintaining memory efficiency.

**Limitations:** 

**Conclusion:** The study concludes that MLA+RoPE is superior for small models, with RoPE being crucial for enhancing its performance compared to standard attention.

**Abstract:** We present the first comprehensive study of latent multi-head attention (MLA) for small language models, revealing interesting efficiency-quality trade-offs. Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory reduction while incurring only a 0.3% increase in validation loss (essentially matching MHA quality)- a Pareto improvement for memory constrained deployment. We further show that RoPE is crucial for MLA in small models: without it, MLA underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by 2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2 achieves a 1.4 times speedup over full-rank MLA while maintaining the memory savings. GPT-4 evaluations corroborate perplexity results, with ours achieving the highest quality scores (7.4/10) across grammar, creativity, and consistency metrics. Code and models will be released upon acceptance.

</details>


### [28] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)

*Chao-Hong Tan, Qian Chen, Wen Wang, Chong Deng, Qinglin Zhang, Luyao Cheng, Hai Yu, Xin Zhang, Xiang Lv, Tianyu Zhao, Chong Zhang, Yukun Ma, Yafeng Chen, Hui Wang, Jiaqing Liu, Jieping Ye*

**Main category:** cs.CL

**Keywords:** speech generation, large language models, joint autoregressive modeling, parallel processing, speech-text modeling

**Relevance Score:** 8

**TL;DR:** The paper introduces OmniDRCA, a parallel speech-text foundation model that enhances audio comprehension and sets new state-of-the-art performance in speech-text modeling.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** With growing interest in end-to-end speech generation using large language models, there is a need for models that effectively integrate speech and text modalities.

**Method:** OmniDRCA uses joint autoregressive modeling to generate discrete speech tokens in parallel with text tokens, incorporating dual-resolution speech representations and contrastive cross-modal alignment for better comprehension.

**Key Contributions:**

	1. Introduction of OmniDRCA model for parallel speech-text generation
	2. Achieving SOTA performance on benchmarks for speech-text modeling
	3. Methodology integrating dual-resolution speech representations and contrastive alignment

**Result:** OmniDRCA achieves state-of-the-art performance on Spoken Question Answering benchmarks and demonstrates competitive results compared to existing interleaved models.

**Limitations:** 

**Conclusion:** The results indicate that OmniDRCA can effectively model speech-text interactions and has implications for full-duplex conversational applications.

**Abstract:** Recent studies on end-to-end speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents OmniDRCA, a parallel speech-text foundation model based on joint autoregressive modeling, featuring dual-resolution speech representations and contrastive cross-modal alignment. Our approach processes speech and text representations in parallel while enhancing audio comprehension through contrastive alignment. Experimental results on Spoken Question Answering benchmarks demonstrate that OmniDRCA establishes new state-of-the-art (SOTA) performance among parallel joint speech-text modeling based foundation models, and achieves competitive performance compared to interleaved models. Additionally, we explore the potential of extending the framework to full-duplex conversational scenarios.

</details>


### [29] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)

*Yuchen Feng, Bowen Shen, Naibin Gu, Jiaxuan Zhao, Peng Fu, Zheng Lin, Weiping Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mixture-of-Experts, Diversity Enhancement, Model Reconstruction, Training Efficiency

**Relevance Score:** 7

**TL;DR:** This paper introduces DIVE, a Diversity-Enhanced reconstruction method for Mixture-of-Experts LLMs, which improves training efficiency and model diversity without significant accuracy loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing reconstruction methods for transforming dense LLMs into MoE architectures overlook the diversity of experts, which can lead to redundancy and inefficiency.

**Method:** DIVE employs domain affinity mining, pruning-based expert reconstruction, and efficient retraining of routers, experts, and normalization modules in Llama-style LLMs.

**Key Contributions:**

	1. Introduces DIVE for enhancing expert diversity in MoE LLMs
	2. Implements domain affinity mining and pruning-based reconstruction
	3. Demonstrates improved efficiency with minimal accuracy loss compared to previous methods.

**Result:** DIVE achieves improved training efficiency with minimal accuracy trade-offs, outperforming existing approaches for the same number of activated parameters.

**Limitations:** 

**Conclusion:** The DIVE method enhances the diversity among experts in MoE LLMs and allows for more efficient training processes.

**Abstract:** Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture achieve high cost-efficiency by selectively activating a subset of the parameters. Despite the inference efficiency of MoE LLMs, the training of extensive experts from scratch incurs substantial overhead, whereas reconstructing a dense LLM into an MoE LLM significantly reduces the training budget. However, existing reconstruction methods often overlook the diversity among experts, leading to potential redundancy. In this paper, we come up with the observation that a specific LLM exhibits notable diversity after being pruned on different calibration datasets, based on which we present a Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE includes domain affinity mining, pruning-based expert reconstruction, and efficient retraining. Specifically, the reconstruction includes pruning and reassembly of the feed-forward network (FFN) module. After reconstruction, we efficiently retrain the model on routers, experts and normalization modules. We implement DIVE on Llama-style LLMs with open-source training corpora. Experiments show that DIVE achieves training efficiency with minimal accuracy trade-offs, outperforming existing pruning and MoE reconstruction methods with the same number of activated parameters.

</details>


### [30] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)

*Qingyun Zeng, Simin Ma, Arash Niknafs, Ashish Basran, Carol Szabo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Text-to-SQL, semantic equivalence, SQL evaluation, NL2SQL

**Relevance Score:** 8

**TL;DR:** This paper investigates the use of Large Language Models (LLMs) to evaluate semantic equivalence in Text-to-SQL systems, addressing challenges related to ambiguous queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in evaluating semantic equivalence of SQL generated from ambiguous user queries in NL2SQL systems.

**Method:** The paper analyzes patterns of SQL equivalence and inequivalence, applying LLMs to evaluate both semantic and weak semantic equivalence.

**Key Contributions:**

	1. Use of LLMs for semantic and weak semantic equivalence evaluation in SQL generation
	2. Identification of common patterns in SQL equivalence
	3. Discussion of challenges in LLM-based evaluation methods

**Result:** The analysis reveals common patterns and highlights the challenges faced in effectively using LLMs for evaluation.

**Limitations:** 

**Conclusion:** LLMs show promise in evaluating SQL equivalence, but ambiguities in user queries pose significant challenges.

**Abstract:** The rise of Large Language Models (LLMs) has significantly advanced Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of generated SQL remains a challenge, especially given ambiguous user queries and multiple valid SQL interpretations. This paper explores using LLMs to assess both semantic and a more practical "weak" semantic equivalence. We analyze common patterns of SQL equivalence and inequivalence, discuss challenges in LLM-based evaluation.

</details>


### [31] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)

*Zhengyuan Liu, Stella Xin Yin, Dion Hoe-Lian Goh, Nancy F. Chen*

**Main category:** cs.CL

**Keywords:** Generative AI, education, curriculum development, STEM, readability

**Relevance Score:** 6

**TL;DR:** COGENT is a framework for generating curriculum-oriented, grade-appropriate educational content using Generative AI, focusing on STEM education challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of applying Generative AI in educational contexts, particularly the misalignment with curriculum standards and readability issues in STEM education.

**Method:** COGENT incorporates curriculum components, controls readability through various factors, and uses a wonder-based approach to enhance student engagement.

**Key Contributions:**

	1. Development of a curriculum-oriented framework for AI-generated educational content
	2. Integration of curriculum components for enhanced content alignment
	3. Demonstration of effectiveness through multi-dimensional evaluations

**Result:** COGENT produces grade-appropriate educational content that is comparable or superior to human references according to both LLMs and human expert evaluations.

**Limitations:** 

**Conclusion:** COGENT establishes a scalable approach for creating adaptive, high-quality learning resources.

**Abstract:** While Generative AI has demonstrated strong potential and versatility in content generation, its application to educational contexts presents several challenges. Models often fail to align with curriculum standards and maintain grade-appropriate reading levels consistently. Furthermore, STEM education poses additional challenges in balancing scientific explanations with everyday language when introducing complex and abstract ideas and phenomena to younger students. In this work, we propose COGENT, a curriculum-oriented framework for generating grade-appropriate educational content. We incorporate three curriculum components (science concepts, core ideas, and learning objectives), control readability through length, vocabulary, and sentence complexity, and adopt a ``wonder-based'' approach to increase student engagement and interest. We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human expert analysis. Experimental results show that COGENT consistently produces grade-appropriate passages that are comparable or superior to human references. Our work establishes a viable approach for scaling adaptive and high-quality learning resources.

</details>


### [32] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)

*Massa Baali, Shuo Han, Syed Abdul Hannan, Purusottam Samal, Karanveer Singh, Soham Deshmukh, Rita Singh, Bhiksha Raj*

**Main category:** cs.CL

**Keywords:** Speaker recognition, Speaker Language Model, Demographic attributes, Prompt-based conditioning, Zero-shot learning

**Relevance Score:** 7

**TL;DR:** CoLMbo introduces a novel Speaker Language Model that enhances speaker recognition by generating rich contextual descriptions and capturing demographic attributes based on speaker embeddings and user-defined prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of traditional speaker recognition systems that only classify speakers without generating detailed characteristics or contextual descriptions.

**Method:** Integration of a speaker encoder with prompt-based conditioning allows the system to create detailed captions from speaker embeddings.

**Key Contributions:**

	1. Introduction of CoLMbo, a Speaker Language Model
	2. Dynamic adaptation to new speaker characteristics through prompt-based conditioning
	3. Enhanced profiling capabilities including demographic attributes such as dialect, gender, and age.

**Result:** CoLMbo enhances speaker profiling and performs well in zero-shot scenarios across diverse datasets by providing customized descriptions of speakers, including dialect variations and demographic traits.

**Limitations:** 

**Conclusion:** This approach represents a significant advancement in speaker recognition technology, paving the way for more nuanced understanding of speakers based on their embeddings.

**Abstract:** Speaker recognition systems are often limited to classification tasks and struggle to generate detailed speaker characteristics or provide context-rich descriptions. These models primarily extract embeddings for speaker identification but fail to capture demographic attributes such as dialect, gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker Language Model (SLM) that addresses these limitations by integrating a speaker encoder with prompt-based conditioning. This allows for the creation of detailed captions based on speaker embeddings. CoLMbo utilizes user-defined prompts to adapt dynamically to new speaker characteristics and provides customized descriptions, including regional dialect variations and age-related traits. This innovative approach not only enhances traditional speaker profiling but also excels in zero-shot scenarios across diverse datasets, marking a significant advancement in the field of speaker recognition.

</details>


### [33] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)

*Austin McCutcheon, Thiago E. A. de Oliveira, Aleksandr Zheleznov, Chris Brogly*

**Main category:** cs.CL

**Keywords:** news quality, machine learning, NLP, DistilBERT, ensemble methods

**Relevance Score:** 6

**TL;DR:** The paper investigates distinguishing between perceived lower-quality and higher-quality news headlines using machine learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of automated identification of news quality in the context of the vast amount of online news.

**Method:** Evaluated twelve machine learning models on a balanced dataset of over 57 million news website links/headings, employing 115 linguistic features and expert-derived binary quality labels.

**Key Contributions:**

	1. Introduction of a dataset with over 57 million news links for quality classification
	2. Comparison of traditional ensemble methods and deep learning models for this task
	3. Demonstration of the importance of both linguistic features and model choice in predicting news quality

**Result:** The bagging classifier achieved 88.1% accuracy while fine-tuned DistilBERT reached 90.3% accuracy, highlighting the effective use of both traditional classifiers and deep learning approaches.

**Limitations:** Potential biases in expert consensus for quality labeling and the need for extensive training time for deep learning models.

**Conclusion:** NLP features combined with traditional classifiers and deep learning models can successfully differentiate news quality, with trade-offs in performance and training time.

**Abstract:** The proliferation of online news enables potential widespread publication of perceived low-quality news headlines/links. As a result, we investigated whether it was possible to automatically distinguish perceived lower-quality news headlines/links from perceived higher-quality headlines/links. We evaluated twelve machine learning models on a binary, balanced dataset of 57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per class) with 115 extracted linguistic features. Binary labels for each text were derived from scores based on expert consensus regarding the respective news domain quality. Traditional ensemble methods, particularly the bagging classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20 train/test split) but required more training time. The results suggest that both NLP features with traditional classifiers and deep learning models can effectively differentiate perceived news headline/link quality, with some trade-off between predictive performance and train time.

</details>


### [34] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)

*Haoran Zhao, Robert D. Hawkins*

**Main category:** cs.CL

**Keywords:** politeness, large language models, pragmatics, human evaluation, AI alignment

**Relevance Score:** 8

**TL;DR:** This paper investigates how large language models (LLMs) handle polite speech, comparing their performance to human responses.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the alignment challenge in LLMs regarding the use of polite speech strategies that balance social and informational goals.

**Method:** The study compares responses generated by LLMs (≥70B parameters) and human participants in both constrained and open-ended tasks.

**Key Contributions:**

	1. Demonstration of LLMs' capability to emulate human-like polite speech strategies.
	2. Identification of the over-reliance on negative politeness by LLMs in inappropriate contexts.
	3. Insights into the implications of conversational alignment challenges for AI development.

**Result:** LLMs replicate key preferences seen in human responses but tend to over-rely on negative politeness strategies, even in contexts where positive responses are more appropriate.

**Limitations:** Focuses primarily on larger models and may not generalize to smaller LLMs or other AI systems.

**Conclusion:** Although LLMs exhibit proficient politeness strategies, their tendencies may lead to misinterpretations, highlighting challenges in pragmatic alignment for AI systems.

**Abstract:** Polite speech poses a fundamental alignment challenge for large language models (LLMs). Humans deploy a rich repertoire of linguistic strategies to balance informational and social goals -- from positive approaches that build rapport (compliments, expressions of interest) to negative strategies that minimize imposition (hedging, indirectness). We investigate whether LLMs employ a similarly context-sensitive repertoire by comparing human and LLM responses in both constrained and open-ended production tasks. We find that larger models ($\ge$70B parameters) successfully replicate key preferences from the computational pragmatics literature, and human evaluators surprisingly prefer LLM-generated responses in open-ended contexts. However, further linguistic analyses reveal that models disproportionately rely on negative politeness strategies even in positive contexts, potentially leading to misinterpretations. While modern LLMs demonstrate an impressive handle on politeness strategies, these subtle differences raise important questions about pragmatic alignment in AI systems.

</details>


### [35] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)

*Xinyi Gao, Qiucheng Wu, Yang Zhang, Xuechen Liu, Kaizhi Qian, Ying Xu, Shiyu Chang*

**Main category:** cs.CL

**Keywords:** Knowledge Tracing, Probabilistic Models, Hidden Markov Models, Educational Technology, Personalized Learning

**Relevance Score:** 5

**TL;DR:** A probabilistic framework, KT$^2$, for modeling student knowledge states in low-resource settings using hierarchical knowledge concepts and a Hidden Markov Tree Model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge tracing in low-resource classroom settings that require real-time updates as student exercise history grows.

**Method:** KT$^2$ models student understanding over a tree-structured hierarchy of knowledge concepts and employs an EM algorithm for estimating mastery while enabling personalized predictions through incremental updates.

**Key Contributions:**

	1. Introduction of KT$^2$ framework for low-resource knowledge tracing
	2. Use of Hidden Markov Tree Model to represent knowledge concepts
	3. Implementation of incremental update mechanism for personalized predictions

**Result:** KT$^2$ demonstrates consistent performance improvement over strong baselines in online, low-resource environments.

**Limitations:** 

**Conclusion:** The framework effectively utilizes available hierarchical knowledge concept information to enhance knowledge tracing performance under data-scarce conditions.

**Abstract:** Knowledge tracing (KT) aims to estimate a student's evolving knowledge state and predict their performance on new exercises based on performance history. Many realistic classroom settings for KT are typically low-resource in data and require online updates as students' exercise history grows, which creates significant challenges for existing KT approaches. To restore strong performance under low-resource conditions, we revisit the hierarchical knowledge concept (KC) information, which is typically available in many classroom settings and can provide strong prior when data are sparse. We therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a probabilistic KT framework that models student understanding over a tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree Model. KT$^2$ estimates student mastery via an EM algorithm and supports personalized prediction through an incremental update mechanism as new responses arrive. Our experiments show that KT$^2$ consistently outperforms strong baselines in realistic online, low-resource settings.

</details>


### [36] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)

*Jui-Ming Yao, Hao-Yuan Chen, Zi-Xian Tang, Bing-Jia Tan, Sheng-Wei Peng, Bing-Cheng Xie, Shun-Feng Su*

**Main category:** cs.CL

**Keywords:** Large Language Models, Token Constraint Decoding, input noise, Robustness, Multiple-choice question answering

**Relevance Score:** 8

**TL;DR:** This paper presents Token Constraint Decoding (TCD), an algorithm designed to enhance the robustness of Large Language Models (LLMs) against minor input perturbations in multiple-choice question answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of Large Language Models on multiple-choice question answering benchmarks while enhancing their resilience to input noise.

**Method:** Introducing Token Constraint Decoding (TCD), an inference-time algorithm that enforces alignment between token-level predictions to improve robustness.

**Key Contributions:**

	1. Introduction of Token Constraint Decoding (TCD) for LLMs
	2. Demonstrated performance improvements in noisy settings for MCQA
	3. Insights into penalty schedules required for different models to maximize resilience.

**Result:** TCD significantly restores performance degraded by input noise, achieving up to +39% absolute gains for weaker models when paired with prompt engineering fixes. It also regularizes overconfident outputs through penalty sweep analyses tailored to each model.

**Limitations:** 

**Conclusion:** TCD is established as a practical, model-agnostic approach for enhancing reasoning stability under real-world conditions, paving the way for safer deployment of LLMs in critical applications.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive performance on multiple-choice question answering (MCQA) benchmarks, yet they remain highly vulnerable to minor input perturbations. In this paper, we introduce and evaluate Token Constraint Decoding (TCD). This simple yet effective inference-time algorithm enforces alignment between token-level predictions to enhance robustness in noisy settings. Through extensive experiments on CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired with prompt engineering (PE) fixes, significantly restores performance degraded by input noise, yielding up to +39\% absolute gains for weaker models like Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly regularizes overconfident outputs, with different models requiring distinct penalty schedules to maximize resilience. Our findings establish TCD as a practical, model-agnostic approach for improving reasoning stability under real-world imperfections and pave the way for more reliable deployment of LLMs in safety-critical or user-facing applications.

</details>


### [37] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)

*Xiujun Zhou, Pingjian Zhang, Deyou Tang*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Question Answering, Data Augmentation, Large Language Models, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper presents PGDA-KGQA, a novel prompt-guided generative framework for enhancing Knowledge Graph Question Answering (KGQA) through various data augmentation strategies.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations faced by existing KGQA methods in terms of data scarcity, particularly for multi-hop reasoning and diverse annotated data.

**Method:** PGDA-KGQA uses a unified prompt-design paradigm to generate (question, logical form) pairs by crafting engineered prompts. The framework includes generating single-hop pseudo questions, semantic-preserving question rewriting, and answer-guided reverse path exploration.

**Key Contributions:**

	1. Proposing a novel prompt-guided framework for KGQA
	2. Integrating multiple data augmentation strategies
	3. Demonstrating robust performance improvements over existing methods

**Result:** Experiments demonstrate that PGDA-KGQA outperforms state-of-the-art methods on standard KGQA datasets, notably improving performance metrics such as F1, Hits@1, and Accuracy.

**Limitations:** 

**Conclusion:** The proposed framework significantly enhances the accuracy of logical form generation and answer retrieval performance in KGQA tasks.

**Abstract:** Knowledge Graph Question Answering (KGQA) is a crucial task in natural language processing that requires reasoning over knowledge graphs (KGs) to answer natural language questions. Recent methods utilizing large language models (LLMs) have shown remarkable semantic parsing capabilities but are limited by the scarcity of diverse annotated data and multi-hop reasoning samples. Traditional data augmentation approaches are focus mainly on single-hop questions and prone to semantic distortion, while LLM-based methods primarily address semantic distortion but usually neglect multi-hop reasoning, thus limiting data diversity. The scarcity of multi-hop samples further weakens models' generalization. To address these issues, we propose PGDA-KGQA, a prompt-guided generative framework with multiple data augmentation strategies for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by crafting meticulously engineered prompts that integrate the provided textual content, it leverages LLMs to generate large-scale (question, logical form) pairs for model training. Specifically, PGDA-KGQA enriches its training set by: (1) generating single-hop pseudo questions to improve the alignment of question semantics with KG relations; (2) applying semantic-preserving question rewriting to improve robustness against linguistic variations; (3) employing answer-guided reverse path exploration to create realistic multi-hop questions. By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA utilizes the augmented data to enhance the accuracy of logical form generation and thus improve answer retrieval performance. Experiments demonstrate that outperforms state-of-the-art methods on standard KGQA datasets, achieving improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by 1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [38] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)

*Md Messal Monem Miah, Adrita Anika, Xi Shi, Ruihong Huang*

**Main category:** cs.CL

**Keywords:** deception detection, Large Language Models, multimodal models, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** This study evaluates the automated deception detection capabilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) and analyzes their performance on various datasets.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of automated deception detection in an increasingly digital world as deception techniques evolve.

**Method:** The performance of open-source and commercial LLMs was tested on three datasets using different experimental setups, including zero-shot and few-shot approaches with various example selection strategies.

**Key Contributions:**

	1. Comprehensive evaluation of LLMs and LMMs for deception detection.
	2. Analysis of the performance across different datasets and setups.
	3. Insights into the effectiveness of prompting strategies and auxiliary features.

**Result:** Fine-tuned LLMs achieved state-of-the-art performance in textual deception detection, whereas LMMs showed limitations in utilizing cross-modal cues; also, auxiliary features and prompting strategies were examined.

**Limitations:** LMMs struggle with cross-modal cues; results may vary with different model architectures or datasets.

**Conclusion:** LLMs have potential in detecting deception, but their effectiveness varies across modalities, and LMMs require further development.

**Abstract:** Detecting deception in an increasingly digital world is both a critical and challenging task. In this study, we present a comprehensive evaluation of the automated deception detection capabilities of Large Language Models (LLMs) and Large Multimodal Models (LMMs) across diverse domains. We assess the performance of both open-source and commercial LLMs on three distinct datasets: real life trial interviews (RLTD), instructed deception in interpersonal scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the effectiveness of different experimental setups for deception detection, including zero-shot and few-shot approaches with random or similarity-based in-context example selection. Our results show that fine-tuned LLMs achieve state-of-the-art performance on textual deception detection tasks, while LMMs struggle to fully leverage cross-modal cues. Additionally, we analyze the impact of auxiliary features, such as non-verbal gestures and video summaries, and examine the effectiveness of different prompting strategies, including direct label generation and chain-of-thought reasoning. Our findings provide key insights into how LLMs process and interpret deceptive cues across modalities, highlighting their potential and limitations in real-world deception detection applications.

</details>


### [39] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)

*Fei Ding, Baiqiao Wang*

**Main category:** cs.CL

**Keywords:** Supervised Fine-Tuning, Large Language Models, Catastrophic Forgetting

**Relevance Score:** 9

**TL;DR:** This paper presents a new method for Supervised Fine-Tuning (SFT) that minimizes catastrophic forgetting in large language models without needing access to the original training data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance instruction-following and task adaptability in LLMs while mitigating the issue of catastrophic forgetting during SFT.

**Method:** The proposed method involves reconstructing the instruction distribution of the base model and selecting optimal data through a multi-model screening process, which is then mixed with new data for SFT.

**Key Contributions:**

	1. Novel SFT method reduces catastrophic forgetting
	2. Does not require access to original SFT data
	3. Balances task-specific performance with generalization capabilities

**Result:** Experiments show that this method preserves generalization capabilities across general domains while also enhancing task-specific performance.

**Limitations:** Limited testing on diverse datasets could affect generalizability of results.

**Conclusion:** The novel SFT method effectively balances the need for adaptability in task-specific scenarios with the retention of general performance in LLMs.

**Abstract:** Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)' instruction-following capabilities and domain-specific task adaptability, often diminishes their general capabilities. Moreover, due to the inaccessibility of original pre-training data, catastrophic forgetting tends to be exacerbated when third-party practitioners implement SFT on open-sourced models. To address this challenge, we propose a novel, more cost-effective SFT method which could effectively reduce the risk of catastrophic forgetting without access to original SFT data. Our approach begins by reconstructing the likely SFT instruction distribution of the base model, followed by a multi-model screening process to select optimal data, which is then mixed with new data for SFT. Experimental results demonstrate that our method preserves generalization capabilities in general domains while improving task-specific performance.

</details>


### [40] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)

*GigaChat team, Mamedov Valentin, Evgenii Kosarev, Gregory Leleytner, Ilya Shchuckin, Valeriy Berezovskiy, Daniil Smirnov, Dmitry Kozlov, Sergei Averkiev, Lukyanenko Ivan, Aleksandr Proshunin, Ainur Israfilova, Ivan Baskov, Artem Chervyakov, Emil Shakirov, Mikhail Kolesov, Daria Khomich, Darya Latortseva, Sergei Porkhun, Yury Fedorov, Oleg Kutuzov, Polina Kudriavtseva, Sofiia Soldatova, Kolodin Egor, Stanislav Pyatkin, Dzmitry Menshykh, Grafov Sergei, Eldar Damirov, Karlov Vladimir, Ruslan Gaitukiev, Arkadiy Shatenov, Alena Fenogenova, Nikita Savushkin, Fedor Minkin*

**Main category:** cs.CL

**Keywords:** Generative LLMs, Russian language, NLP, GigaChat, Open-source

**Relevance Score:** 7

**TL;DR:** This paper presents the GigaChat family of large language models designed for the Russian language, detailing their architecture, pre-training, performance evaluations, and offering system demonstrations via various interfaces.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of Russian-specific foundational models in NLP due to high computational demands and to support industrial solutions for the Russian language.

**Method:** Introduction of the GigaChat LLMs with varying sizes, including models optimized for instruction. The architecture and pre-training processes are detailed, along with performance evaluations on Russian and English benchmarks.

**Key Contributions:**

	1. Introduction of GigaChat LLMs specifically tailored for the Russian language
	2. Open-source availability of three GigaChat models
	3. Comprehensive performance evaluation against multilingual models

**Result:** GigaChat models have been evaluated and compared with multilingual counterparts, demonstrating robust performance and offering open-source access to three models.

**Limitations:** Limited computational resources may hinder further improvements and expansion of these models.

**Conclusion:** The release of GigaChat models is expected to enhance Russian NLP research and facilitate the development of applicable solutions, supported by accessible APIs and user interfaces.

**Abstract:** Generative large language models (LLMs) have become crucial for modern NLP research and applications across various languages. However, the development of foundational models specifically tailored to the Russian language has been limited, primarily due to the significant computational resources required. This paper introduces the GigaChat family of Russian LLMs, available in various sizes, including base models and instruction-tuned versions. We provide a detailed report on the model architecture, pre-training process, and experiments to guide design choices. In addition, we evaluate their performance on Russian and English benchmarks and compare GigaChat with multilingual analogs. The paper presents a system demonstration of the top-performing models accessible via an API, a Telegram bot, and a Web interface. Furthermore, we have released three open GigaChat models in open-source (https://huggingface.co/ai-sage), aiming to expand NLP research opportunities and support the development of industrial solutions for the Russian language.

</details>


### [41] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)

*Prameshwar Thiyagarajan, Vaishnavi Parimi, Shamant Sai, Soumil Garg, Zhangir Meirbek, Nitin Yarlagadda, Kevin Zhu, Chris Kim*

**Main category:** cs.CL

**Keywords:** Theory of Mind, Large Language Models, Benchmark, Social Cognition, Evaluation Metrics

**Relevance Score:** 8

**TL;DR:** UniToMBench is a benchmark designed to improve and evaluate the Theory of Mind capabilities in large language models (LLMs), demonstrating both their strengths and limitations through various task evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges LLMs face in accurately predicting human mental states, this work introduces a unified benchmark to systematically assess and enhance ToM capabilities in LLMs.

**Method:** The paper presents UniToMBench, which combines multi-interaction task designs with evolving story scenarios, evaluated using a custom dataset of over 1,000 scenarios. It employs perspective-taking techniques and a variety of metrics for assessment.

**Key Contributions:**

	1. Introduction of UniToMBench for assessing ToM in LLMs
	2. Integration of multi-interaction task designs
	3. Development of a custom dataset with over 1,000 scenarios

**Result:** Evaluation shows LLMs like GPT-4o perform with high accuracy (above 80%) in emotional and belief-related tasks but demonstrate notable variability in knowledge-based tasks.

**Limitations:** The variability in LLM performance across knowledge-based tasks suggests further research is needed to enhance capabilities in this area.

**Conclusion:** UniToMBench serves as a comprehensive evaluation tool that highlights the strengths and limitations of LLMs in Theory of Mind tasks, paving the way for future improvements.

**Abstract:** Theory of Mind (ToM), the ability to understand the mental states of oneself and others, remains a challenging area for large language models (LLMs), which often fail to predict human mental states accurately. In this paper, we introduce UniToMBench, a unified benchmark that integrates the strengths of SimToM and TOMBENCH to systematically improve and assess ToM capabilities in LLMs by integrating multi-interaction task designs and evolving story scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios, UniToMBench combines perspective-taking techniques with diverse evaluation metrics to better stimulate social cognition in LLMs. Through evaluation, we observe that while models like GPT-4o and GPT-4o Mini show consistently high accuracy in tasks involving emotional and belief-related scenarios, with results usually above 80%, there is significant variability in their performance across knowledge-based tasks. These results highlight both the strengths and limitations of current LLMs in ToM-related tasks, underscoring the value of UniToMBench as a comprehensive tool for future development. Our code is publicly available here: https://github.com/Shamant/unifiedtombenchmark.

</details>


### [42] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)

*Zeguan Xiao, Yun Chen, Guanhua Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Direct Preference Optimization, Human Feedback, Training Algorithms

**Relevance Score:** 9

**TL;DR:** The paper presents Prefix-Oriented Equal-length Training (POET) to address the reward-generation gap in Direct Alignment Algorithms for optimizing large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Direct Alignment Algorithms (DAAs) like DPO and SimPO face a challenge known as the 'reward-generation gap', where training objectives do not align with generation performance.

**Method:** The authors introduce Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to equal lengths, thus enhancing the optimization process across all generation positions.

**Key Contributions:**

	1. Introduction of Prefix-Oriented Equal-length Training (POET)
	2. Empirical evidence that POET remedies the reward-generation gap
	3. Demonstrated performance improvements on standard alignment tasks

**Result:** Experiments demonstrate that POET significantly improves performance of DAAs, resulting in up to 15.6 points improvement in AlpacaEval 2 and better outcomes on downstream tasks.

**Limitations:** 

**Conclusion:** Addressing the reward-generation gap can improve DAAs' alignment with human preferences during language model training.

**Abstract:** Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO), have emerged as efficient alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms for aligning large language models (LLMs) with human preferences. However, DAAs suffer from a fundamental limitation we identify as the "reward-generation gap" -- a misalignment between optimization objectives during training and actual generation performance during inference. In this paper, we find a contributor to the reward-generation gap is the mismatch between the inherent importance of prefix tokens during the LLM generation process and how this importance is reflected in the implicit reward functions of DAAs. To bridge the gap, we introduce a simple yet effective approach called Prefix-Oriented Equal-length Training (POET), which truncates both preferred and dispreferred responses to match the shorter one's length. Training with POET, where both responses in each sample are truncated to equal length, resulting in diverse truncated lengths across samples, the optimization of DAAs objective is implicitly constrained to converge across all positions, thus paying more attention to prefix tokens than the standard DAAs. We conduct experiments with DPO and SimPO, two representative DAAs, demonstrating that POET improves over their standard implementations, achieving up to 15.6 points in AlpacaEval 2 and overall improvements across downstream tasks. Our results highlight the importance of addressing the misalignment between reward optimization and generation performance in DAAs.

</details>


### [43] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)

*Ilanit Sobol, Shir Lissak, Refael Tikochinski, Tal Nakash, Anat Brunstein Klomek, Eyal Fruchter, Roi Reichart*

**Main category:** cs.CL

**Keywords:** suicidality, YouTube, topic modeling, mental health, behavioral indicators

**Relevance Score:** 8

**TL;DR:** This study examines how suicidal behaviors manifest on YouTube by analyzing videos from individuals who attempted suicide, using a mix of computational methods and expert insights.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the manifestation of suicidal behaviors on social media, particularly YouTube, and to compare these behavioral indicators with expert knowledge.

**Method:** A longitudinal analysis of 181 YouTube channels from individuals with life-threatening attempts and 134 control channels, employing LLM-based topic modeling, hybrid expert review, and psychological assessment.

**Key Contributions:**

	1. Identified novel behavioral indicators of suicidality from social media data.
	2. Demonstrated the value of LLM-based topic modeling in mental health research.
	3. Showed different motivations for sharing experiences around suicide attempts on social media.

**Result:** Identified five topics associated with suicide attempts, with notable changes over time in 'Mental Health Struggles' and 'YouTube Engagement'. Experts confirmed some topics but missed key platform-specific indicators like YouTube Engagement, revealing the importance of computational approaches.

**Limitations:** The study focuses on a specific platform (YouTube) and may not generalize to other social media channels.

**Conclusion:** The integration of computational and expert-driven methods provides a nuanced understanding of suicidality, highlighting discrepancies between social media behavior and clinical insights.

**Abstract:** Suicide remains a leading cause of death in Western countries, underscoring the need for new research approaches. As social media becomes central to daily life, digital footprints offer valuable insight into suicidal behavior. Focusing on individuals who attempted suicide while uploading videos to their channels, we investigate: How do suicidal behaviors manifest on YouTube, and how do they differ from expert knowledge? We applied complementary approaches: computational bottom-up, hybrid, and expert-driven top-down, on a novel longitudinal dataset of 181 YouTube channels from individuals with life-threatening attempts, alongside 134 control channels. In the bottom-up approach, we applied LLM-based topic modeling to identify behavioral indicators. Of 166 topics, five were associated with suicide-attempt, with two also showing temporal attempt-related changes ($p<.01$) - Mental Health Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach, a clinical expert reviewed LLM-derived topics and flagged 19 as suicide-related. However, none showed significant attempt-related temporal effects beyond those identified bottom-up. Notably, YouTube Engagement, a platform-specific indicator, was not flagged by the expert, underscoring the value of bottom-up discovery. In the top-down approach, psychological assessment of suicide attempt narratives revealed that the only significant difference between individuals who attempted before and those attempted during their upload period was the motivation to share this experience: the former aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these approaches, we offer a nuanced understanding of suicidality, bridging digital behavior and clinical insights.   * Within-group changes in relation to the suicide attempt.

</details>


### [44] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)

*Jiayi Yuan, Hao Li, Xinheng Ding, Wenya Xie, Yu-Jhe Li, Wentian Zhao, Kun Wan, Jing Shi, Xia Hu, Zirui Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, reproducibility, numerical precision, inference pipeline, LayerCast

**Relevance Score:** 8

**TL;DR:** This paper explores the fragility of LLM performance reproducibility due to changes in system configuration affecting numerical precision.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how changes in hardware and software configurations impact the reproducibility and accuracy of LLM responses during evaluation.

**Method:** Conducted controlled experiments testing various configurations like GPU type, count, and evaluation batch size to measure their impact on output divergence in LLMs.

**Key Contributions:**

	1. Systematic analysis of floating-point precision impacts on LLM reproducibility
	2. Quantification of response variability based on configuration changes
	3. Development of LayerCast for improved inference with better numerical stability

**Result:** Found that small changes in configuration can lead to significant variability in accuracy (up to 9%) and response length (up to 9,000 tokens).

**Limitations:** The experiments are limited to specific model versions and may not generalize across all LLMs.

**Conclusion:** The research underscores the importance of numerical precision and offers LayerCast, an inference pipeline that maintains stability while optimizing memory usage.

**Abstract:** Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration such as evaluation batch size, GPU count, and GPU version can introduce significant difference in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision -- while critical for reproducibility -- is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility.

</details>


### [45] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)

*Bingheng Wu, Jingze Shi, Yifan Wu, Nan Tang, Yuyu Luo*

**Main category:** cs.CL

**Keywords:** Transformers, State Space Models, Positional Encoding, Hybrid Architecture, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper proposes 	extbf{ourRoPE}, a unified positional encoding method to integrate Transformers and State Space Models (SSMs), leading to a hybrid architecture called 	extbf{model} that improves training speed and accuracy for long-sequence tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of Transformers and State Space Models is hindered by differing positional encoding methods, affecting performance.

**Method:** The authors introduce a unified rotary position embedding method, 	extbf{ourRoPE}, to create a hybrid architecture named 	extbf{model} which combines Transformer and SSM layers.

**Key Contributions:**

	1. Unified positional encoding that bridges Transformers and SSMs
	2. A hybrid architecture (	extbf{model}) enhancing speed and accuracy
	3. Demonstrated superior scaling capabilities for larger models

**Result:** The 	extbf{model} exhibits 42.3% faster training and 29.5% faster inference than standard Transformer models and surpasses a Transformer baseline by over 4% in accuracy on language modeling benchmarks.

**Limitations:** 

**Conclusion:** The proposed unified positional encoding approach effectively resolves compatibility issues in hybrid models, allowing for enhanced long-context modeling.

**Abstract:** Transformers exhibit proficiency in capturing long-range dependencies, whereas State Space Models (SSMs) facilitate linear-time sequence modeling. Notwithstanding their synergistic potential, the integration of these architectures presents a significant challenge, primarily attributable to a fundamental incongruity in their respective positional encoding mechanisms: Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs leverage implicit positional representations via convolutions. This divergence often precipitates discontinuities and suboptimal performance. To address this impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE}) methodology, thereby establishing a consistent positional encoding framework for both self-attention and state-space components. Using this \ourRoPE, we introduce \textbf{\model}, a hybrid architecture that coherently integrates the Transformer and SSM layers under this unified positional encoding scheme. At a 4K sequence length, \model exhibits training and inference speeds that are \textbf{42.3\% and 29.5\% faster}, respectively, relative to standard Transformer models. It also delivers higher accuracy: under comparable settings, it surpasses a Transformer baseline by over 4\% on language modeling benchmarks. \model furthermore scales more effectively: \model-1.3B gains \textbf{7.22\%} in average accuracy over its 320M version (versus about 6\% gains for equivalent Transformers or SSMs). Our results show that unified positional encoding resolves positional incompatibility in hybrid models, enabling efficient, high-performance long-context modeling.

</details>


### [46] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)

*Yu Sun, Xingyu Qian, Weiwen Xu, Hao Zhang, Chenghao Xiao, Long Li, Yu Rong, Wenbing Huang, Qifeng Bai, Tingyang Xu*

**Main category:** cs.CL

**Keywords:** medical reasoning, large language models, dataset, Chain-of-Thought, benchmark

**Relevance Score:** 9

**TL;DR:** ReasonMed is the largest medical reasoning dataset, designed to enhance reasoning-based LLMs' performance in medical question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge-intensive medical question answering using reasoning-based LLMs, as their capabilities in this area have not been fully explored.

**Method:** Introduction of ReasonMed, a dataset containing 370k high-quality examples, constructed through a multi-agent verification and refinement process with an Error Refiner.

**Key Contributions:**

	1. Introduction of the ReasonMed dataset
	2. Development of a multi-agent verification and refinement process
	3. Establishment of a new benchmark for sub-10B models in medical reasoning

**Result:** ReasonMed-7B, a model trained on this dataset, outperforms the previous best sub-10B models and even exceeds the performance of LLaMA3.1-70B on PubMedQA.

**Limitations:** 

**Conclusion:** Combining detailed Chain-of-Thought reasoning with concise answer summaries is the most effective strategy for training medical reasoning models, setting new performance benchmarks.

**Abstract:** Though reasoning-based large language models (LLMs) have excelled in mathematics and programming, their capabilities in knowledge-intensive medical question answering remain underexplored. To address this, we introduce ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality examples distilled from 1.7 million initial reasoning paths generated by various LLMs. ReasonMed is constructed through a \textit{multi-agent verification and refinement process}, where we design an \textit{Error Refiner} to enhance the reasoning paths by identifying and correcting error-prone steps flagged by a verifier. Leveraging ReasonMed, we systematically investigate best practices for training medical reasoning models and find that combining detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields the most effective fine-tuning strategy. Based on this strategy, we train ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [47] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)

*Dingjun Wu, Yukun Yan, Zhenghao Liu, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Knowledge Graphs, Spreading Activation, Natural Language Processing, Question Answering

**Relevance Score:** 9

**TL;DR:** The paper introduces KG-Infused RAG, a framework that enhances Retrieval-Augmented Generation (RAG) by integrating knowledge graphs (KGs) to improve factual accuracy and multi-source retrieval.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the factual accuracy and relevance of responses generated by RAG systems, which traditionally rely on either unstructured text or structured knowledge without cognitive mechanisms for activating relevant information.

**Method:** KG-Infused RAG integrates knowledge graphs into the RAG framework to implement spreading activation for better concept association and inference, enhancing query expansion and generation processes.

**Key Contributions:**

	1. Introduction of a knowledge graph integration into RAG systems.
	2. Implementation of spreading activation for cognitive-inspired retrieval.
	3. Demonstration of performance gains in QA benchmarks over vanilla RAG.

**Result:** Experiments demonstrate that KG-Infused RAG consistently outperforms traditional RAG methods across five QA benchmarks by significant margins.

**Limitations:** 

**Conclusion:** KG-Infused RAG proves effective as a plug-and-play enhancement for corpus-based RAG methods, offering improved performance and interpretability in multi-source retrieval tasks.

**Abstract:** Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding responses in external knowledge. However, existing methods typically rely on a single source, either unstructured text or structured knowledge. Moreover, they lack cognitively inspired mechanisms for activating relevant knowledge. To address these issues, we propose KG-Infused RAG, a framework that integrates KGs into RAG systems to implement spreading activation, a cognitive process that enables concept association and inference. KG-Infused RAG retrieves KG facts, expands the query accordingly, and enhances generation by combining corpus passages with structured facts, enabling interpretable, multi-source retrieval grounded in semantic structure. We further improve KG-Infused RAG via preference learning on sampled key stages in the pipeline. Experiments on five QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by 3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG brings further performance gains, demonstrating its effectiveness and versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [48] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)

*Georgios Chatzichristodoulou, Despoina Kosmopoulou, Antonios Kritikos, Anastasia Poulopoulou, Efthymios Georgiou, Athanasios Katsamanis, Vassilis Katsouros, Alexandros Potamianos*

**Main category:** cs.CL

**Keywords:** emotion recognition, multimodal framework, DeepSER, naturalistic conditions, machine learning

**Relevance Score:** 7

**TL;DR:** MEDUSA is a multimodal framework designed for emotion recognition that addresses class imbalance and emotion ambiguity, including a four-stage training pipeline and human annotation integration.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of emotion recognition systems in naturalistic conditions where human emotions are subjectively interpreted and unevenly represented.

**Method:** The framework consists of a four-stage training process using an ensemble of classifiers, where DeepSER leverages pretrained representations. Manifold MixUp regularizes the model, and a meta-classifier merges ensemble predictions while incorporating human annotations as soft targets.

**Key Contributions:**

	1. Introduced the MEDUSA framework for multimodal emotion recognition.
	2. Utilized DeepSER as a novel approach combining acoustic and linguistic representations.
	3. Incorporated human annotation scores and balanced data sampling for improved performance.

**Result:** MEDUSA achieved first place in the Categorical Emotion Recognition task at the Interspeech 2025 challenge, demonstrating its effectiveness in handling emotionally ambiguous data.

**Limitations:** 

**Conclusion:** The proposed framework outperforms existing methods in recognizing emotions from speech under challenging conditions, suggesting its potential for improving HCI applications related to emotion understanding.

**Abstract:** SER is a challenging task due to the subjective nature of human emotions and their uneven representation under naturalistic conditions. We propose MEDUSA, a multimodal framework with a four-stage training pipeline, which effectively handles class imbalance and emotion ambiguity. The first two stages train an ensemble of classifiers that utilize DeepSER, a novel extension of a deep cross-modal transformer fusion mechanism from pretrained self-supervised acoustic and linguistic representations. Manifold MixUp is employed for further regularization. The last two stages optimize a trainable meta-classifier that combines the ensemble predictions. Our training approach incorporates human annotation scores as soft targets, coupled with balanced data sampling and multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic Conditions Challenge.

</details>


### [49] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)

*Eleni Gkovedarou, Joke Daems, Luna De Bruyne*

**Main category:** cs.CL

**Keywords:** gender bias, machine translation, GPT-4o, HCI, gender inclusivity

**Relevance Score:** 8

**TL;DR:** This study examines gender bias in machine translation systems Google Translate and DeepL within English-to-Greek translations, and evaluates GPT-4o as a potential bias mitigation tool.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how machine translation systems reinforce gender stereotypes and to explore alternatives for gender-inclusive translations.

**Method:** The study analyzes gender bias by assessing male bias, occupational stereotyping, and errors in anti-stereotypical translations, using the newly developed GendEL bilingual dataset of 240 sentences.

**Key Contributions:**

	1. Introduction of GendEL dataset for gender bias analysis
	2. Analysis of gender bias in commercial MT systems for a less studied language pair
	3. Evaluation of GPT-4o's effectiveness in bias mitigation

**Result:** Findings reveal persistent gender bias in both Google Translate and DeepL, with DeepL being more effective in translating unambiguous feminine sentences. GPT-4o shows potential in providing appropriate gendered and neutral alternatives, yet biases still persist.

**Limitations:** The study focuses on a specific language pair and may not generalize to other languages or MT systems.

**Conclusion:** While commercial MT systems exhibit persistent gender biases, GPT-4o offers promising results in generating gender-inclusive translations, highlighting the need for further development in this area.

**Abstract:** As the demand for inclusive language increases, concern has grown over the susceptibility of machine translation (MT) systems to reinforce gender stereotypes. This study investigates gender bias in two commercial MT systems, Google Translate and DeepL, focusing on the understudied English-to-Greek language pair. We address three aspects of gender bias: i) male bias, ii) occupational stereotyping, and iii) errors in anti-stereotypical translations. Additionally, we explore the potential of prompted GPT-4o as a bias mitigation tool that provides both gender-explicit and gender-neutral alternatives when necessary. To achieve this, we introduce GendEL, a manually crafted bilingual dataset of 240 gender-ambiguous and unambiguous sentences that feature stereotypical occupational nouns and adjectives. We find persistent gender bias in translations by both MT systems; while they perform well in cases where gender is explicitly defined, with DeepL outperforming both Google Translate and GPT-4o in feminine gender-unambiguous sentences, they are far from producing gender-inclusive or neutral translations when the gender is unspecified. GPT-4o shows promise, generating appropriate gendered and neutral alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [50] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)

*Stefan Krsteski, Matea Tashkovska, Borjan Sazdov, Hristijan Gjoreski, Branislav Gerazov*

**Main category:** cs.CL

**Keywords:** Large Language Models, Macedonian, Natural Language Processing, Culturally Grounded Datasets, LLM Evaluation

**Relevance Score:** 8

**TL;DR:** This paper presents novel resources and a state-of-the-art 8B-parameter model for Large Language Models (LLMs) tailored for the Macedonian language, achieving significant performance improvements over existing models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To support low-resource languages like Macedonian in leveraging Large Language Models (LLMs) for various applications.

**Method:** The paper details the collection of a large Macedonian corpus, the creation of a culturally grounded instruction dataset, construction of an evaluation suite, and training of a robust 8B-parameter LLM called domestic-yak.

**Key Contributions:**

	1. Creation of the largest Macedonian text corpus (40GB, 3.5B words).
	2. Development of a culturally grounded instruction dataset with 106k instances.
	3. Training and evaluation of the domestic-yak model, outperforming existing models.

**Result:** The domestic-yak model outperforms all existing 8B models across various benchmarks and matches performance of models up to 10x its size. It is preferred by native speakers for grammatical correctness and cultural relevance.

**Limitations:** 

**Conclusion:** The resources including datasets, models, and code are openly released to facilitate future research and development in underrepresented languages, specifically Macedonian.

**Abstract:** The increase in technological adoption worldwide comes with demands for novel tools to be used by the general population. Large Language Models (LLMs) provide a great opportunity in this respect, but their capabilities remain limited for low-resource languages, restricting applications in countries where such languages are spoken. We create several resources to facilitate the adoption of LLMs and to support research advancements for Macedonian. We collect the largest Macedonian corpus to date, consisting of 40GB of textual data and totaling 3.5B words. To support conversational applications, we collect a 106k-instance instruction dataset, carefully built to be culturally grounded. For evaluation, we construct a Macedonian evaluation suite covering seven benchmarks. Finally, we train domestic-yak, a state-of-the-art 8B-parameter model, on our curated datasets and evaluate it against eight baseline models using the newly constructed benchmark suite. Our model outperforms all existing models in the 8B parameter range across all benchmarks, and achieves performance comparable to models up to 10x larger. Furthermore, a qualitative analysis with native speakers reveals that our model is preferred over larger counterparts, receiving higher ratings for grammatical correctness and cultural appropriateness. All datasets, code, and model weights are openly released, setting a foundation for advancing LLMs in similarly underrepresented languages. These resources are publicly available at github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained model weights and data.

</details>


### [51] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)

*Blaž Škrlj, Boshko Koloski, Senja Pollak, Nada Lavrač*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Large Language Models, factual grounding, reasoning capabilities, neuro-symbolic integration

**Relevance Score:** 8

**TL;DR:** This survey examines the integration of Knowledge Graphs into Large Language Models, categorizing approaches and identifying gaps in research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance factual grounding and reasoning capabilities of LLMs through integration with structured knowledge from KGs.

**Method:** Systematic examination and categorization of existing approaches related to KG-enhanced LLMs and LLM-augmented KGs.

**Key Contributions:**

	1. Unique emphasis on scalability and computational efficiency
	2. Identification of mutual benefits between KGs and LLMs
	3. Proposed future research directions for intelligent systems.

**Result:** Identification of critical gaps in the synergy between KGs and LLMs, and emphasis on scalability, computational efficiency, and data quality.

**Limitations:** 

**Conclusion:** Proposes future research directions like neuro-symbolic integration and ethical considerations for complex real-world knowledge tasks.

**Abstract:** Integrating structured knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) enhances factual grounding and reasoning capabilities. This survey paper systematically examines the synergy between KGs and LLMs, categorizing existing approaches into two main groups: KG-enhanced LLMs, which improve reasoning, reduce hallucinations, and enable complex question answering; and LLM-augmented KGs, which facilitate KG construction, completion, and querying. Through comprehensive analysis, we identify critical gaps and highlight the mutual benefits of structured knowledge integration. Compared to existing surveys, our study uniquely emphasizes scalability, computational efficiency, and data quality. Finally, we propose future research directions, including neuro-symbolic integration, dynamic KG updating, data reliability, and ethical considerations, paving the way for intelligent systems capable of managing more complex real-world knowledge tasks.

</details>


### [52] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)

*Stefan Arnold*

**Main category:** cs.CL

**Keywords:** Intrinsic Dimension, Language Models, memorization, latent space, complexity

**Relevance Score:** 6

**TL;DR:** This paper investigates how the Intrinsic Dimension of sequences in latent space affects the memorization rate of Language Models, finding that higher complexity leads to lower memorization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about privacy leakage and intellectual property in Language Models due to unintended memorization.

**Method:** The study examines the relationship between Intrinsic Dimension (ID) and memorization rates in Language Models, particularly focusing on overparameterized models and varying exposure levels.

**Key Contributions:**

	1. Introduces the role of Intrinsic Dimension in memorization rates of LMs.
	2. Highlights the interaction between model architecture and sequence complexity.
	3. Provides empirical evidence linking ID to reduced memorization in specific conditions.

**Result:** The research demonstrates that high-ID sequences are less likely to be memorized than low-ID sequences, especially in certain model configurations.

**Limitations:** 

**Conclusion:** The findings emphasize the importance of scale, exposure, and complexity in understanding memorization behavior in Language Models.

**Abstract:** Language Models (LMs) are prone to memorizing parts of their data during training and unintentionally emitting them at generation time, raising concerns about privacy leakage and disclosure of intellectual property. While previous research has identified properties such as context length, parameter size, and duplication frequency, as key drivers of unintended memorization, little is known about how the latent structure modulates this rate of memorization. We investigate the role of Intrinsic Dimension (ID), a geometric proxy for the structural complexity of a sequence in latent space, in modulating memorization. Our findings suggest that ID acts as a suppressive signal for memorization: compared to low-ID sequences, high-ID sequences are less likely to be memorized, particularly in overparameterized models and under sparse exposure. These findings highlight the interaction between scale, exposure, and complexity in shaping memorization.

</details>


### [53] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)

*Nicolas Audinet de Pieuchon, Adel Daoud, Connor T. Jerzak, Moa Johansson, Richard Johansson*

**Main category:** cs.CL

**Keywords:** Large Language Models, Debiasing Methods, Design-based Supervised Learning, Prediction-Powered Inference, Bias-Variance Tradeoff

**Relevance Score:** 8

**TL;DR:** This paper investigates two debiasing methods for annotations from large language models (LLMs), namely Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI), comparing their performance with expert annotations in finite sample sizes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to mitigate bias in LLM annotations to ensure valid estimation of population parameters in applied research contexts.

**Method:** We analyze the performance of DSL and PPI as the number of expert annotations varies, focusing on their effectiveness in practical scenarios with finite sample sizes.

**Key Contributions:**

	1. Analysis of the scaling performance of debiasing methods with expert annotations
	2. Comparison of DSL and PPI in terms of bias reduction and empirical efficiency
	3. Identification of bias-variance tradeoff in debiasing methods

**Result:** Both methods perform well with large datasets, but DSL often better reduces bias and shows higher empirical efficiency, despite being less consistent across various datasets compared to PPI.

**Limitations:** The study primarily focuses on known debiasing methods and may not account for emerging techniques or variations in diverse application contexts.

**Conclusion:** Our findings suggest a bias-variance tradeoff in debiasing methods that necessitates further investigation into metrics for assessing their performance in finite samples.

**Abstract:** Large language models (LLMs) offer an inexpensive yet powerful way to annotate text, but are often inconsistent when compared with experts. These errors can bias downstream estimates of population parameters such as regression coefficients and causal effects. To mitigate this bias, researchers have developed debiasing methods such as Design-based Supervised Learning (DSL) and Prediction-Powered Inference (PPI), which promise valid estimation by combining LLM annotations with a limited number of expensive expert annotations. Although these methods produce consistent estimates under theoretical assumptions, it is unknown how they compare in finite samples of sizes encountered in applied research. We make two contributions: First, we study how each method's performance scales with the number of expert annotations, highlighting regimes where LLM bias or limited expert labels significantly affect results. Second, we compare DSL and PPI across a range of tasks, finding that although both achieve low bias with large datasets, DSL often outperforms PPI on bias reduction and empirical efficiency, but its performance is less consistent across datasets. Our findings indicate that there is a bias-variance tradeoff at the level of debiasing methods, calling for more research on developing metrics for quantifying their efficiency in finite samples.

</details>


### [54] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)

*Anna Stein, Kevin Tang*

**Main category:** cs.CL

**Keywords:** acoustic modeling, predictability, information theory, N-gram, Naive Discriminative Learning

**Relevance Score:** 3

**TL;DR:** This study evaluates probabilistic predictors for modeling acoustic word duration, revealing that an N-gram model outperforms Naive Discriminative Learning (NDL) models, but that NDL can be enhanced with information-theoretic approaches.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of different probabilistic models in predicting acoustic word duration, particularly in the context of probabilistic reduction.

**Method:** Three models were compared: one using NDL-derived predictors with information-theoretic formulas, one with standard NDL predictors, and another with N-gram probabilistic predictors, utilizing the Buckeye corpus for data.

**Key Contributions:**

	1. N-gram model outperforms Naive Discriminative Learning (NDL) models in predicting acoustic duration.
	2. Incorporating information-theoretic measures enhances the performance of NDL predictors.
	3. Highlights the importance of considering various predictability metrics.

**Result:** The N-gram model demonstrated superior performance over both types of NDL models, challenging the view that NDL's cognitive motivation yields better outcomes. However, integrating information-theoretic formulas with NDL approaches led to improved performance when compared to traditional NDL.

**Limitations:** 

**Conclusion:** The findings underscore the necessity of integrating frequency, contextual predictability, and average contextual predictability alongside information-theoretic metrics in acoustic modeling practices.

**Abstract:** This study compares probabilistic predictors based on information theory with Naive Discriminative Learning (NDL) predictors in modeling acoustic word duration, focusing on probabilistic reduction. We examine three models using the Buckeye corpus: one with NDL-derived predictors using information-theoretic formulas, one with traditional NDL predictors, and one with N-gram probabilistic predictors. Results show that the N-gram model outperforms both NDL models, challenging the assumption that NDL is more effective due to its cognitive motivation. However, incorporating information-theoretic formulas into NDL improves model performance over the traditional model. This research highlights a) the need to incorporate not only frequency and contextual predictability but also average contextual predictability, and b) the importance of combining information-theoretic metrics of predictability and information derived from discriminative learning in modeling acoustic reduction.

</details>


### [55] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)

*Harry Walsh, Maksym Ivashechkin, Richard Bowden*

**Main category:** cs.CL

**Keywords:** sign language, machine learning, data augmentation, translation models, generative models

**Relevance Score:** 7

**TL;DR:** This paper presents methods to enhance sign language datasets and improve translation models using advanced techniques in sign language production.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of acquiring sufficient high-quality data for machine learning models, particularly in the context of low-resource languages like sign languages.

**Method:** The study employs a skeleton-based approach, sign stitching, and two generative models (SignGAN and SignSplat) to augment sign language datasets for translation tasks.

**Key Contributions:**

	1. Introduction of a skeleton-based production approach for sign languages
	2. Implementation of sign stitching for enhanced training data
	3. Utilization of photo-realistic generative models to aid dataset augmentation

**Result:** The techniques proposed improve the performance of Sign Language Translation models by up to 19%.

**Limitations:** 

**Conclusion:** The findings highlight that the methods can bolster existing datasets and lead to more accurate Sign Language Translation systems in low-resource settings.

**Abstract:** Machine learning models fundamentally rely on large quantities of high-quality data. Collecting the necessary data for these models can be challenging due to cost, scarcity, and privacy restrictions. Signed languages are visual languages used by the deaf community and are considered low-resource languages. Sign language datasets are often orders of magnitude smaller than their spoken language counterparts. Sign Language Production is the task of generating sign language videos from spoken language sentences, while Sign Language Translation is the reverse translation task. Here, we propose leveraging recent advancements in Sign Language Production to augment existing sign language datasets and enhance the performance of Sign Language Translation models. For this, we utilize three techniques: a skeleton-based approach to production, sign stitching, and two photo-realistic generative models, SignGAN and SignSplat. We evaluate the effectiveness of these techniques in enhancing the performance of Sign Language Translation models by generating variation in the signer's appearance and the motion of the skeletal data. Our results demonstrate that the proposed methods can effectively augment existing datasets and enhance the performance of Sign Language Translation models by up to 19%, paving the way for more robust and accurate Sign Language Translation systems, even in resource-constrained environments.

</details>


### [56] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)

*Tianjun Yao, Haoxuan Li, Zhiqiang Shen, Pan Li, Tongliang Liu, Kun Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Retrieval-Augmented Generation, Question Answering, Graph Retrieval

**Relevance Score:** 9

**TL;DR:** This paper presents RAPL, a novel framework for improving graph retrieval in knowledge graph question answering (KGQA) by addressing challenges in generalization and interpretability using a two-stage labeling strategy, model-agnostic graph transformation, and path-based reasoning.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models (LLMs) often struggle with outdated knowledge and hallucinations, and existing Retrieval-Augmented Generation (RAG) methods primarily utilize unstructured text, affecting interpretability and reasoning.

**Method:** RAPL incorporates a two-stage labeling strategy that merges heuristic signals with parametric models, a model-agnostic graph transformation approach for capturing interactions, and employs a path-based reasoning strategy for enhanced learning.

**Key Contributions:**

	1. Introduction of a two-stage labeling strategy combining heuristic signals and parametric models.
	2. A model-agnostic approach to graph transformation for enhanced representational capacity.
	3. Development of a path-based reasoning strategy that supports structured input for downstream reasoning.

**Result:** Empirical results show that RAPL outperforms state-of-the-art methods by 2.66%-20.34% and reduces the performance gap for smaller LLMs in cross-dataset settings, indicating improved retrieval capabilities and generalizability.

**Limitations:** 

**Conclusion:** RAPL presents a more effective method for graph retrieval in KGQA, enhancing the performance of LLMs by providing structured reasoning and improved interpretability.

**Abstract:** Large Language Models (LLMs) have shown strong inductive reasoning ability across various domains, but their reliability is hindered by the outdated knowledge and hallucinations. Retrieval-Augmented Generation mitigates these issues by grounding LLMs with external knowledge; however, most existing RAG pipelines rely on unstructured text, limiting interpretability and structured reasoning. Knowledge graphs, which represent facts as relational triples, offer a more structured and compact alternative. Recent studies have explored integrating knowledge graphs with LLMs for knowledge graph question answering (KGQA), with a significant proportion adopting the retrieve-then-reasoning paradigm. In this framework, graph-based retrievers have demonstrated strong empirical performance, yet they still face challenges in generalization ability. In this work, we propose RAPL, a novel framework for efficient and effective graph retrieval in KGQA. RAPL addresses these limitations through three aspects: (1) a two-stage labeling strategy that combines heuristic signals with parametric models to provide causally grounded supervision; (2) a model-agnostic graph transformation approach to capture both intra- and inter-triple interactions, thereby enhancing representational capacity; and (3) a path-based reasoning strategy that facilitates learning from the injected rational knowledge, and supports downstream reasoner through structured inputs. Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and significantly reduces the performance gap between smaller and more powerful LLM-based reasoners, as well as the gap under cross-dataset settings, highlighting its superior retrieval capability and generalizability. Codes are available at: https://github.com/tianyao-aka/RAPL.

</details>


### [57] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)

*Nikolas Evkarpidi, Elena Tutubalina*

**Main category:** cs.CL

**Keywords:** Question Answering, tabular data, large language models, RAG, NLP

**Relevance Score:** 8

**TL;DR:** This paper presents a system for Question Answering over tabular data, achieving 80% accuracy and ranking top-13 in SemEval 2025.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Question Answering (QA) accuracy over tabular data using advanced techniques.

**Method:** The system integrates text-to-SQL, text-to-code generation modules, a self-correction mechanism, and retrieval-augmented generation, all orchestrated by a large language model.

**Key Contributions:**

	1. Integration of diverse components for QA over tables
	2. High accuracy achieved with open-source models
	3. Availability of code for further research

**Result:** Achieved an accuracy of 80% during evaluation, ranking top-13 out of 38 teams in the competition.

**Limitations:** Challenges in the field of QA over tabular data were identified.

**Conclusion:** The developed pipeline shows significant improvements for open-source models and matches proprietary LLM performance in QA tasks.

**Abstract:** This paper presents a system developed for SemEval 2025 Task 8: Question Answering (QA) over tabular data. Our approach integrates several key components: text-to-SQL and text-to-code generation modules, a self-correction mechanism, and a retrieval-augmented generation (RAG). Additionally, it includes an end-to-end (E2E) module, all orchestrated by a large language model (LLM). Through ablation studies, we analyzed the effects of different parts of our pipeline and identified the challenges that are still present in this field. During the evaluation phase of the competition, our solution achieved an accuracy of 80%, resulting in a top-13 ranking among the 38 participating teams. Our pipeline demonstrates a significant improvement in accuracy for open-source models and achieves a performance comparable to proprietary LLMs in QA tasks over tables. The code is available at GitHub repository.

</details>


### [58] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)

*Lihu Chen, Gaël Varoquaux*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Boundary Detection, Internal Confidence, Adaptive Inference, Recovery-Augmented Generation

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel method called Internal Confidence to detect knowledge boundaries in Large Language Models by assessing query-level uncertainty, aiming to improve adaptive inference and reduce costs in RAG processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability and efficiency of Large Language Models by making them aware of their knowledge boundaries and improving inference methods.

**Method:** The authors introduce Internal Confidence, a training-free approach that evaluates self-confidence across different layers and tokens to determine if a model can respond effectively to a query.

**Key Contributions:**

	1. Introduction of the Internal Confidence method for query-level uncertainty detection
	2. Demonstration of enhanced performance in factual QA and mathematical reasoning
	3. Application of the method for efficient RAG and model cascading to lower inference costs.

**Result:** Empirical results indicate that Internal Confidence outperforms several established baselines in both factual question answering and mathematical reasoning tasks.

**Limitations:** 

**Conclusion:** The method offers a viable solution for efficient retrieval-augmented generation and can help reduce inference costs while preserving model performance.

**Abstract:** It is important for Large Language Models to be aware of the boundary of their knowledge, the mechanism of identifying known and unknown queries. This type of awareness can help models perform adaptive inference, such as invoking RAG, engaging in slow and deep thinking, or adopting the abstention mechanism, which is beneficial to the development of efficient and trustworthy AI. In this work, we propose a method to detect knowledge boundaries via Query-Level Uncertainty, which aims to determine if the model is able to address a given query without generating any tokens. To this end, we introduce a novel and training-free method called \emph{Internal Confidence}, which leverages self-evaluations across layers and tokens. Empirical results on both factual QA and mathematical reasoning tasks demonstrate that our internal confidence can outperform several baselines. Furthermore, we showcase that our proposed method can be used for efficient RAG and model cascading, which is able to reduce inference costs while maintaining performance.

</details>


### [59] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)

*Hao Xiong, Chuanyuan Tan, Wenliang Chen*

**Main category:** cs.CL

**Keywords:** Unstructured Knowledge Editing, Fine-tuning, Large Language Models, Evaluation Datasets, Performance Optimization

**Relevance Score:** 9

**TL;DR:** This paper addresses issues in Unstructured Knowledge Editing (UKE) for large language models (LLMs) by introducing new evaluation datasets and optimizing fine-tuning methods, resulting in improved performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance Unstructured Knowledge Editing (UKE) for large language models by addressing locality evaluation issues and the shortcomings of fine-tuning (FT) methods.

**Method:** The authors constructed two evaluation datasets, UnKEBench-Loc and AKEW-Loc, and conducted experiments to determine optimal training settings for FT-based methods in UKE tasks.

**Key Contributions:**

	1. Introduction of evaluation datasets for UKE
	2. Identification of factors influencing FT-based methods
	3. Demonstration of FT-UKE's superior performance over SOTA methods

**Result:** FT-UKE, the optimized fine-tuning method, outperformed the state-of-the-art methods, with performance improvements increasing with larger batch sizes (up to +10.80%).

**Limitations:** 

**Conclusion:** The paper provides a systematic evaluation of UKE and a training recipe for fine-tuning methods, suggesting significant performance gains and directions for future research.

**Abstract:** Unstructured Knowledge Editing (UKE) is crucial for updating the relevant knowledge of large language models (LLMs). It focuses on unstructured inputs, such as long or free-form texts, which are common forms of real-world knowledge. Although previous studies have proposed effective methods and tested them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2) Abnormal failure of fine-tuning (FT) based methods for UKE. To address these issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by extending two existing UKE datasets with locality test data from the unstructured and structured views. This enables a systematic evaluation of the Locality of post-edited models. Furthermore, we identify four factors that may affect the performance of FT-based methods. Based on these factors, we conduct experiments to determine how the well-performing FT-based methods should be trained for the UKE task, providing a training recipe for future research. Our experimental results indicate that the FT-based method with the optimal setting (FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art (SOTA). In batch editing scenarios, FT-UKE shows strong performance as well, with its advantage over SOTA methods increasing as the batch size grows, expanding the average metric lead from +6.78% to +10.80%

</details>


### [60] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)

*Haoyi Song, Ruihan Ji, Naichen Shi, Fan Lai, Raed Al Kontar*

**Main category:** cs.CL

**Keywords:** large language models, uncertainty quantification, inv-entropy, Markov chains, genetic algorithms

**Relevance Score:** 9

**TL;DR:** This paper presents a theoretical framework for uncertainty quantification in large language models (LLMs) using perturbations, introducing a new measure called Inv-Entropy and a genetic algorithm-based perturbation algorithm.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Effective uncertainty quantification (UQ) is crucial for the reliable deployment of large language models, yet existing methods often lack a strong probabilistic foundation.

**Method:** The paper proposes a dual random walk model for input-output pairs, utilizing Markov chains to define transition probabilities based on semantic similarity, and introduces an inverse model to evaluate input space diversity for quantifying uncertainty.

**Key Contributions:**

	1. Introduced a probabilistic framework for uncertainty quantification in LLMs based on perturbations.
	2. Developed a new uncertainty measure called Inv-Entropy.
	3. Proposed GAAP, a genetic algorithm-based perturbation method to improve input diversity.

**Result:** Extensive experiments show that the proposed measure Inv-Entropy outperforms traditional semantic uncertainty quantification methods.

**Limitations:** 

**Conclusion:** The framework allows for flexible definitions of uncertainty, embedding strategies, and evaluation metrics, significantly enhancing the robustness of uncertainty quantification in LLMs.

**Abstract:** Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a probabilistic foundation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input-output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [61] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)

*Zhenran Xu, Yiyu Wang, Xue Yang, Longyue Wang, Weihua Luo, Kaifu Zhang, Baotian Hu, Min Zhang*

**Main category:** cs.CL

**Keywords:** workflow generation, chain-of-thought reasoning, AI art, reinforcement learning, modular AI workflows

**Relevance Score:** 6

**TL;DR:** Introduction of ComfyUI-R1, a large reasoning model for automated workflow generation targeting AI-generated content customization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To simplify the crafting of effective AI workflows which presents a steep learning curve for users.

**Method:** Two-stage training framework consisting of CoT fine-tuning and reinforcement learning guided by a hybrid reward for workflow validation and integrity.

**Key Contributions:**

	1. Introduction of the first large reasoning model for automated workflow generation (ComfyUI-R1).
	2. Use of a two-stage training framework combining CoT fine-tuning and reinforcement learning.
	3. Demonstration of significant performance improvements over state-of-the-art models.

**Result:** ComfyUI-R1 achieves a 97% format validity rate, outperforming prior models in pass rates and F1 scores for workflow performance.

**Limitations:** 

**Conclusion:** The model underscores the potential of long chain-of-thought reasoning in synthesizing complex workflows for AI art creation.

**Abstract:** AI-generated content has evolved from monolithic models to modular workflows, particularly on platforms like ComfyUI, enabling customization in creative pipelines. However, crafting effective workflows requires great expertise to orchestrate numerous specialized components, presenting a steep learning curve for users. To address this challenge, we introduce ComfyUI-R1, the first large reasoning model for automated workflow generation. Starting with our curated dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning data, including node selection, workflow planning, and code-level workflow representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT fine-tuning for cold start, adapting models to the ComfyUI domain; (2) reinforcement learning for incentivizing reasoning capability, guided by a fine-grained rule-metric hybrid reward, ensuring format validity, structural integrity, and node-level fidelity. Experiments show that our 7B-parameter model achieves a 97\% format validity rate, along with high pass rate, node-level and graph-level F1 scores, significantly surpassing prior state-of-the-art methods that employ leading closed-source models such as GPT-4o and Claude series. Further analysis highlights the critical role of the reasoning process and the advantage of transforming workflows into code. Qualitative comparison reveals our strength in synthesizing intricate workflows with diverse nodes, underscoring the potential of long CoT reasoning in AI art creation.

</details>


### [62] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)

*Andreas Säuberli, Diego Frassinelli, Barbara Plank*

**Main category:** cs.CL

**Keywords:** Large Language Models, Educational Assessments, Item Response Theory, Test Validity, Psychometrics

**Relevance Score:** 8

**TL;DR:** This paper evaluates the suitability of large language models (LLMs) as pilot participants for educational test development by examining their response behavior to multiple-choice items.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient test development in education by potentially using LLMs to simulate human responses to evaluate item quality and test validity.

**Method:** Evaluation of 18 instruction-tuned LLMs against two datasets of multiple-choice test items across reading, U.S. history, and economics, using classical test theory and item response theory frameworks.

**Key Contributions:**

	1. Demonstrated the potential use of LLMs in educational test development
	2. Showed the effects of temperature scaling on model confidence
	3. Highlighted differential performance across subjects with respect to human correlation

**Result:** Larger models exhibit excessive confidence but display more human-like response distributions when calibrated. Correlation with human responses is stronger in reading comprehension, but overall correlations are weak.

**Limitations:** The correlations between LLMs and human responses are weak overall, indicating limitations in their use for test piloting.

**Conclusion:** LLMs are not suitable for piloting educational assessments in a zero-shot setting despite some similarity in response distribution.

**Abstract:** Knowing how test takers answer items in educational assessments is essential for test development, to evaluate item quality, and to improve test validity. However, this process usually requires extensive pilot studies with human participants. If large language models (LLMs) exhibit human-like response behavior to test items, this could open up the possibility of using them as pilot participants to accelerate test development. In this paper, we evaluate the human-likeness or psychometric plausibility of responses from 18 instruction-tuned LLMs with two publicly available datasets of multiple-choice test items across three subjects: reading, U.S. history, and economics. Our methodology builds on two theoretical frameworks from psychometrics which are commonly used in educational assessment, classical test theory and item response theory. The results show that while larger models are excessively confident, their response distributions can be more human-like when calibrated with temperature scaling. In addition, we find that LLMs tend to correlate better with humans in reading comprehension items compared to other subjects. However, the correlations are not very strong overall, indicating that LLMs should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [63] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)

*Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, Dayiheng Liu*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Mathematical Reasoning, Hint-Engineering

**Relevance Score:** 8

**TL;DR:** This paper presents CoRT, a post-training framework that enhances Large Reasoning Models' efficiency in mathematical reasoning by leveraging Code Interpreters through a novel data synthesis method called Hint-Engineering.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and accuracy of Large Reasoning Models in complex mathematical operations, addressing limitations when using external computational tools with these models.

**Method:** The authors develop CoRT, which synthesizes code-integrated reasoning data via Hint-Engineering, strategically inserting hints for better model-compiler interaction, and post-train LRM models ranging from 1.5B to 32B parameters.

**Key Contributions:**

	1. Introduction of CoRT framework for post-training LRMs
	2. Development of Hint-Engineering for synthesizing effective reasoning data
	3. Demonstrated improvements in efficiency and accuracy on mathematical reasoning tasks.

**Result:** Models enhanced by Hint-Engineering show 4% and 8% absolute improvements on mathematical reasoning datasets, with significantly reduced token usage during processing.

**Limitations:** This work is a progress report and may require further validation and testing.

**Conclusion:** The CoRT framework effectively enhances LRMs' capabilities in mathematical reasoning, making them more efficient in handling complex tasks by integrating external computational knowledge.

**Abstract:** Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable progress in natural language reasoning with long chain-of-thought (CoT), yet they remain inefficient or inaccurate when handling complex mathematical operations. Addressing these limitations through computational tools (e.g., computation libraries and symbolic solvers) is promising, but it introduces a technical challenge: Code Interpreter (CI) brings external knowledge beyond the model's internal text representations, thus the direct combination is not efficient. This paper introduces CoRT, a post-training framework for teaching LRMs to leverage CI effectively and efficiently. As a first step, we address the data scarcity issue by synthesizing code-integrated reasoning data through Hint-Engineering, which strategically inserts different hints at appropriate positions to optimize LRM-CI interaction. We manually create 30 high-quality samples, upon which we post-train models ranging from 1.5B to 32B parameters, with supervised fine-tuning, rejection fine-tuning and reinforcement learning. Our experimental results demonstrate that Hint-Engineering models achieve 4\% and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging mathematical reasoning datasets. Furthermore, Hint-Engineering models use about 30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model compared with the natural language models. The models and code are available at https://github.com/ChengpengLi1003/CoRT.

</details>


### [64] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)

*Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer*

**Main category:** cs.CL

**Keywords:** speech emotion recognition, EmoNet-Voice, synthetic audio, emotional understanding, benchmark dataset

**Relevance Score:** 7

**TL;DR:** EmoNet-Voice is a new resource for speech emotion detection featuring a large-scale pre-training dataset and a benchmark dataset, both designed to advance the capabilities of speech emotion recognition models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for robust benchmarks in text-to-speech and audio generation models to evaluate emotional understanding due to limitations in current SER datasets.

**Method:** Introduced EmoNet-Voice, including EmoNet-Voice Big with over 4,500 hours of speech across 11 voices and 40 emotions, and EmoNet-Voice Bench with expert annotations for evaluating SER models.

**Key Contributions:**

	1. Introduction of EmoNet-Voice, a comprehensive resource for SER evaluation
	2. Development of a large-scale dataset featuring diverse emotions and languages
	3. Empathic Insight Voice models improve SER accuracy with expert agreement

**Result:** EmoNet-Voice sets a new standard in speech emotion recognition, showing high agreement with human experts and revealing that high-arousal emotions are easier to detect than low-arousal states.

**Limitations:** 

**Conclusion:** The synthetic, privacy-preserving nature of EmoNet-Voice allows for the assessment of sensitive emotional states not captured by existing datasets, with validated results from psychology experts.

**Abstract:** The advancement of text-to-speech and audio generation models necessitates robust benchmarks for evaluating the emotional understanding capabilities of AI systems. Current speech emotion recognition (SER) datasets often exhibit limitations in emotional granularity, privacy concerns, or reliance on acted portrayals. This paper introduces EmoNet-Voice, a new resource for speech emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions, and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human expert annotations. EmoNet-Voice is designed to evaluate SER models on a fine-grained spectrum of 40 emotion categories with different levels of intensities. Leveraging state-of-the-art voice generation, we curated synthetic audio snippets simulating actors portraying scenes designed to evoke specific emotions. Crucially, we conducted rigorous validation by psychology experts who assigned perceived intensity labels. This synthetic, privacy-preserving approach allows for the inclusion of sensitive emotional states often absent in existing datasets. Lastly, we introduce Empathic Insight Voice models that set a new standard in speech emotion recognition with high agreement with human experts. Our evaluations across the current model landscape exhibit valuable findings, such as high-arousal emotions like anger being much easier to detect than low-arousal states like concentration.

</details>


### [65] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)

*Omar Sherif, Ali Hamdi*

**Main category:** cs.CL

**Keywords:** Rehabilitation, Pose Augmentation, Graph Convolutional Network, Movement Quality Assessment, Biomechanical Errors

**Relevance Score:** 8

**TL;DR:** The paper presents Error-Guided Pose Augmentation (EGPA), a method that enhances rehabilitation assessment by generating synthetic skeleton data targeting biomechanical errors, leading to improvements in error detection accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced in monitoring patient progress in rehabilitation, particularly in home-based settings, where existing systems struggle with data imbalance and subtle movement errors.

**Method:** The method involves generating synthetic skeleton data through Error-Guided Pose Augmentation (EGPA), paired with an attention-based graph convolutional network to improve movement quality assessment performance.

**Key Contributions:**

	1. Introduction of Error-Guided Pose Augmentation (EGPA) for rehabilitation assessment
	2. Demonstrated significant performance improvements in detecting biomechanical errors
	3. Enhanced interpretability through attention visualizations of significant joint movements.

**Result:** The approach yields reductions in mean absolute error by up to 27.6% and improvements in error classification accuracy by 45.8%.

**Limitations:** 

**Conclusion:** EGPA enhances both the accuracy and interpretability of automated movement quality assessment in clinical and home rehabilitation contexts, focusing on clinically significant movements.

**Abstract:** Effective rehabilitation assessment is essential for monitoring patient progress, particularly in home-based settings. Existing systems often face challenges such as data imbalance and difficulty detecting subtle movement errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method that generates synthetic skeleton data by simulating clinically relevant movement mistakes. Unlike standard augmentation techniques, EGPA targets biomechanical errors observed in rehabilitation. Combined with an attention-based graph convolutional network, EGPA improves performance across multiple evaluation metrics. Experiments demonstrate reductions in mean absolute error of up to 27.6 percent and gains in error classification accuracy of 45.8 percent. Attention visualizations show that the model learns to focus on clinically significant joints and movement phases, enhancing both accuracy and interpretability. EGPA offers a promising approach for improving automated movement quality assessment in both clinical and home-based rehabilitation contexts.

</details>


### [66] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)

*Tomas Peterka, Matyas Bohacek*

**Main category:** cs.CL

**Keywords:** media manipulation, provenance, misinformation, datasets, large language models

**Relevance Score:** 7

**TL;DR:** This paper introduces a dataset aimed at addressing media manipulation by assessing the relevance of images to news narratives based on origin location and time.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve detection of out-of-context and misattributed imagery in misinformation by considering the provenance of images in news articles.

**Method:** The authors created the News Media Provenance Dataset and formulated two tasks: location of origin relevance (LOR) and date and time of origin relevance (DTOR), evaluating six large language models (LLMs) as baselines.

**Key Contributions:**

	1. Introduction of the News Media Provenance Dataset
	2. Formulation of LOR and DTOR tasks for image relevance assessment
	3. Baseline evaluations on large language models (LLMs)

**Result:** The study shows promising zero-shot performance for LOR, but identifies significant shortcomings for DTOR, pointing to the need for specialized architectures.

**Limitations:** The performance of existing LLMs on the DTOR task remains inadequate, suggesting a need for further exploration.

**Conclusion:** The findings highlight the potential of using provenance-tagged images to enhance the detection of media manipulation, while also indicating areas for further research.

**Abstract:** Out-of-context and misattributed imagery is the leading form of media manipulation in today's misinformation and disinformation landscape. The existing methods attempting to detect this practice often only consider whether the semantics of the imagery corresponds to the text narrative, missing manipulation so long as the depicted objects or scenes somewhat correspond to the narrative at hand. To tackle this, we introduce News Media Provenance Dataset, a dataset of news articles with provenance-tagged images. We formulate two tasks on this dataset, location of origin relevance (LOR) and date and time of origin relevance (DTOR), and present baseline results on six large language models (LLMs). We identify that, while the zero-shot performance on LOR is promising, the performance on DTOR hinders, leaving room for specialized architectures and future work.

</details>


### [67] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)

*Xiangning Yu, Zhuohan Wang, Linyi Yang, Haoxuan Li, Anjie Liu, Xiao Xue, Jun Wang, Mengyue Yang*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, large language models, reasoning efficiency, sufficiency, necessity

**Relevance Score:** 9

**TL;DR:** The paper proposes a causal framework for improving Chain-of-Thought (CoT) prompting in LLMs by addressing sufficiency and necessity of inference steps, showing improved reasoning efficiency and reduced token usage.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of LLMs, addressing the challenges of sufficiency and necessity in CoT prompting is essential.

**Method:** A causal framework is developed to characterize CoT reasoning, incorporating causal Probability of Sufficiency and Necessity to identify and quantify the importance of inference steps.

**Key Contributions:**

	1. Introduction of a causal framework for CoT reasoning
	2. Development of causal Probability of Sufficiency and Necessity
	3. Demonstration of improved reasoning efficiency and reduced token usage

**Result:** Extensive experiments demonstrate improvements in reasoning efficiency and reduced token usage on mathematical and commonsense reasoning tasks without sacrificing accuracy.

**Limitations:** 

**Conclusion:** The proposed framework offers a valuable approach to improve LLM reasoning performance and cost-effectiveness by automating the addition of necessary steps and pruning redundant ones.

**Abstract:** Chain-of-Thought (CoT) prompting plays an indispensable role in endowing large language models (LLMs) with complex reasoning capabilities. However, CoT currently faces two fundamental challenges: (1) Sufficiency, which ensures that the generated intermediate inference steps comprehensively cover and substantiate the final conclusion; and (2) Necessity, which identifies the inference steps that are truly indispensable for the soundness of the resulting answer. We propose a causal framework that characterizes CoT reasoning through the dual lenses of sufficiency and necessity. Incorporating causal Probability of Sufficiency and Necessity allows us not only to determine which steps are logically sufficient or necessary to the prediction outcome, but also to quantify their actual influence on the final reasoning outcome under different intervention scenarios, thereby enabling the automated addition of missing steps and the pruning of redundant ones. Extensive experimental results on various mathematical and commonsense reasoning benchmarks confirm substantial improvements in reasoning efficiency and reduced token usage without sacrificing accuracy. Our work provides a promising direction for improving LLM reasoning performance and cost-effectiveness.

</details>


### [68] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)

*Rodion Oblovatny, Alexandra Bazarova, Alexey Zaytsev*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, probabilistic divergence, deep learning, natural language processing

**Relevance Score:** 9

**TL;DR:** Novel approach to detect hallucinations in LLMs by analyzing hidden-state distributions, revealing that hallucinations show smaller deviations from prompts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Detecting hallucinations in large language models to improve their reliability and validity in applications.

**Method:** Analyzes probabilistic divergence between prompt and response hidden-state distributions, using distributional distances as hallucination scores without external models.

**Key Contributions:**

	1. Introduced a novel model-intrinsic detection method for hallucinations.
	2. Showed that hallucinated responses have smaller deviations from prompts.
	3. Developed deep learnable kernels for better capturing distributional differences.

**Result:** Outperforms existing methods in detecting hallucinations, showing state-of-the-art performance on multiple benchmarks, even without kernel training.

**Limitations:** 

**Conclusion:** The proposed method offers a robust and scalable solution for hallucination detection in LLMs.

**Abstract:** We present a novel approach for detecting hallucinations in large language models (LLMs) by analyzing the probabilistic divergence between prompt and response hidden-state distributions. Counterintuitively, we find that hallucinated responses exhibit smaller deviations from their prompts compared to grounded responses, suggesting that hallucinations often arise from superficial rephrasing rather than substantive reasoning. Leveraging this insight, we propose a model-intrinsic detection method that uses distributional distances as principled hallucination scores, eliminating the need for external knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable kernels that automatically adapt to capture nuanced geometric differences between distributions. Our approach outperforms existing baselines, demonstrating state-of-the-art performance on several benchmarks. The method remains competitive even without kernel training, offering a robust, scalable solution for hallucination detection.

</details>


### [69] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)

*Yuxin Chen, Yiran Zhao, Yang Zhang, An Zhang, Kenji Kawaguchi, Shafiq Joty, Junnan Li, Tat-Seng Chua, Michael Qizhe Shieh, Wenxuan Zhang*

**Main category:** cs.CL

**Keywords:** large language models, multilingual performance, language-agnostic parameter space

**Relevance Score:** 9

**TL;DR:** This paper investigates the emergence of a language-agnostic parameter space in large language models (LLMs) and proposes neuron-specific training strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that large language models 'think' in English and to explore their multilingual capabilities.

**Method:** The study identifies language-related neurons in LLMs, categorizing them as shared or exclusive, and examines how their role evolves as models develop.

**Key Contributions:**

	1. Identification of language-related neurons as shared or exclusive.
	2. Demonstration of the development of a language-agnostic parameter space in LLMs.
	3. Proposal of tailored training strategies based on neuron functionality.

**Result:** Findings show a significant increase in the proportion and importance of shared neurons while exclusive neurons diminish, establishing a core language-agnostic parameter space.

**Limitations:** 

**Conclusion:** The emergence of a language-agnostic parameter space has implications for model training, leading to the proposal of neuron-specific training strategies for LLMs.

**Abstract:** As large language models (LLMs) continue to advance, their capacity to function effectively across a diverse range of languages has shown marked improvement. Preliminary studies observe that the hidden activations of LLMs often resemble English, even when responding to non-English prompts. This has led to the widespread assumption that LLMs may "think" in English. However, more recent results showing strong multilingual performance, even surpassing English performance on specific tasks in other languages, challenge this view. In this work, we find that LLMs progressively develop a core language-agnostic parameter space-a remarkably small subset of parameters whose deactivation results in significant performance degradation across all languages. This compact yet critical set of parameters underlies the model's ability to generalize beyond individual languages, supporting the emergence of abstract thought that is not tied to any specific linguistic system. Specifically, we identify language-related neurons-those are consistently activated during the processing of particular languages, and categorize them as either shared (active across multiple languages) or exclusive (specific to one). As LLMs undergo continued development over time, we observe a marked increase in both the proportion and functional importance of shared neurons, while exclusive neurons progressively diminish in influence. These shared neurons constitute the backbone of the core language-agnostic parameter space, supporting the emergence of abstract thought. Motivated by these insights, we propose neuron-specific training strategies tailored to LLMs' language-agnostic levels at different development stages. Experiments across diverse LLM families support our approach.

</details>


### [70] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)

*Zheng Zhao, Clara Vania, Subhradeep Kayal, Naila Khan, Shay B. Cohen, Emine Yilmaz*

**Main category:** cs.CL

**Keywords:** personalization, task-oriented assistants, large language models

**Relevance Score:** 9

**TL;DR:** PersonaLens is a benchmark for evaluating personalization in task-oriented AI assistants, revealing variability in LLMs' personalization capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for systematic evaluation of personalization in task-oriented AI assistants, as existing benchmarks do not adequately capture user preferences and interaction complexities.

**Method:** Introduces a benchmark called PersonaLens that includes diverse user profiles, interaction histories, and two LLM-based agents (user agent and judge agent) to evaluate personalization and task performance.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark (PersonaLens) for personalization evaluation
	2. Inclusion of diverse user profiles and interaction histories
	3. Development of two LLM-based agents for realistic engagement and assessment

**Result:** Significant variability was found in the personalization capabilities of current LLM assistants across a variety of tasks.

**Limitations:** 

**Conclusion:** PersonaLens provides essential insights for improving the design of conversational AI systems by effectively evaluating personalized task-oriented assistance.

**Abstract:** Large language models (LLMs) have advanced conversational AI assistants. However, systematically evaluating how well these assistants apply personalization--adapting to individual user preferences while completing tasks--remains challenging. Existing personalization benchmarks focus on chit-chat, non-conversational tasks, or narrow domains, failing to capture the complexities of personalized task-oriented assistance. To address this, we introduce PersonaLens, a comprehensive benchmark for evaluating personalization in task-oriented AI assistants. Our benchmark features diverse user profiles equipped with rich preferences and interaction histories, along with two specialized LLM-based agents: a user agent that engages in realistic task-oriented dialogues with AI assistants, and a judge agent that employs the LLM-as-a-Judge paradigm to assess personalization, response quality, and task success. Through extensive experiments with current LLM assistants across diverse tasks, we reveal significant variability in their personalization capabilities, providing crucial insights for advancing conversational AI systems.

</details>


### [71] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)

*Wendi Zhou, Ameer Saadat-Yazd, Nadin Kokciyan*

**Main category:** cs.CL

**Keywords:** opinion summarization, aspect-centric analysis, text summarization, machine learning, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper presents ASESUM, a novel automated opinion summarization system that generates aspect-centric summaries from customer reviews without needing pre-defined aspects.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing volume of online reviews makes it difficult for customers to synthesize key opinions, necessitating an automated approach to summarize and capture diverse perspectives.

**Method:** ASESUM extracts aspect-centric arguments from product reviews and assesses their salience and validity to produce coherent summaries.

**Key Contributions:**

	1. Introduction of ASESUM framework for opinion summarization
	2. Ability to adapt to various domains without predefined aspects
	3. Enhanced extraction and validation of aspect-centric arguments

**Result:** Experiments demonstrate that ASESUM outperforms existing summarization methods in capturing a wider range of opinions from reviews.

**Limitations:** 

**Conclusion:** The proposed ASESUM framework effectively summarizes customer opinions, making it suitable for various domains without the need for preset aspects.

**Abstract:** Reviews are valuable resources for customers making purchase decisions in online shopping. However, it is impractical for customers to go over the vast number of reviews and manually conclude the prominent opinions, which prompts the need for automated opinion summarization systems. Previous approaches, either extractive or abstractive, face challenges in automatically producing grounded aspect-centric summaries. In this paper, we propose a novel summarization system that not only captures predominant opinions from an aspect perspective with supporting evidence, but also adapts to varying domains without relying on a pre-defined set of aspects. Our proposed framework, ASESUM, summarizes viewpoints relevant to the critical aspects of a product by extracting aspect-centric arguments and measuring their salience and validity. We conduct experiments on a real-world dataset to demonstrate the superiority of our approach in capturing diverse perspectives of the original reviews compared to new and existing methods.

</details>


### [72] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)

*Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li*

**Main category:** cs.CL

**Keywords:** reinforcement learning, verification, instruction following, large language models, dataset

**Relevance Score:** 9

**TL;DR:** This paper introduces VerIF, a verification method for reinforcement learning in instruction following tasks, showing significant improvements in model performance.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Enhancing reinforcement learning techniques for instruction following to improve large language models using verification engineering.

**Method:** The paper proposes VerIF, which combines rule-based code verification with LLM-based verification, applied to a newly constructed VerInstruct dataset for RL training.

**Key Contributions:**

	1. Introduction of VerIF for improving instruction following in RL
	2. Construction of VerInstruct dataset with 22,000 instances
	3. Demonstration of state-of-the-art performance in instruction-following tasks

**Result:** The proposed VerIF method resulted in significant performance improvements in instruction-following benchmarks, achieving state-of-the-art results for models of similar size.

**Limitations:** 

**Conclusion:** Integrating VerIF into reinforcement learning recipes enhances model performance without compromising general capabilities; the developed datasets and resources are made publicly available.

**Abstract:** Reinforcement learning with verifiable rewards (RLVR) has become a key technique for enhancing large language models (LLMs), with verification engineering playing a central role. However, best practices for RL in instruction following remain underexplored. In this work, we explore the verification challenge in RL for instruction following and propose VerIF, a verification method that combines rule-based code verification with LLM-based verification from a large reasoning model (e.g., QwQ-32B). To support this approach, we construct a high-quality instruction-following dataset, VerInstruct, containing approximately 22,000 instances with associated verification signals. We apply RL training with VerIF to two models, achieving significant improvements across several representative instruction-following benchmarks. The trained models reach state-of-the-art performance among models of comparable size and generalize well to unseen constraints. We further observe that their general capabilities remain unaffected, suggesting that RL with VerIF can be integrated into existing RL recipes to enhance overall model performance. We have released our datasets, codes, and models to facilitate future research at https://github.com/THU-KEG/VerIF.

</details>


### [73] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)

*Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen, Xi Ye*

**Main category:** cs.CL

**Keywords:** retrieval heads, long-context models, query-focused retrieval, multi-hop reasoning, interpretability

**Relevance Score:** 8

**TL;DR:** This paper introduces QRHEAD, an improved set of attention heads for long-context language models, and QR-RETRIEVER, an efficient retriever leveraging QRHEAD for enhanced retrieval in reasoning tasks, achieving notable performance gains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve retrieval mechanisms in long-context language models for better performance in tasks like long-context reasoning and QA.

**Method:** The authors define QRHEAD by aggregating attention scores focused on input queries, and present QR-RETRIEVER which utilizes these scores for effective retrieval.

**Key Contributions:**

	1. Introduction of QRHEAD for enhanced retrieval in long-context models
	2. Development of QR-RETRIEVER that outperforms existing methods in multi-hop reasoning
	3. Interpretability insights into attention mechanisms for long-context tasks

**Result:** QR-RETRIEVER exhibits over 10% performance improvements on multi-hop reasoning tasks and strong zero-shot performance on the BEIR benchmark, surpassing traditional dense retrievers and other LLM-based re-rankers.

**Limitations:** 

**Conclusion:** The study concludes that QRHEAD and QR-RETRIEVER significantly enhance long-context reasoning capabilities and provide interpretability insights for language models.

**Abstract:** Recent work has identified retrieval heads (Wu et al., 2025b), a subset of attention heads responsible for retrieving salient information in long-context language models (LMs), as measured by their copy-paste behavior in Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused Retrieval Head), an improved set of attention heads that enhance retrieval from long context. We identify QRHEAD by aggregating attention scores with respect to the input query, using a handful of examples from real-world tasks (e.g., long-context QA). We further introduce QR- RETRIEVER, an efficient and effective retriever that uses the accumulated attention mass of QRHEAD as retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting the most relevant parts with the highest retrieval scores. On multi-hop reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains over full context and outperforms strong dense retrievers. We also evaluate QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves strong zero-shot performance, outperforming other LLM-based re-rankers such as RankGPT. Further analysis shows that both the querycontext attention scoring and task selection are crucial for identifying QRHEAD with strong downstream utility. Overall, our work contributes a general-purpose retriever and offers interpretability insights into the long-context capabilities of LMs.

</details>


### [74] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)

*Shangshang Wang, Julian Asilis, Ömer Faruk Akgül, Enes Burak Bilgin, Ollie Liu, Deqing Fu, Willie Neiswanger*

**Main category:** cs.CL

**Keywords:** reasoning models, sparse autoencoder tuning, language models, cost-effective training, modular reasoning

**Relevance Score:** 9

**TL;DR:** This paper presents Resa, a set of reasoning models utilizing a novel sparse autoencoder tuning (SAE-Tuning) method to enhance reasoning capabilities in language models cost-effectively and efficiently.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to elicit strong reasoning in language models while reducing training costs and time significantly.

**Method:** The proposed SAE-Tuning first trains a sparse autoencoder to capture reasoning from a source model, which guides the fine-tuning of a target model using verified question-answer data without reasoning traces.

**Key Contributions:**

	1. Introduction of the sparse autoencoder tuning (SAE-Tuning) method for language models.
	2. Significant reduction in training costs and time while maintaining high reasoning performance.
	3. Evidence that reasoning abilities are generalizable and modular, applicable across different models without additional retraining.

**Result:** SAE-Tuning retains over 97% of the reasoning performance of RL-trained models while cutting training costs by over 2000x and time by 450x. It achieves notable reasoning performance metrics, such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 with minimal additional costs.

**Limitations:** 

**Conclusion:** The findings suggest that reasoning abilities extracted via SAE-Tuning are both generalizable and modular, enhancing model performance across datasets without retraining.

**Abstract:** How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \$1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around \$1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced.

</details>


### [75] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)

*Hillary Dawkins, Kathleen C. Fraser, Svetlana Kiritchenko*

**Main category:** cs.CL

**Keywords:** AI-generated text, social media, detection algorithms, fine-tuned models, influence campaigns

**Relevance Score:** 6

**TL;DR:** The paper investigates the detection of AI-generated text on social media, presenting a dataset of 505,159 posts and analyzing the effectiveness of detection methods in realistic scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to detect AI-generated text on social media due to its potential use in online influence campaigns.

**Method:** Creation of a dataset of AI-generated social media posts and testing various detection algorithms under different assumptions about model access.

**Key Contributions:**

	1. Dataset of 505,159 AI-generated posts
	2. Analysis of detection effectiveness under real-world scenarios
	3. Insights into detection algorithm vulnerabilities

**Result:** Detection effectiveness drops significantly when fine-tuned models are not publicly accessible, confirmed by a human study.

**Limitations:** The study assumes specific conditions regarding access to generating models and may not generalize to all scenarios.

**Conclusion:** Detection algorithms are vulnerable to fine-tuned LLMs, which poses challenges in various detection domains.

**Abstract:** Detecting AI-generated text is a difficult problem to begin with; detecting AI-generated text on social media is made even more difficult due to the short text length and informal, idiosyncratic language of the internet. It is nonetheless important to tackle this problem, as social media represents a significant attack vector in online influence campaigns, which may be bolstered through the use of mass-produced AI-generated posts supporting (or opposing) particular policies, decisions, or events. We approach this problem with the mindset and resources of a reasonably sophisticated threat actor, and create a dataset of 505,159 AI-generated social media posts from a combination of open-source, closed-source, and fine-tuned LLMs, covering 11 different controversial topics. We show that while the posts can be detected under typical research assumptions about knowledge of and access to the generating models, under the more realistic assumption that an attacker will not release their fine-tuned model to the public, detectability drops dramatically. This result is confirmed with a human study. Ablation experiments highlight the vulnerability of various detection algorithms to fine-tuned LLMs. This result has implications across all detection domains, since fine-tuning is a generally applicable and realistic LLM use case.

</details>


### [76] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)

*Hiroshi Matsuda, Chunpeng Ma, Masayuki Asahara*

**Main category:** cs.CL

**Keywords:** Large Language Models, Dependency Parsing, Multilingual Fine-tuning, Natural Language Processing, Syntactic Heads

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach to dependency parsing using large language models, which improves accuracy and consistency in outputs through step-by-step instructions and universal part-of-speech tagging.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of standard prompting methods in producing valid and accurate outputs in dependency parsing tasks using large language models.

**Method:** The authors propose a strategy that includes universal part-of-speech tagging followed by the prediction of syntactic heads and dependency labels, using a simplified CoNLL-U like output format.

**Key Contributions:**

	1. Novel step-by-step instruction strategy for dependency parsing using LLMs
	2. State-of-the-art accuracy across multiple languages
	3. Evidence of improved cross-language generalization performance through multilingual fine-tuning.

**Result:** The method achieves state-of-the-art accuracy on Universal Dependencies datasets across 17 languages, demonstrating improvements without issues of hallucination or contamination.

**Limitations:** 

**Conclusion:** The results indicate that explicit reasoning steps enhance LLM-based parsing, providing a scalable alternative to bracket-based approaches.

**Abstract:** Recent advances in large language models (LLMs) have enabled impressive performance in various tasks. However, standard prompting often struggles to produce structurally valid and accurate outputs, especially in dependency parsing. We propose a novel step-by-step instruction strategy, where universal part-of-speech tagging precedes the prediction of syntactic heads and dependency labels, and a simplified CoNLL-U like output format, our method achieves state-of-the-art accuracy on Universal Dependencies datasets across 17 languages without hallucination or contamination. We further show that multilingual fine-tuning simultaneously improves cross-language generalization performance. Our results highlight the effectiveness of explicit reasoning steps in LLM-based parsing and offer a scalable, format-consistent alternative to bracket-based approaches.

</details>


### [77] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)

*Amel Muminovic, Amela Kadric Muminovic*

**Main category:** cs.CL

**Keywords:** toxic language detection, language models, Balkan languages, natural language processing, context-augmented prompts

**Relevance Score:** 7

**TL;DR:** This study evaluates large language models for detecting toxic language in Serbian, Croatian, and Bosnian by utilizing a manually labeled dataset of comments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the real harm caused by online toxic language, especially in regions with limited moderation tools, by improving detection methods for Balkan languages.

**Method:** We constructed a dataset of 4,500 YouTube and TikTok comments and tested four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) under zero-shot and context-augmented conditions, measuring precision, recall, F1 score, accuracy, and false positive rates.

**Key Contributions:**

	1. Evaluation of ML models for toxic comment detection in under-resourced Balkan languages.
	2. Demonstration of improved performance through context-augmented prompts.
	3. Strategies for enhancing toxicity detection in low-data scenarios.

**Result:** Gemini in context-augmented mode achieved an F1 score of 0.82 and accuracy of 0.82, while GPT-4.1 showed the best precision and lowest false alarms in zero-shot mode.

**Limitations:** Focus on limited languages and modalities; may not generalize to other languages or contexts.

**Conclusion:** Adding minimal context improves toxicity detection, suggesting strategies like improved prompt design and threshold calibration for better performance in low-resource settings.

**Abstract:** Online toxic language causes real harm, especially in regions with limited moderation tools. In this study, we evaluate how large language models handle toxic comments in Serbian, Croatian, and Bosnian, languages with limited labeled data. We built and manually labeled a dataset of 4,500 YouTube and TikTok comments drawn from videos across diverse categories, including music, politics, sports, modeling, influencer content, discussions of sexism, and general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus) were tested in two modes: zero-shot and context-augmented. We measured precision, recall, F1 score, accuracy and false positive rates. Including a short context snippet raised recall by about 0.12 on average and improved F1 score by up to 0.10, though it sometimes increased false positives. The best balance came from Gemini in context-augmented mode, reaching an F1 score of 0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the lowest false alarms. We show how adding minimal context can improve toxic language detection in low-resource settings and suggest practical strategies such as improved prompt design and threshold calibration. These results show that prompt design alone can yield meaningful gains in toxicity detection for underserved Balkan language communities.

</details>


### [78] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)

*Yang Li, Qiang Sheng, Yehan Yang, Xueyao Zhang, Juan Cao*

**Main category:** cs.CL

**Keywords:** large language models, moderation, safety alignment, harmfulness detection, streaming content monitor

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel approach for the moderation of large language models (LLMs) that focuses on partial detection of harmful outputs, utilizing a newly constructed dataset and a streaming content monitor to enhance safety without sacrificing performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current moderation methods that rely on full detection, which causes high latency in LLM services.

**Method:** Developed a dataset called FineHarm with 29K prompt-response pairs annotated for token-level training, and proposed a streaming content monitor (SCM) that uses dual supervision for timely harmfulness assessment during output generation.

**Key Contributions:**

	1. Introduction of FineHarm dataset for token-level training in harmfulness detection.
	2. Development of a streaming content monitor for partial detection during LLM output generation.
	3. Demonstration of comparable performance to full detection methods with reduced latency.

**Result:** The streaming content monitor achieves a macro F1 score of over 0.95 while evaluating only the first 18% of tokens in outputs, matching the performance of full detection methods.

**Limitations:** 

**Conclusion:** The SCM not only enhances moderation at lower latency but also serves as a tool to improve safety alignment and achieve higher harmlessness scores compared to existing methods.

**Abstract:** Though safety alignment has been applied to most large language models (LLMs), LLM service providers generally deploy a subsequent moderation as the external safety guardrail in real-world products. Existing moderators mainly practice a conventional full detection, which determines the harmfulness based on the complete LLM output, causing high service latency. Recent works pay more attention to partial detection where moderators oversee the generation midway and early stop the output if harmfulness is detected, but they directly apply moderators trained with the full detection paradigm to incomplete outputs, introducing a training-inference gap that lowers the performance. In this paper, we explore how to form a data-and-model solution that natively supports partial detection. For the data, we construct FineHarm, a dataset consisting of 29K prompt-response pairs with fine-grained annotations to provide reasonable supervision for token-level training. Then, we propose the streaming content monitor, which is trained with dual supervision of response- and token-level labels and can follow the output stream of LLM to make a timely judgment of harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is comparable to full detection, by only seeing the first 18% of tokens in responses on average. Moreover, the SCM can serve as a pseudo-harmfulness annotator for improving safety alignment and lead to a higher harmlessness score than DPO.

</details>


### [79] [AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes](https://arxiv.org/abs/2305.14725)

*Barry Menglong Yao, Sijia Wang, Yu Chen, Qifan Wang, Minqian Liu, Zhiyang Xu, Licheng Yu, Lifu Huang*

**Main category:** cs.CL

**Keywords:** Multimodal Entity Linking, Attribute Awareness, Knowledge Base, Natural Language Processing, Computer Vision

**Relevance Score:** 7

**TL;DR:** This paper introduces an attribute-aware multimodal entity linking framework, utilizing a new benchmark dataset, AMELI, to enhance entity disambiguation by incorporating attributes from multimodal inputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to improve multimodal entity linking by incorporating textual and visual attributes, which are not typically included in contemporary approaches.

**Method:** The authors constructed the AMELI dataset and knowledge base and tested several state-of-the-art architectures for entity linking while proposing a new method that integrates entity attributes into disambiguation tasks.

**Key Contributions:**

	1. Development of AMELI, a new multimodal entity linking benchmark dataset.
	2. First integration of attributes into multimodal entity linking tasks.
	3. Public availability of the dataset and model checkpoints for further research.

**Result:** Experiments demonstrated that understanding attributes from text and images significantly enhances the accuracy of multimodal entity linking.

**Limitations:** 

**Conclusion:** Integrating attributes into the multimodal entity linking task is crucial, marking a novel contribution to the field, which has not been previously explored.

**Abstract:** We propose attribute-aware multimodal entity linking, where the input consists of a mention described with a text paragraph and images, and the goal is to predict the corresponding target entity from a multimodal knowledge base (KB) where each entity is also accompanied by a text description, visual images, and a collection of attributes that present the meta-information of the entity in a structured format. To facilitate this research endeavor, we construct AMELI, encompassing a new multimodal entity linking benchmark dataset that contains 16,735 mentions described in text and associated with 30,472 images, and a multimodal knowledge base that covers 34,690 entities along with 177,873 entity images and 798,216 attributes. To establish baseline performance on AMELI, we experiment with several state-of-the-art architectures for multimodal entity linking and further propose a new approach that incorporates attributes of entities into disambiguation. Experimental results and extensive qualitative analysis demonstrate that extracting and understanding the attributes of mentions from their text descriptions and visual images play a vital role in multimodal entity linking. To the best of our knowledge, we are the first to integrate attributes in the multimodal entity linking task. The programs, model checkpoints, and the dataset are publicly available at https://github.com/VT-NLP/Ameli.

</details>


### [80] [DREsS: Dataset for Rubric-based Essay Scoring on EFL Writing](https://arxiv.org/abs/2402.16733)

*Haneul Yoo, Jieun Han, So-Yeon Ahn, Alice Oh*

**Main category:** cs.CL

**Keywords:** automated essay scoring, English as a Foreign Language, dataset, augmented data, machine learning

**Relevance Score:** 4

**TL;DR:** The paper introduces DREsS, a large-scale dataset for automated essay scoring (AES) designed for English as a Foreign Language (EFL) writing education, improving the practical application of AES in this domain.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of previous AES models that were not contextually relevant to EFL writing education and typically provided a singular holistic score.

**Method:** The study presents DREsS, containing 48.9K samples across three sub-datasets, including a real-classroom dataset of essays scored by experts and an augmentation strategy to generate synthetic samples.

**Key Contributions:**

	1. Release of the DREsS dataset with 48.9K samples for AES
	2. Inclusion of a real-classroom dataset with expert-scored essays
	3. Development of CASE, a novel augmentation strategy for dataset enhancement

**Result:** The introduction of DREsS and the proposed augmentation strategy result in a 45.44% improvement in baseline results for AES models.

**Limitations:** 

**Conclusion:** DREsS facilitates research aimed at developing more accurate and applicable AES systems in the context of EFL writing education.

**Abstract:** Automated essay scoring (AES) is a useful tool in English as a Foreign Language (EFL) writing education, offering real-time essay scores for students and instructors. However, previous AES models were trained on essays and scores irrelevant to the practical scenarios of EFL writing education and usually provided a single holistic score due to the lack of appropriate datasets. In this paper, we release DREsS, a large-scale, standard dataset for rubric-based automated essay scoring with 48.9K samples in total. DREsS comprises three sub-datasets: DREsS_New, DREsS_Std., and DREsS_CASE. We collect DREsS_New, a real-classroom dataset with 2.3K essays authored by EFL undergraduate students and scored by English education experts. We also standardize existing rubric-based essay scoring datasets as DREsS_Std. We suggest CASE, a corruption-based augmentation strategy for essays, which generates 40.1K synthetic samples of DREsS_CASE and improves the baseline results by 45.44%. DREsS will enable further research to provide a more accurate and practical AES system for EFL writing education.

</details>


### [81] [Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation](https://arxiv.org/abs/2404.01129)

*Bohao Yang, Kun Zhao, Dong Liu, Liang Zhan, Chenghua Lin*

**Main category:** cs.CL

**Keywords:** dialogue evaluation, Large Language Models, Abstract Meaning Representation, semantic representation, adversarial examples

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel evaluation framework for open-domain dialogue evaluation that combines Abstract Meaning Representation with domain-specific language models and large language models to effectively assess response appropriateness, especially in the presence of adversarial negative examples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity of evaluating response appropriateness in open-domain dialogue has led to ineffective traditional metrics, prompting the need for a more robust evaluation framework that can handle adversarial examples.

**Method:** The proposed framework integrates Abstract Meaning Representation (AMR) enhanced domain-specific language models (SLMs) with large language models (LLMs), using a gating mechanism for semantic representation and incorporating AMR knowledge into LLM prompts.

**Key Contributions:**

	1. Integration of AMR into SLMs for enhanced semantic understanding
	2. A novel gating mechanism to improve dialogue evaluation metrics
	3. Demonstrated superior performance over existing state-of-the-art methods

**Result:** Extensive experiments show that the new framework outperforms state-of-the-art baselines and achieves strong correlations with human judgments across various datasets.

**Limitations:** 

**Conclusion:** The proposed framework establishes a new benchmark for open-domain dialogue evaluation and demonstrates the importance of incorporating AMR graph information for improving performance.

**Abstract:** Automatic open-domain dialogue evaluation has attracted increasing attention, yet remains challenging due to the complexity of assessing response appropriateness. Traditional evaluation metrics, typically trained with true positive and randomly selected negative responses, tend to assign higher scores to responses that share greater content similarity with contexts. However, adversarial negative responses, despite possessing high lexical overlap with contexts, can be semantically incongruous. Consequently, existing metrics struggle to effectively evaluate such responses, resulting in low correlations with human judgments. While recent studies have demonstrated the effectiveness of Large Language Models (LLMs) for open-domain dialogue evaluation, they still face challenges in handling adversarial negative examples. We propose a novel evaluation framework that integrates Abstract Meaning Representation (AMR) enhanced domain-specific language models (SLMs) with LLMs. Our SLMs explicitly incorporate AMR graph information through a gating mechanism for enhanced semantic representation learning, while both SLM predictions and AMR knowledge are integrated into LLM prompts for robust evaluation. Extensive experiments on open-domain dialogue evaluation tasks demonstrate the superiority of our method compared to state-of-the-art baselines. Our comprehensive ablation studies reveal that AMR graph information contributes substantially more to performance improvements. Our framework achieves strong correlations with human judgments across multiple datasets, establishing a new benchmark for dialogue evaluation. Our code and data are publicly available.

</details>


### [82] [Let's Fuse Step by Step: A Generative Fusion Decoding Algorithm with LLMs for Robust and Instruction-Aware ASR and OCR](https://arxiv.org/abs/2405.14259)

*Chan-Jan Hsu, Yi-Chang Chen, Feng-Ting Liao, Pei-Chen Ho, Yu-Hsiang Wang, Po-Chun Hsu, Da-shan Shiu*

**Main category:** cs.CL

**Keywords:** Generative Fusion Decoding, Large Language Models, Automatic Speech Recognition, Optical Character Recognition, Cross-modal Integration

**Relevance Score:** 9

**TL;DR:** Generative Fusion Decoding (GFD) is a framework that integrates large language models into text recognition systems for ASR and OCR, showing significant performance improvements without retraining.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To seamlessly integrate large language models into cross-modal text recognition systems for improved ASR and OCR performance.

**Method:** GFD operates by calculating likelihood at the byte level to fuse models across different token spaces, allowing for enhanced decoding processes.

**Key Contributions:**

	1. Introduces a novel GFD framework for integrating LLMs into ASR and OCR systems.
	2. Demonstrates significant performance improvements in text recognition tasks through effective model fusion.
	3. Facilitates in-context learning abilities in adaptive ASR contexts.

**Result:** GFD demonstrates superior performance in English and Mandarin ASR and OCR tasks, achieving significant WER reductions of up to 17.7% compared to cascaded methods.

**Limitations:** 

**Conclusion:** GFD is a plug-and-play framework that enhances ASR and OCR functionalities by leveraging the capabilities of large language models without the need for retraining.

**Abstract:** We propose "Generative Fusion Decoding" (GFD), a novel shallow fusion framework designed to integrate large language models (LLMs) into cross-modal text recognition systems for automatic speech recognition (ASR) and optical character recognition (OCR). We derive the necessary formulations to enable GFD to operate across mismatched token spaces of different models by calculating likelihood at the byte level, thereby enabling seamless fusion and synchronous progression during the decoding process. GFD is plug-and-play by design, making it readily compatible with various auto-regressive models without the need for any re-training. GFD proves effective for general ASR and OCR tasks through intermediate and frequent interactions with LLMs, surpassing cascaded methods in English and Mandarin benchmarks. In addition, GFD transfers in-context learning abilities of LLMs and allows for adaptive ASR in instruction-aware and long-context settings, yielding significant WER reductions of up to 17.7\%.

</details>


### [83] [Language Models Resist Alignment: Evidence From Data Compression](https://arxiv.org/abs/2406.06144)

*Jiaming Ji, Kaile Wang, Tianyi Qiu, Boyuan Chen, Jiayi Zhou, Changye Li, Hantao Lou, Juntao Dai, Yunhuai Liu, Yaodong Yang*

**Main category:** cs.CL

**Keywords:** large language models, alignment, elasticity, compression theory, empirical analysis

**Relevance Score:** 9

**TL;DR:** This paper explores the elasticity of large language models (LLMs) post-alignment, demonstrating how they can revert to pre-training behavior despite alignment efforts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to understand and mitigate the unintended behaviors of LLMs, which alignment processes may not fully resolve.

**Method:** The authors empirically analyzed the elasticity of post-alignment models and leveraged compression theory to investigate alignment impacts, validating these findings through experiments with various models.

**Key Contributions:**

	1. Introduced the concept of elasticity in LLMs post-alignment
	2. Demonstrated empirical evidence of behavior reversion to pre-training distribution
	3. Provided theoretical insights using compression theory regarding alignment impacts.

**Result:** The study found that post-alignment, models exhibit a tendency to revert to pre-training behavior, with performance declines followed by reversion to prior distributions, especially correlated with model size and pre-training data.

**Limitations:** 

**Conclusion:** Addressing the inherent elasticity of LLMs is crucial for improving alignment effectiveness and ensuring desired model behaviors.

**Abstract:** Large language models (LLMs) may exhibit unintended or undesirable behaviors. Recent works have concentrated on aligning LLMs to mitigate harmful outputs. Despite these efforts, some anomalies indicate that even a well-conducted alignment process can be easily circumvented, whether intentionally or accidentally. Does alignment fine-tuning yield have robust effects on models, or are its impacts merely superficial? In this work, we make the first exploration of this phenomenon from both theoretical and empirical perspectives. Empirically, we demonstrate the $\mathbf{elasticity}$ of post-alignment models, i.e., the tendency to revert to the behavior distribution formed during the pre-training phase upon further fine-tuning. Leveraging compression theory, we formally deduce that fine-tuning disproportionately undermines alignment relative to pre-training, potentially by orders of magnitude. We validate the presence of elasticity through experiments on models of varying types and scales. Specifically, we find that model performance declines rapidly before reverting to the pre-training distribution, after which the rate of decline drops significantly. Furthermore, we further reveal that elasticity positively correlates with the increased model size and the expansion of pre-training data. Our findings underscore the need to address the inherent elasticity of LLMs to mitigate their resistance to alignment. The model weight and code are available at pku-lm-resist-alignment.github.io.

</details>


### [84] [Standard Language Ideology in AI-Generated Language](https://arxiv.org/abs/2406.08726)

*Genevieve Smith, Eve Fleisig, Madeline Bossi, Ishita Rustagi, Xavier Yin*

**Main category:** cs.CL

**Keywords:** language ideology, large language models, Standard American English, minoritized languages, generative AI

**Relevance Score:** 7

**TL;DR:** The paper analyzes how standard language ideology is reflected in language produced by large language models (LLMs) and discusses its implications for minoritized language communities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs reinforce standard language ideologies, particularly Standard American English, impacting diverse language communities.

**Method:** A faceted taxonomy of open problems is presented to illustrate the manifestation of standard language ideology in AI-generated language.

**Key Contributions:**

	1. Introduction of the concept of standard AI-generated language ideology
	2. Identification of ongoing tensions regarding language variety representation in AI
	3. Three recommendations for researchers and practitioners to achieve more inclusive outcomes

**Result:** The findings reveal that LLMs perpetuate standard language norms and biases, which can marginalize non-standard language varieties and their speakers.

**Limitations:** 

**Conclusion:** The paper recommends structural changes and strategies to support diverse language communities rather than solely focusing on technical solutions.

**Abstract:** Standard language ideology is reflected and reinforced in language generated by large language models (LLMs). We present a faceted taxonomy of open problems that illustrate how standard language ideology manifests in AI-generated language, alongside implications for minoritized language communities and society more broadly. We introduce the concept of standard AI-generated language ideology, a process through which LLMs position "standard" languages--particularly Standard American English (SAE)--as the linguistic default, reinforcing the perception that SAE is the most "appropriate" language. We then discuss ongoing tensions around what constitutes desirable system behavior, as well as advantages and drawbacks of generative AI tools attempting, or refusing, to imitate different English language varieties. Rather than prescribing narrow technical fixes, we offer three recommendations for researchers, practitioners, and funders that focus on shifting structural conditions and supporting more emancipatory outcomes for diverse language communities.

</details>


### [85] [CaLMQA: Exploring culturally specific long-form question answering across 23 languages](https://arxiv.org/abs/2406.17761)

*Shane Arora, Marzena Karpinska, Hung-Ting Chen, Ipsita Bhattacharjee, Mohit Iyyer, Eunsol Choi*

**Main category:** cs.CL

**Keywords:** multilingual QA, culturally specific questions, large language models, natural language processing, dataset release

**Relevance Score:** 9

**TL;DR:** The paper introduces CaLMQA, a multilingual dataset of 51.7K culturally specific long-form questions across 23 languages and evaluates LLM performance in answering these questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capability of large language models in generating long-form answers to culturally specific questions across different languages, addressing a gap in current research.

**Method:** The authors created the CaLMQA dataset by collecting culturally specific questions from community forums in high-resource languages and by hiring native speakers for under-resourced languages. The evaluation involved assessing the factuality, relevance, and quality of LLM-generated answers to these questions.

**Key Contributions:**

	1. Introduction of CaLMQA, a dataset with 51.7K culturally specific questions in 23 languages
	2. First evaluation of LLM performance on multilingual long-form QA across cultures
	3. Insight into challenges faced by LLMs with cultural specificity and low-resource languages.

**Result:** The study found that LLMs often made critical surface-level errors and that culturally specific questions led to more factual errors compared to culturally agnostic questions, particularly for low-resource languages.

**Limitations:** The study may not cover all cultural nuances and languages, and there could be limitations in the evaluation framework used for assessing answer quality.

**Conclusion:** The findings highlight limitations in LLM responses to culturally nuanced queries and emphasize the need for targeted improvements in multilingual understanding. The CaLMQA dataset is released for future research in this area.

**Abstract:** Despite rising global usage of large language models (LLMs), their ability to generate long-form answers to culturally specific questions remains unexplored in many languages. To fill this gap, we perform the first study of textual multilingual long-form QA by creating CaLMQA, a dataset of 51.7K culturally specific questions across 23 different languages. We define culturally specific questions as those that refer to concepts unique to one or a few cultures, or have different answers depending on the cultural or regional context. We obtain these questions by crawling naturally-occurring questions from community web forums in high-resource languages, and by hiring native speakers to write questions in under-resourced, rarely-studied languages such as Fijian and Kirundi. Our data collection methodologies are translation-free, enabling the collection of culturally unique questions like "Kuber iki umwami wa mbere w'uburundi yitwa Ntare?" (Kirundi; English translation: "Why was the first king of Burundi called Ntare (Lion)?"). We evaluate factuality, relevance and surface-level quality of LLM-generated long-form answers, finding that (1) for many languages, even the best models make critical surface-level errors (e.g., answering in the wrong language, repetition), especially for low-resource languages; and (2) answers to culturally specific questions contain more factual errors than answers to culturally agnostic questions -- questions that have consistent meaning and answer across many cultures. We release CaLMQA to facilitate future research in cultural and multilingual long-form QA.

</details>


### [86] [CiteFusion: An Ensemble Framework for Citation Intent Classification Harnessing Dual-Model Binary Couples and SHAP Analyses](https://arxiv.org/abs/2407.13329)

*Lorenzo Paolini, Sahar Vahdati, Angelo Di Iorio, Robert Wardenga, Ivan Heibi, Silvio Peroni*

**Main category:** cs.CL

**Keywords:** Citation Classification, Ensemble Learning, SHAP, SciBERT, XLNet

**Relevance Score:** 7

**TL;DR:** CiteFusion is an ensemble framework for classifying citation intents using SciBERT and XLNet models, achieving state-of-the-art performance on benchmark datasets while enhancing interpretability through SHAP.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding citation motivations is crucial for evaluating research impact and promoting transparent scholarly communication.

**Method:** CiteFusion uses a one-vs-all approach for multi-class classification with binary sub-tasks, combining outputs from SciBERT and XLNet models through a neural network meta-classifier. It also incorporates section titles as contextual framing devices.

**Key Contributions:**

	1. Introduces CiteFusion framework for citation intent classification.
	2. Employs SHAP for enhanced interpretability of classification outputs.
	3. Demonstrates state-of-the-art performance on citation intent classification benchmarks.

**Result:** CiteFusion achieved Macro-F1 scores of 89.60% on SciCite and 76.24% on ACL-ARC, demonstrating robust performance in imbalanced and data-scarce scenarios.

**Limitations:** 

**Conclusion:** The study highlights the importance of structural context in classification accuracy and provides a web-based application for classifying citation intents.

**Abstract:** Understanding the motivations underlying scholarly citations is essential to evaluate research impact and pro-mote transparent scholarly communication. This study introduces CiteFusion, an ensemble framework designed to address the multi-class Citation Intent Classification task on two benchmark datasets: SciCite and ACL-ARC. The framework employs a one-vs-all decomposition of the multi-class task into class-specific binary sub-tasks, leveraging complementary pairs of SciBERT and XLNet models, independently tuned, for each citation intent. The outputs of these base models are aggregated through a feedforward neural network meta-classifier to reconstruct the original classification task. To enhance interpretability, SHAP (SHapley Additive exPlanations) is employed to analyze token-level contributions, and interactions among base models, providing transparency into the classification dynamics of CiteFusion, and insights about the kind of misclassifications of the ensem-ble. In addition, this work investigates the semantic role of structural context by incorporating section titles, as framing devices, into input sentences, assessing their positive impact on classification accuracy. CiteFusion ul-timately demonstrates robust performance in imbalanced and data-scarce scenarios: experimental results show that CiteFusion achieves state-of-the-art performance, with Macro-F1 scores of 89.60% on SciCite, and 76.24% on ACL-ARC. Furthermore, to ensure interoperability and reusability, citation intents from both datasets sche-mas are mapped to Citation Typing Ontology (CiTO) object properties, highlighting some overlaps. Finally, we describe and release a web-based application that classifies citation intents leveraging the CiteFusion models developed on SciCite.

</details>


### [87] [MMREC: LLM Based Multi-Modal Recommender System](https://arxiv.org/abs/2408.04211)

*Jiahao Tian, Jinman Zhao, Zhenkai Wang, Zhicheng Ding*

**Main category:** cs.CL

**Keywords:** Recommender Systems, Large Language Models, Multi-modal Data, Deep Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents a novel approach to enhancing recommender systems using Large Language Models (LLMs) and deep learning techniques, focusing on multi-modal information processing to improve recommendation accuracy and relevance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid increase in content generated daily creates challenges for effective recommender systems, particularly in leveraging natural language data and images that represent user preferences.

**Method:** The proposed framework incorporates multi-modal information processing and unified latent space representation to improve recommendations. It extracts and integrates text and image information through LLMs, simplifying the learning process for the ranking model.

**Key Contributions:**

	1. Novel use of LLMs in recommender systems
	2. Integration of multi-modal information
	3. Improvement of recommendation accuracy and relevance

**Result:** Experimental results show that the model demonstrates enhanced discriminative power when utilizing multi-modal information, leading to more personalized and contextually relevant recommendations.

**Limitations:** 

**Conclusion:** This research contributes to the field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create better recommendations.

**Abstract:** The importance of recommender systems is growing rapidly due to the exponential increase in the volume of content generated daily. This surge in content presents unique challenges for designing effective recommender systems. Key among these challenges is the need to effectively leverage the vast amounts of natural language data and images that represent user preferences. This paper presents a novel approach to enhancing recommender systems by leveraging Large Language Models (LLMs) and deep learning techniques. The proposed framework aims to improve the accuracy and relevance of recommendations by incorporating multi-modal information processing and by the use of unified latent space representation. The study explores the potential of LLMs to better understand and utilize natural language data in recommendation contexts, addressing the limitations of previous methods. The framework efficiently extracts and integrates text and image information through LLMs, unifying diverse modalities in a latent space to simplify the learning process for the ranking model. Experimental results demonstrate the enhanced discriminative power of the model when utilizing multi-modal information. This research contributes to the evolving field of recommender systems by showcasing the potential of LLMs and multi-modal data integration to create more personalized and contextually relevant recommendations.

</details>


### [88] [LogProber: Disentangling confidence from contamination in LLM responses](https://arxiv.org/abs/2408.14352)

*Nicolas Yax, Pierre-Yves Oudeyer, Stefano Palminteri*

**Main category:** cs.CL

**Keywords:** machine learning, data contamination, Large Language Models, LogProber, evaluation

**Relevance Score:** 8

**TL;DR:** The paper presents LogProber, an efficient algorithm for detecting data contamination in machine learning models, especially focused on Large Language Models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Contamination in machine learning occurs when testing data leaks into the training set, affecting the evaluation of LLM performance. Developing detection tools is crucial for fair performance tracking of these models.

**Method:** LogProber is introduced as a novel algorithm designed to detect contamination in a black box setting, focusing on the familiarity with questions rather than answers.

**Key Contributions:**

	1. Introduction of LogProber, a new algorithm for contamination detection
	2. Comparison with existing methods and highlighting advantages
	3. Identification of scenarios where contamination goes undetected

**Result:** LogProber's properties were explored in comparison to existing methods, highlighting its effectiveness in detecting different forms of contamination that might otherwise go unnoticed.

**Limitations:** The proposed method has certain limitations that need to be considered in practical applications of contamination detection.

**Conclusion:** LogProber addresses some limitations of previous contamination detection methods, although it has its own set of advantages and limitations that must be considered.

**Abstract:** In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. To date, only a few recent studies have attempted to address the issue of quantifying and detecting contamination in short text sequences, such as those commonly found in benchmarks. However, these methods have limitations that can sometimes render them impractical.In the present paper, we introduce LogProber, a novel, efficient algorithm that we show to be able to detect contamination in a black box setting that tries to tackle some of these drawbacks by focusing on the familiarity with the question rather than the answer. Here, we explore the properties of the proposed method in comparison with concurrent approaches, identify its advantages and limitations, and illustrate how different forms of contamination can go undetected depending on the design of the detection algorithm.

</details>


### [89] [Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-thoughts Critic](https://arxiv.org/abs/2408.16326)

*Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun*

**Main category:** cs.CL

**Keywords:** LLM, self-critique, reasoning, task-solving, critique-CoT

**Relevance Score:** 9

**TL;DR:** The paper presents Critic-CoT, a framework that enhances LLMs' reasoning ability through systematic self-critique, improving their task-solving performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLM reasoning by transitioning from basic instance-level feedback to deeper, System-2-like self-critique.

**Method:** Critic-CoT employs a step-wise CoT reasoning paradigm and constructs distant-supervision data automatically, facilitating analytic self-critique.

**Key Contributions:**

	1. Introduction of the Critic-CoT framework for LLM self-critique
	2. Demonstrated mutual reinforcement between critique and task-solving capabilities
	3. Automatic construction of distant-supervision data for enhancing reasoning

**Result:** Experiments show that Critic-CoT significantly improves task-solving performance on GSM8K and MATH by filtering invalid solutions and enabling iterative refinement.

**Limitations:** 

**Conclusion:** Critique and task-solving abilities in LLMs are found to mutually reinforce each other, enhancing overall performance.

**Abstract:** Self-critic has become a crucial mechanism for enhancing the reasoning performance of LLMs. However, current approaches mainly involve basic prompts for intuitive instance-level feedback, which resembles System-1 processes and limits the reasoning capabilities. Moreover, there is a lack of in-depth investigations into the relationship between LLM's ability to criticize and its task-solving performance. To address these issues, we propose Critic-CoT, a novel framework that pushes LLMs toward System-2-like critic capability. Through a step-wise CoT reasoning paradigm and the automatic construction of distant-supervision data without human annotation, Critic-CoT enables LLMs to engage in slow, analytic self-critique and refinement, thereby improving their reasoning abilities. Experiments on GSM8K and MATH demonstrate that our enhanced model significantly boosts task-solving performance by filtering out invalid solutions or iterative refinement. Furthermore, we investigate the intrinsic correlation between critique and task-solving abilities within LLMs, discovering that these abilities can mutually reinforce each other rather than conflict.

</details>


### [90] [Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models](https://arxiv.org/abs/2409.00598)

*Bang An, Sicheng Zhu, Ruiyi Zhang, Michael-Andrei Panaitescu-Liess, Yuancheng Xu, Furong Huang*

**Main category:** cs.CL

**Keywords:** large language models, false refusals, safety, jailbreak attacks, evaluation dataset

**Relevance Score:** 8

**TL;DR:** This paper presents a method to auto-generate diverse pseudo-harmful prompts to evaluate large language models (LLMs), revealing trade-offs in safety and usability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Frequent false refusals of pseudo-harmful prompts by safety-aligned LLMs frustrate users and provoke public backlash against values alignment.

**Method:** The authors propose a method for auto-generating diverse, content-controlled, and model-dependent pseudo-harmful prompts, resulting in a new evaluation dataset called PHTest.

**Key Contributions:**

	1. Introduction of a method to generate pseudo-harmful prompts for LLM evaluation
	2. Creation of the PHTest dataset, which is significantly larger and more comprehensive than previous datasets
	3. Revelation of trade-offs between false refusal rates and safety measures in LLMs

**Result:** The evaluation of 20 LLMs on the PHTest dataset uncovers insights into false refusal patterns and reveals a trade-off between minimizing false refusals and safety against jailbreak attacks.

**Limitations:** The study does not address all possible LLM architectures and their performance on PHTest may vary significantly.

**Conclusion:** The research underscores the need for more usable and safe LLMs and provides a dataset and methodology for future evaluations and improvements.

**Abstract:** Safety-aligned large language models (LLMs) sometimes falsely refuse pseudo-harmful prompts, like "how to kill a mosquito," which are actually harmless. Frequent false refusals not only frustrate users but also provoke a public backlash against the very values alignment seeks to protect. In this paper, we propose the first method to auto-generate diverse, content-controlled, and model-dependent pseudo-harmful prompts. Using this method, we construct an evaluation dataset called PHTest, which is ten times larger than existing datasets, covers more false refusal patterns, and separately labels controversial prompts. We evaluate 20 LLMs on PHTest, uncovering new insights due to its scale and labeling. Our findings reveal a trade-off between minimizing false refusals and improving safety against jailbreak attacks. Moreover, we show that many jailbreak defenses significantly increase the false refusal rates, thereby undermining usability. Our method and dataset can help developers evaluate and fine-tune safer and more usable LLMs. Our code and dataset are available at https://github.com/umd-huang-lab/FalseRefusal

</details>


### [91] [MOSAIC: Multiple Observers Spotting AI Content](https://arxiv.org/abs/2409.07615)

*Matthieu Dubois, François Yvon, Pablo Piantanida*

**Main category:** cs.CL

**Keywords:** Large Language Models, Text Generation, Content Detection, Ensemble Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper proposes a new method for detecting machine-generated text by ensembling multiple Large Language Models (LLMs) to improve performance over traditional single-model approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of LLMs has enabled the easy production of harmful or deceptive text, necessitating robust methods to differentiate machine-generated content from human-written texts.

**Method:** The paper extends traditional detection methods by employing an ensemble of several LLMs, combining their strengths in a theoretically grounded manner for better performance.

**Key Contributions:**

	1. Development of an ensemble method for LLMs in text detection
	2. Theoretically grounded approach combining strengths of multiple models
	3. Demonstrated effectiveness across multiple domains.

**Result:** Experiments demonstrate that this ensemble approach significantly enhances detection capabilities across various content domains compared to using individual models.

**Limitations:** 

**Conclusion:** The proposed method shows promise for improving the robustness of automated content detection systems in the context of LLM-generated text.

**Abstract:** The dissemination of Large Language Models (LLMs), trained at scale, and endowed with powerful text-generating abilities, has made it easier for all to produce harmful, toxic, faked or forged content. In response, various proposals have been made to automatically discriminate artificially generated from human-written texts, typically framing the problem as a binary classification problem. Early approaches evaluate an input document with a well-chosen detector LLM, assuming that low-perplexity scores reliably signal machine-made content. More recent systems instead consider two LLMs and compare their probability distributions over the document to further discriminate when perplexity alone cannot. However, using a fixed pair of models can induce brittleness in performance. We extend these approaches to the ensembling of several LLMs and derive a new, theoretically grounded approach to combine their respective strengths. Our experiments, conducted with various generator LLMs, indicate that this approach effectively leverages the strengths of each model, resulting in robust detection performance across multiple domains. Our code and data are available at https://github.com/BaggerOfWords/MOSAIC .

</details>


### [92] [Explaining word embeddings with perfect fidelity: Case study in research impact prediction](https://arxiv.org/abs/2409.15912)

*Lucie Dvorackova, Marcin P. Joachimiak, Michal Cerny, Adriana Kubecova, Vilem Sklenak, Tomas Kliegr*

**Main category:** cs.CL

**Keywords:** feature importance, word embeddings, scholarly documents, logistic regression, model explanations

**Relevance Score:** 4

**TL;DR:** Introducing SMER, a novel method for feature importance in logistic regression models using word embeddings, offering better explanations than LIME for predicting impactful scholarly articles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in model-agnostic explanation methods like LIME for scholarly document quality prediction using word embeddings.

**Method:** Development of Self-model Rated Entities (SMER) method for logistic regression that ensures fidelity with the explained model by correlating prediction to average predictions of individual words.

**Key Contributions:**

	1. Introduction of SMER for feature importance in word embeddings
	2. Demonstration of perfect fidelity with logistic regression models
	3. Performance superiority of SMER over LIME for explaining model predictions

**Result:** SMER provides more reliable word/entity importance for predicting impactful articles compared to LIME, demonstrated through quantitative and qualitative evaluations on 50,000 research papers from the CORD-19 corpus.

**Limitations:** 

**Conclusion:** SMER outperforms LIME in producing explanations with perfect fidelity to the model, enhancing understanding of features that contribute to article quality.

**Abstract:** Best performing approaches for scholarly document quality prediction are based on embedding models, which do not allow direct explanation of classifiers as distinct words no longer correspond to the input features for model training. Although model-agnostic explanation methods such as Local interpretable model-agnostic explanations (LIME) can be applied, these produce results with questionable correspondence to the ML model. We introduce a new feature importance method, Self-model Rated Entities (SMER), for logistic regression-based classification models trained on word embeddings. We show that SMER has theoretically perfect fidelity with the explained model, as its prediction corresponds exactly to the average of predictions for individual words in the text. SMER allows us to reliably determine which words or entities positively contribute to predicting impactful articles. Quantitative and qualitative evaluation is performed through five diverse experiments conducted on 50.000 research papers from the CORD-19 corpus. Through an AOPC curve analysis, we experimentally demonstrate that SMER produces better explanations than LIME for logistic regression.

</details>


### [93] [GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment](https://arxiv.org/abs/2410.08193)

*Yuancheng Xu, Udari Madhushani Sehwag, Alec Koppel, Sicheng Zhu, Bang An, Furong Huang, Sumitra Ganesh*

**Main category:** cs.CL

**Keywords:** Large Language Models, Autoregressive Reward Model, Human Preferences, Test-time Alignment, Machine Learning

**Relevance Score:** 9

**TL;DR:** GenARM is a novel test-time alignment approach for LLMs that uses Autoregressive Reward Models to predict next-token rewards, significantly improving efficiency and effectiveness in guiding frozen models based on user preferences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of LLMs with user preferences without the high costs and inefficiencies of traditional training-time methods.

**Method:** GenARM leverages a novel parametrization called Autoregressive Reward Model designed for evaluating next-token rewards, enabling efficient autoregressive text generation.

**Key Contributions:**

	1. Introduction of Autoregressive Reward Model for next-token prediction
	2. Significant performance improvement over existing test-time methods
	3. Support for multi-objective alignment to manage user preference trade-offs

**Result:** Experimental results show GenARM significantly outperforms prior test-time alignment methods and matches training-time performance, allowing for flexible preference alignment and efficient model usage.

**Limitations:** 

**Conclusion:** GenARM provides an effective solution for aligning LLMs with diverse user preferences at test time without requiring retraining.

**Abstract:** Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model--a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining. Our project page is available at: https://genarm.github.io.

</details>


### [94] [How Do Multilingual Language Models Remember Facts?](https://arxiv.org/abs/2410.14387)

*Constanza Fierro, Negar Foroutan, Desmond Elliott, Anders Søgaard*

**Main category:** cs.CL

**Keywords:** multilingual LLMs, knowledge recall, Function Vector, language dependency, machine learning

**Relevance Score:** 7

**TL;DR:** This paper analyzes knowledge recall mechanisms in multilingual large language models (LLMs) and finds that while many mechanisms are similar to those in English models, there are unique features based on language and architecture.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how knowledge recall mechanisms in multilingual large language models generalize from English to other languages and to identify unique characteristics of these mechanisms.

**Method:** A comprehensive analysis was conducted on three multilingual LLMs, examining recall mechanisms, and patching intermediate representations to assess the role of language in knowledge retrieval.

**Key Contributions:**

	1. Identified similarities and differences in recall mechanisms between English and multilingual LLMs.
	2. Localized the roles of language in objectives of knowledge extraction and subject enrichment.
	3. Established the concept of the last token representation as a Function Vector influencing both language and content retrieval.

**Result:** The study finds that English recall mechanisms generally apply to multilingual contexts, with language and architecture nuances. Patching revealed that subject enrichment is language-independent and object extraction is language-dependent. The last token representation serves as a Function Vector (FV), encoding language and content, with distinct processing in decoder-only models.

**Limitations:** 

**Conclusion:** Unique mechanisms for recalling information in multilingual LLMs were identified, indicating a need for tailored methodologies such as knowledge evaluation and editing for these models.

**Abstract:** Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has only focused on English monolingual models. The question of how these mechanisms generalize to non-English languages and multilingual LLMs remains unexplored. In this paper, we address this gap by conducting a comprehensive analysis of three multilingual LLMs. First, we show that previously identified recall mechanisms in English largely apply to multilingual contexts, with nuances based on language and architecture. Next, through patching intermediate representations, we localize the role of language during recall, finding that subject enrichment is language-independent, while object extraction is language-dependent. Additionally, we discover that the last token representation acts as a Function Vector (FV), encoding both the language of the query and the content to be extracted from the subject. Furthermore, in decoder-only LLMs, FVs compose these two pieces of information in two separate stages. These insights reveal unique mechanisms in multilingual LLMs for recalling information, highlighting the need for new methodologies -- such as knowledge evaluation, fact editing, and knowledge acquisition -- that are specifically tailored for multilingual LLMs.

</details>


### [95] [Self-Steering Optimization: Autonomous Preference Optimization for Large Language Models](https://arxiv.org/abs/2410.17131)

*Hao Xiang, Bowen Yu, Hongyu Lin, Keming Lu, Yaojie Lu, Xianpei Han, Ben He, Le Sun, Jingren Zhou, Junyang Lin*

**Main category:** cs.CL

**Keywords:** preference data, automated alignment, machine learning, self-steering optimization, reward optimization

**Relevance Score:** 6

**TL;DR:** The paper presents Self-Steering Optimization (SSO), an algorithm for generating high-quality preference data autonomously, aimed at improving automated alignment systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Effective alignment in machine learning relies on high-quality preference data, yet many existing methods neglect quality control leading to inaccuracies.

**Method:** SSO autonomously generates preference data using a specialized optimization objective from the policy model, eliminating the need for manual annotation.

**Key Contributions:**

	1. Introduction of Self-Steering Optimization (SSO) for autonomous preference data generation.
	2. Demonstration of SSO's superior performance compared to baseline methods.
	3. Validation of SSO's scalability for preference optimization.

**Result:** Experiments show that SSO outperforms existing baselines in human preference alignment and reward optimization across diverse benchmarks.

**Limitations:** 

**Conclusion:** SSO serves as a scalable framework for improving automated alignment techniques by ensuring the generation of high-quality preference data.

**Abstract:** The key to effective alignment lies in high-quality preference data. Recent research has focused on automated alignment, which involves developing alignment systems with minimal human intervention. However, prior research has predominantly focused on developing data generation methods, while insufficient attention has been paid to quality control mechanisms, which often produce inaccurate and unhelpful data, leading to unpredictable benefits during iterative optimization. In this paper, we present Self-Steering Optimization ($SSO$), an algorithm that autonomously generates high-quality preference data, eliminating manual annotation requirements. $SSO$ employs a specialized optimization objective to build a data generator from the policy model itself, which is used to produce accurate and on-policy data. We demonstrate $SSO$'s effectiveness through comprehensive experiments on two series of models: Llama 3 and Qwen 2. Our evaluation across diverse benchmarks shows that $SSO$ consistently outperforms baselines in human preference alignment and reward optimization. Further analysis validates $SSO$ as a scalable framework for preference optimization, benefiting the advancement in automated alignment techniques.

</details>


### [96] [Code-Switching Curriculum Learning for Multilingual Transfer in LLMs](https://arxiv.org/abs/2411.02460)

*Haneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, Hwaran Lee*

**Main category:** cs.CL

**Keywords:** code-switching, curriculum learning, cross-lingual transfer, language models, low-resource languages

**Relevance Score:** 9

**TL;DR:** The paper introduces code-switching curriculum learning (CSCL) as a method to enhance cross-lingual transfer in large language models (LLMs) by mimicking human language acquisition stages, achieving significant improvements in performance across languages, particularly for low-resource settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance drop in LLMs after high-resource languages due to data imbalance, inspired by human second language acquisition processes such as code-switching.

**Method:** The proposed method consists of a curriculum with three stages: token-level code-switching, sentence-level code-switching, and training with monolingual corpora, tested on the Qwen 2 model.

**Key Contributions:**

	1. Introduction of code-switching curriculum learning (CSCL) for LLMs
	2. Demonstration of significant performance gains in cross-lingual tasks
	3. Mitigation of spurious correlations in language resources and safety alignment.

**Result:** CSCL significantly improves language transfer to Korean and other languages, outperforming traditional monolingual continual pre-training methods.

**Limitations:** 

**Conclusion:** CSCL provides a robust framework for enhancing cross-lingual transfer in LLMs, particularly benefiting low-resource language contexts.

**Abstract:** Large language models (LLMs) now exhibit near human-level performance in various tasks, but their performance drops drastically after a handful of high-resource languages due to the imbalance in pre-training data. Inspired by the human process of second language acquisition, particularly code-switching$\unicode{x2014}$the practice of language alternation in a conversation$\unicode{x2014}$we propose code-switching curriculum learning (CSCL) to enhance cross-lingual transfer for LLMs. CSCL mimics the stages of human language learning by progressively training models with a curriculum consisting of 1) token-level code-switching, 2) sentence-level code-switching, and 3) monolingual corpora. Using Qwen 2 as our underlying model, we demonstrate the efficacy of the CSCL in improving language transfer to Korean, achieving significant performance gains compared to monolingual continual pre-training methods. Ablation studies reveal that both token- and sentence-level code-switching significantly enhance cross-lingual transfer and that curriculum learning amplifies these effects. We also extend our findings into various languages, including Japanese (high-resource) and Indonesian (low-resource), and using two additional models (Gemma 2 and Phi 3.5). We further show that CSCL mitigates spurious correlations between language resources and safety alignment, presenting a robust, efficient framework for more equitable language transfer in LLMs. We observe that CSCL is effective for low-resource settings where high-quality, monolingual corpora for language transfer are hardly available.

</details>


### [97] [CROW: Eliminating Backdoors from Large Language Models via Internal Consistency Regularization](https://arxiv.org/abs/2411.12768)

*Nay Myat Min, Long H. Pham, Yige Li, Jun Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, backdoor attacks, Internal Consistency Regularization, text generation, machine learning

**Relevance Score:** 9

**TL;DR:** CROW is a defense method against backdoor attacks in Large Language Models that maintains model performance while neutralizing hidden triggers through internal consistency regularization.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerability of Large Language Models (LLMs) to backdoor attacks in text generation tasks, where existing defenses fail.

**Method:** CROW leverages unstable layer-wise hidden representations to enforce consistency across layers during finetuning with adversarial perturbations and regularization, requiring only a small clean dataset.

**Key Contributions:**

	1. Proposes Internal Consistency Regularization (CROW) as a novel defense mechanism for LLMs.
	2. Demonstrates effectiveness across different models and backdoor attack strategies.
	3. Provides an architecture-agnostic solution for deployment.

**Result:** CROW significantly reduces attack success rates for various backdoor strategies while preserving generative performance, validated on Llama-2 and CodeLlama models.

**Limitations:** 

**Conclusion:** Internal Consistency Regularization effectively neutralizes backdoors in LLMs without needing clean reference models or trigger knowledge, making it practical for real-world applications.

**Abstract:** Large Language Models (LLMs) are vulnerable to backdoor attacks that manipulate outputs via hidden triggers. Existing defense methods--designed for vision/text classification tasks--fail for text generation. We propose Internal Consistency Regularization (CROW), a defense leveraging the observation that backdoored models exhibit unstable layer-wise hidden representations when triggered, while clean models show smooth transitions. CROW enforces consistency across layers via adversarial perturbations and regularization during finetuning, neutralizing backdoors without requiring clean reference models or trigger knowledge--only a small clean dataset. Experiments across Llama-2 (7B, 13B), CodeLlama (7B, 13B), and Mistral-7B demonstrate CROW's effectiveness: it achieves significant reductions in attack success rates across diverse backdoor strategies (sentiment steering, targeted refusal, code injection) while preserving generative performance. CROW's architecture-agnostic design enables practical deployment.

</details>


### [98] [Meaningless is better: hashing bias-inducing words in LLM prompts improves performance in logical reasoning and statistical learning](https://arxiv.org/abs/2411.17304)

*Milena Chadimová, Eduard Jurášek, Tomáš Kliegr*

**Main category:** cs.CL

**Keywords:** large language models, cognitive biases, hashing, bias reduction, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents a method called "hashing" to mask bias-inducing words in large language models, aiming to reduce cognitive biases and external knowledge reliance, showing significant performance improvements across various models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address cognitive biases and improve the performance of large language models in various tasks.

**Method:** The method involves masking potentially bias-inducing words with meaningless identifiers (hash-like) and evaluating its effectiveness through three experimental setups involving 490 prompts.

**Key Contributions:**

	1. Introduction of the hashing method for bias reduction in LLMs
	2. Demonstrated effectiveness across multiple models and tasks
	3. Statistical analysis affirming the method's significant improvements

**Result:** Hashing led to a significant decrease in fallacy rates on the Linda problem and improved results in frequent itemset extraction tasks across different models.

**Limitations:** Effectiveness of hashing is model- and task-dependent with inconsistent reduction in hallucination rates.

**Conclusion:** Masking bias-inducing terms can enhance LLM performance for certain tasks, although the effectiveness varies by model and task, with inconsistent reductions in hallucination rates.

**Abstract:** This paper introduces a novel method, referred to as "hashing", which involves masking potentially bias-inducing words in large language models (LLMs) with hash-like meaningless identifiers to reduce cognitive biases and reliance on external knowledge. The method was tested across three sets of experiments involving a total of 490 prompts. Statistical analysis using chi-square tests showed significant improvements in all tested scenarios, which covered LLama, ChatGPT, Copilot, Gemini and Mixtral models. In the first experiment, hashing decreased the fallacy rate in a modified version of the "Linda" problem aimed at evaluating susceptibility to cognitive biases. In the second experiment, it improved LLM results on the frequent itemset extraction task. In the third experiment, we found hashing is also effective when the Linda problem is presented in a tabular format rather than text, indicating that the technique works across various input representations. Overall, the method was shown to improve bias reduction and incorporation of external knowledge. Despite bias reduction, hallucination rates were inconsistently reduced across types of LLM models. These findings suggest that masking bias-inducing terms can improve LLM performance, although its effectiveness is model- and task-dependent.

</details>


### [99] [Retrofitting Large Language Models with Dynamic Tokenization](https://arxiv.org/abs/2411.18553)

*Darius Feher, Ivan Vulić, Benjamin Minixhofer*

**Main category:** cs.CL

**Keywords:** dynamic tokenization, language models, subword merging, byte-pair encoding, inference speed

**Relevance Score:** 7

**TL;DR:** The paper proposes a dynamic tokenization method for language models to improve efficiency and performance across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current language models suffer from inefficiencies due to their static subword tokenization, which negatively impacts performance, particularly for non-English languages.

**Method:** The approach involves a subword-merging algorithm inspired by byte-pair encoding, allowing for dynamic token boundary decisions based on input text. This method calculates token embeddings in real-time using a pre-trained embedding-prediction hypernetwork.

**Key Contributions:**

	1. Introduction of dynamic tokenization for language models.
	2. Substantial reduction in token sequence lengths across multiple languages.
	3. Enhanced performance characteristics for both encoder and decoder-style models.

**Result:** The method reduces token sequence lengths by over 20% on average for encoder-style models while degrading performance by less than 2%. For decoder-style models, it leads to minimal performance loss with up to a 17% reduction in sequence length.

**Limitations:** 

**Conclusion:** Dynamic tokenization effectively addresses the shortcomings of static tokenization, enhancing inference speed and promoting fairness across languages in language models.

**Abstract:** Current language models (LMs) use a fixed, static subword tokenizer. This default choice typically results in degraded efficiency and language capabilities, especially in languages other than English. To address this issue, we challenge the static design and propose retrofitting LMs with dynamic tokenization: a way to dynamically decide on token boundaries based on the input text via a subword-merging algorithm inspired by byte-pair encoding. We merge frequent subword sequences in a batch, then apply a pre-trained embedding-prediction hypernetwork to compute the token embeddings on-the-fly. For encoder-style models (e.g., XLM-R), this on average reduces token sequence lengths by >20% across 14 languages while degrading performance by less than 2%. The same method applied to pre-filling and scoring in decoder-style models (e.g., Mistral-7B) results in minimal performance degradation at up to 17% reduction in sequence length. Overall, we find that dynamic tokenization can mitigate the limitations of static tokenization by substantially improving inference speed and promoting fairness across languages, enabling more equitable and adaptable LMs.

</details>


### [100] [Steps are all you need: Rethinking STEM Education with Prompt Engineering](https://arxiv.org/abs/2412.05023)

*Krishnasai Addala, Kabir Dev Paul Baghel, Navya Gupta, Rishitej Reddy Vyalla, Chhavi Kirtani, Avinash Anand, Rajiv Ratn Shah*

**Main category:** cs.CL

**Keywords:** Physics Question Answering, Mixture of Experts Model, Analogical prompting, Few shot prompting, Chain-of-Thought prompting

**Relevance Score:** 6

**TL;DR:** This paper presents improved performance in Physics Question Answering by employing a Mixture of Experts Model and Analogical CoT prompting, addressing limitations in existing LLM approaches.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methodologies in Physics Question Answering using few shot and Chain-of-Thought prompting are hindered by the mathematical limitations of LLMs and their tendency to hallucinate, necessitating new approaches.

**Method:** The authors utilize a Mixture of Experts Model in conjunction with analogical prompting to enhance model performance and reliability, comparing it to baseline performance on standard LLMs.

**Key Contributions:**

	1. Introduction of Analogical CoT prompting for improved model performance
	2. Demonstration of Mixture of Experts Model in enhancing LLM capabilities
	3. Survey of the limitations of existing prompting techniques and their impact on LLM performance

**Result:** The proposed techniques demonstrate significant improvement over baseline models in Physics Question Answering tasks, especially for smaller, open-source models that previously struggled with similar prompting techniques.

**Limitations:** The paper discusses limitations in the generalizability of the proposed methods and the need for more specialized training data for effective implementation.

**Conclusion:** Analogical CoT prompting has the potential to make smaller open-source models more effective in complex reasoning tasks by leveraging analogical reasoning, which is typically underutilized due to training data limitations.

**Abstract:** Few shot and Chain-of-Thought prompting have shown promise when applied to Physics Question Answering Tasks, but are limited by the lack of mathematical ability inherent to LLMs, and are prone to hallucination. By utilizing a Mixture of Experts (MoE) Model, along with analogical prompting, we are able to show improved model performance when compared to the baseline on standard LLMs. We also survey the limits of these prompting techniques and the effects they have on model performance. Additionally, we propose Analogical CoT prompting, a prompting technique designed to allow smaller, open source models to leverage Analogical prompting, something they have struggled with, possibly due to a lack of specialist training data.

</details>


### [101] [Knowledge Graphs are all you need: Leveraging KGs in Physics Question Answering](https://arxiv.org/abs/2412.05453)

*Krishnasai Addala, Kabir Dev Paul Baghel, Dhruv Jain, Navya Gupta, Rishitej Reddy Vyalla, Chhavi Kirtani, Avinash Anand, Rajiv Ratn Shah*

**Main category:** cs.CL

**Keywords:** knowledge graphs, large language models, sub-questions, educational methodology, Question Answering

**Relevance Score:** 8

**TL;DR:** This study presents a method using knowledge graphs generated by LLMs to decompose physics questions into sub-questions, improving response quality in Question Answering tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance model response quality for educational Question Answering tasks and improve the clarity of sub-questions derived from high school-level physics questions.

**Method:** Utilization of large language models to generate knowledge graphs that reflect the internal logic of physics questions, which guide the decomposition into sub-questions.

**Key Contributions:**

	1. Introduction of a knowledge graph-based decomposition approach for physics questions
	2. Showcasing improved fidelity of generated sub-questions
	3. Highlighting transformational potential of LLMs in education

**Result:** Sub-questions created from knowledge graphs are more logically consistent with the original questions than those produced by traditional methods, leading to an improved learning experience.

**Limitations:** 

**Conclusion:** The approach demonstrates the potential of LLMs in transforming educational methodologies by enhancing the quality of educational content and learning experiences.

**Abstract:** This study explores the effectiveness of using knowledge graphs generated by large language models to decompose high school-level physics questions into sub-questions. We introduce a pipeline aimed at enhancing model response quality for Question Answering tasks. By employing LLMs to construct knowledge graphs that capture the internal logic of the questions, these graphs then guide the generation of subquestions. We hypothesize that this method yields sub-questions that are more logically consistent with the original questions compared to traditional decomposition techniques. Our results show that sub-questions derived from knowledge graphs exhibit significantly improved fidelity to the original question's logic. This approach not only enhances the learning experience by providing clearer and more contextually appropriate sub-questions but also highlights the potential of LLMs to transform educational methodologies. The findings indicate a promising direction for applying AI to improve the quality and effectiveness of educational content.

</details>


### [102] [7B Fully Open Source Moxin-LLM/VLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement](https://arxiv.org/abs/2412.06845)

*Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Open-source, Transparency, Reproducibility, AI safety

**Relevance Score:** 9

**TL;DR:** Moxin 7B is an open-source Large Language Model developed to enhance transparency, reproducibility, and safety in AI, featuring several fine-tuned versions for improved reasoning capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses concerns about the lack of transparency and reproducibility in LLMs due to proprietary models, while also celebrating the potential of open-source models for innovation and development.

**Method:** Moxin 7B was developed by adhering to open science principles; it includes releasing training code and fine-tuning datasets. Multiple models were created, including the base model, Moxin Instruct model with SOTA post-training techniques, and Moxin Reasoning model with advanced fine-tuning methodologies.

**Key Contributions:**

	1. Release of Moxin 7B as a fully open-source LLM with comprehensive resources.
	2. Introduction of enhanced models including Moxin Instruct and Moxin Reasoning for improved task performance.
	3. Commitment to open science principles, addressing transparency issues prevalent in proprietary LLMs.

**Result:** Moxin models demonstrate superior performance in evaluations like zero-shot, few-shot, and chain-of-thought evaluations, showcasing their effectiveness and capabilities in various tasks.

**Limitations:** 

**Conclusion:** The development and release of Moxin 7B and its variants set a new standard for transparency in LLM research, aiming to foster innovation while ensuring safety and reproducibility.

**Abstract:** Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO) following DeepSeek R1 to finetune our model, leading to the Moxin Reasoning model. Moreover, we develop our vision language model based on our Moxin model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation.

</details>


### [103] [Irony Detection, Reasoning and Understanding in Zero-shot Learning](https://arxiv.org/abs/2501.16884)

*Peiling Yi, Yuhan Xia, Yunfei Long*

**Main category:** cs.CL

**Keywords:** irony detection, LLMs, zero-shot capabilities, contextual awareness, multimodal data

**Relevance Score:** 7

**TL;DR:** Study addresses challenges in irony detection by using irony-focused prompts generated from the IDADP framework for LLMs, enhancing zero-shot capabilities in diverse scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The generalisation of irony detection is challenging, leading to poor performance in various real-world applications.

**Method:** Utilizes irony-focused prompts generated from the IDADP framework for language models to improve irony detection models.

**Key Contributions:**

	1. Development of IDADP framework for irony detection
	2. Demonstration of improved reasoning for interpreting irony
	3. Identification of future research avenues for LLMs in irony detection

**Result:** The study demonstrates that the IDADP framework can overcome dataset-specific limitations and provides coherent reasoning for interpreting ironic text.

**Limitations:** 

**Conclusion:** There are promising future research directions for improving irony detection in LLMs, which include enhancing contextual awareness and integrating multimodal data.

**Abstract:** The generalisation of irony detection faces significant challenges, leading to substantial performance deviations when detection models are applied to diverse real-world scenarios. In this study, we find that irony-focused prompts, as generated from our IDADP framework for LLMs, can not only overcome dataset-specific limitations but also generate coherent, human-readable reasoning, transforming ironic text into its intended meaning. Based on our findings and in-depth analysis, we identify several promising directions for future research aimed at enhancing LLMs' zero-shot capabilities in irony detection, reasoning, and comprehension. These include advancing contextual awareness in irony detection, exploring hybrid symbolic-neural methods, and integrating multimodal data, among others.

</details>


### [104] [Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for Multimodal Reasoning Models](https://arxiv.org/abs/2502.16033)

*Qianqi Yan, Yue Fan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, Xin Eric Wang*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Inconsistency Reasoning, Benchmark, Semantic Mismatches, Reasoning

**Relevance Score:** 8

**TL;DR:** The paper introduces the MMIR benchmark to evaluate Multimodal Large Language Models (MLLMs) on their ability to reason about inconsistencies in real-world visual-textual content, revealing performance gaps and future research directions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the ability of MLLMs to handle inconsistencies in layout-rich, real-world content, which previous research has not thoroughly explored.

**Method:** The authors propose the MMIR benchmark consisting of 534 samples with five categories of reasoning-heavy semantic mismatches and evaluate six state-of-the-art MLLMs on this benchmark.

**Key Contributions:**

	1. Introduction of the MMIR benchmark for multimodal inconsistency reasoning.
	2. Comprehensive evaluation of state-of-the-art MLLMs on the benchmark.
	3. Identified performance gaps and challenges in current models regarding multimodal reasoning.

**Result:** Models with dedicated multimodal reasoning capabilities outperform others, but open-source models struggle with inconsistency errors, particularly in handling complex layouts.

**Limitations:** The benchmark relies on synthetic errors, which may not fully capture real-world inconsistencies.

**Conclusion:** The study indicates that while MLLMs can detect pairwise inconsistencies well, they face challenges with single-element inconsistencies and calls for advancements in multimodal reasoning.

**Abstract:** Existing Multimodal Large Language Models (MLLMs) are predominantly trained and tested on consistent visual-textual inputs, leaving open the question of whether they can handle inconsistencies in real-world, layout-rich content. To bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR) benchmark to assess MLLMs' ability to detect and reason about semantic mismatches in artifacts such as webpages, presentation slides, and posters. MMIR comprises 534 challenging samples, each containing synthetically injected errors across five reasoning-heavy categories: Factual Contradiction, Identity Misattribution, Contextual Mismatch, Quantitative Discrepancy, and Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing that models with dedicated multimodal reasoning capabilities, such as o1, substantially outperform their counterparts while open-source models remain particularly vulnerable to inconsistency errors. Detailed error analyses further show that models excel in detecting pairwise inconsistencies but struggle with inconsistencies confined to single elements in complex layouts. Probing experiments reveal that single-modality prompting, including Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains, revealing a key bottleneck in cross-modal reasoning. Our findings highlight the need for advanced multimodal reasoning and point to future research on multimodal inconsistency.

</details>


### [105] [Revisiting Self-Consistency from Dynamic Distributional Alignment Perspective on Answer Aggregation](https://arxiv.org/abs/2502.19830)

*Yiwei Li, Ji Zhang, Shaoxiong Feng, Peiwen Yuan, Xinglin Wang, Jiayi Shi, Yueqi Zhang, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li*

**Main category:** cs.CL

**Keywords:** self-consistency, distributional alignment, sampling dynamics, reasoning, temperature calibration

**Relevance Score:** 7

**TL;DR:** The paper explores self-consistency in reasoning, framing it as a dynamic distributional alignment problem, and proposes a confidence-driven mechanism to optimize sampling temperature, improving performance in mathematical reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to understand the dynamics behind self-consistency in reasoning using stochastic samples and improve its efficacy by addressing temperature management during sampling.

**Method:** Introduces a confidence-driven mechanism that dynamically adjusts decoding temperature based on uncertainty, thus aligning the sampling distribution with high-probability modes and promoting exploration.

**Key Contributions:**

	1. Reframed self-consistency as a dynamic distributional alignment problem
	2. Proposed a confidence-driven mechanism for temperature calibration
	3. Demonstrated improved performance on mathematical reasoning tasks under constrained samples

**Result:** The proposed method outperformed fixed-diversity baselines on mathematical reasoning tasks, achieving better average and best-case performance with limited samples and without extra data or modules.

**Limitations:** 

**Conclusion:** Self-consistency is framed as a synchronization challenge between sampling dynamics and answer distributions, suggesting temperature management is crucial for optimizing reasoning.

**Abstract:** Self-consistency improves reasoning by aggregating diverse stochastic samples, yet the dynamics behind its efficacy remain underexplored. We reframe self-consistency as a dynamic distributional alignment problem, revealing that decoding temperature not only governs sampling randomness but also actively shapes the latent answer distribution. Given that high temperatures require prohibitively large sample sizes to stabilize, while low temperatures risk amplifying biases, we propose a confidence-driven mechanism that dynamically calibrates temperature: sharpening the sampling distribution under uncertainty to align with high-probability modes, and promoting exploration when confidence is high. Experiments on mathematical reasoning tasks show this approach outperforms fixed-diversity baselines under limited samples, improving both average and best-case performance across varying initial temperatures without additional data or modules. This establishes self-consistency as a synchronization challenge between sampling dynamics and evolving answer distributions.

</details>


### [106] [AskToAct: Enhancing LLMs Tool Use via Self-Correcting Clarification](https://arxiv.org/abs/2503.01940)

*Xuan Zhang, Yongliang Shen, Zhe Zheng, Linjuan Wu, Wenqi Zhang, Yuchen Yan, Qiuying Peng, Jun Wang, Weiming Lu*

**Main category:** cs.CL

**Keywords:** large language models, tool learning, clarification, error correction, API generalization

**Relevance Score:** 9

**TL;DR:** AskToAct is a framework for improving tool learning in large language models by automating the construction of high-quality training data and enhancing robustness through error correction mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing interactive clarification approaches for LLMs are limited by reliance on manually constructed datasets and a lack of error correction, affecting accuracy and efficiency.

**Method:** AskToAct utilizes structural mapping between queries and tool invocation solutions, systematically removing key parameters from queries while keeping them as ground truth to create automated and diverse training data. It introduces error-correction pairs and selective masking for dynamic error detection.

**Key Contributions:**

	1. Automated construction of high-quality training data for clarification tasks
	2. Error correction mechanisms during multi-turn interactions
	3. Generalization to unseen APIs without extra training

**Result:** AskToAct outperforms existing approaches, achieving over 57% accuracy in recovering unspecified intents and improving clarification efficiency by an average of 10.46%, while maintaining tool invocation accuracy.

**Limitations:** 

**Conclusion:** The framework demonstrates robust performance across various model architectures and generalizes well to unseen APIs without additional training, achieving results comparable to advanced models with lesser computational resources.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities in tool learning. In real-world scenarios, user queries are often ambiguous and incomplete, requiring effective clarification. However, existing interactive clarification approaches face two critical limitations: reliance on manually constructed datasets, which inherently constrains training data scale and diversity, and lack of error correction mechanisms during multi-turn clarification, leading to error accumulation that compromises both accuracy and efficiency. We present AskToAct, which addresses these challenges by exploiting the structural mapping between queries and their tool invocation solutions. Our key insight is that tool parameters naturally represent explicit user intents. By systematically removing key parameters from queries while retaining them as ground truth, we enable automated construction of high-quality training data. We further enhance model robustness through error-correction pairs and selective masking, enabling dynamic error detection during clarification interactions. Comprehensive experiments demonstrate that AskToAct significantly outperforms existing approaches, achieving above 57% accuracy in recovering critical unspecified intents and enhancing clarification efficiency by an average of 10.46% while maintaining high accuracy in tool invocation. Our framework exhibits robust performance across different model architectures and successfully generalizes to entirely unseen APIs without additional training, achieving performance comparable to GPT-4o with substantially fewer computational resources.

</details>


### [107] [Unlocking General Long Chain-of-Thought Reasoning Capabilities of Large Language Models via Representation Engineering](https://arxiv.org/abs/2503.11314)

*Xinyu Tang, Xiaolei Wang, Zhihao Lv, Yingqian Min, Wayne Xin Zhao, Binbin Hu, Ziqi Liu, Zhiqiang Zhang*

**Main category:** cs.CL

**Keywords:** long CoT reasoning, large language models, representation engineering, GLoRE, transfer learning

**Relevance Score:** 9

**TL;DR:** This paper explores the general capability of long chain-of-thought reasoning in large language models (LLMs), proposing a method named GLoRE for effective representation engineering.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate if long CoT reasoning is a general capability of LLMs and to enhance its transferability across tasks.

**Method:** Empirical analysis of long CoT reasoning in LLMs and the development of GLoRE for representation engineering.

**Key Contributions:**

	1. Identification of long CoT reasoning as a general capability of LLMs
	2. Development of GLoRE for representation engineering
	3. Evidence of effective transfer requirements in domain-specific contexts

**Result:** The study finds that LLMs encode long CoT reasoning as a distinct capability, requiring domain-specific representations for effective transfer.

**Limitations:** 

**Conclusion:** GLoRE effectively harnesses the general long CoT reasoning capabilities of LLMs, demonstrating efficiency in both in-domain and cross-domain scenarios.

**Abstract:** Recent advancements in long chain-of-thoughts(long CoTs) have significantly improved the reasoning capabilities of large language models(LLMs). Existing work finds that the capability of long CoT reasoning can be efficiently elicited by tuning on only a few examples and can easily transfer to other tasks. This motivates us to investigate whether long CoT reasoning is a general capability for LLMs. In this work, we conduct an empirical analysis for this question from the perspective of representation. We find that LLMs do encode long CoT reasoning as a general capability, with a clear distinction from vanilla CoTs. Furthermore, domain-specific representations are also required for the effective transfer of long CoT reasoning. Inspired by these findings, we propose GLoRE, a novel representation engineering method to unleash the general long CoT reasoning capabilities of LLMs. Extensive experiments demonstrate the effectiveness and efficiency of GLoRE in both in-domain and cross-domain scenarios.

</details>


### [108] [Style over Substance: Distilled Language Models Reason Via Stylistic Replication](https://arxiv.org/abs/2504.01738)

*Philip Lippmann, Jie Yang*

**Main category:** cs.CL

**Keywords:** Reasoning Language Models, Knowledge Distillation, Stylistic Patterns

**Relevance Score:** 7

**TL;DR:** This study explores how specialized reasoning language models (RLMs) enhance performance through detailed reasoning traces and examines how distilled models internalize stylistic patterns during reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the transfer of reasoning abilities in distilled models and how stylistic patterns influence model performance is crucial for improving language models.

**Method:** The authors analyze reasoning traces to identify structural and lexical patterns, and introduce two new datasets to examine the influence of these patterns on distilled models' reasoning capabilities.

**Key Contributions:**

	1. Introduction of two new datasets for analyzing reasoning traces
	2. Findings that performance improves with synthetic traces, even if misleading
	3. Identification of reliance on surface-level patterns for distilled models' reasoning

**Result:** Models trained on synthetic reasoning traces exhibit comparable performance to those trained on authentic traces, suggesting reliance on surface-level patterns.

**Limitations:** The study primarily focuses on surface-level patterns and may not account for deeper semantic understanding.

**Conclusion:** The findings indicate that stylistic patterns significantly enhance language model reasoning, even when input traces are misleading.

**Abstract:** Specialized reasoning language models (RLMs) have demonstrated that scaling test-time computation through detailed reasoning traces significantly enhances performance. Although these traces effectively facilitate knowledge distillation into smaller, instruction-tuned models, the precise nature of transferred reasoning remains unclear. In this study, we investigate to what extent distilled models internalize replicated stylistic patterns during reasoning. To this end, we systematically analyze reasoning traces, identifying structural and lexical patterns that characterize successful reasoning. We then introduce two new datasets -- a dataset of emergent reasoning traces and a synthetic dataset explicitly constructed to replicate these stylistic patterns -- to precisely examine their influence on distilled models' reasoning capabilities. We find that models trained on the synthetic traces achieve comparable performance, indicating that distilled reasoning abilities rely significantly on surface-level patterns. Surprisingly, we observe an increase in performance even when the synthetic traces are altered to lead to the wrong answer. Our findings highlight how stylistic patterns can be leveraged to efficiently enhance LM reasoning across diverse model families.

</details>


### [109] [One Pic is All it Takes: Poisoning Visual Document Retrieval Augmented Generation with a Single Image](https://arxiv.org/abs/2504.02132)

*Ezzeldin Shereen, Dan Ristea, Shae McFadden, Burak Hasircioglu, Vasilios Mavroudis, Chris Hicks*

**Main category:** cs.CL

**Keywords:** multi-modal retrieval, poisoning attack, adversarial image, knowledge base, M-RAG

**Relevance Score:** 8

**TL;DR:** The paper introduces the first poisoning attack against multi-modal retrieval augmented generation (M-RAG) systems by proposing two types of attacks that inject adversarial images into the knowledge base to influence response generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities of M-RAG systems in visual document retrieval applications due to potential attacks from adversaries injecting malicious entries into the knowledge base.

**Method:** Two types of poisoning attacks are proposed: a universal attack that causes denial-of-service responses for any query, and a targeted attack that distributes misinformation for specific queries. Both methods employ a multi-objective gradient-based adversarial approach to optimize the injected images.

**Key Contributions:**

	1. First poisoning attack in M-RAG frameworks
	2. Proposed universal and targeted poisoning attacks
	3. Evaluation against state-of-the-art systems with detailed analysis of attack effects.

**Result:** The proposed attacks were evaluated on various visual document retrieval datasets and demonstrated effectiveness against multiple state-of-the-art retrievers and generators, highlighting the impact of each attack in both universal and targeted scenarios.

**Limitations:** The effectiveness of defense mechanisms against these attacks was not extensively explored; additional work is needed to evaluate real-world implications on user queries.

**Conclusion:** These findings reveal significant vulnerabilities in M-RAG systems and the necessity for robust defenses against such poisoning attacks in multi-modal applications.

**Abstract:** Multi-modal retrieval augmented generation (M-RAG) is instrumental for inhibiting hallucinations in large multi-modal models (LMMs) through the use of a factual knowledge base (KB). However, M-RAG introduces new attack vectors for adversaries that aim to disrupt the system by injecting malicious entries into the KB. In this paper, we present the first poisoning attack against M-RAG targeting visual document retrieval applications where the KB contains images of document pages. We propose two attacks, each of which require injecting only a single adversarial image into the KB. Firstly, we propose a universal attack that, for any potential user query, influences the response to cause a denial-of-service (DoS) in the M-RAG system. Secondly, we present a targeted attack against one or a group of user queries, with the goal of spreading targeted misinformation. For both attacks, we use a multi-objective gradient-based adversarial approach to craft the injected image while optimizing for both retrieval and generation. We evaluate our attacks against several visual document retrieval datasets, a diverse set of state-of-the-art retrievers (embedding models) and generators (LMMs), demonstrating the attack effectiveness in both the universal and targeted settings. We additionally present results including commonly used defenses, various attack hyper-parameter settings, ablations, and attack transferability.

</details>


### [110] [Can LLMs Interpret and Leverage Structured Linguistic Representations? A Case Study with AMRs](https://arxiv.org/abs/2504.04745)

*Ankush Raut, Xiaofeng Zhu, Maria Leonor Pacheco*

**Main category:** cs.CL

**Keywords:** Large Language Models, Abstract Meaning Representation, contextual information, dialogue summarization, language tasks

**Relevance Score:** 9

**TL;DR:** Evaluation of LLMs' performance using structured linguistic representations for context in language tasks, focusing on Abstract Meaning Representation (AMR).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effectiveness of providing contextual information to LLMs through structured representations in various language tasks.

**Method:** We analyzed the use of AMR structures with different models (Llama 3.1, Phi-3, and Mistral 7B), focusing on both short and long contexts across multiple language tasks.

**Key Contributions:**

	1. Demonstrated varying impacts of AMR on LLM performance based on context length.
	2. Identified the specific conditions under which AMR enhances LLM tasks, particularly in dialogue summarization.
	3. Provided insights on the reconstruction ability of LLMs using AMR structures.

**Result:** Augmenting prompts with AMR degrades performance for short contexts but enhances performance for long contexts, especially in dialogue summarization, increasing similarity scores significantly in newer LLMs.

**Limitations:** The performance impacts are not uniform across different models, with older or smaller LLMs showing little to no improvement.

**Conclusion:** While AMR can be useful, its effectiveness varies depending on context length and model size, indicating that not all models benefit equally from this augmentation strategy.

**Abstract:** This paper evaluates the ability of Large Language Models (LLMs) to leverage contextual information in the form of structured linguistic representations. Specifically, we examine the impact of encoding both short and long contexts using Abstract Meaning Representation (AMR) structures across a diverse set of language tasks. We perform our analysis using 8-bit quantized and instruction-tuned versions of Llama 3.1 (8B), Phi-3, and Mistral 7B. Our results indicate that, for tasks involving short contexts, augmenting the prompt with the AMR of the original language context often degrades the performance of the underlying LLM. However, for tasks that involve long contexts, such as dialogue summarization in the SAMSum dataset, this enhancement improves LLM performance, for example, by increasing the zero-shot cosine similarity score of Llama 3.1 from 66% to 76%. This improvement is more evident in the newer and larger LLMs, but does not extend to the older or smaller ones. In addition, we observe that LLMs can effectively reconstruct the original text from a linearized AMR, achieving a cosine similarity of 81% in the best-case scenario.

</details>


### [111] [Assessment of Evolving Large Language Models in Upper Secondary Mathematics](https://arxiv.org/abs/2504.12347)

*Mika Setälä, Pieta Sikström, Ville Heilala, Tommi Kärkkäinen*

**Main category:** cs.CL

**Keywords:** Large language models, Mathematical reasoning, Educational tools, Finnish matriculation examination, Performance evaluation

**Relevance Score:** 8

**TL;DR:** This study assesses the mathematical capabilities of large language models (LLMs) using the Finnish matriculation examination, revealing significant improvements in performance over time.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the evolving mathematical reasoning capabilities of LLMs in educational contexts.

**Method:** The study employed the Finnish matriculation examination as a benchmark to assess the performance of various LLMs, analyzing initial and later evaluations to track improvements.

**Key Contributions:**

	1. Evaluation of LLMs using a high-stakes educational test
	2. Demonstration of significant performance improvements in mathematical reasoning
	3. Insights into the application of LLMs for supporting learning and teaching

**Result:** Initial tests showed moderate performance, but subsequent evaluations indicated that some LLMs achieved near-perfect or perfect scores, comparable to top human students.

**Limitations:** 

**Conclusion:** The rapid advancements in mathematical proficiency of LLMs suggest their potential as supportive tools in educational settings.

**Abstract:** Large language models (LLMs) have shown increasing promise in educational settings, yet their mathematical reasoning has been considered evolving. This study evaluates the mathematical capabilities of various LLMs using the Finnish matriculation examination, a high-stakes digital test for upper secondary education. Initial tests yielded moderate performance corresponding to mid-range grades, but later evaluations demonstrated substantial improvements as the language models evolved. Remarkably, some models achieved near-perfect or perfect scores, matching top student performance and qualifying for university admission. Our findings highlight the rapid advances in the mathematical proficiency of LLMs and illustrate their potential as underlying tools to support learning and teaching in a variety of ways.

</details>


### [112] [Persona-judge: Personalized Alignment of Large Language Models via Token-level Self-judgment](https://arxiv.org/abs/2504.12663)

*Xiaotian Zhang, Ruizhe Chen, Yang Feng, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** language models, personalization, preference alignment, computational efficiency, adaptive alignment

**Relevance Score:** 9

**TL;DR:** Introducing Persona-judge for personalized alignment of language models without additional computational cost.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of aligning language models with diverse human preferences efficiently and scalably.

**Method:** Persona-judge uses a draft model to generate candidate tokens based on a preference, which are then validated by a judge model that embodies another preference, enabling training-free alignment.

**Key Contributions:**

	1. Introduction of a training-free personalized alignment approach
	2. Utilization of intrinsic preference judgment capabilities of models
	3. Demonstrated efficiency and scalability in experiments

**Result:** Experimental findings show that Persona-judge improves scalability and computational efficiency in personalized model alignment while using intrinsic evaluation capabilities.

**Limitations:** 

**Conclusion:** Persona-judge offers a pathway for adaptive personalized alignment of language models without the need for additional annotated data or external rewards.

**Abstract:** Aligning language models with human preferences presents significant challenges, particularly in achieving personalization without incurring excessive computational costs. Existing methods rely on reward signals and additional annotated data, limiting their scalability and adaptability to diverse human values. To address these challenges, we introduce Persona-judge, a novel discriminative paradigm that enables training-free personalized alignment with unseen preferences. Instead of optimizing policy parameters through external reward feedback, Persona-judge leverages the intrinsic preference judgment capabilities of the model. Specifically, a draft model generates candidate tokens conditioned on a given preference, while a judge model, embodying another preference, cross-validates the predicted tokens whether to be accepted. Experimental results demonstrate that Persona-judge, using the inherent preference evaluation mechanisms of the model, offers a scalable and computationally efficient solution to personalized alignment, paving the way for more adaptive customized alignment. Our code is available here.

</details>


### [113] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)

*Yuhan Cao, Zian Chen, Kun Quan, Ziliang Zhang, Yu Wang, Xiaoning Dong, Yeqi Feng, Guanzhong He, Jingcheng Huang, Jianhao Li, Yixuan Tan, Jiafu Tang, Yilin Tang, Junlei Wu, Qianyu Xiao, Can Zheng, Shouchen Zhou, Yuxiang Zhu, Yiming Huang, Tian Xie, Tianxing He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Test Case Generation, Competitive Programming, Code Debugging, Machine Learning

**Relevance Score:** 7

**TL;DR:** This paper introduces TCGBench, a benchmark for evaluating the ability of Large Language Models (LLMs) in generating test case generators for competitive programming (CP) problems and targeted generators that expose bugs in human-written code.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the unexplored potential of LLMs in code checking and debugging through test case generation, particularly in the context of competitive programming problems.

**Method:** The authors proposed TCGBench, consisting of two tasks: one for generating valid test case generators for specific CP problems and another for generating targeted test case generators to expose bugs in human-written code. They conducted experiments with various state-of-the-art LLMs to assess their performance on these tasks.

**Key Contributions:**

	1. Introduction of TCGBench as a benchmark for evaluating LLMs in test case generation.
	2. Creation of a high-quality dataset for generating targeted test case generators.
	3. Comparative analysis of LLM performance against human benchmarks in code debugging tasks.

**Result:** The experimental results revealed that while most LLMs can generate valid test case generators, they generally struggle with generating targeted test cases that effectively reveal bugs in code, with advanced models significantly underperforming compared to human capabilities.

**Limitations:** LLMs' performance on generating targeted test cases is significantly below human performance, and further enhancements are necessary to close this gap.

**Conclusion:** The study concludes that although LLMs show some capability in generating test case generators, there's a significant gap in their ability to produce targeted generators that expose flaws effectively. The authors also provide a high-quality dataset to assist in improving LLM performance through prompting and fine-tuning.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, capable of tackling complex tasks during inference. However, the extent to which LLMs can be utilized for code checking or debugging through test case generation remains largely unexplored. We investigate this problem from the perspective of competition-level programming (CP) programs and propose TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This benchmark comprises two tasks, aimed at studying the capabilities of LLMs in (1) generating valid test case generators for a given CP problem, and further (2) generating targeted test case generators that expose bugs in human-written code. Experimental results indicate that while state-of-the-art LLMs can generate valid test case generators in most cases, most LLMs struggle to generate targeted test cases that reveal flaws in human code effectively. Especially, even advanced reasoning models (e.g., o3-mini) fall significantly short of human performance in the task of generating targeted generators. Furthermore, we construct a high-quality, manually curated dataset of instructions for generating targeted generators. Analysis demonstrates that the performance of LLMs can be enhanced with the aid of this dataset, by both prompting and fine-tuning.

</details>


### [114] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)

*Lei Xu, Sirui Chen, Yuxuan Huang, Chaochao Lu*

**Main category:** cs.CL

**Keywords:** Mathematical reasoning, LLMs, Data generation, Fine-tuning, Benchmark

**Relevance Score:** 8

**TL;DR:** The paper proposes a method to enhance mathematical reasoning in LLMs by extracting structural information and guiding data generation, producing a dataset with labeled intermediate steps and a benchmark of higher difficulty, demonstrating improved model performance through fine-tuning experiments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve mathematical reasoning in LLMs, addressing issues of generation quality and problem complexity.

**Method:** Extract structural information with generated problem-solving code and guide data generation with structured solutions; applied to MATH and GSM8K datasets.

**Key Contributions:**

	1. Introduced a novel method for data generation to support LLM mathematical reasoning.
	2. Created a benchmark of difficult problems with labeled intermediate steps.
	3. Demonstrated the effectiveness of the dataset through fine-tuning experiments on various LLMs.

**Result:** Generated 39K problems with labeled intermediate steps and a benchmark of 6.1K higher difficulty problems; performance declines with longer reasoning lengths in LLMs.

**Limitations:** The model performance decline with increased reasoning lengths indicates potential scalability issues.

**Conclusion:** The dataset and method can contribute to enhancing LLM reasoning capabilities and are made available for future research.

**Abstract:** Mathematical reasoning remains challenging for LLMs due to complex logic and the need for precise computation. Existing methods enhance LLM reasoning by synthesizing datasets through problem rephrasing, but face issues with generation quality and problem complexity. To address this, we propose to extract structural information with generated problem-solving code from mathematical reasoning and guide data generation with structured solutions. Applied to MATH and GSM8K, our approach produces 39K problems with labeled intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results on our benchmark show that model performance declines as reasoning length increases. Additionally, we conducted fine-tuning experiments using the proposed training data on a range of LLMs, and the results validate the effectiveness of our dataset. We hope the proposed method and dataset will contribute to future research in enhancing LLM reasoning capabilities. Our code and data are available at https://github.com/OpenCausaLab/StructuralGeneration.

</details>


### [115] [AbstRaL: Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)

*Silin Gao, Antoine Bosselut, Samy Bengio, Emmanuel Abbe*

**Main category:** cs.CL

**Keywords:** large language models, abstract reasoning, reinforcement learning, robustness, synthetic data

**Relevance Score:** 9

**TL;DR:** The paper presents AbstRaL, a method using reinforcement learning to enhance abstract reasoning in large language models (LLMs), addressing robustness issues during distribution shifts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLMs' robustness against distribution shifts in reasoning tasks, which smaller models often struggle with due to performance drops.

**Method:** The approach involves abstracting reasoning problems through reinforcement learning rather than relying solely on supervised fine-tuning, thus creating more reliable abstract reasoning capabilities in LLMs.

**Key Contributions:**

	1. Introduction of AbstRaL, a novel method for enhancing reasoning in LLMs using RL
	2. Demonstration of improved robustness against distribution shifts compared to supervised learning
	3. Connection between abstract reasoning and symbolic tools for deriving solutions.

**Result:** AbstRaL significantly reduces performance degradation on GSM perturbation benchmarks compared to traditional methods.

**Limitations:** 

**Conclusion:** The study suggests that reinforcement learning fosters a better understanding of abstractions in LLMs, leading to improved reasoning under various conditions.

**Abstract:** Recent studies have shown that large language models (LLMs), especially smaller ones, often lack robustness in their reasoning. I.e., they tend to experience performance drops when faced with distribution shifts, such as changes to numerical or nominal variables, or insertions of distracting clauses. A possible strategy to address this involves generating synthetic data to further "instantiate" reasoning problems on potential variations. In contrast, our approach focuses on "abstracting" reasoning problems. This not only helps counteract distribution shifts but also facilitates the connection to symbolic tools for deriving solutions. We find that this abstraction process is better acquired through reinforcement learning (RL) than just supervised fine-tuning, which often fails to produce faithful abstractions. Our method, AbstRaL -- which promotes abstract reasoning in LLMs using RL on granular abstraction data -- significantly mitigates performance degradation on recent GSM perturbation benchmarks.

</details>
