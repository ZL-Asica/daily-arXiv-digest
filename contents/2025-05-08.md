# 2025-05-08

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 38]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Facilitating Video Story Interaction with Multi-Agent Collaborative System](https://arxiv.org/abs/2505.03807)

*Yiwen Zhang, Jianing Hao, Zhan Wang, Hongling Sheng, Wei Zeng*

**Main category:** cs.HC

**Keywords:** Interactive Systems, Video Stories, Vision Language Model, Multi-Agent Systems, Character Growth

**Relevance Score:** 8

**TL;DR:** A new interactive system enhances video story engagement by understanding user intent through a Vision Language Model, integrating retrieval-augmented generation and multi-agent systems for character development and scene customization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing interactive video story methods are limited in customization and personalization, which can hinder viewer engagement and narrative exploration.

**Method:** The system processes video stories using a Vision Language Model to simulate narrative understanding across modalities, incorporates a multi-agent system for character interaction, and allows scene customization based on user dialogue.

**Key Contributions:**

	1. Introduction of a user intent-based interactive system for video stories
	2. Combining Vision Language Models with Retrieval-Augmented Generation and Multi-Agent Systems
	3. Demonstrated application on the Harry Potter series showing emergent character behaviors.

**Result:** The system effectively demonstrated emergent character social behavior and growth in a study applied to the Harry Potter series, leading to enhanced interactivity in video narratives.

**Limitations:** The study is based on a specific narrative (Harry Potter), and the efficacy in broader contexts or different genres is not tested.

**Conclusion:** The proposed interactive system significantly improves personalized engagement with video stories, suggesting new avenues for future narrative experiences.

**Abstract:** Video story interaction enables viewers to engage with and explore narrative content for personalized experiences. However, existing methods are limited to user selection, specially designed narratives, and lack customization. To address this, we propose an interactive system based on user intent. Our system uses a Vision Language Model (VLM) to enable machines to understand video stories, combining Retrieval-Augmented Generation (RAG) and a Multi-Agent System (MAS) to create evolving characters and scene experiences. It includes three stages: 1) Video story processing, utilizing VLM and prior knowledge to simulate human understanding of stories across three modalities. 2) Multi-space chat, creating growth-oriented characters through MAS interactions based on user queries and story stages. 3) Scene customization, expanding and visualizing various story scenes mentioned in dialogue. Applied to the Harry Potter series, our study shows the system effectively portrays emergent character social behavior and growth, enhancing the interactive experience in the video story world.

</details>


### [2] [Scratch Copilot: Supporting Youth Creative Coding with AI](https://arxiv.org/abs/2505.03867)

*Stefania Druga, Amy J. Ko*

**Main category:** cs.HC

**Keywords:** AI coding assistant, creative coding, children, Scratch, youth agency

**Relevance Score:** 5

**TL;DR:** This paper introduces the Cognimates Scratch Copilot, an AI-powered assistant for children, which aids in coding processes such as ideation, code generation, and debugging within a Scratch-like environment.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to assist children in translating imaginative ideas into functional code, addressing the challenges they face in creative coding environments.

**Method:** The paper presents the system architecture of the Cognimates Scratch Copilot and includes findings from a qualitative evaluation involving 18 children aged 7 to 12, focusing on their interactions with the AI system.

**Key Contributions:**

	1. Introduction of Cognimates Scratch Copilot for children in coding environments
	2. Highlighting children's agency in interaction with AI
	3. Proposing design guidelines for supportive AI coding tools

**Result:** The findings indicate that the AI Copilot supported key coding processes, particularly in ideation and debugging, while also revealing children's agency in negotiating AI suggestions.

**Limitations:** The study is exploratory and based on a limited sample size of 18 children, potentially affecting the generalizability of the findings.

**Conclusion:** The study proposes initial design guidelines for AI coding assistants that emphasize youth agency and critical interaction while providing supportive scaffolding to enhance creative coding.

**Abstract:** Creative coding platforms like Scratch have democratized programming for children, yet translating imaginative ideas into functional code remains a significant hurdle for many young learners. While AI copilots assist adult programmers, few tools target children in block-based environments. Building on prior research \cite{druga_how_2021,druga2023ai, druga2023scratch}, we present Cognimates Scratch Copilot: an AI-powered assistant integrated into a Scratch-like environment, providing real-time support for ideation, code generation, debugging, and asset creation. This paper details the system architecture and findings from an exploratory qualitative evaluation with 18 international children (ages 7--12). Our analysis reveals how the AI Copilot supported key creative coding processes, particularly aiding ideation and debugging. Crucially, it also highlights how children actively negotiated the use of AI, demonstrating strong agency by adapting or rejecting suggestions to maintain creative control. Interactions surfaced design tensions between providing helpful scaffolding and fostering independent problem-solving, as well as learning opportunities arising from navigating AI limitations and errors. Findings indicate Cognimates Scratch Copilot's potential to enhance creative self-efficacy and engagement. Based on these insights, we propose initial design guidelines for AI coding assistants that prioritize youth agency and critical interaction alongside supportive scaffolding.

</details>


### [3] [State-of-the-Art HCI for Dementia Care: A Scoping Review of Recent Technological Advances](https://arxiv.org/abs/2505.04184)

*Yong Ma, Yuchong Zhang, Oda Elise Nordberg, Arvid Rongve, Miroslav Bachinski, Morten Fjeld*

**Main category:** cs.HC

**Keywords:** Dementia, Human-Computer Interaction, Assistive Technology, Caregiver Support, User-Centered Design

**Relevance Score:** 8

**TL;DR:** This scoping review examines 32 recent technological interventions aimed at supporting individuals with dementia and their caregivers, categorizing them into four key domains.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant challenges faced by individuals with dementia and their caregivers through technological advancements in Human-Computer Interaction.

**Method:** Systematic review of 32 publications from leading digital libraries focusing on technological interventions for dementia care.

**Key Contributions:**

	1. Categorization of interventions into four domains: daily life support, social engagement, well-being support, and caregiver training.
	2. Highlighting critical gaps in support for early-stage dementia.
	3. Providing a structured roadmap for future HCI-driven research in dementia care.

**Result:** Technological advancements enhance quality of life for PwD by increasing independence, social engagement, and emotional support; however, critical gaps remain, especially for early-stage dementia support.

**Limitations:** Identified gaps in individualized support mechanisms and needs of early-stage dementia individuals.

**Conclusion:** The review provides insights into current advancements and proposes a roadmap for future research emphasizing user-centered design, accessibility, and ethical considerations in dementia care.

**Abstract:** Dementia significantly impacts cognitive, behavioral, and functional abilities, creating challenges for both individuals and caregivers. Recent advancements in HCI have introduced innovative technological solutions to support people with dementia (PwD) and their caregivers. This scoping review systematically examines 32 recent publications from leading digital libraries, categorizing technological interventions into four key domains: Assistive and Smart Technology for Daily Life, Social Interaction and Communication, Well-being and Psychological Support, and Caregiver Support and Training. Our analysis highlights how emerging technologies are transforming dementia care. These technologies enhance quality of life by promoting independence, fostering social engagement, and providing emotional and cognitive support. However, the review also identifies critical gaps, particularly in addressing the needs of individuals with early-stage dementia and the lack of individualized support mechanisms. By emphasizing user-centered design, accessibility, and ethical considerations, this paper offers a structured roadmap for future research and practice in dementia care. It bridges the gap between technological innovation and the real-world needs of PwD and their caregivers, providing valuable insights for researchers, practitioners, and policymakers. This review not only synthesizes current advancements but also sets the stage for future HCI-driven innovations in dementia care, aiming to improve outcomes for an aging global population.

</details>


### [4] [Sick of being driven? -- Prevalence and modulating factors of carsickness in the European population in context of automated driving](https://arxiv.org/abs/2505.04210)

*Myriam Metzulat, Barbara Metz, Aaron Edelmann, Alexandra Neukum, Wilfried Kunde*

**Main category:** cs.HC

**Keywords:** carsickness, automated driving, passenger comfort, survey study, HCI

**Relevance Score:** 4

**TL;DR:** The study investigates the prevalence of carsickness in passengers of automated vehicles and factors influencing it through an online survey of 3999 participants from four countries.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding carsickness is vital for designing automated vehicles that maximize passenger comfort.

**Method:** An online survey was conducted with 3999 participants from Spain, Sweden, Poland, and Germany to assess the prevalence and factors influencing carsickness.

**Key Contributions:**

	1. Identification of demographic and situational factors influencing carsickness
	2. Evaluation of countermeasures for mitigating carsickness in automated driving
	3. Exploration of carsickness impact on the acceptability of automated vehicles

**Result:** 30% of participants reported experiencing carsickness as adults, influenced by demographic factors, non-driving tasks, road type, and seating position.

**Limitations:** 

**Conclusion:** Mitigating carsickness is essential for improving the acceptance and experience of automated driving.

**Abstract:** As in automated driving the driver becomes a passenger, carsickness might reduce comfort for susceptible individuals. Insights in the prevalence of carsickness and its modulating factors are considered useful for the development of automated vehicles to mitigate or prevent its occurrence. An online survey was conducted with N = 3999 participants in Spain, Sweden, Poland, and Germany. 30% of participants reported to have already experienced carsickness as adult. The frequency of carsickness was modulated not only by demographic factors (country, gender, age), but also by frequency of being a passenger, type of non-driving related task, road type, and the seating position in car. Furthermore, the efficiency of applied countermeasures, temporal aspects of carsickness development, as well as the relation of carsickness with the acceptability of automated driving and the effect on subjective fitness to drive was investigated. The results are discussed with focus on automated driving.

</details>


### [5] [Steerable Chatbots: Personalizing LLMs with Preference-Based Activation Steering](https://arxiv.org/abs/2505.04260)

*Jessica Y. Bo, Tianyu Xu, Ishan Chatterjee, Katrina Passarella-Ward, Achin Kulshrestha, D Shin*

**Main category:** cs.HC

**Keywords:** large language models, activation steering, personalization, user preferences, chatbot interfaces

**Relevance Score:** 9

**TL;DR:** The paper presents a method for personalizing large language model responses using activation steering, making it easier for users to express their preferences in AI conversations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving user satisfaction and retention in AI assistants by better aligning responses with user preferences.

**Method:** Activation steering is used to guide LLMs towards interpretable preference dimensions during inference, allowing for lightweight personalization without needing extensive user history.

**Key Contributions:**

	1. Introduction of activation steering for personalized responses in LLMs
	2. User study revealing preferences for different chatbot interfaces
	3. Insights into user values around control, usability, and transparency

**Result:** The study shows that preference-based steering effectively aligns conversations with hidden user preferences and reveals insights into user preferences for interface control and usability.

**Limitations:** The study is limited by a small sample size (n=14) and focuses on specific interactive chatbot interfaces.

**Conclusion:** Preference-based steering enhances the personalization of AI assistants, providing insights that can inform the design of interactive interfaces.

**Abstract:** As large language models (LLMs) improve in their capacity to serve as personal AI assistants, their ability to output uniquely tailored, personalized responses that align with the soft preferences of their users is essential for enhancing user satisfaction and retention. However, untrained lay users have poor prompt specification abilities and often struggle with conveying their latent preferences to AI assistants. To address this, we leverage activation steering to guide LLMs to align with interpretable preference dimensions during inference. In contrast to memory-based personalization methods that require longer user history, steering is extremely lightweight and can be easily controlled by the user via an linear strength factor. We embed steering into three different interactive chatbot interfaces and conduct a within-subjects user study (n=14) to investigate how end users prefer to personalize their conversations. The results demonstrate the effectiveness of preference-based steering for aligning real-world conversations with hidden user preferences, and highlight further insights on how diverse values around control, usability, and transparency lead users to prefer different interfaces.

</details>


### [6] [With Friends Like These, Who Needs Explanations? Evaluating User Understanding of Group Recommendations](https://arxiv.org/abs/2505.04273)

*Cedric Waterschoot, Raciel Yera Toledo, Nava Tintarev, Francesco Barile*

**Main category:** cs.HC

**Keywords:** Group Recommender Systems, explanations, user understanding, aggregation strategies, social choice

**Relevance Score:** 6

**TL;DR:** This paper investigates the impact of different types of explanations on user understanding of Group Recommender Systems (GRS) across various aggregation strategies and modality of explanations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to uncover the relationship between explanations in GRS and user understanding, particularly in minority preference scenarios, an area that has been relatively unexplored despite its relevance to transparent GRS.

**Method:** A randomized controlled trial was conducted with 271 participants, assessing two factors: aggregation strategy (additive, least misery, and approval voting) and explanation modality (no explanation, textual explanation, or multimodal explanation). Understanding was measured through subjective self-perception and objective performance on tasks.

**Key Contributions:**

	1. Investigating the impact of explanations on GRS user understanding.
	2. Highlighting the importance of aggregation strategies in understanding.
	3. Providing insights on measuring understanding beyond single aggregated metrics.

**Result:** The study found that detailed explanations did not improve user understanding either subjectively or objectively. However, the aggregation strategy used significantly affected both aspects of user understanding.

**Limitations:** The study's findings may not generalize to all contexts of GRS and rely on self-reported metrics which can introduce bias.

**Conclusion:** When designing GRS, it is crucial to consider the aggregation strategy as it influences users' understanding. A focus on multiple performance tasks is recommended for a more comprehensive assessment of understanding.

**Abstract:** Group Recommender Systems (GRS) employing social choice-based aggregation strategies have previously been explored in terms of perceived consensus, fairness, and satisfaction. At the same time, the impact of textual explanations has been examined, but the results suggest a low effectiveness of these explanations. However, user understanding remains fairly unexplored, even if it can contribute positively to transparent GRS. This is particularly interesting to study in more complex or potentially unfair scenarios when user preferences diverge, such as in a minority scenario (where group members have similar preferences, except for a single member in a minority position). In this paper, we analyzed the impact of different types of explanations on user understanding of group recommendations. We present a randomized controlled trial (n = 271) using two between-subject factors: (i) the aggregation strategy (additive, least misery, and approval voting), and (ii) the modality of explanation (no explanation, textual explanation, or multimodal explanation). We measured both subjective (self-perceived by the user) and objective understanding (performance on model simulation, counterfactuals and error detection). In line with recent findings on explanations for machine learning models, our results indicate that more detailed explanations, whether textual or multimodal, did not increase subjective or objective understanding. However, we did find a significant effect of aggregation strategies on both subjective and objective understanding. These results imply that when constructing GRS, practitioners need to consider that the choice of aggregation strategy can influence the understanding of users. Post-hoc analysis also suggests that there is value in analyzing performance on different tasks, rather than through a single aggregated metric of understanding.

</details>


### [7] [Improving Inclusivity for Emotion Recognition Based on Face Tracking](https://arxiv.org/abs/2505.04433)

*Mats Ole Ellenberg, Katja Krug*

**Main category:** cs.HC

**Keywords:** emotion recognition, face tracking, Mixed Reality, Virtual Reality, affective computing

**Relevance Score:** 7

**TL;DR:** This paper addresses the challenges of emotional expression in Mixed Reality and Virtual Reality due to limited virtual user representations and proposes strategies to enhance face tracking systems for emotion recognition.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper highlights the limitation of current virtual user representations in conveying emotional expressions in Mixed and Virtual Reality, which is essential for effective communication.

**Method:** The authors propose several strategies for improving face tracking systems to enhance emotion recognition both with and without user intervention.

**Key Contributions:**

	1. Strategies for enhancing emotion recognition in virtual environments
	2. New approaches for individual variation in emotional expressions
	3. Consideration of user intervention in emotion recognition systems

**Result:** Strategies are expected to improve the accuracy and expressiveness of emotion recognition in virtual environments.

**Limitations:** 

**Conclusion:** Improving face tracking for emotion recognition can lead to more authentic and emotionally expressive interactions in Mixed Reality and Virtual Reality.

**Abstract:** The limited expressiveness of virtual user representations in Mixed Reality and Virtual Reality can inhibit an integral part of communication: emotional expression. Emotion recognition based on face tracking is often used to compensate for this. However, emotional facial expressions are highly individual, which is why many approaches have difficulties recognizing unique variations of emotional expressions. We propose several strategies to improve face tracking systems for emotion recognition with and without user intervention for the Affective Interaction Workshop at CHI '25.

</details>


### [8] [Practice Support for Violin Bowing by Measuring Bow Pressure and Position](https://arxiv.org/abs/2505.04446)

*Yurina Mizuho, Yuta Sugiura*

**Main category:** cs.HC

**Keywords:** violin, bowing technique, visual feedback, musical performance, beginner training

**Relevance Score:** 2

**TL;DR:** This study investigates the role of bow pressure in violin playing, comparing experienced and beginner players, and develops a visual feedback system to enhance practice.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and improve the bowing technique in violin playing, focusing on the parameters that affect tone production.

**Method:** The study involved comparing bow pressure, position, and speed between experienced and beginner violinists, followed by the development of a visual feedback system to assist beginners in practice.

**Key Contributions:**

	1. Identification of differences in bowing techniques between experienced and beginner players
	2. Development of a visual feedback system to aid beginner violinists
	3. Evaluation of teaching methods incorporating feedback

**Result:** The findings demonstrated that experienced players exhibited distinct bowing characteristics and that providing explicit feedback to beginners improved their bowing technique.

**Limitations:** The study is limited to a small sample size and focuses only on bow pressure without considering other factors in string instrument performance.

**Conclusion:** The clarification of bowing characteristics for beginners, along with the introduction of a feedback system, significantly supports their learning process.

**Abstract:** The violin is one of the most popular musical instruments. Various parameters of bowing motion, such as pressure, position, and speed, are crucial for producing a beautiful tone. However, mastering them is challenging and requires extensive practice. In this study, we aimed to support practice of bowing, focusing on bow pressure. First, we compared the bowing movements, specifically bow pressure, bow position, and bow speed, of eight experienced players with those of eight beginners. Next, we developed and evaluated a visual feedback system that displays bow pressure to support practice. We taught the identified differences to 14 beginners, dividing them into two groups: one practiced with an explanation, and the other with both an explanation and a feedback system. These two experiments found that clarifying the characteristics unique to experienced players can support practice.

</details>


### [9] [A Design Space for the Critical Validation of LLM-Generated Tabular Data](https://arxiv.org/abs/2505.04487)

*Madhav Sachdeva, Christopher Narayanan, Marvin Wiedenkeller, Jana Sedlakova, Jürgen Bernard*

**Main category:** cs.HC

**Keywords:** LLM-generated data, tabular data validation, validation framework

**Relevance Score:** 8

**TL;DR:** The paper presents a structured design space for validating LLM-generated tabular data, focusing on various analysis granularity and data sources.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing need for critical validation of LLM-generated tabular data, leveraging its potential applications while ensuring reliability.

**Method:** The design space is structured along two dimensions: Analysis Granularity and Data Source, coupled with a mapping of existing validation approaches and detailed discussion of select methods.

**Key Contributions:**

	1. Introduction of a two-dimensional design space for validation
	2. Mapping of existing validation methods to the design framework
	3. In-depth analysis of select validation approaches.

**Result:** The authors map 19 validation approaches and detail characteristics of two approaches, showing the descriptive power of the validation tasks.

**Limitations:** 

**Conclusion:** A structured framework for validating LLM-generated tabular data is necessary to ensure the reliability of data-driven applications in academia and industry.

**Abstract:** LLM-generated tabular data is creating new opportunities for data-driven applications in academia, business, and society. To leverage benefits like missing value imputation, labeling, and enrichment with context-aware attributes, LLM-generated data needs a critical validation process. The number of pioneering approaches is increasing fast, opening a promising validation space that, so far, remains unstructured. We present a design space for the critical validation of LLM-generated tabular data with two dimensions: First, the Analysis Granularity dimension: from within-attribute (single-item and multi-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second, the Data Source dimension: differentiating between LLM-generated values, ground truth values, explanations, and their combinations. We discuss analysis tasks for each dimension cross-cut, map 19 existing validation approaches, and discuss the characteristics of two approaches in detail, demonstrating descriptive power.

</details>


### [10] [SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for Open-Ended Questions](https://arxiv.org/abs/2505.04584)

*Chloe Qianhui Zhao, Jie Cao, Eason Chen, Kenneth R. Koedinger, Jionghao Lin*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence, feedback generation, multimodal learning, Large Language Models, educational technology

**Relevance Score:** 8

**TL;DR:** This study explores AI-generated multimodal feedback in education, combining text from LLMs with relevant lecture slides to assess its impact on student learning and perceptions.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the benefits of multimodal feedback in education as a means to enhance student learning and experience compared to traditional text-only feedback.

**Method:** An online crowdsourcing study with N=91 participants employed a 2x2 design comparing human vs AI feedback and with vs without relevant slides, assessing learning gains and student perceptions.

**Key Contributions:**

	1. Combines textual feedback from LLMs with relevant lecture materials for enhanced learning support.
	2. Evaluates the effectiveness of AI feedback in comparison to human feedback across modalities.
	3. Provides insights into student perceptions and trust towards AI-generated content.

**Result:** The study found significant pre-to-post learning gains in all conditions but no statistically significant differences in gains between the feedback types. Students found slide feedback helpful yet confusing, while AI feedback was seen as personalized but less trusted than human feedback.

**Limitations:** The differences in learning gains between conditions were not statistically significant, and students experienced difficulty in understanding slide feedback.

**Conclusion:** AI facilitates multimodal feedback that may enhance learning, but issues with clarity and trustworthiness need addressing to improve its adoption in educational settings.

**Abstract:** Feedback is important in supporting student learning. While various automated feedback systems have been implemented to make the feedback scalable, many existing solutions only focus on generating text-based feedback. As is indicated in the multimedia learning principle, learning with more modalities could help utilize more separate channels, reduce the cognitive load and facilitate students' learning. Hence, it is important to explore the potential of Artificial Intelligence (AI) in feedback generation from and to different modalities. Our study leverages Large Language Models (LLMs) for textual feedback with the supplementary guidance from other modality - relevant lecture slide retrieved from the slides hub. Through an online crowdsourcing study (N=91), this study investigates learning gains and student perceptions using a 2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant slide), evaluating the clarity, engagement, perceived effectiveness, and reliability) of AI-facilitated multimodal feedback. We observed significant pre-to-post learning gains across all conditions. However, the differences in these gains were not statistically significant between conditions. The post-survey revealed that students found the slide feedback helpful in their learning process, though they reported difficulty in understanding it. Regarding the AI-generated open-ended feedback, students considered it personalized and relevant to their responses, but they expressed lower trust in the AI feedback compared to human-generated feedback.

</details>


### [11] [OV-MER: Towards Open-Vocabulary Multimodal Emotion Recognition](https://arxiv.org/abs/2410.01495)

*Zheng Lian, Haiyang Sun, Licai Sun, Haoyu Chen, Lan Chen, Hao Gu, Zhuofan Wen, Shun Chen, Siyuan Zhang, Hailiang Yao, Bin Liu, Rui Liu, Shan Liang, Ya Li, Jiangyan Yi, Jianhua Tao*

**Main category:** cs.HC

**Keywords:** emotion recognition, open vocabulary, machine learning, emotional states, multimodal

**Relevance Score:** 8

**TL;DR:** This paper introduces Open-Vocabulary Emotion Recognition (OV-MER), allowing for emotion prediction beyond predefined labels, addressing the complexity of human emotions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current machine learning methods in Multimodal Emotion Recognition are limited by fixed emotion taxonomies, which do not capture the complexity of human emotional experiences.

**Method:** The paper proposes a novel paradigm called Open-Vocabulary MER (OV-MER), facilitating emotion prediction without confining to predefined categories and includes the creation of a comprehensive dataset and novel evaluation metrics.

**Key Contributions:**

	1. Introduction of Open-Vocabulary MER (OV-MER)
	2. Development of a new comprehensive dataset for emotion recognition
	3. Novel evaluation metrics for assessing emotion prediction

**Result:** The authors provide a newly curated database and preliminary benchmark for OV-MER, demonstrating improved generalizability and applicability in recognizing nuanced emotional states.

**Limitations:** 

**Conclusion:** By moving beyond basic emotions to a broader spectrum, this work aims to inspire advancements in MER and better applicability in various real-world contexts.

**Abstract:** Multimodal Emotion Recognition (MER) is a critical research area that seeks to decode human emotions from diverse data modalities. However, existing machine learning methods predominantly rely on predefined emotion taxonomies, which fail to capture the inherent complexity, subtlety, and multi-appraisal nature of human emotional experiences, as demonstrated by studies in psychology and cognitive science. To overcome this limitation, we advocate for introducing the concept of open vocabulary into MER. This paradigm shift aims to enable models to predict emotions beyond a fixed label space, accommodating a flexible set of categories to better reflect the nuanced spectrum of human emotions. To achieve this, we propose a novel paradigm: Open-Vocabulary MER (OV-MER), which enables emotion prediction without being confined to predefined spaces. However, constructing a dataset that encompasses the full range of emotions for OV-MER is practically infeasible; hence, we present a comprehensive solution including a newly curated database, novel evaluation metrics, and a preliminary benchmark. By advancing MER from basic emotions to more nuanced and diverse emotional states, we hope this work can inspire the next generation of MER, enhancing its generalizability and applicability in real-world scenarios. Code and dataset are available at: https://github.com/zeroQiaoba/AffectGPT.

</details>


### [12] [Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT](https://arxiv.org/abs/2411.10246)

*Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi, Lei Liu, Michael Flor*

**Main category:** cs.HC

**Keywords:** Collaborative problem solving, ChatGPT, communication data, CPS assessment, artificial intelligence

**Relevance Score:** 7

**TL;DR:** The paper explores the effectiveness of ChatGPT in coding communication data for assessing Collaborative Problem Solving (CPS), highlighting variations in performance by model and framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of scaling assessments of Collaborative Problem Solving (CPS) skills by improving the coding process of communication data.

**Method:** The study analyzes five datasets using two coding frameworks to evaluate how ChatGPT models perform in coding communication data relevant to CPS assessments.

**Key Contributions:**

	1. Demonstrated ChatGPT's ability to code communication data for CPS assessments.
	2. Identified performance variations across different ChatGPT models and frameworks.
	3. Provided practical guidance on prompt refinement for improved coding accuracy.

**Result:** ChatGPT can code communication data effectively, but its performance varies by model and task characteristics; performance does not necessarily improve with newer models, and prompt refinement can enhance accuracy in some cases.

**Limitations:** Performance is inconsistent across different tasks, and not all refinement techniques improve results.

**Conclusion:** The findings provide insights for researchers and practitioners on developing efficient coding methods for analyzing communication data, supporting CPS skill assessments.

**Abstract:** Collaborative problem solving (CPS) is widely recognized as a critical 21st-century skill. Assessing CPS depends heavily on coding the communication data using a construct-relevant framework, and this process has long been a major bottleneck to scaling up such assessments. Based on five datasets and two coding frameworks, we demonstrate that ChatGPT can code communication data to a satisfactory level, though performance varies across ChatGPT models, and depends on the coding framework and task characteristics. Interestingly, newer reasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily yield better coding results. Additionally, we show that refining prompts based on feedback from miscoded cases can improve coding accuracy in some instances, though the effectiveness of this approach is not consistent across all tasks. These findings offer practical guidance for researchers and practitioners in developing scalable, efficient methods to analyze communication data in support of 21st-century skill assessment.

</details>


### [13] [Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting Content Creators From AI Crawlers](https://arxiv.org/abs/2411.15091)

*Enze Liu, Elisa Luo, Shawn Shan, Geoffrey M. Voelker, Ben Y. Zhao, Stefan Savage*

**Main category:** cs.HC

**Keywords:** AI crawling, content protection, robots.txt, professional artists, network security

**Relevance Score:** 7

**TL;DR:** This paper investigates the effectiveness of current tools for protecting content against AI web crawlers, focusing on technical awareness and agency among human artists.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the copyright, privacy, and ethical concerns surrounding the use of crawlers by generative AI, particularly affecting content creators like artists.

**Method:** The study utilizes large-scale measurements and a user study involving 203 professional artists to evaluate the demand and effectiveness of crawler-blocking tools like robots.txt and reverse proxies.

**Key Contributions:**

	1. Assessment of tools used to protect against AI web crawling.
	2. User study revealing the technical challenges faced by artists.
	3. Evaluation of the efficacy of network-level crawler blockers.

**Result:** The findings indicate a strong demand for crawler-blocking tools among artists, but identify significant barriers in technical knowledge, deployment agency, and limited effectiveness against persistent crawlers.

**Limitations:** Challenges include low technical awareness among artists and limited efficacy against sophisticated crawlers.

**Conclusion:** While reverse proxies can provide enhanced protection against AI crawlers, they are not widely implemented and have their own limitations.

**Abstract:** The success of generative AI relies heavily on training on data scraped through extensive crawling of the Internet, a practice that has raised significant copyright, privacy, and ethical concerns. While few measures are designed to resist a resource-rich adversary determined to scrape a site, crawlers can be impacted by a range of existing tools such as robots.txt, NoAI meta tags, and active crawler blocking by reverse proxies.   In this work, we seek to understand the ability and efficacy of today's networking tools to protect content creators against AI-related crawling. For targeted populations like human artists, do they have the technical knowledge and agency to utilize crawler-blocking tools such as robots.txt, and can such tools be effective? Using large scale measurements and a targeted user study of 203 professional artists, we find strong demand for tools like robots.txt, but significantly constrained by critical hurdles in technical awareness, agency in deploying them, and limited efficacy against unresponsive crawlers. We further test and evaluate network-level crawler blockers provided by reverse proxies. Despite relatively limited deployment today, they offer stronger protections against AI crawlers, but still come with their own set of limitations.

</details>


### [14] [Adaptive Gen-AI Guidance in Virtual Reality: A Multimodal Exploration of Engagement in Neapolitan Pizza-Making](https://arxiv.org/abs/2411.18438)

*Ka Hei Carrie Lau, Sema Sen, Philipp Stark, Efe Bozkir, Enkelejda Kasneci*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Generative AI, User Engagement, Adaptive Learning, Educational Technology

**Relevance Score:** 7

**TL;DR:** The study explores using virtual reality (VR) and generative AI to enhance procedural learning and user engagement through adaptive learning pathways.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate user engagement in adaptive VR environments when traditional metrics fail due to variability in Gen-AI response times.

**Method:** A controlled experiment with 54 participants assessed three levels of adaptivity (high, moderate, non-adaptive baseline) during a Neapolitan pizza-making VR experience, utilizing multimodal behavioral metrics.

**Key Contributions:**

	1. Integration of Gen-AI in VR for procedural learning
	2. Use of multimodal behavioral metrics to evaluate user engagement
	3. Recommendations for adaptive educational technology design

**Result:** Moderate adaptivity was found to significantly enhance user engagement by reducing unnecessary exploratory behavior and increasing focused visual attention on the AI avatar.

**Limitations:** 

**Conclusion:** A balanced level of adaptive AI provides effective user support and guides practical design recommendations for future educational technologies.

**Abstract:** Virtual reality (VR) offers promising opportunities for procedural learning, particularly in preserving intangible cultural heritage. Advances in generative artificial intelligence (Gen-AI) further enrich these experiences by enabling adaptive learning pathways. However, evaluating such adaptive systems using traditional temporal metrics remains challenging due to the inherent variability in Gen-AI response times. To address this, our study employs multimodal behavioural metrics, including visual attention, physical exploratory behaviour, and verbal interaction, to assess user engagement in an adaptive VR environment. In a controlled experiment with 54 participants, we compared three levels of adaptivity (high, moderate, and non-adaptive baseline) within a Neapolitan pizza-making VR experience. Results show that moderate adaptivity optimally enhances user engagement, significantly reducing unnecessary exploratory behaviour and increasing focused visual attention on the AI avatar. Our findings suggest that a balanced level of adaptive AI provides the most effective user support, offering practical design recommendations for future adaptive educational technologies.

</details>


### [15] [AffectGPT: A New Dataset, Model, and Benchmark for Emotion Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2501.16566)

*Zheng Lian, Haoyu Chen, Lan Chen, Haiyang Sun, Licai Sun, Yong Ren, Zebang Cheng, Bin Liu, Rui Liu, Xiaojiang Peng, Jiangyan Yi, Jianhua Tao*

**Main category:** cs.HC

**Keywords:** multimodal emotion recognition, large language models, emotion datasets

**Relevance Score:** 9

**TL;DR:** This paper presents a benchmark for multimodal emotion recognition (MER) using a novel dataset and model, addressing the need for extensive emotion annotations and an integrated framework for MLLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitation of the current community regarding the availability of large-scale datasets with detailed emotion annotations for improving multimodal emotion recognition tasks.

**Method:** The authors introduce a new dataset, MER-Caption, and a model called AffectGPT, which incorporates pre-fusion operations to enhance multimodal integration, along with a unified benchmarking framework, MER-UniBench, for evaluating MER tasks.

**Key Contributions:**

	1. Introduction of the MER-Caption dataset for emotion recognition
	2. Development of the AffectGPT model for multimodal integration
	3. Creation of the MER-UniBench framework for evaluating MER tasks

**Result:** AffectGPT demonstrates robust performance across a variety of MER tasks, supported by a dataset consisting of over 115K samples and 2K fine-grained emotion categories.

**Limitations:** 

**Conclusion:** The study significantly advances the state-of-the-art in emotion understanding through its contributions of a novel dataset, model, and benchmarking framework, with resources available for further research.

**Abstract:** The emergence of multimodal large language models (MLLMs) advances multimodal emotion recognition (MER) to the next level, from naive discriminative tasks to complex emotion understanding with advanced video understanding abilities and natural language description. However, the current community suffers from a lack of large-scale datasets with intensive, descriptive emotion annotations, as well as a multimodal-centric framework to maximize the potential of MLLMs for emotion understanding. To address this, we establish a new benchmark for MLLM-based emotion understanding with a novel dataset (MER-Caption) and a new model (AffectGPT). Utilizing our model-based crowd-sourcing data collection strategy, we construct the largest descriptive emotion dataset to date (by far), featuring over 2K fine-grained emotion categories across 115K samples. We also introduce the AffectGPT model, designed with pre-fusion operations to enhance multimodal integration. Finally, we present MER-UniBench, a unified benchmark with evaluation metrics tailored for typical MER tasks and the free-form, natural language output style of MLLMs. Extensive experimental results show AffectGPT's robust performance across various MER tasks. We have released both the code and the dataset to advance research and development in emotion understanding: https://github.com/zeroQiaoba/AffectGPT.

</details>


### [16] [Automated Coding of Communications in Collaborative Problem-solving Tasks Using ChatGPT](https://arxiv.org/abs/2411.10246)

*Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi, Lei Liu, Michael Flor*

**Main category:** cs.HC

**Keywords:** Collaborative problem solving, ChatGPT, Communication data coding, AI in assessment, Skill assessment

**Relevance Score:** 6

**TL;DR:** This paper explores the use of ChatGPT for coding communication data in collaborative problem solving assessment, demonstrating that its performance varies by model and coding framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for scalable assessments of collaborative problem solving (CPS) skills has led to exploring AI's role in coding communication data effectively.

**Method:** The study utilizes five datasets and two coding frameworks to evaluate the performance of various ChatGPT models in coding communication data for CPS assessment.

**Key Contributions:**

	1. Demonstrated that ChatGPT can effectively code communication data for CPS assessment.
	2. Identified variations in performance across different ChatGPT models and coding frameworks.
	3. Showed that prompt refinement can improve coding accuracy in some cases.

**Result:** ChatGPT can code communication data at a satisfactory level, but its performance varies based on the specific model and task characteristics.

**Limitations:** Performance of coding varies by model, coding framework, and task; some approaches for improvement are inconsistent.

**Conclusion:** Refining prompts based on feedback can enhance coding accuracy, although effectiveness varies across tasks, providing a pathway for improving communication data analysis.

**Abstract:** Collaborative problem solving (CPS) is widely recognized as a critical 21st-century skill. Assessing CPS depends heavily on coding the communication data using a construct-relevant framework, and this process has long been a major bottleneck to scaling up such assessments. Based on five datasets and two coding frameworks, we demonstrate that ChatGPT can code communication data to a satisfactory level, though performance varies across ChatGPT models, and depends on the coding framework and task characteristics. Interestingly, newer reasoning-focused models such as GPT-o1-mini and GPT-o3-mini do not necessarily yield better coding results. Additionally, we show that refining prompts based on feedback from miscoded cases can improve coding accuracy in some instances, though the effectiveness of this approach is not consistent across all tasks. These findings offer practical guidance for researchers and practitioners in developing scalable, efficient methods to analyze communication data in support of 21st-century skill assessment.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [Calibrating Uncertainty Quantification of Multi-Modal LLMs using Grounding](https://arxiv.org/abs/2505.03788)

*Trilok Padhi, Ramneet Kaur, Adam D. Cobb, Manoj Acharya, Anirban Roy, Colin Samplawski, Brian Matejek, Alexander M. Berenbeim, Nathaniel D. Bastian, Susmit Jha*

**Main category:** cs.CL

**Keywords:** uncertainty quantification, multi-modal, large language models, cross-modal consistency, calibration

**Relevance Score:** 9

**TL;DR:** A novel UQ calibration method for multi-modal LLMs improves accuracy by considering cross-modal consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current UQ methods underreport confidence in erroneous outputs of multi-modal LLMs, necessitating a better calibration strategy.

**Method:** The approach combines self-consistency and cross-modal consistency to calibrate confidence in generated responses using a grounding model and temperature scaling.

**Key Contributions:**

	1. Novel calibration method incorporating cross-modal consistency
	2. Utilization of grounding model confidence
	3. Application of temperature scaling to improve UQ

**Result:** Significant improvements in calibration on tasks such as medical question answering and visual question answering with models like LLaVA-Med and LLaVA.

**Limitations:** 

**Conclusion:** The proposed framework enhances the reliability of confidence estimates in multi-modal settings, leading to better output accuracy.

**Abstract:** We introduce a novel approach for calibrating uncertainty quantification (UQ) tailored for multi-modal large language models (LLMs). Existing state-of-the-art UQ methods rely on consistency among multiple responses generated by the LLM on an input query under diverse settings. However, these approaches often report higher confidence in scenarios where the LLM is consistently incorrect. This leads to a poorly calibrated confidence with respect to accuracy. To address this, we leverage cross-modal consistency in addition to self-consistency to improve the calibration of the multi-modal models. Specifically, we ground the textual responses to the visual inputs. The confidence from the grounding model is used to calibrate the overall confidence. Given that using a grounding model adds its own uncertainty in the pipeline, we apply temperature scaling - a widely accepted parametric calibration technique - to calibrate the grounding model's confidence in the accuracy of generated responses. We evaluate the proposed approach across multiple multi-modal tasks, such as medical question answering (Slake) and visual question answering (VQAv2), considering multi-modal models such as LLaVA-Med and LLaVA. The experiments demonstrate that the proposed framework achieves significantly improved calibration on both tasks.

</details>


### [18] [Hesitation is defeat? Connecting Linguistic and Predictive Uncertainty](https://arxiv.org/abs/2505.03910)

*Gianluca Manzo, Julia Ive*

**Main category:** cs.CL

**Keywords:** Deep Learning, Uncertainty Quantification, Bayesian Approximations, Chest Radiographs, Clinical Decision Making

**Relevance Score:** 9

**TL;DR:** This paper explores the role of uncertainty in chest radiograph interpretation using Bayesian Deep Learning, linking predictive uncertainty with human uncertainty from free-text radiology reports.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance clinical workflows and decision-making in automated chest radiograph interpretation, it is crucial to not only optimize predictive performance but also quantify uncertainty.

**Method:** Utilizes BERT to evaluate binarisation methods for uncertainty labels and assesses Monte Carlo Dropout and Deep Ensembles for estimating predictive uncertainty.

**Key Contributions:**

	1. Investigates relationship between predictive and human uncertainty in medical diagnostics.
	2. Evaluates binarisation methods for uncertainty labels and their impact on model performance.
	3. Highlights the necessity for improved methods to align machine and human uncertainty.

**Result:** The study shows good model performance but a modest correlation between predictive and linguistic uncertainty, indicating challenges in aligning machine estimates with human interpretation nuances.

**Limitations:** The correlation between predictive and linguistic uncertainty was modest, suggesting limitations in current approaches.

**Conclusion:** Bayesian approximations can provide valuable uncertainty estimates, but further refinement is needed to capture the subtleties of human uncertainty in clinical contexts.

**Abstract:** Automating chest radiograph interpretation using Deep Learning (DL) models has the potential to significantly improve clinical workflows, decision-making, and large-scale health screening. However, in medical settings, merely optimising predictive performance is insufficient, as the quantification of uncertainty is equally crucial. This paper investigates the relationship between predictive uncertainty, derived from Bayesian Deep Learning approximations, and human/linguistic uncertainty, as estimated from free-text radiology reports labelled by rule-based labellers. Utilising BERT as the model of choice, this study evaluates different binarisation methods for uncertainty labels and explores the efficacy of Monte Carlo Dropout and Deep Ensembles in estimating predictive uncertainty. The results demonstrate good model performance, but also a modest correlation between predictive and linguistic uncertainty, highlighting the challenges in aligning machine uncertainty with human interpretation nuances. Our findings suggest that while Bayesian approximations provide valuable uncertainty estimates, further refinement is necessary to fully capture and utilise the subtleties of human uncertainty in clinical applications.

</details>


### [19] [A Reasoning-Focused Legal Retrieval Benchmark](https://arxiv.org/abs/2505.03970)

*Lucia Zheng, Neel Guha, Javokhir Arifov, Sarah Zhang, Michal Skreta, Christopher D. Manning, Peter Henderson, Daniel E. Ho*

**Main category:** cs.CL

**Keywords:** legal AI, large language models, retrieval-augmented generation, legal benchmarks, legal question answering

**Relevance Score:** 4

**TL;DR:** This paper introduces two benchmarks for legal retrieval-augmented large language models (RAG): Bar Exam QA and Housing Statute QA, aimed at evaluating legal question-answering performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The legal community's exploration of large language models (LLMs) for legal applications lacks realistic benchmarks for legal retrieval and question-answering, necessitating the creation of relevant testing frameworks.

**Method:** The authors developed Bar Exam QA and Housing Statute QA benchmarks that simulate real-world legal research tasks through a comprehensive annotation process.

**Key Contributions:**

	1. Introduction of Bar Exam QA and Housing Statute QA benchmarks for legal RAG systems
	2. Benchmark tasks reflect real-world legal research scenarios
	3. Highlighting the challenges faced in legal retrieval and question-answering.

**Result:** The existing retriever pipelines were evaluated on these benchmarks, highlighting the challenges in legal RAG applications and the need for ongoing research.

**Limitations:** 

**Conclusion:** The introduction of these benchmarks lays the groundwork for enhancing the performance and robustness of legal RAG systems in the future.

**Abstract:** As the legal community increasingly examines the use of large language models (LLMs) for various legal applications, legal AI developers have turned to retrieval-augmented LLMs ("RAG" systems) to improve system performance and robustness. An obstacle to the development of specialized RAG systems is the lack of realistic legal RAG benchmarks which capture the complexity of both legal retrieval and downstream legal question-answering. To address this, we introduce two novel legal RAG benchmarks: Bar Exam QA and Housing Statute QA. Our tasks correspond to real-world legal research tasks, and were produced through annotation processes which resemble legal research. We describe the construction of these benchmarks and the performance of existing retriever pipelines. Our results suggest that legal RAG remains a challenging application, thus motivating future research.

</details>


### [20] [Divide, Optimize, Merge: Fine-Grained LLM Agent Optimization at Scale](https://arxiv.org/abs/2505.03973)

*Jiale Liu, Yifan Zeng, Shaokun Zhang, Chi Zhang, Malte Højmark-Bertelsen, Marie Normann Gadeberg, Huazheng Wang, Qingyun Wu*

**Main category:** cs.CL

**Keywords:** LLM, optimization, fine-grained, agentic systems, scalability

**Relevance Score:** 8

**TL;DR:** Proposes Fine-Grained Optimization (FGO) framework for scalable LLM optimization by breaking down large tasks into manageable subsets, improving performance while reducing token consumption.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address context window overflow and degraded pattern recognition in LLM optimizers due to growing datasets.

**Method:** The proposed FGO framework divides large optimization tasks into subsets, performs targeted optimizations, and systematically merges optimized components.

**Key Contributions:**

	1. Introduces a novel Fine-Grained Optimization framework.
	2. Demonstrates significant performance improvements over existing methods.
	3. Reduces token consumption substantially.

**Result:** FGO outperforms existing methods by 1.6-8.6% and reduces average prompt token consumption by 56.3% across benchmarks like ALFWorld, LogisticsQA, and GAIA.

**Limitations:** 

**Conclusion:** FGO provides a practical and efficient solution for enhancing LLM-based optimization in complex agent systems.

**Abstract:** LLM-based optimization has shown remarkable potential in enhancing agentic systems. However, the conventional approach of prompting LLM optimizer with the whole training trajectories on training dataset in a single pass becomes untenable as datasets grow, leading to context window overflow and degraded pattern recognition. To address these challenges, we propose Fine-Grained Optimization (FGO), a scalable framework that divides large optimization tasks into manageable subsets, performs targeted optimizations, and systematically combines optimized components through progressive merging. Evaluation across ALFWorld, LogisticsQA, and GAIA benchmarks demonstrate that FGO outperforms existing approaches by 1.6-8.6% while reducing average prompt token consumption by 56.3%. Our framework provides a practical solution for scaling up LLM-based optimization of increasingly sophisticated agent systems. Further analysis demonstrates that FGO achieves the most consistent performance gain in all training dataset sizes, showcasing its scalability and efficiency.

</details>


### [21] [X-Reasoner: Towards Generalizable Reasoning Across Modalities and Domains](https://arxiv.org/abs/2505.03981)

*Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, Hoifung Poon*

**Main category:** cs.CL

**Keywords:** multimodal reasoning, generalizability, vision-language model

**Relevance Score:** 8

**TL;DR:** The paper introduces X-Reasoner, a vision-language model that extends reasoning capabilities beyond text and general domains, achieving strong performance on multimodal and medical benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the generalizability of reasoning across modalities and domains, particularly in light of existing work focusing mainly on text-only models.

**Method:** X-Reasoner is post-trained on general-domain text through a two-stage process: an initial supervised fine-tuning with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards.

**Key Contributions:**

	1. Introduction of X-Reasoner model for multimodal reasoning
	2. Demonstration of generalizable reasoning capabilities from general-domain text
	3. Development of X-Reasoner-Med for specialized medical benchmarks

**Result:** X-Reasoner outperforms state-of-the-art models on various general and medical benchmarks, showcasing successful transfer of reasoning capabilities.

**Limitations:** 

**Conclusion:** Continued training on domain-specific text can further enhance the performance of X-Reasoner, leading to the introduction of X-Reasoner-Med for specialized medical applications.

**Abstract:** Recent proprietary models (e.g., o3) have begun to demonstrate strong multimodal reasoning capabilities. Yet, most existing open-source research concentrates on training text-only reasoning models, with evaluations limited to mainly mathematical and general-domain tasks. Therefore, it remains unclear how to effectively extend reasoning capabilities beyond text input and general domains. This paper explores a fundamental research question: Is reasoning generalizable across modalities and domains? Our findings support an affirmative answer: General-domain text-based post-training can enable such strong generalizable reasoning. Leveraging this finding, we introduce X-Reasoner, a vision-language model post-trained solely on general-domain text for generalizable reasoning, using a two-stage approach: an initial supervised fine-tuning phase with distilled long chain-of-thoughts, followed by reinforcement learning with verifiable rewards. Experiments show that X-Reasoner successfully transfers reasoning capabilities to both multimodal and out-of-domain settings, outperforming existing state-of-the-art models trained with in-domain and multimodal data across various general and medical benchmarks (Figure 1). Additionally, we find that X-Reasoner's performance in specialized domains can be further enhanced through continued training on domain-specific text-only data. Building upon this, we introduce X-Reasoner-Med, a medical-specialized variant that achieves new state of the art on numerous text-only and multimodal medical benchmarks.

</details>


### [22] [SLOT: Structuring the Output of Large Language Models](https://arxiv.org/abs/2505.04016)

*Darren Yow-Bang Wang, Zhengyuan Shen, Soumya Smruti Mishra, Zhichao Xu, Yifei Teng, Haibo Ding*

**Main category:** cs.CL

**Keywords:** Structured Outputs, Large Language Models, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** SLOT transforms unstructured LLM outputs into structured formats, improving application reliability across various models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of LLMs generating outputs that deviate from predefined schemas, which hampers reliable application development.

**Method:** SLOT uses a fine-tuned lightweight language model as a post-processing layer to achieve flexibility and accuracy in structured output generation.

**Key Contributions:**

	1. Model-agnostic approach to structure outputs from LLMs
	2. Systematic data curation and synthesis pipeline
	3. Formal evaluation methodology for accuracy and fidelity

**Result:** The fine-tuned Mistral-7B model achieves 99.5% schema accuracy and 94.0% content similarity, outperforming Claude-3.5-Sonnet.

**Limitations:** 

**Conclusion:** SLOT enables even smaller LLMs to generate structured outputs comparable to larger models, which is valuable in resource-constrained environments.

**Abstract:** Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.

</details>


### [23] [Advancing and Benchmarking Personalized Tool Invocation for LLMs](https://arxiv.org/abs/2505.04072)

*Xu Huang, Yuefeng Huang, Weiwen Liu, Xingshan Zeng, Yasheng Wang, Ruiming Tang, Hong Xie, Defu Lian*

**Main category:** cs.CL

**Keywords:** Personalized Tool Invocation, Large Language Models, PTool, PTBench, user preferences

**Relevance Score:** 9

**TL;DR:** This paper introduces Personalized Tool Invocation for Large Language Models, addressing user preferences and missing parameters during tool invocation, utilizing a data synthesis framework called PTool and a benchmark named PTBench.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capability of Large Language Models by incorporating personalized constraints during tool invocation, which is critical for problem-solving and accessing timely information.

**Method:** The authors propose PTool, a data synthesis framework that facilitates personalized tool invocation, and they develop PTBench, the first benchmark for evaluating this personalized approach.

**Key Contributions:**

	1. Introduction of the concept of Personalized Tool Invocation.
	2. Development of the PTool data synthesis framework.
	3. Creation of PTBench, the first benchmark for evaluating personalized tool invocation.

**Result:** Evaluation of the PTool framework shows improved performance and the potential for better user satisfaction in tool invocation tasks.

**Limitations:** The study may be limited by a lack of diverse user profiles and preferences in the evaluation dataset.

**Conclusion:** The introduction of personalized constraints significantly enhances the usability of tool invocation in LLMs, suggesting that personalization is a crucial aspect of future tool interfaces.

**Abstract:** Tool invocation is a crucial mechanism for extending the capabilities of Large Language Models (LLMs) and has recently garnered significant attention. It enables LLMs to solve complex problems through tool calls while accessing up-to-date world knowledge. However, existing work primarily focuses on the fundamental ability of LLMs to invoke tools for problem-solving, without considering personalized constraints in tool invocation. In this work, we introduce the concept of Personalized Tool Invocation and define two key tasks: Tool Preference and Profile-dependent Query. Tool Preference addresses user preferences when selecting among functionally similar tools, while Profile-dependent Query considers cases where a user query lacks certain tool parameters, requiring the model to infer them from the user profile. To tackle these challenges, we propose PTool, a data synthesis framework designed for personalized tool invocation. Additionally, we construct \textbf{PTBench}, the first benchmark for evaluating personalized tool invocation. We then fine-tune various open-source models, demonstrating the effectiveness of our framework and providing valuable insights. Our benchmark is public at https://github.com/hyfshadow/PTBench.

</details>


### [24] [Natural Language Generation in Healthcare: A Review of Methods and Applications](https://arxiv.org/abs/2505.04073)

*Mengxian Lyu, Xiaohan Li, Ziyi Chen, Jinqian Pan, Cheng Peng, Sankalp Talankar, Yonghui Wu*

**Main category:** cs.CL

**Keywords:** natural language generation, healthcare applications, large language models, clinical decision-making, medical documentation

**Relevance Score:** 10

**TL;DR:** This paper reviews natural language generation (NLG) methods and applications in the medical domain, highlighting advancements and challenges in utilizing large language models (LLMs) for healthcare.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing integration of NLG in healthcare necessitates a thorough review to understand its capabilities and limitations across diverse data modalities and clinical applications.

**Method:** Systematic review of 113 scientific publications from 3,988 NLG-related articles, focusing on data modality, model architecture, clinical applications, and evaluation methods, adhering to PRISMA guidelines.

**Key Contributions:**

	1. Comprehensive review of NLG methods in healthcare.
	2. Categorization of clinical applications of NLG.
	3. Assessment of capabilities and limitations of NLG models in medical contexts.

**Result:** Identified key methods in NLG, categorized clinical applications, and assessed the effectiveness, limitations, and emerging challenges in healthcare applications of NLG.

**Limitations:** Review limited to scientific publications identified in literature search; may not encompass all relevant advancements in the field.

**Conclusion:** This review provides significant insights into NLG technologies and their applications in medicine, suggesting directions for future research to enhance healthcare outcomes using NLG.

**Abstract:** Natural language generation (NLG) is the key technology to achieve generative artificial intelligence (AI). With the breakthroughs in large language models (LLMs), NLG has been widely used in various medical applications, demonstrating the potential to enhance clinical workflows, support clinical decision-making, and improve clinical documentation. Heterogeneous and diverse medical data modalities, such as medical text, images, and knowledge bases, are utilized in NLG. Researchers have proposed many generative models and applied them in a number of healthcare applications. There is a need for a comprehensive review of NLG methods and applications in the medical domain. In this study, we systematically reviewed 113 scientific publications from a total of 3,988 NLG-related articles identified using a literature search, focusing on data modality, model architecture, clinical applications, and evaluation methods. Following PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines, we categorize key methods, identify clinical applications, and assess their capabilities, limitations, and emerging challenges. This timely review covers the key NLG technologies and medical applications and provides valuable insights for future studies to leverage NLG to transform medical discovery and healthcare.

</details>


### [25] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)

*Manas Satish Bedmutha, Feng Chen, Andrea Hartzler, Trevor Cohen, Nadir Weibel*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, large language models, social signals, healthcare communication, emotion inference

**Relevance Score:** 9

**TL;DR:** This paper presents a system using large language models (LLMs) to analyze and extract social signals in patient-provider dialogues, demonstrating capabilities to track 20 distinct signals that influence healthcare outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving communication between healthcare providers and patients is crucial for better health outcomes, and understanding social signals can enhance this interaction.

**Method:** The authors designed task-specific prompts and evaluated the performance of various LLM architectures on a dataset with 20 annotated social signals.

**Key Contributions:**

	1. Introduction of a system that tracks 20 distinct social signals in healthcare dialogues
	2. Evaluation of different LLM architectures and prompting styles
	3. Insights into enhancing LLM performance for social signal processing in clinical contexts.

**Result:** The study introduces a system that tracks the 20 coded social signals, revealing patterns in LLM behavior and providing insights on enhancing LLM performance in healthcare dialogue analysis.

**Limitations:** The dataset is highly imbalanced, which may affect the robustness of the findings; further research is needed to generalize the results.

**Conclusion:** The findings indicate the potential for LLMs to automate the analysis of social behaviors in clinical settings, which could improve patient-provider communication.

**Abstract:** Effective communication between providers and their patients influences health and care outcomes. The effectiveness of such conversations has been linked not only to the exchange of clinical information, but also to a range of interpersonal behaviors; commonly referred to as social signals, which are often conveyed through non-verbal cues and shape the quality of the patient-provider relationship. Recent advances in large language models (LLMs) have demonstrated an increasing ability to infer emotional and social behaviors even when analyzing only textual information. As automation increases also in clinical settings, such as for transcription of patient-provider conversations, there is growing potential for LLMs to automatically analyze and extract social behaviors from these interactions. To explore the foundational capabilities of LLMs in tracking social signals in clinical dialogue, we designed task-specific prompts and evaluated model performance across multiple architectures and prompting styles using a highly imbalanced, annotated dataset spanning 20 distinct social signals such as provider dominance, patient warmth, etc. We present the first system capable of tracking all these 20 coded signals, and uncover patterns in LLM behavior. Further analysis of model configurations and clinical context provides insights for enhancing LLM performance on social signal processing tasks in healthcare settings.

</details>


### [26] [Bringing legal knowledge to the public by constructing a legal question bank using large-scale pre-trained language model](https://arxiv.org/abs/2505.04132)

*Mingruo Yuan, Ben Kao, Tien-Hsuan Wu, Michael M. K. Cheung, Henry W. H. Chan, Anne S. Y. Cheung, Felix W. H. Chan, Yongxi Chen*

**Main category:** cs.CL

**Keywords:** Legal Information Access, Natural Language Processing, Legal Question Generation

**Relevance Score:** 4

**TL;DR:** This paper presents a three-step approach to improve the accessibility and comprehensibility of legal information for laypersons, including the use of large language models to generate legal questions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance access to legal information and make it comprehensible for individuals without legal education.

**Method:** The approach involves three steps: translating legal documents into CLIC-pages, constructing a Legal Question Bank (LQB) with relevant legal questions, and designing an interactive CLIC Recommender (CRec) for user queries.

**Key Contributions:**

	1. Formulated a three-step method for legal knowledge accessibility
	2. Developed a Legal Question Bank using large language models
	3. Created an interactive recommender system for legal inquiries

**Result:** The study demonstrates that machine-generated questions (MGQs) are more scalable and diversified compared to human-composed questions (HCQs), although HCQs remain more precise.

**Limitations:** 

**Conclusion:** The three-step approach effectively makes legal knowledge more accessible to the public, and a prototype of the CRec is showcased as a solution.

**Abstract:** Access to legal information is fundamental to access to justice. Yet accessibility refers not only to making legal documents available to the public, but also rendering legal information comprehensible to them. A vexing problem in bringing legal information to the public is how to turn formal legal documents such as legislation and judgments, which are often highly technical, to easily navigable and comprehensible knowledge to those without legal education. In this study, we formulate a three-step approach for bringing legal knowledge to laypersons, tackling the issues of navigability and comprehensibility. First, we translate selected sections of the law into snippets (called CLIC-pages), each being a small piece of article that focuses on explaining certain technical legal concept in layperson's terms. Second, we construct a Legal Question Bank (LQB), which is a collection of legal questions whose answers can be found in the CLIC-pages. Third, we design an interactive CLIC Recommender (CRec). Given a user's verbal description of a legal situation that requires a legal solution, CRec interprets the user's input and shortlists questions from the question bank that are most likely relevant to the given legal situation and recommends their corresponding CLIC pages where relevant legal knowledge can be found. In this paper we focus on the technical aspects of creating an LQB. We show how large-scale pre-trained language models, such as GPT-3, can be used to generate legal questions. We compare machine-generated questions (MGQs) against human-composed questions (HCQs) and find that MGQs are more scalable, cost-effective, and more diversified, while HCQs are more precise. We also show a prototype of CRec and illustrate through an example how our 3-step approach effectively brings relevant legal knowledge to the public.

</details>


### [27] [Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models](https://arxiv.org/abs/2505.04135)

*Vihaan Miriyala, Smrithi Bukkapatnam, Lavanya Prahallad*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought prompting, large language models, sentiment classification, app store reviews

**Relevance Score:** 9

**TL;DR:** The paper investigates Chain-of-Thought (CoT) prompting with large language models to enhance sentiment classification accuracy in app store reviews, significantly improving results compared to simple prompting.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional methods of sentiment analysis often overlook nuanced user feedback in app reviews, necessitating improved techniques for better classification.

**Method:** The authors conducted an evaluation of CoT prompting against simple prompting using 2000 Amazon app reviews and compared predictions with human judgements.

**Key Contributions:**

	1. Demonstrated the effectiveness of Chain-of-Thought prompting in sentiment analysis.
	2. Quantified the accuracy improvement in sentiment classification from 84% to 93%.
	3. Showed the limitations of traditional numeric-based sentiment ratings.

**Result:** CoT prompting improved classification accuracy from 84% to 93%, demonstrating its effectiveness in sentiment analysis.

**Limitations:** The study is limited to app reviews from Amazon, which may not generalize to other contexts.

**Conclusion:** The findings underscore the advantages of incorporating explicit reasoning in LLMs for improved sentiment analysis.

**Abstract:** We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.

</details>


### [28] [Unmasking the Canvas: A Dynamic Benchmark for Image Generation Jailbreaking and LLM Content Safety](https://arxiv.org/abs/2505.04146)

*Variath Madhupal Gautham Nair, Vishal Varma Dantuluri*

**Main category:** cs.CL

**Keywords:** large language models, image generation, content safety, adversarial inputs, benchmark dataset

**Relevance Score:** 7

**TL;DR:** The paper introduces the Unmasking the Canvas (UTC Benchmark; UTCB), a benchmark dataset aimed at evaluating the vulnerability of large language models to prompt-based jailbreaks in image generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities in content safety checks of existing large language models that allow for the generation of compromising images through simple prompts.

**Method:** The methodology involves structured prompt engineering, multilingual obfuscation techniques, and evaluation using the Groq-hosted LLaMA-3 model, supporting various prompting strategies and risk scoring.

**Key Contributions:**

	1. Introduction of a structured benchmark for evaluating LLM vulnerabilities in image generation.
	2. Implementation of multilingual obfuscation techniques to enhance prompt testing.
	3. Development of a tiered system for verifying the safety of generated outputs.

**Result:** The benchmark provides a dynamic framework for testing model vulnerabilities, categorizing outputs into verified and non-verified tiers, and includes rich metadata for all generated content.

**Limitations:** The paper contains visual examples of adversarial inputs which may raise ethical concerns, and all outputs have been redacted.

**Conclusion:** The UTCB aims to continuously evolve, adapting to new data sources and model behaviors to improve the assessment of LLM safety.

**Abstract:** Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.   We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.   Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure.

</details>


### [29] [Can Language Models Understand Social Behavior in Clinical Conversations?](https://arxiv.org/abs/2505.04152)

*Manas Satish Bedmutha, Feng Chen, Andrea Hartzler, Trevor Cohen, Nadir Weibel*

**Main category:** cs.CL

**Keywords:** social signals, large language models, healthcare, patient-provider interactions, conversation analysis

**Relevance Score:** 9

**TL;DR:** The paper explores the use of large language models (LLMs) to analyze social signals in clinical dialogue, presenting a system that tracks 20 distinct social signals effectively.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve health and care outcomes by enhancing the analysis of interactions between healthcare providers and patients through understanding social signals.

**Method:** The authors designed task-specific prompts and evaluated model performance across various architectures using an imbalanced dataset annotated with social signals like provider dominance and patient warmth.

**Key Contributions:**

	1. Development of a system to track 20 social signals in clinical dialogues
	2. Insights into LLM behavior concerning social signals
	3. Evaluation of different model configurations and their performance in healthcare contexts

**Result:** The study introduces a system capable of tracking 20 coded social signals and reveals behavioral patterns of LLMs in understanding these signals.

**Limitations:** 

**Conclusion:** The findings suggest potential ways to enhance LLM performance in identifying social signals in clinical settings, contributing to the automation of conversation analysis.

**Abstract:** Effective communication between providers and their patients influences health and care outcomes. The effectiveness of such conversations has been linked not only to the exchange of clinical information, but also to a range of interpersonal behaviors; commonly referred to as social signals, which are often conveyed through non-verbal cues and shape the quality of the patient-provider relationship. Recent advances in large language models (LLMs) have demonstrated an increasing ability to infer emotional and social behaviors even when analyzing only textual information. As automation increases also in clinical settings, such as for transcription of patient-provider conversations, there is growing potential for LLMs to automatically analyze and extract social behaviors from these interactions. To explore the foundational capabilities of LLMs in tracking social signals in clinical dialogue, we designed task-specific prompts and evaluated model performance across multiple architectures and prompting styles using a highly imbalanced, annotated dataset spanning 20 distinct social signals such as provider dominance, patient warmth, etc. We present the first system capable of tracking all these 20 coded signals, and uncover patterns in LLM behavior. Further analysis of model configurations and clinical context provides insights for enhancing LLM performance on social signal processing tasks in healthcare settings.

</details>


### [30] [LLM-Independent Adaptive RAG: Let the Question Speak for Itself](https://arxiv.org/abs/2505.04253)

*Maria Marina, Nikolay Ivanov, Sergey Pletenev, Mikhail Salnikov, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii*

**Main category:** cs.CL

**Keywords:** large language models, adaptive retrieval, efficient QA systems, information retrieval, external information

**Relevance Score:** 9

**TL;DR:** This paper presents lightweight LLM-independent adaptive retrieval methods that improve efficiency in QA systems while matching complex LLM-based methods' performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination issues in LLMs and the inefficiencies in existing retrieval methods, providing a more efficient alternative for adaptive retrieval.

**Method:** The study investigates 27 features grouped into 7 categories to develop adaptive retrieval methods that utilize external information instead of relying on LLM-based uncertainty estimation.

**Key Contributions:**

	1. Introduction of LLM-independent adaptive retrieval methods
	2. Evaluation of 27 features for hybrid combinations
	3. Demonstration of performance efficiency gains compared to LLM-based methods.

**Result:** The proposed methods demonstrate competitive QA performance compared to complex LLM-based approaches while offering significant efficiency improvements.

**Limitations:** The study may have limitations regarding the specific contexts or datasets where the proposed methods can be applied effectively.

**Conclusion:** The research highlights the effectiveness of utilizing external information for adaptive retrieval, underlining its potential to enhance performance in QA tasks without the heavy computational costs of LLMs.

**Abstract:** Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.

</details>


### [31] [GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer Pharmacovigilance](https://arxiv.org/abs/2505.04284)

*Sofia Jamil, Aryan Dabad, Bollampalli Areen Reddy, Sriparna Saha, Rajiv Misra, Adil A. Shakur*

**Main category:** cs.CL

**Keywords:** adverse drug events, cancer treatment, pharmacovigilance, grouped summarization, large language models

**Relevance Score:** 9

**TL;DR:** This paper presents a novel method for summarizing adverse drug events (ADEs) in cancer treatment using a new dataset and framework called GASCADE.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance pharmacovigilance and improve decision-making in cancer treatment by summarizing ADEs reported by patients.

**Method:** Introduces the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset and the GASCAD framework, which combines LLMs for information extraction and T5 for summarization.

**Key Contributions:**

	1. Development of the MCADRS dataset for summarizing ADEs in cancer treatment
	2. Proposing the GASCADE framework that integrates LLMs with T5 for ADE summarization
	3. Application of novel alignment techniques in summarization tasks

**Result:** Demonstrates superior performance of the GASCADE framework in summarizing ADEs across various metrics, validated by both automated assessments and human evaluations.

**Limitations:** 

**Conclusion:** This work enhances understanding of patient concerns and supports more personalized cancer care, with publicly available code and dataset.

**Abstract:** In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available.

</details>


### [32] [The Aloe Family Recipe for Open and Specialized Healthcare LLMs](https://arxiv.org/abs/2505.04388)

*Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare, Open-source, Ethical AI, Model Safety

**Relevance Score:** 10

**TL;DR:** Aloe Beta models enhance open-source medical LLMs by optimizing data processing and improving safety and efficacy, demonstrating competitive performance in healthcare.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The development of competitive open-source models is essential to protect public interest in healthcare applications of LLMs.

**Method:** The study builds on Llama 3.1 and Qwen 2.5, using a custom dataset with synthetic Chain of Thought examples, and evaluates model alignment using Direct Preference Optimization.

**Key Contributions:**

	1. Optimization of data preprocessing and training for LLMs
	2. Introduction of a comprehensive evaluation methodology
	3. Significant improvements in model safety and efficacy

**Result:** The Aloe Beta models perform competitively across healthcare benchmarks and have shown improvements in safety, particularly against bias and jailbreaking attacks.

**Limitations:** 

**Conclusion:** Aloe Beta models set a new standard in the open-source medical LLM field, balancing high performance with ethical considerations.

**Abstract:** Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.

</details>


### [33] [Large Means Left: Political Bias in Large Language Models Increases with Their Number of Parameters](https://arxiv.org/abs/2505.04393)

*David Exler, Mark Schutera, Markus Reischl, Luca Rettenberger*

**Main category:** cs.CL

**Keywords:** political bias, large language models, Wahl-O-Mat, user education, LLM influence

**Relevance Score:** 8

**TL;DR:** This paper evaluates the political biases of large language models (LLMs) using the context of German Bundestag votes, finding a prevalent bias towards left-leaning parties and highlighting the role of communication language and model origin in shaping these biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the biases in LLMs as they are increasingly relied upon for information, understanding their influence on user opinions and decision-making processes, especially in political contexts.

**Method:** The study quantifies the political bias of LLMs by using the Wahl-O-Mat score to measure alignment with German political parties, comparing models' alignment scores along with factors like communication language, model's origin, and release date.

**Key Contributions:**

	1. Quantifies the political bias in LLMs using specific political metrics.
	2. Establishes a link between communication language and models' political biases.
	3. Highlights the implications of LLM biases on public opinion and decision-making.

**Result:** Results show a significant bias towards left-leaning parties in larger LLMs and suggest communication language influences their political perspectives.

**Limitations:** Focuses on political bias in a specific context (German Bundestag votes), limiting generalizability to other areas or different political landscapes.

**Conclusion:** The findings underscore the responsibility of corporations developing LLMs to mitigate biases as they can impact voter behavior and public opinion at large.

**Abstract:** With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users. Large language models (LLMs) are predominantly used by many as a primary source of information for various topics. LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions. Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification. We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat. This metric measures the alignment between an individual's political views and the positions of German political parties. We compare the models' alignment scores to identify factors influencing their political preferences. Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs. Also, we find that the language we use to communicate with the models affects their political views. Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag. Our results imply that LLMs are prone to exhibiting political bias. Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale.

</details>


### [34] [YABLoCo: Yet Another Benchmark for Long Context Code Generation](https://arxiv.org/abs/2505.04406)

*Aidar Valeev, Roman Garaev, Vadim Lomshakov, Irina Piontkovskaya, Vladimir Ivanov, Israel Adewuyi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Code Generation, Benchmarking, C/C++, Software Repositories

**Relevance Score:** 8

**TL;DR:** This paper presents YABLoCo, a long context code generation benchmark for evaluating Large Language Models (LLMs) on function body generation in large C and C++ codebases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for benchmarking LLMs on code generation tasks in large software repositories, which can contain millions of lines of code.

**Method:** Introduction of YABLoCo, a benchmark featuring 215 function samples from large repositories, with accompanying metadata, documentations, and call graphs.

**Key Contributions:**

	1. Introduction of the YABLoCo benchmark for long context code generation in C and C++.
	2. Inclusion of large repositories with millions of lines of code for benchmark evaluation.
	3. Development of a scalable evaluation pipeline and visual analysis tool for generated code.

**Result:** The benchmark facilitates evaluation of LLMs on function body generation in extensive C and C++ repositories ranging from 200K to 2,000K lines of code.

**Limitations:** 

**Conclusion:** The established benchmark and evaluation pipeline allow comprehensive assessment of LLM performance in real-world coding scenarios.

**Abstract:** Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.

</details>


### [35] [OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models](https://arxiv.org/abs/2505.04416)

*Xiaoyu Xu, Minxin Du, Qingqing Ye, Haibo Hu*

**Main category:** cs.CL

**Keywords:** unlearning, large language models, membership inference, model utility, low-rank adapters

**Relevance Score:** 9

**TL;DR:** The paper introduces OBLIVIATE, a framework for unlearning sensitive content in large language models while maintaining their utility.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** Address the risks of large language models memorizing sensitive, copyrighted, or toxic content.

**Method:** The framework extracts target tokens, builds retain sets, and fine-tunes using a loss function with masking, distillation, and world fact components, implemented using low-rank adapters for efficiency.

**Key Contributions:**

	1. Introduction of a structured unlearning process to remove targeted data from LLMs.
	2. Utilization of low-rank adapters to enhance efficiency in the unlearning process.
	3. Demonstrated effectiveness against membership inference attacks with a minimal impact on retained data.

**Result:** Experiments on various datasets show that OBLIVIATE effectively mitigates membership inference attacks while preserving model utility and fluency.

**Limitations:** 

**Conclusion:** OBLIVIATE is a robust solution for unlearning while maintaining the integrity and performance of large language models.

**Abstract:** Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.

</details>


### [36] [Detecting Spelling and Grammatical Anomalies in Russian Poetry Texts](https://arxiv.org/abs/2505.04507)

*Ilya Koziev*

**Main category:** cs.CL

**Keywords:** natural language processing, generative models, anomaly detection, machine learning, data quality

**Relevance Score:** 8

**TL;DR:** This paper addresses the quality of fine-tuning datasets for generative models, focusing on automated linguistic anomaly detection to improve training data for computational creativity tasks like poetry generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The quality of natural language texts in fine-tuning datasets critically affects generative model performance, especially in tasks like poem generation where fluency is key.

**Method:** A comprehensive comparison of unsupervised and supervised text anomaly detection methods was conducted using synthetic and human-labeled datasets.

**Key Contributions:**

	1. Introduction of automated linguistic anomaly detection for training datasets.
	2. Comparison of unsupervised and supervised detection methods.
	3. Release of the RUPOR dataset for grammatical error detection.

**Result:** The study introduces the RUPOR dataset, which is specifically designed for detecting grammatical errors in Russian-language poems, and presents evaluation code for the community.

**Limitations:** 

**Conclusion:** The findings aim to provide tools and insights for enhancing the quality of training datasets for generative models in creative fields.

**Abstract:** The quality of natural language texts in fine-tuning datasets plays a critical role in the performance of generative models, particularly in computational creativity tasks such as poem or song lyric generation. Fluency defects in generated poems significantly reduce their value. However, training texts are often sourced from internet-based platforms without stringent quality control, posing a challenge for data engineers to manage defect levels effectively.   To address this issue, we propose the use of automated linguistic anomaly detection to identify and filter out low-quality texts from training datasets for creative models. In this paper, we present a comprehensive comparison of unsupervised and supervised text anomaly detection approaches, utilizing both synthetic and human-labeled datasets. We also introduce the RUPOR dataset, a collection of Russian-language human-labeled poems designed for cross-sentence grammatical error detection, and provide the full evaluation code. Our work aims to empower the community with tools and insights to improve the quality of training datasets for generative models in creative domains.

</details>


### [37] [Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs](https://arxiv.org/abs/2505.04519)

*Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan*

**Main category:** cs.CL

**Keywords:** Sparse LLMs, Mixture of Experts, Ascend NPUs, Efficient Training, Deep Learning

**Relevance Score:** 6

**TL;DR:** This paper presents efficient training methods for large sparse language models using Ascend NPUs, culminating in the development of Pangu Ultra MoE with 718 billion parameters.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large language models (LLMs) using Mixture of Experts (MoE), there is a need to manage the challenges posed by their massive scales in terms of software and hardware systems.

**Method:** The authors utilize simulation to compare various model hyperparameters and choose configurations optimal for Ascend NPUs, while focusing on Expert Parallelism to enhance communication and memory efficiency.

**Key Contributions:**

	1. Development of Pangu Ultra MoE with 718 billion parameters
	2. Demonstration of efficient training methods on Ascend NPUs
	3. Optimization of communication and memory efficiency in training processes

**Result:** Achieved an MFU of 30.0% during the training of Pangu Ultra MoE on 6K Ascend NPUs, demonstrating performance on par with DeepSeek R1.

**Limitations:** 

**Conclusion:** The study provides a viable method for efficient training of large-scale sparse LLMs and insights into their behaviors for future research.

**Abstract:** Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.

</details>


### [38] [Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review](https://arxiv.org/abs/2505.04531)

*Josh McGiff, Nikola S. Nikolov*

**Main category:** cs.CL

**Keywords:** natural language processing, generative language modeling, low-resource languages, data scarcity, inclusive AI

**Relevance Score:** 6

**TL;DR:** This paper reviews strategies for improving generative language modeling for low-resource languages (LRLs), identifying and evaluating various technical approaches and highlighting the need for more inclusive AI tools.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address linguistic inequality in natural language processing (NLP) due to the dominance of high-resource languages in generative language models.

**Method:** A systematic review of 54 studies focusing on techniques for data scarcity in generative language modeling for LRLs, analyzing monolingual data augmentation, back-translation, multilingual training, and prompt engineering.

**Key Contributions:**

	1. First systematic review on generative language modeling for low-resource languages.
	2. Evaluation of various technical approaches to address data scarcity.
	3. Recommendations for extending language modeling methods to underrepresented languages.

**Result:** The review reveals a strong reliance on transformer models, a concentration of research on a limited number of LRLs, and inconsistent evaluation methods across studies.

**Limitations:** The study highlights a lack of consistent evaluation methods and an over-focus on a small subset of low-resource languages.

**Conclusion:** Recommendations are provided on how to apply existing methods to a broader range of LRLs, along with a discussion of open challenges in creating equitable generative language systems.

**Abstract:** Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini. While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English. This has amplified concerns over linguistic inequality in natural language processing (NLP). This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL). Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks. We also analyse trends in architecture choices, language family representation, and evaluation methods. Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies. We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems. Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies.

</details>


### [39] [ZeroSearch: Incentivize the Search Capability of LLMs without Searching](https://arxiv.org/abs/2505.04588)

*Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang*

**Main category:** cs.CL

**Keywords:** reinforcement learning, large language models, search capabilities, information retrieval, curriculum learning

**Relevance Score:** 8

**TL;DR:** ZeroSearch is a reinforcement learning framework for improving LLMs' search capabilities without relying on real search engines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning and generation capabilities of LLMs through effective information searching and address challenges in existing RL approaches due to document quality and API costs.

**Method:** ZeroSearch begins with supervised fine-tuning of the LLM to create a retrieval module, followed by RL training using a curriculum-based rollout strategy that gradually decreases document quality to improve reasoning.

**Key Contributions:**

	1. Introduction of the ZeroSearch framework for LLMs' search capabilities
	2. Use of a curriculum-based approach to RL training
	3. Demonstration of performance surpassing real search engines with larger models.

**Result:** ZeroSearch demonstrates that LLMs can effectively enhance their search capabilities, achieving performance comparable to real search engines with 7B and surpassing it with 14B models.

**Limitations:** 

**Conclusion:** The framework generalizes well across various LLM models and is compatible with different reinforcement learning algorithms.

**Abstract:** Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.

</details>


### [40] [Advancements and limitations of LLMs in replicating human color-word associations](https://arxiv.org/abs/2411.02116)

*Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara*

**Main category:** cs.CL

**Keywords:** color-word association, large language models, human cognition, semantic memory, machine learning

**Relevance Score:** 7

**TL;DR:** The study investigates the ability of large language models (LLMs) to match human color-word associations, revealing advancements and limitations across model generations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well LLMs replicate human color-word associations, which are crucial in cognition and design, and to analyze performance variations across different color categories and associated words.

**Method:** We compared various LLM generations (GPT-3 to GPT-4o) against a dataset of color-word associations collected from over 10,000 participants, evaluating their predictive accuracy in word assignments for different colors.

**Key Contributions:**

	1. Comparison of LLM performance across generations in color-word association tasks
	2. Demonstration of performance variations across different word categories and colors
	3. Identification of systematic differences in semantic memory between humans and LLMs

**Result:** GPT-4o showed the highest accuracy in predicting color associations but reached only about 50% median performance, struggling particularly with categories related to emotions, despite aligning closely with human color discrimination patterns.

**Limitations:** The highest median performance was only around 50%, indicating ongoing limitations in LLMs' understanding of color-word associations, especially with emotional context.

**Conclusion:** The findings highlight advancements in LLM capabilities while also indicating significant systematic differences in how humans and LLMs relate colors to words, suggesting the need for further exploration of semantic memory structures between the two.

**Abstract:** Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT-4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and 80 words (10 word from eight categories) in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category. However, the highest median performance was approximately 50% even for GPT-4o with visual inputs (chance level of 10%). Moreover, we found performance variations across word categories and colors: while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions. Interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. Thus, despite reasonable alignment in basic color discrimination, humans and LLMs still diverge systematically in the words they assign to those colors. Our study highlights both the advancements in LLM capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and LLMs in representing color-word associations.

</details>


### [41] [Playing repeated games with Large Language Models](https://arxiv.org/abs/2305.16867)

*Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz*

**Main category:** cs.CL

**Keywords:** LLM, behavioral game theory, cooperation, coordination, social behavior

**Relevance Score:** 9

**TL;DR:** This paper investigates the cooperation and coordination behaviors of large language models (LLMs) through behavioral game theory, demonstrating their effectiveness in self-interested scenarios while highlighting limitations in coordination tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the cooperation and coordination behavior of LLMs when interacting with humans and other agents using behavioral game theory.

**Method:** LLMs are analyzed in finitely repeated 2x2 games against each other and human players, focusing on strategies and outcomes in various game types.

**Key Contributions:**

	1. Demonstrated LLM's performance in game theory settings with human-like strategies.
	2. Identified strengths and weaknesses in LLM cooperation and coordination.
	3. Introduced the 'social chain-of-thought' strategy to enhance LLM interaction with humans.

**Result:** LLMs excel in self-interested games like the iterated Prisoner's Dilemma, but struggle with coordination in games like the Battle of the Sexes. GPT-4's performance improves with opponent information and a 'social chain-of-thought' strategy.

**Limitations:** The study may not account for all potential complexities in real-world human-LLM interactions.

**Conclusion:** The findings enhance the comprehension of LLM behavior in social contexts and propose a framework for applying behavioral game theory to machine interactions.

**Abstract:** LLMs are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour. We let different LLMs play finitely repeated $2\times2$ games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination, like the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We additionally show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a "social chain-of-thought" (SCoT) strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.

</details>


### [42] [Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models](https://arxiv.org/abs/2308.15022)

*Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, Liang Ding*

**Main category:** cs.CL

**Keywords:** large language models, long-term memory, dialogue consistency, recursion, memory generation

**Relevance Score:** 9

**TL;DR:** Proposes a method to enhance long-term memory in large language models for consistent responses in long conversations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address issues of memory recall and response consistency in long conversations with LLMs.

**Method:** Recursively generates summaries/memory using LLMs to memorize conversation contexts and produce new memory for consistent dialogue.

**Key Contributions:**

	1. Recursive memory generation for LLMs
	2. Enhanced consistency in long conversations
	3. Integration with long-context and retrieval-enhanced LLMs

**Result:** Experiments show improved consistency in responses across both open and closed LLMs on public datasets, supporting long-context and retrieval-enhanced dialogues.

**Limitations:** 

**Conclusion:** The method offers a promising solution for LLMs to handle extremely long contexts effectively.

**Abstract:** Recently, large language models (LLMs), such as GPT-4, stand out remarkable conversational abilities, enabling them to engage in dynamic and contextually relevant dialogues across a wide range of topics. However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses. To address this, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the chatbot can easily generate a highly consistent response with the help of the latest memory. We evaluate our method on both open and closed LLMs, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Also, we show that our strategy could nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced LLMs, bringing further long-term dialogue performance. Notably, our method is a potential solution to enable the LLM to model the extremely long context. The code and scripts will be released later.

</details>


### [43] [Large Language Models Are Struggle to Cope with Unreasonability in Math Problems](https://arxiv.org/abs/2403.19346)

*Jingyuan Ma, Damai Dai, Zihang Yuan, Rui li, Weilin Luo, Bin Wang, Qun Liu, Lei Sha, Zhifang Sui*

**Main category:** cs.CL

**Keywords:** LLMs, math problems, benchmark, reasoning, unreasonability

**Relevance Score:** 7

**TL;DR:** This paper introduces the Unreasonable Math Problem benchmark to evaluate LLMs' capacity to address unconventional math problems with unreasonability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the unexplored capacity of LLMs in handling math problems with internal inconsistencies and flawed assumptions.

**Method:** The authors propose a novel benchmark called Unreasonable Math Problem (UMP), which consists of a collection of curated unreasonable math questions and conduct experiments across 19 LLMs.

**Key Contributions:**

	1. Introduction of the Unreasonable Math Problem benchmark
	2. Empirical evaluation of 19 LLMs on unreasonable math problems
	3. Insights into improving LLMs' recognition of unreasonable inputs

**Result:** The results show that even state-of-the-art models like GPT-4o have limited performance (0.6) on the UMP, and other reasoning models demonstrate instability and overthinking.

**Limitations:** The UMP may not cover all possible unreasonable scenarios and the performance metrics are limited to specific models tested.

**Conclusion:** The paper highlights both the potential and limitations of LLMs in recognizing unreasonable inputs in math problems.

**Abstract:** Recent research have demonstrated LLMs' impressive performance in math and reasoning. However, the capacity of LLMs to address math problems under unconventional conditions, such as internal inconsistencies and flawed assumptions, remains largely unexplored. In this paper, we propose a novel benchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to recognize and respond to unreasonability in math problem. The benchmark consists of a carefully curated collection of unreasonable math questions across diverse types. Based on extensive experiments covering 19 LLMs, we observe that even state-of-the-art models such as GPT-4o achieve only limited performance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone to overthinking and unstable. We further explore strategies for improving the recognition of unreasonable inputs, shedding light on both the possibility and limitations of LLMs in this challenging setting.

</details>


### [44] [Re-ReST: Reflection-Reinforced Self-Training for Language Agents](https://arxiv.org/abs/2406.01495)

*Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, Nanyun Peng*

**Main category:** cs.CL

**Keywords:** self-training, language agents, reflection, machine learning, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper introduces Reflection-Reinforced Self-Training (Re-ReST) for finetuning language agents using self-generated supervision, enhancing the quality of model-generated samples to improve performance on various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Obtaining reasoning-action trajectories for finetuning language agents is costly and impractical; hence, there is a need for effective self-training methods.

**Method:** Reflection-Reinforced Self-Training (Re-ReST) utilizes a 'reflector' to refine the quality of model-generated samples during self-training by incorporating feedback from external environments.

**Key Contributions:**

	1. Introduction of Reflection-Reinforced Self-Training (Re-ReST) for language agents
	2. Demonstration of improved sample quality through a reflector mechanism
	3. Validation of self-training efficacy across various language agent tasks

**Result:** Self-training improved performance on HotpotQA by 7.6% and AlfWorld by 28.4%, while Re-ReST further enhanced these by 2.0% and 14.1%, respectively.

**Limitations:** 

**Conclusion:** The study validates the effectiveness of self-training and introduces a new approach to utilize reflection during inference without ground-truth feedback, addressing limitations of prior work.

**Abstract:** Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a \textit{reflector} to refine low-quality generated samples during self-training. The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6\% on HotpotQA and 28.4\% on AlfWorld, and Re-ReST further boosting performance by 2.0\% and 14.1\%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work. Our code is released at https://github.com/PlusLabNLP/Re-ReST.

</details>


### [45] [Towards Universal and Black-Box Query-Response Only Attack on LLMs with QROA](https://arxiv.org/abs/2406.02044)

*Hussein Jawad, Yassine Chenik, Nicolas J. -B. Brunel*

**Main category:** cs.CL

**Keywords:** Large Language Models, adversarial attacks, security vulnerabilities, jailbreak, optimization

**Relevance Score:** 8

**TL;DR:** The paper introduces QROA, a black-box jailbreak method that identifies suffixes to bypass alignment safeguards of Large Language Models without needing model internals or human-crafted templates, achieving over 80% Attack Success Rate.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** This work addresses the critical security and ethical vulnerabilities in Large Language Models, particularly their susceptibility to adversarial manipulations and the need for robust safety evaluations.

**Method:** QROA frames the jailbreak attack as an optimization bandit problem, utilizing a surrogate model and token level optimization to explore suffix variations efficiently.

**Key Contributions:**

	1. QROA provides a novel approach to identify adversarial suffixes without requiring internal model access.
	2. It eliminates the reliance on human-designed templates for suffix generation.
	3. QROA-UNV allows for one-query jailbreaks across different instructions, enhancing the method's versatility.

**Result:** Testing across multiple models revealed an Attack Success Rate (ASR) greater than 80%, indicating significant vulnerabilities in LLMs.

**Limitations:** 

**Conclusion:** The findings highlight the urgent need for advanced defenses and more robust safety evaluations for the deployment of AI systems, with the QROA and QROA-UNV methods offering critical insights.

**Abstract:** The rapid adoption of Large Language Models (LLMs) has exposed critical security and ethical vulnerabilities, particularly their susceptibility to adversarial manipulations. This paper introduces QROA, a novel black-box jailbreak method designed to identify adversarial suffixes that can bypass LLM alignment safeguards when appended to a malicious instruction. Unlike existing suffix-based jailbreak approaches, QROA does not require access to the model's logit or any other internal information. It also eliminates reliance on human-crafted templates, operating solely through the standard query-response interface of LLMs. By framing the attack as an optimization bandit problem, QROA employs a surrogate model and token level optimization to efficiently explore suffix variations. Furthermore, we propose QROA-UNV, an extension that identifies universal adversarial suffixes for individual models, enabling one-query jailbreaks across a wide range of instructions. Testing on multiple models demonstrates Attack Success Rate (ASR) greater than 80\%. These findings highlight critical vulnerabilities, emphasize the need for advanced defenses, and contribute to the development of more robust safety evaluations for secure AI deployment. The code is made public on the following link: https://github.com/qroa/QROA

</details>


### [46] [Is In-Context Learning a Type of Error-Driven Learning? Evidence from the Inverse Frequency Effect in Structural Priming](https://arxiv.org/abs/2406.18501)

*Zhenghao Zhou, Robert Frank, R. Thomas McCoy*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Large Language Models, Error-Driven Learning, Inverse Frequency Effect, Structural Priming

**Relevance Score:** 8

**TL;DR:** This paper investigates whether in-context learning (ICL) in large language models is an error-driven learning mechanism, finding evidence that it is akin to human structural priming through the inverse frequency effect (IFE).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if in-context learning (ICL) within large language models (LLMs) is functionally similar to gradient descent and represents an error-driven learning mechanism.

**Method:** Experiments were conducted simulating structural priming with ICL and analyzing the prevalence of the inverse frequency effect (IFE) in LLM responses.

**Key Contributions:**

	1. Introduces a novel method for diagnosing ICL as an error-driven learning mechanism.
	2. Demonstrates the application of the inverse frequency effect in evaluating LLM behavior.
	3. Provides experimental evidence supporting the hypothesis that LLMs utilize error-driven processing.

**Result:** LLMs exhibited the IFE, with a stronger effect observed in larger models, indicating that ICL aligns with error-driven learning processes.

**Limitations:** 

**Conclusion:** The findings suggest that ICL in LLMs may incorporate error-driven learning mechanisms similar to those found in human language processing.

**Abstract:** Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has claimed that ICL is functionally equivalent to gradient descent, a type of error-driven learning mechanism. In this paper, we introduce a new way of diagnosing whether ICL is functionally performing error-driven learning. Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an agent's behavior is influenced to a greater degree when presented with improbable examples as compared to more likely ones. The IFE has previously been identified in psycholinguistics where humans exhibit the IFE in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently). In that context, the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming with ICL and found that LLMs indeed display the IFE, with the effect being stronger in larger models. We conclude that at least in the case we studied, ICL is indeed a type of error-driven learning, supporting the hypothesis that an error signal is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of error-driven processing mechanisms in on-line processing.

</details>


### [47] [Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning](https://arxiv.org/abs/2408.13184)

*Hourui Deng, Hongjie Zhang, Jie Ou, Chaosheng Feng*

**Main category:** cs.CL

**Keywords:** LLMs, Spatial Reasoning, Q-learning, Path-planning, Curriculum Learning

**Relevance Score:** 6

**TL;DR:** This paper presents the S2RCQL model to enhance spatial reasoning in LLMs for path-planning tasks, improving their performance in maze environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of spatial and context inconsistency hallucinations in LLMs during long-term path-planning in simple environments.

**Method:** The study proposes the Spatial-to-Relational Transformation approach for converting spatial prompts into entity relations and a path-planning algorithm based on Q-learning to guide LLMs in optimal path learning.

**Key Contributions:**

	1. Introduction of the Spatial-to-Relational Transformation approach.
	2. Development of a Q-learning based path-planning algorithm for LLMs.
	3. Implementation of reverse curriculum learning to improve learning efficiency.

**Result:** S2RCQL achieved a 23%--40% improvement in success and optimality rates in path-planning tasks compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed methods effectively mitigate hallucinations in LLMs and enhance their reasoning abilities, demonstrating significant improvements in path-planning.

**Abstract:** Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied intelligence. However, even in simple maze environments, LLMs still encounter challenges in long-term path-planning, primarily influenced by their spatial hallucination and context inconsistency hallucination by long-term reasoning. To address this challenge, this study proposes an innovative model, Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs, we propose the Spatial-to-Relational approach, which transforms spatial prompts into entity relations and paths representing entity relation chains. This approach fully taps the potential of LLMs in terms of sequential thinking. As a result, we design a path-planning algorithm based on Q-learning to mitigate the context inconsistency hallucination, which enhances the reasoning ability of LLMs. Using the Q-value of state-action as auxiliary information for prompts, we correct the hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally, we propose a reverse curriculum learning technique based on LLMs to further mitigate the context inconsistency hallucination. LLMs can rapidly accumulate successful experiences by reducing task difficulty and leveraging them to tackle more complex tasks. We performed comprehensive experiments based on Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our S2RCQL achieved a 23%--40% improvement in both success and optimality rates compared with advanced prompt engineering.

</details>


### [48] [Advancements and limitations of LLMs in replicating human color-word associations](https://arxiv.org/abs/2411.02116)

*Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara*

**Main category:** cs.CL

**Keywords:** Large Language Models, color-word associations, Human-Computer Interaction, Machine Learning, semantics

**Relevance Score:** 7

**TL;DR:** The study examines the ability of Large Language Models (LLMs) to replicate human color-word associations, revealing a generational progression in performance but persistent limitations compared to human cognition.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capacity of LLMs in understanding and predicting color-word associations, an area that has not been extensively studied despite the advancements in LLM capabilities.

**Method:** Comparison of LLM generations (GPT-3 to GPT-4o) against data collected from over 10,000 Japanese participants on their color-word associations using 17 colors and 80 words across various categories.

**Key Contributions:**

	1. Demonstration of LLM performance progression in color-word association tasks across generations
	2. Establishment of a correlation between LLM color discrimination and human patterns
	3. Identification of specific word categories where LLMs excel or struggle

**Result:** GPT-4o showed the highest accuracy in predicting color associations among LLMs, yet median performance was around 50%, with variations in success across word categories. LLMs performed well in categories like Rhythm and Landscape but struggled with Emotions.

**Limitations:** The highest accuracy of LLMs was only about 50%, indicating room for improvement in understanding human-like associations.

**Conclusion:** Despite improvements in LLMs' understanding of color-word associations, significant discrepancies remain compared to human associations, suggesting different underlying semantic memory structures.

**Abstract:** Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT-4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and 80 words (10 word from eight categories) in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category. However, the highest median performance was approximately 50% even for GPT-4o with visual inputs (chance level of 10%). Moreover, we found performance variations across word categories and colors: while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions. Interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. Thus, despite reasonable alignment in basic color discrimination, humans and LLMs still diverge systematically in the words they assign to those colors. Our study highlights both the advancements in LLM capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and LLMs in representing color-word associations.

</details>


### [49] [SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub Issue Resolution](https://arxiv.org/abs/2501.05040)

*Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, Kai Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Code fixing, Open-source framework, Software engineering, Machine learning

**Relevance Score:** 9

**TL;DR:** SWE-Fixer is an open-source framework for resolving GitHub issues by fixing code, featuring a two-module system for file retrieval and code editing, achieving competitive performance with efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing proprietary LLMs in software engineering tasks and improve reproducibility and accessibility in code fixing.

**Method:** SWE-Fixer includes a code file retrieval module using BM25 for initial file selection and a code editing module for generating code patches, trained on a newly compiled dataset of 110K GitHub issues.

**Key Contributions:**

	1. Introduction of SWE-Fixer, an open-source framework for GitHub issue resolution
	2. Development of a large dataset of GitHub issues and corresponding patches
	3. Significantly improved efficiency with only two model calls per instance

**Result:** SWE-Fixer achieves competitive results on SWE-Bench Lite (22.0%) and Verified (30.2%) benchmarks, reaching a state-of-the-art 24.7% and 32.8% with filtering techniques.

**Limitations:** 

**Conclusion:** SWE-Fixer demonstrates effectiveness in practical code-fixing tasks, easily accessible due to its open-source nature and efficient model usage.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving competitive performance among open-source models with scores of 22.0% and 30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.

</details>


### [50] [Estimating LLM Uncertainty with Logits](https://arxiv.org/abs/2502.00290)

*Huan Ma, Jingdong Chen, Joey Tianyi Zhou, Guangyu Wang, Changqing Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, uncertainty estimation, halluinations, token reliability, evidence modeling

**Relevance Score:** 9

**TL;DR:** This paper introduces Logits-induced token uncertainty (LogTokU) to enhance uncertainty estimation in Large Language Models (LLMs), addressing the limitations of probability-based methods in token reliability assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid development of LLMs has led to their widespread application, but issues such as hallucinations pose reliability challenges. Understanding token reliability is crucial for improving LLM responses.

**Method:** LogTokU framework estimates decoupled token uncertainty in real-time by utilizing evidence modeling, avoiding the need for multiple sampling processes.

**Key Contributions:**

	1. Introduction of Logits-induced token uncertainty (LogTokU) framework
	2. Real-time uncertainty estimation without multiple sampling
	3. Employing evidence modeling for token reliability assessment

**Result:** Experimental results indicate that LogTokU significantly improves the effectiveness of uncertainty estimation in LLMs, thereby enhancing their reliability.

**Limitations:** 

**Conclusion:** LogTokU provides a promising approach for real-time uncertainty estimation that can be leveraged in downstream applications of LLMs.

**Abstract:** Over the past few years, Large Language Models (LLMs) have developed rapidly and are widely applied in various domains. However, LLMs face the issue of hallucinations, generating responses that may be unreliable when the models lack relevant knowledge. To be aware of potential hallucinations, uncertainty estimation methods have been introduced, and most of them have confirmed that reliability lies in critical tokens. However, probability-based methods perform poorly in identifying token reliability, limiting their practical utility. In this paper, we reveal that the probability-based method fails to estimate token reliability due to the loss of evidence strength information which is accumulated in the training stage. Therefore, we present Logits-induced token uncertainty (LogTokU), a framework for estimating decoupled token uncertainty in LLMs, enabling real-time uncertainty estimation without requiring multiple sampling processes. We employ evidence modeling to implement LogTokU and use the estimated uncertainty to guide downstream tasks. The experimental results demonstrate that LogTokU has significant effectiveness and promise.

</details>


### [51] [Liger: Linearizing Large Language Models to Gated Recurrent Structures](https://arxiv.org/abs/2503.01496)

*Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, Yu Cheng*

**Main category:** cs.CL

**Keywords:** linear recurrent modeling, large language models, Low-Rank Adaptation, gated recurrent models, hybrid attention

**Relevance Score:** 9

**TL;DR:** Liger is a novel approach for converting pretrained large language models (LLMs) into gated linear recurrent models, enhancing efficiency without additional parameters and achieving competitive performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of efficiently pretraining non-standard linear recurrent architectures from scratch, while improving deployment efficacy for pretrained LLMs.

**Method:** Liger repurposes pretrained key matrix weights to form diverse gating mechanisms, utilizing lightweight fine-tuning through Low-Rank Adaptation (LoRA) to optimize performance without training new components from scratch.

**Key Contributions:**

	1. Introduction of Liger for linearizing LLMs to gated recurrent structures
	2. Repurposing key matrix weights for gating mechanisms
	3. Liger Attention as a hybrid attention mechanism that enhances performance

**Result:** Liger successfully matches the performance of linearized gated recurrent models to that of the original LLMs, achieving competitive results across various benchmarks, validated on models ranging from 1B to 8B parameters.

**Limitations:** 

**Conclusion:** Liger effectively enables the efficient deployment of linear recurrent models while maintaining high performance, showcasing the potential for hybrid attention mechanisms in LLMs.

**Abstract:** Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\% of the Transformer-based LLM at 0.02\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.

</details>


### [52] [Designing Speech Technologies for Australian Aboriginal English: Opportunities, Risks and Participation](https://arxiv.org/abs/2503.03186)

*Ben Hutchinson, Celeste Rodríguez Louro, Glenys Collard, Ned Cooper*

**Main category:** cs.CL

**Keywords:** Australian Aboriginal English, speech technologies, participatory design, Indigenous communities, language technology

**Relevance Score:** 4

**TL;DR:** This paper explores the support of speech technologies for Australian Aboriginal English, highlighting opportunities and risks in development practices that include community participation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of support for post-contact language varieties used by Indigenous communities, which hinders their participation in society.

**Method:** The paper discusses a case study focused on design practices in a project aimed at improving speech technologies for Australian Aboriginal English, incorporating participatory and culturally appropriate processes.

**Key Contributions:**

	1. Identification of barriers faced by Indigenous communities using contact varieties
	2. Case study demonstrating successful participatory design practices
	3. Call for technological support inclusive of Indigenous languages and cultures

**Result:** The case study demonstrates that integrating community involvement leads to better development practices, while also highlighting inherent risks.

**Limitations:** Further research is needed to explore long-term sustainable support mechanisms for these languages.

**Conclusion:** There are opportunities for technological support of Indigenous languages, but they must be approached with care to ensure inclusivity and safety for the communities involved.

**Abstract:** In Australia, post-contact language varieties, including creoles and local varieties of international languages, emerged as a result of forced contact between Indigenous communities and English speakers. These contact varieties are widely used, yet are poorly supported by language technologies. This gap presents barriers to participation in civil and economic society for Indigenous communities using these varieties, and reproduces minoritisation of contemporary Indigenous sociolinguistic identities. This paper concerns three questions regarding this context. First, can speech technologies support speakers of Australian Aboriginal English, a local indigenised variety of English? Second, what risks are inherent in such a project? Third, what technology development practices are appropriate for this context, and how can researchers integrate meaningful community participation in order to mitigate risks? We argue that opportunities do exist -- as well as risks -- and demonstrate this through a case study exploring design practices in a real-world project aiming to improve speech technologies for Australian Aboriginal English. We discuss how we integrated culturally appropriate and participatory processes throughout the project. We call for increased support for languages used by Indigenous communities, including contact varieties, which provide practical economic and socio-cultural benefits, provided that participatory and culturally safe practices are enacted.

</details>


### [53] [High-Dimensional Interlingual Representations of Large Language Models](https://arxiv.org/abs/2503.11280)

*Bryan Wilie, Samuel Cahyawijaya, Junxian He, Pascale Fung*

**Main category:** cs.CL

**Keywords:** interlingual representations, multilingual LLMs, cross-lingual alignment, fine-tuning, semantic subspace

**Relevance Score:** 8

**TL;DR:** This study investigates the interlingual representations in multilingual large language models (LLMs) and introduces a framework and metric to evaluate their alignments across diverse languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The nature of interlingual constructs in multilingual LLMs is unclear, with mixed evidence regarding the existence of unified representations.

**Method:** The paper examines 31 languages with varying resources and typologies, introducing the Interlingual Local Overlap (ILO) score to measure interlingual alignment and researching the effects of single-language fine-tuning.

**Key Contributions:**

	1. Introduced the Interlingual Local Overlap (ILO) score for measuring interlingual alignment.
	2. Developed a framework for understanding shared and fragmented interlingual representations in multilingual LLMs.
	3. Demonstrated the impact of single-language fine-tuning on cross-lingual generalization.

**Result:** Findings show inconsistent cross-lingual alignments in multilingual LLMs, and that single-language fine-tuning disrupts interlingual alignment in early layers; freezing these layers helps maintain alignment and improves generalization.

**Limitations:** The study focuses only on a limited set of languages, and further research is needed to explore additional linguistic contexts.

**Conclusion:** The framework and ILO score provide valuable insights into interlingual representation, emphasizing the importance of alignment for effective multilingual learning.

**Abstract:** Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.

</details>


### [54] [OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language Model Hallucinations in Ontology Matching](https://arxiv.org/abs/2503.21813)

*Zhangcheng Qiang*

**Main category:** cs.CL

**Keywords:** large language models, ontology matching, dataset, hallucinations, benchmarking

**Relevance Score:** 8

**TL;DR:** The paper introduces OAEI-LLM-T, a benchmark dataset aimed at addressing hallucinations in large language models for ontology matching tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the persistent issue of hallucinations that occur in tasks using LLMs, particularly in the context of ontology matching systems.

**Method:** The authors create the OAEI-LLM-T dataset based on existing schema-matching datasets, classifying hallucinations into two main categories and six sub-categories relevant to ontology matching.

**Key Contributions:**

	1. Introduction of the OAEI-LLM-T benchmark dataset for ontology matching
	2. Classification of hallucinations into specific categories
	3. Facilitation of LLM leaderboard and fine-tuning for better performance in OM tasks

**Result:** The dataset demonstrates its usefulness by enabling the construction of an LLM leaderboard and allowing for the fine-tuning of foundational LLMs specifically for ontology matching applications.

**Limitations:** 

**Conclusion:** OAEI-LLM-T provides a valuable resource for improving LLM performance in ontology matching tasks by systematically addressing hallucinations.

**Abstract:** Hallucinations are often inevitable in downstream tasks using large language models (LLMs). To tackle the substantial challenge of addressing hallucinations for LLM-based ontology matching (OM) systems, we introduce a new benchmark dataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching) datasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of different LLMs performing OM tasks. These OM-specific hallucinations are carefully classified into two primary categories and six sub-categories. We showcase the usefulness of the dataset in constructing the LLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.

</details>
