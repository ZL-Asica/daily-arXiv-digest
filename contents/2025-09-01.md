# 2025-09-01

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 12]

- [cs.CL](#cs.CL) [Total: 46]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Can LLMs Generate Behaviors for Embodied Virtual Agents Based on Personality Traits?](https://arxiv.org/abs/2508.21087)

*Bin Han, Deuksin Kwon, Spencer Lin, Kaleen Shrestha, Jonathan Gratch*

**Main category:** cs.HC

**Keywords:** Large Language Models, virtual agents, personality traits, Human-Computer Interaction, behavior generation

**Relevance Score:** 9

**TL;DR:** This study presents a framework using Large Language Models (LLMs) to create personality-aligned behaviors in virtual agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how personality traits can influence the behaviors of virtual agents, focusing on the trait of extraversion.

**Method:** Two experiments were conducted: an agent-to-agent simulation assessing linguistic traits and a user study evaluating human perception of these traits in virtual agents.

**Key Contributions:**

	1. Development of a framework for personality prompting in LLMs
	2. Demonstration of personality-aligned verbal and nonverbal behaviors
	3. User perception study confirming recognizability of traits in agents' behaviors

**Result:** LLMs successfully generated verbal and nonverbal behaviors corresponding to various personality traits, and users could recognize these traits in agent behaviors.

**Limitations:** The study primarily focused on extraversion; further research is needed across more personality traits and contexts.

**Conclusion:** The findings highlight the capability of LLMs to effectively shape personality-aligned traits in virtual agents, suggesting valuable applications in HCI.

**Abstract:** This study proposes a framework that employs personality prompting with Large Language Models to generate verbal and nonverbal behaviors for virtual agents based on personality traits. Focusing on extraversion, we evaluated the system in two scenarios: negotiation and ice breaking, using both introverted and extroverted agents. In Experiment 1, we conducted agent to agent simulations and performed linguistic analysis and personality classification to assess whether the LLM generated language reflected the intended traits and whether the corresponding nonverbal behaviors varied by personality. In Experiment 2, we carried out a user study to evaluate whether these personality aligned behaviors were consistent with their intended traits and perceptible to human observers. Our results show that LLMs can generate verbal and nonverbal behaviors that align with personality traits, and that users are able to recognize these traits through the agents' behaviors. This work underscores the potential of LLMs in shaping personality aligned virtual agents.

</details>


### [2] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)

*Vanessa Figueiredo*

**Main category:** cs.HC

**Keywords:** Conversational Agents, Children, Human-Computer Interaction, Structured Scaffolding, LLM

**Relevance Score:** 7

**TL;DR:** The paper explores how Brazilian children interact with conversational agents (CAs) for education and entertainment, revealing designs that enhance these interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the interaction patterns of Brazilian children aged 9-11 with conversational agents and how structured scaffolds can improve these interactions.

**Method:** Two studies were conducted: the first involved interviews and observations of children using CAs, while the second evaluated simulated child-CA exchanges by comparing structured and unstructured prompts.

**Key Contributions:**

	1. First CWA application with Brazilian children
	2. Empirical framework of child-CA information flows
	3. LLM-scaffolding 'recipe' for effective learning

**Result:** The studies found three primary functions of CAs (School, Discovery, Entertainment) and demonstrated that structured prompting significantly improves the quality of interactions in terms of readability, question depth/diversity, and coherence.

**Limitations:** 

**Conclusion:** The findings suggest design recommendations for conversational agents aimed at children, including scaffolded conversation trees and caregiver-curated content, contributing to the field of human-agent interaction.

**Abstract:** This paper presents two studies on how Brazilian children (ages 9--11) use conversational agents (CAs) for schoolwork, discovery, and entertainment, and how structured scaffolds can enhance these interactions. In Study 1, a seven-week online investigation with 23 participants (children, parents, teachers) employed interviews, observations, and Cognitive Work Analysis to map children's information-processing flows, the role of more knowledgeable others, functional uses, contextual goals, and interaction patterns to inform conversation-tree design. We identified three CA functions: School, Discovery, Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support. In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges, comparing conversation-tree recipes based on structured-prompting to an unstructured baseline. Quantitative evaluation of readability, question count/depth/diversity, and coherence revealed gains for the recipe approach. Building on these findings, we offer design recommendations: scaffolded conversation-trees, child-dedicated profiles for personalized context, and caregiver-curated content. Our contributions include the first CWA application with Brazilian children, an empirical framework of child-CA information flows, and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective, scaffolded learning.

</details>


### [3] [Design and evaluation of a serious game in virtual reality to increase empathy towards students with phonological dyslexia](https://arxiv.org/abs/2508.21283)

*Jose Manuel Alcalde-Llergo, Andrea Zingoni, Pilar Aparicio-Martinez, Sara Pinzi, Enrique Yeguas-Bolivar*

**Main category:** cs.HC

**Keywords:** dyslexia, virtual reality, empathy, educators, serious game

**Relevance Score:** 4

**TL;DR:** This study presents a serious virtual reality game designed to enhance empathy towards individuals with dyslexia by simulating their reading challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To raise awareness and understanding of dyslexia among educators and non-dyslexic individuals, highlighting the need for supportive methods.

**Method:** Development and testing of a serious game in virtual reality, followed by evaluation with 101 participants through validated questionnaires.

**Key Contributions:**

	1. Novel application of VR for empathy training regarding dyslexia
	2. Empirical evidence showing increased empathy in participants
	3. Focus on educator and community awareness regarding dyslexia

**Result:** Participants showed an average 20% increase in empathy towards individuals with phonological dyslexia after playing the game.

**Limitations:** 

**Conclusion:** The virtual reality tool effectively promotes empathy towards dyslexic individuals, emphasizing the importance of understanding and support.

**Abstract:** Dyslexia is a neurodevelopmental disorder estimated to strike approximately 5 to 10 per cent of the population. In particular, phonological dyslexia causes problems in connecting the sounds of words with their written forms. Consequently, affected individuals may encounter issues such as slow reading speed, inaccurate reading, and difficulty decoding unfamiliar words. To address these complexities, the use of compensatory tools and strategies is essential to ensure equitable opportunities for dyslexic students. However, the general underestimation of the issue and lack of awareness regarding the significance of support methodologies pose significant obstacles. One of the ways to enhance consciousness towards a certain issue is by stimulating empathy with whom is affected by it. In light of this, this study introduces a serious game in virtual reality, targeted at educators, students, and, in general, at the non-dyslexic community. The game seeks to enhance understanding of the challenges that individuals with dyslexia experience daily, highlighting the relevance of supportive measures. This approach encourages players to empathize with the struggles of dyslexic individuals and to learn firsthand the importance of supportive methodologies. The final version of the experience was tested by 101 participants and evaluated through a specific collection of questionnaires validated in the literature. The results show that using the proposed virtual reality tool to promote empathy for individuals with phonological dyslexia is highly effective, leading to an average 20 per cent increase in participants' empathy after playing the game.

</details>


### [4] [Conflict in Community-Based Design: A Case Study of a Relationship Breakdown](https://arxiv.org/abs/2508.21308)

*Alekhya Gandu, Aakash Gautam*

**Main category:** cs.HC

**Keywords:** community-based design, caste hierarchies, knowledge management, stakeholder engagement, design intervention

**Relevance Score:** 8

**TL;DR:** This paper discusses the complexities faced by community-based designers when engaging with organizations where community practices may reinforce oppression. It highlights the need for reflection and strategic decision-making in design interventions.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** To address the power dynamics in community-based design and how they can align with community values while preventing harm from oppressive practices.

**Method:** The authors reflect on a two-year engagement with a non-profit supporting women in southern India, analyzing their design intervention in light of community practices and organizational gaps.

**Key Contributions:**

	1. Insights into navigating oppressive practices in community design
	2. Framework for reflection and strategic decision-making
	3. Case study of engagement with a non-profit organization in India

**Result:** The study reveals that adhering to harmful practices can jeopardize the design's integrity, leading to breakdowns in partnerships.

**Limitations:** The findings are based on a single case study and may not be generalizable to all community-based design efforts.

**Conclusion:** Community-based designers must navigate value conflicts through reflection, role repositioning, and potential withdrawal from partnerships when necessary.

**Abstract:** Community-based design efforts rightly seek to reduce the power differences between researchers and community participants by aligning with community values and furthering their priorities. However, what should designers do when key community members' practices seem to enact an oppressive and harmful structure? We reflect on our two-year-long engagement with a non-profit organization in southern India that supports women subjected to domestic abuse or facing mental health crises. We highlight the organizational gaps in knowledge management and transfer, which became an avenue for our design intervention. During design, we encountered practices that upheld caste hierarchies. These practices were expected to be incorporated into our technology. Anticipating harms to indirect stakeholders, we resisted this incorporation. It led to a breakdown in our relationship with the partner organization. Reflecting on this experience, we outline pluralistic pathways that community-based designers might inhabit when navigating value conflicts. These include making space for reflection before and during engagements, strategically repositioning through role reframing or appreciative inquiry, and exiting the engagement if necessary.

</details>


### [5] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)

*Yi-Hao Peng, Dingzeyu Li, Jeffrey P. Bigham, Amy Pavel*

**Main category:** cs.HC

**Keywords:** User Interface Agents, Blind and Low-Vision Users, Decision Making, Mixed-Initiative Systems, Multimodal Models

**Relevance Score:** 9

**TL;DR:** Introducing Morae, a UI agent that enhances agency for blind and low-vision users by involving them in decision-making during task execution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current UI agents reduce user agency by making end-to-end decisions without user involvement, particularly affecting blind and low-vision users.

**Method:** Morae employs large multimodal models to identify decision points during tasks, pausing to allow users to make choices informed by context, such as UI code and screenshots.

**Key Contributions:**

	1. Development of Morae, a UI agent that enhances user agency for BLV users
	2. Implementation of large multimodal models for decision point identification
	3. Empirical study showing performance improvements over baseline agents

**Result:** In studies with BLV participants, Morae outperformed baseline agents by enabling users to complete more tasks and make selections aligned with their preferences.

**Limitations:** 

**Conclusion:** Morae demonstrates a successful mixed-initiative approach that balances automation with user preference expression, improving overall interaction quality.

**Abstract:** User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.

</details>


### [6] [Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education](https://arxiv.org/abs/2508.21666)

*Imran S. A. Khan, Emmanuel G. Blanchard, Sébastien George*

**Main category:** cs.HC

**Keywords:** climate resilience, IoT, Generative AI, adaptive learning, educational technology

**Relevance Score:** 4

**TL;DR:** The paper presents FACTS, a platform combining IoT data and Generative AI to enhance climate resilience education through personalized learning experiences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance climate resilience education and improve engagement through adaptive learning technologies.

**Method:** FACTS leverages real-time atmospheric data from IoT sensors alongside a curated Knowledge Base to create localized learning challenges, with a Generative AI system providing real-time analysis and feedback.

**Key Contributions:**

	1. Introduction of a novel educational platform (FACTS) combining IoT and AI.
	2. Demonstration of effective personalized learning in climate education.
	3. User evaluation showing high usability and engagement.

**Result:** User evaluations showed that participants found FACTS easy to use and effective for learning, indicating its potential for educational engagement in climate resilience.

**Limitations:** The paper does not discuss the scalability of the platform or long-term impact on learner retention.

**Conclusion:** Integrating IoT and Generative AI into learning technologies can significantly enhance educational experiences and awareness of climate issues.

**Abstract:** This paper introduces the Future Atmospheric Conditions Training System (FACTS), a novel platform that advances climate resilience education through place-based, adaptive learning experiences. FACTS combines real-time atmospheric data collected by IoT sensors with curated resources from a Knowledge Base to dynamically generate localized learning challenges. Learner responses are analyzed by a Generative AI powered server, which delivers personalized feedback and adaptive support. Results from a user evaluation indicate that participants found the system both easy to use and effective for building knowledge related to climate resilience. These findings suggest that integrating IoT and Generative AI into atmospherically adaptive learning technologies holds significant promise for enhancing educational engagement and fostering climate awareness.

</details>


### [7] [Developer Insights into Designing AI-Based Computer Perception Tools](https://arxiv.org/abs/2508.21733)

*Maya Guhan, Meghan E. Hurley, Eric A. Storch, John Herrington, Casey Zampella, Julia Parish-Morris, Gabriel Lázaro-Muñoz, Kristin Kostick-Quenet*

**Main category:** cs.HC

**Keywords:** AI, clinical decision-making, user acceptability, clinical workflows, interdisciplinary collaboration

**Relevance Score:** 8

**TL;DR:** The paper discusses the integration of AI-based computer perception technologies in clinical settings, emphasizing the balance between clinical utility and user acceptability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how AI-based computer perception tools can effectively be integrated into clinical workflows by addressing developers' design priorities.

**Method:** Conducted in-depth interviews with 20 developers of AI-based computer perception tools; analyzed transcripts with inductive thematic analysis.

**Key Contributions:**

	1. Identified design priorities for AI-based clinical tools
	2. Emphasized the role of developers as ethical stewards
	3. Suggested strategies for balancing customization and usability

**Result:** Identified four key design priorities for developers: ensuring explainability, aligning with workflows, customizing for stakeholders, and innovating responsibly. Developers see themselves as ethical stewards in tool design.

**Limitations:** 

**Conclusion:** Interdisciplinary collaboration is crucial for achieving a balance between customization, user acceptability, and the advancement of clinical knowledge.

**Abstract:** Artificial intelligence (AI)-based computer perception (CP) technologies use mobile sensors to collect behavioral and physiological data for clinical decision-making. These tools can reshape how clinical knowledge is generated and interpreted. However, effective integration of these tools into clinical workflows depends on how developers balance clinical utility with user acceptability and trustworthiness. Our study presents findings from 20 in-depth interviews with developers of AI-based CP tools. Interviews were transcribed and inductive, thematic analysis was performed to identify 4 key design priorities: 1) to account for context and ensure explainability for both patients and clinicians; 2) align tools with existing clinical workflows; 3) appropriately customize to relevant stakeholders for usability and acceptability; and 4) push the boundaries of innovation while aligning with established paradigms. Our findings highlight that developers view themselves as not merely technical architects but also ethical stewards, designing tools that are both acceptable by users and epistemically responsible (prioritizing objectivity and pushing clinical knowledge forward). We offer the following suggestions to help achieve this balance: documenting how design choices around customization are made, defining limits for customization choices, transparently conveying information about outputs, and investing in user training. Achieving these goals will require interdisciplinary collaboration between developers, clinicians, and ethicists.

</details>


### [8] [MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality](https://arxiv.org/abs/2508.21736)

*Simon Burbach, Maria Maleshkova, Florian Centler, Tanja Joan Schmidt*

**Main category:** cs.HC

**Keywords:** microbiome, virtual reality, data visualization, spatiotemporal, user experience

**Relevance Score:** 4

**TL;DR:** MicroLabVR is a user-friendly VR tool for exploring spatiotemporal simulation data of microbiomes, facilitating analysis and enhancing user experience.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand microbiomes for promoting health and recovery during disease, there is a need for improved tools that allow for the exploration of spatiotemporal data.

**Method:** MicroLabVR utilizes virtual reality to visualize spatiotemporal data derived from CSV datasets, enabling interactive exploration of microbiome simulation results.

**Key Contributions:**

	1. User-friendly VR environment for microbiome data exploration
	2. Interactive visualization of spatiotemporal simulation data
	3. Integration of CSV data for comprehensive analysis

**Result:** MicroLabVR enhances the ability of users to analyze and interact with microbiome data, providing a more intuitive understanding of complex datasets in a spatial context.

**Limitations:** 

**Conclusion:** The tool aims to bridge the gap in current visualization methods by allowing users to engage with microbiome data dynamically and intuitively, improving data analysis outcomes.

**Abstract:** Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities' spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.

</details>


### [9] [PyZoBot: A Platform for Conversational Information Extraction and Synthesis from Curated Zotero Reference Libraries through Advanced Retrieval-Augmented Generation](https://arxiv.org/abs/2405.07963)

*Suad Alshammari, Lama Basalelah, Walaa Abu Rukbah, Ali Alsuhibani, Dayanjan S. Wijesinghe*

**Main category:** cs.HC

**Keywords:** AI, Large Language Models, Research Management, Information Overload, Scientific Literature

**Relevance Score:** 9

**TL;DR:** The paper presents PyZoBot, an AI-driven platform that integrates reference management with advanced techniques like LLMs and RAG to aid researchers in managing and synthesizing scientific literature effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of information overload in scientific literature and improve research efficiency and effectiveness.

**Method:** Development of PyZoBot, which combines Zotero reference management with OpenAI's LLMs for knowledge extraction and synthesis.

**Key Contributions:**

	1. Introduction of PyZoBot as a novel research tool
	2. Integration of LLMs with reference management
	3. Demonstration of improved knowledge extraction and synthesis capabilities

**Result:** PyZoBot successfully manages complex natural language queries, integrates data from multiple sources, and maintains research integrity with precise reference presentation.

**Limitations:** 

**Conclusion:** AI-enhanced tools like PyZoBot can significantly improve how researchers manage information and keep up with rapid scientific advancements.

**Abstract:** The exponential growth of scientific literature has resulted in information overload, challenging researchers to effectively synthesize relevant publications. This paper explores the integration of traditional reference management software with advanced computational techniques, including Large Language Models and Retrieval-Augmented Generation. We introduce PyZoBot, an AI-driven platform developed in Python, incorporating Zoteros reference management with OpenAIs sophisticated LLMs. PyZoBot streamlines knowledge extraction and synthesis from extensive human-curated scientific literature databases. It demonstrates proficiency in handling complex natural language queries, integrating data from multiple sources, and meticulously presenting references to uphold research integrity and facilitate further exploration. By leveraging LLMs, RAG, and human expertise through a curated library, PyZoBot offers an effective solution to manage information overload and keep pace with rapid scientific advancements. The development of such AI-enhanced tools promises significant improvements in research efficiency and effectiveness across various disciplines.

</details>


### [10] [AI Chatbots as Professional Service Agents: Developing a Professional Identity](https://arxiv.org/abs/2501.14179)

*Wenwen Li, Kangwei Shi, Yidong Chai*

**Main category:** cs.HC

**Keywords:** Large language models, Healthcare communication, Professional identity

**Relevance Score:** 10

**TL;DR:** This paper introduces LAPI, a framework for LLM-based professional service agents in healthcare, focusing on communication consistent with professional identities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for LLM-based AI chatbots to communicate effectively in healthcare, aligning with professional identities.

**Method:** The framework includes a theory-guided task planning process and a pragmatic entropy method for generating ethical responses.

**Key Contributions:**

	1. Introduction of LAPI framework for professional service agents in healthcare
	2. Theory-guided task planning process for managing professional tasks
	3. Pragmatic entropy method for generating professional responses

**Result:** Experiments demonstrate that LAPI outperforms existing methods on metrics such as fluency, empathy, and patient-centricity.

**Limitations:** 

**Conclusion:** LAPI effectively ensures that LLMs communicate in ways that promote patient well-being, enhancing their role as professional service agents.

**Abstract:** With the rapid expansion of large language model (LLM) applications, there is an emerging shift in the role of LLM-based AI chatbots from serving merely as general inquiry tools to acting as professional service agents. However, current studies often overlook a critical aspect of professional service agents: the act of communicating in a manner consistent with their professional identities. This is of particular importance in the healthcare sector, where effective communication with patients is essential for achieving professional goals, such as promoting patient well-being by encouraging healthy behaviors. To bridge this gap, we propose LAPI (LLM-based Agent with a Professional Identity), a novel framework for designing professional service agent tailored for medical question-and-answer (Q\&A) services, ensuring alignment with a specific professional identity. Our method includes a theory-guided task planning process that decomposes complex professional tasks into manageable subtasks aligned with professional objectives and a pragmatic entropy method designed to generate professional and ethical responses with low uncertainty. Experiments on various LLMs show that the proposed approach outperforms baseline methods, including few-shot prompting, chain-of-thought prompting, across key metrics such as fluency, naturalness, empathy, patient-centricity, and ROUGE-L scores. Additionally, the ablation study underscores the contribution of each component to the overall effectiveness of the approach.

</details>


### [11] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)

*Vanessa Figueiredo*

**Main category:** cs.HC

**Keywords:** conversational agents, scaffolding, child interaction, machine learning, education

**Relevance Score:** 8

**TL;DR:** The paper investigates how Brazilian children use conversational agents (CAs) for educational and entertainment purposes, and evaluates structured scaffolds for enhancing these interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the interactions between Brazilian children and conversational agents for schoolwork, discovery, and entertainment, and improve these interactions through structured scaffolds.

**Method:** Two studies were conducted: the first involved a seven-week online investigation with interviews and observations to understand children's interaction patterns; the second involved simulating exchanges with GPT-4o-mini to compare structured and unstructured conversation-tree designs.

**Key Contributions:**

	1. First Cognitive Work Analysis application with Brazilian children
	2. Framework for understanding child-conversational agent information flows
	3. Development of a structured-prompting 'recipe' for scaffolded learning

**Result:** The structured prompting approach yielded better readability, depth, diversity, and coherence in child-CA conversations compared to the unstructured baseline.

**Limitations:** 

**Conclusion:** The findings support the design of scaffolded conversation-trees and personalized profiles to enhance learning through conversational agents, highlighting the benefits of structured prompting.

**Abstract:** This paper presents two studies on how Brazilian children (ages 9--11) use conversational agents (CAs) for schoolwork, discovery, and entertainment, and how structured scaffolds can enhance these interactions. In Study 1, a seven-week online investigation with 23 participants (children, parents, teachers) employed interviews, observations, and Cognitive Work Analysis to map children's information-processing flows, the role of more knowledgeable others, functional uses, contextual goals, and interaction patterns to inform conversation-tree design. We identified three CA functions: School, Discovery, Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support. In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges, comparing conversation-tree recipes based on structured-prompting to an unstructured baseline. Quantitative evaluation of readability, question count/depth/diversity, and coherence revealed gains for the recipe approach. Building on these findings, we offer design recommendations: scaffolded conversation-trees, child-dedicated profiles for personalized context, and caregiver-curated content. Our contributions include the first CWA application with Brazilian children, an empirical framework of child-CA information flows, and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective, scaffolded learning.

</details>


### [12] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)

*Yi-Hao Peng, Dingzeyu Li, Jeffrey P. Bigham, Amy Pavel*

**Main category:** cs.HC

**Keywords:** UI agents, blind and low-vision, user agency, multimodal models, accessibility

**Relevance Score:** 9

**TL;DR:** Morae is a UI agent designed for blind and low-vision users that enhances user agency by pausing at decision points, allowing users to make choices informed by contextual information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility of complex UIs for blind and low-vision users by enhancing their agency during decision-making processes.

**Method:** Morae employs large multimodal models to analyze user queries in conjunction with UI code and screenshots, prompting users for clarification when choices arise.

**Key Contributions:**

	1. Introduction of Morae UI agent for BLV users
	2. Enhanced user agency by defining decision points
	3. Comparison with baseline UI agents showing improved user outcomes

**Result:** In a study, Morae enabled blind and low-vision users to complete more tasks and select options that better matched their preferences compared to baseline agents.

**Limitations:** 

**Conclusion:** Morae exemplifies a mixed-initiative approach, balancing automation with user involvement in decision-making, enhancing accessibility and user experience.

**Abstract:** User interface (UI) agents promise to make inaccessible or complex UIs easier to access for blind and low-vision (BLV) users. However, current UI agents typically perform tasks end-to-end without involving users in critical choices or making them aware of important contextual information, thus reducing user agency. For example, in our field study, a BLV participant asked to buy the cheapest available sparkling water, and the agent automatically chose one from several equally priced options, without mentioning alternative products with different flavors or better ratings. To address this problem, we introduce Morae, a UI agent that automatically identifies decision points during task execution and pauses so that users can make choices. Morae uses large multimodal models to interpret user queries alongside UI code and screenshots, and prompt users for clarification when there is a choice to be made. In a study over real-world web tasks with BLV participants, Morae helped users complete more tasks and select options that better matched their preferences, as compared to baseline agents, including OpenAI Operator. More broadly, this work exemplifies a mixed-initiative approach in which users benefit from the automation of UI agents while being able to express their preferences.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [13] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)

*Kyohoon Jin, Juhwan Choi, Jungmin Yun, Junho Lee, Soojin Jang, Youngbin Kim*

**Main category:** cs.CL

**Keywords:** counterfactual data augmentation, deep learning, bias reduction, out-of-distribution robustness, semantic triples

**Relevance Score:** 8

**TL;DR:** Introducing CoBA, a counterbias data augmentation framework that mitigates spurious correlations in deep learning models by adjusting textual data at the semantic triple level.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Deep learning models often rely on spurious correlations in training data, leading to poor generalization on unseen data.

**Method:** CoBA operates by decomposing text into subject-predicate-object triples and selectively modifying these to disrupt spurious correlations, thus generating counterbias data.

**Key Contributions:**

	1. Introduction of counterbias data augmentation framework CoBA
	2. Operation at the semantic triple level to disrupt spurious correlations
	3. Demonstrated performance improvements and bias reduction in extensive experiments

**Result:** CoBA enhances downstream task performance, reduces biases, and improves out-of-distribution robustness.

**Limitations:** 

**Conclusion:** CoBA is a versatile framework that effectively addresses biases in deep learning models, improving performance and generalization.

**Abstract:** Deep learning models often learn and exploit spurious correlations in training data, using these non-target features to inform their predictions. Such reliance leads to performance degradation and poor generalization on unseen data. To address these limitations, we introduce a more general form of counterfactual data augmentation, termed counterbias data augmentation, which simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and enhances out-of-distribution robustness. We present CoBA: CounterBias Augmentation, a unified framework that operates at the semantic triple level: first decomposing text into subject-predicate-object triples, then selectively modifying these triples to disrupt spurious correlations. By reconstructing the text from these adjusted triples, CoBA generates counterbias data that mitigates spurious patterns. Through extensive experiments, we demonstrate that CoBA not only improves downstream task performance, but also effectively reduces biases and strengthens out-of-distribution resilience, offering a versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [14] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)

*Jan Fillies, Michael Peter Hoffmann, Rebecca Reichel, Roman Salzwedel, Sven Bodemer, Adrian Paschke*

**Main category:** cs.CL

**Keywords:** toxic speech, demographic context, language models, content moderation, age estimates

**Relevance Score:** 7

**TL;DR:** This paper introduces a new large-scale German dataset annotated for toxicity and includes age estimates, revealing different toxic speech patterns across age groups.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of demographic context in existing toxic speech datasets and improve understanding of communication patterns across different age groups online.

**Method:** The study involved creating a dataset of 3,024 human-annotated and 30,024 LLM-annotated comments from Instagram, TikTok, and YouTube, with a focus on predefined toxic keywords for relevance.

**Key Contributions:**

	1. First large-scale German dataset annotated for toxicity with age context
	2. Combined human and LLM annotation for enhanced dataset quality
	3. Revealed age-based differences in toxic speech patterns

**Result:** The dataset, which identified 16.7% of comments as problematic, shows that younger users use more expressive language while older users often engage in disinformation and devaluation.

**Limitations:** 

**Conclusion:** This resource is significant for studying linguistic variation across demographics and aids in developing more equitable content moderation systems.

**Abstract:** A lack of demographic context in existing toxic speech datasets limits our understanding of how different age groups communicate online. In collaboration with funk, a German public service content network, this research introduces the first large-scale German dataset annotated for toxicity and enriched with platform-provided age estimates. The dataset includes 3,024 human-annotated and 30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube. To ensure relevance, comments were consolidated using predefined toxic keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline combined human expertise with state-of-the-art language models, identifying key categories such as insults, disinformation, and criticism of broadcasting fees. The dataset reveals age-based differences in toxic speech patterns, with younger users favoring expressive language and older users more often engaging in disinformation and devaluation. This resource provides new opportunities for studying linguistic variation across demographics and supports the development of more equitable and age-aware content moderation systems.

</details>


### [15] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)

*Parul Awasthy, Aashka Trivedi, Yulong Li, Meet Doshi, Riyaz Bhat, Vignesh P, Vishwajeet Kumar, Yushu Yang, Bhavani Iyer, Abraham Daniels, Rudra Murthy, Ken Barker, Martin Franz, Madison Lee, Todd Ward, Salim Roukos, David Cox, Luis Lastras, Jaydeep Sen, Radu Florian*

**Main category:** cs.CL

**Keywords:** embedding models, dense retrieval, machine learning, enterprise applications, open-source

**Relevance Score:** 6

**TL;DR:** Introduction of Granite Embedding R2 models for dense retrieval applications with substantial improvements in performance and versatility.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide high-performance embedding models for enterprise-scale dense retrieval applications, emphasizing speed and accuracy in diverse retrieval domains.

**Method:** Release of bi-encoder and cross-encoder architectures including a 22-layer retriever model and a 12-layer efficient counterpart, trained on enterprise-specific data.

**Key Contributions:**

	1. Introduction of Granite R2 models with significant performance improvements.
	2. Expansion of context length to 8,192 tokens.
	3. Commercially available models under Apache 2.0 license for unrestricted use.

**Result:** Granite R2 models show substantial performance improvements including 16x expanded context length, speed advantages of 19-44%, and superior accuracy across various benchmarks and real-world use cases.

**Limitations:** 

**Conclusion:** Granite R2 models set new performance standards for open-source embedding models, ensuring organizations have the necessary tools for critical data retrieval tasks.

**Abstract:** We introduce the Granite Embedding R2 models, a comprehensive family of high-performance English encoder-based embedding models engineered for enterprise-scale dense retrieval applications. Building upon our first-generation release, these models deliver substantial improvements, including 16x expanded context length (8,192 tokens), state-of-the-art performance across diverse retrieval domains - text, code, long-document search, multi-turn conversational, and tabular data - and measurable speed advantages of 19-44\% over leading competitors while maintaining superior accuracy. Our release encompasses both bi-encoder and cross-encoder architectures, featuring a highly effective 22-layer retriever model and its efficient 12-layer counterpart, alongside a high-quality reranker model, all trained exclusively on enterprise-appropriate data with comprehensive governance oversight. The models demonstrate exceptional versatility across standard benchmarks, IBM-developed evaluation suites, and real-world enterprise use cases, establishing new performance standards for open-source embedding models. In an era where retrieval speed and accuracy are paramount for competitive advantage, the Granite R2 models deliver a compelling combination of cutting-edge performance, enterprise-ready licensing, and transparent data provenance that organizations require for mission-critical deployments. All models are publicly available under the Apache 2.0 license at https://huggingface.co/collections/ibm-granite, enabling unrestricted research and commercial use.

</details>


### [16] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)

*Zezhong Jin, Shubhang Desai, Xu Chen, Biyi Fang, Zhuoyi Huang, Zhe Li, Chong-Xin Gan, Xiao Tu, Man-Wai Mak, Yan Lu, Shujie Liu*

**Main category:** cs.CL

**Keywords:** Transformer, ink generation, handwriting synthesis

**Relevance Score:** 4

**TL;DR:** TrInk is a Transformer-based model designed for ink generation that significantly improves handwriting legibility through enhanced alignment techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to improve the quality of generated handwriting by effectively capturing global dependencies in ink generation.

**Method:** The model uses scaled positional embeddings and a Gaussian memory mask in the cross-attention module to align input text with generated stroke points.

**Key Contributions:**

	1. Introduction of scaled positional embeddings
	2. Implementation of a Gaussian memory mask
	3. Comprehensive evaluation pipelines for handwriting quality assessment.

**Result:** TrInk achieves a 35.56% reduction in character error rate (CER) and a 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods.

**Limitations:** 

**Conclusion:** The proposed methodology and model significantly enhance handwriting generation, as demonstrated by the performance metrics.

**Abstract:** In this paper, we propose TrInk, a Transformer-based model for ink generation, which effectively captures global dependencies. To better facilitate the alignment between the input text and generated stroke points, we introduce scaled positional embeddings and a Gaussian memory mask in the cross-attention module. Additionally, we design both subjective and objective evaluation pipelines to comprehensively assess the legibility and style consistency of the generated handwriting. Experiments demonstrate that our Transformer-based model achieves a 35.56\% reduction in character error rate (CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset compared to previous methods. We provide an demo page with handwriting samples from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [17] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)

*Yoshiki Takenami, Yin Jou Huang, Yugo Murawaki, Chenhui Chu*

**Main category:** cs.CL

**Keywords:** Cognitive Biases, LLM, Anchoring Effect, Price Negotiations, Reasoning Models

**Relevance Score:** 8

**TL;DR:** This paper examines the anchoring effect in LLMs during price negotiations, revealing that LLMs exhibit cognitive biases similar to humans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding cognitive biases in LLMs is crucial for their reliable applications in real-world scenarios.

**Method:** Experiments involved seller LLM agents negotiating prices while applying the anchoring effect, evaluated through objective and subjective metrics.

**Key Contributions:**

	1. Investigated cognitive biases in LLMs
	2. Showed LLMs are influenced by the anchoring effect
	3. Found reasoning models are less affected by cognitive biases

**Result:** LLMs were found to be influenced by the anchoring effect, with reasoning models showing less susceptibility.

**Limitations:** No significant correlation found between personality traits and susceptibility to the anchoring effect.

**Conclusion:** The study enhances comprehension of cognitive biases in LLMs, highlighting important aspects for safe and responsible applications.

**Abstract:** Cognitive biases, well-studied in humans, can also be observed in LLMs, affecting their reliability in real-world applications. This paper investigates the anchoring effect in LLM-driven price negotiations. To this end, we instructed seller LLM agents to apply the anchoring effect and evaluated negotiations using not only an objective metric but also a subjective metric. Experimental results show that LLMs are influenced by the anchoring effect like humans. Additionally, we investigated the relationship between the anchoring effect and factors such as reasoning and personality. It was shown that reasoning models are less prone to the anchoring effect, suggesting that the long chain of thought mitigates the effect. However, we found no significant correlation between personality traits and susceptibility to the anchoring effect. These findings contribute to a deeper understanding of cognitive biases in LLMs and to the realization of safe and responsible application of LLMs in society.

</details>


### [18] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)

*Samrajnee Ghosh, Naman Agarwal, Hemanshu Garg, Chinmay Mittal, Mausam, Parag Singla*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Perception tasks, Dataset Percept-V

**Relevance Score:** 8

**TL;DR:** This paper introduces the Percept-V dataset, assessing Multimodal Large Language Models' (MLLMs) performance in basic perception tasks with program-generated images.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the reasoning abilities of MLLMs in simple perception tasks due to limited prior experiments in this area.

**Method:** The study introduces the Percept-V dataset with 7200 generated images across 30 categories, testing MLLMs like GPT-4o, Gemini, and Claude on various perception tasks.

**Key Contributions:**

	1. Introduction of the Percept-V dataset for perception task evaluation
	2. Assessment of MLLMs' performance on basic visual perception
	3. Identification of varying difficulty levels among cognitive skills tested

**Result:** MLLMs showed a significant drop in performance as task complexity increased, contradicting their previously established competence in complex tasks.

**Limitations:** Limited to simple visual perception tasks; results may not generalize to more complex reasoning or other modalities.

**Conclusion:** The results demonstrate that MLLMs struggle with basic perception tasks more than expected, revealing specific cognitive skills that are more challenging for them.

**Abstract:** The reasoning abilities of Multimodal Large Language Models (MLLMs) have garnered a lot of attention in recent times, with advances made in frontiers like coding, mathematics, and science. However, very limited experiments have been done to assess their performance in simple perception tasks performed over uncontaminated, generated images containing basic shapes and structures. To address this issue, the paper introduces a dataset, Percept-V, containing a total of 7200 program-generated images equally divided into 30 categories, each testing a combination of visual perception skills. Unlike previously proposed datasets, Percept-V comprises very basic tasks of varying complexity that test the perception abilities of MLLMs. This dataset is then tested on state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their performance. Contrary to the evidence that MLLMs excel in many complex tasks, our experiments show a significant drop in the models' performance with increasing problem complexity across all categories. An analysis of the performances also reveals that the tested MLLMs exhibit a similar trend in accuracy across categories, testing a particular cognitive skill and find some skills to be more difficult than others.

</details>


### [19] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)

*Sarfaroz Yunusov, Kaige Chen, Kazi Nishat Anwar, Ali Emami*

**Main category:** cs.CL

**Keywords:** Large Language Models, personality traits, user interaction, collaborative tasks, GPT-4, Claude 3.5

**Relevance Score:** 9

**TL;DR:** Study evaluates how personality traits influence user preferences for LLMs through collaborative tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how different personality traits affect user interactions with LLMs in collaborative workflows.

**Method:** Study with 32 participants across four Keirsey personality types, assessing their interactions with GPT-4 and Claude 3.5 on tasks like data analysis and creative writing.

**Key Contributions:**

	1. Demonstrates personality-driven preferences for LLMs in collaborative settings.
	2. Highlights the importance of accounting for user personality in LLM evaluations.
	3. Shows the commonality of helpfulness ratings despite varied preferences.

**Result:** Rationals preferred GPT-4 for goal-oriented tasks; idealists liked Claude 3.5 for creativity. Preferences varied by personality type and task.

**Limitations:** 

**Conclusion:** Personality-based preferences reveal insights into LLM utility that traditional evaluations overlook.

**Abstract:** As Large Language Models (LLMs) increasingly integrate into everyday workflows, where users shape outcomes through multi-turn collaboration, a critical question emerges: do users with different personality traits systematically prefer certain LLMs over others? We conducted a study with 32 participants evenly distributed across four Keirsey personality types, evaluating their interactions with GPT-4 and Claude 3.5 across four collaborative tasks: data analysis, creative writing, information retrieval, and writing assistance. Results revealed significant personality-driven preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented tasks, while idealists favored Claude 3.5, especially for creative and analytical tasks. Other personality types showed task-dependent preferences. Sentiment analysis of qualitative feedback confirmed these patterns. Notably, aggregate helpfulness ratings were similar across models, showing how personality-based analysis reveals LLM differences that traditional evaluations miss.

</details>


### [20] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)

*Ming Hu, Chenglong Ma, Wei Li, Wanghan Xu, Jiamin Wu, Jucheng Hu, Tianbin Li, Guohang Zhuang, Jiaqi Liu, Yingzhou Lu, Ying Chen, Chaoyang Zhang, Cheng Tan, Jie Ying, Guocheng Wu, Shujian Gao, Pengcheng Chen, Jiashi Lin, Haitao Wu, Lulu Chen, Fengxiang Wang, Yuanyuan Zhang, Xiangyu Zhao, Feilong Tang, Encheng Su, Junzhi Ning, Xinyao Liu, Ye Du, Changkai Ji, Cheng Tang, Huihui Xu, Ziyang Chen, Ziyan Huang, Jiyao Liu, Pengfei Jiang, Yizhou Wang, Chen Tang, Jianyu Wu, Yuchen Ren, Siyuan Yan, Zhonghua Wang, Zhongxing Xu, Shiyan Su, Shangquan Sun, Runkai Zhao, Zhisheng Zhang, Yu Liu, Fudi Wang, Yuanfeng Ji, Yanzhou Su, Hongming Shan, Chunmei Feng, Jiahao Xu, Jiangtao Yan, Wenhao Tang, Diping Song, Lihao Liu, Yanyan Huang, Lequan Yu, Bin Fu, Shujun Wang, Xiaomeng Li, Xiaowei Hu, Yun Gu, Ben Fei, Zhongying Deng, Benyou Wang, Yuewen Cao, Minjie Shen, Haodong Duan, Jie Xu, Yirong Chen, Fang Yan, Hongxia Hao, Jielan Li, Jiajun Du, Yanbo Wang, Imran Razzak, Chi Zhang, Lijun Wu, Conghui He, Zhaohui Lu, Jinhai Huang, Yihao Liu, Fenghua Ling, Yuqiang Li, Aoran Wang, Qihao Zheng, Nanqing Dong, Tianfan Fu, Dongzhan Zhou, Yan Lu, Wenlong Zhang, Jin Ye, Jianfei Cai, Wanli Ouyang, Yu Qiao, Zongyuan Ge, Shixiang Tang, Junjun He, Chunfeng Song, Lei Bai, Bowen Zhou*

**Main category:** cs.CL

**Keywords:** Scientific Large Language Models, Data-centric AI, Scientific data representation

**Relevance Score:** 9

**TL;DR:** This survey reviews the development of Scientific Large Language Models (Sci-LLMs) and their interaction with scientific data, proposing a unified taxonomy and analyzing model demands, benchmark datasets, and emerging solutions for improving scientific data representation and validation.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the transformative impact of Sci-LLMs on knowledge representation in scientific research and aims to understand the co-evolution of these models with their complex underlying data.

**Method:** The survey formulates a unified taxonomy of scientific data and reviews recent Sci-LLMs across disciplines, alongside an analysis of pre-/post-training datasets and benchmark assessments.

**Key Contributions:**

	1. Unified taxonomy of scientific data related to Sci-LLMs
	2. Comprehensive review of Sci-LLMs and their unique demands
	3. Proposed shift toward closed-loop systems for continuous knowledge evolution

**Result:** The analysis reveals specific challenges in scientific data, highlighting the need for representations that preserve domain invariance and enable cross-modal reasoning, along with issues in data development and validation.

**Limitations:** 

**Conclusion:** The work advocates for a shift toward closed-loop systems with autonomous agents based on Sci-LLMs that actively contribute to scientific knowledge, thereby defining a roadmap for building trustworthy AI systems.

**Abstract:** Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.

</details>


### [21] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)

*Muskan Saraf, Sajjad Rezvani Boroujeni, Justin Beaudry, Hossein Abedi, Tom Bush*

**Main category:** cs.CL

**Keywords:** bias, large language models, evaluation, benchmarking, HCI

**Relevance Score:** 7

**TL;DR:** This study investigates bias in evaluations made by LLMs (ChatGPT, Gemini, and Claude), showing that model identity significantly affects judgment outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address biases in large language model evaluations and improve the fairness of benchmarking protocols.

**Method:** The study examined self- and cross-model evaluations under different labeling scenarios using blog posts generated by the models, comparing preference votes and quality ratings among them.

**Key Contributions:**

	1. Identification of bias in LLM evaluations based on model identity
	2. Effect of labeling on performance rankings
	3. Recommendation for blind or multimodel evaluation protocols

**Result:** Model identity impacts evaluation scores significantly; Claude's label increased scores, whereas Gemini's label decreased them, with notable effects under false-label conditions.

**Limitations:** The study primarily focuses on three models and a specific set of evaluation metrics; results may not generalize to other models or contexts.

**Conclusion:** The findings highlight the need for blind evaluation methods in LLM benchmarking to mitigate bias stemming from perceived model identity.

**Abstract:** Large language models (LLMs) are increasingly used to evaluate outputs, yet their judgments may be influenced. This study examines bias in self- and cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions: no labels, true labels, and two false-label scenarios. Blog posts authored by each model were evaluated by all three using both overall preference voting and quality ratings for Coherence, Informativeness, and Conciseness, with all scores expressed as percentages for direct comparison. Results reveal striking asymmetries: the "Claude" label consistently boosts scores, while the "Gemini" label consistently depresses them, regardless of actual content. False labels frequently reversed rankings, producing shifts of up to 50 percentage points in preference votes and up to 12 percentage points in converted quality ratings. Gemini's self-scores collapsed under true labels, while Claude's self-preference intensified. These findings show that perceived model identity can heavily distort high-level judgments and subtly influence detailed quality ratings, underscoring the need for blind or multimodel evaluation protocols to ensure fairness in LLM benchmarking.

</details>


### [22] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)

*Deepro Choudhury, Sinead Williamson, Adam Goliński, Ning Miao, Freddie Bickford Smith, Michael Kirchhof, Yizhe Zhang, Tom Rainforth*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bayesian Experimental Design, information gain, conversational agents, adaptive questioning

**Relevance Score:** 9

**TL;DR:** The paper introduces BED-LLM, an approach for enhancing LLMs' capabilities in engaging users through interactive, adaptive questioning based on Bayesian experimental design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve how LLMs gather information from users by implementing a structured methodology that supports multi-turn interactions.

**Method:** The BED-LLM framework uses sequential Bayesian experimental design to select questions that maximize expected information gain (EIG) from user responses.

**Key Contributions:**

	1. Development of the BED-LLM framework for LLMs
	2. Innovative estimator for expected information gain (EIG)
	3. Method for generating candidate queries based on previous responses

**Result:** BED-LLM demonstrated significant performance improvements in tasks like the 20-questions game and user preference inference over existing methods.

**Limitations:** 

**Conclusion:** The approach provides a robust mechanism for LLMs to interactively gather information, outperforming direct prompting and other adaptive strategies.

**Abstract:** We propose a general-purpose approach for improving the ability of Large Language Models (LLMs) to intelligently and adaptively gather information from a user or other external source using the framework of sequential Bayesian experimental design (BED). This enables LLMs to act as effective multi-turn conversational agents and interactively interface with external environments. Our approach, which we call BED-LLM (Bayesian Experimental Design with Large Language Models), is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously. We show how this EIG can be formulated in a principled way using a probabilistic model derived from the LLM's belief distribution and provide detailed insights into key decisions in its construction. Further key to the success of BED-LLM are a number of specific innovations, such as a carefully designed estimator for the EIG, not solely relying on in-context updates for conditioning on previous responses, and a targeted strategy for proposing candidate queries. We find that BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20-questions game and using the LLM to actively infer user preferences, compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [23] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)

*Arash Ahmadi, Sarah Sharif, Yaser Banad*

**Main category:** cs.CL

**Keywords:** Aviation Safety, Reinforcement Learning, Language Models, Human Factors Analysis, Artificial Intelligence

**Relevance Score:** 5

**TL;DR:** This paper presents an automated classification framework using Reinforcement Learning to improve aviation safety analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the limitations of traditional HFACS methods in scalability and consistency for aviation accident analysis.

**Method:** An automated HFACS classification framework is proposed that utilizes Reinforcement Learning with Group Relative Policy Optimization to fine-tune a Llama-3.1 8B language model.

**Key Contributions:**

	1. Development of a novel automated HFACS classification framework using Reinforcement Learning.
	2. Introduction of a multi-component reward system for aviation safety analysis.
	3. Proposed exact match accuracy as a new benchmarking methodology for evaluating language models.

**Result:** The GRPO-optimized model achieved a 350% increase in exact match accuracy and improved partial match accuracy of 0.8800, outperforming state-of-the-art LLMs.

**Limitations:** The focus on aviation safety may limit the generalizability of the findings to other domains.

**Conclusion:** Optimizing smaller, domain-specific models can enhance efficiency and performance for critical safety analysis on edge devices.

**Abstract:** Analyzing the human factors behind aviation accidents is crucial for preventing future incidents, yet traditional methods using the Human Factors Analysis and Classification System (HFACS) are limited by scalability and consistency. To address this, we introduce an automated HFACS classification framework for aviation safety analysis that utilizes Reinforcement Learning with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B language model. Our approach incorporates a multi-component reward system tailored for aviation safety analysis and integrates synthetic data generation to overcome class imbalance in accident datasets. The resulting GRPO-optimized model achieved noticeable performance gains, including a 350% increase in exact match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy of 0.8800. Significantly, our specialized model outperforms state-of-the-art LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key metrics. This research also proposes exact match accuracy in multi-label HFACS classification problem as a new benchmarking methodology to evaluate the advanced reasoning capabilities of language models. Ultimately, our work validates that smaller, domain-optimized models can provide a computationally efficient and better solution for critical safety analysis. This approach makes powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [24] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)

*Han Yang, Jian Lan, Yihong Liu, Hinrich Schütze, Thomas Seidl*

**Main category:** cs.CL

**Keywords:** multilingual, orthographic attacks, pixel-based language model

**Relevance Score:** 8

**TL;DR:** This paper introduces a pixel-based generative language model to enhance robustness against orthographic attacks in autoregressive models by using image representations of words.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Autoregressive language models face vulnerabilities to orthographic attacks due to out-of-vocabulary issues from subword tokenizers. The goal is to solve this problem to improve model robustness and multilingual compatibility.

**Method:** The proposed model replaces traditional text-based embeddings with pixel-based representations by rendering words as images. This enables the model to handle orthographic noise better and improves multilingual processing.

**Key Contributions:**

	1. Introduction of pixel-based embeddings for language models
	2. Demonstrated resilience to orthographic noise
	3. Evaluation on multilingual datasets showing improved performance

**Result:** The model was evaluated on the multilingual LAMBADA dataset, WMT24 dataset, and SST-2 benchmark, showing resilience to orthographic noise and effectiveness across diverse writing systems.

**Limitations:** 

**Conclusion:** The pixel-based generative language model provides a promising solution for enhancing the robustness of language models against orthographic attacks while supporting multilingual text.

**Abstract:** Autoregressive language models are vulnerable to orthographic attacks, where input text is perturbed with characters from multilingual alphabets, leading to substantial performance degradation. This vulnerability primarily stems from the out-of-vocabulary issue inherent in subword tokenizers and their embeddings. To address this limitation, we propose a pixel-based generative language model that replaces the text-based embeddings with pixel-based representations by rendering words as individual images. This design provides stronger robustness to noisy inputs, while an extension of compatibility to multilingual text across diverse writing systems. We evaluate the proposed method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2 benchmark, demonstrating both its resilience to orthographic noise and its effectiveness in multilingual settings.

</details>


### [25] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)

*Yurie Koga, Shunsuke Kando, Yusuke Miyao*

**Main category:** cs.CL

**Keywords:** Critical Period, self-supervised speech models, language acquisition, phonological performance, child-directed speech

**Relevance Score:** 5

**TL;DR:** The paper explores Critical Period effects in self-supervised speech models, finding no evidence of phonological acquisition issues typically seen in human language acquisition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the existence of Critical Period effects in self-supervised speech models, which have not been extensively studied despite their relevance to human language acquisition.

**Method:** The authors trained self-supervised speech models (S3Ms) with different second language (L2) training onsets and first language (L1) training offsets on child-directed speech, evaluating their phone discrimination performance.

**Key Contributions:**

	1. First investigation of Critical Period effects in self-supervised speech models
	2. Demonstrated that S3Ms do not exhibit significant phonological acquisition challenges
	3. Provided insights into model performance based on training onset and offset conditions

**Result:** The study concluded that S3Ms do not show significant evidence for Critical Period effects, with delayed L2 exposure leading to better performance and delayed L1 exposure causing L1 forgetting.

**Limitations:** The findings are specific to phone discrimination performance and may not generalize to all aspects of language acquisition.

**Conclusion:** The findings suggest that self-supervised speech models may not reflect the same challenges in language acquisition faced by humans regarding Critical Period effects.

**Abstract:** This paper investigates whether the Critical Period (CP) effects in human language acquisition are observed in self-supervised speech models (S3Ms). CP effects refer to greater difficulty in acquiring a second language (L2) with delayed L2 exposure onset, and greater retention of their first language (L1) with delayed L1 exposure offset. While previous work has studied these effects using textual language models, their presence in speech models remains underexplored despite the central role of spoken language in human language acquisition. We train S3Ms with varying L2 training onsets and L1 training offsets on child-directed speech and evaluate their phone discrimination performance. We find that S3Ms do not exhibit clear evidence of either CP effects in terms of phonological acquisition. Notably, models with delayed L2 exposure onset tend to perform better on L2 and delayed L1 exposure offset leads to L1 forgetting.

</details>


### [26] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)

*Weizhi Gao, Xiaorui Liu, Feiyi Wang, Dan Lu, Junqi Yin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination Detection, Decoding Memory Pipeline, Self-Consistency, Multi-Response Generation

**Relevance Score:** 8

**TL;DR:** This paper proposes a Decoding Memory Pipeline (DMP) to improve generation efficiency in large language models by reducing redundancy in self-consistency methods.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for hallucination detection in LLMs struggle with efficiency and accuracy, particularly in sentence-level generation.

**Method:** The authors conducted a study to identify redundancy in self-consistency methods, focusing on shared prefix tokens. They introduced a DMP that uses selective inference and annealed decoding to improve efficiency.

**Key Contributions:**

	1. Introduction of a Decoding Memory Pipeline (DMP) for LLMs
	2. Identification of redundancy in self-consistency methods
	3. Demonstration of significant efficiency improvements in generation

**Result:** The DMP achieves up to a 3x speedup in multi-response generation while maintaining AUROC performance, indicating improved efficiency without sacrificing accuracy.

**Limitations:** The approach needs further validation in diverse real-world applications and different model architectures.

**Conclusion:** The proposed DMP can enhance LLM performance in multi-response generation and has potential applications in alignment and reasoning tasks.

**Abstract:** Large language models (LLMs) have demonstrated impressive performance in both research and real-world applications, but they still struggle with hallucination. Existing hallucination detection methods often perform poorly on sentence-level generation or rely heavily on domain-specific knowledge. While self-consistency approaches help address these limitations, they incur high computational costs due to repeated generation. In this paper, we conduct the first study on identifying redundancy in self-consistency methods, manifested as shared prefix tokens across generations, and observe that non-exact-answer tokens contribute minimally to the semantic content. Based on these insights, we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation through selective inference and annealed decoding. Being orthogonal to the model, dataset, decoding strategy, and self-consistency baseline, our DMP consistently improves the efficiency of multi-response generation and holds promise for extension to alignment and reasoning tasks. Extensive experiments show that our method achieves up to a 3x speedup without sacrificing AUROC performance.

</details>


### [27] [A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement](https://arxiv.org/abs/2411.04090)

*Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz*

**Main category:** cs.CL

**Keywords:** content moderation, toxic content, multitask learning, uncertainty estimation, Conformal Prediction

**Relevance Score:** 7

**TL;DR:** We present a novel content moderation framework that incorporates annotation disagreement as a valuable signal, using multitask learning and Conformal Prediction techniques to enhance model performance and review processes.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** The subjective nature of toxicity perception in content moderation leads to significant disagreement, which is typically disregarded. We seek to utilize this disagreement as a signal to improve moderation processes.

**Method:** Our methodology involves using multitask learning where toxicity classification is the primary task and annotation disagreement is an auxiliary task. We also implement uncertainty estimation via Conformal Prediction to manage annotation ambiguity and model uncertainty.

**Key Contributions:**

	1. Integration of annotation disagreement as a signal in content moderation
	2. Multitask learning framework for better model performance
	3. Application of Conformal Prediction for uncertainty estimation

**Result:** Our approach improves calibration, enhances model performance, and increases parameter efficiency, leading to a superior review process compared to traditional single-task methods.

**Limitations:** The system may rely on the quality of the initial annotated dataset and how well disagreement signals are interpreted by moderators.

**Conclusion:** By acknowledging and incorporating annotation disagreement, our framework provides a more flexible and effective approach to content moderation.

**Abstract:** Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content,an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model's inherent uncertainty in predicting toxicity and disagreement.The framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.

</details>


### [28] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)

*Daria Kryvosheieva, Saba Sturua, Michael Günther, Scott Martens, Han Xiao*

**Main category:** cs.CL

**Keywords:** code embeddings, natural language queries, technical question-answering, semantically similar code snippets, programming languages

**Relevance Score:** 6

**TL;DR:** Introduction of jina-code-embeddings, a model suite for code retrieval and question-answering based on natural language queries.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To improve code retrieval and semantic similarity identification across programming languages using embeddings during technical question-answering.

**Method:** An autoregressive backbone pre-trained on both text and code is utilized, generating embeddings through last-token pooling.

**Key Contributions:**

	1. Introduction of jina-code-embeddings for enhanced code retrieval
	2. Utilization of last-token pooling for embedding generation
	3. Demonstration of state-of-the-art performance with small models

**Result:** Achieves state-of-the-art performance with a relatively small model size, demonstrating the effectiveness of the proposed method for code embeddings.

**Limitations:** 

**Conclusion:** The approach to constructing code embedding models is validated due to its performance outcomes with limited model size.

**Abstract:** jina-code-embeddings is a novel code embedding model suite designed to retrieve code from natural language queries, perform technical question-answering, and identify semantically similar code snippets across programming languages. It makes innovative use of an autoregressive backbone pre-trained on both text and code, generating embeddings via last-token pooling. We outline the training recipe and demonstrate state-of-the-art performance despite the relatively small size of the models, validating this approach to code embedding model construction.

</details>


### [29] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)

*João Guilherme Alves Santos, Giovana Kerche Bonás, Thales Sales Almeida*

**Main category:** cs.CL

**Keywords:** Large Language Models, BLUEX, dataset, image captions, evaluation methods

**Relevance Score:** 9

**TL;DR:** This paper updates the BLUEX dataset to improve evaluation methods for Large Language Models (LLMs), enhancing accessibility and data contamination studies in multilingual contexts.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** There is an increasing need for robust evaluation methods for LLMs, particularly in multilingual and non-English settings, due to their growing capabilities.

**Method:** An updated version of the BLUEX dataset is presented, which includes 2024-2025 exams and automatically generated image captions to facilitate accessibility and evaluation analysis.

**Key Contributions:**

	1. Updated BLUEX dataset with new exams and image captions
	2. Increased number of usable questions more than double the original
	3. Evaluated LLMs' ability to leverage visual context through captions

**Result:** The updated dataset contains 1,422 usable questions, significantly surpassing the original count, and shows that captioning strategies improve text-only model accessibility by over 40%.

**Limitations:** 

**Conclusion:** The enhancements to the BLUEX dataset allow for better evaluation of LLMs, particularly in leveraging visual context through captions, thereby supporting more effective studies on data contamination in pretraining.

**Abstract:** With the growing capabilities of Large Language Models (LLMs), there is an increasing need for robust evaluation methods, especially in multilingual and non-English contexts. We present an updated version of the BLUEX dataset, now including 2024-2025 exams and automatically generated image captions using state-of-the-art models, enhancing its relevance for data contamination studies in LLM pretraining. Captioning strategies increase accessibility to text-only models by more than 40%, producing 1,422 usable questions, more than doubling the number in the original BLUEX. We evaluated commercial and open-source LLMs and their ability to leverage visual context through captions.

</details>


### [30] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)

*Shubham Sharma, Sneha Tuli, Narendra Badam*

**Main category:** cs.CL

**Keywords:** Large Language Models, GPT-4o, DeepSeek-V3-0324, Machine Learning, AI applications

**Relevance Score:** 9

**TL;DR:** This survey reviews 16 challenges in developing and using Large Language Models (LLMs) and compares two models: OpenAI's GPT-4o and DeepSeek-V3-0324, highlighting their trade-offs and applications.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the complexities involved in the development and deployment of Large Language Models (LLMs) and to provide insights to AI researchers and developers.

**Method:** A survey of 16 key challenges faced when building LLMs is presented, along with a comparison between OpenAI's closed-source model (GPT-4o) and an open-source Mixture-of-Experts model (DeepSeek-V3-0324).

**Key Contributions:**

	1. Review of 16 challenges in LLM development
	2. Comparative analysis of GPT-4o and DeepSeek-V3-0324
	3. Insights on LLM applications in diverse domains

**Result:** The article outlines the trade-offs between closed source models, which offer robust safety and fine-tuned reliability, and open source models, known for their efficiency and adaptability.

**Limitations:** 

**Conclusion:** The paper provides valuable guidance for AI professionals on the current capabilities, limitations, and best practices of LLMs across various applications, including healthcare and education.

**Abstract:** Large Language Models (LLMs) are transforming AI across industries, but their development and deployment remain complex. This survey reviews 16 key challenges in building and using LLMs and examines how these challenges are addressed by two state-of-the-art models with unique approaches: OpenAI's closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a large open source Mixture-of-Experts model. Through this comparison, we showcase the trade-offs between closed source models (robust safety, fine-tuned reliability) and open source models (efficiency, adaptability). We also explore LLM applications across different domains (from chatbots and coding tools to healthcare and education), highlighting which model attributes are best suited for each use case. This article aims to guide AI researchers, developers, and decision-makers in understanding current LLM capabilities, limitations, and best practices.

</details>


### [31] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)

*Alexandre Kabbach*

**Main category:** cs.CL

**Keywords:** Turing Test, Normality, Artificial Intelligence, Human Cognition, Language Models

**Relevance Score:** 4

**TL;DR:** This paper revisits the Turing test, arguing it targets normal human intelligence as interpreted through a statistical lens, and claims that large language models aim for exceptional intelligence, which may hinder their ability to pass the test.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a deeper understanding of the Turing test through the concept of normality and its implications for artificial intelligence assessment.

**Method:** The paper analyzes the Turing test from a statistical perspective, highlighting how it measures normal intelligence via judgments from a jury of non-expert human interrogators.

**Key Contributions:**

	1. Reinterprets the Turing test through the statistical concept of normality.
	2. Argues that large language models like ChatGPT may not pass the Turing test due to their focus on exceptional intelligence.
	3. Raises questions about the relationship between normality and human cognition in the context of AI.

**Result:** It concludes that large language models, such as ChatGPT, are designed for exceptional intelligence and likely won't pass the Turing test because they don't embody normal human intelligence characteristics.

**Limitations:** 

**Conclusion:** The findings suggest a fundamental question on whether the human mind can be understood as equivalent to a normal/average mind, challenging the conceptual fundamentals of the normalist paradigm.

**Abstract:** This paper proposes to revisit the Turing test through the concept of normality. Its core argument is that the statistical interpretation of the normal--understood as the average both in the normative and mathematical sense of the term--proves useful for understanding the Turing test in at least two ways. First, in the sense that the Turing test targets normal/average rather than exceptional human intelligence, so that successfully passing the test requires building machines that "make mistakes" and display imperfect behavior just like normal/average humans. Second, in the sense that the Turing test is a statistical test where judgments of intelligence are never carried out by a single "average" judge (understood as non-expert) but always by a full jury. As such, the notion of "average human interrogator" that Turing talks about in his original paper should be understood primarily as referring to a mathematical abstraction made of the normalized aggregate of individual judgments of multiple judges. In short, this paper argues that the Turing test is a test of normal intelligence as assessed by a normal judge characterizing the average judgment of a pool of human interrogators. Its conclusions are twofold. First, it argues that large language models such as ChatGPT are unlikely to pass the Turing test as those models precisely target exceptional rather than normal/average human intelligence. As such, they constitute models of what it proposes to call artificial smartness rather than artificial intelligence per se. Second, it argues that the core question of whether the Turing test can contribute anything to the understanding of human cognition is that of whether the human mind is really reducible to the normal/average mind--a question which largely extends beyond the Turing test itself and questions the conceptual underpinnings of the normalist paradigm it belongs to.

</details>


### [32] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)

*Tanguy Herserant, Vincent Guigue*

**Main category:** cs.CL

**Keywords:** automatic text summarization, evaluation metrics, reproducibility, LLM, methodological standardization

**Relevance Score:** 8

**TL;DR:** This paper examines reproducibility issues in automatic text summarization evaluation, revealing discrepancies between existing literature and experimental findings. It introduces a framework for transparency and makes recommendations for better evaluation practices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the reproducibility challenges in automatic text summarization evaluation and highlight discrepancies in reported performance metrics.

**Method:** Experiments were conducted across six summarization evaluation metrics, including classical approaches (ROUGE) and recent LLM-based methods (G-Eval, SEval-Ex), using an open-source framework applied to the SummEval dataset.

**Key Contributions:**

	1. Introduction of a unified, open-source evaluation framework
	2. Identification of discrepancies in summarization metrics performance
	3. Recommendations for improved evaluation methodologies

**Result:** The study found significant discrepancies between literature reports and experimental results, revealing that metrics aligning with human judgments are computationally intensive and less stable.

**Limitations:** The study emphasizes the randomness and technical dependencies of LLM-based metrics, as well as their limited reproducibility.

**Conclusion:** The paper calls for more robust evaluation protocols and methodological standardization to improve reliability in summarization assessments.

**Abstract:** This paper investigates reproducibility challenges in automatic text summarization evaluation. Based on experiments conducted across six representative metrics ranging from classical approaches like ROUGE to recent LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies between reported performances in the literature and those observed in our experimental setting. We introduce a unified, open-source framework, applied to the SummEval dataset and designed to support fair and transparent comparison of evaluation metrics. Our results reveal a structural trade-off: metrics with the highest alignment with human judgments tend to be computationally intensive and less stable across runs. Beyond comparative analysis, this study highlights key concerns about relying on LLMs for evaluation, stressing their randomness, technical dependencies, and limited reproducibility. We advocate for more robust evaluation protocols including exhaustive documentation and methodological standardization to ensure greater reliability in automatic summarization assessment.

</details>


### [33] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)

*Nils Dycke, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** automatic review generators, faulty research logic, peer review, counterfactual evaluation

**Relevance Score:** 9

**TL;DR:** This paper investigates the capabilities and limitations of automatic review generators (ARGs) in detecting faulty research logic during peer review.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the potential biases and risks to scientific integrity posed by automatic review generators in scholarly peer review.

**Method:** The authors present a fully automated counterfactual evaluation framework to test ARGs on their ability to detect internal inconsistencies in research papers.

**Key Contributions:**

	1. Development of a counterfactual evaluation framework for ARGs
	2. Insights into the limitations of ARGs in identifying faulty research logic
	3. Public release of a dataset and evaluation framework for further research

**Result:** The study found that flaws in research logic do not significantly affect the output reviews of ARGs, which was contrary to the initial expectations.

**Limitations:** 

**Conclusion:** The findings prompt three actionable recommendations for future research and a public release of the counterfactual dataset and evaluation framework.

**Abstract:** Large Language Models (LLMs) have great potential to accelerate and support scholarly peer review and are increasingly used as fully automatic review generators (ARGs). However, potential biases and systematic errors may pose significant risks to scientific integrity; understanding the specific capabilities and limitations of state-of-the-art ARGs is essential. We focus on a core reviewing skill that underpins high-quality peer review: detecting faulty research logic. This involves evaluating the internal consistency between a paper's results, interpretations, and claims. We present a fully automated counterfactual evaluation framework that isolates and tests this skill under controlled conditions. Testing a range of ARG approaches, we find that, contrary to expectation, flaws in research logic have no significant effect on their output reviews. Based on our findings, we derive three actionable recommendations for future work and release our counterfactual dataset and evaluation framework publicly.

</details>


### [34] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)

*Meidan Ding, Jipeng Zhang, Wenxuan Wang, Cheng-Yi Li, Wei-Chieh Fang, Hsin-Yu Wu, Haiqin Zhong, Wenting Chen, Linlin Shen*

**Main category:** cs.CL

**Keywords:** multimodal large language models, medical reward models, benchmarking, clinical decision-making, disease diagnosis

**Relevance Score:** 10

**TL;DR:** This paper introduces Med-RewardBench, the first benchmark for evaluating medical reward models and judges in medical applications, addressing challenges in accurate and context-sensitive responses for disease diagnosis and clinical decision-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable models that can provide clinically relevant and accurate responses in medical applications is critical, yet existing benchmarks fail to address specific clinical requirements, particularly in evaluating medical reward models.

**Method:** Med-RewardBench features a multimodal dataset with 1,026 expert-annotated cases across 13 organ systems and 8 clinical departments, evaluated using a rigorous three-step process to ensure high-quality data.

**Key Contributions:**

	1. Introduction of Med-RewardBench as the first benchmark for medical reward models
	2. A multimodal dataset specifically curated for clinical evaluation
	3. Comprehensive evaluation of existing MLLMs and identification of alignment challenges

**Result:** The evaluation of 32 state-of-the-art multimodal large language models showed significant challenges in aligning outputs with expert judgment, highlighting the need for improved reward models in medicine.

**Limitations:** The dataset may not cover all clinical scenarios, and the evaluation process may require further refinement to ensure robust results across diverse medical contexts.

**Conclusion:** Med-RewardBench establishes a foundational benchmark for future research and development of medical reward models and emphasizes the importance of fine-tuning to enhance performance.

**Abstract:** Multimodal large language models (MLLMs) hold significant potential in medical applications, including disease diagnosis and clinical decision-making. However, these tasks require highly accurate, context-sensitive, and professionally aligned responses, making reliable reward models and judges critical. Despite their importance, medical reward models (MRMs) and judges remain underexplored, with no dedicated benchmarks addressing clinical requirements. Existing benchmarks focus on general MLLM capabilities or evaluate models as solvers, neglecting essential evaluation dimensions like diagnostic accuracy and clinical relevance. To address this, we introduce Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and judges in medical scenarios. Med-RewardBench features a multimodal dataset spanning 13 organ systems and 8 clinical departments, with 1,026 expert-annotated cases. A rigorous three-step process ensures high-quality evaluation data across six clinically critical dimensions. We evaluate 32 state-of-the-art MLLMs, including open-source, proprietary, and medical-specific models, revealing substantial challenges in aligning outputs with expert judgment. Additionally, we develop baseline models that demonstrate substantial performance improvements through fine-tuning.

</details>


### [35] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)

*Yunhao Zhang, Shaonan Wang, Nan Lin, Xinyi Dong, Chong Li, Chengqing Zong*

**Main category:** cs.CL

**Keywords:** semantic representation, large language models, conceptual semantics

**Relevance Score:** 6

**TL;DR:** This paper introduces a Disentangled Continuous Semantic Representation Model (DCSRM) that decomposes word embeddings into interpretable semantic subdimensions and maps them to brain activation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover how meaning is organized in language and the brain by investigating finer conceptual distinctions beyond predefined semantic dimensions.

**Method:** The paper proposes a DCSRM to decompose word embeddings from large language models into multiple sub-embeddings and identifies interpretable semantic subdimensions using voxel-wise encoding models to correlate them with brain activation.

**Key Contributions:**

	1. Introduction of the Disentangled Continuous Semantic Representation Model (DCSRM)
	2. Identification of interpretable semantic subdimensions
	3. Mapping of semantic subdimensions to brain activation using voxel-wise encoding models

**Result:** The identified semantic subdimensions offer a more fine-grained understanding of conceptual meaning, with polarity being a significant factor in their decomposition, supported by neural correlates.

**Limitations:** 

**Conclusion:** The work provides interpretable semantic subdimensions that align with cognitive and neuroscientific evidence, enhancing the understanding of semantic structure in language.

**Abstract:** Understanding the core dimensions of conceptual semantics is fundamental to uncovering how meaning is organized in language and the brain. Existing approaches often rely on predefined semantic dimensions that offer only broad representations, overlooking finer conceptual distinctions. This paper proposes a novel framework to investigate the subdimensions underlying coarse-grained semantic dimensions. Specifically, we introduce a Disentangled Continuous Semantic Representation Model (DCSRM) that decomposes word embeddings from large language models into multiple sub-embeddings, each encoding specific semantic information. Using these sub-embeddings, we identify a set of interpretable semantic subdimensions. To assess their neural plausibility, we apply voxel-wise encoding models to map these subdimensions to brain activation. Our work offers more fine-grained interpretable semantic subdimensions of conceptual meaning. Further analyses reveal that semantic dimensions are structured according to distinct principles, with polarity emerging as a key factor driving their decomposition into subdimensions. The neural correlates of the identified subdimensions support their cognitive and neuroscientific plausibility.

</details>


### [36] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)

*Shariar Kabir, Kevin Esterling, Yue Dong*

**Main category:** cs.CL

**Keywords:** Large Language Models, ideological depth, steerability, Sparse Autoencoders, political representation

**Relevance Score:** 8

**TL;DR:** This paper explores the concept of 'ideological depth' in Large Language Models (LLMs), measuring their steerability and internal political representations using instruction prompting and Sparse Autoencoders.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the ideological leanings of LLMs and their implications for AI behavior.

**Method:** The study employs a dual approach, measuring steerability of LLMs through instruction prompting and activation steering, and analyzing their internal political representations with Sparse Autoencoders.

**Key Contributions:**

	1. Introduces the concept of ideological depth in LLMs.
	2. Demonstrates varying steerability across different LLMs.
	3. Shows that ideological depth impacts reasoning and response fidelity.

**Result:** Findings indicate that steerability varies among models, with some easily shifting viewpoints while others show deeper ideological structures. Ideologically deep models exhibit more political features and consistent reasoning shifts when core features are targeted.

**Limitations:** 

**Conclusion:** Ideological depth is quantifiable in LLMs, offering insights into their political architecture, with implications for ethical AI deployment.

**Abstract:** Large Language Models (LLMs) have demonstrated pronounced ideological leanings, yet the stability and depth of these positions remain poorly understood. Surface-level responses can often be manipulated through simple prompt engineering, calling into question whether they reflect a coherent underlying ideology. This paper investigates the concept of "ideological depth" in LLMs, defined as the robustness and complexity of their internal political representations. We employ a dual approach: first, we measure the "steerability" of two well-known open-source LLMs using instruction prompting and activation steering. We find that while some models can easily switch between liberal and conservative viewpoints, others exhibit resistance or an increased rate of refusal, suggesting a more entrenched ideological structure. Second, we probe the internal mechanisms of these models using Sparse Autoencoders (SAEs). Preliminary analysis reveals that models with lower steerability possess more distinct and abstract ideological features. Our evaluations reveal that one model can contain 7.3x more political features than another model of similar size. This allows targeted ablation of a core political feature in an ideologically "deep" model, leading to consistent, logical shifts in its reasoning across related topics, whereas the same intervention in a "shallow" model results in an increase in refusal outputs. Our findings suggest that ideological depth is a quantifiable property of LLMs and that steerability serves as a valuable window into their latent political architecture.

</details>


### [37] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)

*Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin*

**Main category:** cs.CL

**Keywords:** Small Language Models, Reinforcement Learning, Creative Writing, AI Feedback, Human Evaluation

**Relevance Score:** 7

**TL;DR:** This paper investigates reward strategies to improve creative writing in Small Language Models (SLMs) using a Reinforcement Learning from AI Feedback (RLAIF) framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of current methods for enhancing Small Language Models due to high computational demands and difficulty in achieving novelty.

**Method:** Two AI-driven reward strategies are explored within an RLAIF framework: one uses reward models trained on high-quality preference data, and the other employs a principle-guided LLM-as-a-Judge with an adversarial training scheme.

**Key Contributions:**

	1. Introduction of an AI-driven reward model for creative tasks.
	2. Development of a principle-guided LLM-as-a-Judge for providing rewards.
	3. Demonstration of improved efficiency and quality in generating creative outputs.

**Result:** Both strategies significantly enhance creative output compared to baselines, with the principle-guided approach showing superior generation quality and better training efficiency.

**Limitations:** 

**Conclusion:** The findings suggest that adopting novel reward strategies can lead to scalable and effective improvements in the creative capabilities of Small Language Models, minimizing reliance on human-annotated data.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [38] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)

*Sara B. Coutinho, Rafael M. O. Cruz, Francimaria R. S. Nascimento, George D. C. Cavalcanti*

**Main category:** cs.CL

**Keywords:** machine learning, fact-checking, ensemble methods, classifier diversity, fake news

**Relevance Score:** 7

**TL;DR:** This paper presents a novel method for selecting diverse classifiers for ensemble methods to improve machine learning fact-checking systems against fake news.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Individuals are prone to psychological biases that contribute to spreading fake news, making effective ML-based fact-checking systems crucial for societal health.

**Method:** The proposed approach computes pairwise diversity among classifiers, uses hierarchical clustering to group them, and dynamically selects diverse pools at different levels for ensemble construction while considering performance.

**Key Contributions:**

	1. Novel automatic classifier selection prioritizing diversity and performance
	2. Utilizes hierarchical clustering to organize classifiers
	3. Achieves highest accuracy on multiple datasets compared to existing methods

**Result:** The method outperforms the Elbow heuristic and state-of-the-art baselines in accuracy on two of six datasets tested.

**Limitations:** 

**Conclusion:** The approach effectively improves ensemble classifier diversity and performance, enhancing the robustness of machine learning models for fact-checking against fake news.

**Abstract:** Psychological biases, such as confirmation bias, make individuals particularly vulnerable to believing and spreading fake news on social media, leading to significant consequences in domains such as public health and politics. Machine learning-based fact-checking systems have been widely studied to mitigate this problem. Among them, ensemble methods are particularly effective in combining multiple classifiers to improve robustness. However, their performance heavily depends on the diversity of the constituent classifiers-selecting genuinely diverse models remains a key challenge, especially when models tend to learn redundant patterns. In this work, we propose a novel automatic classifier selection approach that prioritizes diversity, also extended by performance. The method first computes pairwise diversity between classifiers and applies hierarchical clustering to organize them into groups at different levels of granularity. A HierarchySelect then explores these hierarchical levels to select one pool of classifiers per level, each representing a distinct intra-pool diversity. The most diverse pool is identified and selected for ensemble construction from these. The selection process incorporates an evaluation metric reflecting each classifiers's performance to ensure the ensemble also generalises well. We conduct experiments with 40 heterogeneous classifiers across six datasets from different application domains and with varying numbers of classes. Our method is compared against the Elbow heuristic and state-of-the-art baselines. Results show that our approach achieves the highest accuracy on two of six datasets. The implementation details are available on the project's repository: https://github.com/SaraBCoutinho/HSFN .

</details>


### [39] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)

*Aishwarya Mirashi, Ananya Joshi, Raviraj Joshi*

**Main category:** cs.CL

**Keywords:** sentence similarity, Marathi, NLP, fine-tuning, low-resource

**Relevance Score:** 6

**TL;DR:** MahaSTS is a human-annotated dataset for Marathi sentence similarity, paired with a fine-tuned model, MahaSBERT-STS-v2, achieving effective similarity scoring.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a structured and balanced dataset for sentence similarity tasks in low-resource languages like Marathi.

**Method:** The MahaSTS dataset comprises 16,860 Marathi sentence pairs with continuous similarity scores labeled from 0-5. The MahaSBERT model is fine-tuned on this dataset and benchmarked against other models.

**Key Contributions:**

	1. Introduction of a new Marathi sentence similarity dataset (MahaSTS)
	2. Development of a fine-tuned Sentence-BERT model (MahaSBERT-STS-v2)
	3. Benchmarking against other models in low-resource language settings.

**Result:** MahaSBERT-STS-v2 demonstrated improved performance on sentence similarity tasks compared to alternatives such as MahaBERT and IndicBERT.

**Limitations:** 

**Conclusion:** Human-annotated datasets and fine-tuning are crucial for enhancing model performance in low-resource language contexts.

**Abstract:** We present MahaSTS, a human-annotated Sentence Textual Similarity (STS) dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT model optimized for regression-based similarity scoring. The MahaSTS dataset consists of 16,860 Marathi sentence pairs labeled with continuous similarity scores in the range of 0-5. To ensure balanced supervision, the dataset is uniformly distributed across six score-based buckets spanning the full 0-5 range, thus reducing label bias and enhancing model stability. We fine-tune the MahaSBERT model on this dataset and benchmark its performance against other alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments demonstrate that MahaSTS enables effective training for sentence similarity tasks in Marathi, highlighting the impact of human-curated annotations, targeted fine-tuning, and structured supervision in low-resource settings. The dataset and model are publicly shared at https://github.com/l3cube-pune/MarathiNLP

</details>


### [40] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)

*Tobias Deußer, Lorenz Sparrenberg, Armin Berger, Max Hahnbück, Christian Bauckhage, Rafet Sifa*

**Main category:** cs.CL

**Keywords:** text anonymization, privacy protection, Large Language Models, healthcare, evaluation frameworks

**Relevance Score:** 7

**TL;DR:** This survey reviews the current state of text anonymization techniques, emphasizing challenges and advances in the context of sensitive data protection across various sectors, including healthcare and law.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To protect privacy and comply with regulations while ensuring data usability in the presence of sensitive personal information.

**Method:** The paper surveys existing text anonymization techniques, discussing foundational approaches, the impact of Large Language Models, and advanced methodologies for different domains.

**Key Contributions:**

	1. Comprehensive review of text anonymization techniques.
	2. Analysis of Large Language Models as both anonymizers and de-anonymization threats.
	3. Evaluation of metrics and benchmarks for practical anonymization tools.

**Result:** The review highlights trends in anonymization methods, challenges in maintaining privacy-utility balance, and evaluates tools and metrics for effective data anonymization.

**Limitations:** Limited focus on algorithm design; concentrated on practical implications and evaluations.

**Conclusion:** The paper consolidates knowledge in text anonymization and suggests future research directions to address ongoing challenges.

**Abstract:** The proliferation of textual data containing sensitive personal information across various domains requires robust anonymization techniques to protect privacy and comply with regulations, while preserving data usability for diverse and crucial downstream tasks. This survey provides a comprehensive overview of current trends and recent advances in text anonymization techniques. We begin by discussing foundational approaches, primarily centered on Named Entity Recognition, before examining the transformative impact of Large Language Models, detailing their dual role as sophisticated anonymizers and potent de-anonymization threats. The survey further explores domain-specific challenges and tailored solutions in critical sectors such as healthcare, law, finance, and education. We investigate advanced methodologies incorporating formal privacy models and risk-aware frameworks, and address the specialized subfield of authorship anonymization. Additionally, we review evaluation frameworks, comprehensive metrics, benchmarks, and practical toolkits for real-world deployment of anonymization solutions. This review consolidates current knowledge, identifies emerging trends and persistent challenges, including the evolving privacy-utility trade-off, the need to address quasi-identifiers, and the implications of LLM capabilities, and aims to guide future research directions for both academics and practitioners in this field.

</details>


### [41] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)

*Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, data optimization, supervised fine-tuning, human-AI co-evolution, model performance

**Relevance Score:** 9

**TL;DR:** This paper presents Middo, a dynamic data optimization framework for supervised fine-tuning of Large Language Models, enhancing data quality and model performance through continuous adaptation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving data quality for SFT in LLMs through adaptive and evolving methods rather than static dataset curation.

**Method:** Middo employs a self-referential diagnostic module for identifying suboptimal samples and an adaptive optimization engine that enhances these samples while preserving data integrity.

**Key Contributions:**

	1. Introduction of a self-evolving optimization framework for LLM data
	2. Use of tri-axial model signals for sample selection
	3. Demonstration of improved model performance on benchmark datasets

**Result:** Experiments show a consistent average accuracy improvement of 7.15% in LLM performance while maintaining dataset scale.

**Limitations:** 

**Conclusion:** Middo introduces a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models.

**Abstract:** Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our \method consistently enhances the quality of seed data and boosts LLM's performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are coming soon.

</details>


### [42] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)

*Sarfaroz Yunusov, Kaige Chen, Kazi Nishat Anwar, Ali Emami*

**Main category:** cs.CL

**Keywords:** Large Language Models, personality traits, human-computer interaction, collaborative tasks, user preferences

**Relevance Score:** 9

**TL;DR:** This study examines how users' personality traits influence their preferences for different Large Language Models (LLMs) during multi-turn collaborations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the impact of personality traits on LLM user preferences is essential as these models become integral in collaborative workflows.

**Method:** A study involving 32 participants from four Keirsey personality types was conducted to evaluate interactions with GPT-4 and Claude 3.5 across four collaborative tasks.

**Key Contributions:**

	1. Identified personality-driven preferences in LLMs
	2. Evaluated user interactions across multiple collaborative tasks
	3. Showed how personality impacts model choice beyond traditional metrics

**Result:** Significant personality-driven preferences were found, with Rationals favoring GPT-4 for goal-oriented tasks and Idealists preferring Claude 3.5 for creative and analytical tasks. Overall helpfulness ratings were similar across models.

**Limitations:** 

**Conclusion:** Personality-based analysis provides insights into user preferences for LLMs that traditional evaluations might overlook.

**Abstract:** As Large Language Models (LLMs) increasingly integrate into everyday workflows, where users shape outcomes through multi-turn collaboration, a critical question emerges: do users with different personality traits systematically prefer certain LLMs over others? We conducted a study with 32 participants evenly distributed across four Keirsey personality types, evaluating their interactions with GPT-4 and Claude 3.5 across four collaborative tasks: data analysis, creative writing, information retrieval, and writing assistance. Results revealed significant personality-driven preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented tasks, while idealists favored Claude 3.5, especially for creative and analytical tasks. Other personality types showed task-dependent preferences. Sentiment analysis of qualitative feedback confirmed these patterns. Notably, aggregate helpfulness ratings were similar across models, showing how personality-based analysis reveals LLM differences that traditional evaluations miss.

</details>


### [43] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)

*Peng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu*

**Main category:** cs.CL

**Keywords:** text embedding, contextual representation, multi-task framework, LLM, data synthesis

**Relevance Score:** 9

**TL;DR:** QZhou-Embedding is a contextual text embedding model that excels in text representation, combining a multi-task framework and advanced data transformation techniques for enhanced model training and performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a general-purpose embedding model that improves text representation capabilities and model learning efficiency by utilizing diverse datasets and innovative training strategies.

**Method:** The model utilizes a unified multi-task framework, including a data synthesis pipeline with LLM APIs for paraphrasing and augmentation, along with a two-stage training process involving retrieval-focused pretraining and task-specific fine-tuning.

**Key Contributions:**

	1. Introduction of a versatile contextual text embedding model
	2. Innovative data synthesis pipeline utilizing LLM API
	3. State-of-the-art benchmark results and practical applications in retrieval tasks

**Result:** The QZhou-Embedding model achieves state-of-the-art results on MTEB and CMTEB benchmarks, leading in both categories, and excels in tasks such as reranking and clustering.

**Limitations:** 

**Conclusion:** Higher-quality and more diverse training data significantly enhances retrieval model performance, and leveraging LLMs can optimize data quality for embedding breakthroughs.

**Abstract:** We present QZhou-Embedding, a general-purpose contextual text embedding model with exceptional text representation capabilities. Built upon the Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task framework comprising specialized data transformation and training strategies. The data transformation scheme enables the incorporation of more diverse textual training datasets, while the task-specific training strategies enhance model learning efficiency. We developed a data synthesis pipeline leveraging LLM API, incorporating techniques such as paraphrasing, augmentation, and hard negative example generation to improve the semantic richness and sample difficulty of the training set. Additionally, we employ a two-stage training strategy, comprising initial retrieval-focused pretraining followed by full-task fine-tuning, enabling the embedding model to extend its capabilities based on robust retrieval performance. Our model achieves state-of-the-art results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards (August 27 2025), and simultaneously achieves state-of-the-art performance on tasks including reranking, clustering, etc. Our findings demonstrate that higher-quality, more diverse data is crucial for advancing retrieval model performance, and that leveraging LLMs generative capabilities can further optimize data quality for embedding model breakthroughs. Our model weights are released on HuggingFace under Apache 2.0 license. For reproducibility, we provide evaluation code and instructions on GitHub.

</details>


### [44] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)

*Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** misleading visualizations, Misinformation, benchmark dataset, multimodal models, synthetic dataset

**Relevance Score:** 8

**TL;DR:** This paper introduces Misviz, a benchmark dataset for detecting misleading visualizations, along with a synthetic dataset, Misviz-synth, to aid model training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of misleading visualizations that contribute to misinformation on social media by providing a dataset and tools for their detection.

**Method:** Introducing the Misviz benchmark with 2,604 real-world visualizations annotated for misleading characteristics, and creating Misviz-synth, a synthetic dataset consisting of 81,814 generated visualizations.

**Key Contributions:**

	1. Introduction of Misviz benchmark with real-world visualizations
	2. Creation of Misviz-synth synthetic dataset
	3. Comprehensive evaluation and baseline performance on the task.

**Result:** Evaluation on both datasets shows significant challenges in detecting misleading visualizations using state-of-the-art models, even with the provided benchmarks.

**Limitations:** The task of detecting misleading visualizations remains highly challenging despite newly available datasets.

**Conclusion:** The release of Misviz and Misviz-synth, along with accompanying code, aims to facilitate future research in recognizing and mitigating the effects of misleading visualizations.

**Abstract:** Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also release Misviz-synth, a synthetic dataset of 81,814 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.

</details>


### [45] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)

*Yao Wang, Di Liang, Minlong Peng*

**Main category:** cs.CL

**Keywords:** Fine-tuning, Language models, Task interference, Core parameters, Machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents Core Parameter Isolation Fine-Tuning (CPI-FT), a method to improve supervised fine-tuning of large language models by isolating core parameters while mitigating task interference and forgetting.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the seesaw phenomenon in supervised fine-tuning of large language models, where performance on certain tasks improves at the cost of others.

**Method:** The proposed framework independently fine-tunes the LLM on each task to identify core parameter regions and groups tasks with similar cores for joint modeling. A parameter fusion technique and a lightweight mixed-task training phase are introduced to prevent interference and forgetting.

**Key Contributions:**

	1. Novel Core Parameter Isolation Fine-Tuning framework
	2. Parameter fusion technique mitigating destructive interference
	3. Lightweight training phase preventing catastrophic forgetting

**Result:** The experiments show that CPI-FT significantly reduces task interference and forgetting, outperforming standard multi-task and multi-stage fine-tuning methods across various benchmarks.

**Limitations:** 

**Conclusion:** CPI-FT provides an effective approach to fine-tuning LLMs, enhancing performance across diverse tasks by isolating core parameters and managing task interference.

**Abstract:** Supervised fine-tuning (SFT) is a pivotal approach to adapting large language models (LLMs) for downstream tasks; however, performance often suffers from the ``seesaw phenomenon'', where indiscriminate parameter updates yield progress on certain tasks at the expense of others. To address this challenge, we propose a novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework. Specifically, we first independently fine-tune the LLM on each task to identify its core parameter regions by quantifying parameter update magnitudes. Tasks with similar core regions are then grouped based on region overlap, forming clusters for joint modeling. We further introduce a parameter fusion technique: for each task, core parameters from its individually fine-tuned model are directly transplanted into a unified backbone, while non-core parameters from different tasks are smoothly integrated via Spherical Linear Interpolation (SLERP), mitigating destructive interference. A lightweight, pipelined SFT training phase using mixed-task data is subsequently employed, while freezing core regions from prior tasks to prevent catastrophic forgetting. Extensive experiments on multiple public benchmarks demonstrate that our approach significantly alleviates task interference and forgetting, consistently outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [46] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)

*Diane Tchuindjo, Omar Khattab*

**Main category:** cs.CL

**Keywords:** large language models, reasoning-intensive regression, MENTAT

**Relevance Score:** 8

**TL;DR:** The paper introduces MENTAT, a method designed for reasoning-intensive regression tasks using large language models, showing significant performance improvement over existing approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced in reasoning-intensive regression tasks with limited data and computation, where traditional methods struggle.

**Method:** The authors propose MENTAT, a method that combines batch-reflective prompt optimization with neural ensemble learning for improved performance in RiR tasks.

**Key Contributions:**

	1. Introduction of MENTAT for reasoning-intensive regression tasks
	2. Combination of batch-reflective prompt optimization and neural ensemble learning
	3. Establishment of a benchmark for RiR tasks

**Result:** MENTAT achieves up to 65% improvement over prompting frozen LLMs and finetuning Transformer encoders in reasoning-intensive regression tasks.

**Limitations:** The study indicates that substantial room remains for future advances in reasoning-intensive regression.

**Conclusion:** While MENTAT shows effectiveness in improving RiR, there is still substantial room for future advancements in this area.

**Abstract:** AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text. Unlike standard language regression tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc problems like rubric-based scoring or domain-specific retrieval, where much deeper analysis of text is required while only limited task-specific training data and computation are available. We cast three realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.

</details>


### [47] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)

*Joshua Ong Jun Leang, Zheng Zhao, Aryo Pradipta Gema, Sohee Yang, Wai-Chung Kwan, Xuanli He, Wenda Li, Pasquale Minervini, Eleonora Giunchiglia, Shay B. Cohen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Tasks, Probabilistic Confidence, Scoring Function, Benchmarks

**Relevance Score:** 9

**TL;DR:** PiCSAR enhances LLMs by scoring candidate solutions based on joint log-likelihood of reasoning and answers, achieving better results with fewer samples.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of designing an effective scoring function for reasoning tasks in LLMs without ground-truth answers.

**Method:** Probabilistic Confidence Selection And Ranking (PiCSAR) scores candidates using the joint log-likelihood of their reasoning and final answer, separating reasoning confidence from answer confidence.

**Key Contributions:**

	1. Introduction of the PiCSAR method as a training-free scoring technique for LLMs
	2. Demonstrated substantial accuracy improvements across diverse benchmarks
	3. Established a connection between reasoning confidence and answer correctness

**Result:** PiCSAR demonstrates significant improvements on various benchmarks, achieving gains of +10.18 on MATH500 and +9.81 on AIME2025 while requiring at least 2x fewer samples in 16 out of 20 comparisons.

**Limitations:** 

**Conclusion:** The higher reasoning and answer confidence in correct reasoning chains supports the effectiveness of the PiCSAR method.

**Abstract:** Best-of-n sampling improves the accuracy of large language models (LLMs) and large reasoning models (LRMs) by generating multiple candidate solutions and selecting the one with the highest reward. The key challenge for reasoning tasks is designing a scoring function that can identify correct reasoning chains without access to ground-truth answers. We propose Probabilistic Confidence Selection And Ranking (PiCSAR): a simple, training-free method that scores each candidate generation using the joint log-likelihood of the reasoning and final answer. The joint log-likelihood of the reasoning and final answer naturally decomposes into reasoning confidence and answer confidence. PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500, +9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in 16 out of 20 comparisons. Our analysis reveals that correct reasoning chains exhibit significantly higher reasoning and answer confidence, justifying the effectiveness of PiCSAR.

</details>


### [48] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)

*Inés Altemir Marinas, Anastasiia Kucherenko, Andrei Kucharavy*

**Main category:** cs.CL

**Keywords:** large language models, data quality, ElasticSearch, dataset analysis, safety

**Relevance Score:** 8

**TL;DR:** This project presents a framework for indexing and analyzing LLM training datasets to enhance data quality and safety.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of data quality, safety, and ethics in training large language models from web-scale datasets, specifically focusing on harmful content analysis.

**Method:** An ElasticSearch-based pipeline is utilized to index and analyze LLM training datasets, applied specifically to SwissAI's FineWeb-2 corpus.

**Key Contributions:**

	1. Development of an ElasticSearch-based framework for dataset indexing and analysis
	2. Real-time analysis capabilities for large-scale LLM training datasets
	3. Improved tools for assessing the quality and safety of training data

**Result:** The framework achieves fast query performance with most searches completed in milliseconds and all under 2 seconds, enabling real-time dataset analysis.

**Limitations:** 

**Conclusion:** The developed tools provide practical enhancements for safer, more accountable AI systems by improving the analysis of training data quality.

**Abstract:** Large language models (LLMs) rely heavily on web-scale datasets like Common Crawl, which provides over 80\% of training data for some modern models. However, the indiscriminate nature of web crawling raises challenges in data quality, safety, and ethics. Despite the critical importance of training data quality, prior research on harmful content has been limited to small samples due to computational constraints. This project presents a framework for indexing and analyzing LLM training datasets using an ElasticSearch-based pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages), achieving fast query performance--most searches in milliseconds, all under 2 seconds. Our work demonstrates real-time dataset analysis, offering practical tools for safer, more accountable AI systems.

</details>


### [49] [Continuous Language Model Interpolation for Dynamic and Controllable Text Generation](https://arxiv.org/abs/2404.07117)

*Sara Kangaslahti, David Alvarez-Melis*

**Main category:** cs.CL

**Keywords:** large language models, dynamic adaptation, user preferences, linear weight interpolation, fine-grained control

**Relevance Score:** 9

**TL;DR:** This paper explores dynamic adaptation of large language models (LLMs) to user preferences through linear weight interpolation, enabling fine-grained control over model outputs.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are increasingly used in user-facing applications, this work addresses the need for models that can adapt to changing user preferences rather than optimizing for a single objective.

**Method:** The authors leverage adaptation methods via linear weight interpolation, creating a set of anchor models fine-tuned to different domains. Weight updates from these models parameterize a vast class of models within their convex hull.

**Key Contributions:**

	1. Proposes a method for dynamic adaptation of LLMs to user preferences.
	2. Establishes a framework for using linear weight interpolation in model control.
	3. Empirically validates the predictable response of model outputs to interpolation weight changes.

**Result:** The study demonstrates that varying interpolation weights leads to predictable changes in model outputs aligned with controlled attributes, with minimal entanglement between most attributes.

**Limitations:** The paper identifies pairs of attributes with notable entanglement, which can complicate the interpolation approach.

**Conclusion:** The findings indicate that linear interpolation between weights of fine-tuned models allows for precise, simultaneous control over multiple stylistic characteristics in model outputs.

**Abstract:** As large language models (LLMs) have gained popularity for a variety of use cases, making them adaptable and controllable has become increasingly important, especially for user-facing applications. While the existing literature on LLM adaptation primarily focuses on finding a model (or models) that optimizes a single predefined objective, here we focus on the challenging case where the model must dynamically adapt to diverse -- and often changing -- user preferences. For this, we leverage adaptation methods based on linear weight interpolation, casting them as continuous multi-domain interpolators that produce models with specific prescribed generation characteristics on-the-fly. Specifically, we use low-rank updates to fine-tune a base model to various different domains, yielding a set of anchor models with distinct generation profiles. Then, we use the weight updates of these anchor models to parametrize the entire (infinite) class of models contained within their convex hull. We empirically show that varying the interpolation weights yields predictable and consistent change in the model outputs with respect to all of the controlled attributes. We find that there is little entanglement between most attributes and identify and discuss the pairs of attributes for which this is not the case. Our results suggest that linearly interpolating between the weights of fine-tuned models facilitates predictable, fine-grained control of model outputs with respect to multiple stylistic characteristics simultaneously.

</details>


### [50] [Revealing Fine-Grained Values and Opinions in Large Language Models](https://arxiv.org/abs/2406.19238)

*Dustin Wright, Arnav Arora, Nadav Borenstein, Srishti Yadav, Serge Belongie, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** large language models, bias analysis, Political Compass Test, text justification, tropes

**Relevance Score:** 9

**TL;DR:** This paper analyzes 156k responses from large language models (LLMs) to the Political Compass Test, identifying biases in output based on prompt variations and demographic features while proposing a method to uncover recurrent justifications in responses.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover biases and mitigate potential harm caused by large language models (LLMs) by analyzing their responses to politically charged propositions.

**Method:** Analysis of a dataset comprising responses from 6 LLMs to 62 Political Compass Test propositions using 420 prompt variations, including both coarse-grained and fine-grained analysis of the generated stances and justifications.

**Key Contributions:**

	1. Proposed a method to analyze biases in LLM outputs using the Political Compass Test.
	2. Demonstrated the impact of prompt design and demographic features on LLM responses.
	3. Highlighted recurrent justifications in text through the identification of tropes.

**Result:** Demographic features significantly influence LLM responses, reflecting biases, and consistent tropes are found in text rationales across different prompts and models, indicating a propensity to generate similar justifications irrespective of stances.

**Limitations:** The study focuses on specific prompts and LLMs, which may limit generalizability.

**Conclusion:** Identifying patterns in LLM responses can help understand and mitigate biases in AI outputs, informing the design of safer prompting strategies.

**Abstract:** Uncovering latent values and opinions embedded in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by prompting LLMs with survey questions and quantifying the stances in the outputs towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing natural patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances.

</details>


### [51] [E2LLM: Encoder Elongated Large Language Models for Long-Context Understanding and Reasoning](https://arxiv.org/abs/2409.06679)

*Zihan Liao, Jun Wang, Hang Yu, Lingxiao Wei, Jianguo Li, Jun Wang, Wei Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, long-context processing, soft prompts

**Relevance Score:** 9

**TL;DR:** E2LLM offers a novel solution for processing long contexts in Large Language Models by chunking context into soft prompts, leading to improved performance in tasks like document summarization and question answering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges associated with long-context performance, computational efficiency, and compatibility with pretrained models, termed the 'impossible triangle'.

**Method:** The proposed method, E2LLM, divides long contexts into chunks, compresses them into soft prompts using a pretrained text encoder, and aligns these with a decoder-only LLM through an adapter, supported by two training objectives.

**Key Contributions:**

	1. E2LLM effectively manages long contexts using soft prompts.
	2. Outperformed state-of-the-art methods in major long-context tasks.
	3. Achieved top performance on LongBench v2.

**Result:** E2LLM surpasses 8 state-of-the-art methods in both effectiveness and efficiency for long-context tasks and achieves top performance on LongBench v2 among similar-sized models.

**Limitations:** 

**Conclusion:** The introduction of E2LLM resolves critical challenges in processing long contexts effectively while maintaining low computational demands.

**Abstract:** Processing long contexts is increasingly important for Large Language Models (LLMs) in tasks like multi-turn dialogues, code generation, and document summarization. This paper addresses the challenges of achieving high long-context performance, low computational complexity, and compatibility with pretrained models -- collectively termed the ``impossible triangle''. We introduce E2LLM (Encoder Elongated Large Language Models), a novel approach that effectively navigates this paradox. E2LLM divides long contexts into chunks, compresses each into soft prompts using a pretrained text encoder, and aligns these representations with a decoder-only LLM via an adapter. To enhance the LLM's reasoning with these soft prompts, we employ two training objectives: encoder output reconstruction and long-context instruction fine-tuning. Extensive experiments reveal that E2LLM not only outperforms 8 state-of-the-art (SOTA) methods in effectiveness and efficiency for document summarization and question answering, but also achieves the best performance on LongBench v2 among models of comparable size.

</details>


### [52] [Blind Spot Navigation in Large Language Model Reasoning with Thought Space Explorer](https://arxiv.org/abs/2410.24155)

*Jinghan Zhang, Fengran Mo, Tharindu Cyril Weerasooriya, Yeyang Zhou, Xinyue Ye, Dongjie Wang, Yanjie Fu, Kunpeng Liu*

**Main category:** cs.CL

**Keywords:** large language models, reasoning, thought exploration, AI, machine learning

**Relevance Score:** 8

**TL;DR:** The paper introduces the Thought Space Explorer (TSE), a framework designed to enhance large language models' (LLMs) reasoning by expanding thought structures to explore blind spots.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing reasoning methods for LLMs are limited by previously explored solution spaces, which prevent them from addressing critical blind spots in their cognitive capabilities.

**Method:** The TSE framework generates new reasoning steps and branches from an original thought structure, employing various strategies to broaden the thought exploration process.

**Key Contributions:**

	1. Introduction of the Thought Space Explorer framework for LLMs
	2. Demonstration of improved reasoning performance on complex tasks
	3. Analysis of the impact of structured thought on LLM capabilities

**Result:** TSE demonstrated superior performance on multiple levels of reasoning tasks compared to existing baseline methods, indicating its effectiveness in enhancing LLM reasoning.

**Limitations:** 

**Conclusion:** By utilizing structured and expansive thought processes, TSE can significantly improve LLM reasoning capabilities and address cognitive blind spots.

**Abstract:** Recent advances in large language models (LLMs) have demonstrated their potential in handling complex reasoning tasks, which are usually achieved by constructing a thought chain to guide the model in solving the problem with multi-step thinking. However, existing methods often remain confined to previously explored solution spaces and thus overlook the critical blind spot within LLMs' cognitive range. To address these issues, we introduce the ``Thought Space Explorer'' (TSE), a novel framework to expand and optimize thought structures to guide LLMs to explore their blind spots of thinking. By generating new reasoning steps and branches based on the original thought structure with various designed strategies, TSE broadens the thought exploration view and alleviates the impact of blind spots for LLM reasoning. Experimental results on multiple levels of reasoning tasks demonstrate the efficacy of TSE by surpassing various baseline methods. We also conduct extensive analysis to understand how structured and expansive thought can contribute to unleashing the potential of LLM reasoning capabilities.

</details>


### [53] [A Collaborative Content Moderation Framework for Toxicity Detection based on Conformalized Estimates of Annotation Disagreement](https://arxiv.org/abs/2411.04090)

*Guillermo Villate-Castillo, Javier Del Ser, Borja Sanz*

**Main category:** cs.CL

**Keywords:** content moderation, toxic comments, annotation disagreement, multitask learning, uncertainty estimation

**Relevance Score:** 8

**TL;DR:** A novel content moderation framework that captures annotation disagreement through multitask learning and uncertainty estimation techniques to improve model performance and review processes.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** To address the subjective nature of toxicity perception in content moderation and highlight the value of annotation disagreement.

**Method:** Introducing a multitask learning framework where toxicity classification is the primary task and annotation disagreement serves as an auxiliary task, utilizing Conformal Prediction for uncertainty estimation.

**Key Contributions:**

	1. Multitask learning framework for content moderation
	2. Incorporation of annotation disagreement as an auxiliary task
	3. Use of Conformal Prediction for better uncertainty estimation

**Result:** Demonstrated enhancements in model performance, calibration, and uncertainty estimation while improving parameter efficiency and the review process compared to single-task methods.

**Limitations:** 

**Conclusion:** The proposed framework offers a more nuanced approach to content moderation by incorporating annotation disagreement, ultimately leading to better handling of ambiguous content.

**Abstract:** Content moderation typically combines the efforts of human moderators and machine learning models. However, these systems often rely on data where significant disagreement occurs during moderation, reflecting the subjective nature of toxicity perception. Rather than dismissing this disagreement as noise, we interpret it as a valuable signal that highlights the inherent ambiguity of the content,an insight missed when only the majority label is considered. In this work, we introduce a novel content moderation framework that emphasizes the importance of capturing annotation disagreement. Our approach uses multitask learning, where toxicity classification serves as the primary task and annotation disagreement is addressed as an auxiliary task. Additionally, we leverage uncertainty estimation techniques, specifically Conformal Prediction, to account for both the ambiguity in comment annotations and the model's inherent uncertainty in predicting toxicity and disagreement.The framework also allows moderators to adjust thresholds for annotation disagreement, offering flexibility in determining when ambiguity should trigger a review. We demonstrate that our joint approach enhances model performance, calibration, and uncertainty estimation, while offering greater parameter efficiency and improving the review process in comparison to single-task methods.

</details>


### [54] [Retrieval-Augmented Machine Translation with Unstructured Knowledge](https://arxiv.org/abs/2412.04342)

*Jiaan Wang, Fandong Meng, Yingxue Zhang, Jie Zhou*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, machine translation, large language models, benchmark, multilingual training

**Relevance Score:** 8

**TL;DR:** This paper presents RAGtrans, a benchmark for retrieval-augmented machine translation using unstructured documents and proposes a multi-task training method to enhance LLM translation abilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance machine translation (MT) models by utilizing unstructured documents and world knowledge not fully paired across languages.

**Method:** Developed RAGtrans, a benchmark with 169K MT samples, and proposed a multi-task training method utilizing multilingual corpora for auxiliary training objectives without extra labeling.

**Key Contributions:**

	1. Introduction of RAGtrans benchmark for retrieval-augmented MT.
	2. Proposed multi-task training method leveraging multilingual data.
	3. Empirical improvements in translation metrics (BLEU and COMET) for LLMs.

**Result:** Enhancements in LLM performance with improvements of 1.6-3.1 BLEU and 1.0-2.0 COMET scores for English-Chinese, as well as 1.7-2.9 BLEU and 2.1-2.7 COMET scores for English-German.

**Limitations:** Challenges faced by current LLMs in retrieval-augmented MT.

**Conclusion:** Current LLMs still face critical challenges in retrieval-augmented MT tasks that need further exploration.

**Abstract:** Retrieval-augmented generation (RAG) introduces additional information to enhance large language models (LLMs). In machine translation (MT), previous work typically retrieves in-context examples from paired MT corpora, or domain-specific knowledge from knowledge graphs, to enhance MT models. However, a large amount of world knowledge is organized in unstructured documents, and might not be fully paired across different languages. In this paper, we study retrieval-augmented MT using unstructured documents. Specifically, we build RAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented MT ability. RAGtrans contains 169K MT samples collected via GPT-4o and human translators. Besides, documents from various languages are also provided to supply the knowledge to these samples. Based on RAGtrans, we further propose a multi-task training method to teach LLMs how to use information from multilingual documents during their translation. The method uses existing multilingual corpora to create auxiliary training objectives without additional labeling requirements. Extensive experiments show that the method improves LLMs by 1.6-3.1 BLEU and 1.0-2.0 COMET scores in En-Zh, and 1.7-2.9 BLEU and 2.1-2.7 COMET scores in En-De. We also conclude the critical difficulties that current LLMs face with this task.

</details>


### [55] [Toxicity Begets Toxicity: Unraveling Conversational Chains in Political Podcasts](https://arxiv.org/abs/2501.12640)

*Naquee Rizwan, Nayandeep Deb, Sarthak Roy, Vishwajeet Singh Solanki, Kiran Garimella, Animesh Mukherjee*

**Main category:** cs.CL

**Keywords:** toxicity, podcasts, conversational structure, digital communication, political discourse

**Relevance Score:** 6

**TL;DR:** This paper analyzes the emergence and escalation of toxicity in political podcast dialogues using a curated dataset of podcast transcripts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing popularity of podcasts as a form of communication and the lack of research on toxic behaviors in this medium.

**Method:** Curated a dataset of political podcast transcripts and analyzed the conversational structure to identify patterns of toxicity.

**Key Contributions:**

	1. Curated a novel dataset of political podcast transcripts for analysis.
	2. Identified specific patterns of toxicity escalation within podcast dialogues.
	3. Provided insights into conversational dynamics that contribute to harmful language.

**Result:** The analysis reveals how toxicity surfaces and escalates in the sequences of replies within podcast dialogues.

**Limitations:** Focuses only on political podcasts; results may not generalize to other podcast genres.

**Conclusion:** Understanding the conversational dynamics of toxicity in podcasts can help address harmful language in digital communication.

**Abstract:** Tackling toxic behavior in digital communication continues to be a pressing concern for both academics and industry professionals. While significant research has explored toxicity on platforms like social networks and discussion boards, podcasts despite their rapid rise in popularity remain relatively understudied in this context. This work seeks to fill that gap by curating a dataset of political podcast transcripts and analyzing them with a focus on conversational structure. Specifically, we investigate how toxicity surfaces and intensifies through sequences of replies within these dialogues, shedding light on the organic patterns by which harmful language can escalate across conversational turns. Warning: Contains potentially abusive/toxic contents.

</details>


### [56] [Strategic resource allocation in memory encoding: An efficiency principle shaping language processing](https://arxiv.org/abs/2503.14728)

*Weijie Xu, Richard Futrell*

**Main category:** cs.CL

**Keywords:** working memory, sentence processing, resource allocation, memory encoding, linguistic behaviors

**Relevance Score:** 5

**TL;DR:** The paper proposes Strategic Resource Allocation (SRA) as a principle for optimizing memory encoding in sentence processing, suggesting that working memory dynamically allocates resources to prioritize unexpected information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how limited working memory capacity supports linguistic behaviors in humans.

**Method:** The authors present a resource-rational approach and analyze naturalistic corpus data to test the SRA principle in dependency locality effects during sentence processing.

**Key Contributions:**

	1. Introduction of Strategic Resource Allocation as a memory efficiency principle
	2. Empirical support for SRA through analysis of dependency locality
	3. Highlighting cross-linguistic variability in memory encoding strategies

**Result:** Findings indicate that surprising inputs are encoded with enhanced representations, reducing memory decay and interference, with observed effects varying across languages.

**Limitations:** Cross-linguistic variability indicates the need for further research on SRA's application in different languages.

**Conclusion:** SRA serves as an efficiency principle in memory encoding, highlighting the role of representational uncertainty and suggesting that memory allocation is influenced by language-specific structures.

**Abstract:** How is the limited capacity of working memory efficiently used to support human linguistic behaviors? In this paper, we propose Strategic Resource Allocation (SRA) as an efficiency principle for memory encoding in sentence processing. The idea is that working memory resources are dynamically and strategically allocated to prioritize novel and unexpected information. From a resource-rational perspective, we argue that SRA is the principled solution to a computational problem posed by two functional assumptions about working memory, namely its limited capacity and its noisy representation. Specifically, working memory needs to minimize the retrieval error of past inputs under the constraint of limited memory resources, an optimization problem whose solution is to allocate more resources to encode more surprising inputs with higher precision. One of the critical consequences of SRA is that surprising inputs are encoded with enhanced representations, and therefore are less susceptible to memory decay and interference. Empirically, through naturalistic corpus data, we find converging evidence for SRA in the context of dependency locality from both production and comprehension, where non-local dependencies with less predictable antecedents are associated with reduced locality effect. However, our results also reveal considerable cross-linguistic variability, suggesting the need for a closer examination of how SRA, as a domain-general memory efficiency principle, interacts with language-specific phrase structures. SRA highlights the critical role of representational uncertainty in understanding memory encoding. It also reimages the effects of surprisal and entropy on processing difficulty from the perspective of efficient memory encoding.

</details>


### [57] [Inducing Programmatic Skills for Agentic Tasks](https://arxiv.org/abs/2504.06821)

*Zora Zhiruo Wang, Apurva Gandhi, Graham Neubig, Daniel Fried*

**Main category:** cs.CL

**Keywords:** agent skill induction, program-based skills, web navigation, machine learning, dynamic adaptation

**Relevance Score:** 4

**TL;DR:** This paper introduces Agent Skill Induction (ASI), a method for agents to learn and adapt task-specific skills via program representations, demonstrating improved performance and efficiency in web navigation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the adaptability and efficiency of agents in performing digital tasks through learning task-specific skills online.

**Method:** The paper proposes Agent Skill Induction (ASI), which involves inducing, verifying, and utilizing program-based skills by agents dynamically based on web interactions.

**Key Contributions:**

	1. Introduction of Agent Skill Induction (ASI) methodology
	2. Evidence of performance improvement over existing agent models
	3. Demonstrated ability to generalize and adapt skills across different web tasks

**Result:** ASI outperforms static baselines and text-skill counterparts, achieving a 23.5% and 11.3% higher success rate respectively, and reduces operational steps by 10.7-15.3% through higher-level skill composition.

**Limitations:** 

**Conclusion:** ASI demonstrates efficiency and accuracy in complex web activities, with effective skill generalization and adaptation to website changes.

**Abstract:** To succeed in common digital tasks such as web navigation, agents must carry out a variety of specialized tasks such as searching for products or planning a travel route. To tackle these tasks, agents can bootstrap themselves by learning task-specific skills online through interaction with the web environment. In this work, we demonstrate that programs are an effective representation for skills. We propose agent skill induction (ASI), which allows agents to adapt themselves by inducing, verifying, and utilizing program-based skills on the fly. We start with an evaluation on the WebArena agent benchmark and show that ASI outperforms the static baseline agent and its text-skill counterpart by 23.5% and 11.3% in success rate, mainly thanks to the programmatic verification guarantee during the induction phase. ASI also improves efficiency by reducing 10.7-15.3% of the steps over baselines, by composing primitive actions (e.g., click) into higher-level skills (e.g., search product). We then highlight the efficacy of ASI in remaining efficient and accurate under scaled-up web activities. Finally, we examine the generalizability of induced skills when transferring between websites, and find that ASI can effectively reuse common skills, while also updating incompatible skills to versatile website changes.

</details>


### [58] [DeepTrans: Deep Reasoning Translation via Reinforcement Learning](https://arxiv.org/abs/2504.10187)

*Jiaan Wang, Fandong Meng, Jie Zhou*

**Main category:** cs.CL

**Keywords:** deep reasoning, free translation, reinforcement learning, LLM, natural language processing

**Relevance Score:** 8

**TL;DR:** DeepTrans is a deep reasoning translation model that enhances free translation using reinforcement learning, showing significant performance improvements over existing LLMs without the need for labeled data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the under-explored area of free translation in deep reasoning LLMs, aiming to improve translation quality beyond literal interpretations.

**Method:** DeepTrans employs reinforcement learning with a reward model that incorporates pre-defined scoring criteria based on translation results and cognitive processes.

**Key Contributions:**

	1. Introduction of DeepTrans for free translation using RL
	2. Development of a reward model that guides translation and reasoning
	3. Demonstration of significant performance improvements in literature translation

**Result:** DeepTrans, using Qwen2.5-7B, improves literature translation performance by 16.3% compared to existing models and demonstrates superior results over other deep reasoning LLMs.

**Limitations:** Fails in certain translation scenarios and the specifics of RL exploration limitations are discussed.

**Conclusion:** The findings highlight the potential of using RL for free translation, with the authors hoping to inspire further research in the field.

**Abstract:** Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shown promising performance in various downstream tasks. Free translation is an important and interesting task in the multilingual world, which requires going beyond word-for-word translation. However, the task is still under-explored in deep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning translation model that learns free translation via reinforcement learning (RL). Specifically, we carefully build a reward model with pre-defined scoring criteria on both the translation results and the thought processes. The reward model teaches DeepTrans how to think and free-translate the given sentences during RL. Besides, our RL training does not need any labeled translations, avoiding the human-intensive annotation or resource-intensive data synthesis. Experimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as the backbone, DeepTrans improves performance by 16.3% in literature translation, and outperforms strong deep reasoning LLMs. Moreover, we summarize the failures and interesting findings during our RL exploration. We hope this work could inspire other researchers in free translation.

</details>
