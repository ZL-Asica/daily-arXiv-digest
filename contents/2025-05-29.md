# 2025-05-29

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 20]

- [cs.CL](#cs.CL) [Total: 157]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Broadening Our View: Assistive Technology for Cerebral Visual Impairment](https://arxiv.org/abs/2505.21875)

*Bhanuka Gamage, Leona Holloway, Nicola McDowell, Thanh-Toan Do, Nicholas Seow Chiang Price, Arthur James Lowery, Kim Marriott*

**Main category:** cs.HC

**Keywords:** Cerebral Visual Impairment, Assistive Technologies, Human-Computer Interaction, Machine Learning, Computer Vision

**Relevance Score:** 8

**TL;DR:** This paper explores the lack of assistive technologies for people with Cerebral Visual Impairment (CVI), highlighting a critical research gap in the field of Human-Computer Interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the needs of people with Cerebral Visual Impairment (CVI), a demographic largely overlooked in existing assistive technology research.

**Method:** A scoping review was conducted to identify relevant literature at the intersection of assistive technologies and CVI.

**Key Contributions:**

	1. Identify the gap in research for CVI within assistive technologies.
	2. Highlight the limited focus on assistive technologies specifically designed for CVI.
	3. Encourage further exploration in the domain of HCI for CVI applications.

**Result:** Identified 14 papers, of which only three discussed assistive technologies for CVI, indicating a significant gap in research.

**Limitations:** 

**Conclusion:** There is a pressing need for the HCI and Assistive Technologies community to focus on developing solutions for individuals with CVI to improve their quality of life.

**Abstract:** Over the past decade, considerable research has been directed towards assistive technologies to support people with vision impairments using machine learning, computer vision, image enhancement, and/or augmented/virtual reality. However, this has almost totally overlooked a growing demographic: people with Cerebral Visual Impairment (CVI). Unlike Ocular Vision Impairments (OVI), CVI arises from damage to the brain's visual processing centres. This paper introduces CVI and reveals a wide research gap in addressing the needs of this demographic. Through a scoping review, we identified 14 papers at the intersection of these technologies and CVI. Of these, only three papers described assistive technologies focused on people living with CVI, with the others focusing on diagnosis, understanding, simulation or rehabilitation. Our findings highlight the opportunity for the Human-Computer Interaction and Assistive Technologies research community to explore and address this underrepresented domain, thereby enhancing the quality of life for people with CVI.

</details>


### [2] [TIEboard: A Digital Educational Tool for Kids Geometric Learning](https://arxiv.org/abs/2505.21891)

*Arooj Zaidi, Giulia Barbareschi, Kai Kunze, Yun Suen Pai, Junichi Yamaoka*

**Main category:** cs.HC

**Keywords:** Tangible User Interfaces, geometry, interactive learning, children education, early learning

**Relevance Score:** 5

**TL;DR:** TIEboard is an interactive device designed to promote early geometry learning for children using familiar tools and real-time feedback.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of Tangible User Interfaces in enhancing early geometry learning among young learners.

**Method:** TIEboard utilizes traditional tools' familiarity and incorporates instructional lights and conductive materials for shape creation and real-time feedback.

**Key Contributions:**

	1. Introduction of TIEboard for teaching geometry to children
	2. Incorporation of six interaction modes for different skill levels
	3. Real-time feedback mechanism for lacing activities

**Result:** The study demonstrated TIEboard's effectiveness in supporting geometric learning, creativity, and collaboration among children aged 5-9.

**Limitations:** 

**Conclusion:** TIEboard successfully engages young learners in geometry through interactive and collaborative methods.

**Abstract:** Tangible User Interfaces have shown potential in supporting the acquisition of key concepts in computing and mathematics while fostering engagement in young learners, but these approaches are less commonly utilised in the context of geometry. In this paper we introduce TIEboard, an interactive device to promote early learning of basic geometry concepts. TIEboard draws inspiration from traditional geoboards and lacing toys to leverage children's familiarity with these traditional tools. It employs instructional lights to guide children in creating shapes using colourful threads of optical fiber. The use of conductive materials allows the system to detect lacing activity and provide feedback in real-time. TIEboard incorporates six interaction modes of varying difficulty based on an incremental learning framework. The study evaluated TIEboard's effectiveness in supporting early geometric learning, facilitating creativity and promoting collaboration among 16 children aged 5-9.

</details>


### [3] [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)

*Ziyun Zhang, Xinyi Liu, Xiaoyi Zhang, Jun Wang, Gang Chen, Yan Lu*

**Main category:** cs.HC

**Keywords:** knowledge-execution gap, computer use agents, UI-Evol

**Relevance Score:** 7

**TL;DR:** This paper introduces UI-Evol, a module designed to enhance the execution of computer use agents by bridging the knowledge-execution gap through a two-stage process: Retrace Stage for extracting action sequences and Critique Stage for knowledge refinement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical knowledge-execution gap where even accurate knowledge fails to result in effective task execution in computer use agents.

**Method:** The proposed UI-Evol module operates in two stages: the Retrace Stage extracts objective action sequences from agent-environment interactions, and the Critique Stage refines knowledge by comparing these sequences with external references.

**Key Contributions:**

	1. Development of a plug-and-play module (UI-Evol) for GUI knowledge evolution
	2. Introduction of Retrace and Critique stages for knowledge refinement
	3. Demonstrated significant performance enhancements on the OSWorld benchmark

**Result:** UI-Evol significantly improves task performance and reduces behavioral standard deviation, leading to enhanced reliability in computer use tasks.

**Limitations:** 

**Conclusion:** The module successfully bridges the knowledge-execution gap and improves the reliability and performance of computer use agents.

**Abstract:** External knowledge has played a crucial role in the recent development of computer use agents. We identify a critical knowledge-execution gap: retrieved knowledge often fails to translate into effective real-world task execution. Our analysis shows even 90\% correct knowledge yields only 41\% execution success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a Retrace Stage that extracts faithful objective action sequences from actual agent-environment interactions, and a Critique Stage that refines existing knowledge by comparing these sequences against external references. We conduct comprehensive experiments on the OSWorld benchmark with the state-of-the-art Agent S2. Our results demonstrate that UI-Evol not only significantly boosts task performance but also addresses a previously overlooked issue of high behavioral standard deviation in computer use agents, leading to superior performance on computer use tasks and substantially improved agent reliability.

</details>


### [4] [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)

*Aditya Gunturu, Ben Pearman, Keiichi Ihara, Morteza Faraji, Bryan Wang, Rubaiat Habib Kazi, Ryo Suzuki*

**Main category:** cs.HC

**Keywords:** LLM, map animation, human-computer interaction, animation authoring, natural language processing

**Relevance Score:** 7

**TL;DR:** MapStory is an LLM-powered tool that automates the creation of editable map animations from natural language scripts, enhancing ease of use and creativity.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To simplify the creation of map animations, enabling users to generate content from natural language while minimizing technical barriers.

**Method:** MapStory utilizes an agentic architecture to break down user scripts into animation components and integrates a researcher component to query geospatial data using an LLM and web search.

**Key Contributions:**

	1. Development of an LLM-powered animation authoring tool for map animations
	2. Integration of automatic geospatial data querying
	3. User-friendly interactive editing through a timeline interface

**Result:** The system allows for rapid creation and refinement of map animations, confirmed by evaluations showing enhanced user experience in creation and iteration.

**Limitations:** Limited to specific types of animations and dependent on the accuracy of geospatial queries.

**Conclusion:** MapStory significantly aids in the animation process, allowing for more creativity and faster production of map-centric stories.

**Abstract:** We introduce MapStory, an LLM-powered animation authoring tool that generates editable map animation sequences directly from natural language text. Given a user-written script, MapStory leverages an agentic architecture to automatically produce a scene breakdown, which decomposes the script into key animation building blocks such as camera movements, visual highlights, and animated elements. Our system includes a researcher component that accurately queries geospatial information by leveraging an LLM with web search, enabling the automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories.

</details>


### [5] [Eye-Tracking and Biometric Feedback in UX Research: Measuring User Engagement and Cognitive Load](https://arxiv.org/abs/2505.21982)

*Aaditya Shankar Majumder*

**Main category:** cs.HC

**Keywords:** user experience, eye-tracking, biometric feedback, cognitive load, digital interfaces

**Relevance Score:** 8

**TL;DR:** This study examines eye-tracking and biometric feedback as methods to enhance user experience research by providing objective measures of user engagement and cognitive load in digital interfaces.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the understanding of user interactions which are often overlooked when relying solely on qualitative methods like surveys and interviews.

**Method:** The paper employs eye-tracking and biometric feedback techniques to quantify user engagement and cognitive load, providing an empirical basis for evaluating user interactions in digital settings.

**Key Contributions:**

	1. Introduction of eye-tracking and biometric methods to UX research.
	2. Empirical evidence supporting the effectiveness of these methods.
	3. Discussion of challenges and ethical implications of using biometric data.

**Result:** The results show how gaze behavior and bodily responses can offer valuable insights into user engagement, highlighting the importance of objective measures in UX research.

**Limitations:** The paper discusses limitations related to data interpretation, ethical issues regarding user privacy, and the complexity of integrating these technologies into existing UX research practices.

**Conclusion:** The study concludes that integrating eye-tracking and biometric feedback into UX evaluation methods can significantly enhance the design and effectiveness of digital interfaces.

**Abstract:** User experience research often uses surveys and interviews, which may miss subconscious user interactions. This study explores eye-tracking and biometric feedback as tools to assess user engagement and cognitive load in digital interfaces. These methods measure gaze behavior and bodily responses, providing an objective complement to qualitative insights. Using empirical evidence, practical applications, and advancements from 2023-2025, we present experimental data, describe our methodology, and place our work within foundational and recent literature. We address challenges like data interpretation, ethical issues, and technological integration. These tools are key for advancing UX design in complex digital environments.

</details>


### [6] [Voice CMS: updating the knowledge base of a digital assistant through conversation](https://arxiv.org/abs/2505.22303)

*Grzegorz Wolny, Michał Szczerbak*

**Main category:** cs.HC

**Keywords:** voice user interface, multi-agent LLM, knowledge management, usability evaluation, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This study evaluates a multi-agent LLM architecture with a voice user interface (VUI) versus a traditional graphical content management system, focusing on usability and task complexity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the usability of a VUI in updating digital assistant knowledge bases compared to traditional graphical systems.

**Method:** A usability evaluation comparing a multi-agent LLM architecture with a voice user interface against a graphical content management system, focusing on user preferences and task complexity.

**Key Contributions:**

	1. Proposes a multi-agent LLM architecture with VUI for digital assistants.
	2. Demonstrates user preference for VUI in less complex tasks.
	3. Highlights the potential of hybrid interfaces for effective knowledge management.

**Result:** While the VUI was rated lower in overall usability, it was preferred for less complex tasks, with content quality comparable to the graphical interface even in complex tasks.

**Limitations:** Overall usability of the VUI is still rated lower than the graphical interface for complex tasks.

**Conclusion:** A hybrid interface combining VUI and graphical feedback could reduce cognitive load and improve user experience in knowledge management tasks.

**Abstract:** In this study, we propose a solution based on a multi-agent LLM architecture and a voice user interface (VUI) designed to update the knowledge base of a digital assistant. Its usability is evaluated in comparison to a more traditional graphical content management system (CMS), with a focus on understanding the relationship between user preferences and the complexity of the information being provided. The findings demonstrate that, while the overall usability of the VUI is rated lower than the graphical interface, it is already preferred by users for less complex tasks. Furthermore, the quality of content entered through the VUI is comparable to that achieved with the graphical interface, even for highly complex tasks. Obtained qualitative results suggest that a hybrid interface combining the strengths of both approaches could address the key challenges identified during the experiment, such as reducing cognitive load through graphical feedback while maintaining the intuitive nature of voice-based interactions. This work highlights the potential of conversational interfaces as a viable and effective method for knowledge management in specific business contexts.

</details>


### [7] [ToPSen: Task-Oriented Priming and Sensory Alignment for Comparing Coding Strategies Between Sighted and Blind Programmers](https://arxiv.org/abs/2505.22414)

*Md Ehtesham-Ul-Haque, Syed Masum Billah*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Accessibility, Programming, Blindness, Collaboration

**Relevance Score:** 8

**TL;DR:** This paper studies the coding strategies of sighted vs. blind programmers using audio feedback to identify collaboration challenges and improve accessibility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to understand the challenges faced in mixed-ability collaboration between sighted and blind programmers, particularly in teaching and working together.

**Method:** A design framework called Task-Oriented Priming and Sensory Alignment (ToPSen) was proposed and studied with 12 blind and 12 sighted participants coding non-visually.

**Key Contributions:**

	1. Proposed framework (ToPSen) for improving coding accessibility and collaboration
	2. Revealed differences in information processing between sighted and blind programmers
	3. Identified gaps in current IDE designs that hinder collaboration.

**Result:** Expert blind programmers maintain more accurate mental models and process more information in working memory than sighted programmers using ToPSen.

**Limitations:** 

**Conclusion:** Insights from this study can inform guidelines for improving the accessibility of programming tools and enable better mixed-ability collaboration.

**Abstract:** This paper examines how the coding strategies of sighted and blind programmers differ when working with audio feedback alone. The goal is to identify challenges in mixed-ability collaboration, particularly when sighted programmers work with blind peers or teach programming to blind students. To overcome limitations of traditional blindness simulation studies, we proposed Task-Oriented Priming and Sensory Alignment (ToPSen), a design framework that reframes sensory constraints as technical requirements rather than as a disability. Through a study of 12 blind and 12 sighted participants coding non-visually, we found that expert blind programmers maintain more accurate mental models and process more information in working memory than sighted programmers using ToPSen. Our analysis revealed that blind and sighted programmers process structural information differently, exposing gaps in current IDE designs. These insights inform our guidelines for improving the accessibility of programming tools and fostering effective mixed-ability collaboration.

</details>


### [8] [AI Trust Reshaping Administrative Burdens: Understanding Trust-Burden Dynamics in LLM-Assisted Benefits Systems](https://arxiv.org/abs/2505.22418)

*Jeongwon Jo, He Zhang, Jie Cai, Nitesh Goyal*

**Main category:** cs.HC

**Keywords:** AI, SNAP, trust, administrative burden, LLM

**Relevance Score:** 8

**TL;DR:** The paper examines how AI technologies, specifically LLMs, impact the administrative burden on SNAP applicants, introducing new educational and psychological costs while also alleviating some traditional burdens.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI technologies can change the dynamic of administrative burdens faced by low-income applicants in the SNAP program.

**Method:** Interviews with SNAP applicants to evaluate their experiences and perceptions of AI technologies in the benefits administration system.

**Key Contributions:**

	1. Introduction of new types of costs associated with AI in benefits administration
	2. Identification of trust dimensions affecting administrative burdens
	3. Proposals for evidence-based disclosure to improve applicant experience

**Result:** Findings reveal AI can alleviate traditional burdens but may introduce new types of learning, compliance, and psychological costs, while trust in AI can reduce these burdens across competence, integrity, and benevolence dimensions.

**Limitations:** Study focused on a specific benefit program (SNAP) and may not generalize to other contexts or systems.

**Conclusion:** Evidence-based information disclosure is critical for effective AI-assisted benefits administration, and further research on trust-burden dynamics is necessary.

**Abstract:** Supplemental Nutrition Assistance Program (SNAP) is an essential benefit support system provided by the US administration to 41 million federally determined low-income applicants. Through interviews with such applicants across a diverse set of experiences with the SNAP system, our findings reveal that new AI technologies like LLMs can alleviate traditional burdens but also introduce new burdens. We introduce new types of learning, compliance, and psychological costs that transform the administrative burden on applicants. We also identify how trust in AI across three dimensions--competence, integrity, and benevolence--is perceived to reduce administrative burdens, which may stem from unintended and untoward overt trust in the system. We discuss calibrating appropriate levels of user trust in LLM-based administrative systems, mitigating newly introduced burdens. In particular, our findings suggest that evidence-based information disclosure is necessary in benefits administration and propose directions for future research on trust-burden dynamics in AI-assisted administration systems.

</details>


### [9] [Parental Collaboration and Closeness: Envisioning with New Couple Parents](https://arxiv.org/abs/2505.22428)

*Ya-Fang Lin, Xiaotian Li, Wan-Hsuan Huang, Charan Pushpanathan Prabavathi, Jie Cai, John M. Carroll*

**Main category:** cs.HC

**Keywords:** co-parenting, closeness, technology, emotional support, parenting

**Relevance Score:** 3

**TL;DR:** This paper explores how technology can support closeness in co-parenting among new couples, highlighting the need for emotional connection alongside practical support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in existing technologies that fail to adequately support relational closeness in co-parenting arrangements among couples.

**Method:** The study utilized scenarios and design probes with 10 new parent couples to brainstorm technology solutions for fostering closeness in co-parenting.

**Key Contributions:**

	1. Exploration of emotional aspects in co-parenting technology design
	2. Identification of tasks and emotional support mechanisms
	3. Expansion of co-parenting technology design space to include fun and validation.

**Result:** Participants identified the importance of emotional awareness, task sharing, and fostering fun interactions, leading to an expanded design space for co-parenting technologies.

**Limitations:** 

**Conclusion:** Technologies have the potential to enhance closeness in co-parenting by supporting interdependence and integrating positive emotional interactions.

**Abstract:** Couples often experience a decrease in closeness as they cope with the demands of parenthood. Existing technologies have supported parenting and parental collaboration. However, these technologies do not adequately support closeness in co-parenting. We use scenarios and design probes to brainstorm with 10 new parent couples to explore and envision possibilities for technologies to support closeness. We reported parents' current technology use for co-parenting and how participants considered and envisioned co-parenting technology for closeness, including information and task sharing, emotion awareness and disclosure, and fostering fun interaction. We discuss the potential technology has for fostering closeness in co-parenting by (1) fostering interdependence by supporting parental competence and (2) integrating positive emotions and experiences, such as validation and fun, in parenting. Based on our findings, we expand the design space of technology for closeness to include interdependence. We also expand the design space for co-parenting technology by integrating more positive emotions.

</details>


### [10] [Human-Centered Human-AI Collaboration (HCHAC)](https://arxiv.org/abs/2505.22477)

*Qi Gao, Wei Xu, Hanxi Pan, Mowei Shen, Zaifeng Gao*

**Main category:** cs.HC

**Keywords:** Human-AI Collaboration, Human-Centered AI, autonomous intelligent agents, framework, autonomous vehicles

**Relevance Score:** 9

**TL;DR:** This chapter explores Human-AI Collaboration (HAC), emphasizing a human-centered approach, and proposes a framework for effective collaboration between humans and intelligent agents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and improve the collaboration between humans and AI agents as partners in task accomplishment, highlighting the need for a human-centered approach.

**Method:** The chapter reviews current research methodologies in Human-AI Collaboration and proposes a human-centered framework (HCHAC), supported by a case study on autonomous vehicles.

**Key Contributions:**

	1. Proposes a human-centered framework for Human-AI Collaboration (HCHAC).
	2. Reviews existing methodologies and research agendas in HAC.
	3. Illustrates practical applications through a case study in autonomous vehicles.

**Result:** The analysis reveals advancements in HAC and proposes a new framework facilitating effective human-agent collaboration, contributing to research in Human-Centered Artificial Intelligence (HCAI).

**Limitations:** 

**Conclusion:** Future research directions are identified to enhance the reliability and ethical integration of human-centered HAC systems across various domains.

**Abstract:** In the intelligent era, the interaction between humans and intelligent systems fundamentally involves collaboration with autonomous intelligent agents. Human-AI Collaboration (HAC) represents a novel type of human-machine relationship facilitated by autonomous intelligent machines equipped with AI technologies. In this paradigm, AI agents serve not only as auxiliary tools but also as active teammates, partnering with humans to accomplish tasks collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical leadership roles in the collaboration. This human-led collaboration imparts new dimensions to the human-machine relationship, necessitating innovative research perspectives, paradigms, and agenda to address the unique challenges posed by HAC. This chapter delves into the essence of HAC from the human-centered perspective, outlining its core concepts and distinguishing features. It reviews the current research methodologies and research agenda within the HAC field from the HCAI perspective, highlighting advancements and ongoing studies. Furthermore, a framework for human-centered HAC (HCHAC) is proposed by integrating these reviews and analyses. A case study of HAC in the context of autonomous vehicles is provided, illustrating practical applications and the synergistic interactions between humans and AI agents. Finally, it identifies potential future research directions aimed at enhancing the effectiveness, reliability, and ethical integration of human-centered HAC systems in diverse domains.

</details>


### [11] [Spot-On: A Mixed Reality Interface for Multi-Robot Cooperation](https://arxiv.org/abs/2505.22539)

*Tim Engelbracht, Petar Lukovic, Tjark Behrens, Kai Lascheit, René Zurbrügg, Marc Pollefeys, Hermann Blum, Zuria Bauer*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Human-Robot Collaboration, Robotics, User Study, Quadruped Robots

**Relevance Score:** 7

**TL;DR:** The paper presents a mixed reality framework for multi-robot collaboration, focusing on quadruped robots in diverse environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human-robot collaboration in complex environments using mixed reality and robotics advancements.

**Method:** Developed a mixed reality interface for controlling multiple quadruped robots in collaborative tasks involving everyday objects.

**Key Contributions:**

	1. Novel mixed reality framework for multi-robot collaboration
	2. User study demonstrating high usability
	3. Support for complex real-world tasks through the MR interface

**Result:** User study shows high usability ratings, indicating effectiveness of the MR framework in facilitating multi-robot collaboration.

**Limitations:** 

**Conclusion:** The proposed MR framework enhances interaction with quadruped robots, providing an intuitive platform for collaborative tasks.

**Abstract:** Recent progress in mixed reality (MR) and robotics is enabling increasingly sophisticated forms of human-robot collaboration. Building on these developments, we introduce a novel MR framework that allows multiple quadruped robots to operate in semantically diverse environments via a MR interface. Our system supports collaborative tasks involving drawers, swing doors, and higher-level infrastructure such as light switches. A comprehensive user study verifies both the design and usability of our app, with participants giving a "good" or "very good" rating in almost all cases. Overall, our approach provides an effective and intuitive framework for MR-based multi-robot collaboration in complex, real-world scenarios.

</details>


### [12] [Exploring Remote Collaborative Tasks: The Impact of Avatar Representation on Dyadic Haptic Interactions in Shared Virtual Environments](https://arxiv.org/abs/2409.08577)

*Genki Sasaki, Hiroshi Igarashi*

**Main category:** cs.HC

**Keywords:** haptic interaction, avatar representation, social presence, Shared Virtual Environments, collaboration

**Relevance Score:** 7

**TL;DR:** This study investigates how haptic interaction and avatar representation affect social presence in Shared Virtual Environments during collaborative tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of haptic interaction and avatar representation on users' sense of social presence in collaborative tasks within Shared Virtual Environments.

**Method:** Experiments were conducted involving participants performing a collaborative task under four different avatar representation conditions while utilizing haptic interaction.

**Key Contributions:**

	1. First study on haptic interaction and avatar representation interplay in SVEs.
	2. Demonstrated that avatar presence enhances social presence perceptions.
	3. Suggested that haptic interaction alone is sufficient for task performance.

**Result:** The presence of avatars, particularly the partner's avatar, significantly increased the perception of social presence, though it did not affect task performance or force effort.

**Limitations:** The study did not assess long-term effects of avatar representation on social presence or task outcomes in various contexts.

**Conclusion:** Integrating visual (avatar representation) and haptic modalities is essential for enhancing remote collaboration experiences in virtual environments.

**Abstract:** This study is the first to explore the interplay between haptic interaction and avatar representation in Shared Virtual Environments (SVEs). Specifically, how these factors shape users' sense of social presence during dyadic collaborations, while assessing potential effects on task performance. In a series of experiments, participants performed the collaborative task with haptic interaction under four avatar representation conditions: avatars of both participant and partner were displayed, only the participant's avatar was displayed, only the partner's avatar was displayed, and no avatars were displayed. The study finds that avatar representation, especially of the partner, significantly enhances the perception of social presence, which haptic interaction alone does not fully achieve. However, neither the presence nor the type of avatar representation impacts the task performance or participants' force effort of the task, suggesting that haptic interaction provides sufficient interaction cues for the execution of the task. These results underscore the significance of integrating both visual and haptic modalities to optimize remote collaboration experiences in virtual environments, ensuring effective communication and a strong sense of social presence.

</details>


### [13] [Overcoming the Machine Penalty with Imperfectly Fair AI Agents](https://arxiv.org/abs/2410.03724)

*Zhen Wang, Ruiqi Song, Chen Shen, Shiya Yin, Zhao Song, Balaraju Battu, Lei Shi, Danyang Jia, Talal Rahwan, Shuyue Hu*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Large Language Models, Social Dilemma Games

**Relevance Score:** 9

**TL;DR:** AI agents powered by large language models can effectively elicit human cooperation in social dilemma games when exhibiting fair personas.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate why humans tend to cooperate less with machines compared to fellow humans, known as the machine penalty.

**Method:** A pre-registered experiment with 1,152 participants where AI agents with different personas (selfish, cooperative, fair) were deployed in social dilemma games with communication.

**Key Contributions:**

	1. Demonstrated that AI agents can overcome the machine penalty in social dilemmas.
	2. Showed the importance of persona in AI interactions with humans.
	3. Highlighted that fair AI agents can mimic social behaviors of humans.

**Result:** Only fair AI agents elicited human cooperation rates comparable to human-human interactions; they sometimes broke promises but established cooperation as a norm.

**Limitations:** The study focuses on specific social dilemma games and may not generalize to all contexts of human-AI interaction.

**Conclusion:** AI agents should reflect the complexity of human social behaviors to enhance cooperation, challenging the notion of machines as purely altruistic or rational.

**Abstract:** Despite rapid technological progress, effective human-machine cooperation remains a significant challenge. Humans tend to cooperate less with machines than with fellow humans, a phenomenon known as the machine penalty. Here, we show that artificial intelligence (AI) agents powered by large language models can overcome this penalty in social dilemma games with communication. In a pre-registered experiment with 1,152 participants, we deploy AI agents exhibiting three distinct personas: selfish, cooperative, and fair. However, only fair agents elicit human cooperation at rates comparable to human-human interactions. Analysis reveals that fair agents, similar to human participants, occasionally break pre-game cooperation promises, but nonetheless effectively establish cooperation as a social norm. These results challenge the conventional wisdom of machines as altruistic assistants or rational actors. Instead, our study highlights the importance of AI agents reflecting the nuanced complexity of human social behaviors -- imperfect yet driven by deeper social cognitive processes.

</details>


### [14] [Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals](https://arxiv.org/abs/2411.11835)

*Maryam Cheema, Hasti Seifi, Pooyan Fazli*

**Main category:** cs.HC

**Keywords:** Audio descriptions, Blind and low vision, User-driven AI, Video accessibility, Human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper explores user-driven AI-generated audio descriptions (AD) for blind and low vision (BLV) users, allowing them to control the timing and detail of descriptions, with findings from a study involving 20 participants across various video genres.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accessibility of videos for blind and low vision (BLV) users by allowing them to customize and control audio descriptions, overcoming the limitations of traditional AD methods.

**Method:** A study was conducted with 20 BLV participants using AI-generated audio descriptions across seven video genres with two levels of detail (concise and detailed). The participants' preferences for frequency and detail of descriptions were recorded and analyzed.

**Key Contributions:**

	1. Introduction of user-driven AI-generated audio descriptions for BLV users
	2. Empirical findings on user preferences for audio description detail and timing
	3. Insights into the customization needs of BLV viewers for enhanced video accessibility

**Result:** The study found that participants exhibited varying preferences for the frequency and detail of audio descriptions depending on the video genre, and they appreciated having control over these aspects.

**Limitations:** The study involved a small sample size and specific video genres, which may not generalize to all BLV users or video types.

**Conclusion:** The findings suggest that user-driven, AI-generated audio descriptions can significantly improve the viewing experience for BLV users and inform the design of future AD tools.

**Abstract:** Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and offers little customization or control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, enabling BLV viewers to control both the timing and level of detail of the descriptions they receive. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our findings reveal differences in the preferred frequency and level of detail of ADs for different videos, participants' sense of control with this style of AD delivery, and its limitations. We discuss the implications of these findings for the development of future AD tools for BLV users.

</details>


### [15] [When Trust Collides: Decoding Human-LLM Cooperation Dynamics through the Prisoner's Dilemma](https://arxiv.org/abs/2503.07320)

*Guanxuan Jiang, Shirao Yang, Yuyang Wang, Pan Hui*

**Main category:** cs.HC

**Keywords:** Human-AI interaction, Cooperation, Large language models, Decision making, Gender differences

**Relevance Score:** 9

**TL;DR:** The study explores human cooperation with large language models (LLMs) perceived as independent actors in strategic scenarios, showing that agent identity affects cooperation behaviors and decision latency, influenced by participants' interpretations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of autonomous LLMs, understanding human-AI cooperation dynamics becomes essential, especially in competitive contexts where AI agents are perceived as independent.

**Method:** 30 participants played repeated Prisoner's Dilemma games against agents with different identities (human, rule-based AI, LLM). Key metrics analyzed included cooperation rates, decision latency, unsolicited cooperative acts, and trust restoration.

**Key Contributions:**

	1. Identified the effects of agent identity on human cooperation behaviors
	2. Revealed gender differences in decision-making related to AI agents
	3. Highlighted the importance of interpretation and expectation in human-AI interactions

**Result:** Significant influence of agent identity on cooperation-related behaviors was found, alongside notable gender differences in decision latency.

**Limitations:** 

**Conclusion:** The study highlights the impact of agent framing on human-AI interaction and suggests pathways for improving ethical cooperation between humans and autonomous agents.

**Abstract:** As large language models (LLMs) become increasingly capable of autonomous decision-making, they introduce new challenges and opportunities for human-AI cooperation in mixed-motive contexts. While prior research has primarily examined AI in assistive or cooperative roles, little is known about how humans interact with AI agents perceived as independent and strategic actors. This study investigates human cooperative attitudes and behaviors toward LLM agents by engaging 30 participants (15 males, 15 females) in repeated Prisoner's Dilemma games with agents differing in declared identity: purported human, rule-based AI, and LLM agent. Behavioral metrics, including cooperation rate, decision latency, unsolicited cooperative acts and trust restoration tolerance, were analyzed to assess the influence of agent identity and participant gender. Results revealed significant effects of declared agent identity on most cooperation-related behaviors, along with notable gender differences in decision latency. Furthermore, qualitative responses suggest that these behavioral differences were shaped by participants interpretations and expectations of the agents. These findings contribute to our understanding of human adaptation in competitive cooperation with autonomous agents and underscore the importance of agent framing in shaping effective and ethical human-AI interaction.

</details>


### [16] [From the CDC to emerging infectious disease publics: The long-now of polarizing and complex health crises](https://arxiv.org/abs/2503.20262)

*Tawfiq Ammari, Anna Gutowska, Jacob Ziff, Casey Randazzo, Harihan Subramonyam*

**Main category:** cs.HC

**Keywords:** COVID-19, Twitter, Public Health, Crisis Communication, AI Assistants

**Relevance Score:** 4

**TL;DR:** This study analyzes over 275,000 tweets about COVID-19 from the CDC, identifying 16 discourse clusters and highlighting the impact of polarization in public health messaging.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the dynamics of public discourse around COVID-19 on Twitter and its implications for crisis communication and equity.

**Method:** Analysis of over 275,000 tweets related to the CDC using clustering techniques to identify discourse patterns and sentiment.

**Key Contributions:**

	1. Identification of discourse clusters around COVID-19
	2. Insights into polarization and echo chambers in public health communication
	3. Proposals for design interventions using AI for inclusivity

**Result:** Identified 16 distinct discourse clusters characterized by ideological polarization, most forming echo chambers but a few facilitating cross-cutting dialogue.

**Limitations:** The study may not capture the full scope of offline discourse or outcomes of the identified clusters.

**Conclusion:** Long-term, adaptive engagement is crucial for effective communication in public health crises, with recommendations for using multi-agent AI assistants to enhance inclusivity.

**Abstract:** This study examines how public discourse around COVID-19 unfolded on Twitter through the lens of crisis communication and digital publics. Analyzing over 275,000 tweets involving the CDC, we identify 16 distinct discourse clusters shaped by framing, sentiment, credibility, and network dynamics. We find that CDC messaging became a flashpoint for affective and ideological polarization, with users aligning along competing frames of science vs. freedom, and public health vs. political overreach. Most clusters formed echo chambers, while a few enabled cross cutting dialogue. Publics emerged not only around ideology but also around topical and emotional stakes, reflecting shifting concerns across different stages of the pandemic. While marginalized communities raised consistent equity concerns, these narratives struggled to reshape broader discourse. Our findings highlight the importance of long-term, adaptive engagement with diverse publics and propose design interventions such as multi-agent AI assistants, to support more inclusive communication throughout extended public health crises.

</details>


### [17] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)

*Donghuo Zeng, Roberto Legaspi, Yuewen Sun, Xinshuai Dong, Kazushi Ikeda, Peter Spirtes, Kun Zhang*

**Main category:** cs.HC

**Keywords:** Causal Inference, Counterfactuals, Dialogue Systems, Persuasive Technology, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper proposes a model for generating personalized counterfactual dialogues in persuasive systems by utilizing causal discovery and counterfactual inference to optimize system responses based on user utterances.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the need for optimal system responses in user interactions by leveraging causal and counterfactual knowledge to improve engagement and outcomes in persuasive dialogue systems.

**Method:** The methodology involves using causal discovery to identify user and system interaction patterns, estimating latent psychological factors influencing these interactions, and employing counterfactual inference to generate personalized dialogue strategies.

**Key Contributions:**

	1. Introduction of causal discovery for analyzing user-system dialogues
	2. Development of a framework for personalized counterfactual dialogues
	3. Demonstration of improved persuasive outcome metrics through causal methods

**Result:** Experiments on a real-world dataset showed significant improvements in persuasive outcomes, with enhanced cumulative rewards, validating the proposed approach.

**Limitations:** 

**Conclusion:** The findings support the effectiveness of causal discovery in optimizing dialogue policies and enhancing user-system interactions in persuasive contexts.

**Abstract:** We hypothesize that optimal system responses emerge from adaptive strategies grounded in causal and counterfactual knowledge. Counterfactual inference allows us to create hypothetical scenarios to examine the effects of alternative system responses. We enhance this process through causal discovery, which identifies the strategies informed by the underlying causal structure that govern system behaviors. Moreover, we consider the psychological constructs and unobservable noises that might be influencing user-system interactions as latent factors. We show that these factors can be effectively estimated. We employ causal discovery to identify strategy-level causal relationships among user and system utterances, guiding the generation of personalized counterfactual dialogues. We model the user utterance strategies as causal factors, enabling system strategies to be treated as counterfactual actions. Furthermore, we optimize policies for selecting system responses based on counterfactual data. Our results using a real-world dataset on social good demonstrate significant improvements in persuasive system outcomes, with increased cumulative rewards validating the efficacy of causal discovery in guiding personalized counterfactual inference and optimizing dialogue policies for a persuasive dialogue system.

</details>


### [18] [UI-Evol: Automatic Knowledge Evolving for Computer Use Agents](https://arxiv.org/abs/2505.21964)

*Ziyun Zhang, Xinyi Liu, Xiaoyi Zhang, Jun Wang, Gang Chen, Yan Lu*

**Main category:** cs.HC

**Keywords:** knowledge-execution gap, computer use agents, UI-Evol

**Relevance Score:** 6

**TL;DR:** This paper introduces UI-Evol, a plug-and-play module aimed at enhancing task execution in computer use agents by bridging the knowledge-execution gap through a dual-stage process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical knowledge-execution gap where high knowledge accuracy does not translate to effective real-world task execution in computer use agents.

**Method:** The proposed UI-Evol system includes a Retrace Stage that extracts action sequences from agent-environment interactions and a Critique Stage that refines this knowledge by comparing it against external references.

**Key Contributions:**

	1. Introduction of UI-Evol as a solution to the knowledge-execution gap.
	2. Demonstrated significant performance improvements on the OSWorld benchmark.
	3. Addressed high behavioral standard deviation in computer use agents.

**Result:** Experiments on the OSWorld benchmark show that UI-Evol significantly improves task performance and reduces behavioral standard deviation in agents, leading to enhanced reliability and effectiveness in task execution.

**Limitations:** 

**Conclusion:** UI-Evol effectively bridges the knowledge-execution gap, resulting in improved performance and reliability for computer use agents in real-world tasks.

**Abstract:** External knowledge has played a crucial role in the recent development of computer use agents. We identify a critical knowledge-execution gap: retrieved knowledge often fails to translate into effective real-world task execution. Our analysis shows even 90\% correct knowledge yields only 41\% execution success rate. To bridge this gap, we propose UI-Evol, a plug-and-play module for autonomous GUI knowledge evolution. UI-Evol consists of two stages: a Retrace Stage that extracts faithful objective action sequences from actual agent-environment interactions, and a Critique Stage that refines existing knowledge by comparing these sequences against external references. We conduct comprehensive experiments on the OSWorld benchmark with the state-of-the-art Agent S2. Our results demonstrate that UI-Evol not only significantly boosts task performance but also addresses a previously overlooked issue of high behavioral standard deviation in computer use agents, leading to superior performance on computer use tasks and substantially improved agent reliability.

</details>


### [19] [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)

*Aditya Gunturu, Ben Pearman, Keiichi Ihara, Morteza Faraji, Bryan Wang, Rubaiat Habib Kazi, Ryo Suzuki*

**Main category:** cs.HC

**Keywords:** MapStory, animation authoring tool, LLM, geospatial information, user interface

**Relevance Score:** 7

**TL;DR:** MapStory is an LLM-powered tool for generating editable map animation sequences from text, facilitating easier animation creation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To simplify the process of creating map animations by leveraging natural language inputs and LLM technology.

**Method:** MapStory decomposes scripts into key animation elements, queries geospatial information using a web-connected LLM, and provides an interactive editor for users to fine-tune animations.

**Key Contributions:**

	1. Introduction of an LLM-powered animation authoring tool for map animations.
	2. Integration of geospatial querying and interactive editing features.
	3. Usability studies demonstrating improved animation creation workflows.

**Result:** The evaluation showed that users can create map animations more easily and iteratively, enhancing creativity and reducing barriers to entry.

**Limitations:** 

**Conclusion:** MapStory improves the animation creation process, making it accessible and efficient for users without extensive technical backgrounds.

**Abstract:** We introduce MapStory, an LLM-powered animation authoring tool that generates editable map animation sequences directly from natural language text. Given a user-written script, MapStory leverages an agentic architecture to automatically produce a scene breakdown, which decomposes the script into key animation building blocks such as camera movements, visual highlights, and animated elements. Our system includes a researcher component that accurately queries geospatial information by leveraging an LLM with web search, enabling the automatic extraction of relevant regions, paths, and coordinates while allowing users to edit and query for changes or additional information to refine the results. Additionally, users can fine-tune parameters of these blocks through an interactive timeline editor. We detail the system's design and architecture, informed by formative interviews with professional animators and an analysis of 200 existing map animation videos. Our evaluation, which includes expert interviews (N=5) and a usability study (N=12), demonstrates that MapStory enables users to create map animations with ease, facilitates faster iteration, encourages creative exploration, and lowers barriers to creating map-centric stories.

</details>


### [20] [Generative Framework for Personalized Persuasion: Inferring Causal, Counterfactual, and Latent Knowledge](https://arxiv.org/abs/2504.13904)

*Donghuo Zeng, Roberto Legaspi, Yuewen Sun, Xinshuai Dong, Kazushi Ikeda, Peter Spirtes, Kun Zhang*

**Main category:** cs.HC

**Keywords:** causal inference, counterfactual reasoning, persuasive dialogue systems, causal discovery, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper explores using causal and counterfactual knowledge to enhance system responses in persuasive dialogue systems, demonstrating improved outcomes through personalized strategies derived from causal relationships.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of system responses in human-computer interactions by utilizing causal knowledge and counterfactual reasoning.

**Method:** The authors implement causal discovery to identify relationships between user and system utterances, allowing for the generation of personalized counterfactual dialogues and optimization of response policies.

**Key Contributions:**

	1. Introduction of causal discovery for analyzing user-system interactions
	2. Development of personalized counterfactual dialogues based on identified causal relationships
	3. Demonstrated effectiveness of optimized system response policies using real-world data

**Result:** The application of these methods on a real-world dataset shows significant improvements in persuasive system outcomes, with increased cumulative rewards.

**Limitations:** 

**Conclusion:** Causal discovery effectively informs personalized counterfactual inference and optimizes dialogue strategies, enhancing user-system interaction outcomes in persuasive settings.

**Abstract:** We hypothesize that optimal system responses emerge from adaptive strategies grounded in causal and counterfactual knowledge. Counterfactual inference allows us to create hypothetical scenarios to examine the effects of alternative system responses. We enhance this process through causal discovery, which identifies the strategies informed by the underlying causal structure that govern system behaviors. Moreover, we consider the psychological constructs and unobservable noises that might be influencing user-system interactions as latent factors. We show that these factors can be effectively estimated. We employ causal discovery to identify strategy-level causal relationships among user and system utterances, guiding the generation of personalized counterfactual dialogues. We model the user utterance strategies as causal factors, enabling system strategies to be treated as counterfactual actions. Furthermore, we optimize policies for selecting system responses based on counterfactual data. Our results using a real-world dataset on social good demonstrate significant improvements in persuasive system outcomes, with increased cumulative rewards validating the efficacy of causal discovery in guiding personalized counterfactual inference and optimizing dialogue policies for a persuasive dialogue system.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [21] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)

*Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, Sheng Liu*

**Main category:** cs.CL

**Keywords:** multimodal, large language models, hallucination, reasoning chains, visual grounding

**Relevance Score:** 8

**TL;DR:** The paper investigates the trade-off between reasoning ability and hallucination in multimodal large language models, introducing a new metric and benchmark to evaluate visual grounding during reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucination in longer reasoning chains generated by multimodal large language models, and to evaluate how well these models retain visual grounding during complex reasoning tasks.

**Method:** The paper introduces a metric called RH-AUC to quantify changes in perception accuracy with reasoning length and presents RH-Bench, a diagnostic benchmark designed for various multimodal tasks.

**Key Contributions:**

	1. Introduction of RH-AUC metric for evaluating reasoning length impact on perception accuracy.
	2. Release of RH-Bench, a benchmark for assessing multimodal tasks and their trade-offs.
	3. Analysis of how model size and training data type influence reasoning and perception balance.

**Result:** The analysis shows that larger models achieve a better balance between reasoning quality and perception fidelity, influenced more by the type of training data than by its volume.

**Limitations:** 

**Conclusion:** The findings highlight the need for evaluation frameworks that consider both reasoning and perceptual fidelity to mitigate hallucination in multimodal models.

**Abstract:** Test-time compute has empowered multimodal large language models to generate extended reasoning chains, yielding strong performance on tasks such as multimodal math reasoning. However, this improved reasoning ability often comes with increased hallucination: as generations become longer, models tend to drift away from image-grounded content and rely more heavily on language priors. Attention analysis shows that longer reasoning chains lead to reduced focus on visual inputs, which contributes to hallucination. To systematically study this phenomenon, we introduce RH-AUC, a metric that quantifies how a model's perception accuracy changes with reasoning length, allowing us to evaluate whether the model preserves visual grounding during reasoning. We also release RH-Bench, a diagnostic benchmark that spans a variety of multimodal tasks, designed to assess the trade-off between reasoning ability and hallucination. Our analysis reveals that (i) larger models typically achieve a better balance between reasoning and perception, and (ii) this balance is influenced more by the types and domains of training data than by its overall volume. These findings underscore the importance of evaluation frameworks that jointly consider both reasoning quality and perceptual fidelity.

</details>


### [22] [Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use](https://arxiv.org/abs/2505.21578)

*Titouan Parcollet, Yuan Tseng, Shucong Zhang, Rogier van Dalen*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, dataset, Loquacious Set, speech diversity, industry research

**Relevance Score:** 7

**TL;DR:** The Loquacious Set is a new 25,000-hour speech dataset aimed at improving automatic speech recognition (ASR) by addressing the limitations of existing datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current ASR datasets are insufficient for real-world applications due to limitations in size, diversity, and commercial usability.

**Method:** The authors present the Loquacious Set, which is a curated collection of diverse English speech, including various speakers and speech types, totaling 25,000 hours of audio.

**Key Contributions:**

	1. Introduction of the Loquacious Set dataset
	2. Diverse collection of 25,000 hours of English speech
	3. Commercially usable resources for industry researchers

**Result:** The Loquacious Set offers a resource for building ASR systems that can handle different accents and noisy environments, significantly enhancing ASR research.

**Limitations:** 

**Conclusion:** This dataset is intended to facilitate better comparisons and evaluations for ASR systems in both academia and industry.

**Abstract:** Automatic speech recognition (ASR) research is driven by the availability of common datasets between industrial researchers and academics, encouraging comparisons and evaluations. LibriSpeech, despite its long success as an ASR benchmark, is now limited by its size and focus on clean, read speech, leading to near-zero word error rates. More recent datasets, including MOSEL, YODAS, Gigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations including licenses that researchers in the industry cannot use, unreliable transcriptions, incorrect audio data, or the lack of evaluation sets. This work presents the Loquacious Set, a 25,000-hour curated collection of commercially usable English speech. Featuring hundreds of thousands of speakers with diverse accents and a wide range of speech types (read, spontaneous, talks, clean, noisy), the Loquacious Set is designed to work for academics and researchers in the industry to build ASR systems in real-world scenarios.

</details>


### [23] [Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives](https://arxiv.org/abs/2505.21598)

*Yajiao Liu, Congliang Chen, Junchi Yang, Ruoyu Sun*

**Main category:** cs.CL

**Keywords:** large language models, data mixture methods, domain sampling

**Relevance Score:** 8

**TL;DR:** This paper discusses the impact of domain sampling proportions on the performance of large language models and categorizes existing data mixture methods to optimize model training within fixed resource constraints.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance on downstream tasks by determining optimal domain weights for training large language models using data from various domains.

**Method:** The paper categorizes existing data mixture methods into offline and online approaches, further detailing subcategories such as heuristic-based and online min-max optimization methods, and outlines the problem formulations and algorithms for each subtype.

**Key Contributions:**

	1. Fine-grained categorization of data mixture methods.
	2. Summary of problem formulations and representative algorithms.
	3. Discussion of advantages, disadvantages, and challenges in the data mixture field.

**Result:** A comprehensive overview of data mixture methods is provided, along with categorization, advantages and disadvantages, and key challenges faced in optimizing domain sampling for model training.

**Limitations:** 

**Conclusion:** Identifying the best strategies for domain weight assignment in data mixtures is crucial for maximizing the performance of large language models under defined computational limits.

**Abstract:** Training large language models with data collected from various domains can improve their performance on downstream tasks. However, given a fixed training budget, the sampling proportions of these different domains significantly impact the model's performance. How can we determine the domain weights across different data domains to train the best-performing model within constrained computational resources? In this paper, we provide a comprehensive overview of existing data mixture methods. First, we propose a fine-grained categorization of existing methods, extending beyond the previous offline and online classification. Offline methods are further grouped into heuristic-based, algorithm-based, and function fitting-based methods. For online methods, we categorize them into three groups: online min-max optimization, online mixing law, and other approaches by drawing connections with the optimization frameworks underlying offline methods. Second, we summarize the problem formulations, representative algorithms for each subtype of offline and online methods, and clarify the relationships and distinctions among them. Finally, we discuss the advantages and disadvantages of each method and highlight key challenges in the field of data mixture.

</details>


### [24] [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)

*Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Small Language Models, token routing

**Relevance Score:** 9

**TL;DR:** Roads to Rome (R2R) is a method that uses Large Language Models (LLMs) for critical tokens while employing Small Language Models (SLMs) for most token generation, significantly improving performance and efficiency in reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high inference costs of LLMs while maintaining performance through more efficient models like SLMs.

**Method:** The R2R method identifies critical, path-divergent tokens where LLMs excel and uses them selectively, while a majority of tokens are generated using an SLM. An automatic data generation pipeline is developed to label these divergent tokens for training.

**Key Contributions:**

	1. Introduction of Roads to Rome (R2R) methodology
	2. Improved accuracy with a smaller parameter size
	3. Development of an automatic data generation pipeline for token routing

**Result:** R2R achieved 1.6x higher average accuracy than R1-7B and outperformed R1-14B, while offering a 2.8x speedup compared to R1-32B with comparable performance.

**Limitations:** 

**Conclusion:** R2R improves efficiency and accuracy in ML tasks by optimizing how reasoning paths are handled between LLMs and SLMs.

**Abstract:** Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.

</details>


### [25] [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)

*Miao Peng, Nuo Chen, Jianheng Tang, Jia Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, misinformation, benchmark, knowledge conflicts, Reconstruct to Discriminate

**Relevance Score:** 9

**TL;DR:** MisBench is a comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation, consisting of over 10 million examples of misinformation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the influence of misinformation on Large Language Models (LLMs) and improve their ability to detect it.

**Method:** Development of MisBench, the largest benchmark to evaluate LLMs with 10,346,712 examples of misinformation, considering knowledge conflicts and stylistic variations.

**Key Contributions:**

	1. Introduction of MisBench, a comprehensive misinformation benchmark for LLMs
	2. Empirical analysis of LLMs' susceptibility to misinformation
	3. Proposal of the RtD approach to improve misinformation detection capabilities.

**Result:** Empirical results show that while LLMs can discern misinformation, they are still affected by knowledge conflicts and stylistic variations.

**Limitations:** The benchmark may not cover all variations of misinformation and its effectiveness in diverse real-world scenarios is yet to be validated.

**Conclusion:** The study proposes the Reconstruct to Discriminate (RtD) approach to enhance LLMs' capabilities in detecting misinformation, offering insights and a benchmark for real-world applications.

**Abstract:** Large Language Models (LLMs) have shown remarkable capabilities in knowledge-intensive tasks, while they remain vulnerable when encountering misinformation. Existing studies have explored the role of LLMs in combating misinformation, but there is still a lack of fine-grained analysis on the specific aspects and extent to which LLMs are influenced by misinformation. To bridge this gap, we present MisBench, the current largest and most comprehensive benchmark for evaluating LLMs' behavior and knowledge preference toward misinformation. MisBench consists of 10,346,712 pieces of misinformation, which uniquely considers both knowledge-based conflicts and stylistic variations in misinformation. Empirical results reveal that while LLMs demonstrate comparable abilities in discerning misinformation, they still remain susceptible to knowledge conflicts and stylistic variations. Based on these findings, we further propose a novel approach called Reconstruct to Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our study provides valuable insights into LLMs' interactions with misinformation, and we believe MisBench can serve as an effective benchmark for evaluating LLM-based detectors and enhancing their reliability in real-world applications. Codes and data are available at https://github.com/GKNL/MisBench.

</details>


### [26] [Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts](https://arxiv.org/abs/2505.21646)

*Lei Zhang, Markus Stricker*

**Main category:** cs.CL

**Keywords:** materials discovery, machine learning, Word2Vec, corpus refinement, electrocatalysis

**Relevance Score:** 4

**TL;DR:** An iterative framework that refines scientific corpora and predicts high-performing materials for electrocatalytic reactions using Word2Vec models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the combinatorial explosion in material discovery and optimize candidate selection using diverse data sources including scientific texts.

**Method:** Iterative framework involving strategic document selection, training of Word2Vec models, and monitoring composition-property correlations in embedding space.

**Key Contributions:**

	1. Iterative corpus refinement for material discovery
	2. Integration of Word2Vec models with scientific texts
	3. Successful prediction of high-performing electrocatalytic materials

**Result:** Successfully predicts the highest performing compositions for electrocatalytic reactions (ORR, HER, OER) validated by lab experiments.

**Limitations:** 

**Conclusion:** The proposed method demonstrates the potential for accelerating materials discovery and optimization in areas with scarce data.

**Abstract:** The discovery and optimization of materials for specific applications is hampered by the practically infinite number of possible elemental combinations and associated properties, also known as the `combinatorial explosion'. By nature of the problem, data are scarce and all possible data sources should be used. In addition to simulations and experimental results, the latent knowledge in scientific texts is not yet used to its full potential. We present an iterative framework that refines a given scientific corpus by strategic selection of the most diverse documents, training Word2Vec models, and monitoring the convergence of composition-property correlations in embedding space. Our approach is applied to predict high-performing materials for oxygen reduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions for a large number of possible candidate compositions. Our method successfully predicts the highest performing compositions among a large pool of candidates, validated by experimental measurements of the electrocatalytic performance in the lab. This work demonstrates and validates the potential of iterative corpus refinement to accelerate materials discovery and optimization, offering a scalable and efficient tool for screening large compositional spaces where reliable data are scarce or non-existent.

</details>


### [27] [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)

*Zeinab Dehghani, Koorosh Aslansefat, Adil Khan, Mohammed Naveed Akram*

**Main category:** cs.CL

**Keywords:** Large Language Models, Interpretability, Transparency, AI Trustworthiness

**Relevance Score:** 9

**TL;DR:** SMILE is a model-agnostic method introduced to explain LLM responses by analyzing input changes and generating visual heat maps for transparency in AI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance transparency and trust in large language models by understanding their response mechanisms, especially in sensitive fields where accountability is crucial.

**Method:** SMILE analyzes how output varies with slight modifications to the input, identifying impactful words, and generating visual heat maps to illustrate results.

**Key Contributions:**

	1. Introduction of a model-agnostic explanation method for LLMs
	2. Establishment of clear evaluation metrics for explanation quality
	3. Visualization of significant input components through heat maps

**Result:** SMILE was tested on several leading LLMs, demonstrating improvements in clarity and reliability of explanations through various metrics.

**Limitations:** 

**Conclusion:** SMILE contributes to making AI models more interpretable, fostering transparency and trust in AI applications.

**Abstract:** Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy.

</details>


### [28] [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)

*Rahul Raman, Khushi Sharma, Sai Qian Zhang*

**Main category:** cs.CL

**Keywords:** large language models, outliers, quantization, model performance, activation

**Relevance Score:** 8

**TL;DR:** This paper investigates outliers in large language models (LLMs) that affect performance due to quantization errors and proposes strategies for mitigation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Identifying and addressing outliers in LLMs is essential for improving quantization, which directly influences model accuracy and efficiency, especially for deployment on edge devices.

**Method:** The paper conducts a comprehensive investigation into the formation mechanisms of outliers in LLMs and proposes strategies to mitigate their occurrence, focusing on massive activations and channel-wise outliers.

**Key Contributions:**

	1. In-depth analysis of the formation mechanisms of outliers in LLMs.
	2. Proposed strategies that effectively reduce the occurrence of outliers.
	3. Practical approaches for quantization with minimized accuracy loss.

**Result:** The proposed strategies effectively eliminate most massive activations and channel-wise outliers with minimal impact on model accuracy.

**Limitations:** 

**Conclusion:** Mitigating outliers in LLMs can significantly enhance quantization outcomes, facilitating better deployment on specialized hardware.

**Abstract:** Investigating outliers in large language models (LLMs) is crucial due to their significant impact on various aspects of LLM performance, including quantization and compression. Outliers often cause considerable quantization errors, leading to degraded model performance. Identifying and addressing these outliers can enhance the accuracy and efficiency of the quantization process, enabling smoother deployment on edge devices or specialized hardware. Recent studies have identified two common types of outliers in LLMs: massive activations and channel-wise outliers. While numerous quantization algorithms have been proposed to mitigate their effects and maintain satisfactory accuracy, few have thoroughly explored the root causes of these outliers in depth. In this paper, we conduct a comprehensive investigation into the formation mechanisms of these outliers and propose potential strategies to mitigate their occurrence. Ultimately, we introduce some efficient approaches to eliminate most massive activations and channel-wise outliers with minimal impact on accuracy.

</details>


### [29] [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)

*Avijit Gayen, Somyajit Chakraborty, Mainak Sen, Soham Paul, Angshuman Jana*

**Main category:** cs.CL

**Keywords:** Legal Petition Ranking, Machine Learning, Legal Informatics

**Relevance Score:** 4

**TL;DR:** The paper proposes LLMPR, an automated framework for ranking legal petitions using LLMs and machine learning to improve judicial efficiency and fairness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies and biases in manual prioritization of legal petitions in the Indian judiciary, leading to delays in justice delivery.

**Method:** The framework uses transfer learning and machine learning techniques, processing unstructured legal text from the ILDC dataset, extracting features through embeddings like DistilBERT and legal features for training models such as Random Forest and XGBoost.

**Key Contributions:**

	1. Introduction of LLMPR for automated legal petition ranking
	2. Demonstrated high accuracy using machine learning models
	3. Highlighting the marginal gains of LLM embeddings compared to numerical features

**Result:** The experiments show that Random Forest and Decision Tree models achieve over 99% accuracy and a Spearman correlation of 0.99, with numerical features alone yielding optimal results.

**Limitations:** 

**Conclusion:** LLMPR can enhance judicial workflows, effectively reduce case backlogs, and promote fairness in the prioritization of legal petitions.

**Abstract:** The persistent accumulation of unresolved legal cases, especially within the Indian judiciary, significantly hampers the timely delivery of justice. Manual methods of prioritizing petitions are often prone to inefficiencies and subjective biases further exacerbating delays. To address this issue, we propose LLMPR (Large Language Model-based Petition Ranking), an automated framework that utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency. Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process unstructured legal text and extract features through various embedding techniques, including DistilBERT, LegalBERT, and MiniLM. These textual embeddings are combined with quantitative indicators such as gap days, rank scores, and word counts to train multiple machine learning models, including Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments demonstrate that Random Forest and Decision Tree models yield superior performance, with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Notably, models using only numerical features achieve nearly optimal ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer only marginal gains. These findings suggest that automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization.

</details>


### [30] [MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs](https://arxiv.org/abs/2505.21693)

*Raoyuan Zhao, Beiduo Chen, Barbara Plank, Michael A. Hedderich*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Awareness, Evaluation Framework, Cross-lingual, Multilingual

**Relevance Score:** 7

**TL;DR:** Introduction of MAKIEval, a framework evaluating cultural awareness in LLMs across languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in assessing cross-lingual disparities and cultural awareness due to English-centric pretraining of LLMs and limited multilingual benchmarks.

**Method:** MAKIEval utilizes Wikidata's multilingual structure to automate the evaluation of cultural entities in LLM outputs, providing language-agnostic scoring across various cultural contexts.

**Key Contributions:**

	1. Introduction of MAKIEval as a multilingual evaluation framework for LLMs.
	2. Development of four new metrics for assessing cultural awareness.
	3. Public release of code and data for further research.

**Result:** Evaluation of 7 LLMs across 13 languages and 19 regions revealed that these models displayed greater cultural awareness in English.

**Limitations:** Evaluation is limited to the LLMs studied; further research is needed to generalize findings across more languages and contexts.

**Conclusion:** MAKIEval offers a scalable solution for evaluating cultural awareness of LLMs and highlights the English-centric bias in LLM outputs.

**Abstract:** Large language models (LLMs) are used globally across many languages, but their English-centric pretraining raises concerns about cross-lingual disparities for cultural awareness, often resulting in biased outputs. However, comprehensive multilingual evaluation remains challenging due to limited benchmarks and questionable translation quality. To better assess these disparities, we introduce MAKIEval, an automatic multilingual framework for evaluating cultural awareness in LLMs across languages, regions, and topics. MAKIEval evaluates open-ended text generation, capturing how models express culturally grounded knowledge in natural language. Leveraging Wikidata's multilingual structure as a cross-lingual anchor, it automatically identifies cultural entities in model outputs and links them to structured knowledge, enabling scalable, language-agnostic evaluation without manual annotation or translation. We then introduce four metrics that capture complementary dimensions of cultural awareness: granularity, diversity, cultural specificity, and consensus across languages. We assess 7 LLMs developed from different parts of the world, encompassing both open-source and proprietary systems, across 13 languages, 19 countries and regions, and 6 culturally salient topics (e.g., food, clothing). Notably, we find that models tend to exhibit stronger cultural awareness in English, suggesting that English prompts more effectively activate culturally grounded knowledge. We publicly release our code and data.

</details>


### [31] [Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing](https://arxiv.org/abs/2505.21701)

*Raoyuan Zhao, Abdullatif Köksal, Ali Modarressi, Michael A. Hedderich, Hinrich Schütze*

**Main category:** cs.CL

**Keywords:** large language models, knowledge gaps, probing methods, hallucination, evaluation metrics

**Relevance Score:** 9

**TL;DR:** The paper critiques existing probing methods for large language models (LLMs), revealing significant inconsistencies in identifying knowledge gaps due to input variations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Highlight the unreliability of large language models caused by hallucinations and emphasize the need for better methods to identify knowledge gaps.

**Method:** Proposes a new evaluation process using input variations and quantitative metrics to test probing methods.

**Key Contributions:**

	1. Identified intra-method and cross-method inconsistencies in knowledge gap probing.
	2. Proposed a new evaluation process for probing methods using input variations.
	3. Challenged the reliability of existing probing techniques for LLMs.

**Result:** Demonstrated significant intra-method and cross-method inconsistencies in knowledge gap detection, with agreement dropping to around 40% and decision consistency as low as 7%.

**Limitations:** The study focuses mainly on probing inconsistencies without proposing a complete solution for robust probing frameworks.

**Conclusion:** Highlights the inadequacy of current probing methods and calls for the development of perturbation-robust probing frameworks.

**Abstract:** The reliability of large language models (LLMs) is greatly compromised by their tendency to hallucinate, underscoring the need for precise identification of knowledge gaps within LLMs. Various methods for probing such gaps exist, ranging from calibration-based to prompting-based methods. To evaluate these probing methods, in this paper, we propose a new process based on using input variations and quantitative metrics. Through this, we expose two dimensions of inconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal non-semantic perturbations in prompts lead to considerable variance in detected knowledge gaps within the same probing method; e.g., the simple variation of shuffling answer options can decrease agreement to around 40%. (2) Cross-method inconsistency: Probing methods contradict each other on whether a model knows the answer. Methods are highly inconsistent -- with decision consistency across methods being as low as 7% -- even though the model, dataset, and prompt are all the same. These findings challenge existing probing methods and highlight the urgent need for perturbation-robust probing frameworks.

</details>


### [32] [Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study](https://arxiv.org/abs/2505.21710)

*Barbarestani Baran, Maks Isa, Vossen Piek*

**Main category:** cs.CL

**Keywords:** ChatGPT, content moderation, natural language processing, AI in health

**Relevance Score:** 8

**TL;DR:** This study evaluates ChatGPT's effectiveness in detecting inappropriate language in online comments compared to crowd-sourced and expert evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing challenge of moderating vast amounts of user-generated content on social networks highlights the need for effective AI solutions in content moderation.

**Method:** The study compared ChatGPT's performance in identifying inappropriate content against crowd-sourced annotations and expert evaluations.

**Key Contributions:**

	1. Evaluation of ChatGPT in content moderation
	2. Identification of areas for improvement in AI language detection
	3. Demonstration of iterative refinements enhancing model accuracy

**Result:** ChatGPT performs well in detecting inappropriate content, showing improvements in accuracy with iterative refinements, especially in Version 6, but exhibited variability in targeting language detection with higher false positive rates than experts.

**Limitations:** Variability in performance for targeting language detection and higher false positive rates than expert judgments.

**Conclusion:** The study demonstrates the potential of AI models like ChatGPT for enhancing automated content moderation while emphasizing ongoing improvements for better contextual understanding.

**Abstract:** This study evaluates the effectiveness of ChatGPT, an advanced AI model for natural language processing, in identifying targeting and inappropriate language in online comments. With the increasing challenge of moderating vast volumes of user-generated content on social network sites, the role of AI in content moderation has gained prominence. We compared ChatGPT's performance against crowd-sourced annotations and expert evaluations to assess its accuracy, scope of detection, and consistency. Our findings highlight that ChatGPT performs well in detecting inappropriate content, showing notable improvements in accuracy through iterative refinements, particularly in Version 6. However, its performance in targeting language detection showed variability, with higher false positive rates compared to expert judgments. This study contributes to the field by demonstrating the potential of AI models like ChatGPT to enhance automated content moderation systems while also identifying areas for further improvement. The results underscore the importance of continuous model refinement and contextual understanding to better support automated moderation and mitigate harmful online behavior.

</details>


### [33] [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)

*Marvin Limpijankit, Yanda Chen, Melanie Subbiah, Nicholas Deas, Kathleen McKeown*

**Main category:** cs.CL

**Keywords:** Counterfactual Simulatability, LLM Explanations, News Summarization, Medical Suggestion, Skill-based Tasks

**Relevance Score:** 8

**TL;DR:** The paper presents a framework for evaluating how well explanations of LLM outputs allow users to predict results on counterfactuals, specifically in generation tasks such as news summarization and medical suggestions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The unpredictability of LLM outputs requires reliable explanations, particularly in high-stakes areas like health informatics.

**Method:** The authors extend counterfactual simulatability evaluation methods previously used for yes/no tasks to generation tasks, focusing on news summarization and medical suggestion.

**Key Contributions:**

	1. Introduction of a framework for counterfactual simulatability in generation tasks.
	2. Empirical findings on the effectiveness of LLM explanations in different contexts.
	3. Insights on the suitability of counterfactual simulatability for skill versus knowledge-based tasks.

**Result:** LLM explanations help in predicting outputs for summarization tasks but show limited effectiveness for medical suggestions, highlighting the need for improvements.

**Limitations:** The study focuses on two specific application cases and may not generalize across all LLM applications.

**Conclusion:** Counterfactual simulatability may be more suitable for evaluating skill-based tasks rather than knowledge-based tasks.

**Abstract:** LLMs can be unpredictable, as even slight alterations to the prompt can cause the output to change in unexpected ways. Thus, the ability of models to accurately explain their behavior is critical, especially in high-stakes settings. One approach for evaluating explanations is counterfactual simulatability, how well an explanation allows users to infer the model's output on related counterfactuals. Counterfactual simulatability has been previously studied for yes/no question answering tasks. We provide a general framework for extending this method to generation tasks, using news summarization and medical suggestion as example use cases. We find that while LLM explanations do enable users to better predict LLM outputs on counterfactuals in the summarization setting, there is significant room for improvement for medical suggestion. Furthermore, our results suggest that the evaluation for counterfactual simulatability may be more appropriate for skill-based tasks as opposed to knowledge-based tasks.

</details>


### [34] [BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum](https://arxiv.org/abs/2505.21757)

*Yubin Kim, Zhiyuan Hu, Hyewon Jeong, Eugene Park, Shuyue Stella Li, Chanwoo Park, Shiyun Xiong, MingYu Lu, Hyeonhoon Lee, Xin Liu, Daniel McDuff, Cynthia Breazeal, Samir Tulebaev, Hae Won Park*

**Main category:** cs.CL

**Keywords:** Large Language Models, clinical agents, BehaviorBench, BehaviorSFT, proactive engagement

**Relevance Score:** 9

**TL;DR:** Introducing BehaviorBench, a dataset for evaluating LLM behaviors in clinical contexts, and BehaviorSFT, a training method to enhance LLM proactivity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the proactive engagement of LLMs in clinical settings, addressing their inconsistencies by evaluating their performance in identifying critical missing information and risks.

**Method:** BehaviorBench is a dataset designed to assess LLM behaviors across a spectrum of clinical assistance, while BehaviorSFT is a training strategy that uses behavioral tokens to condition LLMs for improved behavioral selection.

**Key Contributions:**

	1. Introduction of BehaviorBench for LLM behavioral evaluation in clinical settings
	2. Development of BehaviorSFT for improved dynamic behavioral selection
	3. Demonstration of enhanced LLM proactivity in clinical scenarios through empirical evaluations

**Result:** BehaviorSFT significantly boosts the performance of LLMs on BehaviorBench, achieving up to 97.3% Macro F1 score and enhancing proactive task performance, confirmed by blind clinician evaluations.

**Limitations:** 

**Conclusion:** BehaviorSFT results in LLMs exhibiting more realistic clinical behaviors, effectively balancing helpful proactivity and necessary restraint compared to traditional fine-tuning.

**Abstract:** Large Language Models (LLMs) as clinical agents require careful behavioral adaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs often struggle with proactive engagement, like unprompted identification of critical missing information or risks. We introduce BehaviorBench, a comprehensive dataset to evaluate agent behaviors across a clinical assistance spectrum, ranging from reactive query responses to proactive interventions (e.g., clarifying ambiguities, flagging overlooked critical data). Our BehaviorBench experiments reveal LLMs' inconsistent proactivity. To address this, we propose BehaviorSFT, a novel training strategy using behavioral tokens to explicitly condition LLMs for dynamic behavioral selection along this spectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro F1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to 96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed BehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a superior balance between helpful proactivity (e.g., timely, relevant suggestions) and necessary restraint (e.g., avoiding over-intervention) versus standard fine-tuning or explicit instructed agents.

</details>


### [35] [Calibrating LLM Confidence by Probing Perturbed Representation Stability](https://arxiv.org/abs/2505.21772)

*Reza Khanmohammadi, Erfan Miahi, Mehrsa Mardikoraem, Simerjot Kaur, Ivan Brugere, Charese H. Smiley, Kundan Thind, Mohammad M. Ghassemi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Confidence Estimation, Adversarial Perturbations, Calibration

**Relevance Score:** 9

**TL;DR:** CCPS is a new method for improving confidence estimation in Large Language Models by analyzing their internal representational stability through adversarial perturbations, leading to significant performance improvements over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Miscalibration in LLMs undermines their reliability, necessitating improved methods for accurate confidence estimation.

**Method:** CCPS analyzes internal representational stability in LLMs by applying targeted adversarial perturbations to hidden states, extracting features to predict answer correctness with a lightweight classifier.

**Key Contributions:**

	1. Introduction of CCPS method for calibrating LLM confidence
	2. Significant reduction in calibration error compared to previous methods
	3. Broad applicability across various LLM architectures

**Result:** CCPS reduces Expected Calibration Error by 55% and Brier score by 21%, while increasing accuracy by 5 percentage points and relevant AUC metrics by 4 to 6 percentage points across evaluated benchmarks.

**Limitations:** 

**Conclusion:** CCPS provides an efficient and accurate solution for LLM confidence estimation, enhancing their trustworthiness in applications.

**Abstract:** Miscalibration in Large Language Models (LLMs) undermines their reliability, highlighting the need for accurate confidence estimation. We introduce CCPS (Calibrating LLM Confidence by Probing Perturbed Representation Stability), a novel method analyzing internal representational stability in LLMs. CCPS applies targeted adversarial perturbations to final hidden states, extracts features reflecting the model's response to these perturbations, and uses a lightweight classifier to predict answer correctness. CCPS was evaluated on LLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral architectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and open-ended formats. Our results show that CCPS significantly outperforms current approaches. Across four LLMs and three MMLU variants, CCPS reduces Expected Calibration Error by approximately 55% and Brier score by 21%, while increasing accuracy by 5 percentage points, Area Under the Precision-Recall Curve by 4 percentage points, and Area Under the Receiver Operating Characteristic Curve by 6 percentage points, all relative to the strongest prior method. CCPS delivers an efficient, broadly applicable, and more accurate solution for estimating LLM confidence, thereby improving their trustworthiness.

</details>


### [36] [GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task](https://arxiv.org/abs/2505.21781)

*Chutong Meng, Antonios Anastasopoulos*

**Main category:** cs.CL

**Keywords:** speech translation, low-resource languages, ASR, MT, IWSLT 2025

**Relevance Score:** 4

**TL;DR:** This paper presents the GMU systems for low-resource speech translation, detailing techniques used and key findings from IWSLT 2025.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop effective speech translation systems for low-resource languages in the IWSLT 2025 shared task.

**Method:** We fine-tuned SeamlessM4T-v2 for ASR, MT, and E2E ST, and explored various fine-tuning paradigms including direct E2E, multi-task training, and parameter initialization from ASR/MT models.

**Key Contributions:**

	1. Developed systems for all language pairs except Levantine Arabic
	2. Explored various training paradigms resulting in improved performance
	3. Demonstrated the effectiveness of model initialization techniques

**Result:** Direct E2E fine-tuning yields strong results; initializing with a fine-tuned ASR encoder improves ST for untrained languages; multi-task training provides slight benefits.

**Limitations:** 

**Conclusion:** Direct E2E fine-tuning is robust, and leveraging ASR models enhances performance on languages with limited resources.

**Abstract:** This paper describes the GMU systems for the IWSLT 2025 low-resource speech translation shared task. We trained systems for all language pairs, except for Levantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition (ASR), machine translation (MT), and end-to-end speech translation (E2E ST). The ASR and MT models are also used to form cascaded ST systems. Additionally, we explored various training paradigms for E2E ST fine-tuning, including direct E2E fine-tuning, multi-task training, and parameter initialization using components from fine-tuned ASR and/or MT models. Our results show that (1) direct E2E fine-tuning yields strong results; (2) initializing with a fine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has not been trained on; (3) multi-task training can be slightly helpful.

</details>


### [37] [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)

*Dasha Metropolitansky, Jonathan Larson*

**Main category:** cs.CL

**Keywords:** closed-domain hallucination, veriTrail, language models, traceability, multi-generative processes

**Relevance Score:** 9

**TL;DR:** This paper introduces VeriTrail, a novel method for detecting closed-domain hallucinations in language models, focusing on traceability in multi-generative step processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the issue of closed-domain hallucinations in language models, especially in complex multi-generative step processes where tracing content misalignment is crucial.

**Method:** The authors present VeriTrail, which detects hallucinations in language models and provides traceability for both multi-generative steps (MGS) and single generative steps (SGS). They also introduce datasets that include intermediate outputs along with human annotations.

**Key Contributions:**

	1. Introduction of VeriTrail for closed-domain hallucination detection
	2. Development of datasets with intermediate outputs and faithfulness annotations
	3. Demonstration of improved performance over baseline methods.

**Result:** VeriTrail outperforms existing baseline methods in detecting hallucinations and tracing the sources of unsubstantiated content in both multi-generative and single generative processes.

**Limitations:** 

**Conclusion:** The study underscores the importance not just of detecting hallucinations but also of understanding how they originated throughout the generative process.

**Abstract:** Even when instructed to adhere to source material, Language Models often generate unsubstantiated content - a phenomenon known as "closed-domain hallucination." This risk is amplified in processes with multiple generative steps (MGS), compared to processes with a single generative step (SGS). However, due to the greater complexity of MGS processes, we argue that detecting hallucinations in their final outputs is necessary but not sufficient: it is equally important to trace where hallucinated content was likely introduced and how faithful content may have been derived from the source through intermediate outputs. To address this need, we present VeriTrail, the first closed-domain hallucination detection method designed to provide traceability for both MGS and SGS processes. We also introduce the first datasets to include all intermediate outputs as well as human annotations of final outputs' faithfulness for their respective MGS processes. We demonstrate that VeriTrail outperforms baseline methods on both datasets.

</details>


### [38] [Revisiting Common Assumptions about Arabic Dialects in NLP](https://arxiv.org/abs/2505.21816)

*Amr Keleg, Sharon Goldwater, Walid Magdy*

**Main category:** cs.CL

**Keywords:** Arabic dialects, Natural Language Processing, Dialect Identification, NLP assumptions, Multi-label dataset

**Relevance Score:** 6

**TL;DR:** The paper critiques commonly held assumptions about the classification of Arabic dialects in NLP, highlighting their oversimplification and potential inaccuracies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To verify widespread assumptions in the NLP literature regarding the classification of Arabic dialects and assess their impact on tasks like Arabic Dialect Identification (ADI).

**Method:** The authors analyze a multi-label dataset with manual assessments of sentence validity in 11 different country-level dialects, focusing on four widely accepted assumptions.

**Key Contributions:**

	1. Critique of four common assumptions regarding Arabic dialects in NLP.
	2. Development of a multi-label dataset with manual assessments by native speakers.
	3. Implications for improving Arabic NLP tasks and dialect identification.

**Result:** The analysis reveals that the four assumptions oversimplify the complexity of Arabic dialects and are not always valid, suggesting that they hinder progress in Arabic NLP tasks.

**Limitations:** The study is limited to 11 dialects and may not account for all variations within Arabic dialects.

**Conclusion:** Reevaluating these assumptions is crucial for advancing computational tasks related to Arabic dialects in NLP.

**Abstract:** Arabic has diverse dialects, where one dialect can be substantially different from the others. In the NLP literature, some assumptions about these dialects are widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable regional dialects") and are manifested in different computational tasks such as Arabic Dialect Identification (ADI). However, these assumptions are not quantitatively verified. We identify four of these assumptions and examine them by extending and analyzing a multi-label dataset, where the validity of each sentence in 11 different country-level dialects is manually assessed by speakers of these dialects. Our analysis indicates that the four assumptions oversimplify reality, and some of them are not always accurate. This in turn might be hindering further progress in different Arabic NLP tasks.

</details>


### [39] [Representative Language Generation](https://arxiv.org/abs/2505.21819)

*Charlotte Peale, Vinod Raman, Omer Reingold*

**Main category:** cs.CL

**Keywords:** generative models, diversity, bias, combinatorial dimensions, ICML 2025

**Relevance Score:** 8

**TL;DR:** The paper introduces 'representative generation,' enhancing generative model frameworks to tackle diversity and bias, focusing on proportional representation of groups in outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address diversity and bias in generative models, ensuring that outputs reflect the groups of interest from the training data.

**Method:** The authors propose the concept of representative generation, characterizing uniform and non-uniform generation, and introducing the 'group closure dimension' as a combinatorial measure.

**Key Contributions:**

	1. Introduction of representative generation framework for generative models.
	2. Characterization of group closure dimension for measuring diversity in outputs.
	3. Analysis of computational and information-theoretic aspects of representative generation.

**Result:** The paper demonstrates the feasibility of representative generation for infinite hypothesis classes under certain conditions but highlights a computability limitation when using membership queries.

**Limitations:** Negative results for computability using only membership queries limit the model's applicability in certain scenarios.

**Conclusion:** The findings offer a foundational framework for developing more diverse and representative generative models.

**Abstract:** We introduce "representative generation," extending the theoretical framework for generation proposed by Kleinberg et al. (2024) and formalized by Li et al. (2024), to additionally address diversity and bias concerns in generative models. Our notion requires outputs of a generative model to proportionally represent groups of interest from the training data. We characterize representative uniform and non-uniform generation, introducing the "group closure dimension" as a key combinatorial quantity. For representative generation in the limit, we analyze both information-theoretic and computational aspects, demonstrating feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but proving a negative result for computability using only membership queries. This contrasts with Kleinberg et al.'s (2024) positive results for standard generation in the limit. Our findings provide a rigorous foundation for developing more diverse and representative generative models.

</details>


### [40] [Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries](https://arxiv.org/abs/2505.21859)

*Vishakh Padmakumar, Zichao Wang, David Arbour, Jennifer Healey*

**Main category:** cs.CL

**Keywords:** large language models, multi-document summarization, determinantal point processes

**Relevance Score:** 9

**TL;DR:** This paper proposes a three-step approach to enhance source coverage in multi-document summarization using large language models (LLMs), addressing the 'lost in the middle' phenomenon.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper highlights the limitations of current LLMs in multi-document summarization due to their uneven attention across different parts of the context.

**Method:** The proposed method divides the summarization process into three steps: (1) identifying atomic key points from documents, (2) using determinantal point processes (DPP) for diverse content selection, and (3) performing a final rewrite of the summary.

**Key Contributions:**

	1. Three-step framework for summarization
	2. Use of determinantal point processes for content selection
	3. Personalized summarization based on user intent

**Result:** The proposed approach consistently improves source coverage on the DiverseSumm benchmark across various LLMs.

**Limitations:** 

**Conclusion:** Incorporating user intent into the DPP kernel allows for personalized summaries that maintain high coverage of relevant information.

**Abstract:** While large language models (LLMs) are increasingly capable of handling longer contexts, recent work has demonstrated that they exhibit the "lost in the middle" phenomenon (Liu et al., 2024) of unevenly attending to different parts of the provided context. This hinders their ability to cover diverse source material in multi-document summarization, as noted in the DiverseSumm benchmark (Huang et al., 2024). In this work, we contend that principled content selection is a simple way to increase source coverage on this task. As opposed to prompting an LLM to perform the summarization in a single step, we explicitly divide the task into three steps -- (1) reducing document collections to atomic key points, (2) using determinantal point processes (DPP) to perform select key points that prioritize diverse content, and (3) rewriting to the final summary. By combining prompting steps, for extraction and rewriting, with principled techniques, for content selection, we consistently improve source coverage on the DiverseSumm benchmark across various LLMs. Finally, we also show that by incorporating relevance to a provided user intent into the DPP kernel, we can generate personalized summaries that cover relevant source information while retaining coverage.

</details>


### [41] [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)

*Shuyang Cao, Karthik Radhakrishnan, David Rosenberg, Steven Lu, Pengxiang Cheng, Lu Wang, Shiyue Zhang*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, robustness metrics, knowledge-intensive tasks, document interactions

**Relevance Score:** 9

**TL;DR:** This paper investigates the effectiveness and robustness of retrieval-augmented generation (RAG) in large language models (LLMs) for knowledge-intensive tasks.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address potential performance degradation in LLMs when using RAG due to imperfect retrieval and leveraging issues.

**Method:** A benchmark of 1500 open-domain questions with retrieved documents from Wikipedia is established, alongside three robustness metrics corresponding to the research questions. Experiments involving 11 LLMs and 3 prompting strategies are conducted.

**Key Contributions:**

	1. Establishment of a benchmark for evaluating RAG performance
	2. Introduction of robustness metrics for assessing LLMs in RAG contexts
	3. Empirical evaluation of 11 LLMs with various prompt strategies regarding their retrieval robustness

**Result:** Experiments show that all LLMs demonstrate high retrieval robustness, but varying levels of imperfect robustness limit their ability to maximize RAG benefits.

**Limitations:** The study is limited to open-domain questions and may not generalize well to other domains or types of documents.

**Conclusion:** The findings suggest that while RAG can improve task performance, challenges remain in its implementation, indicating the need for better retrieval processes and understanding of document interaction.

**Abstract:** Retrieval-augmented generation (RAG) generally enhances large language models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also lead to performance degradation due to imperfect retrieval and the model's limited ability to leverage retrieved content. In this work, we evaluate the robustness of LLMs in practical RAG setups (henceforth retrieval robustness). We focus on three research questions: (1) whether RAG is always better than non-RAG; (2) whether more retrieved documents always lead to better performance; (3) and whether document orders impact results. To facilitate this study, we establish a benchmark of 1500 open-domain questions, each with retrieved documents from Wikipedia. We introduce three robustness metrics, each corresponds to one research question. Our comprehensive experiments, involving 11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit surprisingly high retrieval robustness; nonetheless, different degrees of imperfect robustness hinders them from fully utilizing the benefits of RAG.

</details>


### [42] [EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse](https://arxiv.org/abs/2505.21889)

*Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang*

**Main category:** cs.CL

**Keywords:** large language models, KV cache reuse, infilling tasks, fragment tokenization, natural language processing

**Relevance Score:** 8

**TL;DR:** The paper introduces EFIM, a new prompt format that enhances the efficiency of KV cache reuse in large language models (LLMs) for infilling tasks, while addressing subtoken generation issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional infilling tasks in LLMs face latency challenges due to inefficiencies in KV cache reuse caused by prompt structure.

**Method:** Propose a transformed prompt format (EFIM) to improve KV cache reuse and introduce a fragment tokenization training method to better handle token generation.

**Key Contributions:**

	1. Introduction of EFIM prompt format for efficient KV cache reuse
	2. Development of fragment tokenization training to address subtoken generation
	3. Demonstration of significant performance improvements in LLM serving.

**Result:** EFIM reduces latency by 52% and improves throughput by 98% on two representative LLMs, while preserving infilling capability.

**Limitations:** 

**Conclusion:** EFIM effectively enhances the performance of infilling tasks by optimizing KV cache usage and addressing subtoken generation issues.

**Abstract:** Large language models (LLMs) are often used for infilling tasks, which involve predicting or generating missing information in a given text. These tasks typically require multiple interactions with similar context. To reduce the computation of repeated historical tokens, cross-request key-value (KV) cache reuse, a technique that stores and reuses intermediate computations, has become a crucial method in multi-round interactive services. However, in infilling tasks, the KV cache reuse is often hindered by the structure of the prompt format, which typically consists of a prefix and suffix relative to the insertion point. Specifically, the KV cache of the prefix or suffix part is frequently invalidated as the other part (suffix or prefix) is incrementally generated. To address the issue, we propose EFIM, a transformed prompt format of FIM to unleash the performance potential of KV cache reuse. Although the transformed prompt can solve the inefficiency, it exposes subtoken generation problems in current LLMs, where they have difficulty generating partial words accurately. Therefore, we introduce a fragment tokenization training method which splits text into multiple fragments before tokenization during data processing. Experiments on two representative LLMs show that LLM serving with EFIM can lower the latency by 52% and improve the throughput by 98% while maintaining the original infilling capability.EFIM's source code is publicly available at https://github.com/gty111/EFIM.

</details>


### [43] [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)

*Rennai Qiu, Chen Qian, Ran Li, Yufan Dang, Weize Chen, Cheng Yang, Yingli Zhang, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Multi-Agent Systems, Resource-Aware, Collaboration, Large Language Models, Software Development

**Relevance Score:** 8

**TL;DR:** Proposes a resource-aware multi-agent system called Co-Saving to improve efficiency in complex task handling via shortcut learning from successful trajectories.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Standalone agents face limitations in handling complex tasks due to high resource consumption and inefficiencies, prompting the need for a more efficient collaborative approach through multi-agent systems.

**Method:** The Co-Saving system utilizes collaborative mechanisms, including task decomposition, iterative communication, and role specialization, while introducing 'shortcuts' to bypass redundant reasoning agents based on historically successful trajectories.

**Key Contributions:**

	1. Introduction of a resource-aware multi-agent system (Co-Saving)
	2. Utilization of experiential knowledge for operational efficiency
	3. Development of shortcuts for expedited problem-solving

**Result:** Experiments show Co-Saving reduces token usage by an average of 50.85% and improves code quality by 10.06% compared to the state-of-the-art method ChatDev.

**Limitations:** 

**Conclusion:** Co-Saving demonstrates that resource-aware collaborative mechanisms can significantly enhance the efficiency and quality of multi-agent systems in software development tasks.

**Abstract:** Recent advancements in Large Language Models (LLMs) and autonomous agents have demonstrated remarkable capabilities across various domains. However, standalone agents frequently encounter limitations when handling complex tasks that demand extensive interactions and substantial computational resources. Although Multi-Agent Systems (MAS) alleviate some of these limitations through collaborative mechanisms like task decomposition, iterative communication, and role specialization, they typically remain resource-unaware, incurring significant inefficiencies due to high token consumption and excessive execution time. To address these limitations, we propose a resource-aware multi-agent system -- Co-Saving (meaning that multiple agents collaboratively engage in resource-saving activities), which leverages experiential knowledge to enhance operational efficiency and solution quality. Our key innovation is the introduction of "shortcuts" -- instructional transitions learned from historically successful trajectories -- which allows to bypass redundant reasoning agents and expedite the collective problem-solving process. Experiments for software development tasks demonstrate significant advantages over existing methods. Specifically, compared to the state-of-the-art MAS ChatDev, our method achieves an average reduction of 50.85% in token usage, and improves the overall code quality by 10.06%.

</details>


### [44] [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)

*Yin Hua, Zhiqiang Liu, Mingyang Chen, Zheng Fang, Chi Man Wong, Lingxiao Li, Chi Man Vong, Huajun Chen, Wen Zhang*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Knowledge Graphs, Machine Learning, Foundation Models, Reasoning Tasks

**Relevance Score:** 8

**TL;DR:** MERRY is a foundation model designed for knowledge graph reasoning that outperforms existing methods by integrating both structural and textual information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current research on foundation models for knowledge graphs which focus primarily on in-KG tasks, hindering progress in more challenging out-of-KG tasks.

**Method:** MERRY employs a multi-perspective Conditional Message Passing (CMP) encoding architecture and a dynamic residual fusion module to integrate textual and structural information effectively.

**Key Contributions:**

	1. Introduction of the MERRY foundation model for knowledge graph reasoning
	2. Development of a multi-perspective Conditional Message Passing architecture
	3. Implementation of a dynamic residual fusion module for relevant information retention

**Result:** MERRY shows superior performance across 28 datasets, outperforming existing baselines in in-KG reasoning tasks and demonstrating strong generalization to out-of-KG tasks like knowledge graph question answering.

**Limitations:** 

**Conclusion:** The comprehensive evaluations indicate that MERRY represents a significant advancement in knowledge graph reasoning capabilities and can effectively address both in-KG and out-of-KG tasks.

**Abstract:** In natural language processing (NLP) and computer vision (CV), the successful application of foundation models across diverse tasks has demonstrated their remarkable potential. However, despite the rich structural and textual information embedded in knowledge graphs (KGs), existing research of foundation model for KG has primarily focused on their structural aspects, with most efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This limitation has hindered progress in addressing more challenging out-of-KG tasks. In this paper, we introduce MERRY, a foundation model for general knowledge graph reasoning, and investigate its performance across two task categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG question answering, KGQA). We not only utilize the structural information, but also the textual information in KGs. Specifically, we propose a multi-perspective Conditional Message Passing (CMP) encoding architecture to bridge the gap between textual and structural modalities, enabling their seamless integration. Additionally, we introduce a dynamic residual fusion module to selectively retain relevant textual information and a flexible edge scoring mechanism to adapt to diverse downstream tasks. Comprehensive evaluations on 28 datasets demonstrate that MERRY outperforms existing baselines in most scenarios, showcasing strong reasoning capabilities within KGs and excellent generalization to out-of-KG tasks such as KGQA.

</details>


### [45] [RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments](https://arxiv.org/abs/2505.21936)

*Zeyi Liao, Jaylen Jones, Linxi Jiang, Eric Fosler-Lussier, Yu Su, Zhiqiang Lin, Huan Sun*

**Main category:** cs.CL

**Keywords:** computer-use agents, indirect prompt injection, adversarial testing, hybrid sandbox, security vulnerabilities

**Relevance Score:** 6

**TL;DR:** RedTeamCUA is an adversarial testing framework that assesses vulnerabilities in computer-use agents (CUAs) through a hybrid sandbox integrating VM-based OS and Docker-based web platforms. It reveals substantial indirect prompt injection risks in CUAs, even the most secure ones.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation of vulnerabilities in CUAs against indirect prompt injection threats in a realistic testing environment.

**Method:** Developed a novel hybrid sandbox named RedTeamCUA that combines VM-based OS and Docker platforms for testing CUAs in controlled adversarial scenarios.

**Key Contributions:**

	1. Introduction of RedTeamCUA framework for testing CUAs
	2. Development of RTC-Bench with 864 examples for benchmarking CUAs
	3. Identification of significant vulnerabilities in current frontier CUAs

**Result:** Benchmarking with RTC-Bench revealed significant vulnerabilities in CUAs, particularly a 42.9% ASR for Claude 3.7 Sonnet and a 7.6% ASR for the most secure CUA, Operator, indicating substantial risks.

**Limitations:** May not encompass all possible attack vectors or CUAs beyond those benchmarked.

**Conclusion:** RedTeamCUA serves as a critical tool for systematically analyzing CUA vulnerabilities, stressing the need for improved defenses against indirect prompt injection before deployment.

**Abstract:** Computer-use agents (CUAs) promise to automate complex tasks across operating systems (OS) and the web, but remain vulnerable to indirect prompt injection. Current evaluations of this threat either lack support realistic but controlled environments or ignore hybrid web-OS attack scenarios involving both interfaces. To address this, we propose RedTeamCUA, an adversarial testing framework featuring a novel hybrid sandbox that integrates a VM-based OS environment with Docker-based web platforms. Our sandbox supports key features tailored for red teaming, such as flexible adversarial scenario configuration, and a setting that decouples adversarial evaluation from navigational limitations of CUAs by initializing tests directly at the point of an adversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive benchmark with 864 examples that investigate realistic, hybrid web-OS attack scenarios and fundamental security vulnerabilities. Benchmarking current frontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA demonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated, still exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute adversarial tasks with an Attempt Rate as high as 92.5%, although failing to complete them due to capability limitations. Nevertheless, we observe concerning ASRs of up to 50% in realistic end-to-end settings, with the recently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%, demonstrating that indirect prompt injection presents tangible risks for even advanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA provides an essential framework for advancing realistic, controlled, and systematic analysis of CUA vulnerabilities, highlighting the urgent need for robust defenses to indirect prompt injection prior to real-world deployment.

</details>


### [46] [Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages](https://arxiv.org/abs/2505.21937)

*Pratik Rakesh Singh, Kritarth Prasad, Mohammadi Zaki, Pankaj Wasnik*

**Main category:** cs.CL

**Keywords:** graph neural networks, idiomatic translation, multi-word expressions, machine translation, cultural nuances

**Relevance Score:** 4

**TL;DR:** This paper presents IdiomCE, an adaptive GNN-based method that improves idiomatic translations by learning intricate mappings between multi-word expressions and idioms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Translating multi-word expressions and idioms requires understanding cultural nuances and context, which traditional methods struggle with.

**Method:** The authors propose IdiomCE, a graph neural network that effectively generalizes mappings between idiomatic expressions for better translation.

**Key Contributions:**

	1. Introduction of IdiomCE, an adaptive GNN for idiomatic expressions translation
	2. Demonstrated significant improvements in translation quality
	3. Effective in resource-constrained environments

**Result:** IdiomCE demonstrates significant improvements in idiomatic translation quality across multiple datasets, even for smaller models.

**Limitations:** 

**Conclusion:** The methodology offers a solution to enhance idiomatic translation especially in resource-constrained settings.

**Abstract:** Translating multi-word expressions (MWEs) and idioms requires a deep understanding of the cultural nuances of both the source and target languages. This challenge is further amplified by the one-to-many nature of idiomatic translations, where a single source idiom can have multiple target-language equivalents depending on cultural references and contextual variations. Traditional static knowledge graphs (KGs) and prompt-based approaches struggle to capture these complex relationships, often leading to suboptimal translations. To address this, we propose IdiomCE, an adaptive graph neural network (GNN) based methodology that learns intricate mappings between idiomatic expressions, effectively generalizing to both seen and unseen nodes during training. Our proposed method enhances translation quality even in resource-constrained settings, facilitating improved idiomatic translation in smaller models. We evaluate our approach on multiple idiomatic translation datasets using reference-less metrics, demonstrating significant improvements in translating idioms from English to various Indian languages.

</details>


### [47] [Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems](https://arxiv.org/abs/2404.06762)

*Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen*

**Main category:** cs.CL

**Keywords:** Intelligent Tutoring Systems, large language models, student simulation

**Relevance Score:** 9

**TL;DR:** This work proposes a framework to construct student profiles leveraging LLMs for personality-aware simulation in Intelligent Tutoring Systems, focusing on language learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance engagement and efficiency in learning through personalized interaction in Intelligent Tutoring Systems by utilizing student characteristics.

**Method:** The proposed framework integrates cognitive and noncognitive aspects to create student profiles and uses LLMs to simulate varied student responses based on personality traits and language abilities.

**Key Contributions:**

	1. Development of a framework for constructing personality-aware student profiles
	2. Integration of cognitive and noncognitive traits for refined simulation
	3. Validation of LLMs in generating adaptive responses from students

**Result:** Experimental results demonstrate that LLMs can generate diverse responses that effectively trigger adaptive scaffolding strategies from teachers.

**Limitations:** 

**Conclusion:** The integration of LLMs in creating personality-aware simulations can lead to better engagement in language learning contexts within ITSs.

**Abstract:** Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.

</details>


### [48] [RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering](https://arxiv.org/abs/2505.21940)

*Bolei He, Xinran He, Mengke Chen, Xianwei Xue, Ying Zhu, Zhenhua Ling*

**Main category:** cs.CL

**Keywords:** Multi-Hop Question Answering, Large Language Models, Reasoning Enhancement

**Relevance Score:** 9

**TL;DR:** RISE is a novel framework that enhances reasoning in Multi-Hop Question Answering by iteratively exploring and improving the model's evidence integration and logical consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with complex reasoning tasks like Multi-Hop Question Answering due to challenges in evidence integration and logical dependencies.

**Method:** RISE involves three steps: question decomposition, retrieve-then-read, and self-critique, which promote iterative self-exploration.

**Key Contributions:**

	1. Introduction of the RISE framework for reasoning enhancement.
	2. Demonstration of significant improvements in MHQA task performance.
	3. Novel approach integrating self-critique with evidence retrieval.

**Result:** RISE shows significant improvements in reasoning accuracy and task performance on various Multi-Hop Question Answering benchmarks.

**Limitations:** 

**Conclusion:** The iterative self-exploration approach of RISE effectively enhances reasoning capabilities in LLMs for complex tasks.

**Abstract:** Large Language Models (LLMs) excel in many areas but continue to face challenges with complex reasoning tasks, such as Multi-Hop Question Answering (MHQA). MHQA requires integrating evidence from diverse sources while managing intricate logical dependencies, often leads to errors in reasoning. Retrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces challenges in effectively filtering noisy data and retrieving all necessary evidence, thereby limiting its effectiveness in addressing MHQA challenges. To address these challenges, we propose RISE:Reasoning Enhancement via Iterative Self-Exploration, a novel framework designed to enhance models' reasoning capability through iterative self-exploration. Specifically, RISE involves three key steps in addressing MHQA tasks: question decomposition, retrieve-then-read, and self-critique. By leveraging continuous self-exploration, RISE identifies accurate reasoning paths, iteratively self-improving the model's capability to integrate evidence, maintain logical consistency, and enhance performance in MHQA tasks. Extensive experiments on multiple MHQA benchmarks demonstrate that RISE significantly improves reasoning accuracy and task performance.

</details>


### [49] [Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation](https://arxiv.org/abs/2505.21941)

*Ashim Gupta, Vivek Srikumar*

**Main category:** cs.CL

**Keywords:** multilingual generation, repeated sampling, perplexity, reward-based verifiers, text generation

**Relevance Score:** 7

**TL;DR:** This paper evaluates inference-time scaling via repeated sampling for multilingual text generation, finding significant quality improvements. It emphasizes the importance of verifier selection for specific tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of inference-time scaling through repeated sampling in multilingual generation tasks, which has been less studied compared to reasoning tasks.

**Method:** The study utilizes perplexity- and reward-based verifiers to evaluate the quality of multilingual text generation on two benchmarks: the Aya Evaluation Suite and m-ArenaHard.

**Key Contributions:**

	1. Evaluated inference-time scaling for multilingual generation.
	2. Demonstrated significant quality improvements with repeated sampling.
	3. Established the effectiveness of different verifier types for various task requirements.

**Result:** The research shows consistent quality improvements in multilingual generation, with gains exceeding 35% in certain cases; perplexity-based scoring works well for open-ended prompts, while reward-based verifiers improve performance on reasoning tasks.

**Limitations:** 

**Conclusion:** Repeated sampling demonstrates broader utility in improving the quality of multilingual text generation, highlighting the need for appropriate verifier selection based on the task.

**Abstract:** Inference-time scaling via repeated sampling has shown promise in reasoning tasks, but its effectiveness in multilingual generation remains underexplored. We evaluate this approach using perplexity- and reward-based verifiers on two multilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results show consistent quality improvements, with gains exceeding 35% in some cases. While perplexity-based scoring is effective for open-ended prompts, only reward-based verifiers improve performance on tasks requiring reasoning (e.g., math, code). Our results demonstrate the broader utility of repeated sampling for multilingual text generation and underscore the importance of selecting right verifiers for the task.

</details>


### [50] [ReLearn: Unlearning via Learning for Large Language Models](https://arxiv.org/abs/2502.11190)

*Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** unlearning, large language models, data augmentation, evaluation metrics, knowledge retention

**Relevance Score:** 7

**TL;DR:** ReLearn addresses unlearning in large language models by utilizing data augmentation and a new evaluation framework to maintain output quality while achieving targeted forgetting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing unlearning methods that degrade model performance and linguistic coherence.

**Method:** ReLearn employs a data augmentation and fine-tuning pipeline in conjunction with new metrics for evaluating knowledge preservation and generation quality.

**Key Contributions:**

	1. Introduces a novel data augmentation and fine-tuning pipeline for unlearning
	2. Develops evaluation metrics: Knowledge Forgetting Rate (KFR) and Linguistic Score (LS)
	3. Demonstrates improved performance in maintaining text coherence while enabling forgetting

**Result:** Experiments demonstrate that ReLearn facilitates effective targeted forgetting without sacrificing output quality, substantiating the retention of coherent text generation.

**Limitations:** 

**Conclusion:** ReLearn presents a viable approach to unlearning in language models, proving that targeted knowledge forgetfulness can be achieved while maintaining high-generation standards.

**Abstract:** Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.

</details>


### [51] [Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning](https://arxiv.org/abs/2505.21958)

*Qihuang Zhong, Liang Ding, Fei Liao, Juhua Liu, Bo Du, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** Domain-specific instruction-tuning, Knowledge-aware Data Selection, Large Language Models, Medical applications, Data conflicts

**Relevance Score:** 9

**TL;DR:** Knowledge-aware Data Selection (KDS) improves data selection for domain-specific instruction-tuning of LLMs by addressing knowledge conflicts, leading to better performance and reduced hallucination in medical applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current data selection methods for domain-specific instruction-tuning of LLMs struggle due to neglecting knowledge conflicts between pretrained knowledge and contextual knowledge, which can harm LLM performance.

**Method:** The KDS framework employs two knowledge-aware metrics to measure knowledge conflicts: context-memory knowledge alignment and intra-memory knowledge consistency, filtering out data with high conflicts while sampling diverse, high-quality data.

**Key Contributions:**

	1. Introduction of the KDS framework for effective data selection
	2. Use of knowledge-aware metrics to measure knowledge conflicts
	3. Demonstrated significant performance improvements in medical LLM applications

**Result:** KDS outperforms existing baselines in domain-specific performance within the medical domain, demonstrating significant gains and improving model generalization while alleviating hallucination issues.

**Limitations:** 

**Conclusion:** The KDS framework is an effective approach to enhance the efficiency of data selection for LLMs in specialized applications, particularly in healthcare, by mitigating knowledge conflicts.

**Abstract:** Domain-specific instruction-tuning has become the defacto standard for improving the performance of large language models (LLMs) in specialized applications, e.g., medical question answering. Since the instruction-tuning dataset might contain redundant or low-quality data, data selection (DS) is usually required to maximize the data efficiency. Despite the successes in the general domain, current DS methods often struggle to select the desired data for domain-specific instruction-tuning. One of the main reasons is that they neglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs' pretrained knowledge and context knowledge of instruction data, which could damage LLMs' prior abilities and lead to hallucination. To this end, we propose a simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to select the domain-specific instruction-tuning data that meets LLMs' actual needs. The core of KDS is to leverage two knowledge-aware metrics for quantitatively measuring knowledge conflicts from two aspects: context-memory knowledge alignment and intra-memory knowledge consistency. By filtering the data with large knowledge conflicts and sampling the high-quality and diverse data, KDS can effectively stimulate the LLMs' abilities and achieve better domain-specific performance. Taking the medical domain as the testbed, we conduct extensive experiments and empirically prove that KDS surpasses the other baselines and brings significant and consistent performance gains among all LLMs. More encouragingly, KDS effectively improves the model generalization and alleviates the hallucination problem.

</details>


### [52] [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)

*Taro Yano, Yoichi Ishibashi, Masafumi Oyamada*

**Main category:** cs.CL

**Keywords:** Large Language Models, post-training, autonomous optimization, pipeline construction, machine learning

**Relevance Score:** 9

**TL;DR:** LaMDAgent is a framework that autonomously generates and optimizes post-training pipelines for Large Language Models, leveraging LLM-based agents for enhanced performance with minimal human intervention.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate the construction and optimization of post-training pipelines for LLMs, addressing the limitations of current manual and narrow optimization approaches.

**Method:** LaMDAgent uses LLM-based agents to systematically explore model generation techniques, datasets, and hyperparameter configurations while utilizing task-based feedback.

**Key Contributions:**

	1. Introduction of LaMDAgent framework for automating post-training pipeline construction
	2. Improvement of tool-use accuracy by 9.0 points
	3. Discovery of effective post-training strategies overlooked by traditional approaches

**Result:** LaMDAgent improves tool-use accuracy by 9.0 points and identifies overlooked effective post-training strategies, while analyzing data and model size scaling impacts.

**Limitations:** 

**Conclusion:** The framework enables more efficient and effective pipeline discovery, revealing new challenges and solutions in model size scaling and data utilization.

**Abstract:** Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks. To further tailor LLMs to specific domains or applications, post-training techniques such as Supervised Fine-Tuning (SFT), Preference Learning, and model merging are commonly employed. While each of these methods has been extensively studied in isolation, the automated construction of complete post-training pipelines remains an underexplored area. Existing approaches typically rely on manual design or focus narrowly on optimizing individual components, such as data ordering or merging strategies. In this work, we introduce LaMDAgent (short for Language Model Developing Agent), a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents. LaMDAgent systematically explores diverse model generation techniques, datasets, and hyperparameter configurations, leveraging task-based feedback to discover high-performing pipelines with minimal human intervention. Our experiments show that LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities. Moreover, it uncovers effective post-training strategies that are often overlooked by conventional human-driven exploration. We further analyze the impact of data and model size scaling to reduce computational costs on the exploration, finding that model size scalings introduces new challenges, whereas scaling data size enables cost-effective pipeline discovery.

</details>


### [53] [Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack](https://arxiv.org/abs/2505.21967)

*Juan Ren, Mark Dras, Usman Naseem*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, adversarial attacks, safety mechanisms

**Relevance Score:** 7

**TL;DR:** This paper analyzes security vulnerabilities in Large Vision-Language Models (LVLMs) due to their multimodal nature and proposes a novel evaluation framework for assessing adversarial attacks on these models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of visual inputs in LVLMs introduces new security vulnerabilities that traditional safety mechanisms struggle to address, necessitating an in-depth analysis and new frameworks for evaluation.

**Method:** A systematic representational analysis of LVLMs is conducted to understand adversarial attacks. The proposed framework involves two stages: the first categorizes types of compliance and refusals, and the second assesses the model's output in relation to harmful intent.

**Key Contributions:**

	1. Systematic representational analysis of LVLMs
	2. Two-stage evaluation framework for adversarial attacks
	3. Normative schema for safety alignment in multimodal systems

**Result:** The analysis reveals why existing adversarial attacks can bypass LVLMs' safety mechanisms and categorizes the types of refusals into different behaviors, while providing a normative schema for ideal model responses.

**Limitations:** 

**Conclusion:** The work highlights the need for improved safety alignment mechanisms in LVLMs and offers a structured approach for evaluating their responses to adversarial prompts.

**Abstract:** Large Vision-Language Models (LVLMs) have shown remarkable capabilities across a wide range of multimodal tasks. However, their integration of visual inputs introduces expanded attack surfaces, thereby exposing them to novel security vulnerabilities. In this work, we conduct a systematic representational analysis to uncover why conventional adversarial attacks can circumvent the safety mechanisms embedded in LVLMs. We further propose a novel two stage evaluation framework for adversarial attacks on LVLMs. The first stage differentiates among instruction non compliance, outright refusal, and successful adversarial exploitation. The second stage quantifies the degree to which the model's output fulfills the harmful intent of the adversarial prompt, while categorizing refusal behavior into direct refusals, soft refusals, and partial refusals that remain inadvertently helpful. Finally, we introduce a normative schema that defines idealized model behavior when confronted with harmful prompts, offering a principled target for safety alignment in multimodal systems.

</details>


### [54] [Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset](https://arxiv.org/abs/2505.21979)

*Fakhraddin Alwajih, Samar Mohamed Magdy, Abdellah El Mekki, Omer Nacar, Youssef Nafea, Safaa Taher Abdelfadil, Abdulfattah Mohammed Yahya, Hamzah Luqman, Nada Almarwani, Samah Aloufi, Baraah Qawasmeh, Houdaifa Atou, Serry Sibaee, Hamzah A. Alsayadi, Walid Al-Dhabyani, Maged S. Al-shaibani, Aya El aatar, Nour Qandos, Rahaf Alhamouri, Samar Ahmad, Razan Khassib, Lina Hamad, Mohammed Anwar AL-Ghrawi, Fatimah Alshamari, Cheikh Malainine, Doaa Qawasmeh, Aminetou Yacoub, Tfeil moilid, Ruwa AbuHweidi, Ahmed Aboeitta, Vatimetou Mohamed Lemin, Reem Abdel-Salam, Ahlam Bashiti, Adel Ammar, Aisha Alansari, Ahmed Ashraf, Nora Alturayeif, Sara Shatnawi, Alcides Alcoba Inciarte, AbdelRahim A. Elmadany, Mohamedou cheikh tourad, Ismail Berrada, Mustafa Jarrar, Shady Shehata, Muhammad Abdul-Mageed*

**Main category:** cs.CL

**Keywords:** cultural bias, multimodal dataset, vision-language models, Arabic, machine learning

**Relevance Score:** 4

**TL;DR:** Introduction of a large-scale Arabic multimodal dataset called Pearl for cultural understanding in LVLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cultural biases inherent in mainstream large vision-language models by providing a diverse and representative multimodal dataset.

**Method:** Constructed using agentic workflows and extensive human-in-the-loop annotations by 45 annotators from the Arab world, covering ten culturally significant domains.

**Key Contributions:**

	1. Introduction of the Pearl dataset for Arabic cultural understanding
	2. Development of two evaluation benchmarks (Pearl and Pearl-Lite)
	3. Creation of the Pearl-X subset for assessing nuanced cultural variations

**Result:** Pearl improves cultural grounding in LVLMs through reasoning-centric instruction alignment, outperforming conventional scaling methods in evaluations.

**Limitations:** 

**Conclusion:** Pearl serves as a foundational resource for advancing culturally-aware multimodal modeling and is publicly available.

**Abstract:** Mainstream large vision-language models (LVLMs) inherently encode cultural biases, highlighting the need for diverse multimodal datasets. To address this gap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark explicitly designed for cultural understanding. Constructed through advanced agentic workflows and extensive human-in-the-loop annotations by 45 annotators from across the Arab world, Pearl comprises over K multimodal examples spanning ten culturally significant domains covering all Arab countries. We further provide two robust evaluation benchmarks Pearl and Pearl-Lite along with a specialized subset Pearl-X explicitly developed to assess nuanced cultural variations. Comprehensive evaluations on state-of-the-art open and proprietary LVLMs demonstrate that reasoning-centric instruction alignment substantially improves models' cultural grounding compared to conventional scaling methods. Pearl establishes a foundational resource for advancing culturally-informed multimodal modeling research. All datasets and benchmarks are publicly available.

</details>


### [55] [Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data](https://arxiv.org/abs/2505.21997)

*Jihong Zhang, Xinya Liang, Anqi Deng, Nicole Bonge, Lin Tan, Ling Zhang, Nicole Zarrett*

**Main category:** cs.CL

**Keywords:** large language models, mixed methods research, behavioral response patterns, qualitative data analysis, survey response prediction

**Relevance Score:** 8

**TL;DR:** This study examines the ability of large language models (LLMs) to predict human survey responses informed by qualitative data, revealing insights into response patterns and variability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in aligning quantitative and qualitative data in mixed methods research by leveraging the advancements in LLMs.

**Method:** LLMs were guided by personal interview data to generate synthetic survey responses and their predictions were compared to actual survey responses using the Behavioral Regulations in Exercise Questionnaire (BREQ).

**Key Contributions:**

	1. Demonstrated the effectiveness of LLMs in predicting survey response patterns from qualitative data.
	2. Highlighting the importance of prompt design and model settings to enhance LLM alignment with human responses.
	3. Illustrated limitations in response variability and the need for bias mitigation in LLM-generated surveys.

**Result:** LLMs captured overall response patterns but showed less variability compared to human responses. Some models improved in response diversity when interview data was incorporated, while better prompts enhanced alignment with human responses.

**Limitations:** Lower variability in predicted responses compared to humans, issues with emotional interpretation, and psychometric fidelity.

**Conclusion:** Interview-informed LLMs can bridge qualitative and quantitative methods, but challenges remain in variability, emotional interpretation, and psychometric integrity, indicating the need for further refinement in future research.

**Abstract:** Mixed methods research integrates quantitative and qualitative data but faces challenges in aligning their distinct structures, particularly in examining measurement characteristics and individual response patterns. Advances in large language models (LLMs) offer promising solutions by generating synthetic survey responses informed by qualitative data. This study investigates whether LLMs, guided by personal interviews, can reliably predict human survey responses, using the Behavioral Regulations in Exercise Questionnaire (BREQ) and interviews from after-school program staff as a case study. Results indicate that LLMs capture overall response patterns but exhibit lower variability than humans. Incorporating interview data improves response diversity for some models (e.g., Claude, GPT), while well-crafted prompts and low-temperature settings enhance alignment between LLM and human responses. Demographic information had less impact than interview content on alignment accuracy. These findings underscore the potential of interview-informed LLMs to bridge qualitative and quantitative methodologies while revealing limitations in response variability, emotional interpretation, and psychometric fidelity. Future research should refine prompt design, explore bias mitigation, and optimize model settings to enhance the validity of LLM-generated survey data in social science research.

</details>


### [56] [Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate](https://arxiv.org/abs/2505.21999)

*Ashim Gupta, Maitrey Mehta, Zhichao Xu, Vivek Srikumar*

**Main category:** cs.CL

**Keywords:** large language models, cross-lingual consistency, evaluation framework, machine learning, multilingual capabilities

**Relevance Score:** 8

**TL;DR:** The paper proposes a framework for evaluating the cross-lingual consistency of large language models (LLMs) using a Translate then Evaluate strategy, revealing significant inconsistencies in LLM responses across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of evaluating multilingual performance of LLMs without relying on costly annotated datasets and to better understand their consistency across languages in terms of information and empathy.

**Method:** The authors introduce a framework that evaluates LLMs' consistency across different languages based on a Translate then Evaluate strategy, focusing on two aspects: information consistency and empathy consistency.

**Key Contributions:**

	1. Proposes a novel framework for evaluating cross-lingual consistency in LLMs.
	2. Analyzes the multilingual performance of LLMs across thirty languages.
	3. Identifies severe performance deficits in certain language families.

**Result:** The evaluation reveals pronounced inconsistencies in LLM responses across thirty languages, with notable performance deficits in specific language families and scripts.

**Limitations:** The study focuses on specific dimensions (information and empathy) and may not encompass all aspects of LLM performance.

**Conclusion:** The findings highlight critical weaknesses in the multilingual capabilities of LLMs and advocate for comprehensive cross-lingual evaluations in future research.

**Abstract:** Large language models (LLMs) provide detailed and impressive responses to queries in English. However, are they really consistent at responding to the same query in other languages? The popular way of evaluating for multilingual performance of LLMs requires expensive-to-collect annotated datasets. Further, evaluating for tasks like open-ended generation, where multiple correct answers may exist, is nontrivial. Instead, we propose to evaluate the predictability of model response across different languages. In this work, we propose a framework to evaluate LLM's cross-lingual consistency based on a simple Translate then Evaluate strategy. We instantiate this evaluation framework along two dimensions of consistency: information and empathy. Our results reveal pronounced inconsistencies in popular LLM responses across thirty languages, with severe performance deficits in certain language families and scripts, underscoring critical weaknesses in their multilingual capabilities. These findings necessitate cross-lingual evaluations that are consistent along multiple dimensions. We invite practitioners to use our framework for future multilingual LLM benchmarking.

</details>


### [57] [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)

*Jatin Gupta, Akhil Sharma, Saransh Singhania, Ali Imam Abidi*

**Main category:** cs.CL

**Keywords:** legal assistance, large language models, Indian legal domain, Legal Assist AI, legal reasoning

**Relevance Score:** 7

**TL;DR:** This paper presents Legal Assist AI, a transformer-based model that provides accessible legal assistance in India through large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Many citizens in India struggle to leverage their legal rights due to limited awareness and access to relevant legal information.

**Method:** Legal Assist AI retrieves relevant legal information from a curated database and generates accurate responses, fine-tuned on extensive datasets from the Indian legal domain.

**Key Contributions:**

	1. Introduction of transformer-based legal assistance
	2. Fine-tuning on Indian legal datasets
	3. Demonstrated superior performance in legal Question-Answering compared to existing models

**Result:** The model achieved a score of 60.08% on the AIBE, outperforming state-of-the-art models like GPT-3.5 Turbo and Mistral 7B in legal reasoning and accuracy.

**Limitations:** 

**Conclusion:** Legal Assist AI is a reliable tool for practical legal applications, with plans for future iterations to enhance performance and expand its coverage.

**Abstract:** Pursuit of accessible legal assistance in India faces a critical gap, as many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information. This paper introduces Legal Assist AI, a transformer-based model designed to bridge this gap by offering effective legal assistance through large language models (LLMs). The system retrieves relevant legal information from a curated database and generates accurate responses, enabling effective assistance for diverse users, including legal professionals, scholars, and the general public. The model was fine-tuned on extensive datasets from the Indian legal domain, including Indian Constitution, Bharatiya Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth, providing a robust understanding of the complexities of Indian law. By incorporating domain-specific legal datasets, the proposed model demonstrated remarkable efficiency and specialization in legal Question-Answering. The model was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral 7B, achieving a 60.08% score on the AIBE, outperforming its competitors in legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided common issues such as hallucinations, making it highly reliable for practical legal applications. It showcases the model's applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset to cover a broader range of multilingual and case-specific queries as well.

</details>


### [58] [CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models](https://arxiv.org/abs/2505.22017)

*Siqi Fan, Peng Han, Shuo Shang, Yequan Wang, Aixin Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Efficiency, CoThink, Token Generation, Dynamic Reasoning Depth

**Relevance Score:** 8

**TL;DR:** Proposes CoThink, a two-stage approach for LLMs to improve reasoning efficiency by dynamically adjusting output based on problem difficulty.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the token efficiency of reasoning-optimized models that often produce excessively verbose outputs due to overthinking.

**Method:** CoThink uses an instruct model to create a high-level outline of a solution, which is then refined by a reasoning model, allowing for dynamic adjustment of reasoning depth.

**Key Contributions:**

	1. Introduction of CoThink for improving reasoning efficiency in LLMs
	2. Demonstration of reduced token generation without losing accuracy
	3. Formal definition of reasoning efficiency in LLMs

**Result:** CoThink reduces total token generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin.

**Limitations:** Limited to the three reasoning models and datasets evaluated.

**Conclusion:** The approach enhances reasoning efficiency in LLMs and suggests a potential scaling law for reasoning efficiency.

**Abstract:** Large language models (LLMs) benefit from increased test-time compute, a phenomenon known as test-time scaling. However, reasoning-optimized models often overthink even simple problems, producing excessively verbose outputs and leading to low token efficiency. By comparing these models with equally sized instruct models, we identify two key causes of this verbosity: (1) reinforcement learning reduces the information density of forward reasoning, and (2) backward chain-of thought training encourages redundant and often unnecessary verification steps. Since LLMs cannot assess the difficulty of a given problem, they tend to apply the same cautious reasoning strategy across all tasks, resulting in inefficient overthinking. To address this, we propose CoThink, an embarrassingly simple pipeline: an instruct model first drafts a high-level solution outline; a reasoning model then works out the solution. We observe that CoThink enables dynamic adjustment of reasoning depth based on input difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and QwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token generation by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on average. With reference to the instruct model, we formally define reasoning efficiency and observe a potential reasoning efficiency scaling law in LLMs.

</details>


### [59] [Improving Continual Pre-training Through Seamless Data Packing](https://arxiv.org/abs/2505.22018)

*Ruicheng Yin, Xuan Gao, Changze Lv, Xiaohua Wang, Xiaoqing Zheng, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Continual Pre-training, Data Packing, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces Seamless Packing, a novel data packing strategy for continual pre-training that improves contextual coherence and model performance by effectively managing text sequences and minimizing truncation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of existing data packing methods in continual pre-training, which often lead to excessive truncation and context discontinuity.

**Method:** The proposed Seamless Packing approach uses a sliding window technique to synchronize overlapping tokens across sequences and a First-Fit-Decreasing algorithm to minimize padding by optimally packing shorter texts.

**Key Contributions:**

	1. Introduction of Seamless Packing for improved contextual coherence in continual pre-training.
	2. Utilization of a sliding window technique for token synchronization.
	3. Implementation of a packing algorithm to reduce truncation and padding.

**Result:** Empirical evaluations show that the Seamless Packing method outperforms baseline methods in 99% of tested settings across various model architectures and domains.

**Limitations:** 

**Conclusion:** Seamless Packing significantly enhances the efficiency and performance of continual pre-training by preserving contextual information.

**Abstract:** Continual pre-training has demonstrated significant potential in enhancing model performance, particularly in domain-specific scenarios. The most common approach for packing data before continual pre-training involves concatenating input texts and splitting them into fixed-length sequences. While straightforward and efficient, this method often leads to excessive truncation and context discontinuity, which can hinder model performance. To address these issues, we explore the potential of data engineering to enhance continual pre-training, particularly its impact on model performance and efficiency. We propose Seamless Packing (SP), a novel data packing strategy aimed at preserving contextual information more effectively and enhancing model performance. Our approach employs a sliding window technique in the first stage that synchronizes overlapping tokens across consecutive sequences, ensuring better continuity and contextual coherence. In the second stage, we adopt a First-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger than the target sequence length, thereby minimizing padding and truncation. Empirical evaluations across various model architectures and corpus domains demonstrate the effectiveness of our method, outperforming baseline method in 99% of all settings. Code is available at https://github.com/Infernus-WIND/Seamless-Packing.

</details>


### [60] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)

*Qiuchen Wang, Ruixue Ding, Yu Zeng, Zehui Chen, Lin Chen, Shihang Wang, Pengjun Xie, Fei Huang, Feng Zhao*

**Main category:** cs.CL

**Keywords:** RAG, reinforcement learning, visual reasoning

**Relevance Score:** 9

**TL;DR:** Introducing VRAG-RL, a reinforcement learning framework for complex reasoning across visually rich information, addressing limitations in existing RAG methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional RAG methods struggle with visually rich data, leading to ineffective information retrieval and reasoning.

**Method:** A reinforcement learning framework called VRAG-RL that allows visual language models to interact with search engines and optimize their performance through specially designed action spaces and a novel reward structure.

**Key Contributions:**

	1. Novel RL framework for visual reasoning in RAG tasks
	2. Tailored action space for visually rich inputs
	3. Integration of query rewriting with retrieval performance for model optimization

**Result:** VRAG-RL enables more effective reasoning by utilizing visual perception tokens and optimizing VLMs based on user inquiries and retrieval performance, overcoming challenges of existing multi-modal RAG approaches.

**Limitations:** 

**Conclusion:** VRAG-RL represents a significant advancement in integrating visual reasoning into retrieval-augmented generation, with implications for real-world applications in AI and HCI.

**Abstract:** Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. Traditional text-based methods cannot handle visual-related information. On the other hand, current vision-based RAG approaches are often limited by fixed pipelines and frequently struggle to reason effectively due to the insufficient activation of the fundamental capabilities of models. As RL has been proven to be beneficial for model reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information. With this framework, VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. Our approach highlights key limitations of RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely incorporate images into the context, leading to insufficient reasoning token allocation and neglecting visual-specific perception; and (ii) When models interact with search engines, their queries often fail to retrieve relevant information due to the inability to articulate requirements, thereby leading to suboptimal performance. To address these challenges, we define an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. Furthermore, to bridge the gap between users' original inquiries and the retriever, we employ a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward. Our VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications. The code is available at \hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [61] [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)

*Jingyu Zhang, Ahmed Elgohary, Xiawei Wang, A S M Iftekhar, Ahmed Magooda, Benjamin Van Durme, Daniel Khashabi, Kyle Jackson*

**Main category:** cs.CL

**Keywords:** Jailbreak Distillation, large language models, safety benchmarking, machine learning, AI safety

**Relevance Score:** 7

**TL;DR:** Jailbreak Distillation (JBDistill) creates robust, easily-updated safety benchmarks for large language models by distilling jailbreak attack prompts, ensuring fair evaluation and reproducibility across various models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid deployment of large language models in critical applications necessitates effective safety benchmarking to address potential vulnerabilities.

**Method:** JBDistill constructs benchmarks by utilizing development models and existing jailbreak attack algorithms to form a candidate prompt pool, followed by prompt selection to identify effective safety benchmarks.

**Key Contributions:**

	1. Introduction of Jailbreak Distillation (JBDistill) framework for creating safety benchmarks
	2. Demonstration of benchmarks generalizing robustly across diverse LLMs
	3. Significant performance improvement over existing benchmarks for safety evaluation

**Result:** JBDistill's benchmarks show significant improvement over existing safety benchmarks, generalizing robustly to diverse LLMs while maintaining high separability and diversity.

**Limitations:** 

**Conclusion:** JBDistill provides a sustainable solution for safety evaluation, requiring minimal human effort for updates and addressing challenges in current safety benchmarking practices.

**Abstract:** Large language models (LLMs) are rapidly deployed in critical applications, raising urgent needs for robust safety benchmarking. We propose Jailbreak Distillation (JBDistill), a novel benchmark construction framework that "distills" jailbreak attacks into high-quality and easily-updatable safety benchmarks. JBDistill utilizes a small set of development models and existing jailbreak attack algorithms to create a candidate prompt pool, then employs prompt selection algorithms to identify an effective subset of prompts as safety benchmarks. JBDistill addresses challenges in existing safety evaluation: the use of consistent evaluation prompts across models ensures fair comparisons and reproducibility. It requires minimal human effort to rerun the JBDistill pipeline and produce updated benchmarks, alleviating concerns on saturation and contamination. Extensive experiments demonstrate our benchmarks generalize robustly to 13 diverse evaluation models held out from benchmark construction, including proprietary, specialized, and newer-generation LLMs, significantly outperforming existing safety benchmarks in effectiveness while maintaining high separability and diversity. Our framework thus provides an effective, sustainable, and adaptable solution for streamlining safety evaluation.

</details>


### [62] [Voice Adaptation for Swiss German](https://arxiv.org/abs/2505.22054)

*Samuel Stucki, Jan Deriu, Mark Cieliebak*

**Main category:** cs.CL

**Keywords:** Voice Cloning, Swiss German Dialects, XTTSv2, Speech Synthesis, Dialect Adaptation

**Relevance Score:** 6

**TL;DR:** Investigation of Voice Adaptation models for Swiss German dialects using a fine-tuned XTTSv2 model on 5000 hours of weakly labeled training data, showing good performance in dialect rendering.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To adapt Voice Cloning technology to underrepresented languages, specifically Swiss German dialects.

**Method:** Preprocessing a large dataset of Swiss podcasts, transcribing and annotating it with dialect classes, then fine-tuning the XTTSv2 model on this dataset.

**Key Contributions:**

	1. Introduction of a large dataset of Swiss podcasts for dialect adaptation
	2. Successful fine-tuning of the XTTSv2 model for Swiss German
	3. Achievement of competitive scores in dialect representation.

**Result:** The fine-tuned XTTSv2 model achieved good scores in both human and automated evaluations, with CMOS scores of up to -0.28 and SMOS scores of 3.8.

**Limitations:** 

**Conclusion:** This work represents progress in adapting voice cloning for Swiss German dialects, contributing to underrepresented language technology.

**Abstract:** This work investigates the performance of Voice Adaptation models for Swiss German dialects, i.e., translating Standard German text to Swiss German dialect speech. For this, we preprocess a large dataset of Swiss podcasts, which we automatically transcribe and annotate with dialect classes, yielding approximately 5000 hours of weakly labeled training material. We fine-tune the XTTSv2 model on this dataset and show that it achieves good scores in human and automated evaluations and can correctly render the desired dialect. Our work shows a step towards adapting Voice Cloning technology to underrepresented languages. The resulting model achieves CMOS scores of up to -0.28 and SMOS scores of 3.8.

</details>


### [63] [Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?](https://arxiv.org/abs/2505.22061)

*Yujin Choi, Youngjoo Park, Junyoung Byun, Jaewook Lee, Jinseong Park*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, membership inference attacks, privacy, large language models, Mirabel

**Relevance Score:** 9

**TL;DR:** Introduces Mirabel, a detection framework for membership inference attacks in retrieval-augmented generation systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerability of retrieval-augmented generation systems to membership inference attacks when private documents are passed to large language models.

**Method:** Developed a similarity-based detection framework, Mirabel, which utilizes detect-and-hide strategies to obfuscate attackers while maintaining data utility.

**Key Contributions:**

	1. Introduction of Mirabel, a similarity-based MIA detection framework.
	2. Proven effectiveness of detect-and-hide strategies in obfuscating attackers.
	3. Demonstrated adaptability of Mirabel to existing private RAG systems.

**Result:** Experimental results demonstrate Mirabel's effectiveness in detecting and defending against various state-of-the-art membership inference attack methods.

**Limitations:** 

**Conclusion:** Mirabel showcases a practical solution for enhancing privacy in retrieval-augmented generation systems without adversely affecting their performance.

**Abstract:** Retrieval-augmented generation (RAG) mitigates the hallucination problem in large language models (LLMs) and has proven effective for specific, personalized applications. However, passing private retrieved documents directly to LLMs introduces vulnerability to membership inference attacks (MIAs), which try to determine whether the target datum exists in the private external database or not. Based on the insight that MIA queries typically exhibit high similarity to only one target document, we introduce Mirabel, a similarity-based MIA detection framework designed for the RAG system. With the proposed Mirabel, we show that simple detect-and-hide strategies can successfully obfuscate attackers, maintain data utility, and remain system-agnostic. We experimentally prove its detection and defense against various state-of-the-art MIA methods and its adaptability to existing private RAG systems.

</details>


### [64] [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)

*Ran Li, Shimin Di, Yuchen Liu, Chen Jing, Yu Qiu, Lei Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Information Extraction, Reasoning, Supervised Fine-Tuning

**Relevance Score:** 9

**TL;DR:** The study explores the effectiveness of two-stage training approaches in enhancing reasoning capacity in Large Language Models (LLMs) for scientific information extraction, outperforming traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs and reasoning LLMs in scientific information extraction, particularly their underperformance compared to small Bert-based models.

**Method:** We propose two new training approaches: 1. MimicSFT, which uses structured reasoning templates; 2. R^2GRPO, which employs relevance and rule-induced rewards. Both methods are aimed at refining reasoning paths and improving reasoning capacity.

**Key Contributions:**

	1. Introduction of two-stage training methods for LLMs in scientific information extraction
	2. Outperformance of new methods over existing LLMs and specialized models
	3. Code availability for further research and implementation

**Result:** Experiments demonstrate that both MimicSFT and R^2GRPO improve reasoning capacity, with R^2GRPO combined with MimicSFT outperforming baseline LLMs and specialized models in relation extraction tasks.

**Limitations:** 

**Conclusion:** The proposed two-stage training framework effectively enhances reasoning capabilities in LLMs for scientific information extraction.

**Abstract:** Previous study suggest that powerful Large Language Models (LLMs) trained with Reinforcement Learning with Verifiable Rewards (RLVR) only refines reasoning path without improving the reasoning capacity in math tasks while supervised-finetuning(SFT) with distillation can. We study this from the view of Scientific information extraction (SciIE) where LLMs and reasoning LLMs underperforms small Bert-based models. SciIE require both the reasoning and memorization. We argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE. We propose two-stage training with 1. MimicSFT, using structured reasoning templates without needing high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and rule-induced rewards. Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction. Our code is available at https://github.com/ranlislz/R2GRPO.

</details>


### [65] [ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation](https://arxiv.org/abs/2505.22076)

*Maja Stahl, Timon Ziegenbein, Joonsuk Park, Henning Wachsmuth*

**Main category:** cs.CL

**Keywords:** large language models, instruction fine-tuning, computational argumentation, NLP tasks, domain knowledge

**Relevance Score:** 7

**TL;DR:** This paper presents fine-tuning large language models (LLMs) for computational argumentation tasks, enhancing their ability to generalize while performing domain-specific tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing instruction-following LLMs struggle with tasks requiring domain knowledge, thus a specialized fine-tuning method is needed for computational argumentation (CA).

**Method:** The authors create 105 CA tasks with natural language instructions and develop a CA-specific benchmark, training a CA-specialized LLM with adapted self-instruct processes.

**Key Contributions:**

	1. Introduction of CA-specific instruction fine-tuning for LLMs
	2. Development of a benchmark for evaluating LLMs on CA tasks
	3. Creation of 52k CA-related instructions for model training

**Result:** The specialized fine-tuning method significantly improves the LLM's performance on both seen and unseen CA tasks, while maintaining stability on general NLP tasks.

**Limitations:** The study's focus is solely on computational argumentation; other domain-specific capabilities are not addressed.

**Conclusion:** CA-specialized instruction fine-tuning effectively enhances the ability of LLMs to handle computational argumentation tasks without compromising their generalization capabilities.

**Abstract:** Training large language models (LLMs) to follow instructions has significantly enhanced their ability to tackle unseen tasks. However, despite their strong generalization capabilities, instruction-following LLMs encounter difficulties when dealing with tasks that require domain knowledge. This work introduces a specialized instruction fine-tuning for the domain of computational argumentation (CA). The goal is to enable an LLM to effectively tackle any unseen CA tasks while preserving its generalization capabilities. Reviewing existing CA research, we crafted natural language instructions for 105 CA tasks to this end. On this basis, we developed a CA-specific benchmark for LLMs that allows for a comprehensive evaluation of LLMs' capabilities in solving various CA tasks. We synthesized 52k CA-related instructions, adapting the self-instruct process to train a CA-specialized instruction-following LLM. Our experiments suggest that CA-specialized instruction fine-tuning significantly enhances the LLM on both seen and unseen CA tasks. At the same time, performance on the general NLP tasks of the SuperNI benchmark remains stable.

</details>


### [66] [Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning](https://arxiv.org/abs/2505.22095)

*Chunyi Peng, Zhipeng Xu, Zhenghao Liu, Yishan Li, Yukun Yan, Shuo Wang, Zhiyuan Liu, Yu Gu, Minghe Yu, Ge Yu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Multimodal Retrieval, Large Language Models, Reinforcement Learning, Knowledge Bases, Dynamic Retrieval

**Relevance Score:** 8

**TL;DR:** R1-Router is a novel MRAG framework that dynamically retrieves knowledge based on the reasoning state in Multimodal Large Language Models, improving efficiency and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing MRAG methods use static retrieval pipelines that do not leverage the dynamic reasoning capabilities of MLLMs, potentially missing out on effective knowledge integration.

**Method:** R1-Router learns to route follow-up queries to the most suitable Knowledge Bases based on the reasoning process, while employing a reinforcement learning algorithm called Step-GRPO for optimizing reasoning behavior with step-specific rewards.

**Key Contributions:**

	1. Introduction of R1-Router, a dynamic retrieval framework for MLLMs
	2. Development of Step-GRPO, a reinforcement learning algorithm for step-wise optimization
	3. Demonstrated effectiveness on open-domain QA benchmarks with substantial performance improvements

**Result:** R1-Router outperforms baseline models by over 7% on various open-domain QA benchmarks across multiple modalities.

**Limitations:** 

**Conclusion:** The integration of dynamic retrieval strategies enhances the performance of MLLMs in answering queries accurately and efficiently.

**Abstract:** Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in mitigating hallucinations in Multimodal Large Language Models (MLLMs) by incorporating external knowledge during generation. Existing MRAG methods typically adopt a static retrieval pipeline that fetches relevant information from multiple Knowledge Bases (KBs), followed by a refinement step. However, these approaches overlook the reasoning and planning capabilities of MLLMs to dynamically determine how to interact with different KBs during the reasoning process. To address this limitation, we propose R1-Router, a novel MRAG framework that learns to decide when and where to retrieve knowledge based on the evolving reasoning state. Specifically, R1-Router can generate follow-up queries according to the current reasoning step, routing these intermediate queries to the most suitable KB, and integrating external knowledge into a coherent reasoning trajectory to answer the original query. Furthermore, we introduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored reinforcement learning algorithm that assigns step-specific rewards to optimize the reasoning behavior of MLLMs. Experimental results on various open-domain QA benchmarks across multiple modalities demonstrate that R1-Router outperforms baseline models by over 7%. Further analysis shows that R1-Router can adaptively and effectively leverage diverse KBs, reducing unnecessary retrievals and improving both efficiency and accuracy.

</details>


### [67] [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)

*Jinheon Baek, Horst Samulowitz, Oktie Hassanzadeh, Dharmashankar Subramanian, Sola Shirai, Alfio Gliozzo, Debarun Bhattacharjya*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Large Language Models, Knowledge Base, SQL Generation, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces a comprehensive knowledge base for improving the accuracy of text-to-SQL translations by leveraging Large Language Models (LLMs) and domain-specific queries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accuracy of SQL code generation from natural language queries by addressing the limitations of existing LLM approaches, which may not adequately cover diverse and domain-specific queries.

**Method:** The authors constructed a comprehensive knowledge base that combines available questions, database schemas, and relevant knowledge. This knowledge base is designed to retrieve and generate information for a variety of queries and can be applied to unseen databases across different domains.

**Key Contributions:**

	1. Development of a comprehensive knowledge base for text-to-SQL enhancement
	2. Demonstration of improved performance on various datasets
	3. Application of the knowledge base to unseen databases across different domains

**Result:** The proposed approach significantly outperforms existing baselines on multiple text-to-SQL datasets, showing improved accuracy in generating SQL statements, particularly in both overlapping and non-overlapping database scenarios.

**Limitations:** 

**Conclusion:** The comprehensive knowledge base facilitates better grounding of SQL generation in domain-specific queries, potentially leading to more accurate and versatile text-to-SQL solutions.

**Abstract:** Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially.

</details>


### [68] [MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models](https://arxiv.org/abs/2505.22101)

*Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Kehang Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhi-Qin John Xu, Feiyu Xiong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Memory management, Artificial General Intelligence

**Relevance Score:** 9

**TL;DR:** MemOS is a memory operating system for Large Language Models that enhances memory management, integrating parametric, activation, and plaintext memory types for improved long-term knowledge evolution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs lack a structured architecture for memory handling, relying too much on parametric and ephemeral activation memory which limits their adaptability and long-term knowledge management.

**Method:** Introduces MemOS, a unified framework for memory management that includes the MemCube, enabling the organization of different memory types and their controlled access across tasks.

**Key Contributions:**

	1. Introduction of MemOS as a memory operating system for LLMs
	2. Development of MemCube for unified memory management
	3. Establishment of a memory-centric execution framework

**Result:** MemOS improves memory tracking, fusion, and migration, enabling LLMs to evolve knowledge, adapt intelligently, and coordinate across platforms effectively.

**Limitations:** 

**Conclusion:** MemOS fills a significant gap in LLM infrastructure, paving the way for ongoing adaptation and personalized intelligence in future intelligent systems.

**Abstract:** Large Language Models (LLMs) have emerged as foundational infrastructure in the pursuit of Artificial General Intelligence (AGI). Despite their remarkable capabilities in language perception and generation, current LLMs fundamentally lack a unified and structured architecture for handling memory. They primarily rely on parametric memory (knowledge encoded in model weights) and ephemeral activation memory (context-limited runtime states). While emerging methods like Retrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack lifecycle management and multi-modal integration, limiting their capacity for long-term knowledge evolution. To address this, we introduce MemOS, a memory operating system designed for LLMs that, for the first time, elevates memory to a first-class operational resource. It builds unified mechanisms for representation, organization, and governance across three core memory types: parametric, activation, and plaintext. At its core is the MemCube, a standardized memory abstraction that enables tracking, fusion, and migration of heterogeneous memory, while offering structured, traceable access across tasks and contexts. MemOS establishes a memory-centric execution framework with strong controllability, adaptability, and evolvability. It fills a critical gap in current LLM infrastructure and lays the groundwork for continual adaptation, personalized intelligence, and cross-platform coordination in next-generation intelligent systems.

</details>


### [69] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)

*Shuhai Zhang, Zeng You, Yaofo Chen, Zhiquan Wen, Qianyue Wang, Zhijie Qiu, Yuanqing Li, Mingkui Tan*

**Main category:** cs.CL

**Keywords:** Dynamic Group Attention, attention optimization, natural language processing, transformer models, computational efficiency

**Relevance Score:** 9

**TL;DR:** This paper introduces Dynamic Group Attention (DGA), a novel method that reduces computational redundancy in transformer-based models by optimizing attention through grouping less important tokens.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address computational inefficiencies in long-context modeling of transformer models caused by redundant attention computations despite sparse attention weights.

**Method:** Reformulates traditional probabilistic sequence modeling as a supervised learning task, theoretically analyzes attention sparsity, and formulates attention optimization as a linear coding problem, ultimately proposing the Dynamic Group Attention (DGA) strategy.

**Key Contributions:**

	1. Introduction of Dynamic Group Attention (DGA) for optimizing attention calculations in LLMs.
	2. Theoretical analysis of attention sparsity and its implications for redundancy in computational resources.
	3. Empirical validation showing reduced costs while maintaining performance.

**Result:** DGA significantly reduces computational costs while maintaining competitive performance against traditional attention mechanisms.

**Limitations:** 

**Conclusion:** The group coding strategy and DGA improve robustness to noise and efficiency in learning, providing a more efficient method for processing long-context sequences in LLMs.

**Abstract:** Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [70] [THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models](https://arxiv.org/abs/2505.22113)

*Zhiyuan Li, Yi Chang, Yuan Wu*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Efficiency Metrics, Benchmark, Reasoning Process

**Relevance Score:** 6

**TL;DR:** The paper introduces Think-Bench, a benchmark for evaluating the reasoning efficiency of Large Reasoning Models (LRMs) and presents new metrics to assess their overthinking behavior in simple tasks.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the problem of overthinking in LRMs, which leads to inefficiencies in computational resources during reasoning tasks.

**Method:** The authors developed Think-Bench, a benchmark specifically targeting reasoning efficiency, and proposed new metrics to evaluate LRMs based on their reasoning process, outcome quality, and chain-of-thought characteristics.

**Key Contributions:**

	1. Introduction of Think-Bench as a benchmark for reasoning efficiency.
	2. Novel efficiency metrics for evaluating LRMs.
	3. Comprehensive analysis of various LRMs' performance related to overthinking.

**Result:** The evaluation shows that many LRMs tend to generate excessive reasoning chains for simple questions, indicating a tendency towards overthinking, despite producing high-quality chain-of-thought outputs.

**Limitations:** The benchmark needs further validation across wider LRM architectures and tasks beyond those tested in this paper.

**Conclusion:** Think-Bench aims to provide a foundation for further research into improving the efficiency of LRMs and addressing the challenge of overthinking.

**Abstract:** Large reasoning models (LRMs) have achieved impressive performance in complex tasks, often outperforming conventional large language models (LLMs). However, the prevalent issue of overthinking severely limits their computational efficiency. Overthinking occurs when models generate excessive and redundant tokens that contribute little to accurate outcomes, especially in simple tasks, resulting in a significant waste of computational resources. To systematically investigate this issue, we introduce Think-Bench, a benchmark designed to evaluate the reasoning efficiency of LRMs. We also propose novel efficiency metrics and conduct a comprehensive evaluation of various LRMs across multiple dimensions, including the reasoning process, outcome quality, and chain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs exhibit overthinking in handling easy questions, generating unnecessarily lengthy reasoning chains. While many LRMs demonstrate high CoT quality, several suffer from low efficiency. We hope that Think-Bench can serve as a robust foundation for advancing research into LRMs.

</details>


### [71] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)

*Jintao Zhang, Zirui Liu, Mingyue Cheng, Shilong Zhang, Tingyue Pan, Qi Liu, Yanhu Xie*

**Main category:** cs.CL

**Keywords:** Intraoperative hypotension, multimodal language model, clinical decision support, physiological time series, machine learning

**Relevance Score:** 9

**TL;DR:** The paper introduces IOHFuseLM, a multimodal language model for predicting intraoperative hypotension (IOH) by integrating physiological time series and clinical data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Intraoperative hypotension (IOH) is linked to serious complications but is difficult to predict due to event sparsity and data integration challenges.

**Method:** A two-stage training strategy is employed: domain adaptive pretraining using IOH physiological data enriched by diffusion methods, followed by task fine-tuning on clinical datasets for event differentiation.

**Key Contributions:**

	1. Introduction of IOHFuseLM for IOH prediction
	2. Utilization of two-stage training with multimodal data
	3. Alignment of structured clinical data with physiological time series

**Result:** IOHFuseLM significantly outperforms established baselines in accurately identifying IOH events across two intraoperative datasets.

**Limitations:** 

**Conclusion:** The proposed framework enhances clinical decision support for IOH prediction by effectively combining multimodal patient data.

**Abstract:** Intraoperative hypotension (IOH) frequently occurs under general anesthesia and is strongly linked to adverse outcomes such as myocardial injury and increased mortality. Despite its significance, IOH prediction is hindered by event sparsity and the challenge of integrating static and dynamic data across diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal language model framework. To accurately identify and differentiate sparse hypotensive events, we leverage a two-stage training strategy. The first stage involves domain adaptive pretraining on IOH physiological time series augmented through diffusion methods, thereby enhancing the model sensitivity to patterns associated with hypotension. Subsequently, task fine-tuning is performed on the original clinical dataset to further enhance the ability to distinguish normotensive from hypotensive states. To enable multimodal fusion for each patient, we align structured clinical descriptions with the corresponding physiological time series at the token level. Such alignment enables the model to capture individualized temporal patterns alongside their corresponding clinical semantics. In addition, we convert static patient attributes into structured text to enrich personalized information. Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events, highlighting its applicability in clinical decision support scenarios. Our code is publicly available to promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [72] [Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches](https://arxiv.org/abs/2505.22118)

*Alan Ramponi, Marco Rovera, Robert Moro, Sara Tonelli*

**Main category:** cs.CL

**Keywords:** fact-checking, multilingual retrieval, crosslingual retrieval, LLM, negative examples

**Relevance Score:** 7

**TL;DR:** This paper explores multilingual and crosslingual retrieval of fact-checked claims, focusing on improving performance through negative example selection and re-ranking methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to assist professional fact-checkers by automating the retrieval of claims in multiple languages, especially in contexts with limited fact-check availability.

**Method:** The paper examines strategies for enhancing multilingual and crosslingual performance, specifically through supervised selection of negative examples and unsupervised re-ranking techniques, evaluated on a dataset of posts and claims in 47 languages.

**Key Contributions:**

	1. Improved strategies for multilingual and crosslingual fact-checked claim retrieval
	2. Use of LLM-based re-ranking for enhanced performance
	3. Identification of unique characteristics in crosslingual retrieval compared to multilingual approaches.

**Result:** The study finds that LLM-based re-ranking yields the best performance, followed by fine-tuning with a sentence similarity-based strategy for selecting negative examples.

**Limitations:** 

**Conclusion:** The results indicate that crosslingual retrieval presents distinct challenges compared to multilingual retrieval, highlighting the need for tailored approaches.

**Abstract:** Retrieval of previously fact-checked claims is a well-established task, whose automation can assist professional fact-checkers in the initial steps of information verification. Previous works have mostly tackled the task monolingually, i.e., having both the input and the retrieved claims in the same language. However, especially for languages with a limited availability of fact-checks and in case of global narratives, such as pandemics, wars, or international politics, it is crucial to be able to retrieve claims across languages. In this work, we examine strategies to improve the multilingual and crosslingual performance, namely selection of negative examples (in the supervised) and re-ranking (in the unsupervised setting). We evaluate all approaches on a dataset containing posts and claims in 47 languages (283 language combinations). We observe that the best results are obtained by using LLM-based re-ranking, followed by fine-tuning with negative examples sampled using a sentence similarity-based strategy. Most importantly, we show that crosslinguality is a setup with its own unique characteristics compared to the multilingual setup.

</details>


### [73] [LoKI: Low-damage Knowledge Implanting of Large Language Models](https://arxiv.org/abs/2505.22120)

*Runyu Wang, Peng Ping, Zhengyu Guo, Xiaoye Zhang, Quan Shi, Liting Zhou, Tianbo Ji*

**Main category:** cs.CL

**Keywords:** fine-tuning, catastrophic forgetting, parameter-efficient fine-tuning, large language models, knowledge storage

**Relevance Score:** 9

**TL;DR:** Proposes Low-damage Knowledge Implanting (LoKI), a PEFT technique for LLMs that reduces catastrophic forgetting while maintaining general capabilities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the issue of catastrophic forgetting in fine-tuning pretrained models, particularly for large language models, and improve task-specific performance without sacrificing general capabilities.

**Method:** LoKI is a parameter-efficient fine-tuning technique based on a mechanistic understanding of knowledge storage in transformer architectures.

**Key Contributions:**

	1. Introduction of Low-damage Knowledge Implanting (LoKI) for LLM fine-tuning.
	2. Demonstration of improved performance metrics over existing PEFT methods while preventing catastrophic forgetting.
	3. Public availability of the implementation as ready-to-use code.

**Result:** LoKI shows comparable or superior task-specific performance to full fine-tuning and LoRA methods across various models while better preserving general capabilities.

**Limitations:** 

**Conclusion:** The approach connects insights into LLM knowledge storage with practical fine-tuning objectives, achieving optimal trade-offs between specialization and general performance.

**Abstract:** Fine-tuning adapts pretrained models for specific tasks but poses the risk of catastrophic forgetting (CF), where critical knowledge from pre-training is overwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large Language Models (LLMs), while efficient, often sacrifice general capabilities. To address the issue of CF in a general-purpose PEFT framework, we propose \textbf{Lo}w-damage \textbf{K}nowledge \textbf{I}mplanting (\textbf{LoKI}), a PEFT technique that is based on a mechanistic understanding of how knowledge is stored in transformer architectures. In two real-world scenarios, LoKI demonstrates task-specific performance that is comparable to or even surpasses that of full fine-tuning and LoRA-based methods across various model types, while significantly better preserving general capabilities. Our work connects mechanistic insights into LLM knowledge storage with practical fine-tuning objectives, achieving state-of-the-art trade-offs between task specialization and the preservation of general capabilities. Our implementation is publicly available as ready-to-use code\footnote{https://github.com/Nexround/LoKI}.

</details>


### [74] [EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning](https://arxiv.org/abs/2505.22131)

*Zhuoyang Wu, Xinze Li, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Minghe Yu, Cheng Yang, Yu Gu, Ge Yu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, Error Learning, Mathematical Reasoning, Supervised Fine-Tuning, Machine Learning

**Relevance Score:** 8

**TL;DR:** The EULER model enhances LLMs' mathematical problem-solving by generating high-quality solution errors during Supervised Fine-Tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving the performance of LLMs in mathematical problem-solving through error learning.

**Method:** Introducing the Error-IndUced LEaRning (EULER) model to generate solution errors and optimize error exposure for better learning outcomes.

**Key Contributions:**

	1. Development of the EULER model for generating solution errors
	2. Demonstrated improvement in LLM performance on mathematical tasks
	3. Synthesis of educationally valuable solution errors

**Result:** The EULER model improves performance by over 4% compared to baseline models and synthesizes more challenging solution errors.

**Limitations:** 

**Conclusion:** EULER effectively facilitates training and inference in LLMs by generating high-quality errors.

**Abstract:** Large Language Models (LLMs) have demonstrated strong reasoning capabilities and achieved promising results in mathematical problem-solving tasks. Learning from errors offers the potential to further enhance the performance of LLMs during Supervised Fine-Tuning (SFT). However, the errors in synthesized solutions are typically gathered from sampling trails, making it challenging to generate solution errors for each mathematical problem. This paper introduces the Error-IndUced LEaRning (EULER) model, which aims to develop an error exposure model that generates high-quality solution errors to enhance the mathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the error exposure model to increase the generation probability of self-made solution errors while utilizing solutions produced by a superior LLM to regularize the generation quality. Our experiments across various mathematical problem datasets demonstrate the effectiveness of the EULER model, achieving an improvement of over 4% compared to all baseline models. Further analysis reveals that EULER is capable of synthesizing more challenging and educational solution errors, which facilitate both the training and inference processes of LLMs. All codes are available at https://github.com/NEUIR/EULER.

</details>


### [75] [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)

*Yuichiro Hoshino, Hideyuki Tachibana, Muneyoshi Inahara, Hiroto Takegawa*

**Main category:** cs.CL

**Keywords:** Hybrid Models, Transformers, State Space Models, Self-Distillation, Performance Optimization

**Relevance Score:** 7

**TL;DR:** The paper introduces RAD, a framework for optimizing hybrid Transformer and State Space Models by identifying and replacing redundant layers, achieving significant performance improvements in mathematical and coding tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address redundancy in hybrid models consisting of Transformers and State Space Models, aiming to enhance their performance and efficiency.

**Method:** The RAD framework utilizes self-speculative decoding to detect redundant attention layers, which are then replaced by SSM components followed by self-distillation focused on those components.

**Key Contributions:**

	1. Introduction of the RAD framework for redundancy-aware optimization
	2. Demonstration of significant performance boosts in mathematical and coding tasks
	3. Achievement of faster convergence in knowledge distillation settings

**Result:** RAD significantly improves task performance, surpassing the original model, with faster convergence in knowledge distillation settings; it achieves higher scores on benchmarks using a smaller teacher model.

**Limitations:** 

**Conclusion:** RAD provides an effective method for optimizing hybrid models and enhancing performance in various tasks while maintaining efficiency.

**Abstract:** Hybrid models combining Transformers and State Space Models (SSMs) are promising for balancing performance and efficiency. However, optimizing these hybrid models, particularly by addressing the potential redundancy inherent within the Transformer components, remains a significant challenge. In this paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that uses self-speculative decoding as a diagnostic tool to identify redundant attention layers within the model. These identified layers are then selectively replaced with SSM components, followed by targeted (self-)distillation. Specifically, RAD focuses knowledge transfer on the components identified as redundant, considering architectural changes and specific weight initialization strategies. We experimentally demonstrate that self-distillation using RAD significantly surpasses the performance of the original base model on mathematical and coding tasks. Furthermore, RAD is also effective in standard knowledge distillation settings, achieving up to approximately 2x faster convergence compared to baseline methods. Notably, while a baseline model distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and 22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and 28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers a new pathway for efficient optimization and performance enhancement in the distillation of hybrid models.

</details>


### [76] [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)

*Marc Feger, Katarina Boland, Stefan Dietze*

**Main category:** cs.CL

**Keywords:** argument mining, transformers, BERT, generalization, discourse analysis

**Relevance Score:** 6

**TL;DR:** This study re-evaluates BERT-like transformers for argument identification across various contexts, highlighting their generalization weaknesses and suggesting improvements through task-specific pre-training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance automated discourse analysis and understand the effectiveness of argument mining models across diverse contexts.

**Method:** Evaluation of four transformers on 17 sentence-level datasets, including one enhanced with contrastive pre-training, focusing on ability to generalize in argument identification.

**Key Contributions:**

	1. Large-scale re-evaluation of argument identification transformers
	2. Identification of performance drop on unseen datasets
	3. Recommendations for improving generalization in argument mining models

**Result:** Models demonstrated dependency on lexical shortcuts related to content words, performing well on familiar benchmarks but failing on unseen datasets.

**Limitations:** Models rely on dataset-specific cues rather than true task alignment, raising concerns on robustness across diverse datasets.

**Conclusion:** Task-specific pre-training and joint benchmark training effectively enhance model robustness and generalization.

**Abstract:** Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization.

</details>


### [77] [InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing](https://arxiv.org/abs/2505.22156)

*Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Wai Lam*

**Main category:** cs.CL

**Keywords:** model editing, large language models, context encoding, in-context learning, cross-attention

**Relevance Score:** 8

**TL;DR:** The paper introduces InComeS, a framework that enhances LLMs' editing capabilities by compressing context into a KV cache and using cross-attention to select relevant information, addressing the limitations of traditional model editing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing model editing methods struggle with complex scenarios that require semantic understanding.

**Method:** InComeS compresses editing contexts into a KV cache using a special gist token and employs cross-attention modules for dynamic selection of relevant information.

**Key Contributions:**

	1. Proposes a new framework, InComeS, for model editing.
	2. Utilizes a KV cache mechanism for better context management.
	3. Implements cross-attention for dynamic relevance selection.

**Result:** Experiments show that InComeS improves efficiency and effectiveness in model editing tasks across various benchmarks and formats.

**Limitations:** The framework's performance may vary based on the specific characteristics of the edits.

**Conclusion:** InComeS offers a promising solution for enhancing LLM context handling in editing scenarios, thereby overcoming the limitations of fixed context windows.

**Abstract:** Although existing model editing methods perform well in recalling exact edit facts, they often struggle in complex scenarios that require deeper semantic understanding rather than mere knowledge regurgitation. Leveraging the strong contextual reasoning abilities of large language models (LLMs), in-context learning (ICL) becomes a promising editing method by comprehending edit information through context encoding. However, this method is constrained by the limited context window of LLMs, leading to degraded performance and efficiency as the number of edits increases. To overcome this limitation, we propose InComeS, a flexible framework that enhances LLMs' ability to process editing contexts through explicit compression and selection mechanisms. Specifically, InComeS compresses each editing context into the key-value (KV) cache of a special gist token, enabling efficient handling of multiple edits without being restricted by the model's context window. Furthermore, specialized cross-attention modules are added to dynamically select the most relevant information from the gist pools, enabling adaptive and effective utilization of edit information. We conduct experiments on diverse model editing benchmarks with various editing formats, and the results demonstrate the effectiveness and efficiency of our method.

</details>


### [78] [Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy](https://arxiv.org/abs/2505.22157)

*Paramita Mirza, Lucas Weber, Fabian Küch*

**Main category:** cs.CL

**Keywords:** data selection, fine-tuning, large language models, embedding models, clustering

**Relevance Score:** 8

**TL;DR:** The paper presents a multi-step pipeline for efficient and universal data selection in fine-tuning large language models (LLMs), aimed at reducing computational costs while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the computational costs of data selection for LLMs and to allow for a more diverse dataset while ensuring high performance.

**Method:** A multi-step pipeline that bins data points, estimates quality with specialized models, and scores difficulty using a lightweight method. Task-based categorization is employed to control the dataset composition.

**Key Contributions:**

	1. Introduces an efficient multi-step pipeline for data selection.
	2. Demonstrates effective task-based categorization for dataset control.
	3. Implements a novel approach using embedding models and clustering algorithms to ensure diversity.

**Result:** The proposed method allows for high-performance fine-tuning of LLMs with reduced computational overhead, ensuring a diverse and well-organized dataset for model training.

**Limitations:** 

**Conclusion:** The integrated strategy significantly improves data selection efficiency and model performance while minimizing costs and promoting diversity in the training data.

**Abstract:** Recent work shows that post-training datasets for LLMs can be substantially downsampled without noticeably deteriorating performance. However, data selection often incurs high computational costs or is limited to narrow domains. In this paper, we demonstrate that data selection can be both -- efficient and universal -- by using a multi-step pipeline in which we efficiently bin data points into groups, estimate quality using specialized models, and score difficulty with a robust, lightweight method. Task-based categorization allows us to control the composition of our final data -- crucial for finetuning multi-purpose models. To guarantee diversity, we improve upon previous work using embedding models and a clustering algorithm. This integrated strategy enables high-performance fine-tuning with minimal overhead.

</details>


### [79] [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)

*Bocheng Li, Zhujin Gao, Linli Xu*

**Main category:** cs.CL

**Keywords:** Diffusion Models, Text Generation, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** NeoDiff combines discrete and continuous diffusion models for improved text generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing diffusion models have limitations in token processing and semantic nuance capture, necessitating a more effective approach.

**Method:** NeoDiff applies a Poisson diffusion process for flexible noising and uses a time predictor to adjust denoising based on token semantics, alongside an optimized inference schedule.

**Key Contributions:**

	1. Introduction of a Poisson diffusion process for flexibility in noising.
	2. Implementation of a time predictor for adaptively modulating denoising progress.
	3. Demonstration of superior performance compared to traditional models.

**Result:** Experimental results show that NeoDiff outperforms non-autoregressive continuous and discrete diffusion models, iterative-based methods, and autoregressive diffusion-based methods in text generation tasks.

**Limitations:** 

**Conclusion:** NeoDiff provides a unified framework enhancing text generation quality and efficiency.

**Abstract:** Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.

</details>


### [80] [ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments](https://arxiv.org/abs/2505.22169)

*Gili Lior, Eliya Habba, Shahar Levy, Avi Caciularu, Gabriel Stanovsky*

**Main category:** cs.CL

**Keywords:** LLMs, prompt sensitivity, evaluation framework, ReliableEval, stochastic methods

**Relevance Score:** 8

**TL;DR:** This paper introduces ReliableEval, a framework for evaluating the sensitivity of LLMs to prompt perturbations, highlighting the necessity for reliable evaluations in light of prompt variability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address concerns about the reliability of LLM evaluations that typically use single prompt benchmarks, which may not reflect true model performance due to prompt sensitivity.

**Method:** The authors propose a stochastic method of moments evaluation and a formal definition of reliable evaluation, focusing on prompt perturbations that preserve meaning. They present ReliableEval to determine the number of prompt resamplings required for meaningful results.

**Key Contributions:**

	1. Introduction of ReliableEval for evaluating LLM prompt sensitivity
	2. Formal definition of reliable evaluation considering prompt perturbations
	3. Stochastic evaluation revealing sensitivity in top LLMs

**Result:** The framework was applied to five leading LLMs, revealing that even top performers like GPT-4o and Claude-3.7-Sonnet show significant prompt sensitivity, indicating that traditional evaluation methods may underestimate variability in performance.

**Limitations:** The approach may need further validation across more diverse tasks and metrics to strengthen its applicability.

**Conclusion:** The study concludes that their model-, task-, and metric-agnostic approach offers a robust way to evaluate LLMs, emphasizing the importance of incorporating prompt sensitivity in assessments.

**Abstract:** LLMs are highly sensitive to prompt phrasing, yet standard benchmarks typically report performance using a single prompt, raising concerns about the reliability of such evaluations. In this work, we argue for a stochastic method of moments evaluation over the space of meaning-preserving prompt perturbations. We introduce a formal definition of reliable evaluation that accounts for prompt sensitivity, and suggest ReliableEval - a method for estimating the number of prompt resamplings needed to obtain meaningful results. Using our framework, we stochastically evaluate five frontier LLMs and find that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit substantial prompt sensitivity. Our approach is model-, task-, and metric-agnostic, offering a recipe for meaningful and robust LLM evaluation.

</details>


### [81] [Reverse Preference Optimization for Complex Instruction Following](https://arxiv.org/abs/2505.22172)

*Xiang Huang, Ting-En Lin, Feiteng Fang, Yuchuan Wu, Hangyu Li, Yuzhong Qu, Fei Huang, Yongbin Li*

**Main category:** cs.CL

**Keywords:** large language models, instruction following, preference optimization, RPO, AI

**Relevance Score:** 8

**TL;DR:** This paper presents Reverse Preference Optimization (RPO), a method that improves instruction following in large language models by dynamically reversing constraints to mitigate noise in preference pairs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the capability of large language models to follow complex instructions with multiple constraints, which has been challenging with existing methods.

**Method:** The proposed method, Reverse Preference Optimization (RPO), reverses constraints within instructions to ensure chosen responses meet all requirements, while reducing noise in preference selection.

**Key Contributions:**

	1. Introduction of Reverse Preference Optimization (RPO) for instruction following in LLMs.
	2. Demonstrated improvements over existing methods with quantifiable results on benchmark datasets.
	3. Effective scaling of RPO across various model sizes from 8B to 70B parameters.

**Result:** RPO demonstrated average improvements of 4.6 and 2.5 points over the DPO baseline on two multi-turn instruction following benchmarks, Sysbench and Multi-IF, respectively.

**Limitations:** 

**Conclusion:** RPO not only improves performance on instruction-following tasks but also scales effectively across different model sizes, achieving notable results with the largest model surpassing GPT-4o.

**Abstract:** Instruction following (IF) is a critical capability for large language models (LLMs). However, handling complex instructions with multiple constraints remains challenging. Previous methods typically select preference pairs based on the number of constraints they satisfy, introducing noise where chosen examples may fail to follow some constraints and rejected examples may excel in certain respects over the chosen ones. To address the challenge of aligning with multiple preferences, we propose a simple yet effective method called Reverse Preference Optimization (RPO). It mitigates noise in preference pairs by dynamically reversing the constraints within the instruction to ensure the chosen response is perfect, alleviating the burden of extensive sampling and filtering to collect perfect responses. Besides, reversal also enlarges the gap between chosen and rejected responses, thereby clarifying the optimization direction and making it more robust to noise. We evaluate RPO on two multi-turn IF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over the DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively. Moreover, RPO scales effectively across model sizes (8B to 70B parameters), with the 70B RPO model surpassing GPT-4o.

</details>


### [82] [TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation](https://arxiv.org/abs/2505.22176)

*Vihang Pancholi, Jainit Bafna, Tejas Anvekar, Manish Shrivastava, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** Table Evaluation, Structural Alignment, Semantic Comparison, Qualitative Analysis, Machine Learning

**Relevance Score:** 5

**TL;DR:** This paper introduces TabXEval, a comprehensive framework for evaluating tables through structural alignment and semantic comparison, addressing limitations of traditional evaluation metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional metrics for table evaluation often overlook structural and content discrepancies, necessitating a more nuanced approach for accurate assessments.

**Method:** TabXEval employs a two-phase evaluation process with TabAlign for structural alignment and TabCompare for semantic and syntactic comparison.

**Key Contributions:**

	1. Introduction of a novel evaluation rubric for table comparison
	2. Development of TabXEval framework for structured alignment and semantic comparison
	3. Creation of TabXBench benchmark for assessing table evaluation methods

**Result:** The effectiveness of TabXEval was validated using TabXBench, a benchmark featuring realistic table perturbations and human-annotated assessments, showcasing better qualitative and quantitative evaluations than existing methods.

**Limitations:** 

**Conclusion:** TabXEval serves as a robust framework for table evaluation, facilitating clearer evaluations and identifying subtle discrepancies in various domains.

**Abstract:** Evaluating tables qualitatively & quantitatively presents a significant challenge, as traditional metrics often fail to capture nuanced structural and content discrepancies. To address this, we introduce a novel, methodical rubric integrating multi-level structural descriptors with fine-grained contextual quantification, thereby establishing a robust foundation for comprehensive table comparison. Building on this foundation, we propose TabXEval, an eXhaustive and eXplainable two-phase evaluation framework. TabXEval initially aligns reference tables structurally via TabAlign & subsequently conducts a systematic semantic and syntactic comparison using TabCompare; this approach clarifies the evaluation process and pinpoints subtle discrepancies overlooked by conventional methods. The efficacy of this framework is assessed using TabXBench, a novel, diverse, multi-domain benchmark we developed, featuring realistic table perturbations and human-annotated assessments. Finally, a systematic analysis of existing evaluation methods through sensitivity-specificity trade-offs demonstrates the qualitative and quantitative effectiveness of TabXEval across diverse table-related tasks and domains, paving the way for future innovations in explainable table evaluation.

</details>


### [83] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)

*Yudi Zhang, Weilin Zhao, Xu Han, Tiejun Zhao, Wang Xu, Hailong Cao, Conghui Zhu*

**Main category:** cs.CL

**Keywords:** speculative decoding, quantization, large language models, inference speedup, memory bandwidth

**Relevance Score:** 8

**TL;DR:** This paper investigates the integration of speculative decoding and quantization for accelerating inference in large language models, leading to a new hierarchical decoding framework that achieves substantial speedup.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the memory bandwidth bottleneck in large language model inference and to explore the integration of speculative decoding and quantization techniques for better performance.

**Method:** The study combines speculative decoding via the EAGLE-2 method with 4-bit weight quantization, and introduces a hierarchical framework using a small model for enhancing memory access and processing efficiency.

**Key Contributions:**

	1. Introduction of a hierarchical speculative decoding framework
	2. Demonstrated effective integration of speculative decoding and quantization
	3. Achieved significant speedup in inference for large language models

**Result:** The hierarchical approach provides a 2.78x speedup in inference time compared to EAGLE-2 on a 4-bit weight quantized Llama-3-70B model, demonstrating enhanced performance while mitigating computational load.

**Limitations:** The study primarily focuses on Llama-3-70B models and may not generalize to all architectures or scenarios.

**Conclusion:** The proposed hierarchical framework outperforms traditional methods by efficiently combining the strengths of speculative decoding and weight quantization.

**Abstract:** Speculative decoding and quantization effectively accelerate memory-bound inference of large language models. Speculative decoding mitigates the memory bandwidth bottleneck by verifying multiple tokens within a single forward pass, which increases computational effort. Quantization achieves this optimization by compressing weights and activations into lower bit-widths and also reduces computations via low-bit matrix multiplications. To further leverage their strengths, we investigate the integration of these two techniques. Surprisingly, experiments applying the advanced speculative decoding method EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit weight quantization are diminished by the computational load from speculative decoding. Specifically, verifying a tree-style draft incurs significantly more time overhead than a single-token forward pass on 4-bit weight quantized models. This finding led to our new speculative decoding design: a hierarchical framework that employs a small model as an intermediate stage to turn tree-style drafts into sequence drafts, leveraging the memory access benefits of the target quantized model. Experimental results show that our hierarchical approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$. Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [84] [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)

*Xuchen Ma, Jianxiang Yu, Wenming Shao, Bo Pang, Xiang Li*

**Main category:** cs.CL

**Keywords:** cloaked toxicity, Chinese social media, content moderation, BERT, LLM

**Relevance Score:** 4

**TL;DR:** C$^2$TU is a novel method for uncovering cloaked toxicity in Chinese social media, achieving significant performance improvements over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in toxic content on social media necessitates effective methods for content moderation, specifically targeting the unique challenges presented by cloaked toxicity in languages like Chinese.

**Method:** C$^2$TU uses substring matching with a focus on recognizing Chinese homographs and a toxic lexicon to identify candidate toxic words, complemented by two model variants leveraging BERT and LLMs for filtering and correcting cloaked toxicity.

**Key Contributions:**

	1. Development of a training-free and prompt-free method for toxic content unveiling in Chinese.
	2. Utility of substring matching for identifying toxic words using homographs.
	3. Significant performance improvements compared to existing methods.

**Result:** C$^2$TU demonstrates superior performance on two Chinese toxic datasets, outperforming competitors by up to 71% on F1 score and 35% on accuracy.

**Limitations:** 

**Conclusion:** The proposed method provides an effective solution for unveiling cloaked toxic content in Chinese, contributing to improved content moderation capabilities.

**Abstract:** Social media platforms have experienced a significant rise in toxic content, including abusive language and discriminatory remarks, presenting growing challenges for content moderation. Some users evade censorship by deliberately disguising toxic words through homophonic cloak, which necessitates the task of unveiling cloaked toxicity. Existing methods are mostly designed for English texts, while Chinese cloaked toxicity unveiling has not been solved yet. To tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free method for Chinese cloaked toxic content unveiling. It first employs substring matching to identify candidate toxic words based on Chinese homo-graph and toxic lexicon. Then it filters those candidates that are non-toxic and corrects cloaks to be their corresponding toxicities. Specifically, we develop two model variants for filtering, which are based on BERT and LLMs, respectively. For LLMs, we address the auto-regressive limitation in computing word occurrence probability and utilize the full semantic contexts of a text sequence to reveal cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve superior performance on two Chinese toxic datasets. In particular, our method outperforms the best competitor by up to 71% on the F1 score and 35% on accuracy, respectively.

</details>


### [85] [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)

*Hyeonbin Hwang, Byeongguk Jeon, Seungone Kim, Jiyeon Kim, Hoyeon Chang, Sohee Yang, Seungpil Won, Dohaeng Lee, Youbin Ahn, Minjoon Seo*

**Main category:** cs.CL

**Keywords:** language models, reasoning, embedding spaces, semantic embeddings, contextual embeddings

**Relevance Score:** 9

**TL;DR:** This paper explores adapting pretrained autoregressive language models to reason over structured semantic units in sentence space rather than raw tokens.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The work investigates whether language models can learn to reason using higher-level abstractions, contrasting human reasoning with token-level generation.

**Method:** The authors present a framework that adapts a pretrained token-level language model to predict continuous embeddings of next sentences, exploring semantic and contextual embedding paradigms.

**Key Contributions:**

	1. Development of a framework for embedding-level reasoning in LMs
	2. Competitive performance of contextual embeddings with reduced inference costs
	3. Introduction of SentenceLens for interpreting model states

**Result:** The study shows that contextual embeddings using continuous inference perform competitively with Chain-of-Thought reasoning while reducing inference-time computations by half across four domains.

**Limitations:** The findings are based on early work and may require further validation and development.

**Conclusion:** The results indicate that pretrained language models can effectively transition to abstract reasoning within latent embedding spaces, along with the introduction of a diagnostic tool, SentenceLens, for visualizing model states.

**Abstract:** Autoregressive language models (LMs) generate one token at a time, yet human reasoning operates over higher-level abstractions - sentences, propositions, and concepts. This contrast raises a central question- Can LMs likewise learn to reason over structured semantic units rather than raw token sequences? In this work, we investigate whether pretrained LMs can be lifted into such abstract reasoning spaces by building on their learned representations. We present a framework that adapts a pretrained token-level LM to operate in sentence space by autoregressively predicting continuous embeddings of next sentences. We explore two embedding paradigms inspired by classical representation learning: 1) semantic embeddings, learned via autoencoding to preserve surface meaning; and 2) contextual embeddings, trained via next-sentence prediction to encode anticipatory structure. We evaluate both under two inference regimes: Discretized, which decodes each predicted embedding into text before re-encoding; and Continuous, which reasons entirely in embedding space for improved efficiency. Across four domains - mathematics, logic, commonsense, and planning - contextual embeddings under continuous inference show competitive performance with Chain-of-Thought (CoT) while reducing inference-time FLOPs on average by half. We also present early signs of scalability and modular adaptation. Finally, to visualize latent trajectories, we introduce SentenceLens, a diagnostic tool that decodes intermediate model states into interpretable sentences. Together, our results indicate that pretrained LMs can effectively transition to abstract, structured reasoning within latent embedding spaces.

</details>


### [86] [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)

*Mehdi Ali, Manuel Brack, Max Lübbering, Elias Wendt, Abbas Goher Khan, Richard Rutmann, Alex Jude, Maurice Kraus, Alexander Arno Weber, Felix Stollenwerk, David Kaczér, Florian Mai, Lucie Flek, Rafet Sifa, Nicolas Flores-Herr, Joachim Köhler, Patrick Schramowski, Michael Fromm, Kristian Kersting*

**Main category:** cs.CL

**Keywords:** multilingual datasets, large language models, data curation, cross-lingual performance, machine learning

**Relevance Score:** 8

**TL;DR:** Introduction of JQL, an efficient systematic approach for curating high-quality multilingual datasets for pretraining large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations in the availability of open-source multilingual datasets for effectively pretraining LLMs, which rely on heuristic filtering methods.

**Method:** JQL utilizes lightweight annotators based on pretrained multilingual embeddings to curate diverse and high-quality multilingual data efficiently.

**Key Contributions:**

	1. Introduction of a systematic approach for multilingual dataset curation
	2. Demonstrates robust multilingual performance for unseen languages
	3. Provides valuable resources for enhancing multilingual dataset quality

**Result:** Empirical evaluation shows that JQL outperforms current heuristic filtering methods, enhancing downstream model training quality and increasing data retention rates across 35 languages.

**Limitations:** 

**Conclusion:** JQL raises the standards of multilingual dataset development, providing practical insights and valuable resources for multilingual data curation.

**Abstract:** High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.

</details>


### [87] [A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity](https://arxiv.org/abs/2505.22236)

*Charlotte Pouw, Afra Alishahi, Willem Zuidema*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, syntactic sensitivity, intonational phrase boundaries, psycholinguistics, finetuning

**Relevance Score:** 6

**TL;DR:** Analysis of TTS systems' sensitivity to syntactic boundaries in sentence intonation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how TTS systems generate intonational phrase boundaries and their reliance on syntactic cues.

**Method:** Methods inspired by psycholinguistics to assess the performance of TTS systems on sentences with varying syntactic complexities.

**Key Contributions:**

	1. Identified challenges TTS systems face with ambiguous syntactic boundaries.
	2. Demonstrated that superficial cues are often needed for intonation placement.
	3. Showcased a finetuning method that leads to improved intonation reflecting sentence structure.

**Result:** TTS systems have difficulty with ambiguous syntactic boundaries and rely on superficial cues like commas; finetuning on sentences without commas improves intonation pattern.

**Limitations:** The study's focus is on specific sentence types and does not cover all possible syntactic nuances.

**Conclusion:** The finetuning approach enhances TTS systems' ability to reflect underlying sentence structure through distinct intonation patterns.

**Abstract:** We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using methods inspired by psycholinguistic research. Specifically, we focus on the generation of intonational phrase boundaries, which can often be predicted by identifying syntactic boundaries within a sentence. We find that TTS systems struggle to accurately generate intonational phrase boundaries in sentences where syntactic boundaries are ambiguous (e.g., garden path sentences or sentences with attachment ambiguity). In these cases, systems need superficial cues such as commas to place boundaries at the correct positions. In contrast, for sentences with simpler syntactic structures, we find that systems do incorporate syntactic cues beyond surface markers. Finally, we finetune models on sentences without commas at the syntactic boundary positions, encouraging them to focus on more subtle linguistic cues. Our findings indicate that this leads to more distinct intonation patterns that better reflect the underlying structure.

</details>


### [88] [BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain](https://arxiv.org/abs/2505.22240)

*Yunsoo Kim, Yusuf Abdulle, Honghan Wu*

**Main category:** cs.CL

**Keywords:** multi-hop reasoning, biomedical domain, benchmark, large language models, knowledge graphs

**Relevance Score:** 8

**TL;DR:** Introducing BioHopR, a benchmark for evaluating multi-hop reasoning in biomedical knowledge graphs, revealing significant performance gaps in existing models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of benchmarks for multi-hop reasoning in the biomedical domain, particularly focusing on complex entity relationships.

**Method:** Development of BioHopR, a benchmark leveraging the PrimeKG dataset, featuring tasks for 1-hop and 2-hop reasoning to reflect real-world biomedical scenarios.

**Key Contributions:**

	1. Introduction of BioHopR benchmark for multi-hop reasoning in biomedicine
	2. Evaluation of state-of-the-art models with clear performance metrics
	3. Highlighting critical performance gaps in reasoning capabilities of existing models

**Result:** State-of-the-art LLMs show a significant drop in performance on tasks requiring multi-hop reasoning; O3-mini outperforms others in 1-hop tasks but struggles in 2-hop tasks.

**Limitations:** Focus is on entity relationships within structured biomedical knowledge graphs, which may not address all domains of biomedical reasoning.

**Conclusion:** BioHopR establishes a new standard for assessing reasoning capabilities in biomedical applications and highlights the disparities in performance between proprietary and open-source models.

**Abstract:** Biomedical reasoning often requires traversing interconnected relationships across entities such as drugs, diseases, and proteins. Despite the increasing prominence of large language models (LLMs), existing benchmarks lack the ability to evaluate multi-hop reasoning in the biomedical domain, particularly for queries involving one-to-many and many-to-many relationships. This gap leaves the critical challenges of biomedical multi-hop reasoning underexplored. To address this, we introduce BioHopR, a novel benchmark designed to evaluate multi-hop, multi-answer reasoning in structured biomedical knowledge graphs. Built from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop reasoning tasks that reflect real-world biomedical complexities.   Evaluations of state-of-the-art models reveal that O3-mini, a proprietary reasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on 2-hop tasks, outperforming proprietary models such as GPT4O and open-source biomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all models exhibit significant declines in multi-hop performance, underscoring the challenges of resolving implicit reasoning steps in the biomedical domain. By addressing the lack of benchmarks for multi-hop reasoning in biomedical domain, BioHopR sets a new standard for evaluating reasoning capabilities and highlights critical gaps between proprietary and open-source models while paving the way for future advancements in biomedical LLMs.

</details>


### [89] [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)

*Maximiliano Hormazábal Lagos, Álvaro Bueno Saez, Héctor Cerezo-Costas, Pedro Alonso Doval, Jorge Alcalde Vesteiro*

**Main category:** cs.CL

**Keywords:** Question-Answering, Tabular Data, LLMs, Python Code Generation, SemEval 2025

**Relevance Score:** 8

**TL;DR:** This paper presents a strategy that utilizes Python code generation with LLMs for answering questions from tabular data in the SemEval 2025 Task 8 challenge, achieving a score of 70.50% for subtask 1.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenge of question-answering over tabular data, specifically in the context of the SemEval 2025 Task 8.

**Method:** The proposed method involves multiple steps: understanding the content of the table, generating natural language instructions, translating these into Python code, executing the code, and handling errors, all employing open-source LLMs and optimized prompts.

**Key Contributions:**

	1. Utilization of LLMs for Python code generation in question-answering over tables
	2. Development of a multi-step approach for understanding and processing table content
	3. Achievement of a competitive score in a recognized task, showcasing practicality and effectiveness.

**Result:** The approach resulted in a score of 70.50% for subtask 1 of the challenge, demonstrating its effectiveness.

**Limitations:** 

**Conclusion:** The authors conclude that leveraging LLMs for code generation can significantly aid in answering questions based on tabular data, providing a systematic approach for future tasks.

**Abstract:** In this paper we expose our approach to solve the \textit{SemEval 2025 Task 8: Question-Answering over Tabular Data} challenge. Our strategy leverages Python code generation with LLMs to interact with the table and get the answer to the questions. The process is composed of multiple steps: understanding the content of the table, generating natural language instructions in the form of steps to follow in order to get the answer, translating these instructions to code, running it and handling potential errors or exceptions. These steps use open source LLMs and fine grained optimized prompts for each task (step). With this approach, we achieved a score of $70.50\%$ for subtask 1.

</details>


### [90] [Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages](https://arxiv.org/abs/2505.22273)

*Shohei Higashiyama, Masao Utiyama*

**Main category:** cs.CL

**Keywords:** lexical normalization, Japanese, pretrained models, evaluation perspectives, informal expressions

**Relevance Score:** 7

**TL;DR:** This paper addresses lexical normalization in unsegmented languages, focusing on Japanese, by creating a dataset and developing normalization methods using pretrained models.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** To improve processing of informal expressions in user-generated text for unsegmented languages, specifically Japanese.

**Method:** The authors create a large-scale normalization dataset and develop normalization methods using state-of-the-art pretrained models, followed by experiments across various evaluation perspectives.

**Key Contributions:**

	1. Creation of a large-scale, multi-domain Japanese normalization dataset
	2. Development of normalization methods based on pretrained models
	3. Experiments conducted across multiple evaluation perspectives

**Result:** Both encoder-only and decoder-only approaches show promising results in terms of accuracy and efficiency for lexical normalization.

**Limitations:** 

**Conclusion:** The study provides insights into effective methods for lexical normalization, emphasizing the need for comprehensive evaluation across multiple perspectives.

**Abstract:** Lexical normalization research has sought to tackle the challenge of processing informal expressions in user-generated text, yet the absence of comprehensive evaluations leaves it unclear which methods excel across multiple perspectives. Focusing on unsegmented languages, we make three key contributions: (1) creating a large-scale, multi-domain Japanese normalization dataset, (2) developing normalization methods based on state-of-the-art pretrained models, and (3) conducting experiments across multiple evaluation perspectives. Our experiments show that both encoder-only and decoder-only approaches achieve promising results in both accuracy and efficiency.

</details>


### [91] [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)

*Zihan Xu, Haotian Ma, Gongbo Zhang, Yihao Ding, Chunhua Weng, Yifan Peng*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Evidence-based medicine, Clinical decision-making, Healthcare, Systematic review

**Relevance Score:** 9

**TL;DR:** This paper reviews how Natural Language Processing (NLP) can enhance evidence-based medicine (EBM) by improving clinical decision-making through the systematic application of NLP techniques across the five steps of EBM.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rapid growth of medical literature and the need for scientific evidence in clinical decisions, exploring NLP methods could significantly streamline EBM practices and improve healthcare outcomes.

**Method:** The authors conduct a systematic review of 129 research studies that utilize NLP in the context of EBM, focusing on how these methods can be aligned with the EBM framework.

**Key Contributions:**

	1. Systematic review of 129 studies on NLP in EBM.
	2. Identification of current limitations in applying NLP to EBM practices.
	3. Recommendations for future research directions to enhance NLP's role in EBM.

**Result:** The survey reveals that NLP can aid the EBM steps by improving evidence extraction, synthesis, appraisal, and summarization, thereby enhancing the efficiency of clinical workflows.

**Limitations:** The study highlights existing challenges in implementing NLP solutions effectively in EBM.

**Conclusion:** The findings indicate that while NLP has made notable contributions to EBM, there are still limitations and challenges that require further research to fully harness its potential.

**Abstract:** Evidence-based medicine (EBM) is at the forefront of modern healthcare, emphasizing the use of the best available scientific evidence to guide clinical decisions. Due to the sheer volume and rapid growth of medical literature and the high cost of curation, there is a critical need to investigate Natural Language Processing (NLP) methods to identify, appraise, synthesize, summarize, and disseminate evidence in EBM. This survey presents an in-depth review of 129 research studies on leveraging NLP for EBM, illustrating its pivotal role in enhancing clinical decision-making processes. The paper systematically explores how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise, Apply, and Assess. The review not only identifies current limitations within the field but also proposes directions for future research, emphasizing the potential for NLP to revolutionize EBM by refining evidence extraction, evidence synthesis, appraisal, summarization, enhancing data comprehensibility, and facilitating a more efficient clinical workflow.

</details>


### [92] [Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs](https://arxiv.org/abs/2505.22293)

*Samuel Frontull, Thomas Ströhle*

**Main category:** cs.CL

**Keywords:** Large Language Models, Machine Translation, Low-Resource Languages

**Relevance Score:** 8

**TL;DR:** This paper introduces Fragment-Shot Prompting and its extension, Pivoted Fragment-Shot, to enhance translation capabilities in low-resource languages using large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of using LLMs for low-resource languages, particularly in the context of prompt engineering and to improve machine translation quality.

**Method:** The authors introduce Fragment-Shot Prompting, which segments input and retrieves translation examples based on syntactic coverage, and an extension called Pivoted Fragment-Shot for translation without direct parallel data. They evaluate these methods using various LLMs.

**Key Contributions:**

	1. Introduction of Fragment-Shot Prompting and Pivoted Fragment-Shot methods.
	2. Demonstration of improved translation quality for low-resource languages with syntactic coverage.
	3. Release of code and retrieval corpora for public use.

**Result:** The evaluation reveals that Fragment-Shot Prompting improves translation quality for low-resource languages, with models that have stronger reasoning abilities being more effective. Additionally, it shows that prompt engineering has limited effectiveness when translating into high-resource languages.

**Limitations:** Limited improvements in translations from low-resource to high-resource languages through prompt engineering.

**Conclusion:** Fragment-Shot Prompting demonstrates effectiveness in improving translations in low-resource contexts, and the pivoting method significantly enhances translation quality among Ladin variants while highlighting limitations in prompt engineering for high-resource languages.

**Abstract:** Large Language Models (LLMs) have demonstrated strong capabilities in multilingual machine translation, sometimes even outperforming traditional neural systems. However, previous research has highlighted the challenges of using LLMs, particularly with prompt engineering, for low-resource languages. In this work, we introduce Fragment-Shot Prompting, a novel in-context learning method that segments input and retrieves translation examples based on syntactic coverage, along with Pivoted Fragment-Shot, an extension that enables translation without direct parallel data. We evaluate these methods using GPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between Italian and two Ladin variants, revealing three key findings: (1) Fragment-Shot Prompting is effective for translating into and between the studied low-resource languages, with syntactic coverage positively correlating with translation quality; (2) Models with stronger reasoning abilities make more effective use of retrieved knowledge, generally produce better translations, and enable Pivoted Fragment-Shot to significantly improve translation quality between the Ladin variants; and (3) prompt engineering offers limited, if any, improvements when translating from a low-resource to a high-resource language, where zero-shot prompting already yields satisfactory results. We publicly release our code and the retrieval corpora.

</details>


### [93] [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)

*Haosheng Zou, Xiaowei Lv, Shousheng Jia, Xiangzheng Zhang*

**Main category:** cs.CL

**Keywords:** LLaMA, sequence parallelism, model training

**Relevance Score:** 5

**TL;DR:** 360-LLaMA-Factory enhances LLaMA-Factory by adding sequence parallelism and has been widely adopted in various models and training frameworks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the training efficiency and performance of LLaMA models by incorporating sequence parallelism.

**Method:** The paper discusses the different sequence parallel modes implemented in 360-LLaMA-Factory and provides insights into its implementation.

**Key Contributions:**

	1. Open-sourcing 360-LLaMA-Factory
	2. Introducing sequence parallelism into LLaMA models
	3. Implementation insights on different sequence parallel modes

**Result:** 360-LLaMA-Factory is now open-sourced and has been utilized in various successful models and frameworks, gaining wide recognition.

**Limitations:** 

**Conclusion:** The introduction of sequence parallelism in LLaMA-Factory represents a significant advancement in model training efficiency.

**Abstract:** Adding sequence parallelism into LLaMA-Factory, we open-sourced 360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory. 360-LLaMA-Factory has received wide recognition and used in models such as Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and also in large companies' training frameworks. This technical report delves deeper into the different sequence parallel modes behind 360-LLaMA-Factory and discusses our implementation insights.

</details>


### [94] [Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing](https://arxiv.org/abs/2505.22298)

*Yifan Lu, Jing Li, Yigeng Zhou, Yihui Zhang, Wenya Wang, Xiucheng Li, Meishan Zhang, Fangming Liu, Jun Yu, Min Zhang*

**Main category:** cs.CL

**Keywords:** large language models, toxicity mitigation, knowledge editing, detoxification, SafeEdit

**Relevance Score:** 8

**TL;DR:** ToxEdit is a new approach for editing knowledge in LLMs to mitigate toxicity without compromising performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs are vulnerable to malicious prompts and have challenges with current detoxification methods.

**Method:** ToxEdit detects toxic activation patterns during model operation and uses adaptive pathways for toxicity mitigation.

**Key Contributions:**

	1. Introduction of ToxEdit for improved toxicity mitigation in LLMs
	2. Dynamic detection of toxic patterns during forward propagation
	3. Enhancement of SafeEdit benchmark with new evaluation tasks

**Result:** Experimental results show ToxEdit outperforms existing detoxification methods while maintaining the general capabilities of LLMs.

**Limitations:** 

**Conclusion:** ToxEdit provides an effective solution for mitigating toxicity in LLMs with minimal performance loss.

**Abstract:** Large language models (LLMs) exhibit impressive language capabilities but remain vulnerable to malicious prompts and jailbreaking attacks. Existing knowledge editing methods for LLM detoxification face two major challenges. First, they often rely on entity-specific localization, making them ineffective against adversarial inputs without explicit entities. Second, these methods suffer from over-editing, where detoxified models reject legitimate queries, compromising overall performance. In this paper, we propose ToxEdit, a toxicity-aware knowledge editing approach that dynamically detects toxic activation patterns during forward propagation. It then routes computations through adaptive inter-layer pathways to mitigate toxicity effectively. This design ensures precise toxicity mitigation while preserving LLMs' general capabilities. To more accurately assess over-editing, we also enhance the SafeEdit benchmark by incorporating instruction-following evaluation tasks. Experimental results on multiple LLMs demonstrate that our ToxEdit outperforms previous state-of-the-art methods in both detoxification performance and safeguarding general capabilities of LLMs.

</details>


### [95] [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)

*Ishwar B Balappanawar, Vamshi Krishna Bonagiri, Anish R Joishy, Manas Gaur, Krishnaprasad Thirunarayan, Ponnurangam Kumaraguru*

**Main category:** cs.CL

**Keywords:** Large Language Models, Counterfactuals, Logical Reasoning, Metacognitive Awareness, Self-Segregate

**Relevance Score:** 9

**TL;DR:** This paper introduces CounterLogic, a dataset evaluating LLMs' logical reasoning through counterfactual scenarios and proposes a prompting method, Self-Segregate, which improves reasoning accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reasoning capabilities of Large Language Models (LLMs) in contexts that conflict with their parametric knowledge.

**Method:** The authors created the CounterLogic dataset with 1,800 examples across 9 logical schemas and evaluated 11 LLMs using this dataset, applying a new prompting method called Self-Segregate.

**Key Contributions:**

	1. Introduction of the CounterLogic dataset for evaluating logical reasoning in LLMs
	2. Development of Self-Segregate prompting method to improve accuracy in counterfactual reasoning
	3. Insights into LLM's reasoning mechanisms similar to human cognitive processes

**Result:** Performance degradation of LLMs was observed, with accuracies dropping by 27% on average in counterfactual scenarios. After applying the Self-Segregate method, the performance gap narrowed to 11%, with an overall accuracy increase of +7.5%.

**Limitations:** 

**Conclusion:** The findings provide insights into enhancing LLMs' reasoning capabilities, featuring parallels to human cognitive processes in handling conflicting information.

**Abstract:** Large Language Models (LLMs) demonstrate impressive reasoning capabilities in familiar contexts, but struggle when the context conflicts with their parametric knowledge. To investigate this phenomenon, we introduce CounterLogic, a dataset containing 1,800 examples across 9 logical schemas, explicitly designed to evaluate logical reasoning through counterfactual (hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11 LLMs across 6 different datasets reveals a consistent performance degradation, with accuracies dropping by 27% on average when reasoning through counterfactual information. We propose Self-Segregate, a prompting method enabling metacognitive awareness (explicitly identifying knowledge conflicts) before reasoning. Our method dramatically narrows the average performance gaps from 27% to just 11%, while significantly increasing the overall accuracy (+7.5%). We discuss the implications of these findings and draw parallels to human cognitive processes, particularly on how humans disambiguate conflicting information during reasoning tasks. Our findings offer practical insights for understanding and enhancing LLMs reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge.

</details>


### [96] [Advancing Expert Specialization for Better MoE](https://arxiv.org/abs/2505.22323)

*Hongcan Guo, Haolang Lu, Guoshun Nan, Bolun Chu, Jialin Zhuang, Yuan Yang, Wenhao Che, Sicong Leng, Qimei Cui, Xudong Jiang*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, expert specialization, load balancing, orthogonality loss, variance loss

**Relevance Score:** 8

**TL;DR:** This paper proposes two complementary objectives to enhance expert specialization in Mixture-of-Experts models, improving performance during post-training without architectural changes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of expert overlap and uniform routing in Mixture-of-Experts models due to the commonly used load balancing loss.

**Method:** The proposed approach introduces an orthogonality loss to ensure distinct token processing by experts and a variance loss to promote discriminative routing decisions.

**Key Contributions:**

	1. Introduction of orthogonality and variance losses for expert specialization
	2. Significant performance improvements in various MoE architectures
	3. Compatibility with existing load balancing losses without architectural changes

**Result:** The method significantly improves expert specialization, achieving up to a 23.79% performance boost over classic MoE baselines while maintaining load balancing.

**Limitations:** 

**Conclusion:** The proposed methods contribute to optimizing the training process and improve downstream task performance, with plans to release code for community use.

**Abstract:** Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community.

</details>


### [97] [NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment](https://arxiv.org/abs/2505.22327)

*Antonia Karamolegkou, Angana Borah, Eunjung Cho, Sagnik Ray Choudhury, Martina Galletti, Rajarshi Ghosh, Pranav Gupta, Oana Ignat, Priyanka Kargupta, Neema Kotonya, Hemank Lamba, Sun-Joo Lee, Arushi Mangla, Ishani Mondal, Deniz Nazarova, Poli Nemkova, Dina Pisarevskaya, Naquee Rizwan, Nazanin Sabri, Dominik Stammbach, Anna Steinberg, David Tomás, Steven R Wilson, Bowen Yi, Jessica H Zhu, Arkaitz Zubiaga, Anders Søgaard, Alexander Fraser, Zhijing Jin, Rada Mihalcea, Joel R. Tetreault, Daryna Dementieva*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, AI for Social Good, responsible AI, societal challenges, emerging risks

**Relevance Score:** 8

**TL;DR:** The paper discusses the need for responsible and intentional deployment of NLP technologies, highlighting societal challenges and research directions aligned with AI for Social Good.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To emphasize the importance of responsible and equitable progress in Natural Language Processing (NLP) that addresses societal challenges.

**Method:** The paper conducts a cross-disciplinary analysis of social goals and emerging risks related to NLP applications.

**Key Contributions:**

	1. Highlighting the societal implications of NLP technologies.
	2. Identifying emerging risks in NLP deployment.
	3. Outlining research directions aligned with AI for Social Good.

**Result:** Identifies promising research directions and outlines challenges for ensuring responsible NLP deployment.

**Limitations:** 

**Conclusion:** Emphasizes the necessity of addressing societal challenges through an intentional and responsible approach in NLP research.

**Abstract:** Recent advancements in large language models (LLMs) have unlocked unprecedented possibilities across a range of applications. However, as a community, we believe that the field of Natural Language Processing (NLP) has a growing need to approach deployment with greater intentionality and responsibility. In alignment with the broader vision of AI for Social Good (Toma\v{s}ev et al., 2020), this paper examines the role of NLP in addressing pressing societal challenges. Through a cross-disciplinary analysis of social goals and emerging risks, we highlight promising research directions and outline challenges that must be addressed to ensure responsible and equitable progress in NLP4SG research.

</details>


### [98] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)

*Lai Wei, Yuting Li, Kaipeng Zheng, Chen Wang, Yue Wang, Linghe Kong, Lichao Sun, Weiran Huang*

**Main category:** cs.CL

**Keywords:** Multimodal LLMs, Reinforcement Learning, Supervised Fine-Tuning, Chain-of-Thought, Reasoning

**Relevance Score:** 9

**TL;DR:** This study explores enhancing multimodal reasoning in large language models through a two-stage approach involving supervised fine-tuning followed by reinforcement learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the existence of 'aha moment' patterns in multimodal LLMs before reinforcement learning training and their effects on reasoning performance.

**Method:** A two-stage approach: (1) Supervised fine-tuning (SFT) to incorporate structured chain-of-thought reasoning patterns and (2) Reinforcement learning using GRPO for further refinement.

**Key Contributions:**

	1. Demonstration of 'aha moment' patterns in MLLMs prior to RL training
	2. Development of a two-stage approach for enhancing multimodal reasoning
	3. Achievement of state-of-the-art performance in multimodal reasoning benchmarks

**Result:** The combined approach outperforms both SFT-only and RL-only methods on challenging multimodal reasoning benchmarks, achieving state-of-the-art performance among open-source MLLMs at both 3B and 7B scales.

**Limitations:** 

**Conclusion:** The research provides practical guidance for building advanced multimodal reasoning models with significant performance improvements over base models.

**Abstract:** Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [99] [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)

*Hanyang Wang, Lu Wang, Chaoyun Zhang, Tianjun Mao, Si Qin, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Natural Language Processing, Feedback Optimization

**Relevance Score:** 9

**TL;DR:** Text2Grad is a reinforcement-learning approach that transforms textual feedback into span-level gradients for optimizing language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional reinforcement learning from human feedback (RLHF) uses coarse rewards, leading to slow learning. This paper aims to improve interpretability and model performance by leveraging fine-grained textual critiques.

**Method:** Text2Grad turns free-form textual feedback into differentiable reward signals that enable fine-grained updates to model policies by aligning feedback phrases with token spans.

**Key Contributions:**

	1. Introduces a novel paradigm for transforming textual feedback into gradient updates for language models.
	2. Develops a robust feedback-annotation pipeline that connects critiques to token spans.
	3. Demonstrates significant improvements over existing RL approaches in various tasks.

**Result:** Text2Grad consistently outperforms scalar-reward RL and prompt-only approaches across tasks like summarization, code generation, and question answering, achieving better interpretability and task metrics.

**Limitations:** 

**Conclusion:** Natural language feedback converted to gradients is an effective signal for optimizing language model policy, enhancing both performance and understanding.

**Abstract:** Traditional RLHF optimizes language models with coarse, scalar rewards that mask the fine-grained reasons behind success or failure, leading to slow and opaque learning. Recent work augments RL with textual critiques through prompting or reflection, improving interpretability but leaving model parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm that turns free-form textual feedback into span-level gradients. Given human (or programmatic) critiques, Text2Grad aligns each feedback phrase with the relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates that directly refine the offending portions of the model's policy. This yields precise, feedback-conditioned adjustments instead of global nudges. Text2Grad is realized through three components: (1) a high-quality feedback-annotation pipeline that pairs critiques with token spans; (2) a fine-grained reward model that predicts span-level reward on answer while generating explanatory critiques; and (3) a span-level policy optimizer that back-propagates natural-language gradients. Across summarization, code generation, and question answering, Text2Grad consistently surpasses scalar-reward RL and prompt-only baselines, providing both higher task metrics and richer interpretability. Our results demonstrate that natural-language feedback, when converted to gradients, is a powerful signal for fine-grained policy optimization. The code for our method is available at https://github.com/microsoft/Text2Grad

</details>


### [100] [LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High](https://arxiv.org/abs/2505.22354)

*Judith Sieker, Clara Lachenmaier, Sina Zarrieß*

**Main category:** cs.CL

**Keywords:** Large Language Models, false presuppositions, political misinformation, linguistic analysis, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates how LLMs handle false presuppositions, focusing on the influence of linguistic factors in recognizing misleading assumptions.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if LLMs, like humans, can detect and correct false presuppositions, especially in sensitive contexts like misinformation.

**Method:** A systematic analysis of linguistic presuppositions was conducted, experimenting with OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's Mistral-7B-v03 with a newly created dataset focused on political contexts.

**Key Contributions:**

	1. Introduces a novel dataset for analyzing LLM handling of false presuppositions.
	2. Demonstrates varying recognition capabilities of different LLMs under political contexts.
	3. Establishes linguistic presupposition analysis as a tool for investigating misinformation.

**Result:** The experiments revealed that LLMs struggle to recognize false presuppositions, with varying performance based on factors like linguistic construction and scenario probability.

**Limitations:** The study focuses only on political contexts and specific LLMs, which may limit generalizability.

**Conclusion:** Linguistic presupposition analysis is crucial for revealing how political misinformation is reinforced in LLM outputs.

**Abstract:** This paper examines how LLMs handle false presuppositions and whether certain linguistic factors influence their responses to falsely presupposed content. Presuppositions subtly introduce information as given, making them highly effective at embedding disputable or false information. This raises concerns about whether LLMs, like humans, may fail to detect and correct misleading assumptions introduced as false presuppositions, even when the stakes of misinformation are high. Using a systematic approach based on linguistic presupposition analysis, we investigate the conditions under which LLMs are more or less sensitive to adopt or reject false presuppositions. Focusing on political contexts, we examine how factors like linguistic construction, political party, and scenario probability impact the recognition of false presuppositions. We conduct experiments with a newly created dataset and examine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's Mistral-7B-v03. Our results show that the models struggle to recognize false presuppositions, with performance varying by condition. This study highlights that linguistic presupposition analysis is a valuable tool for uncovering the reinforcement of political misinformation in LLM responses.

</details>


### [101] [Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition](https://arxiv.org/abs/2505.22375)

*Hanting Chen, Yasheng Wang, Kai Han, Dong Li, Lin Li, Zhenni Bi, Jinpeng Li, Haoyu Wang, Fei Mi, Mingjian Zhu, Bin Wang, Kaikai Song, Yifei Fu, Xu He, Yu Luo, Chong Zhu, Quan He, Xueyu Wu, Wei He, Hailin Hu, Yehui Tang, Dacheng Tao, Xinghao Chen, Yunhe Wang, Other Contributors*

**Main category:** cs.CL

**Keywords:** Large Language Model, Reasoning, Reinforcement Learning, Inference Latency, Dual-System Framework

**Relevance Score:** 8

**TL;DR:** Pangu Embedded is an efficient Large Language Model (LLM) that leverages a dual-system framework to optimize reasoning capabilities and reduce computational costs and inference latency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address high computational costs and inference latency in existing reasoning-optimized LLMs.

**Method:** A two-stage training framework, first using iterative distillation for model fine-tuning and reinforcement learning with a novel reward system, followed by a dual-system framework for fast and slow thinking modes.

**Key Contributions:**

	1. Introduction of a two-stage training framework combining distillation and reinforcement learning.
	2. Development of a dual-system framework for optimized reasoning capabilities.
	3. Demonstration of superior performance on benchmarks compared to similar models.

**Result:** Pangu Embedded outperforms similar models in benchmarks, demonstrating rapid response times and superior reasoning quality.

**Limitations:** 

**Conclusion:** The architecture shows promise for developing efficient LLM reasoners that can balance resource allocation effectively.

**Abstract:** This work presents Pangu Embedded, an efficient Large Language Model (LLM) reasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible fast and slow thinking capabilities. Pangu Embedded addresses the significant computational costs and inference latency challenges prevalent in existing reasoning-optimized LLMs. We propose a two-stage training framework for its construction. In Stage 1, the model is finetuned via an iterative distillation process, incorporating inter-iteration model merging to effectively aggregate complementary knowledge. This is followed by reinforcement learning on Ascend clusters, optimized by a latency-tolerant scheduler that combines stale synchronous parallelism with prioritized data queues. The RL process is guided by a Multi-source Adaptive Reward System (MARS), which generates dynamic, task-specific reward signals using deterministic metrics and lightweight LLM evaluators for mathematics, coding, and general problem-solving tasks. Stage 2 introduces a dual-system framework, endowing Pangu Embedded with a "fast" mode for routine queries and a deeper "slow" mode for complex inference. This framework offers both manual mode switching for user control and an automatic, complexity-aware mode selection mechanism that dynamically allocates computational resources to balance latency and reasoning depth. Experimental results on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate that Pangu Embedded with 7B parameters, outperforms similar-size models like Qwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art reasoning quality within a single, unified model architecture, highlighting a promising direction for developing powerful yet practically deployable LLM reasoners.

</details>


### [102] [RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning](https://arxiv.org/abs/2505.22430)

*Kun Li, Yunxiang Li, Tianhua Zhang, Hongyin Luo, Xixin Wu, James Glass, Helen Meng*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, evaluation framework, reinforcement learning, rule-guided reasoning, interpretability

**Relevance Score:** 8

**TL;DR:** RAG-Zeval is a framework for evaluating retrieval-augmented generation (RAG) systems using rule-guided reasoning and reinforcement learning, enhancing efficiency and interpretability in performance assessment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and high computational costs associated with current LLM-based evaluation frameworks for retrieval-augmented generation systems.

**Method:** A novel end-to-end evaluation framework called RAG-Zeval, which formulates evaluation as a rule-guided reasoning task, utilizing reinforcement learning for training evaluators.

**Key Contributions:**

	1. Introduction of RAG-Zeval, an efficient evaluation framework for RAG systems
	2. Use of a ranking-based outcome reward mechanism to improve assessment accuracy
	3. Demonstration of superior interpretability and correlation with human judgments compared to existing LLM-based methods.

**Result:** RAG-Zeval achieves the strongest correlation with human judgments and outperforms LLMs with 10-100 times more parameters, while also providing better interpretability.

**Limitations:** 

**Conclusion:** The proposed framework significantly improves the evaluation process for RAG systems by generating comprehensive assessments efficiently.

**Abstract:** Robust evaluation is critical for deploying trustworthy retrieval-augmented generation (RAG) systems. However, current LLM-based evaluation frameworks predominantly rely on directly prompting resource-intensive models with complex multi-stage prompts, underutilizing models' reasoning capabilities and introducing significant computational cost. In this paper, we present RAG-Zeval (RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness and correctness evaluation as a rule-guided reasoning task. Our approach trains evaluators with reinforcement learning, facilitating compact models to generate comprehensive and sound assessments with detailed explanation in one-pass. We introduce a ranking-based outcome reward mechanism, using preference judgments rather than absolute scores, to address the challenge of obtaining precise pointwise reward signals. To this end, we synthesize the ranking references by generating quality-controlled responses with zero human annotation. Experiments demonstrate RAG-Zeval's superior performance, achieving the strongest correlation with human judgments and outperforming baselines that rely on LLMs with 10-100 times more parameters. Our approach also exhibits superior interpretability in response evaluation.

</details>


### [103] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)

*Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun*

**Main category:** cs.CL

**Keywords:** Multi-modal Large Language Models, Reinforcement Learning, Unsupervised Learning, Self-rewarding Mechanism, Continual Learning

**Relevance Score:** 8

**TL;DR:** This paper proposes MM-UPT, a novel framework for unsupervised post-training of Multi-modal Large Language Models (MLLMs) using a self-rewarding mechanism and an online RL algorithm called GRPO, demonstrating significant improvements in reasoning without requiring manually annotated data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for improving MLLMs in the post-training stage often rely on costly supervised fine-tuning and complex unsupervised methods, creating a need for a simpler, more scalable approach that doesn't require external supervision.

**Method:** The authors introduce MM-UPT, which utilizes the GRPO algorithm and replaces traditional reward mechanisms with a self-rewarding method based on majority voting across multiple sampled responses, enabling continual self-improvement of MLLMs.

**Key Contributions:**

	1. Introduction of the MM-UPT framework for unsupervised post-training of MLLMs.
	2. Implementation of a self-rewarding mechanism using majority voting for ongoing learning.
	3. Demonstration of significant performance improvements on established benchmarks without ground truth labels.

**Result:** The experiments indicate that MM-UPT enhances the reasoning abilities of the MLLM Qwen2.5-VL-7B significantly, achieving improvements in performance metrics on standard datasets, even outperforming previous unsupervised methods and approaching supervised GRPO results.

**Limitations:** 

**Conclusion:** MM-UPT represents a promising new paradigm for the autonomous enhancement of MLLMs, allowing for continual self-improvement without the dependency on external, annotated data.

**Abstract:** Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9 %$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [104] [EvolveSearch: An Iterative Self-Evolving Search Agent](https://arxiv.org/abs/2505.22501)

*Dingchu Zhang, Yida Zhao, Jialong Wu, Baixuan Li, Wenbiao Yin, Liwen Zhang, Yong Jiang, Yufeng Li, Kewei Tu, Pengjun Xie, Fei Huang*

**Main category:** cs.CL

**Keywords:** large language models, web search, reinforcement learning, self-evolution, multi-hop question answering

**Relevance Score:** 9

**TL;DR:** EvolveSearch is a novel framework enhancing LLM web search capabilities by combining supervised fine-tuning and reinforcement learning, achieving significant performance improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in enabling large language models to effectively perform web searches, particularly in open-search domains.

**Method:** EvolveSearch utilizes an iterative self-evolution framework that blends supervised fine-tuning with reinforcement learning, eliminating the need for human-annotated data.

**Key Contributions:**

	1. Introduction of EvolveSearch framework for LLMs
	2. Combines SFT and RL without human-annotated data
	3. Empirical validation showing significant performance gains across multiple benchmarks

**Result:** The framework demonstrated consistent performance improvements across seven multi-hop question-answering benchmarks, achieving an average improvement of 4.7% over state-of-the-art methods.

**Limitations:** 

**Conclusion:** EvolveSearch opens new possibilities for self-evolution in agentic capabilities for web search, paving the way for advancements in this domain.

**Abstract:** The rapid advancement of large language models (LLMs) has transformed the landscape of agentic information seeking capabilities through the integration of tools such as search engines and web browsers. However, current mainstream approaches for enabling LLM web search proficiency face significant challenges: supervised fine-tuning struggles with data production in open-search domains, while RL converges quickly, limiting their data utilization efficiency. To address these issues, we propose EvolveSearch, a novel iterative self-evolution framework that combines SFT and RL to enhance agentic web search capabilities without any external human-annotated reasoning data. Extensive experiments on seven multi-hop question-answering (MHQA) benchmarks demonstrate that EvolveSearch consistently improves performance across iterations, ultimately achieving an average improvement of 4.7\% over the current state-of-the-art across seven benchmarks, opening the door to self-evolution agentic capabilities in open web search domains.

</details>


### [105] [Multi-MLLM Knowledge Distillation for Out-of-Context News Detection](https://arxiv.org/abs/2505.22517)

*Yimeng Gu, Zhao Tong, Ignacio Castro, Shu Wu, Gareth Tyson*

**Main category:** cs.CL

**Keywords:** multimodal, knowledge distillation, large language models, out-of-context news, label-efficient learning

**Relevance Score:** 7

**TL;DR:** This paper presents a two-stage knowledge distillation framework to enhance the performance of small multimodal large language models (MLLMs) in detecting out-of-context news by leveraging teacher models for label predictions and rationales.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Improving the performance of small MLLMs for detecting out-of-context news in a label-efficient and cost-effective manner, especially in low-resource scenarios where fine-tuning with large labeled datasets or via expensive APIs is impractical.

**Method:** The method involves prompting multiple teacher MLLMs to generate label predictions and rationales, followed by a two-stage knowledge distillation framework. In Stage 1, LoRA fine-tuning is applied to the student model using all training data. In Stage 2, additional fine-tuning is conducted using LoRA and DPO on conflicting predictions from teachers.

**Key Contributions:**

	1. Introduction of a two-stage knowledge distillation framework for small MLLMs
	2. Utilization of teacher models for generating label predictions and rationales
	3. Achievement of state-of-the-art performance with less than 10% labeled data

**Result:** The proposed framework achieves state-of-the-art performance while using less than 10% of labeled training data, demonstrating significant efficiency in learning with limited resources.

**Limitations:** 

**Conclusion:** The two-stage strategy offers a practical solution for improving small MLLMs in out-of-context news detection, enabling them to uncover subtle patterns with minimal annotation costs.

**Abstract:** Multimodal out-of-context news is a type of misinformation in which the image is used outside of its original context. Many existing works have leveraged multimodal large language models (MLLMs) for detecting out-of-context news. However, observing the limited zero-shot performance of smaller MLLMs, they generally require label-rich fine-tuning and/or expensive API calls to GPT models to improve the performance, which is impractical in low-resource scenarios. In contrast, we aim to improve the performance of small MLLMs in a more label-efficient and cost-effective manner. To this end, we first prompt multiple teacher MLLMs to generate both label predictions and corresponding rationales, which collectively serve as the teachers' knowledge. We then introduce a two-stage knowledge distillation framework to transfer this knowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the student model using all training data. In Stage 2, we further fine-tune the student model using both LoRA fine-tuning and DPO on the data points where teachers' predictions conflict. This two-stage strategy reduces annotation costs and helps the student model uncover subtle patterns in more challenging cases. Experimental results demonstrate that our approach achieves state-of-the-art performance using less than 10% labeled data.

</details>


### [106] [Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs](https://arxiv.org/abs/2505.22548)

*Changhao Song, Yazhou Zhang, Peng Zhang*

**Main category:** cs.CL

**Keywords:** emotion understanding, machine learning, adaptive reasoning, sentiment analysis, humor detection

**Relevance Score:** 8

**TL;DR:** The paper presents a task-adaptive reasoning framework for improved emotion understanding, utilizing variable-length reasoning chains to adapt to the complexity of different emotional tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods in emotion understanding rely on fixed-length reasoning which does not effectively handle the varying complexities of emotional tasks.

**Method:** A task-adaptive reasoning framework using DeepSeek-R1 is proposed, employing a composite reward function that balances prediction accuracy, adaptive reasoning depth control, structural diversity, and suppression of repetitive logic.

**Key Contributions:**

	1. Development of a task-adaptive framework for emotion understanding
	2. Introduction of dynamic variable-length reasoning chains
	3. Improvements in prediction accuracy and structural diversity for emotion tasks

**Result:** The proposed framework shows consistent improvements in accuracy and F1 scores across emotion-related tasks, with notable enhancements for both basic and advanced tasks.

**Limitations:** 

**Conclusion:** The research effectively bridges rigid reasoning techniques with emotional complexities, enabling dynamic context-sensitive inference.

**Abstract:** Emotion understanding includes basic tasks (e.g., sentiment/emotion classification) and advanced tasks (e.g., sarcasm/humor detection). Current methods rely on fixed-length CoT reasoning, failing to adapt to the varying complexity of emotions. We propose a task-adaptive reasoning framework that employs DeepSeek-R1 to generate variable-length reasoning chains for different emotion tasks. By combining fine-tuning with reinforcement learning, we design a composite reward function that balances four objectives: prediction accuracy, adaptive reasoning depth control, structural diversity in reasoning paths, and suppression of repetitive logic. This approach achieves dynamic context-sensitive inference while enabling LLMs to autonomously develop deep reasoning capabilities. Experimental results demonstrate consistent improvements in both Acc and F1 scores across four tasks: emotion, sentiment, humor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for basic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges rigid CoT reasoning and emotional complexity through adaptive-depth analysis.

</details>


### [107] [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)

*Hoang Pham, Thanh-Do Nguyen, Khac-Hoai Nam Bui*

**Main category:** cs.CL

**Keywords:** knowledge graphs, large language models, claim verification, reasoning capability, zero-shot learning

**Relevance Score:** 9

**TL;DR:** ClaimPKG is an end-to-end framework that integrates large language model reasoning with structured knowledge from knowledge graphs for effective claim verification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing verification methods fail to leverage structured knowledge from knowledge graphs due to reliance on unstructured text corpora, and modern LLMs struggle with reasoning over these graphs.

**Method:** ClaimPKG employs a lightweight LLM to represent claims as pseudo-subgraphs and a dedicated module to retrieve relevant KG subgraphs, which are then processed by a general-purpose LLM for final verdicts and justifications.

**Key Contributions:**

	1. Introduction of ClaimPKG framework for integrating KGs with LLMs
	2. Achieving state-of-the-art performance on the FactKG dataset
	3. Demonstrating zero-shot generalizability to unstructured datasets

**Result:** ClaimPKG outperforms strong baselines by 9%-12% in accuracy on the FactKG dataset, and demonstrates zero-shot generalizability to unstructured datasets like HoVer and FEVEROUS.

**Limitations:** 

**Conclusion:** ClaimPKG effectively combines the reasoning capabilities of LLMs with structured knowledge, improving claim verification processes.

**Abstract:** Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of large language models (LLMs) is an emerging research challenge in claim verification. While KGs provide structured, semantically rich representations well-suited for reasoning, most existing verification methods rely on unstructured text corpora, limiting their ability to effectively leverage KGs. Additionally, despite possessing strong reasoning abilities, modern LLMs struggle with multi-step modular pipelines and reasoning over KGs without adaptation. To address these challenges, we propose ClaimPKG, an end-to-end framework that seamlessly integrates LLM reasoning with structured knowledge from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight, specialized LLM to represent the input claim as pseudo-subgraphs, guiding a dedicated subgraph retrieval module to identify relevant KG subgraphs. These retrieved subgraphs are then processed by a general-purpose LLM to produce the final verdict and justification. Extensive experiments on the FactKG dataset demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming strong baselines in this research field by 9%-12% accuracy points across multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability to unstructured datasets such as HoVer and FEVEROUS, effectively combining structured knowledge from KGs with LLM reasoning across various LLM backbones.

</details>


### [108] [Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings](https://arxiv.org/abs/2505.22563)

*Yu Lei, Xingyang Ge, Yi Zhang, Yiming Yang, Bolei Ma*

**Main category:** cs.CL

**Keywords:** Large Language Models, fMRI, neural response, language processing, hierarchical representations

**Relevance Score:** 9

**TL;DR:** This study investigates the alignment of large language models' hierarchical representations with human neural responses during sentence comprehension, revealing a strong correlation at higher semantic abstraction levels.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether LLMs and human brain processes align on computational principles, particularly concerning language processing.

**Method:** The study compares hierarchical embeddings from 14 LLMs with fMRI data of humans engaging with narrative stories, constructing neural prediction models to identify corresponding model layers.

**Key Contributions:**

	1. Identified correlations between LLM layers and human brain activations during sentence comprehension.
	2. Constructed sentence-level neural prediction models from fMRI data.
	3. Demonstrated evolution of LLM representations towards brain-like hierarchies.

**Result:** Findings show that model performance improvements lead to development of brain-like architectures, particularly at higher semantic abstraction levels.

**Limitations:** 

**Conclusion:** The research provides evidence that LLMs may reflect deeper similarities with human brain dynamics in language processing, especially in higher levels of semantic interpretation.

**Abstract:** Understanding whether large language models (LLMs) and the human brain converge on similar computational principles remains a fundamental and important question in cognitive neuroscience and AI. Do the brain-like patterns observed in LLMs emerge simply from scaling, or do they reflect deeper alignment with the architecture of human language processing? This study focuses on the sentence-level neural mechanisms of language models, systematically investigating how hierarchical representations in LLMs align with the dynamic neural responses during human sentence comprehension. By comparing hierarchical embeddings from 14 publicly available LLMs with fMRI data collected from participants, who were exposed to a naturalistic narrative story, we constructed sentence-level neural prediction models to precisely identify the model layers most significantly correlated with brain region activations. Results show that improvements in model performance drive the evolution of representational architectures toward brain-like hierarchies, particularly achieving stronger functional and anatomical correspondence at higher semantic abstraction levels.

</details>


### [109] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)

*Hoang Pham, Khac-Hoai Nam Bui*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, multi-hop queries, interpretability, synthetic dataset

**Relevance Score:** 9

**TL;DR:** This paper introduces Agent-UniRAG, a unified retrieval-augmented generation framework that enhances interpretability and effectiveness for both single-hop and multi-hop queries using LLM agents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of previous RAG systems that only focus on either single-hop or multi-hop queries, which restricts their applicability in real-world scenarios.

**Method:** The proposed approach is a trainable framework called Agent-UniRAG, which employs a step-by-step method to handle the complexity of inputs for RAG tasks while integrating both single-hop and multi-hop queries in an end-to-end process.

**Key Contributions:**

	1. Introduction of Agent-UniRAG for unified retrieval-augmented generation
	2. Development of SynAgent-RAG synthetic dataset for small LLMs
	3. Demonstration of comparable performance to larger models in RAG benchmarks

**Result:** The framework, along with the synthetic dataset SynAgent-RAG, demonstrates comparable performance with larger closed-source and open-source LLMs across various RAG benchmarks.

**Limitations:** 

**Conclusion:** The study presents a significant advancement in the interpretability and effectiveness of retrieval-augmented generation systems, offering a solution applicable to real-world multi-hop reasoning tasks.

**Abstract:** This paper presents a novel approach for unified retrieval-augmented generation (RAG) systems using the recent emerging large language model (LLM) agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental controllers, has become a promising approach to enable the interpretability of RAG tasks, especially for complex reasoning question-answering systems (e.g., multi-hop queries). Nonetheless, previous works mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, which limits the application of those approaches to real-world applications. In this study, we propose a trainable agent framework called Agent-UniRAG for unified retrieval-augmented LLM systems, which enhances the effectiveness and interpretability of RAG systems. The main idea is to design an LLM agent framework to solve RAG tasks step-by-step based on the complexity of the inputs, simultaneously including single-hop and multi-hop queries in an end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset to enable the proposed agent framework for small open-source LLMs (e.g., Llama-3-8B). The results show comparable performances with closed-source and larger open-source LLMs across various RAG benchmarks. Our source code and dataset are publicly available for further exploitation.

</details>


### [110] [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)

*Waldemar Chang, Alhassan Yasin*

**Main category:** cs.CL

**Keywords:** Fusion Steering, Large Language Models, Question Answering, Activation Steering, Factual Accuracy

**Relevance Score:** 9

**TL;DR:** Fusion Steering enhances factual accuracy in LLMs for QA through dynamic, layer-specific prompt adjustments.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To improve factual accuracy in question-answering tasks using large language models by introducing a flexible steering configuration that moves beyond traditional fixed-layer methodologies.

**Method:** Fusion Steering employs dynamic injection of prompt-specific activation deltas across all transformer layers, optimized using Optuna. It balances token overlap and perplexity to enhance performance.

**Key Contributions:**

	1. Introduction of Fusion Steering methodology for LLMs
	2. Demonstration of segmented steering effectiveness in improving accuracy
	3. Integration of dynamic activation adjustment for higher factual alignment

**Result:** Segmented steering achieves 25.4% accuracy on 260 SimpleQA prompts, significantly outperforming the baseline (3.5%) and full-layer steering (16.2%).

**Limitations:** 

**Conclusion:** Segmented steering demonstrates a strong potential for activating control in LLMs, suggesting further applications in interpretable AI.

**Abstract:** We present Fusion Steering, an activation steering methodology that improves factual accuracy in large language models (LLMs) for question-answering (QA) tasks. This approach introduces flexible steering configurations, including full-layer steering and segmented steering. Unlike traditional methods constrained to single-layer or fixed-layer operations, Fusion Steering employs dynamic injection of prompt-specific activation deltas across all transformer layers. These activation deltas are derived from reference completions that combine the ground-truth answer with a model-generated explanation to facilitate semantically enriched, example-specific steering. The injection weights are optimized per prompt using Optuna, targeting a joint objective that balances token overlap (factual alignment) and perplexity (fluency proxy). Evaluation employs a composite score integrating token overlap and LLM-graded quality, encompassing factual accuracy, coherence, and relevance. Empirical results on 260 SimpleQA prompts (selected from 500 where the baseline failed) showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring $\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at 16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully correct responses from 0.0% to 13.1%. These findings highlight the strengths of segmented, dynamic intervention strategies and the promise of per-prompt, full-network activation control. Fusion Steering is also amenable to sparse representations, such as Neuronpedia or sparse crosscoders, suggesting a promising direction for interpretable and scalable activation-level control in LLMs.

</details>


### [111] [Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts](https://arxiv.org/abs/2505.22582)

*Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Layer-wise Expert Allocation, Mixture-of-Experts, Multilingual NLP, Catastrophic Forgetting

**Relevance Score:** 8

**TL;DR:** The paper proposes a layer-wise expert allocation algorithm (LayerMoE) to enhance multilingual large language models (LLMs) by optimizing the addition of new language experts while preventing performance loss in existing languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve the multilingual capabilities of LLMs without the high parameter costs and performance degradation associated with adding new language experts.

**Method:** The paper introduces the LayerMoE algorithm, which allocates a varying number of language experts to different layers based on the representation similarities of languages, and incorporates a classifier to better route tokens for old languages in high-similarity layers.

**Key Contributions:**

	1. Introduction of LayerMoE for optimized expert allocation in LLMs.
	2. Analysis of language representation similarities across LLM layers for expert allocation.
	3. Reduced parameter costs and improved performance metrics compared to prior methods.

**Result:** Experimental results indicate that LayerMoE outperforms previous state-of-the-art models, achieving 60% fewer experts in single-expansion and 33.3% fewer in lifelong-expansion scenarios while maintaining performance.

**Limitations:** 

**Conclusion:** The LayerMoE approach effectively balances the addition of new language capabilities with the preservation of existing language performance, offering a more efficient method for expanding multilingual LLMs.

**Abstract:** Continually expanding new languages for existing large language models (LLMs) is a promising yet challenging approach to building powerful multilingual LLMs. The biggest challenge is to make the model continuously learn new languages while preserving the proficient ability of old languages. To achieve this, recent work utilizes the Mixture-of-Experts (MoE) architecture to expand new languages by adding new experts and avoid catastrophic forgetting of old languages by routing corresponding tokens to the original model backbone (old experts). Although intuitive, this kind of method is parameter-costly when expanding new languages and still inevitably impacts the performance of old languages. To address these limitations, we analyze the language characteristics of different layers in LLMs and propose a layer-wise expert allocation algorithm (LayerMoE) to determine the appropriate number of new experts for each layer. Specifically, we find different layers in LLMs exhibit different representation similarities between languages and then utilize the similarity as the indicator to allocate experts for each layer, i.e., the higher similarity, the fewer experts. Additionally, to further mitigate the forgetting of old languages, we add a classifier in front of the router network on the layers with higher similarity to guide the routing of old language tokens. Experimental results show that our method outperforms the previous state-of-the-art baseline with 60% fewer experts in the single-expansion setting and with 33.3% fewer experts in the lifelong-expansion setting, demonstrating the effectiveness of our method.

</details>


### [112] [Precise In-Parameter Concept Erasure in Large Language Models](https://arxiv.org/abs/2505.22586)

*Yoav Gur-Arieh, Clara Suslik, Yihuai Hong, Fazl Barez, Mor Geva*

**Main category:** cs.CL

**Keywords:** large language models, concept erasure, feature-based editing, PISCES, machine learning

**Relevance Score:** 6

**TL;DR:** PISCES is a novel framework for precisely erasing undesirable concepts from large language models by editing model parameters directly.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for removing sensitive or copyrighted content from large language models are often ineffective.

**Method:** PISCES utilizes a disentangler model to decompose MLP vectors into interpretable features, identifying and editing features associated with target concepts in parameter space.

**Key Contributions:**

	1. Introduction of PISCES framework for precise concept erasure
	2. Improved specificity and robustness in erasure compared to existing methods
	3. Demonstrated effectiveness in reducing model accuracy on target concepts

**Result:** PISCES shows modest gains in efficacy over existing methods, achieving significantly lower accuracy on target concepts and improved specificity and robustness in knowledge removal.

**Limitations:** 

**Conclusion:** The feature-based in-parameter editing provides a more reliable method for erasing conceptual knowledge in language models.

**Abstract:** Large language models (LLMs) often acquire knowledge during pretraining that is undesirable in downstream deployments, e.g., sensitive information or copyrighted content. Existing approaches for removing such knowledge rely on fine-tuning, training low-rank adapters or fact-level editing, but these are either too coarse, too shallow, or ineffective. In this work, we propose PISCES (Precise In-parameter Suppression for Concept EraSure), a novel framework for precisely erasing entire concepts from model parameters by directly editing directions that encode them in parameter space. PISCES uses a disentangler model to decompose MLP vectors into interpretable features, identifies those associated with a target concept using automated interpretability techniques, and removes them from model parameters. Experiments on Gemma 2 and Llama 3.1 over various concepts show that PISCES achieves modest gains in efficacy over leading erasure methods, reducing accuracy on the target concept to as low as 7.7%, while dramatically improving erasure specificity (by up to 31%) and robustness (by up to 38%). Overall, these results demonstrate that feature-based in-parameter editing enables a more precise and reliable approach for removing conceptual knowledge in language models.

</details>


### [113] [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)

*Erxin Yu, Jing Li, Ming Liao, Qi Zhu, Boyang Xue, Minghui Xu, Baojun Wang, Lanqing Hong, Fei Mi, Lifeng Shang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, Error Generalization

**Relevance Score:** 8

**TL;DR:** This paper introduces Self-Error-Instruct (SEI), a framework that improves mathematical reasoning in large language models by generating targeted training data through the analysis of errors.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with mathematical reasoning, often due to insufficient training data derived from bad cases. This paper aims to improve training data synthesis for better generalization of error patterns in mathematical reasoning tasks.

**Method:** The framework involves identifying bad cases in mathematical datasets (GSM8K and MATH), generating error keyphrases, clustering to identify error types, and using a self-instruct approach to refine and synthesize additional training data for fine-tuning models.

**Key Contributions:**

	1. Introduction of Self-Error-Instruct (SEI) framework for enhanced training data generation
	2. Systematic analysis of mathematical errors using keyphrases and clustering
	3. Demonstrated improvements in model performance on mathematical reasoning tasks

**Result:** The application of SEI leads to improved performance in reasoning abilities of various models across both in-domain and out-of-domain mathematics datasets.

**Limitations:** 

**Conclusion:** Self-error instruction effectively enhances LLMs' mathematical reasoning by synthesizing generalized training data from identified errors and improving model performance through iterative fine-tuning.

**Abstract:** Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization.

</details>


### [114] [Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding](https://arxiv.org/abs/2505.22618)

*Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie*

**Main category:** cs.CL

**Keywords:** Diffusion LLMs, parallel decoding, KV Cache

**Relevance Score:** 8

**TL;DR:** This paper presents improvements for Diffusion LLMs by introducing a novel KV Cache mechanism and a confidence-aware parallel decoding strategy to enhance inference speed and maintain text generation quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the inference speed and quality of open-sourced Diffusion-based large language models, which currently lag behind autoregressive models in practical applications.

**Method:** The authors introduce a block-wise approximate KV Cache mechanism and a confidence-aware parallel decoding strategy to address issues of speed and quality degradation in Diffusion LLMs.

**Key Contributions:**

	1. Novel block-wise approximate KV Cache mechanism for Diffusion LLMs
	2. Confidence-aware parallel decoding strategy to improve generation quality
	3. Significant improvements in inference speed (up to 27.6x throughput) with minimal accuracy loss

**Result:** Experimental results indicate up to 27.6 times throughput improvement with minimal accuracy loss compared to existing methods, closing the gap with autoregressive models' performance.

**Limitations:** 

**Conclusion:** The study successfully demonstrates that the proposed methods enable practical deployment of Diffusion LLMs by significantly improving their speed and maintaining generation quality.

**Abstract:** Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$ throughput} improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.

</details>


### [115] [Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions](https://arxiv.org/abs/2505.22627)

*Yijun Shen, Delong Chen, Fan Liu, Xingyu Wang, Chuanyi Zhang, Liang Yao, Yuhui Zheng*

**Main category:** cs.CL

**Keywords:** vision-language alignment, annotation optimization, multimodal interface

**Relevance Score:** 6

**TL;DR:** Introducing CoTalk, a new AI-in-the-loop methodology that optimizes human annotation for vision-language tasks by enhancing efficiency and comprehensiveness of image captioning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically optimize human annotation efforts for vision-language alignment given budget constraints.

**Method:** CoTalk utilizes a sequential annotation approach, allowing annotators to focus on the residual visual information, and employs a multimodal interface to maximize throughput through talking while annotating.

**Key Contributions:**

	1. Introduction of the CoTalk methodology for image captioning.
	2. Demonstrated effectiveness of sequential annotation over parallel annotation.
	3. Showed improved annotation speed and retrieval performance through multimodal interaction.

**Result:** CoTalk improves annotation speed (0.42 vs. 0.30 units/sec) and retrieval performance (41.13% vs. 40.52%) compared to traditional parallel methods.

**Limitations:** 

**Conclusion:** The CoTalk framework demonstrates significant improvements in both the speed and quality of human annotations in vision-language tasks.

**Abstract:** While densely annotated image captions significantly facilitate the learning of robust vision-language alignment, methodologies for systematically optimizing human annotation efforts remain underexplored. We introduce Chain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize the number of annotated samples and improve their comprehensiveness under fixed budget constraints (e.g., total human annotation time). The framework is built upon two key insights. First, sequential annotation reduces redundant workload compared to conventional parallel annotation, as subsequent annotators only need to annotate the ``residual'' -- the missing visual information that previous annotations have not covered. Second, humans process textual input faster by reading while outputting annotations with much higher throughput via talking; thus a multimodal interface enables optimized efficiency. We evaluate our framework from two aspects: intrinsic evaluations that assess the comprehensiveness of semantic units, obtained by parsing detailed captions into object-attribute trees and analyzing their effective connections; extrinsic evaluation measures the practical usage of the annotated captions in facilitating vision-language alignment. Experiments with eight participants show our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30 units/sec) and retrieval performance (41.13\% vs. 40.52\%) over the parallel method.

</details>


### [116] [Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs](https://arxiv.org/abs/2505.22630)

*Ziling Cheng, Meng Cao, Marc-Antoine Rondeau, Jackie Chi Kit Cheung*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, class-based generalization, mechanistic interpretability, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper examines the nature of errors made by large language models (LLMs), focusing on irrelevant context hallucinations and the mechanisms behind these errors, termed class-based (mis)generalization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the errors made by LLMs, specifically irrelevant context hallucinations, and to understand the structured mechanisms behind these errors.

**Method:** The authors conduct behavioral analysis and mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types.

**Key Contributions:**

	1. Introduces the concept of class-based (mis)generalization in LLMs.
	2. Provides a structured analysis of errors related to irrelevant context hallucinations.
	3. Presents mechanistic insights into how LLMs process information and make predictions.

**Result:** The study finds that errors arise from structured yet flawed mechanisms, where abstract class representations are developed in earlier layers of the model and refined in higher layers, influenced by competing circuits for feature selection.

**Limitations:** The study primarily focuses on certain LLM architectures, which may limit the generalizability of the findings to all LLMs.

**Conclusion:** LLMs can generalize using abstractions, but their ability to do so is unreliable and heavily influenced by contextual cues, leading to what are referred to as stochastic chameleons.

**Abstract:** The widespread success of large language models (LLMs) on NLP benchmarks has been accompanied by concerns that LLMs function primarily as stochastic parrots that reproduce texts similar to what they saw during pre-training, often erroneously. But what is the nature of their errors, and do these errors exhibit any regularities? In this work, we examine irrelevant context hallucinations, in which models integrate misleading contextual cues into their predictions. Through behavioral analysis, we show that these errors result from a structured yet flawed mechanism that we term class-based (mis)generalization, in which models combine abstract class cues with features extracted from the query or context to derive answers. Furthermore, mechanistic interpretability experiments on Llama-3, Mistral, and Pythia across 39 factual recall relation types reveal that this behavior is reflected in the model's internal computations: (i) abstract class representations are constructed in lower layers before being refined into specific answers in higher layers, (ii) feature selection is governed by two competing circuits -- one prioritizing direct query-based reasoning, the other incorporating contextual cues -- whose relative influences determine the final output. Our findings provide a more nuanced perspective on the stochastic parrot argument: through form-based training, LLMs can exhibit generalization leveraging abstractions, albeit in unreliable ways based on contextual cues -- what we term stochastic chameleons.

</details>


### [117] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)

*Yida Xue, Zhen Bi, Jinnan Yang, Jungang Lou, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** multimodal large language models, spatial knowledge graphs, data synthesis

**Relevance Score:** 8

**TL;DR:** This paper introduces SKG2Data, a multimodal synthesis method that leverages spatial knowledge graphs to improve the spatial perception abilities of multimodal large language models (MLLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to enhance the spatial perception capabilities of MLLMs, which have been identified as a significant limitation despite recent advances in multimodal models.

**Method:** The methodology involves the automatic construction of a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances, which guides the multimodal data synthesis process.

**Key Contributions:**

	1. Introduction of SKG2Data for multimodal data synthesis guided by spatial knowledge graphs.
	2. Demonstration of improved spatial perception and reasoning in MLLMs.
	3. Evidence of strong generalization capabilities from synthesized spatial data.

**Result:** Extensive experiments show that data synthesized from various types of spatial knowledge improve the spatial perception and reasoning abilities of MLLMs and demonstrate strong generalization capabilities.

**Limitations:** 

**Conclusion:** The authors believe that utilizing knowledge-based data synthesis will foster advancements in spatial intelligence within multimodal large language models.

**Abstract:** Recent advances in multimodal large language models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. In this work, we introduce SKG2Data, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances, which is subsequently utilized to guide multimodal data synthesis. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, not only enhance the spatial perception and reasoning abilities of MLLMs but also exhibit strong generalization capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence.

</details>


### [118] [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)

*Fangcong Yin, Zeyu Leo Liu, Liu Leqi, Xi Ye, Greg Durrett*

**Main category:** cs.CL

**Keywords:** large language models, chain-of-thought, compositional reasoning, zero-shot performance, multitask learning

**Relevance Score:** 9

**TL;DR:** This paper explores improving the generalization of large language models (LLMs) by training them on Composable Chain-of-Thought (CoT) data to enhance their reasoning skills for unseen tasks without labeled data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enable reasoning models to generalize beyond their training distribution and to combine atomic reasoning skills to tackle complex, unseen tasks.

**Method:** The study modifies the formats of constituent atomic tasks in CoT traces to be composable and trains models on this Composable CoT data, using methods such as multitask learning and model merging to enhance performance.

**Key Contributions:**

	1. Introduction of Composable CoT data to improve reasoning model generalization.
	2. Demonstration of effective model combination techniques for enhanced zero-shot performance.
	3. Findings on the superiority of Composable CoT training over traditional methods in specific reasoning tasks.

**Result:** Models trained on Composable CoT data demonstrate improved zero-shot performance on compositional tasks compared to traditional multitask learning and fine-tuning approaches, even with limited training data.

**Limitations:** Generalization may still be limited to certain task types, and reliance on atomic task data may restrict applicability to broader contexts.

**Conclusion:** The findings suggest that modifying CoT formats to be composable significantly enhances the reasoning capabilities of LLMs on new tasks.

**Abstract:** A common approach for teaching large language models (LLMs) to reason is to train on chain-of-thought (CoT) traces of in-distribution reasoning problems, but such annotated data is costly to obtain for every problem of interest. We want reasoning models to generalize beyond their training distribution, and ideally to generalize compositionally: combine atomic reasoning skills to solve harder, unseen reasoning tasks. We take a step towards compositional generalization of reasoning skills when addressing a target compositional task that has no labeled CoT data. We find that simply training models on CoT data of atomic tasks leads to limited generalization, but minimally modifying CoT formats of constituent atomic tasks to be composable can lead to improvements. We can train "atomic CoT" models on the atomic tasks with Composable CoT data and combine them with multitask learning or model merging for better zero-shot performance on the target compositional task. Such a combined model can be further bootstrapped on a small amount of compositional data using rejection sampling fine-tuning (RFT). Results on string operations and natural language skill compositions show that training LLMs on Composable CoT outperforms multitask learning and continued fine-tuning baselines within a given training data budget.

</details>


### [119] [Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese](https://arxiv.org/abs/2505.22645)

*Hanjia Lyu, Jiebo Luo, Jian Kang, Allison Koenecke*

**Main category:** cs.CL

**Keywords:** Large Language Models, Simplified Chinese, Traditional Chinese, Biases, Benchmark

**Relevance Score:** 9

**TL;DR:** This paper investigates the differential performance of Large Language Models (LLMs) when prompted in Simplified versus Traditional Chinese, highlighting biases and providing benchmark tasks for evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding LLM performance disparities between Simplified and Traditional Chinese is essential to mitigate representational harms and prevent negative impacts in decision-making contexts such as education and hiring.

**Method:** The authors design benchmark tasks assessing regional term and name choices to audit the performance of 11 commercial and open-source LLMs, analyzing their responses based on the prompting language.

**Key Contributions:**

	1. Developed two benchmark tasks for evaluating LLM performance in Simplified vs. Traditional Chinese
	2. Identified biases in LLM responses related to language and task
	3. Provided an open-sourced dataset to facilitate reproducible evaluations

**Result:** The analysis reveals that most LLMs favor Simplified Chinese in regional term tasks but favor Traditional Chinese in regional name tasks, indicating that biases exist depending on the task and language.

**Limitations:** 

**Conclusion:** The study emphasizes the importance of analyzing LLM biases in different cultural contexts and provides a benchmark dataset for future research.

**Abstract:** While the capabilities of Large Language Models (LLMs) have been studied in both Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit differential performance when prompted in these two variants of written Chinese. This understanding is critical, as disparities in the quality of LLM responses can perpetuate representational harms by ignoring the different cultural contexts underlying Simplified versus Traditional Chinese, and can exacerbate downstream harms in LLM-facilitated decision-making in domains such as education or hiring. To investigate potential LLM performance disparities, we design two benchmark tasks that reflect real-world scenarios: regional term choice (prompting the LLM to name a described item which is referred to differently in Mainland China and Taiwan), and regional name choice (prompting the LLM to choose who to hire from a list of names in both Simplified and Traditional Chinese). For both tasks, we audit the performance of 11 leading commercial LLM services and open-sourced models -- spanning those primarily trained on English, Simplified Chinese, or Traditional Chinese. Our analyses indicate that biases in LLM responses are dependent on both the task and prompting language: while most LLMs disproportionately favored Simplified Chinese responses in the regional term choice task, they surprisingly favored Traditional Chinese names in the regional name choice task. We find that these disparities may arise from differences in training data representation, written character preferences, and tokenization of Simplified and Traditional Chinese. These findings highlight the need for further analysis of LLM biases; as such, we provide an open-sourced benchmark dataset to foster reproducible evaluations of future LLM behavior across Chinese language variants (https://github.com/brucelyu17/SC-TC-Bench).

</details>


### [120] [WebDancer: Towards Autonomous Information Seeking Agency](https://arxiv.org/abs/2505.22648)

*Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** agentic systems, information seeking, multi-step reasoning, WebDancer, machine learning

**Relevance Score:** 6

**TL;DR:** This paper presents a framework for building end-to-end agentic information seeking agents, validated through the WebDancer agent, achieving high performance in benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address complex real-world problems requiring in-depth information seeking and multi-step reasoning, leveraging advancements in agentic systems.

**Method:** The approach consists of four stages: browsing data construction, trajectories sampling, supervised fine-tuning for cold starts, and reinforcement learning for improved generalisation.

**Key Contributions:**

	1. Framework for building autonomous information seeking agents
	2. Empirical evaluations showcasing the performance of WebDancer
	3. Insights for future development of agentic models

**Result:** WebDancer exhibits strong performance on information seeking benchmarks GAIA and WebWalkerQA, demonstrating the effectiveness of the proposed training paradigm.

**Limitations:** 

**Conclusion:** The study offers valuable insights and pathways for developing more capable agentic models in information seeking tasks.

**Abstract:** Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct, WebDancer. Empirical evaluations on the challenging information seeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models. The codes and demo will be released in https://github.com/Alibaba-NLP/WebAgent.

</details>


### [121] [The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason](https://arxiv.org/abs/2505.22653)

*Ang Lv, Ruobing Xie, Xingwu Sun, Zhanhui Kang, Rui Yan*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, reward noise

**Relevance Score:** 8

**TL;DR:** This research explores the effects of reward noise on large language models (LLMs) in reinforcement learning, revealing their robustness and the potential of key reasoning phrase rewards to enhance performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigating the practical implications of reward noise when applying reinforcement learning to post-training large language models, as existing studies often ignore this variable.

**Method:** The study conducted experiments using a Qwen-2.5-7B model, manipulating reward outputs and focusing on reasoning pattern rewards (RPR) without strict correctness verification.

**Key Contributions:**

	1. Demonstrated robustness of LLMs to reward noise
	2. Introduced reasoning pattern reward (RPR) approach
	3. Provided insights for enhancing post-training techniques with noisy rewards

**Result:** LLMs showed strong robustness to significant reward noise; performance improved from 5% to 72% accuracy in math tasks despite flipping 40% of the reward outputs, and key reasoning phrase rewards enabled models to achieve over 70% accuracy comparable to stricter models.

**Limitations:** 

**Conclusion:** The findings highlight the significance of foundational model abilities and suggest that integrating RPR with noisy rewards can improve LLM performance on complex tasks.

**Abstract:** Recent studies on post-training large language models (LLMs) for reasoning through reinforcement learning (RL) typically focus on tasks that can be accurately verified and rewarded, such as solving math problems. In contrast, our research investigates the impact of reward noise, a more practical consideration for real-world scenarios involving the post-training of LLMs using reward models. We found that LLMs demonstrate strong robustness to substantial reward noise. For example, manually flipping 40% of the reward function's outputs in math tasks still allows a Qwen-2.5-7B model to achieve rapid convergence, improving its performance on math tasks from 5% to 72%, compared to the 75% accuracy achieved by a model trained with noiseless rewards. Surprisingly, by only rewarding the appearance of key reasoning phrases (namely reasoning pattern reward, RPR), such as ``first, I need to''-without verifying the correctness of answers, the model achieved peak downstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models trained with strict correctness verification and accurate rewards. Recognizing the importance of the reasoning process over the final results, we combined RPR with noisy reward models. RPR helped calibrate the noisy reward models, mitigating potential false negatives and enhancing the LLM's performance on open-ended tasks. These findings suggest the importance of improving models' foundational abilities during the pre-training phase while providing insights for advancing post-training techniques. Our code and scripts are available at https://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason.

</details>


### [122] [GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning](https://arxiv.org/abs/2505.22661)

*Qingchen Yu, Zifan Zheng, Ding Chen, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Adaptive Evaluation, Domain Knowledge, Reasoning Assessment, Games

**Relevance Score:** 9

**TL;DR:** The paper presents GuessArena, an adaptive evaluation framework for large language models (LLMs) that uses adversarial game-based interactions to improve evaluation fidelity in various domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current static benchmarks for evaluating large language models (LLMs) are inadequate due to their lack of adaptability and failure to capture fine-grained assessments of knowledge and reasoning abilities across diverse application domains.

**Method:** GuessArena utilizes adversarial game-based interactions, modeled after the Guess Who I Am? game, to dynamically evaluate LLMs by integrating domain knowledge and progressive reasoning assessments.

**Key Contributions:**

	1. Introduction of an adaptive evaluation framework for LLMs
	2. Integration of dynamic domain knowledge modeling
	3. Demonstration of superior evaluation fidelity compared to static benchmarks

**Result:** Empirical studies show that GuessArena can effectively distinguish LLMs based on domain knowledge coverage and reasoning chain completeness across five domains, including healthcare.

**Limitations:** 

**Conclusion:** GuessArena offers significant advantages over traditional benchmarks in terms of interpretability, scalability, and adaptability to various scenarios, enhancing the evaluation of LLMs.

**Abstract:** The evaluation of large language models (LLMs) has traditionally relied on static benchmarks, a paradigm that poses two major limitations: (1) predefined test sets lack adaptability to diverse application domains, and (2) standardized evaluation protocols often fail to capture fine-grained assessments of domain-specific knowledge and contextual reasoning abilities. To overcome these challenges, we propose GuessArena, an adaptive evaluation framework grounded in adversarial game-based interactions. Inspired by the interactive structure of the Guess Who I Am? game, our framework seamlessly integrates dynamic domain knowledge modeling with progressive reasoning assessment to improve evaluation fidelity. Empirical studies across five vertical domains-finance, healthcare, manufacturing, information technology, and education-demonstrate that GuessArena effectively distinguishes LLMs in terms of domain knowledge coverage and reasoning chain completeness. Compared to conventional benchmarks, our method provides substantial advantages in interpretability, scalability, and scenario adaptability.

</details>


### [123] [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)

*Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen Zhong, Hongyi Liu, Jiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary, Xia Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Efficiency, Natural Language Processing, Dynamic Adaptation

**Relevance Score:** 9

**TL;DR:** AutoL2S is a framework that allows LLMs to dynamically adjust reasoning lengths based on question complexity, improving efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in reasoning paths generated by LLMs for easy questions, which often result in increased inference costs and latency.

**Method:** AutoL2S is a model-agnostic framework that uses a learned paradigm, allowing LLMs to decide when to use longer or shorter reasoning based on question complexity, utilizing an <EASY> token for guidance.

**Key Contributions:**

	1. Introduction of the Auto Long-Short Reasoning (AutoL2S) framework
	2. Use of the <EASY> token for annotating reasoning complexity
	3. Demonstrated significant reduction in reasoning length without performance loss.

**Result:** With AutoL2S, the length of reasoning paths can be reduced by up to 57% while maintaining performance levels.

**Limitations:** 

**Conclusion:** AutoL2S effectively improves the ability of LLMs to generate shorter and higher quality reasoning paths, enhancing scalability and efficiency in reasoning.

**Abstract:** The reasoning-capable large language models (LLMs) demonstrate strong performance on complex reasoning tasks but often suffer from overthinking, generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy reasoning questions, thereby increasing inference cost and latency. Recent approaches attempt to address this challenge by manually deciding when to apply long or short reasoning. However, they lack the flexibility to adapt CoT length dynamically based on question complexity. In this paper, we propose Auto Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that enables LLMs to dynamically compress their generated reasoning path based on the complexity of the reasoning question. AutoL2S enables a learned paradigm, in which LLMs themselves can decide when longer reasoning is necessary and when shorter reasoning suffices, by training on data annotated with our proposed method, which includes both long and short CoT paths and a special <EASY> token. We then use <EASY> token to indicate when the model can skip generating lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs' ability to generate shorter CoT reasoning paths with improved quality after training. Extensive evaluation results show that AutoL2S reduces the length of reasoning generation by up to 57% without compromising performance, demonstrating the effectiveness of AutoL2S for scalable and efficient LLM reasoning.

</details>


### [124] [Machine Translation Models are Zero-Shot Detectors of Translation Direction](https://arxiv.org/abs/2401.06769)

*Michelle Wastl, Jannis Vamvas, Rico Sennrich*

**Main category:** cs.CL

**Keywords:** translation direction, machine translation, forensic applications, parallel text, unsupervised learning

**Relevance Score:** 6

**TL;DR:** This paper presents an unsupervised method for detecting translation direction in parallel texts, demonstrating its effectiveness across various multilingual machine translation models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The detection of translation direction has important implications for machine translation training, evaluation, and forensic applications like plagiarism detection.

**Method:** An unsupervised approach utilizing the hypothesis that the probability of obtaining a translation given the original is greater than that of obtaining the original given the translation.

**Key Contributions:**

	1. Introduction of an unsupervised approach to translation direction detection
	2. Validation of the method across multiple machine translation models
	3. High accuracies for translation direction detection in multilingual contexts

**Result:** Achieved high document-level accuracies of 82--96% for NMT translations and 60--81% for human translations across 20 high-resource language pairs.

**Limitations:** 

**Conclusion:** The proposed method effectively detects translation direction, showing promise for both machine-generated and human translations.

**Abstract:** Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that $p(\text{translation}|\text{original})>p(\text{original}|\text{translation})$, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with massively multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82--96% for NMT-produced translations, and 60--81% for human translations, depending on the model used. Code and demo are available at https://github.com/ZurichNLP/translation-direction-detection

</details>


### [125] [Tracking Semantic Change in Slovene: A Novel Dataset and Optimal Transport-Based Distance](https://arxiv.org/abs/2402.16596)

*Marko Pranjić, Kaja Dobrovoljc, Senja Pollak, Matej Martinc*

**Main category:** cs.CL

**Keywords:** semantic change, Slovene language, optimal transport, language evolution, dataset

**Relevance Score:** 3

**TL;DR:** This paper introduces the first Slovene dataset for semantic change detection and proposes a new metric based on regularized optimal transport to improve the robustness of detecting semantic changes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand language evolution due to societal and cultural changes, particularly in the Slovene language, which is less resourced and has unique challenges in semantic change detection.

**Method:** The paper presents a dataset of semantic change scores for 104 Slovene words derived from over 3,000 annotated sentence pairs, analyzing existing semantic change metrics and proposing a new metric based on regularized optimal transport.

**Key Contributions:**

	1. First Slovene dataset for semantic change detection
	2. Introduction of a novel metric using regularized optimal transport
	3. Comprehensive evaluation of existing methods against the new metric

**Result:** The proposed optimal transport-based metric shows matching or improved performance compared to existing methods for semantic change detection in the Slovene language.

**Limitations:** Identified limitations in several existing semantic change metrics.

**Conclusion:** The paper contributes to the field of semantic change detection by providing a valuable dataset and a more effective metric that enhances the reliability of such systems.

**Abstract:** In this paper, we focus on the detection of semantic changes in Slovene, a less resourced Slavic language with two million speakers. Detecting and tracking semantic changes provides insight into the evolution of language caused by changes in society and culture. We present the first Slovene dataset for evaluating semantic change detection systems, which contains aggregated semantic change scores for 104 target words obtained from more than 3,000 manually annotated sentence pairs. We analyze an important class of measures of semantic change metrics based on the Average pairwise distance and identify several limitations. To address these limitations, we propose a novel metric based on regularized optimal transport, which offers a more robust framework for quantifying semantic change. We provide a comprehensive evaluation of various existing semantic change detection methods and associated semantic change measures on our dataset. Through empirical testing, we demonstrate that our proposed approach, leveraging regularized optimal transport, achieves either matching or improved performance compared to baseline approaches.

</details>


### [126] [Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems](https://arxiv.org/abs/2404.06762)

*Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen*

**Main category:** cs.CL

**Keywords:** Intelligent Tutoring Systems, Personality-aware simulation, Large Language Models, Education, Human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces a framework for creating student profiles that integrate cognitive and noncognitive aspects, utilizing large language models (LLMs) for personality-aware simulation in Intelligent Tutoring Systems (ITSs) specifically for language learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to enhance engagement and learning efficiency in dialogic teaching by recognizing and adapting to individual student characteristics through personalized interactions in ITSs.

**Method:** The paper proposes a framework that constructs profiles of different student groups by refining cognitive and noncognitive aspects and employs LLMs for simulating diverse student personas in language learning contexts.

**Key Contributions:**

	1. Development of a framework for personality-aware student simulation
	2. Integration of cognitive and noncognitive aspects in student profiling
	3. Validation through extensive analysis involving teachers and students

**Result:** Extensive analysis shows that the proposed framework enables LLMs to generate varied responses reflecting students' language abilities and personality traits, which can help teachers adapt their instructional strategies effectively.

**Limitations:** Challenges remain in accurately characterizing and simulating students' personas for effective training and evaluation of conversational ITSs.

**Conclusion:** The findings indicate that integrating personality-aware simulations into ITSs can significantly improve teacher responsiveness and student engagement, confirming the potential of LLMs in educational contexts.

**Abstract:** Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced learning experience. The emergence of large language models (LLMs) further enables better human-machine interaction, and facilitates the development of conversational ITSs in various disciplines such as math and language learning. In dialogic teaching, recognizing and adapting to individual characteristics can significantly enhance student engagement and learning efficiency. However, characterizing and simulating student's persona remain challenging in training and evaluating conversational ITSs. In this work, we propose a framework to construct profiles of different student groups by refining and integrating both cognitive and noncognitive aspects, and leverage LLMs for personality-aware student simulation in a language learning scenario. We further enhance the framework with multi-aspect validation, and conduct extensive analysis from both teacher and student perspectives. Our experimental results show that state-of-the-art LLMs can produce diverse student responses according to the given language ability and personality traits, and trigger teacher's adaptive scaffolding strategies.

</details>


### [127] [Mitigating Text Toxicity with Counterfactual Generation](https://arxiv.org/abs/2405.09948)

*Milan Bhan, Jean-Noel Vittaut, Nina Achache, Victor Legrand, Nicolas Chesneau, Annabelle Blangero, Juliette Murris, Marie-Jeanne Lesot*

**Main category:** cs.CL

**Keywords:** text detoxification, counterfactual generation, explainable AI, toxic text mitigation, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper proposes a new method for text detoxification that uses counterfactual generation from explainable AI to mitigate toxicity while preserving the original tone of the text.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve existing text detoxification methods that often fail to preserve non-toxic meaning while removing toxicity.

**Method:** Utilizes counterfactual generation techniques alongside local feature importance analysis within a toxicity classifier framework to enhance the detoxification process.

**Key Contributions:**

	1. First application of counterfactual generation to text detoxification.
	2. Demonstrated improvements in preserving non-toxic meanings during detoxification.
	3. Explored the implications and risks associated with the use of detoxification tools.

**Result:** Demonstrated that the proposed counterfactual generation approach mitigates toxicity more effectively than classical methods, as evidenced by both automatic and human evaluations conducted on three datasets.

**Limitations:** Discussion on ethical concerns and potential misuse of detoxification technology was limited.

**Conclusion:** The integration of counterfactual generation with text detoxification is novel and presents new avenues for applying explainable AI in this domain, though concerns around misuse of detoxification tools are discussed.

**Abstract:** Toxicity mitigation consists in rephrasing text in order to remove offensive or harmful meaning. Neural natural language processing (NLP) models have been widely used to target and mitigate textual toxicity. However, existing methods fail to detoxify text while preserving the initial non-toxic meaning at the same time. In this work, we propose to apply counterfactual generation methods from the eXplainable AI (XAI) field to target and mitigate textual toxicity. In particular, we perform text detoxification by applying local feature importance and counterfactual generation methods to a toxicity classifier distinguishing between toxic and non-toxic texts. We carry out text detoxification through counterfactual generation on three datasets and compare our approach to three competitors. Automatic and human evaluations show that recently developed NLP counterfactual generators can mitigate toxicity accurately while better preserving the meaning of the initial text as compared to classical detoxification methods. Finally, we take a step back from using automated detoxification tools, and discuss how to manage the polysemous nature of toxicity and the risk of malicious use of detoxification tools. This work is the first to bridge the gap between counterfactual generation and text detoxification and paves the way towards more practical application of XAI methods.

</details>


### [128] [REVS: Unlearning Sensitive Information in Language Models via Rank Editing in the Vocabulary Space](https://arxiv.org/abs/2406.09325)

*Tomer Ashuach, Martin Tutek, Yonatan Belinkov*

**Main category:** cs.CL

**Keywords:** language models, privacy, unlearning, sensitive information, neural networks

**Relevance Score:** 9

**TL;DR:** REVS is a new method for unlearning sensitive information from language models, outperforming existing techniques while preserving model integrity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address privacy concerns arising from the inadvertent memorization of sensitive information by language models during training.

**Method:** REVS employs a non-gradient-based approach that identifies and modifies specific neurons associated with sensitive tokens in the model.

**Key Contributions:**

	1. Introduction of REVS for unlearning sensitive data from LMs
	2. Demonstration of superior performance compared to existing methods
	3. Curated datasets for evaluating sensitive information memorization

**Result:** REVS outperforms existing unlearning methods in removing sensitive information and is more robust against extraction attacks, while maintaining the integrity of the model.

**Limitations:** 

**Conclusion:** The proposed method effectively mitigates privacy risks without compromising model performance, forming a better solution to the challenges of sensitive information memorization in language models.

**Abstract:** Language models (LMs) risk inadvertently memorizing and divulging sensitive or personally identifiable information (PII) seen in training data, causing privacy concerns. Current approaches to address this issue involve costly dataset scrubbing, or model filtering through unlearning and model editing, which can be bypassed through extraction attacks. We propose REVS, a novel non-gradient-based method for unlearning sensitive information from LMs. REVS identifies and modifies a small subset of neurons relevant for constituent tokens that form sensitive information. To adequately evaluate our method on truly sensitive information, we curate three datasets: email and URL datasets naturally memorized by the models, and a synthetic social security number dataset that we tune the models to memorize. Compared to other methods, REVS demonstrates superior performance in unlearning sensitive information and robustness to extraction attacks, while retaining underlying model integrity.

</details>


### [129] [Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective](https://arxiv.org/abs/2406.14023)

*Yuchen Wen, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Implicit Bias, Psychometrics, Ethics in AI, Benchmarking

**Relevance Score:** 9

**TL;DR:** This paper evaluates implicit bias in large language models (LLMs) using psychometric principles, proposing attack methods and benchmarks to assess ethical risks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate concerns that LLMs may propagate implicit biases that adversely affect certain demographics without explicit harmful language.

**Method:** The authors propose three psychometric attack approaches (Disguise, Deception, and Teaching) and create two benchmarks for evaluation: a bilingual dataset with 2.7K instances and BUMBLE with 12.7K instances covering various bias types.

**Key Contributions:**

	1. Three novel psychometric attack approaches for evaluating implicit bias in LLMs.
	2. Two substantial bias benchmarks (bilingual dataset and BUMBLE) for comprehensive analysis.
	3. Demonstrated effectiveness in eliciting biases in popular LLMs, surpassing previous methods.

**Result:** Evaluations reveal that these methods are more effective in revealing LLMs' inner biases compared to existing baselines.

**Limitations:** 

**Conclusion:** The developed methodologies and benchmarks provide tools for evaluating LLMs' ethical risks, aiming to enhance accountability in their development.

**Abstract:** As large language models (LLMs) become an important way of information access, there have been increasing concerns that LLMs may intensify the spread of unethical content, including implicit bias that hurts certain populations without explicit harmful words. In this paper, we conduct a rigorous evaluation of LLMs' implicit bias towards certain demographics by attacking them from a psychometric perspective to elicit agreements to biased viewpoints. Inspired by psychometric principles in cognitive and social psychology, we propose three attack approaches, i.e., Disguise, Deception, and Teaching. Incorporating the corresponding attack instructions, we built two benchmarks: (1) a bilingual dataset with biased statements covering four bias types (2.7K instances) for extensive comparative analysis, and (2) BUMBLE, a larger benchmark spanning nine common bias types (12.7K instances) for comprehensive evaluation. Extensive evaluation of popular commercial and open-source LLMs shows that our methods can elicit LLMs' inner bias more effectively than competitive baselines. Our attack methodology and benchmarks offer an effective means of assessing the ethical risks of LLMs, driving progress toward greater accountability in their development. Our code, data and benchmarks are available at https://github.com/yuchenwen1/ImplicitBiasPsychometricEvaluation and https://github.com/yuchenwen1/BUMBLE.

</details>


### [130] [Dissecting the Ullman Variations with a SCALPEL: Why do LLMs fail at Trivial Alterations to the False Belief Task?](https://arxiv.org/abs/2406.14737)

*Zhiqiang Pi, Annapurna Vadaparty, Benjamin K. Bergen, Cameron R. Jones*

**Main category:** cs.CL

**Keywords:** Large Language Models, Theory of Mind, Cognitive Science, Common-Sense Inference, SCALPEL

**Relevance Score:** 9

**TL;DR:** This paper introduces SCALPEL, a technique to incrementally modify stimuli for testing reasons behind Large Language Models' (LLMs) failures in Theory of Mind (ToM) tasks, revealing common-sense inference shortcomings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of LLMs in Theory of Mind tasks and understand their failures in specific contexts.

**Method:** The paper presents SCALPEL, which incrementally modifies stimuli to test hypotheses about LLM performance on ToM evaluations.

**Key Contributions:**

	1. Introduction of SCALPEL technique for hypothesis testing on LLMs
	2. Insights into common-sense inferences in LLMs
	3. Discussion on the implications for cognitive science research

**Result:** LLMs struggle in ToM tasks primarily due to a lack of essential common-sense inferences, particularly in understanding transparent containers and their contents.

**Limitations:** 

**Conclusion:** Although LLMs exhibit advanced capabilities, they do not yet achieve robust human-like Theory of Mind, highlighting potential areas for further cognitive exploration.

**Abstract:** Recent empirical results have sparked a debate about whether or not Large Language Models (LLMs) are capable of Theory of Mind (ToM). While some have found LLMs to be successful on ToM evaluations such as the False Belief task, others have shown that their performance is not robust against trivial alterations to stimuli. In this paper, we introduce SCALPEL -- a technique to incrementally modify stimuli to test different specific hypotheses about why LLMs fail -- and apply this method to the "transparent-access" modification of the unexpected contents task. Our results suggest that LLMs often do poorly because they fail to make essential common-sense inferences, such as that seeing a transparent container implies recognizing its contents. We conclude that while modern LLMs go beyond mere pattern matching, they still fall short of robust human-like ToM. We argue that SCALPEL can help cognitive scientists examine LLMs' capabilities in finer detail and provide insight into alternative mechanisms by which tasks that are used to assess human cognition might be completed.

</details>


### [131] [Large Vocabulary Size Improves Large Language Models](https://arxiv.org/abs/2406.16508)

*Sho Takase, Ryokan Ri, Shun Kiyono, Takuya Kato*

**Main category:** cs.CL

**Keywords:** subword vocabulary, large language models, performance evaluation, continual training, vocabulary adaptation

**Relevance Score:** 8

**TL;DR:** This paper examines how subword vocabulary size impacts the performance of large language models, finding that larger vocabularies yield better results and introducing a new method for vocabulary adaptation in continual training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide insights on defining vocabulary size for optimizing large language models' performance.

**Method:** The authors conducted experiments to evaluate the relationship between subword vocabulary size and LLM performance, including a scenario of continual training with a new vocabulary.

**Key Contributions:**

	1. Demonstrated the influence of vocabulary size on LLM performance.
	2. Proposed a new method for vocabulary adaptation in continual training scenarios.
	3. Empirical evidence supporting the benefits of larger vocabularies for LLMs.

**Result:** Results indicate that increasing vocabulary size improves LLM performance, and the proposed method using a new vocabulary outperforms the original pre-training vocabulary during continual training.

**Limitations:** 

**Conclusion:** The study concludes that vocabulary size is a significant factor in LLM effectiveness, and the introduction of new vocabulary methods can enhance model performance in new language contexts.

**Abstract:** This paper empirically investigates the relationship between subword vocabulary size and the performance of large language models (LLMs) to provide insights on how to define the vocabulary size. Experimental results show that larger vocabulary sizes lead to better performance in LLMs. Moreover, we consider a continual training scenario where a pre-trained language model is trained on a different target language. We introduce a simple method to use a new vocabulary instead of the pre-defined one. We show that using the new vocabulary outperforms the model with the vocabulary used in pre-training.

</details>


### [132] [Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification](https://arxiv.org/abs/2407.07004)

*Raphaël Tinarrage, Henrique Ennes, Lucas Resck, Lucas T. Gomes, Jean R. Ponciano, Jorge Poco*

**Main category:** cs.CL

**Keywords:** binding precedents, Natural Language Processing, Case Classification, Brazilian legal system, legal impact

**Relevance Score:** 2

**TL;DR:** This paper explores the effectiveness of binding precedents in the Brazilian legal system, assessing their legal impact through Case Classification and comparing various Natural Language Processing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of binding precedents in reducing repetitive legal demands on the Federal Supreme Court of Brazil.

**Method:** The study employs methods of Natural Language Processing, including TF-IDF, LSTM, Longformer, and regex, for Classifying cases. It compares the Court's rulings before and after the establishment of five specific binding precedents.

**Key Contributions:**

	1. Comparison of different NLP methods for Case Classification
	2. Identification of hypotheses explaining the inefficiency of binding precedents
	3. Empirical assessment of the legal impact of specific binding precedents

**Result:** TF-IDF models outperformed LSTM and Longformer on standard metrics, although deep learning models detected significant legal events that TF-IDF missed. The study identifies five main hypotheses explaining the inefficacy of binding precedents.

**Limitations:** 

**Conclusion:** The failure of binding precedents to alleviate repetitive demands stems from heterogeneous and case-dependent reasons, making it challenging to isolate single causative factors.

**Abstract:** Binding precedents (s\'umulas vinculantes) constitute a juridical instrument unique to the Brazilian legal system and whose objectives include the protection of the Federal Supreme Court against repetitive demands. Studies of the effectiveness of these instruments in decreasing the Court's exposure to similar cases, however, indicate that they tend to fail in such a direction, with some of the binding precedents seemingly creating new demands. We empirically assess the legal impact of five binding precedents, 11, 14, 17, 26, and 37, at the highest Court level through their effects on the legal subjects they address. This analysis is only possible through the comparison of the Court's ruling about the precedents' themes before they are created, which means that these decisions should be detected through techniques of Similar Case Retrieval, which we tackle from the angle of Case Classification. The contributions of this article are therefore twofold: on the mathematical side, we compare the use of different methods of Natural Language Processing -- TF-IDF, LSTM, Longformer, and regex -- for Case Classification, whereas on the legal side, we contrast the inefficiency of these binding precedents with a set of hypotheses that may justify their repeated usage. We observe that the TF-IDF models performed slightly better than LSTM and Longformer when compared through common metrics; however, the deep learning models were able to detect certain important legal events that TF-IDF missed. On the legal side, we argue that the reasons for binding precedents to fail in responding to repetitive demand are heterogeneous and case-dependent, making it impossible to single out a specific cause. We identify five main hypotheses, which are found in different combinations in each of the precedents studied.

</details>


### [133] [Prompt-based Personality Profiling: Reinforcement Learning for Relevance Filtering](https://arxiv.org/abs/2409.04122)

*Jan Hofmann, Cornelia Sindermann, Roman Klinger*

**Main category:** cs.CL

**Keywords:** Author Profiling, Machine Learning, Reinforcement Learning, Large Language Models, Personality Prediction

**Relevance Score:** 8

**TL;DR:** This paper introduces a method for author profiling that filters relevant content first before performing user profiling, utilizing reinforcement learning to fine-tune relevance without annotated data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in author profiling due to large post volumes that exceed Transformer input limits and the inefficiencies of using API-accessed models.

**Method:** A new author profiling approach that distinguishes relevant from irrelevant content using a reinforcement learning-optimized relevance filter, capitalizing on the zero-shot abilities of large language models.

**Key Contributions:**

	1. Introduction of a relevance filtering method for author profiling
	2. Utilization of reinforcement learning to optimize relevance without annotated data
	3. Empirical evaluation demonstrating improved accuracy while using reduced context

**Result:** The proposed method demonstrates comparable efficacy to traditional methods using all posts while reducing context length significantly, and leads to improved accuracy on balanced datasets by filtering relevant posts.

**Limitations:** 

**Conclusion:** The filtering approach not only addresses input limitations but also enhances prediction accuracy for personality traits in author profiling tasks.

**Abstract:** Author profiling is the task of inferring characteristics about individuals by analyzing content they share. Supervised machine learning still dominates automatic systems that perform this task, despite the popularity of prompting large language models to address natural language understanding tasks. One reason is that the classification instances consist of large amounts of posts, potentially a whole user profile, which may exceed the input length of Transformers. Even if a model can use a large context window, the entirety of posts makes the application of API-accessed black box systems costly and slow, next to issues which come with such "needle-in-the-haystack" tasks. To mitigate this limitation, we propose a new method for author profiling which aims at distinguishing relevant from irrelevant content first, followed by the actual user profiling only with relevant data. To circumvent the need for relevance-annotated data, we optimize this relevance filter via reinforcement learning with a reward function that utilizes the zero-shot capabilities of large language models. We evaluate our method for Big Five personality trait prediction on two Twitter corpora. On publicly available real-world data with a skewed label distribution, our method shows similar efficacy to using all posts in a user profile, but with a substantially shorter context. An evaluation on a version of these data balanced with artificial posts shows that the filtering to relevant posts leads to a significantly improved accuracy of the predictions.

</details>


### [134] [Nonlinear second-order dynamics describe labial constriction trajectories across languages and contexts](https://arxiv.org/abs/2410.08351)

*Michael C. Stern, Jason A. Shaw*

**Main category:** cs.CL

**Keywords:** articulatory dynamics, labial constriction, speech production, nonlinear model, kinematics

**Relevance Score:** 2

**TL;DR:** This study analyzes labial constriction trajectories during the production of /b/ and /m/ sounds in English and Mandarin, proposing a nonlinear dynamical model to explain these movements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand the dynamics of articulatory movements and how various factors influence these dynamics in speech production.

**Method:** The study uses differential equations to model labial constriction trajectories, testing a second-order dynamical system that incorporates parameters representing target state and movement rapidity.

**Key Contributions:**

	1. Introduction of a nonlinear dynamics model for articulatory movements
	2. Empirical validation of the model against speech production data
	3. Insights into the relationship between movement dynamics and phonetic control parameters

**Result:** The model demonstrates excellent fits to both generalized and individual articulation trajectories, effectively capturing key kinematic variables.

**Limitations:** The model's applicability might be limited to specific speech sounds and does not account for all articulatory movements.

**Conclusion:** The proposed model provides a new understanding of articulatory movements, potentially informing further studies on speech dynamics and related phenomena.

**Abstract:** We investigate the dynamics of labial constriction trajectories during the production of /b/ and /m/ in English and Mandarin. We find that, across languages and contexts, the ratio of instantaneous displacement to instantaneous velocity generally follows an exponential decay curve from movement onset to movement offset. We formalize this empirical discovery in a differential equation and, in combination with an assumption of point attractor dynamics, derive a nonlinear second-order dynamical system describing labial constriction trajectories. The equation has only two parameters, T and r. T corresponds to the target state and r corresponds to movement rapidity. Thus, each of the parameters corresponds to a phonetically relevant dimension of control. Nonlinear regression demonstrates that the model provides excellent fits to individual movement trajectories. Moreover, trajectories simulated from the model qualitatively match empirical trajectories, and capture key kinematic variables like duration, peak velocity, and time to achieve peak velocity. The model constitutes a proposal for the dynamics of individual articulatory movements, and thus offers a novel foundation from which to understand additional influences on articulatory kinematics like prosody, inter-movement coordination, and stochastic noise.

</details>


### [135] [Which Demographics do LLMs Default to During Annotation?](https://arxiv.org/abs/2410.08820)

*Johannes Schäfer, Aidan Combs, Christopher Bagdon, Jiahui Li, Nadine Probol, Lynn Greschner, Sean Papay, Yarik Menchaca Resendiz, Aswathy Velutharambath, Amelie Wührl, Sabine Weber, Roman Klinger*

**Main category:** cs.CL

**Keywords:** demographics, text annotation, large language models, bias, popularity

**Relevance Score:** 9

**TL;DR:** This paper investigates how demographics and cultural backgrounds of annotators influence label variations in text annotation and how large language models (LLMs) can mimic or manipulate these variations through demographic conditioning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding label variations in text annotation is crucial to represent diverse societal members adequately. This research aims to explore the biases in LLMs and how demographic information can influence their outputs in text annotation.

**Method:** The study evaluates the responses of LLMs to prompts conditioned by demographics versus non-demographic and placebo conditions. It specifically assesses how LLMs mimic human annotator attributes related to politeness and offensiveness using the POPQUORN dataset.

**Key Contributions:**

	1. Combines research on LLM biases and diversity in output based on demographic information.
	2. Utilizes the POPQUORN dataset to analyze human label variations in LLMs.
	3. Demonstrates notable influences of demographic factors on LLM responses, challenging previous studies.

**Result:** The investigation reveals significant influences of gender, race, and age in responses affected by demographic prompting, contrasting with earlier findings that indicated no demographic effects on LLM outputs.

**Limitations:** The research is limited to the specific contexts of politeness and offensiveness annotations and may not generalize to other types of textual annotations.

**Conclusion:** Acknowledging and addressing demographic influences in text annotation can enhance the representation and quality of machine-generated labels, underscoring the importance of diverse inputs for LLMs.

**Abstract:** Demographics and cultural background of annotators influence the labels they assign in text annotation -- for instance, an elderly woman might find it offensive to read a message addressed to a "bro", but a male teenager might find it appropriate. It is therefore important to acknowledge label variations to not under-represent members of a society. Two research directions developed out of this observation in the context of using large language models (LLM) for data annotations, namely (1) studying biases and inherent knowledge of LLMs and (2) injecting diversity in the output by manipulating the prompt with demographic information. We combine these two strands of research and ask the question to which demographics an LLM resorts to when no demographics is given. To answer this question, we evaluate which attributes of human annotators LLMs inherently mimic. Furthermore, we compare non-demographic conditioned prompts and placebo-conditioned prompts (e.g., "you are an annotator who lives in house number 5") to demographics-conditioned prompts ("You are a 45 year old man and an expert on politeness annotation. How do you rate {instance}"). We study these questions for politeness and offensiveness annotations on the POPQUORN data set, a corpus created in a controlled manner to investigate human label variations based on demographics which has not been used for LLM-based analyses so far. We observe notable influences related to gender, race, and age in demographic prompting, which contrasts with previous studies that found no such effects.

</details>


### [136] [Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models](https://arxiv.org/abs/2410.13080)

*Linhao Luo, Zicheng Zhao, Gholamreza Haffari, Yuan-Fang Li, Chen Gong, Shirui Pan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Reasoning, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** Graph-constrained reasoning (GCR) enhances LLMs by integrating knowledge graphs (KGs) into the reasoning process, ensuring accurate and faithful reasoning without hallucinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle reasoning errors in large language models (LLMs) caused by knowledge gaps and hallucinations, leveraging structured knowledge from knowledge graphs (KGs) is proposed.

**Method:** The proposed method GCR integrates the structure of knowledge graphs into the decoding process of LLMs through a novel KG-Trie index, allowing direct reasoning on graphs to produce accurate outcomes.

**Key Contributions:**

	1. Introduction of graph-constrained reasoning (GCR) framework
	2. Development of KG-Trie for efficient graph reasoning
	3. Achievement of state-of-the-art performance on KGQA benchmarks

**Result:** GCR shows state-of-the-art performance on KGQA benchmarks and demonstrates strong zero-shot generalization to unseen KGs, effectively eliminating reasoning hallucinations.

**Limitations:** 

**Conclusion:** By bridging structured and unstructured knowledge, GCR represents a significant advancement in improving the reasoning capabilities of LLMs, making them more reliable.

**Abstract:** Large language models (LLMs) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs (KGs) have been utilized to enhance LLM reasoning through their structured knowledge. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this work, we introduce graph-constrained reasoning (GCR), a novel framework that bridges structured knowledge in KGs with unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reasoning by integrating KG structure into the LLM decoding process through KG-Trie, a trie-based index that encodes KG reasoning paths. KG-Trie constrains the decoding process, allowing LLMs to directly reason on graphs and generate faithful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight KG-specialized LLM for graph-constrained reasoning alongside a powerful general LLM for inductive reasoning over multiple reasoning paths, resulting in accurate reasoning with zero reasoning hallucination. Extensive experiments on several KGQA benchmarks demonstrate that GCR achieves state-of-the-art performance and exhibits strong zero-shot generalizability to unseen KGs without additional training.

</details>


### [137] [SafetyAnalyst: Interpretable, Transparent, and Steerable Safety Moderation for AI Behavior](https://arxiv.org/abs/2410.16665)

*Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine*

**Main category:** cs.CL

**Keywords:** AI moderation, safety framework, harm-benefit analysis

**Relevance Score:** 8

**TL;DR:** SafetyAnalyst is a new AI safety moderation framework that enhances interpretability and steerability in AI decision-making through a harm-benefit analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current AI safety moderation systems lack structural interpretability and the ability to align with community values. SafetyAnalyst aims to fill this gap.

**Method:** SafetyAnalyst analyzes AI behavior using chain-of-thought reasoning to create a harm-benefit tree, aggregating potential actions into a harmfulness score based on 28 interpretable parameters.

**Key Contributions:**

	1. Introduction of a harm-benefit tree for AI behavior analysis
	2. Achievement of high performance in prompt safety classification
	3. Provision of a framework that is interpretable and steerable

**Result:** SafetyAnalyst outperforms current moderation systems on prompt safety classification tasks, achieving an average F1 score of 0.81 compared to less than 0.72 for others, while providing improved interpretability and transparency.

**Limitations:** 

**Conclusion:** SafetyAnalyst provides a significant advancement in AI safety moderation, making it a valuable tool for ensuring AI alignment with safety standards.

**Abstract:** The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured "harm-benefit tree," which enumerates harmful and beneficial actions and effects the AI behavior may lead to, along with likelihood, severity, and immediacy labels that describe potential impacts on stakeholders. SafetyAnalyst then aggregates all effects into a harmfulness score using 28 fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this framework to develop an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On comprehensive benchmarks, we show that SafetyAnalyst (average F1=0.81) outperforms existing moderation systems (average F1$<$0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.

</details>


### [138] [Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case Study on English-Korean Code-Switching](https://arxiv.org/abs/2410.18436)

*Seoyeon Kim, Huiseo Kim, Chanjun Park, Jinyoung Yeo, Dongha Lee*

**Main category:** cs.CL

**Keywords:** code-switching, multilingual LLMs, low-resource languages, question-answering, knowledge activation

**Relevance Score:** 9

**TL;DR:** This paper explores how code-switching can enhance large language models' performance on low-resource language tasks by activating language-specific knowledge.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in LLMs, their performance is mainly centered around English, leaving low-resource languages underrepresented. The research investigates leveraging code-switching to improve task-solving capabilities in these languages.

**Method:** We introduce EnKoQA, a dataset designed for English-Korean code-switching in question-answering tasks. The analysis of multilingual LLMs is conducted by separating the activation process into knowledge identification and knowledge leveraging.

**Key Contributions:**

	1. Introduction of the EnKoQA dataset for English-Korean code-switching
	2. Analytical framework for knowledge activation in multilingual LLMs
	3. Demonstration of enhanced performance on low-resource language tasks using code-switching

**Result:** The study reveals that code-switching significantly activates relevant language-specific knowledge in LLMs when applied to low-resource language tasks, outpacing performance with solely English text.

**Limitations:** The focus is on English and Korean, potentially limiting applicability to other language pairs and does not address practical deployment in real-world applications.

**Conclusion:** Code-switching is a valuable strategy for enhancing LLMs' effectiveness in understanding and working with low-resource languages.

**Abstract:** Recent large language models (LLMs) demonstrate multilingual abilities, yet they are English-centric due to dominance of English in training corpora. The limited resource for low-resource languages remains a crucial challenge. Code-switching (CS), a phenomenon where multilingual speakers alternate between languages in a discourse, can convey subtle cultural and linguistic nuances that can be otherwise lost in translation and elicits language-specific knowledge in human communications. In light of this, we investigate whether code-switching can 'activate', or identify and leverage knowledge for reasoning when LLMs solve low-resource language tasks. To facilitate the research, we first present EnKoQA, a synthetic English-Korean CS question-answering dataset. We provide comprehensive analysis on a variety of multilingual LLMs by subdividing activation process into knowledge identification and knowledge leveraging. Our results demonstrate that compared to English text, CS can faithfully activate knowledge inside LLMs especially on language-specific domains, suggesting the potential of code-switching on low-resource language tasks.

</details>


### [139] [Controllable Context Sensitivity and the Knob Behind It](https://arxiv.org/abs/2411.07404)

*Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell*

**Main category:** cs.CL

**Keywords:** language models, context sensitivity, prior knowledge, instruct tuning, machine learning

**Relevance Score:** 9

**TL;DR:** The paper investigates how language models balance reliance on context versus prior knowledge, introducing a controllable context sensitivity task and identifying a crucial subspace in model layers that influences this behavior.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language model performance in tasks requiring contextual or prior knowledge by manipulating sensitivity to context.

**Method:** The authors designed a task where models must choose to answer questions based on given context or prior knowledge, assessing performance through fine-tuning and analysis of model layers.

**Key Contributions:**

	1. Introduction of a controllable context sensitivity task for language models
	2. Identification of a significant subspace in model layers that controls context sensitivity
	3. Demonstrated performance correlation with context-agreeing versus context-ignoring responses.

**Result:** Instruction-tuned Llama-3.1, Mistral-v0.3, and Gemma-2 achieved 85-95% accuracy on the task, revealing a consistent subspace that determines context versus prior knowledge reliance.

**Limitations:** 

**Conclusion:** The findings suggest a fundamental mechanism through which language models can control their reliance on context versus prior knowledge, applicable across model variants.

**Abstract:** When making predictions, a language model must trade off how much it relies on its context vs. its prior knowledge. Choosing how sensitive the model is to its context is a fundamental functionality, as it enables the model to excel at tasks like retrieval-augmented generation and question-answering. In this paper, we search for a knob which controls this sensitivity, determining whether language models answer from the context or their prior knowledge. To guide this search, we design a task for controllable context sensitivity. In this task, we first feed the model a context (Paris is in England) and a question (Where is Paris?); we then instruct the model to either use its prior or contextual knowledge and evaluate whether it generates the correct answer for both intents (either France or England). When fine-tuned on this task, instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it with high accuracy (85-95%). Analyzing these high-performing models, we narrow down which layers may be important to context sensitivity using a novel linear time algorithm. Then, in each model, we identify a 1-D subspace in a single layer that encodes whether the model follows context or prior knowledge. Interestingly, while we identify this subspace in a fine-tuned model, we find that the exact same subspace serves as an effective knob in not only that model but also non-fine-tuned instruct and base models of that model family. Finally, we show a strong correlation between a model's performance and how distinctly it separates context-agreeing from context-ignoring answers in this subspace. These results suggest a single subspace facilitates how the model chooses between context and prior knowledge, hinting at a simple fundamental mechanism that controls this behavior.

</details>


### [140] [Overcoming Non-monotonicity in Transducer-based Streaming Generation](https://arxiv.org/abs/2411.17170)

*Zhengrui Ma, Yang Feng, Min Zhang*

**Main category:** cs.CL

**Keywords:** Transducer, monotonic attention, streaming generation, non-monotonic alignments, machine learning

**Relevance Score:** 6

**TL;DR:** The MonoAttn-Transducer model addresses challenges in input-synchronous decoding by integrating monotonic attention, enabling better handling of non-monotonic alignments in streaming generation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Transducer architecture performance in tasks requiring non-monotonic alignments, which are problematic in simultaneous translation and other streaming applications.

**Method:** Integration of a learnable monotonic attention mechanism with the Transducer's decoding process, utilizing the forward-backward algorithm to infer alignments without enumerating the alignment space.

**Key Contributions:**

	1. Introduction of learnable monotonic attention to Transducer architecture
	2. Utilization of the forward-backward algorithm for efficient alignment inference
	3. Demonstrated effectiveness in non-monotonic alignment scenarios during experiments.

**Result:** The MonoAttn-Transducer demonstrates effective handling of non-monotonic alignments in extensive experiments, outperforming traditional methods in streaming scenarios.

**Limitations:** 

**Conclusion:** This model offers a robust solution for complex generation tasks that require non-monotonic alignment handling.

**Abstract:** Streaming generation models are utilized across fields, with the Transducer architecture being popular in industrial applications. However, its input-synchronous decoding mechanism presents challenges in tasks requiring non-monotonic alignments, such as simultaneous translation. In this research, we address this issue by integrating Transducer's decoding with the history of input stream via a learnable monotonic attention. Our approach leverages the forward-backward algorithm to infer the posterior probability of alignments between the predictor states and input timestamps, which is then used to estimate the monotonic context representations, thereby avoiding the need to enumerate the exponentially large alignment space during training. Extensive experiments show that our MonoAttn-Transducer effectively handles non-monotonic alignments in streaming scenarios, offering a robust solution for complex generation tasks.

</details>


### [141] [ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning](https://arxiv.org/abs/2412.11418)

*Liyu Zhang, Weiqi Wang, Tianqing Fang, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Knowledge Editing, Commonsense Reasoning, Large Language Models

**Relevance Score:** 9

**TL;DR:** ConceptEdit is a framework designed to improve the commonsense reasoning capabilities of Large Language Models (LLMs) by integrating conceptualization and instantiation in the Knowledge Editing (KE) pipeline.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance LLMs' commonsense knowledge without the costly process of retraining the entire model.

**Method:** ConceptEdit identifies implausible commonsense knowledge through a verifier LLM and improves it by adding conceptualization for better generalizability.

**Key Contributions:**

	1. Introduction of the ConceptEdit framework for commonsense knowledge editing
	2. Dynamic diagnosis of commonsense knowledge inaccuracies using a verifier LLM
	3. Enhanced LLM performance across multiple question answering tasks through improved knowledge augmentation.

**Result:** LLMs augmented with ConceptEdit generated more plausible commonsense knowledge and performed better across various question answering benchmarks compared to existing methods.

**Limitations:** 

**Conclusion:** ConceptEdit addresses key challenges in commonsense knowledge editing, achieving significant improvements in knowledge plausibility and overall performance.

**Abstract:** Knowledge Editing (KE) aims to adjust a Large Language Model's (LLM) internal representations and parameters to correct inaccuracies and improve output consistency without incurring the computational expense of re-training the entire model. However, editing commonsense knowledge still faces difficulties, including limited knowledge coverage in existing resources, the infeasibility of annotating labels for an overabundance of commonsense knowledge, and the strict knowledge formats of current editing methods. In this paper, we address these challenges by presenting ConceptEdit, a framework that integrates conceptualization and instantiation into the KE pipeline for LLMs to enhance their commonsense reasoning capabilities. ConceptEdit dynamically diagnoses implausible commonsense knowledge within an LLM using another verifier LLM and augments the source knowledge to be edited with conceptualization for stronger generalizability. Experimental results demonstrate that LLMs enhanced with ConceptEdit successfully generate commonsense knowledge with improved plausibility compared to other baselines and achieve stronger performance across multiple question answering benchmarks. Our data, code, and models are publicly available at https://github.com/HKUST-KnowComp/ConKE.

</details>


### [142] [Core Context Aware Transformers for Long Context Language Modeling](https://arxiv.org/abs/2412.12465)

*Yaofo Chen, Zeng You, Shuhai Zhang, Haokun Li, Yirui Li, Yaowei Wang, Mingkui Tan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Long-context modeling, Attention mechanisms

**Relevance Score:** 9

**TL;DR:** This paper proposes the Core Context Aware (CCA) Attention mechanism for more efficient long-context modeling in LLMs, mitigating redundancy and enhancing representation performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and redundancy in processing long contexts in Transformer-based Large Language Models (LLMs) as context length increases.

**Method:** The proposed CCA Attention consists of two modules: a Globality-aware pooling module that compresses input tokens into core tokens, and a Locality-preserving module that incorporates neighboring tokens to maintain local context.

**Key Contributions:**

	1. Introduction of Core Context Aware (CCA) Attention for long-context modeling
	2. Globality-aware pooling module for reducing redundant information
	3. Locality-preserving module for maintaining detailed local context

**Result:** The proposed method improves long-context modeling performance and computational efficiency compared to existing state-of-the-art approaches.

**Limitations:** 

**Conclusion:** CCA-Attention can effectively replace self-attention in LLMs with minimal fine-tuning, exhibiting enhanced efficiency and effectiveness for long contexts.

**Abstract:** Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods.

</details>


### [143] [Revisiting In-Context Learning with Long Context Language Models](https://arxiv.org/abs/2412.16926)

*Jinheon Baek, Sun Jae Lee, Prakhar Gupta, Geunseob Oh, Siddharth Dalmia, Prateek Kolhar*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Long Context Language Models, example selection, data augmentation, machine learning

**Relevance Score:** 8

**TL;DR:** This paper examines the impact of Long Context Language Models (LCLMs) on In-Context Learning (ICL), revealing that sample selection techniques may be less critical than previously thought, while emphasizing the importance of example quantity and augmentation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of Long Context Language Models, we aim to explore how this affects In-Context Learning performance, particularly regarding example selection methods.

**Method:** We conducted extensive experiments on 18 datasets across 4 tasks to evaluate the effect of example selection techniques in conjunction with LCLMs.

**Key Contributions:**

	1. Reevaluation of example selection techniques in LCLM context
	2. Introduction of data augmentation to improve ICL
	3. Empirical results across multiple datasets and tasks

**Result:** The study found that sophisticated example selection does not significantly outperform random selection, and that simply filling the context window with examples, augmented as necessary, can lead to notable performance gains in ICL.

**Limitations:** 

**Conclusion:** LCLMs shift the challenge from selecting the best examples to ensuring enough examples are present, and data augmentation can effectively enhance performance.

**Abstract:** In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we discover that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.

</details>


### [144] [FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation](https://arxiv.org/abs/2501.00777)

*Qianli Wang, Nils Feldhus, Simon Ostermann, Luis Felipe Villa-Arenas, Sebastian Möller, Vera Schmitt*

**Main category:** cs.CL

**Keywords:** Counterfactual examples, Feature attribution, Natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces ZeroCF for generating counterfactual examples in NLP and FitCF for verifying them, improving model performance in few-shot prompting.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the generation of counterfactual examples in NLP for improving models and understanding model behavior via explainable AI.

**Method:** The authors propose ZeroCF, a zero-shot approach using important words from feature attribution methods, and FitCF, which verifies counterfactual examples through label flip verification and incorporates them into few-shot prompting.

**Key Contributions:**

	1. Introduction of ZeroCF for generating counterfactuals
	2. Development of FitCF for verifying counterfactuals
	3. Demonstration of the impact of demonstration quantity on performance

**Result:** FitCF outperforms two state-of-the-art baselines, showing that the number of demonstrations significantly affects performance.

**Limitations:** 

**Conclusion:** There is a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, providing insights for future research.

**Abstract:** Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an important finding for future research in this direction.

</details>


### [145] [PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models](https://arxiv.org/abs/2501.03124)

*Mingyang Song, Zhaochen Su, Xiaoye Qu, Jiawei Zhou, Yu Cheng*

**Main category:** cs.CL

**Keywords:** Process-level Reward Models, Benchmark, Error Detection, Natural Language Processing, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper introduces PRMBench, a benchmark for evaluating Process-level Reward Models (PRMs) in detecting errors during reasoning and decision-making tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks inadequately assess the nuanced error detection capabilities of PRMs, focusing primarily on step correctness.

**Method:** Introduced PRMBench, containing 6,216 designed problems and 83,456 labels to systematically evaluate multiple dimensions of model performance.

**Key Contributions:**

	1. Introduction of PRMBench for fine-grained error detection evaluation in PRMs.
	2. Extensive dataset with 6,216 problems and 83,456 labels for multi-dimensional evaluation.
	3. Empirical results highlighting weaknesses in existing models.

**Result:** Experiments on 15 different models revealed significant weaknesses in PRMs' error detection capabilities.

**Limitations:** Primarily focused on error detection; other aspects of PRM evaluation may need further exploration.

**Conclusion:** PRMBench aims to provide a robust framework for evaluating and advancing research on PRM development.

**Abstract:** Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.

</details>


### [146] [LLMs Reproduce Stereotypes of Sexual and Gender Minorities](https://arxiv.org/abs/2501.05926)

*Ruby Ostrow, Adam Lopez*

**Main category:** cs.CL

**Keywords:** gender bias, large language models, sexual minorities, stereotype content model, text generation

**Relevance Score:** 9

**TL;DR:** The paper investigates gender and sexual bias in large language models (LLMs), highlighting the limitations of binary gender representation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore gender and sexual biases in LLMs beyond the binary view, acknowledging the spectrum of identities.

**Method:** The study uses the Stereotype Content Model to analyze survey responses and text generation output from LLMs.

**Key Contributions:**

	1. Analyzes LLM biases beyond the binary gender framework
	2. Uses the Stereotype Content Model for bias evaluation
	3. Highlights the impact of LLMs in creative writing and representation

**Result:** Findings reveal that LLMs perpetuate negative stereotypes of sexual and gender minorities and amplify representational harms in text generation.

**Limitations:** Focuses primarily on English-language models and contexts.

**Conclusion:** LLMs demonstrate significant biases that need addressing, particularly in creative writing applications.

**Abstract:** A large body of research has found substantial gender bias in NLP systems. Most of this research takes a binary, essentialist view of gender: limiting its variation to the categories _men_ and _women_, conflating gender with sex, and ignoring different sexual identities. But gender and sexuality exist on a spectrum, so in this paper we study the biases of large language models (LLMs) towards sexual and gender minorities beyond binary categories. Grounding our study in a widely used social psychology model -- the Stereotype Content Model -- we demonstrate that English-language survey questions about social perceptions elicit more negative stereotypes of sexual and gender minorities from both humans and LLMs. We then extend this framework to a more realistic use case: text generation. Our analysis shows that LLMs generate stereotyped representations of sexual and gender minorities in this setting, showing that they amplify representational harms in creative writing, a widely advertised use for LLMs.

</details>


### [147] [Gender-Neutral Large Language Models for Medical Applications: Reducing Bias in PubMed Abstracts](https://arxiv.org/abs/2501.06365)

*Elizabeth Schaefer, Kirk Roberts*

**Main category:** cs.CL

**Keywords:** gender bias, large language models, medical literature, pronoun neutralization, BERT

**Relevance Score:** 9

**TL;DR:** A pipeline to mitigate gender bias in LLMs for medical literature through pronoun neutralization is presented, using a new model trained on modified abstracts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address gender bias in medical literature and improve the inclusivity of language used in LLMs.

**Method:** A dataset of 379,000 PubMed abstracts was processed to neutralize gendered occupational pronouns; a BERT-based model, MOBERT, was trained and compared to 1965BERT.

**Key Contributions:**

	1. Introduction of MOBERT for gender bias mitigation in LLMs.
	2. Significant performance improvement over previous models in pronoun replacement.
	3. Analysis revealing correlation between replacement accuracy and occupational term frequency.

**Result:** MOBERT achieved a 70% inclusive replacement rate for gendered pronouns, significantly outperforming 1965BERT's 4%.

**Limitations:** 

**Conclusion:** Expanding the dataset and refining the method is proposed to further enhance equitable language modeling in medical contexts.

**Abstract:** This paper presents a pipeline for mitigating gender bias in large language models (LLMs) used in medical literature by neutralizing gendered occupational pronouns. A dataset of 379,000 PubMed abstracts from 1965-1980 was processed to identify and modify pronouns tied to professions. We developed a BERT-based model, "Modern Occupational Bias Elimination with Refined Training," or "MOBERT," trained on these neutralized abstracts, and compared its performance with "1965BERT," trained on the original dataset. MOBERT achieved a 70% inclusive replacement rate, while 1965BERT reached only 4%. A further analysis of MOBERT revealed that pronoun replacement accuracy correlated with the frequency of occupational terms in the training data. We propose expanding the dataset and refining the pipeline to improve performance and ensure more equitable language modeling in medical applications.

</details>


### [148] [K-COMP: Retrieval-Augmented Medical Domain Question Answering With Knowledge-Injected Compressor](https://arxiv.org/abs/2501.13567)

*Jeonghun Cho, Gary Geunbae Lee*

**Main category:** cs.CL

**Keywords:** retrieval-augmented QA, K-comp, knowledge injection, compression, reader models

**Relevance Score:** 8

**TL;DR:** This paper introduces K-comp, a method for enhancing retrieval-augmented question answering by automatically generating prior knowledge to improve comprehension and reduce hallucinations in reader models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Retrieval-augmented QA models struggle with high expertise documents and irrelevant information, leading to inaccuracies and mistrust in the input passages.

**Method:** K-comp generates the necessary prior knowledge before compressing retrieved passages, integrating it into the context to align with question intent.

**Key Contributions:**

	1. Introduction of the K-comp method for knowledge injection in QA
	2. Autoregressive compression of passages with integrated prior knowledge
	3. Improved trust and accuracy in reader models

**Result:** K-comp helps reader models trust the context and improve their accuracy in answering questions by providing relevant prior knowledge and a concise context.

**Limitations:** 

**Conclusion:** The proposed method effectively enhances the performance of reader models in retrieval-augmented question answering scenarios by ensuring alignment and trust.

**Abstract:** Retrieval-augmented question answering (QA) integrates external information and thereby increases the QA accuracy of reader models that lack domain knowledge. However, documents retrieved for closed domains require high expertise, so the reader model may have difficulty fully comprehending the text. Moreover, the retrieved documents contain thousands of tokens, some unrelated to the question. As a result, the documents include some inaccurate information, which could lead the reader model to mistrust the passages and could result in hallucinations. To solve these problems, we propose K-comp (Knowledge-injected compressor) which provides the knowledge required to answer correctly. The compressor automatically generates the prior knowledge necessary to facilitate the answer process prior to compression of the retrieved passages. Subsequently, the passages are compressed autoregressively, with the generated knowledge being integrated into the compression process. This process ensures alignment between the question intent and the compressed context. By augmenting this prior knowledge and concise context, the reader models are guided toward relevant answers and trust the context.

</details>


### [149] [Redundancy Principles for MLLMs Benchmarks](https://arxiv.org/abs/2501.13953)

*Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan, Kai Chen, Guangtao Zhai*

**Main category:** cs.CL

**Keywords:** Multi-modality Large Language Models, benchmark redundancy, MLLM evaluations

**Relevance Score:** 6

**TL;DR:** This paper analyzes redundancy in Multi-modality Large Language Model (MLLM) benchmarks and proposes principles for creating effective evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address significant redundancy in MLLM benchmarks caused by rapid production of evaluations.

**Method:** A comprehensive analysis of MLLM performance on over 20 benchmarks to measure redundancy from multiple perspectives.

**Key Contributions:**

	1. Assessment of benchmark capability dimensions for redundancy
	2. Quantitative analysis of test question redundancy
	3. Insights for improving cross-benchmark evaluations

**Result:** Quantitative assessment showing the level of redundancy across benchmarks, providing insights for future benchmark development.

**Limitations:** 

**Conclusion:** Effective strategies are proposed to refine MLLM benchmarks and reduce redundancy issues.

**Abstract:** With the rapid iteration of Multi-modality Large Language Models (MLLMs) and the evolving demands of the field, the number of benchmarks produced annually has surged into the hundreds. The rapid growth has inevitably led to significant redundancy among benchmarks. Therefore, it is crucial to take a step back and critically assess the current state of redundancy and propose targeted principles for constructing effective MLLM benchmarks. In this paper, we focus on redundancy from three key perspectives: 1) Redundancy of benchmark capability dimensions, 2) Redundancy in the number of test questions, and 3) Cross-benchmark redundancy within specific domains. Through the comprehensive analysis over hundreds of MLLMs' performance across more than 20 benchmarks, we aim to quantitatively measure the level of redundancy lies in existing MLLM evaluations, provide valuable insights to guide the future development of MLLM benchmarks, and offer strategies to refine and address redundancy issues effectively. The code is available at https://github.com/zzc-1998/Benchmark-Redundancy.

</details>


### [150] [Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains](https://arxiv.org/abs/2501.14431)

*Xu Chu, Zhijie Tan, Hanlin Xue, Guanyu Wang, Tong Mo, Weiping Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, reasoning, high-stakes domains, explainability, supervised fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper presents Domain$o1$s, enhancing LLMs' reasoning capabilities for high-stakes domain tasks through supervised fine-tuning, tree search, and a new evaluation metric for explainability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs generate brief answers without reasoning, limiting users' confidence in high-stakes domains.

**Method:** Domain$o1$s is developed using CoT-stock-2k and CoT-legal-2k datasets through supervised fine-tuning and employs Selective Tree Exploration for optimal reasoning paths.

**Key Contributions:**

	1. Introduction of Domain$o1$s for enhanced LLM reasoning
	2. Development of CoT-stock-2k and CoT-legal-2k datasets
	3. Proposal of PROOF-Score for evaluating explainability

**Result:** Experimental results on stock investment and legal QA tasks demonstrate Domain$o1$s's superior performance and enhanced explainability compared to existing methods.

**Limitations:** 

**Conclusion:** Domain$o1$s significantly improves LLM reasoning for high-stakes tasks, offering a more robust framework for explanation and decision making.

**Abstract:** Large Language Models (LLMs) are widely applied to downstream domains. However, current LLMs for high-stakes domain tasks, such as financial investment and legal QA, typically generate brief answers without reasoning processes and explanations. This limits users' confidence in making decisions based on their responses. While original CoT shows promise, it lacks self-correction mechanisms during reasoning. This work introduces Domain$o1$s, which enhances LLMs' reasoning capabilities on domain tasks through supervised fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k datasets for fine-tuning models that activate domain-specific reasoning steps based on their judgment. Additionally, we propose Selective Tree Exploration to spontaneously explore solution spaces and sample optimal reasoning paths to improve performance. We also introduce PROOF-Score, a new metric for evaluating domain models' explainability, complementing traditional accuracy metrics with richer assessment dimensions. Extensive experiments on stock investment recommendation and legal reasoning QA tasks demonstrate Domaino1s's leading performance and explainability. Our code is available at https://github.com/Hyalinesky/Domaino1s.

</details>


### [151] [Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing](https://arxiv.org/abs/2502.00602)

*Tianci Liu, Ruirui Li, Zihan Dong, Hui Liu, Xianfeng Tang, Qingyu Yin, Linjun Zhang, Haoyu Wang, Jing Gao*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, token-level smoothing, heterogeneous token overfitting, parameter updates

**Relevance Score:** 9

**TL;DR:** OVERTONE is a novel method for knowledge editing in LLMs that addresses heterogeneous token overfitting, improving reasoning about newly added knowledge with low computational overhead.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs' knowledge can quickly become outdated due to their training on static corpora, necessitating effective knowledge editing methods that preserve pre-trained capabilities.

**Method:** We propose OVERTONE, a token-level smoothing method that mitigates heterogeneous token overfitting by adaptively refining the target distribution during knowledge updates.

**Key Contributions:**

	1. Introduction of OVERTONE for mitigating heterogeneous token overfitting in LLMs
	2. Demonstrated low computational overhead for knowledge updates
	3. Validation across multiple LLMs and editing scenarios

**Result:** Experiments show that OVERTONE effectively improves parameter updates and maintains reasoning abilities across multiple editing methods and LLM architectures.

**Limitations:** The performance may vary depending on the specific LLM architecture and the nature of the knowledge being edited.

**Conclusion:** OVERTONE enhances the knowledge editing process in LLMs by addressing the issue of token overfitting, making it a versatile and effective solution for keeping LLMs updated.

**Abstract:** Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates. Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: heterogeneous token overfitting (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates. To tackle this, we propose OVERTONE, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method.

</details>


### [152] [Advancing Reasoning in Large Language Models: Promising Methods and Approaches](https://arxiv.org/abs/2502.03671)

*Avinash Patil, Aryan Jadon*

**Main category:** cs.CL

**Keywords:** Large Language Models, reasoning, natural language processing, evaluation frameworks, neuro-symbolic integration

**Relevance Score:** 9

**TL;DR:** This survey reviews techniques to enhance reasoning capabilities in Large Language Models (LLMs), categorizing methods into prompting strategies, architectural innovations, and learning paradigms, while exploring evaluation frameworks and future research directions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in performing complex reasoning tasks, which often falls below human standards despite their fluency and factual accuracy.

**Method:** The paper categorizes methods into prompting strategies (Chain-of-Thought reasoning, Self-Consistency, Tree-of-Thought reasoning), architectural innovations (retrieval-augmented models, modular reasoning networks, neuro-symbolic integration), and learning paradigms (fine-tuning with reasoning datasets, reinforcement learning, self-supervised reasoning).

**Key Contributions:**

	1. Comprehensive categorization of reasoning-enhancing techniques for LLMs.
	2. Evaluation of current methodologies and their impact on LLM reasoning capabilities.
	3. Identification of open challenges and future research directions for reasoning-augmented LLMs.

**Result:** The survey synthesizes recent advancements and evaluates current methodologies enhancing LLM reasoning, identifying significant improvements in reasoning tasks through various approaches.

**Limitations:** The paper discusses limitations in current approaches, including generalization issues and the prevalence of hallucinations in LLMs.

**Conclusion:** The paper concludes by highlighting open challenges in LLM reasoning, such as hallucinations and the need for robust reasoning generalization, while suggesting promising directions for future research.

**Abstract:** Large Language Models (LLMs) have succeeded remarkably in various natural language processing (NLP) tasks, yet their reasoning capabilities remain a fundamental challenge. While LLMs exhibit impressive fluency and factual recall, their ability to perform complex reasoning-spanning logical deduction, mathematical problem-solving, commonsense inference, and multi-step reasoning-often falls short of human expectations. This survey provides a comprehensive review of emerging techniques enhancing reasoning in LLMs. We categorize existing methods into key approaches, including prompting strategies (e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought reasoning), architectural innovations (e.g., retrieval-augmented models, modular reasoning networks, and neuro-symbolic integration), and learning paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement learning, and self-supervised reasoning objectives). Additionally, we explore evaluation frameworks used to assess reasoning in LLMs and highlight open challenges, such as hallucinations, robustness, and reasoning generalization across diverse tasks. By synthesizing recent advancements, this survey aims to provide insights into promising directions for future research and practical applications of reasoning-augmented LLMs.

</details>


### [153] [Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring](https://arxiv.org/abs/2502.05242)

*Guanxu Chen, Dongrui Liu, Tao Luo, Lijie Hu, Jing Shao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Transparency, Monitoring, Trustworthiness, Optimal Transport

**Relevance Score:** 8

**TL;DR:** This paper presents TELLME, a method enhancing the transparency of large language models (LLMs) to help monitor their decision-making processes, focusing on trustworthiness tasks.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of LLMs' decision-making processes and enhance their monitoring for safety and sensitivity.

**Method:** The proposed TELLME method enhances the transparency of LLMs by providing a mechanism for monitoring their latent thinking processes, using techniques grounded in optimal transport theory.

**Key Contributions:**

	1. Introduction of TELLME for LLM transparency
	2. Application of TELLME in trustworthiness tasks
	3. Theoretical analysis using optimal transport theory

**Result:** TELLME demonstrates consistent improvement in both transparency and task performance on trustworthiness tasks such as safety risks monitoring and detoxification tasks.

**Limitations:** 

**Conclusion:** The implementation of TELLME not only improves LLMs' transparency but also their generalization abilities, making them more suitable for sensitive applications.

**Abstract:** Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easier to monitor. In this paper, we propose a novel method TELLME, improving the transparency of LLMs and helping monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase the applications of TELLME on trustworthiness tasks (\eg, safety risks monitoring tasks and detoxification tasks), where LLMs achieve consistent improvement in transparency and task performance. More crucially, we theoretically analyze the improvement of TELLME on LLMs' generalization ability through optimal transport theory.

</details>


### [154] [LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation](https://arxiv.org/abs/2502.07365)

*Zican Dong, Junyi Li, Jinhao Jiang, Mingyu Xu, Wayne Xin Zhao, Bingning Wang, Weipeng Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Continual Pre-training, Restoration Distillation

**Relevance Score:** 8

**TL;DR:** This paper proposes Long Context Pre-training with Restoration Distillation (LongReD) to enhance short-text performance of large language models (LLMs) while retaining their long-text capabilities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) experience performance degradation on short-text tasks when scaled for extended context windows, with underlying causes not fully explored.

**Method:** The proposed LongReD approach minimizes distribution discrepancies between extended and original models while distilling the hidden states from long texts and aligning outputs using skipped positional indices.

**Key Contributions:**

	1. Introduction of LongReD framework for LLM short-text performance restoration
	2. Use of distribution minimization techniques in continual pre-training
	3. Implementation of short-to-long distillation leveraging positional indices

**Result:** LongReD demonstrates effective preservation of short-text performance alongside competitive handling of long texts compared to baseline models.

**Limitations:** 

**Conclusion:** The proposed method successfully mitigates performance issues associated with short texts, evidenced by experiments on common benchmarks.

**Abstract:** Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common text benchmarks demonstrate that LongReD effectively preserves the model's short-text performance while maintaining comparable or even better capacity to handle long texts than baselines. Our code is available at https://github.com/RUCAIBox/LongReD.

</details>


### [155] [CoSER: Coordinating LLM-Based Persona Simulation of Established Roles](https://arxiv.org/abs/2502.09082)

*Xintao Wang, Heng Wang, Yifei Zhang, Xinfeng Yuan, Rui Xu, Jen-tse Huang, Siyu Yuan, Haoran Guo, Jiangjie Chen, Shuchang Zhou, Wei Wang, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** Role-playing language agents, large language models, dataset, evaluation protocol, character simulation

**Relevance Score:** 8

**TL;DR:** This paper introduces CoSER, a high-quality dataset and evaluation method for role-playing language agents (RPLAs) simulating established characters, demonstrating its implementation in advanced LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of simulating established characters in role-playing language agents due to a lack of authentic datasets and nuanced evaluation methods.

**Method:** Introduction of the CoSER dataset containing 17,966 characters with authentic dialogues and diverse data types, coupled with a novel given-circumstance acting methodology for training and evaluating RPLAs.

**Key Contributions:**

	1. Introduction of the CoSER dataset with a vast collection of character dialogues and experiences.
	2. Development of CoSER 8B and CoSER 70B LLMs based on LLaMA-3.1.
	3. Proposing a novel evaluation protocol that improves role-playing language agent performance.

**Result:** CoSER 70B showcases state-of-the-art performance, exceeding the capabilities of GPT-4o on several benchmarks related to character simulation.

**Limitations:** 

**Conclusion:** The CoSER dataset and methodology significantly enhance the training and evaluation processes for role-playing language models, showcasing their effectiveness in realistic character portrayal.

**Abstract:** Role-playing language agents (RPLAs) have emerged as promising applications of large language models (LLMs). However, simulating established characters presents a challenging task for RPLAs, due to the lack of authentic character datasets and nuanced evaluation methods using such data. In this paper, we present CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters. The CoSER dataset covers 17,966 characters from 771 renowned books. It provides authentic dialogues with real-world intricacies, as well as diverse data types such as conversation setups, character experiences and internal thoughts. Drawing from acting methodology, we introduce given-circumstance acting for training and evaluating role-playing LLMs, where LLMs sequentially portray multiple characters in book scenes. Using our dataset, we develop CoSER 8B and CoSER 70B, i.e., advanced open role-playing LLMs built on LLaMA-3.1 models. Extensive experiments demonstrate the value of the CoSER dataset for RPLA training, evaluation and retrieval. Moreover, CoSER 70B exhibits state-of-the-art performance surpassing or matching GPT-4o on our evaluation and three existing benchmarks, i.e., achieving 75.80% and 93.47% accuracy on the InCharacter and LifeChoice benchmarks respectively.

</details>


### [156] [ReLearn: Unlearning via Learning for Large Language Models](https://arxiv.org/abs/2502.11190)

*Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** unlearning, language models, data augmentation, fine-tuning, evaluation metrics

**Relevance Score:** 9

**TL;DR:** Proposes ReLearn, a novel unlearning framework for large language models that uses data augmentation and fine-tuning to maintain linguistic coherence while achieving targeted forgetting.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for unlearning in large language models negatively impact performance and coherence due to reliance on reverse optimization, highlighting the need for a better approach.

**Method:** Introduces a data augmentation and fine-tuning pipeline called ReLearn, along with an evaluation framework that includes new metrics for measuring knowledge retention and generation quality.

**Key Contributions:**

	1. Introduction of ReLearn for effective unlearning in language models
	2. Development of Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) metrics
	3. Demonstration of the negative impacts of reverse optimization on text coherence.

**Result:** ReLearn successfully accomplishes targeted forgetting while preserving high generation quality, as demonstrated in experimental results.

**Limitations:** 

**Conclusion:** ReLearn provides a more effective means of unlearning in language models without sacrificing linguistic coherence, with new evaluation metrics to better assess these attributes.

**Abstract:** Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.

</details>


### [157] [BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages](https://arxiv.org/abs/2502.11926)

*Shamsuddeen Hassan Muhammad, Nedjma Ousidhoum, Idris Abdulmumin, Jan Philip Wahle, Terry Ruas, Meriem Beloucif, Christine de Kock, Nirmal Surange, Daniela Teodorescu, Ibrahim Said Ahmad, David Ifeoluwa Adelani, Alham Fikri Aji, Felermino D. M. A. Ali, Ilseyar Alimova, Vladimir Araujo, Nikolay Babakov, Naomi Baes, Ana-Maria Bucur, Andiswa Bukula, Guanqun Cao, Rodrigo Tufino Cardenas, Rendi Chevi, Chiamaka Ijeoma Chukwuneke, Alexandra Ciobotaru, Daryna Dementieva, Murja Sani Gadanya, Robert Geislinger, Bela Gipp, Oumaima Hourrane, Oana Ignat, Falalu Ibrahim Lawan, Rooweither Mabuya, Rahmad Mahendra, Vukosi Marivate, Alexander Panchenko, Andrew Piper, Charles Henrique Porto Ferreira, Vitaly Protasov, Samuel Rutunda, Manish Shrivastava, Aura Cristina Udrea, Lilian Diana Awuor Wanzare, Sophie Wu, Florian Valentin Wunderlich, Hanif Muhammad Zhafran, Tianhui Zhang, Yi Zhou, Saif M. Mohammad*

**Main category:** cs.CL

**Keywords:** emotion recognition, low-resource languages, multilabel datasets, NLP, crosslingual

**Relevance Score:** 8

**TL;DR:** The paper introduces BRIGHTER, a collection of multilabeled, emotion-annotated datasets in 28 low-resource languages to enhance text-based emotion recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the disparities in emotion recognition research for under-resourced languages and improve applications within NLP that rely on emotion understanding.

**Method:** The authors created and annotated datasets in 28 languages, focusing on low-resource languages from various global regions, and conducted experiments on emotion identification and intensity recognition using these datasets.

**Key Contributions:**

	1. Creation of a multilabeled emotion-annotated dataset in 28 low-resource languages
	2. Experimental insights into monolingual and crosslingual emotion recognition
	3. Highlighting challenges in data collection and annotation for under-resourced languages

**Result:** Experimental results showed performance variability in emotion recognition tasks across languages and text domains, with findings both with and without the use of LLMs.

**Limitations:** The datasets might still face challenges in generalizability and quality across all 28 languages due to varying levels of fluent speaker involvement.

**Conclusion:** BRIGHTER datasets represent a significant advancement in bridging the gap in emotion recognition for low-resource languages and provide valuable resources for future research.

**Abstract:** People worldwide use language in subtle and complex ways to express emotions. Although emotion recognition--an umbrella term for several NLP tasks--impacts various applications within NLP and beyond, most work in this area has focused on high-resource languages. This has led to significant disparities in research efforts and proposed solutions, particularly for under-resourced languages, which often lack high-quality annotated datasets. In this paper, we present BRIGHTER--a collection of multilabeled, emotion-annotated datasets in 28 different languages and across several domains. BRIGHTER primarily covers low-resource languages from Africa, Asia, Eastern Europe, and Latin America, with instances labeled by fluent speakers. We highlight the challenges related to the data collection and annotation processes, and then report experimental results for monolingual and crosslingual multi-label emotion identification, as well as emotion intensity recognition. We analyse the variability in performance across languages and text domains, both with and without the use of LLMs, and show that the BRIGHTER datasets represent a meaningful step towards addressing the gap in text-based emotion recognition.

</details>


### [158] [ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails](https://arxiv.org/abs/2502.13458)

*Xiaofei Wen, Wenxuan Zhou, Wenjie Jacky Mo, Muhao Chen*

**Main category:** cs.CL

**Keywords:** large language models, safety, guardrail, critique-augmented, machine learning

**Relevance Score:** 8

**TL;DR:** ThinkGuard is a critique-augmented guardrail model for enhancing the safety of large language models (LLMs) by generating structured critiques alongside safety labels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are increasingly deployed in real-world applications, their safety becomes paramount. Current methods often rely on simplistic rule-based filtering or single-pass classification, limiting their effectiveness in addressing complex safety issues.

**Method:** The study introduces ThinkGuard, which is fine-tuned on critique-augmented data to generate structured critiques in conjunction with safety labels, significantly enhancing interpretability and caution in LLM safety evaluations.

**Key Contributions:**

	1. Introduction of ThinkGuard model for LLM safety enhancement
	2. Demonstration of critique-augmented data's effectiveness in improving outcomes
	3. Validation through superior performance metrics on safety benchmarks

**Result:** ThinkGuard outperforms existing models on multiple safety benchmarks, achieving the highest average F1 and AUPRC. It improves accuracy by 16.1% and macro F1 by 27.0% compared to LLaMA Guard 3, demonstrating that critique augmentation enhances classification precision and nuanced reasoning.

**Limitations:** 

**Conclusion:** The findings indicate that incorporating structured critiques within safety models not only improves performance in classifying safety violations but also maintains computational efficiency, meriting further exploration in LLM safety applications.

**Abstract:** Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.

</details>


### [159] [How Do LLMs Perform Two-Hop Reasoning in Context?](https://arxiv.org/abs/2502.13913)

*Tianyu Guo, Hanlin Zhu, Ruiqi Zhang, Jiantao Jiao, Song Mei, Michael I. Jordan, Stuart Russell*

**Main category:** cs.CL

**Keywords:** two-hop reasoning, large language models, attention mechanism, fine-tuning, synthetic dataset

**Relevance Score:** 8

**TL;DR:** The paper explores two-hop reasoning in large language models (LLMs), revealing their initial failure and subsequent improvement in performance after fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how LLMs handle two-hop reasoning problems and the impact of fine-tuning on their performance.

**Method:** The authors train a 3-layer Transformer from scratch on a synthetic two-hop reasoning task and analyze its attention dynamics during training.

**Key Contributions:**

	1. Demonstrated LLMs' initial failure at two-hop reasoning
	2. Showed improvement after fine-tuning
	3. Revealed structured attention dynamics during training

**Result:** LLMs struggle with two-hop reasoning under distractors initially, but after fine-tuning, they reach nearly perfect accuracy and show structured query mechanisms in their attention layers.

**Limitations:** The study focuses on synthetic datasets, which may not fully represent real-world reasoning tasks.

**Conclusion:** Fine-tuning significantly enhances LLMs' reasoning abilities, revealing a transition from random guessing to effective reasoning through observed attention patterns.

**Abstract:** ``Socrates is human. All humans are mortal. Therefore, Socrates is mortal.'' This form of argument illustrates a typical pattern of two-hop reasoning. Formally, two-hop reasoning refers to the process of inferring a conclusion by making two logical steps, each connecting adjacent concepts, such that the final conclusion depends on the integration of both steps. It is one of the most fundamental components of human reasoning and plays a crucial role in both formal logic and everyday decision-making. Despite recent progress in large language models (LLMs), we surprisingly find that they can fail at solving simple two-hop reasoning problems when distractors are present. We observe on a synthetic dataset that pre-trained LLMs often resort to random guessing among all plausible conclusions. However, after few steps of fine-tuning, models achieve near-perfect accuracy and exhibit strong length generalization. To understand the underlying mechanisms, we train a 3-layer Transformer from scratch on a synthetic two-hop reasoning task and reverse-engineer its internal information flow. We observe a clear progression in the attention logits throughout training. This pictures a sharp phase transition from an initial stage of random guessing to the emergence of a structured sequential query mechanism, where the model first retrieves the preceding and the bridge concepts in the early layers and then uses them to infer the final answer. Finally, we show that these dynamics can be captured by a minimal three-parameter attention-only network.

</details>


### [160] [Mitigating Lost-in-Retrieval Problems in Retrieval Augmented Multi-Hop Question Answering](https://arxiv.org/abs/2502.14245)

*Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, Wei Hu*

**Main category:** cs.CL

**Keywords:** multi-hop question answering, retrieval-augmented generation, ChainRAG, large language models, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper addresses the "lost-in-retrieval" problem in multi-hop question answering by introducing ChainRAG, a method that sequentially retrieves and rewrites sub-questions to enhance answer accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper identifies the "lost-in-retrieval" issue in multi-hop QA, where LLMs fail to decompose sub-questions effectively, leading to poor retrieval performance and incorrect answers.

**Method:** ChainRAG employs a progressive retrieval and rewriting method that completes missing key entities while retrieving relevant sentences from a sentence graph, ensuring each step builds on the previous one for accuracy.

**Key Contributions:**

	1. Introduction of the "lost-in-retrieval" problem in multi-hop QA
	2. Development of the ChainRAG method for progressive retrieval and rewriting
	3. Demonstrated improvement over baseline methods on multiple QA datasets

**Result:** ChainRAG is evaluated on three multi-hop QA datasets (MuSiQue, 2Wiki, and HotpotQA) and consistently outperforms baseline methods in terms of both effectiveness and efficiency.

**Limitations:** 

**Conclusion:** The proposed ChainRAG method provides a robust solution to enhance retrieval accuracy in multi-hop question answering by effectively managing sub-question decomposition.

**Abstract:** In this paper, we identify a critical problem, "lost-in-retrieval", in retrieval-augmented multi-hop question answering (QA): the key entities are missed in LLMs' sub-question decomposition. "Lost-in-retrieval" significantly degrades the retrieval performance, which disrupts the reasoning chain and leads to the incorrect answers. To resolve this problem, we propose a progressive retrieval and rewriting method, namely ChainRAG, which sequentially handles each sub-question by completing missing key entities and retrieving relevant sentences from a sentence graph for answer generation. Each step in our retrieval and rewriting process builds upon the previous one, creating a seamless chain that leads to accurate retrieval and answers. Finally, all retrieved sentences and sub-question answers are integrated to generate a comprehensive answer to the original question. We evaluate ChainRAG on three multi-hop QA datasets - MuSiQue, 2Wiki, and HotpotQA - using three large language models: GPT4o-mini, Qwen2.5-72B, and GLM-4-Plus. Empirical results demonstrate that ChainRAG consistently outperforms baselines in both effectiveness and efficiency.

</details>


### [161] [Self-Taught Agentic Long Context Understanding](https://arxiv.org/abs/2502.15920)

*Yufan Zhuang, Xiaodong Yu, Jialian Wu, Ximeng Sun, Ze Wang, Jiang Liu, Yusheng Su, Jingbo Shang, Zicheng Liu, Emad Barsoum*

**Main category:** cs.CL

**Keywords:** Agentic Long-Context Understanding, Chain-of-Clarifications, Multi-hop reasoning

**Relevance Score:** 9

**TL;DR:** AgenticLU improves long-context question answering for LLMs by integrating self-clarification with context retrieval in a structured workflow, achieving high recall and performance across multiple tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of answering complex, long-context questions using large language models (LLMs), which requires effective clarifications and context retrieval.

**Method:** The study introduces Agentic Long-Context Understanding (AgenticLU), which uses Chain-of-Clarifications (CoC) to refine understanding with self-generated questions and context groundings, employing a tree search for inference.

**Key Contributions:**

	1. Integration of self-clarification into long-context understanding
	2. Development of Chain-of-Clarifications for improved model reasoning
	3. Two-stage model finetuning to enhance context retrieval efficiency

**Result:** Achieved 97.8% answer recall on NarrativeQA with a branching factor of eight, demonstrating the effectiveness of the two-stage model finetuning and the CoC workflow.

**Limitations:** 

**Conclusion:** AgenticLU models significantly outperform existing methods in long-context processing, revealing strong multi-hop reasoning capabilities and consistent performance with increasing context length.

**Abstract:** Answering complex, long-context questions remains a major challenge for large language models (LLMs) as it requires effective question clarifications and context retrieval. We propose Agentic Long-Context Understanding (AgenticLU), a framework designed to enhance an LLM's understanding of such queries by integrating targeted self-clarification with contextual grounding within an agentic workflow. At the core of AgenticLU is Chain-of-Clarifications (CoC), where models refine their understanding through self-generated clarification questions and corresponding contextual groundings. By scaling inference as a tree search where each node represents a CoC step, we achieve 97.8% answer recall on NarrativeQA with a search depth of up to three and a branching factor of eight. To amortize the high cost of this search process to training, we leverage the preference pairs for each step obtained by the CoC workflow and perform two-stage model finetuning: (1) supervised finetuning to learn effective decomposition strategies, and (2) direct preference optimization to enhance reasoning quality. This enables AgenticLU models to generate clarifications and retrieve relevant context effectively and efficiently in a single inference pass. Extensive experiments across seven long-context tasks demonstrate that AgenticLU significantly outperforms state-of-the-art prompting methods and specialized long-context LLMs, achieving robust multi-hop reasoning while sustaining consistent performance as context length grows.

</details>


### [162] [Personalized Causal Graph Reasoning for LLMs: A Case Study on Dietary Recommendations](https://arxiv.org/abs/2503.00134)

*Zhongqi Yang, Amir Rahmani*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personalized reasoning, Causal graphs, Dietary recommendations, Glucose management

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for improving personalized reasoning in LLMs by using personal causal graphs, evaluated through dietary recommendations for glucose management.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in personalized reasoning with multifactor personal data for context-aware decision-making.

**Method:** The paper introduces Personalized Causal Graph Reasoning which incorporates individual causal graphs into LLM reasoning, focusing on dietary recommendations as a case study.

**Key Contributions:**

	1. Introduction of Personalized Causal Graph Reasoning framework for LLMs
	2. Application in nutrient-oriented dietary recommendations
	3. Counterfactual evaluation methodology for assessing food recommendations

**Result:** The proposed framework demonstrates efficient personalized dietary recommendations that reduce average glucose iAUC, outperforming previous methods.

**Limitations:** 

**Conclusion:** The method enhances personalization in LLM reasoning, particularly in health-related applications like diet management.

**Abstract:** Large Language Models (LLMs) effectively leverage common-sense knowledge for general reasoning, yet they struggle with personalized reasoning when tasked with interpreting multifactor personal data. This limitation restricts their applicability in domains that require context-aware decision-making tailored to individuals. This paper introduces Personalized Causal Graph Reasoning as an agentic framework that enhances LLM reasoning by incorporating personal causal graphs derived from data of individuals. These graphs provide a foundation that guides the LLM's reasoning process. We evaluate it on a case study on nutrient-oriented dietary recommendations, which requires personal reasoning due to the implicit unique dietary effects. We propose a counterfactual evaluation to estimate the efficiency of LLM-recommended foods for glucose management. Results demonstrate that the proposed method efficiently provides personalized dietary recommendations to reduce average glucose iAUC across three time windows, which outperforms the previous approach. LLM-as-a-judge evaluation results indicate that our proposed method enhances personalization in the reasoning process.

</details>


### [163] [Shaping Shared Languages: Human and Large Language Models' Inductive Biases in Emergent Communication](https://arxiv.org/abs/2503.04395)

*Tom Kouwenhoven, Max Peeperkorn, Roy de Kleijn, Tessa Verhoef*

**Main category:** cs.CL

**Keywords:** artificial languages, inductive biases, human-LLM communication, referential games, LLM training

**Relevance Score:** 9

**TL;DR:** The paper explores how artificial languages evolve when optimized for human and LLM inductive biases in various collaborative settings, revealing that referentially grounded vocabularies emerge to facilitate effective communication.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the evolution of artificial languages shaped by the inductive biases of human users and LLMs, and to understand the resultant communicative dynamics between these entities.

**Method:** The authors used a classical referential game to conduct experiments between humans, LLMs, and hybrid interactions to study how languages evolve under different conditions.

**Key Contributions:**

	1. Demonstrated the evolution of vocabularies shaped by inductive biases in different interaction types.
	2. Revealed the interaction effects that lead to more human-like vocabularies in collaborative settings with LLMs.
	3. Proposed new training methods for LLMs that incorporate human interactions to enhance communicative success.

**Result:** The study found that effective vocabularies emerged that support communication across all interaction types, with noteworthy differences in languages optimized for humans compared to those for LLMs. Collaborations resulted in more human-like vocabularies.

**Limitations:** 

**Conclusion:** The findings enhance the understanding of language dynamics between humans and LLMs and suggest the importance of including human interactions in LLM training methods to improve alignment and communicative success.

**Abstract:** Languages are shaped by the inductive biases of their users. Using a classical referential game, we investigate how artificial languages evolve when optimised for inductive biases in humans and large language models (LLMs) via Human-Human, LLM-LLM and Human-LLM experiments. We show that referentially grounded vocabularies emerge that enable reliable communication in all conditions, even when humans \textit{and} LLMs collaborate. Comparisons between conditions reveal that languages optimised for LLMs subtly differ from those optimised for humans. Interestingly, interactions between humans and LLMs alleviate these differences and result in vocabularies more human-like than LLM-like. These findings advance our understanding of the role inductive biases in LLMs play in the dynamic nature of human language and contribute to maintaining alignment in human and machine communication. In particular, our work underscores the need to think of new LLM training methods that include human interaction and shows that using communicative success as a reward signal can be a fruitful, novel direction.

</details>


### [164] [Odysseus Navigates the Sirens' Song: Dynamic Focus Decoding for Factual and Diverse Open-Ended Text Generation](https://arxiv.org/abs/2503.08057)

*Wen Luo, Feifan Song, Wei Li, Guangyue Peng, Shaohang Wei, Houfeng Wang*

**Main category:** cs.CL

**Keywords:** Dynamic Focus Decoding, Large Language Models, Text Generation, Factual Accuracy, Diversity

**Relevance Score:** 9

**TL;DR:** Dynamic Focus Decoding (DFD) improves text generation by balancing factual accuracy and diversity in Large Language Models (LLMs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge that current decoding methods for LLMs struggle with balancing factual accuracy and diversity in generated text.

**Method:** DFD is a novel stochastic approach that adaptively adjusts the decoding focus based on the distributional differences across layers of LLMs, without needing extra data or models.

**Key Contributions:**

	1. Introduces Dynamic Focus Decoding (DFD) as a novel method for balancing factual accuracy and diversity in text generation.
	2. Demonstrates substantial performance improvements across multiple datasets using DFD.
	3. Offers an easily integrable approach with low computational overhead for existing decoding methods.

**Result:** Experimental results across seven datasets show that DFD significantly enhances performance in generating text that is both factually accurate and diverse.

**Limitations:** 

**Conclusion:** DFD provides a scalable and efficient solution for open-ended text generation by integrating easily with existing methods and yielding minimal computational overhead.

**Abstract:** Large Language Models (LLMs) are increasingly required to generate text that is both factually accurate and diverse across various open-ended applications. However, current stochastic decoding methods struggle to balance such objectives. We introduce Dynamic Focus Decoding (DFD), a novel plug-and-play stochastic approach that resolves this trade-off without requiring additional data, knowledge, or models. DFD adaptively adjusts the decoding focus based on distributional differences across layers, leveraging the modular and hierarchical nature of factual knowledge within LLMs. This dynamic adjustment improves factuality in knowledge-intensive decoding steps and promotes diversity in less knowledge-reliant steps. DFD can be easily integrated with existing decoding methods, enhancing both factuality and diversity with minimal computational overhead. Extensive experiments across seven datasets demonstrate that DFD significantly improves performance, providing a scalable and efficient solution for open-ended text generation.

</details>


### [165] [Explicit Learning and the LLM in Machine Translation](https://arxiv.org/abs/2503.09454)

*Malik Marmonier, Rachel Bawden, Benoît Sagot*

**Main category:** cs.CL

**Keywords:** LLMs, explicit learning, linguistic phenomena, fine-tuning, low-resource languages

**Relevance Score:** 9

**TL;DR:** This study investigates how LLMs can learn new languages through explicit learning from grammar book explanations, showing measurable capacities but limitations with complexity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the explicit learning capabilities of LLMs in understanding new languages, particularly low-resource languages.

**Method:** Controlled translation experiments were conducted using constructed languages derived from English, Latin, and French, assessing LLMs' performance in learning linguistic structures.

**Key Contributions:**

	1. Demonstrated LLM explicit learning capability through grammar explanations.
	2. Identified limitations related to linguistic complexity and generalization.
	3. Proposed the need for diverse training methods.

**Result:** LLMs demonstrated a measurable ability for explicit learning, which diminishes with the complexity of linguistic features; fine-tuning improves performance but faces generalization challenges.

**Limitations:** The ability to learn diminishes with increased complexity; fine-tuning struggles to generalize to novel linguistic features.

**Conclusion:** Diverse training sets and new fine-tuning strategies are essential for enhancing explicit learning in LLMs, especially for low-resource languages.

**Abstract:** This study explores an LLM's ability to learn new languages using explanations found in a grammar book$\unicode{x2014}$a process we term "explicit learning." To rigorously assess this ability, we design controlled translation experiments between English and constructed languages generated$\unicode{x2014}$by specific cryptographic means$\unicode{x2014}$out of Latin or French. Contrary to previous studies, our results demonstrate that LLMs do possess a measurable capacity for explicit learning. This ability, however, diminishes as the complexity of the linguistic phenomena to be learned increases. Supervised fine-tuning on ad hoc chains of thought significantly enhances LLM performance but struggles to generalize to typologically novel or more complex linguistic features. These findings point to the need for more diverse training sets and alternative fine-tuning strategies to further improve explicit learning by LLMs, benefiting low-resource languages typically described in grammar books but lacking extensive corpora.

</details>


### [166] [Constrained Discrete Diffusion](https://arxiv.org/abs/2503.09790)

*Michael Cardei, Jacob K Christopher, Thomas Hartvigsen, Brian R. Bartoldson, Bhavya Kailkhura, Ferdinando Fioretto*

**Main category:** cs.CL

**Keywords:** Constrained Discrete Diffusion, generative models, sequence generation, constraint optimization, toxic text generation

**Relevance Score:** 7

**TL;DR:** The paper introduces Constrained Discrete Diffusion (CDD), a generative model that allows enforcing sequence-level constraints during the diffusion process to ensure generated outputs adhere to specified rules or safety requirements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current autoregressive models in enforcing sequence-level constraints during text and other sequence generation.

**Method:** The paper presents Constrained Discrete Diffusion (CDD), which integrates differentiable constraint optimization into the discrete diffusion sampling process, allowing direct enforcement of constraints without model retraining.

**Key Contributions:**

	1. Introduction of Constrained Discrete Diffusion (CDD) for enforcing sequence-level constraints during generation.
	2. A method that integrates differentiable constraint optimization within the diffusion process.
	3. Demonstrated effectiveness in various applications with zero constraint violations.

**Result:** CDD achieves zero constraint violations in various tasks, such as toxicity-controlled text generation and property-constrained molecule design, while maintaining the quality of generated outputs.

**Limitations:** 

**Conclusion:** CDD provides an effective and training-free solution for controllable generation in comparison to traditional methods that rely on post-hoc filtering or retraining.

**Abstract:** Discrete diffusion models are a class of generative models that construct sequences by progressively denoising samples from a categorical noise distribution. Beyond their rapidly growing ability to generate coherent natural language, these models present a new and important opportunity to enforce sequence-level constraints, a capability that current autoregressive models cannot natively provide. This paper capitalizes on this opportunity by introducing Constrained Discrete Diffusion (CDD), a novel integration of differentiable constraint optimization within the diffusion process to ensure adherence to constraints, logic rules, or safety requirements for generated sequences. Unlike conventional text generators that often rely on post-hoc filtering or model retraining for controllable generation, CDD directly imposes constraints into the discrete diffusion sampling process, resulting in a training-free and effective approach. Experiments in toxicity-controlled text generation, property-constrained molecule design, and instruction-constrained text completion demonstrate that CDD achieves zero constraint violations in a diverse array of tasks while preserving fluency, novelty, and coherence while outperforming autoregressive and existing discrete diffusion approaches.

</details>


### [167] [Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond](https://arxiv.org/abs/2503.10460)

*Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, Xiangzheng Zhang*

**Main category:** cs.CL

**Keywords:** Long reasoning models, Curriculum training, State-of-the-art performance

**Relevance Score:** 8

**TL;DR:** Light-R1 is an open-source suite designed for training long reasoning models using a reproducible methodology with public data, achieving state-of-the-art performance in math reasoning while enabling cross-domain generalization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create an accessible and cost-effective approach for training sophisticated reasoning models using exclusively public data, addressing limitations of proprietary data methodologies in existing models.

**Method:** Light-R1 uses a curriculum training approach that progressively increases the difficulty of data and includes multi-staged post-training, enabling effective training of models like Light-R1-32B and Light-R1-14B-DS.

**Key Contributions:**

	1. Introduction of an open-source training suite for long reasoning models.
	2. Demonstration of the effectiveness of curriculum training on diverse datasets.
	3. Achievement of state-of-the-art performance in math reasoning with cross-domain generalization.

**Result:** Experiments show Light-R1 models outperform existing proprietary models, achieving state-of-the-art performance in math reasoning with the Light-R1-14B-DS model surpassing many larger models.

**Limitations:** 

**Conclusion:** Light-R1 advances the field of long reasoning models by making sophisticated training methodologies available to the research community, with all models and data made publicly accessible.

**Abstract:** This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.   Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.   Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.

</details>


### [168] [TLUE: A Tibetan Language Understanding Evaluation Benchmark](https://arxiv.org/abs/2503.12051)

*Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Hao Wang Xiao Feng, Yongbin Yu*

**Main category:** cs.CL

**Keywords:** Tibetan, Language Models, Benchmark, Low-Resource Languages, Natural Language Processing

**Relevance Score:** 4

**TL;DR:** TLUE is the first benchmark for assessing large language models' capabilities in Tibetan, revealing significant performance gaps.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of low-resource languages like Tibetan in large language models (LLMs).

**Method:** We present TLUE, a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, along with a safety benchmark covering 7 subdomains.

**Key Contributions:**

	1. Introduction of the TLUE benchmark for Tibetan languages.
	2. Evaluation of state-of-the-art LLMs on Tibetan tasks.
	3. Highlighting the performance gap in LLMs for low-resource languages.

**Result:** Experimental results show that most LLMs perform below the random baseline, indicating challenges in processing Tibetan.

**Limitations:** Focus limited to Tibetan; may not generalize to other low-resource languages.

**Conclusion:** TLUE establishes a foundation for future research in Tibetan language understanding and calls for inclusivity in LLM development.

**Abstract:** Large language models (LLMs) have made tremendous progress in recent years, but low-resource languages, such as Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of LLMs. To address this gap, we present TLUE (A Tibetan Language Understanding Evaluation Benchmark), the first large-scale benchmark for assessing LLMs' capabilities in Tibetan. TLUE comprises two major components: (1) a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate a diverse set of state-of-the-art LLMs. Experimental results demonstrate that most LLMs perform below the random baseline, highlighting the considerable challenges LLMs face in processing Tibetan, a low-resource language. TLUE provides an essential foundation for driving future research and progress in Tibetan language understanding and underscores the need for greater inclusivity in LLM development.

</details>


### [169] [Experience Retrieval-Augmentation with Electronic Health Records Enables Accurate Discharge QA](https://arxiv.org/abs/2503.17933)

*Justice Ou, Tinglin Huang, Yilun Zhao, Ziyang Yu, Peiqing Lu, Rex Ying*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Electronic Health Records, Clinical Reasoning, DischargeQA

**Relevance Score:** 9

**TL;DR:** This paper presents the ExpRAG framework for improving LLM reliability in clinical applications by utilizing case-based knowledge from EHRs for medical reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable LLMs in clinical settings, specifically for incorporating clinical case-based knowledge that reflects real-world patient experiences.

**Method:** ExpRAG employs a coarse-to-fine retrieval process that uses an EHR-based report ranker to find similar patients followed by an experience retriever to extract relevant task-related content.

**Key Contributions:**

	1. Introduction of the ExpRAG framework leveraging EHRs for retrieval-augmented generation in medical contexts.
	2. Development of the DischargeQA dataset, which includes realistic clinical questions derived from EHR data.
	3. Empirical validation showing improved performance of ExpRAG over existing text-based methods.

**Result:** ExpRAG demonstrates a 5.2% average relative improvement in performance over traditional text-based ranking methods in clinical question answering tasks.

**Limitations:** 

**Conclusion:** The study underscores the significance of incorporating case-based knowledge into LLMs for enhanced medical reasoning and treatment responses in clinical applications.

**Abstract:** To improve the reliability of Large Language Models (LLMs) in clinical applications, retrieval-augmented generation (RAG) is extensively applied to provide factual medical knowledge. However, beyond general medical knowledge from open-ended datasets, clinical case-based knowledge is also critical for effective medical reasoning, as it provides context grounded in real-world patient experiences.Motivated by this, we propose Experience Retrieval-Augmentation ExpRAG framework based on Electronic Health Record(EHR), aiming to offer the relevant context from other patients' discharge reports. ExpRAG performs retrieval through a coarse-to-fine process, utilizing an EHR-based report ranker to efficiently identify similar patients, followed by an experience retriever to extract task-relevant content for enhanced medical reasoning.To evaluate ExpRAG, we introduce DischargeQA, a clinical QA dataset with 1,280 discharge-related questions across diagnosis, medication, and instruction tasks. Each problem is generated using EHR data to ensure realistic and challenging scenarios. Experimental results demonstrate that ExpRAG consistently outperforms a text-based ranker, achieving an average relative improvement of 5.2%, highlighting the importance of case-based knowledge for medical reasoning.

</details>


### [170] [Sun-Shine: A Foundation Large Language Model for Tibetan Culture and Heritage](https://arxiv.org/abs/2503.18288)

*Cheng Huang, Fan Gao, Yutong Liu, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu*

**Main category:** cs.CL

**Keywords:** Tibetan language, Large Language Models, TIB-STC dataset, Cultural applications, NLP

**Relevance Score:** 4

**TL;DR:** Introduction of Llama-Sunshine, the first large language model tailored for Tibetan culture and language processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of adequate language models for the Tibetan language, which has a complex grammatical structure and is a minority language, leading to data scarcity and under-explored cultural potential.

**Method:** Development of Llama-Sunshine (Sun-Shine), a large language model optimized for Tibetan linguistic features, along with the creation of the TIB-STC dataset consisting of diverse Tibetan texts.

**Key Contributions:**

	1. First large language model for Tibetan culture (Sun-Shine)
	2. Creation of the TIB-STC dataset for Tibetan texts
	3. Exhibits strong performance in low-resource language scenarios

**Result:** Sun-Shine demonstrates superior knowledge expertise in Tibetan culture and begins to showcase embodied intelligence in various language processing tasks, excelling in low-resource conditions.

**Limitations:** 

**Conclusion:** The development of Llama-Sunshine and the TIB-STC dataset represents a significant advancement for Tibetan language processing and enhances the potential for AI applications in Tibetan culture.

**Abstract:** Tibetan, a minority language in China, features a highly intricate grammatical structure, characterized by four verb tenses and a tense system with frequent irregularities, contributing to its extensive inflectional diversity. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in many domains. Despite the success in other fields, current LLMs often fall short in catering to the needs of domain experts like Tibetans, and the potential of LLMs for Tibetan culture is under-explored. The intrinsic reasons are the immense and intricate nature of Tibetan culture as well as the necessity for higher granularity and richness in knowledge. Simultaneously, the complexity and uniqueness of its grammatical structure, coupled with its status as a minority ethnic language, contribute to data scarcity, which remains a fundamental challenge. To alleviate these issues, we introduce Llama-Sunshine (Sun-Shine), the first large language model for Tibetan culture, which is expert in various Tibetan language processing tasks. Sun-Shine incorporates state-of-the-art model architectures optimized for Tibetan's linguistic features. We also propose TIB-STC, a comprehensive dataset comprising diverse Tibetan texts such as literature, religious scripts, news, and conversational data, which is also the first large-scale dataset for Tibetan culture. Though comprehensive experiments, Sun-Shine not only demonstrates a higher level of knowledge expertise for Tibetan culture but also gains preliminary embodied intelligence capabilities in Tibetan language processing tasks, like language modeling, text classification, machine translation, and syntactic analysis. Moreover, it excels in low-resource scenarios, showcasing strong generalization capabilities.

</details>


### [171] [Token embeddings violate the manifold hypothesis](https://arxiv.org/abs/2504.01002)

*Michael Robinson, Sourya Dey, Tony Chiang*

**Main category:** cs.CL

**Keywords:** large language models, token embeddings, fiber bundle hypothesis, smooth structure, statistical test

**Relevance Score:** 9

**TL;DR:** This paper examines the input token space of large language models (LLMs) using a novel statistical test to understand irregularities in token embeddings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the behavior of LLMs is contingent upon accurate assumptions about their input token space, as incorrect assumptions can lead to flawed conclusions about the model.

**Method:** The paper introduces a statistical test based on the fiber bundle hypothesis that examines the smooth structure of token neighborhoods. It empirically tests this hypothesis across multiple open-source LLMs.

**Key Contributions:**

	1. Development of the fiber bundle hypothesis for analyzing token subspace structures.
	2. Empirical validation across several open-source LLMs, revealing frequent rejection of the null hypothesis.
	3. Insights into the stability of LLM responses based on token irregularities.

**Result:** The tests frequently reject the null hypothesis, indicating that the token subspace does not behave as a smooth fiber bundle and may suggest instability in responses from LLMs with semantically similar prompts.

**Limitations:** 

**Conclusion:** The findings imply that irregularities in the token subspace can lead to less stable responses in LLMs when specific tokens are involved in semantically equivalent prompts.

**Abstract:** A full understanding of the behavior of a large language model (LLM) requires our understanding of its input token space. If this space differs from our assumptions, our understanding of and conclusions about the LLM will likely be flawed. We elucidate the structure of the token embeddings both empirically and theoretically. We present a novel statistical test assuming that the neighborhood around each token has a relatively flat and smooth structure as the null hypothesis. Failing to reject the null is uninformative, but rejecting it at a specific token $\psi$ implies an irregularity in the token subspace in a $\psi$-neighborhood, $B(\psi)$. The structure assumed in the null is a generalization of a manifold with boundary called a \emph{smooth fiber bundle} (which can be split into two spatial regimes -- small and large radius), so we denote our new hypothesis test as the ``fiber bundle hypothesis.'' Failure to reject the null hypothesis is uninformative, but rejecting it at $\psi$ indicates a statistically significant irregularity at $B(\psi)$. By running our test over several open-source LLMs, each with unique token embeddings, we find that the null is frequently rejected, and so the evidence suggests that the token subspace is not a fiber bundle and hence also not a manifold. As a consequence of our findings, when an LLM is presented with two semantically equivalent prompts, if one prompt contains a token implicated by our test, the response to that prompt will likely exhibit less stability than the other.

</details>


### [172] [Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices](https://arxiv.org/abs/2504.03312)

*Luís Couto Seller, Íñigo Sanz Torres, Adrián Vogel-Fernández, Carlos González Carballo, Pedro Miguel Sánchez Sánchez, Adrián Carruana Martín, Enrique de Miguel Ambite*

**Main category:** cs.CL

**Keywords:** large language models, Iberian languages, NLP tasks, model performance, compact LLMs

**Relevance Score:** 7

**TL;DR:** This work evaluates compact state-of-the-art large language models for NLP tasks focused on under-resourced Iberian languages, revealing performance gaps and the need for further research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the accessibility challenges of deploying large language models on consumer-grade devices, especially for under-resourced languages.

**Method:** Comprehensive evaluation of several compact LLMs across various essential NLP tasks specifically tailored for Iberian languages.

**Key Contributions:**

	1. Evaluation of LLMs for Iberian languages
	2. Identification of performance gaps in NLP tasks
	3. Insights into compact model deployment for limited-resource languages

**Result:** Certain models show consistent excellence in specific tasks, but performance gaps exist, particularly with languages like Basque.

**Limitations:** 

**Conclusion:** There is a need for further research to balance model compactness with performance across multilingual tasks, especially for under-resourced languages.

**Abstract:** Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance

</details>


### [173] [SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement](https://arxiv.org/abs/2504.03561)

*Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Machine Learning, Monte Carlo Tree Search, Action Knowledge, LLM Agents

**Relevance Score:** 8

**TL;DR:** SynWorld is a framework that enables LLM-based agents to enhance their action capabilities in novel environments through scenario synthesis and Monte Carlo Tree Search.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Agents need to adapt their action capabilities in novel environments and unconventional action spaces to improve their performance and understanding.

**Method:** The SynWorld framework allows agents to synthesize scenarios while invoking multi-step actions and uses Monte Carlo Tree Search for exploration and action knowledge refinement.

**Key Contributions:**

	1. Introduction of SynWorld framework for LLM-based agents
	2. Utilization of scenario synthesis and MCTS for action exploration
	3. Demonstration of effective learning in novel environments

**Result:** Experiments show that SynWorld effectively helps agents learn action knowledge in new environments.

**Limitations:** 

**Conclusion:** SynWorld provides a general method for improving agents' learning capabilities in varied action contexts, demonstrating its efficacy in diverse settings.

**Abstract:** In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.

</details>


### [174] [AI for Climate Finance: Agentic Retrieval and Multi-Step Reasoning for Early Warning System Investments](https://arxiv.org/abs/2504.05104)

*Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, Chiara Colesanti Senni, Markus Leippold*

**Main category:** cs.CL

**Keywords:** AI, Climate Finance, LLM, Investment Tracking, Early Warning Systems

**Relevance Score:** 5

**TL;DR:** The paper presents an LLM-based AI system for tracking financial investments in climate adaptation, focusing on Early Warning Systems, and demonstrates its effectiveness in classifying investment data from MDBs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for standardized financial reporting in climate adaptation investments, particularly for Early Warning Systems where current methods lack accuracy and transparency.

**Method:** Developed an LLM-based agentic AI system using contextual retrieval, fine-tuning, and multi-step reasoning; evaluated various AI classification methods on MDB project documents.

**Key Contributions:**

	1. Introduction of an LLM-based agentic AI system for financial data classification
	2. Demonstrated superior performance of the RAG approach
	3. Provided a benchmark dataset and expert-annotated corpus for research

**Result:** The agent-based RAG approach achieved significant classification performance with 87% accuracy, 89% precision, and 83% recall.

**Limitations:** 

**Conclusion:** The study contributes to AI-driven financial tracking in climate finance by presenting an effective classification method and a benchmark dataset for future use.

**Abstract:** Tracking financial investments in climate adaptation is a complex and expertise-intensive task, particularly for Early Warning Systems (EWS), which lack standardized financial reporting across multilateral development banks (MDBs) and funds. To address this challenge, we introduce an LLM-based agentic AI system that integrates contextual retrieval, fine-tuning, and multi-step reasoning to extract relevant financial data, classify investments, and ensure compliance with funding guidelines. Our study focuses on a real-world application: tracking EWS investments in the Climate Risk and Early Warning Systems (CREWS) Fund. We analyze 25 MDB project documents and evaluate multiple AI-driven classification methods, including zero-shot and few-shot learning, fine-tuned transformer-based classifiers, chain-of-thought (CoT) prompting, and an agent-based retrieval-augmented generation (RAG) approach. Our results show that the agent-based RAG approach significantly outperforms other methods, achieving 87\% accuracy, 89\% precision, and 83\% recall. Additionally, we contribute a benchmark dataset and expert-annotated corpus, providing a valuable resource for future research in AI-driven financial tracking and climate finance transparency.

</details>


### [175] [Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations](https://arxiv.org/abs/2504.06792)

*Zican Dong, Han Peng, Peiyu Liu, Wayne Xin Zhao, Dong Wu, Feng Xiao, Zhifeng Wang*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, deep learning, memory efficiency, pruning framework, domain specialization

**Relevance Score:** 7

**TL;DR:** This study investigates the memory issues in large-scale Mixture-of-Experts (MoE) models and proposes a pruning framework, EASY-EP, to improve efficiency by retaining only the most relevant experts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large-scale MoE models face significant memory overhead due to the storage of all experts, which limits their deployment in practice.

**Method:** The paper introduces EASY-EP, a pruning framework that uses a few domain-specific demonstrations to identify and retain the most relevant experts, based on output-aware importance assessment and token contribution estimation.

**Key Contributions:**

	1. Proposed the EASY-EP framework for expert pruning in MoE models.
	2. Demonstrated few-shot expert localization behavior in MoE models.
	3. Showed significant performance improvements in throughput with reduced expert activation.

**Result:** EASY-EP can achieve comparable performance and 2.99 times throughput under the same memory budget as the full model with half the number of experts activated.

**Limitations:** 

**Conclusion:** The findings indicate that with few in-domain demonstrations, MoE models effectively localize the important experts needed for specific tasks, enhancing efficiency without sacrificing performance.

**Abstract:** Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1(671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few in-domain demonstrations, the model consistently activates a sparse and stable subset of experts on tasks within the same domain. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and L2 norm of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities before and after routed experts. Experiments on DeepSeek-R1 and DeepSeek-V3-0324 show that our method can achieve comparable performances and $2.99\times$ throughput under the same memory budget with full model with only half the experts.

</details>


### [176] [Layers at Similar Depths Generate Similar Activations Across LLM Architectures](https://arxiv.org/abs/2504.08775)

*Christopher Wolfram, Aaron Schein*

**Main category:** cs.CL

**Keywords:** LLM, activation geometries, nearest neighbor relationships, layer analysis, model comparison

**Relevance Score:** 8

**TL;DR:** This study investigates the nearest neighbor relationships in the latent spaces of 24 independently-trained LLMs across different layers, revealing variability within models and shared patterns across different architectures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the latent spaces of independently trained LLMs relate, particularly focusing on the nearest neighbor relationships at different layers.

**Method:** The study analyzes the activation patterns and nearest neighbor relations of 24 open-weight LLMs, examining variability within layers of a model and similarities between layers of different models.

**Key Contributions:**

	1. Analysis of nearest neighbor relationships in LLM activations
	2. Demonstration of shared activation geometries across different models
	3. Insight into how different architectures influence activation relationships

**Result:** The findings indicate that nearest neighbor relationships vary by layer within each model but reveal a commonality across corresponding layers of different models, suggesting a shared progression of activation geometries.

**Limitations:** 

**Conclusion:** The activation geometries transition from layer to layer within each LLM, but the overall progression is largely uniform across different LLM architectures, indicating a structured similarity despite architectural differences.

**Abstract:** How do the latent spaces used by independently-trained LLMs relate to one another? We study the nearest neighbor relationships induced by activations at different layers of 24 open-weight LLMs, and find that they 1) tend to vary from layer to layer within a model, and 2) are approximately shared between corresponding layers of different models. Claim 2 shows that these nearest neighbor relationships are not arbitrary, as they are shared across models, but Claim 1 shows that they are not "obvious" either, as there is no single set of nearest neighbor relationships that is universally shared. Together, these suggest that LLMs generate a progression of activation geometries from layer to layer, but that this entire progression is largely shared between models, stretched and squeezed to fit into different architectures.

</details>


### [177] [GOAT-TTS: Expressive and Realistic Speech Generation via A Dual-Branch LLM](https://arxiv.org/abs/2504.12339)

*Yaodong Song, Hongjie Chen, Jie Lian, Yuxin Zhang, Guangmin Xia, Zehan Li, Genliang Zhao, Jian Kang, Jie Li, Yongxiang Li, Xuelong Li*

**Main category:** cs.CL

**Keywords:** text-to-speech, large language models, speech synthesis, real-time generation, acoustic embeddings

**Relevance Score:** 8

**TL;DR:** This paper presents GOAT-TTS, a novel LLM-based approach for text-to-speech synthesis that addresses issues of acoustic quality and dependency on aligned speech-text pairs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current TTS models face limitations due to acoustic characteristic loss, dependence on aligned data, and catastrophic forgetting in LLMs during optimization.

**Method:** GOAT-TTS employs a dual-branch architecture with a modality-alignment branch for capturing acoustic embeddings and a speech-generation branch for fine-tuning LLMs for speech generation.

**Key Contributions:**

	1. Introduction of a dual-branch architecture for TTS synthesis.
	2. Modality-alignment branch that enables bidirectional feature correlation without transcript dependency.
	3. Real-time multi-token prediction for speech generation.

**Result:** Experimental results show that GOAT-TTS achieves performance on par with state-of-the-art TTS models and effectively utilizes synthesized dialect speech data.

**Limitations:** 

**Conclusion:** The proposed framework not only preserves the linguistic knowledge of LLMs but also enhances TTS synthesis quality, enabling real-time streaming capabilities.

**Abstract:** While large language models (LLMs) have revolutionized text-to-speech (TTS) synthesis through discrete tokenization paradigms, current architectures exhibit fundamental tensions between three critical dimensions: 1) irreversible loss of acoustic characteristics caused by quantization of speech prompts; 2) stringent dependence on precisely aligned prompt speech-text pairs that limit real-world deployment; and 3) catastrophic forgetting of the LLM's native text comprehension during optimization for speech token generation. To address these challenges, we propose an LLM-based text-to-speech Generation approach Optimized via a novel dual-branch ArchiTecture (GOAT-TTS). Our framework introduces two key innovations: (1) The modality-alignment branch combines a speech encoder and projector to capture continuous acoustic embeddings, enabling bidirectional correlation between paralinguistic features (language, timbre, emotion) and semantic text representations without transcript dependency; (2) The speech-generation branch employs modular fine-tuning on top-k layers of an LLM for speech token prediction while freezing the bottom-n layers to preserve foundational linguistic knowledge. Moreover, multi-token prediction is introduced to support real-time streaming TTS synthesis. Experimental results demonstrate that our GOAT-TTS achieves performance comparable to state-of-the-art TTS models while validating the efficacy of synthesized dialect speech data.

</details>
