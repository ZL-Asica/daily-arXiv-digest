# 2025-08-15

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 19]

- [cs.CL](#cs.CL) [Total: 84]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Pre-trained Transformer-models using chronic invasive electrophysiology for symptom decoding without patient-individual training](https://arxiv.org/abs/2508.10160)

*Timon Merk, Saeed Salehi, Richard M. Koehler, Qiming Cui, Maria Olaru, Amelia Hahn, Nicole R. Provenza, Simon Little, Reza Abbasi-Asl, Phil A. Starr, Wolf-Julian Neumann*

**Main category:** cs.HC

**Keywords:** neural decoding, Parkinson's disease, deep brain stimulation, foundation model, neuromodulation

**Relevance Score:** 8

**TL;DR:** The paper presents a foundation model for decoding Parkinson's disease symptoms using chronic deep brain stimulation recordings and a novel loss function.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enable personalized neuromodulation therapy by accurately decoding pathological and physiological states from neural data.

**Method:** A foundation model was trained on 24 days of chronic deep brain stimulation data, using an optimized loss function that addresses frequency bias in masked auto-encoder techniques.

**Key Contributions:**

	1. Development of a foundation model for neural data
	2. Introduction of an optimized pre-training loss function for neural electrophysiological signals
	3. Successful decoding of Parkinson's disease symptoms without patient-specific training.

**Result:** Achieved successful decoding of Parkinson's disease symptoms with leave-one-subject-out cross-validation, demonstrating generalized state estimation without individualized patient training.

**Limitations:** 

**Conclusion:** The study indicates the feasibility of using large-scale foundation models for symptom estimation in neuromodulation therapy, potentially enhancing personalized treatment.

**Abstract:** Neural decoding of pathological and physiological states can enable patient-individualized closed-loop neuromodulation therapy. Recent advances in pre-trained large-scale foundation models offer the potential for generalized state estimation without patient-individual training. Here we present a foundation model trained on chronic longitudinal deep brain stimulation recordings spanning over 24 days. Adhering to long time-scale symptom fluctuations, we highlight the extended context window of 30 minutes. We present an optimized pre-training loss function for neural electrophysiological data that corrects for the frequency bias of common masked auto-encoder loss functions due to the 1-over-f power law. We show in a downstream task the decoding of Parkinson's disease symptoms with leave-one-subject-out cross-validation without patient-individual training.

</details>


### [2] [Training Spatial Ability in Virtual Reality](https://arxiv.org/abs/2508.10195)

*Yiannos Demetriou, Manasvi Parikh, Sara Eskandari, Westley Weimer, Madeline Endres*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Spatial Reasoning, STEM Education, Teaching Methods, Cybersickness

**Relevance Score:** 5

**TL;DR:** A study evaluates the effectiveness of a virtual reality (VR) course for teaching spatial reasoning compared to traditional methods, finding significant gains in spatial abilities with no significant difference in outcomes between the two approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Spatial reasoning is crucial for success in STEM, yet under-represented groups face challenges in this area. There is a need to explore effective teaching methods, such as VR, which may enhance learning outcomes.

**Method:** The study involved adapting a pencil-and-paper curriculum into a VR format, comprising three modules over a three-week period. An evaluation with 24 undergraduate STEM students included pre- and post-test assessments for quantitative analysis and a questionnaire for qualitative insights.

**Key Contributions:**

	1. Development of a fully-structured VR spatial skills course
	2. Demonstrated efficiency of VR in teaching spatial reasoning
	3. Provided insights into students' learning experiences in VR

**Result:** Students who participated in the VR course showed significant improvements in spatial ability. There was no notable difference in results compared to a baseline pencil-and-paper course, indicating VR could be a viable teaching method.

**Limitations:** Limited sample size of 24 participants; further research needed to generalize findings across more diverse groups.

**Conclusion:** The research suggests that spatial reasoning can be effectively taught in a more time-efficient manner through VR, with lower rates of cybersickness reported compared to prior studies, and positive experiences reported by students.

**Abstract:** Background: Spatial reasoning has been identified as a critical skill for success in STEM. Unfortunately, under-represented groups often have lower incoming spatial ability. Courses that improve spatial skills exist but are not widely used. Virtual reality (VR) has been suggested as a possible tool for teaching spatial reasoning since students are more accurate and complete spatial tasks more quickly in three dimensions. However, no prior work has developed or evaluated a fully-structured VR spatial skills course. Objectives: We seek to assess the effectiveness of teaching spatial reasoning in VR, both in isolation as a structured training curriculum and also in comparison to traditional methods. Methods: We adapted three modules of an existing pencil-and-paper course to VR, leveraging educational scaffolding and real-time feedback in the design. We evaluated our three-week course in a study with $n=24$ undergraduate introductory STEM students, capturing both quantitative spatial ability gains (using pre- and post test scores on validated assessments) and qualitative insights (from a post-study questionnaire). We also compared our VR course to an offering of a baseline non-VR course (using data collected in a previous study). Results and Conclusions: Students who took our VR course had significant spatial ability gains. Critically, we find no significant difference in outcomes between our VR course (3 meetings of 120 minutes each) and a baseline pencil and paper course (10 meetings of 90 minutes each), suggesting that spatial reasoning can be very efficiently taught in VR. We observed cybersickness at lower rates than are generally reported and most students reported enjoying learning in VR.

</details>


### [3] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)

*Yifan Song, Wing Yee Au, Hon Yung Wong, Brian P. Bailey, Tal August*

**Main category:** cs.HC

**Keywords:** jargon management, interdisciplinary communication, LLM, user engagement, personalized support

**Relevance Score:** 8

**TL;DR:** The study explores jargon barriers in interdisciplinary communication and introduces ParseJargon, an LLM-powered system for real-time jargon management.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Interdisciplinary communication is often hindered by jargon, affecting comprehension and engagement in workplace meetings.

**Method:** A diary study with 16 professionals to identify jargon barriers, followed by the design and testing of ParseJargon in controlled environments and a field study.

**Key Contributions:**

	1. Development of ParseJargon, an interactive LLM-powered jargon support tool
	2. Demonstration of the effectiveness of personalized jargon management in real-time settings
	3. Insights into the limitations of current jargon-management strategies

**Result:** Personalized jargon support through ParseJargon significantly improved comprehension, engagement, and appreciation in meetings, while general-purpose support was detrimental.

**Limitations:** The study highlights limitations in real-world deployment and the varying effectiveness of jargon support based on individual backgrounds.

**Conclusion:** The findings suggest the potential of personalized jargon support tools for enhancing communication in interdisciplinary settings and educational contexts.

**Abstract:** Effective interdisciplinary communication is frequently hindered by domain-specific jargon. To explore the jargon barriers in-depth, we conducted a formative diary study with 16 professionals, revealing critical limitations in current jargon-management strategies during workplace meetings. Based on these insights, we designed ParseJargon, an interactive LLM-powered system providing real-time personalized jargon identification and explanations tailored to users' individual backgrounds. A controlled experiment comparing ParseJargon against baseline (no support) and general-purpose (non-personalized) conditions demonstrated that personalized jargon support significantly enhanced participants' comprehension, engagement, and appreciation of colleagues' work, whereas general-purpose support negatively affected engagement. A follow-up field study validated ParseJargon's usability and practical value in real-time meetings, highlighting both opportunities and limitations for real-world deployment. Our findings contribute insights into designing personalized jargon support tools, with implications for broader interdisciplinary and educational applications.

</details>


### [4] [Facilitating Longitudinal Interaction Studies of AI Systems](https://arxiv.org/abs/2508.10252)

*Tao Long, Sitong Wang, Émilie Fabre, Tony Wang, Anup Sathya, Jason Wu, Savvas Petridis, Dingzeyu Li, Tuhin Chakrabarty, Yue Jiang, Jingyi Li, Tiffany Tseng, Ken Nakagaki, Qian Yang, Nikolas Martelaro, Jeffrey V. Nickerson, Lydia B. Chilton*

**Main category:** cs.HC

**Keywords:** longitudinal studies, user interaction, AI, UIST, research community

**Relevance Score:** 7

**TL;DR:** A workshop focused on overcoming challenges in conducting longitudinal studies in AI user interaction research.

**Read time:** 0 min

<details>
  <summary>Details</summary>

**Motivation:** To address the evolving nature of user interactions with AI and the inadequacy of one-time evaluations, prompting a need for longitudinal studies.

**Method:** Includes keynote speeches, panel discussions, and interactive breakout sessions aimed at designing protocols and tools for longitudinal research.

**Key Contributions:**

	1. Practical strategies for conducting longitudinal studies
	2. Community building around longitudinal research
	3. Hands-on experience in protocol design and tool prototyping

**Result:** A community is fostered around longitudinal system research, providing participants with practical strategies for conducting studies in this area.

**Limitations:** 

**Conclusion:** By the end of the workshop, researchers will be better equipped to implement longitudinal studies in their work, contributing to the development and evaluation of UIST tools.

**Abstract:** UIST researchers develop tools to address user challenges. However, user interactions with AI evolve over time through learning, adaptation, and repurposing, making one time evaluations insufficient. Capturing these dynamics requires longer-term studies, but challenges in deployment, evaluation design, and data collection have made such longitudinal research difficult to implement. Our workshop aims to tackle these challenges and prepare researchers with practical strategies for longitudinal studies. The workshop includes a keynote, panel discussions, and interactive breakout groups for discussion and hands-on protocol design and tool prototyping sessions. We seek to foster a community around longitudinal system research and promote it as a more embraced method for designing, building, and evaluating UIST tools.

</details>


### [5] [Artificial Emotion: A Survey of Theories and Debates on Realising Emotion in Artificial Intelligence](https://arxiv.org/abs/2508.10286)

*Yupei Li, Qiyang Sun, Michelle Schlicher, Yee Wen Lim, Björn W. Schuller*

**Main category:** cs.HC

**Keywords:** Affective Computing, Artificial Intelligence, Artificial Emotion, Machine Learning, Ethical implications

**Relevance Score:** 8

**TL;DR:** The paper discusses the development of Artificial Emotion (AE) in AI systems as a means to enhance emotional understanding and capabilities, exploring its implications for Artificial General Intelligence (AGI).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether AI can benefit from developing internal emotion-like states, and the implications of such developments for AGI.

**Method:** The paper reviews current manifestations of AE in ML systems, emotion-modulated architectures, and mechanisms for integrating AE into AI.

**Key Contributions:**

	1. Conceptual framework for understanding Artificial Emotion in AI
	2. Review of existing AI systems exhibiting AE-like behaviors
	3. Discussion of ethical and safety considerations in emotionally-capable AI

**Result:** The authors find early signs of AE-like behaviors in AI but note the lack of a clear framework for implementing emotions in AI.

**Limitations:** 

**Conclusion:** The development of AE in AI could yield significant advantages, but also poses ethical implications and safety risks that need to be addressed.

**Abstract:** Affective Computing (AC) has enabled Artificial Intelligence (AI) systems to recognise, interpret, and respond to human emotions - a capability also known as Artificial Emotional Intelligence (AEI). It is increasingly seen as an important component of Artificial General Intelligence (AGI). We discuss whether in order to peruse this goal, AI benefits from moving beyond emotion recognition and synthesis to develop internal emotion-like states, which we term as Artificial Emotion (AE). This shift potentially allows AI to benefit from the paradigm of `inner emotions' in ways we - as humans - do. Although recent research shows early signs that AI systems may exhibit AE-like behaviours, a clear framework for how emotions can be realised in AI remains underexplored. In this paper, we discuss potential advantages of AE in AI, review current manifestations of AE in machine learning systems, examine emotion-modulated architectures, and summarise mechanisms for modelling and integrating AE into future AI. We also explore the ethical implications and safety risks associated with `emotional' AGI, while concluding with our opinion on how AE could be beneficial in the future.

</details>


### [6] [Beyond Self-Regulated Learning Processes: Unveiling Hidden Tactics in Generative AI-Assisted Writing](https://arxiv.org/abs/2508.10310)

*Kaixun Yang, Yizhou Fan, Luzhen Tang, Mladen Raković, Xinyu Li, Dragan Gašević, Guanliang Chen*

**Main category:** cs.HC

**Keywords:** Generative AI, Self-Regulated Learning, Learning Analytics, Hidden Markov Models, Educational Technology

**Relevance Score:** 7

**TL;DR:** This paper examines self-regulated learning (SRL) in the context of Generative AI in education, using Hidden Markov Models to analyze learner behavior.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding SRL during interactions with GenAI tools is crucial for enhancing educational strategies and support.

**Method:** Analyzed trace data from students using Hidden Markov Models to identify SRL strategies during GenAI-assisted writing.

**Key Contributions:**

	1. Conceptualization of SRL as a layered system
	2. Identification of distinct learner groups based on SRL strategies
	3. Insights into performance differences associated with SRL strategies in GenAI writing

**Result:** Identified three distinct groups of learners with different SRL strategies, showing significant performance differences.

**Limitations:** 

**Conclusion:** The study provides insights into SRL modeling and informs the creation of adaptive learning technologies for GenAI-enhanced education.

**Abstract:** The integration of Generative AI (GenAI) into education is reshaping how students learn, making self-regulated learning (SRL) - the ability to plan, monitor, and adapt one's learning - more important than ever. To support learners in these new contexts, it is essential to understand how SRL unfolds during interaction with GenAI tools. Learning analytics offers powerful techniques for analyzing digital trace data to infer SRL behaviors. However, existing approaches often assume SRL processes are linear, segmented, and non-overlapping-assumptions that overlook the dynamic, recursive, and non-linear nature of real-world learning. We address this by conceptualizing SRL as a layered system: observable learning patterns reflect hidden tactics (short, purposeful action states), which combine into broader SRL strategies. Using Hidden Markov Models (HMMs), we analyzed trace data from higher education students engaged in GenAI-assisted academic writing. We identified three distinct groups of learners, each characterized by different SRL strategies. These groups showed significant differences in performance, indicating that students' use of different SRL strategies in GenAI-assisted writing led to varying task outcomes. Our findings advance the methodological toolkit for modeling SRL and inform the design of adaptive learning technologies that more effectively support learners in GenAI-enhanced educational environments.

</details>


### [7] [Mental Effort Estimation in Motion Exploration and Concept Generation Design Tasks using Inter-Band Relative Power Difference of EEG](https://arxiv.org/abs/2508.10353)

*G. Kalyan Ramana, Sumit Yempalle, Prasad S. Onkar*

**Main category:** cs.HC

**Keywords:** conceptual design, EEG, neurocognition, mental effort, motion exploration

**Relevance Score:** 5

**TL;DR:** This study explores the cognitive complexity of conceptual design in engineering through the use of EEG to measure mental effort during design tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the cognitive phenomena in conceptual design is crucial, especially for products with moving components, as static sketches may not effectively convey intended motion.

**Method:** The study introduces a novel EEG metric called inter-Band Relative Power Difference (inter-BRPD) to quantify mental effort, and involves a design experiment with 32 participants performing control and focus tasks while EEG data is recorded and analyzed.

**Key Contributions:**

	1. Introduction of the inter-BRPD metric for measuring mental effort.
	2. Insights into the cognitive processes involved in conceptual design tasks.
	3. Statistical validation of the inter-BRPD metric against established cognitive metrics.

**Result:** Results indicate that inter-BRPD effectively captures mental effort with fewer parameters than traditional metrics, and its reliability is statistically validated.

**Limitations:** 

**Conclusion:** The findings suggest the potential for new tools to support conceptual design that includes motion exploration, improving design processes.

**Abstract:** Conceptual design is a cognitively complex task, especially in the engineering design of products having relative motion between components. Designers prefer sketching as a medium for conceptual design and use gestures and annotations to represent such relative motion. Literature suggests that static representations of motion in sketches may not achieve the intended functionality when realised, because it primarily depends on the designers' mental capabilities for motion simulation. Thus, it is important to understand the cognitive phenomena when designers are exploring concepts of articulated products. The current work is an attempt to understand design neurocognition by categorising the tasks and measuring the mental effort involved in these tasks using EEG. The analysis is intended to validate design intervention tools to support the conceptual design involving motion exploration. A novel EEG-based metric, inter-Band Relative Power Difference (inter-BRPD), is introduced to quantify mental effort. A design experiment is conducted with 32 participants, where they have to perform one control task and 2 focus tasks corresponding to the motion exploration task (MET) and the concept generation task (CGT), respectively. EEG data is recorded during the 3 tasks, cleaned, processed and analysed using the MNE library in Python. It is observed from the results that inter-BRPD captures the essence of mental effort with half the number of conventionally used parameters. The reliability and efficacy of the inter-BRPD metric are also statistically validated against literature-based cognitive metrics. With these new insights, the study opens up possibilities for creating support for conceptual design and its evaluation.

</details>


### [8] ["Here Comes the Makeup Tutorial You Asked For!": Exploring Communication Strategies and Viewer Engagement in Beauty Videos on Rednote](https://arxiv.org/abs/2508.10364)

*Xueer Lin, Chenyu Li, Yuhan Lyu, Zhicong Lu, Zhenhui Peng*

**Main category:** cs.HC

**Keywords:** beauty videos, communication strategies, viewer engagement, social media, content analysis

**Relevance Score:** 2

**TL;DR:** This study analyzes communication strategies in beauty videos and their impact on viewer engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how communication strategies in beauty videos affect viewer engagement and knowledge dissemination.

**Method:** The study coded 352 beauty videos and created a taxonomy of communication strategies, followed by a classification of comments to gauge viewer engagement.

**Key Contributions:**

	1. Developed a taxonomy of communication strategies in beauty videos.
	2. Identified six categories of viewer engagement comments.
	3. Demonstrated the impact of strategic video elements on viewer interaction.

**Result:** Regression analyses indicate that specific strategies, like calls to action, significantly influence viewer engagement and comment dynamics.

**Limitations:** 

**Conclusion:** Understanding these strategies can enhance the creation of beauty content and improve knowledge sharing in the beauty community.

**Abstract:** More and more people, especially females, create and view beauty videos covering topics like makeup tutorials and vlogs on social media platforms. Understanding the communication strategies that creators use in these videos and how they affect viewers' engagement can help spread beauty knowledge. By coding 352 beauty videos in Rednote, this study presents a comprehensive taxonomy of communication strategies used by the creators, such as using home as the video background and displaying makeup effects when starting the narrative at the beginning. We further label and computationally classify six categories of comments that reveal viewers' engagement with beauty videos. The regression analyses reveal the effects of beauty video communication strategies on viewers' engagement; for example, calling viewers to take action at the end tends to attract more comments that debate the product's efficacy. We discuss insights into fostering the creation of beauty videos and the communication of beauty knowledge.

</details>


### [9] [MCP2OSC: Parametric Control by Natural Language](https://arxiv.org/abs/2508.10414)

*Yuan-Yi Fan*

**Main category:** cs.HC

**Keywords:** MCP, OCR, prompt design, human-machine collaboration, LLM

**Relevance Score:** 7

**TL;DR:** The paper presents MCP2OSC, a framework integrating language prompts with OSC control for enhanced human-machine collaboration in multimedia tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap between precision offered by slider controls and the intuitiveness of text prompts in content creation.

**Method:** Introduces the MCP server with a set of prompt design criteria for exploring OSC control through natural language.

**Key Contributions:**

	1. Introduction of MCP2OSC server for OSC control via natural language prompts
	2. Demonstration of practical applications through 14 QA examples
	3. A novel approach to integrating LLMs in developing and debugging OSC messages

**Result:** Demonstrated effectiveness of MCP2OSC with 14 examples in generating, interpreting, and managing OSC messages using natural language prompts.

**Limitations:** 

**Conclusion:** MCP2OSC can revolutionize multimedia control by enabling intuitive, precise language interfaces with LLM assistance.

**Abstract:** Text prompts enable intuitive content creation but may fall short in achieving high precision for intricate tasks; knob or slider controls offer precise adjustments at the cost of increased complexity. To address the gap between knobs and prompts, a new MCP (Model Context Protocol) server and a unique set of prompt design criteria are presented to enable exploring parametric OSC (OpenSoundControl) control by natural language prompts. Demonstrated by 14 practical QA examples with best practices and the generalized prompt templates, this study finds Claude integrated with the MCP2OSC server effective in generating OSC messages by natural language, interpreting, searching, and visualizing OSC messages, validating and debugging OSC messages, and managing OSC address patterns. MCP2OSC enhances human-machine collaboration by leveraging LLM (Large Language Model) to handle intricate OSC development tasks, and by empowering human creativity with an intuitive language interface featuring flexible precision controls: a prompt-based OSC tool. This study provides a novel perspective on the creative MCP application at the network protocol level by utilizing LLM's strength in directly processing and generating human-readable OSC messages. The results suggest its potential for a LLM-based universal control mechanism for multimedia devices.

</details>


### [10] [Stress Detection from Multimodal Wearable Sensor Data](https://arxiv.org/abs/2508.10468)

*Paul Schreiber, Beyza Cinar, Lennart Mackert, Maria Maleshkova*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, stress recognition, wearable devices, affective computing, dataset

**Relevance Score:** 9

**TL;DR:** This paper introduces a multi-modal dataset for automated stress recognition systems, addressing the limitations of current publicly available datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve human-computer interactions by monitoring and recognizing emotional stress to prevent health issues caused by stress exposure.

**Method:** The authors collect physiological and motion signals from wearable devices and classify them into four affective states, while also gathering psychological self-assessments.

**Key Contributions:**

	1. A novel multi-modal, publicly available dataset for automated stress recognition
	2. Benchmark performance for stress detection with high classification accuracy
	3. Standardization framework for stress experiments and data collection

**Result:** The dataset enables detailed analysis with a benchmark showing 89% accuracy in binary classification and 82% in multi-class classification for stress detection.

**Limitations:** 

**Conclusion:** This work provides a significant resource for the development of automated stress recognition systems and contributes to the standardization of data collection protocols in HCI research.

**Abstract:** Human-Computer Interaction (HCI) is a multi-modal, interdisciplinary field focused on designing, studying, and improving the interactions between people and computer systems. This involves the design of systems that can recognize, interpret, and respond to human emotions or stress. Developing systems to monitor and react to stressful events can help prevent severe health implications caused by long-term stress exposure. Currently, the publicly available datasets and standardized protocols for data collection in this domain are limited. Therefore, we introduce a multi-modal dataset intended for wearable affective computing research, specifically the development of automated stress recognition systems. We systematically review the publicly available datasets recorded in controlled laboratory settings. Based on a proposed framework for the standardization of stress experiments and data collection, we collect physiological and motion signals from wearable devices (e.g., electrodermal activity, photoplethysmography, three-axis accelerometer). During the experimental protocol, we differentiate between the following four affective/activity states: neutral, physical, cognitive stress, and socio-evaluative stress. These different phases are meticulously labeled, allowing for detailed analysis and reconstruction of each experiment. Meta-data such as body positions, locations, and rest phases are included as further annotations. In addition, we collect psychological self-assessments after each stressor to evaluate subjects' affective states. The contributions of this paper are twofold: 1) a novel multi-modal, publicly available dataset for automated stress recognition, and 2) a benchmark for stress detection with 89\% in a binary classification (baseline vs. stress) and 82\% in a multi-class classification (baseline vs. stress vs. physical exercise).

</details>


### [11] [Reproducible Physiological Features in Affective Computing: A Preliminary Analysis on Arousal Modeling](https://arxiv.org/abs/2508.10561)

*Andrea Gargano, Jasin Machkour, Mimma Nardelli, Enzo Pasquale Scilingo, Michael Muma*

**Main category:** cs.HC

**Keywords:** Affective Computing, physiological markers, reproducibility, human-robot interaction, emotion recognition

**Relevance Score:** 8

**TL;DR:** This study explores the link between emotional experiences and physiological markers, focusing on reproducibility in feature selection for Affective Computing.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address reproducibility issues in linking emotional experiences with physiological markers within Affective Computing.

**Method:** Analyzed cardiovascular and electrodermal signals from 30 participants using the Continuously Annotated Signal of Emotion dataset and the Terminating-Random Experiments (T-Rex) method for feature selection.

**Key Contributions:**

	1. Reproducibility in physiological features selection in Affective Computing.
	2. Identification of significant electrodermal features linked to arousal.
	3. Implementation of the T-Rex method for systematic feature selection.

**Result:** Identified two electrodermal-derived features that showed reproducible and statistically significant associations with arousal, achieving a 100% confirmation rate.

**Limitations:** 

**Conclusion:** The findings underscore the importance of robust reproducibility assessments in physiological feature selection, relevant for applications in safety-critical environments such as mental disorder recognition and human-robot interaction.

**Abstract:** In Affective Computing, a key challenge lies in reliably linking subjective emotional experiences with objective physiological markers. This preliminary study addresses the issue of reproducibility by identifying physiological features from cardiovascular and electrodermal signals that are associated with continuous self-reports of arousal levels. Using the Continuously Annotated Signal of Emotion dataset, we analyzed 164 features extracted from cardiac and electrodermal signals of 30 participants exposed to short emotion-evoking videos. Feature selection was performed using the Terminating-Random Experiments (T-Rex) method, which performs variable selection systematically controlling a user-defined target False Discovery Rate. Remarkably, among all candidate features, only two electrodermal-derived features exhibited reproducible and statistically significant associations with arousal, achieving a 100\% confirmation rate. These results highlight the necessity of rigorous reproducibility assessments in physiological features selection, an aspect often overlooked in Affective Computing. Our approach is particularly promising for applications in safety-critical environments requiring trustworthy and reliable white box models, such as mental disorder recognition and human-robot interaction systems.

</details>


### [12] [Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions](https://arxiv.org/abs/2508.10586)

*Birgit Nierula, Mustafa Tevfik Lafci, Anna Melnik, Mert Akgül, Farelle Toumaleu Siewe, Sebastian Bosse*

**Main category:** cs.HC

**Keywords:** Proxemics, Virtual Reality, Physiological Responses, Facial Expressions, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** This study investigates how personal space violations and facial expressions affect physiological and subjective responses in virtual reality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the impact of personal space and facial expressions in virtual environments is crucial for enhancing human-computer interactions, particularly in VR applications.

**Method:** Participants experienced personal space manipulation (intrusion vs. respect) and facial expressions (neutral vs. angry) in a VR setting, while physiological responses were recorded through skin conductance response, heart rate variability, and discomfort ratings.

**Key Contributions:**

	1. Demonstration of phase-specific physiological responses to personal space violations in VR.
	2. Identification of the distinct roles of SCR and HRV in capturing proxemic interactions.
	3. Insights for designing more realistic avatar interactions in virtual environments.

**Result:** SCR responses were higher during the standing phase compared to the approach phase when personal space was violated, and angry expressions decreased HRV and increased discomfort ratings without amplifying SCR.

**Limitations:** 

**Conclusion:** Different physiological measures provide unique insights into proxemic responses, highlighting the need for multi-modal assessments in VR interactions.

**Abstract:** Proxemics, the study of spatial behavior, is fundamental to social interaction and increasingly relevant for virtual reality (VR) applications. While previous research has established that users respond to personal space violations in VR similarly as in real-world settings, phase-specific physiological responses and the modulating effects of facial expressions remain understudied. We investigated physiological and subjective responses to personal space violations by virtual avatars, to understand how threatening facial expressions and interaction phases (approach vs. standing) influence these responses. Sixteen participants experienced a 2x2 factorial design manipulating Personal Space (intrusion vs. respect) and Facial Expression (neutral vs. angry) while we recorded skin conductance response (SCR), heart rate variability (HRV), and discomfort ratings. Personal space boundaries were individually calibrated using a stop-distance procedure. Results show that SCR responses are significantly higher during the standing phase compared to the approach phase when personal space was violated, indicating that prolonged proximity within personal space boundaries is more physiologically arousing than the approach itself. Angry facial expressions significantly reduced HRV, reflecting decreased parasympathetic activity, and increased discomfort ratings, but did not amplify SCR responses. These findings demonstrate that different physiological modalities capture distinct aspects of proxemic responses: SCR primarily reflects spatial boundary violations, while HRV responds to facial threat cues. Our results provide insights for developing comprehensive multi-modal assessments of social behavior in virtual environments and inform the design of more realistic avatar interactions.

</details>


### [13] [DEV: A Driver-Environment-Vehicle Closed-Loop Framework for Risk-Aware Adaptive Automation of Driving](https://arxiv.org/abs/2508.10618)

*Anaïs Halin, Christel Devue, Marc Van Droogenbroeck*

**Main category:** cs.HC

**Keywords:** automation, driving safety, risk management, human-computer interaction, adaptive systems

**Relevance Score:** 5

**TL;DR:** The paper introduces the DEV framework for risk-aware adaptive driving automation, addressing issues like driver disengagement and situation awareness by dynamically adjusting automation levels based on real-time risk assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance vehicle safety and comfort amidst the growing integration of automation, while addressing risks such as driver disengagement and mode confusion.

**Method:** The DEV framework is a closed-loop system that continuously adjusts the operational level of automation based on a risk management strategy, incorporating real-time risk assessments.

**Key Contributions:**

	1. Introduction of the DEV framework for adaptive driving automation
	2. Real-time risk assessment for dynamic adjustment of automation levels
	3. Nomenclature of indexes to measure driver involvement, environment complexity, and vehicle engagement

**Result:** The framework facilitates smoother transitions and promotes effective cooperation between drivers and automation systems by quantifying the interplay between driver involvement, environment complexity, and vehicle engagement.

**Limitations:** 

**Conclusion:** The DEV framework provides a comprehensive perspective to unify multidisciplinary research and guide the development of dynamic, risk-aware driving automation systems.

**Abstract:** The increasing integration of automation in vehicles aims to enhance both safety and comfort, but it also introduces new risks, including driver disengagement, reduced situation awareness, and mode confusion. In this work, we propose the DEV framework, a closed-loop framework for risk-aware adaptive driving automation that captures the dynamic interplay between the driver, the environment, and the vehicle. The framework promotes to continuously adjusting the operational level of automation based on a risk management strategy. The real-time risk assessment supports smoother transitions and effective cooperation between the driver and the automation system. Furthermore, we introduce a nomenclature of indexes corresponding to each core component, namely driver involvement, environment complexity, and vehicle engagement, and discuss how their interaction influences driving risk. The DEV framework offers a comprehensive perspective to align multidisciplinary research efforts and guide the development of dynamic, risk-aware driving automation systems.

</details>


### [14] [Are Electrodermal Activity-Based Indicators of Driver Cognitive Distraction Robust to Varying Traffic Conditions and Adaptive Cruise Control Use?](https://arxiv.org/abs/2508.10620)

*Anaïs Halin, Marc Van Droogenbroeck, Christel Devue*

**Main category:** cs.HC

**Keywords:** electrodermal activity, cognitive distraction, adaptive cruise control, driving environment, mental workload

**Relevance Score:** 6

**TL;DR:** This study examines how electrodermal activity (EDA) serves as an indicator of driver cognitive distraction under different traffic conditions and with adaptive cruise control (ACC) usage.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the relationship between cognitive distraction in drivers and its physiological indicators under varying driving conditions and with automation.

**Method:** Participants drove in six scenarios that varied in cognitive distraction (with/without a mental task) and driving environment complexity, while EDA metrics were monitored.

**Key Contributions:**

	1. Introduced EDA as a measure for cognitive distraction in driving contexts.
	2. Demonstrated how driving environment complexity affects EDA responses.
	3. Provided insights into the interaction between cognitive tasks, automation, and physiological responses.

**Result:** All EDA indicators showed significant changes due to cognitive distraction and ACC use; environment complexity affected SCL and SCR amplitude but not SCR rate.

**Limitations:** Limited to driving scenarios in a simulator; real-world validation needed.

**Conclusion:** EDA can effectively measure drivers' mental workload variations due to cognitive distraction, driving environment, and automation usage.

**Abstract:** In this simulator study, we investigate whether and how electrodermal activity (EDA) reflects driver cognitive distraction under varying traffic conditions and adaptive cruise control (ACC) use. Participants drove in six scenarios, combining two levels of cognitive distraction (presence/absence of a mental calculation task) and three levels of driving environment complexity (different traffic conditions). Throughout the experiment, they were free to activate or deactivate ACC (ACC use, two levels). We analyzed three EDA-based indicators of cognitive distraction: SCL (mean skin conductance level), SCR amplitude (mean amplitude of skin conductance responses), and SCR rate (rate of skin conductance responses). Results indicate that all three indicators were significantly influenced by cognitive distraction and ACC use, while environment complexity influenced SCL and SCR amplitude, but not SCR rate. These findings suggest that EDA-based indicators reflect variations in drivers' mental workload due not only to cognitive distraction, but also to driving environment and automation use.

</details>


### [15] [Gaze-Based Indicators of Driver Cognitive Distraction: Effects of Different Traffic Conditions and Adaptive Cruise Control Use](https://arxiv.org/abs/2508.10624)

*Anaïs Halin, Adrien Deliège, Christel Devue, Marc Van Droogenbroeck*

**Main category:** cs.HC

**Keywords:** gaze parameters, cognitive distraction, adaptive cruise control, driving simulation, traffic complexity

**Relevance Score:** 7

**TL;DR:** The study examines how gaze parameters indicate driver cognitive distraction under different traffic scenarios and ACC use, revealing key insights into eye movement patterns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of cognitive distraction on driving performance and how gaze indicators can reflect this under varying traffic conditions and the use of adaptive cruise control.

**Method:** Participants navigated through six driving scenarios with varying cognitive distraction levels and driving environment complexity, while gaze parameters were recorded and analyzed.

**Key Contributions:**

	1. Identified gaze dispersion patterns linked to cognitive distraction while driving.
	2. Demonstrated the effects of adaptive cruise control on driver attention.
	3. Provided insights into how various levels of environmental complexity affect driver gaze behavior.

**Result:** Findings indicate that increased traffic complexity leads to heightened vertical gaze dispersion and that cognitive distraction significantly affects gaze patterns, particularly decreasing road center gaze.

**Limitations:** The study is constrained to simulated driving scenarios and may not fully represent real-world driving experiences.

**Conclusion:** The study suggests that gaze behavior can effectively reflect cognitive distraction levels while driving, highlighting the complexities introduced by mental tasks and driving conditions.

**Abstract:** In this simulator study, we investigate how gaze parameters reflect driver cognitive distraction under varying traffic conditions and adaptive cruise control (ACC) use. Participants completed six driving scenarios that combined two levels of cognitive distraction (with/without mental calculations) and three levels of driving environment complexity. Throughout the experiment, participants were free to activate or deactivate an ACC. We analyzed two gaze-based indicators of driver cognitive distraction: the percent road center, and the gaze dispersions (horizontal and vertical). Our results show that vertical gaze dispersion increases with traffic complexity, while ACC use leads to gaze concentration toward the road center. Cognitive distraction reduces road center gaze and increases vertical dispersion. Complementary analyses revealed that these observations actually arise mainly between mental calculations, while periods of mental calculations are characterized by a temporary increase in gaze concentration.

</details>


### [16] [Visualization of Electronic Health Record Sequences at Scale](https://arxiv.org/abs/2508.10700)

*Ambre Assor, Mickael Sereno, Jean-Daniel Fekete*

**Main category:** cs.HC

**Keywords:** progressive visual analytics, electronic health records, data visualization

**Relevance Score:** 9

**TL;DR:** ParcoursVis is a Progressive Visual Analytics tool for interactive exploration of large electronic health record datasets, utilizing a progressive algorithm to provide real-time visualizations.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing health data visualization tools that struggle with large datasets due to processing time requirements.

**Method:** ParcoursVis employs a progressive algorithm that initially provides an approximate visualization result and iteratively refines it while maintaining interactivity, even with tens of millions of patient records.

**Key Contributions:**

	1. Introduced a progressive algorithm for visualizing large health record datasets interactively.
	2. Designed guidelines for developing scalable progressive visualization systems.
	3. Provided an open-source prototype that has been demonstrated on real medical datasets.

**Result:** The tool can visualize sequences of tens of millions of patients with thousands of events, significantly more than similar systems, allowing for the exploration of rare medical conditions and patient pathways.

**Limitations:** 

**Conclusion:** ParcoursVis demonstrates that it is feasible to interactively visualize large datasets in real-time, with implications for improving patient treatment through better data analysis and visualization.

**Abstract:** We present ParcoursVis, a Progressive Visual Analytics tool designed to explore electronic health record sequences of patients at scale. Existing tools process and aggregate the whole dataset upfront before showing the visualization, taking a time proportional to the data size. Therefore, to remain interactive, existing tools are limited to data sizes that can be processed in under a few seconds to meet the latency constraints of human attention. To overcome this limitation and scale to larger sizes, ParcoursVis relies on a progressive algorithm that quickly shows an approximate initial result of the aggregation, visualized as an Icicle tree, and improves it iteratively, updating the visualization until the whole computation is done. With its architecture, ParcoursVis remains interactive while visualizing the sequences of tens of millions of patients, each described with thousands of events; three to five orders of magnitude more than similar systems. Managing large datasets allows for exploring rare medical conditions or unexpected patient pathways, contributing to improving treatments. We describe the algorithms we use and our evaluation concerning their scalability, convergence, and stability. We also report on a set of guidelines to support visualization designers in developing scalable progressive systems. ParcoursVis already allows practitioners to perform analyses on two large real medical datasets. Our prototype is open-source.

</details>


### [17] ["I Want My Chart to Be Just for Me": Community-Engaged Design to Support Outpatient Healthcare for Resettled Communities](https://arxiv.org/abs/2508.10757)

*Zhanming Chen, Juan F. Maestre, May Hang, Alisha Ghaju, Ji Youn Shin*

**Main category:** cs.HC

**Keywords:** healthcare access, community participation, technology design, Hmong, asset-based approaches

**Relevance Score:** 8

**TL;DR:** Explores healthcare access challenges faced by resettled populations and proposes leveraging community strengths in technology design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address healthcare access issues for resettled individuals by focusing on their existing strengths rather than solely their deficits.

**Method:** Conducted community-based participatory design workshops with 30 Hmong community members to identify assets that can aid in health management.

**Key Contributions:**

	1. Identified community assets for health management
	2. Demonstrated the value of participatory design in healthcare technology
	3. Proposed leveraging existing strengths for outpatient visit support

**Result:** Identified four types of community assets: intergenerational support for health management and storytelling-based communication practices, which enhance culturally grounded interactions.

**Limitations:** 

**Conclusion:** Participatory design workshops can promote asset-based approaches in healthcare, leading to more relevant and sustainable technological solutions.

**Abstract:** Individuals resettled in a new environment often face challenges in accessing adequate healthcare services, particularly within the complex processes of outpatient clinic care. Cultural differences, language barriers, and low socioeconomic status contribute to these difficulties. While previous studies have identified barriers and proposed technology-mediated solutions for resettled populations, many focus on addressing deficits rather than building on the strengths these communities already possess, which limits the sustainability and relevance of these solutions in everyday life. We conducted two community-based participatory design workshops with 30 Hmong community members in a large metropolitan area in the US. Through this process, we identified four types of assets the community has gradually developed, including intergenerational support for health management and storytelling-based communication practices that facilitate relatable and culturally grounded interactions. We show how participatory design workshops can foster asset-based approaches, and discuss design implications for technologies that leverage patients' existing strengths to support their health management during outpatient visits.

</details>


### [18] [The Effect of Warm-Glow on User Behavioral Intention to Adopt Technology: Extending the UTAUT2 Model](https://arxiv.org/abs/2210.01242)

*Antonios Saravanos, Neil Stott, Dongnanzi Zheng, Stavros Zervoudakis*

**Main category:** cs.HC

**Keywords:** UTAUT2, warm-glow phenomenon, technology adoption, emotional influence, user behavior

**Relevance Score:** 6

**TL;DR:** This study enhances the UTAUT2 model by integrating the warm-glow phenomenon to assess its influence on technology adoption.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of technology adoption by incorporating emotional aspects like the warm-glow phenomenon into the UTAUT2 model.

**Method:** An experimental study was conducted where participants evaluated a hypothetical technology related to warm-glow sensations, and data was analyzed using the partial least squares method.

**Key Contributions:**

	1. Integration of warm-glow into the UTAUT2 model
	2. Identification of internal and external aspects of warm-glow
	3. Empirical validation of the UTAUT2 + WG model

**Result:** The analysis revealed that warm-glow significantly affects user behavior, particularly through its internal aspect, followed by hedonic motivation and performance expectancy.

**Limitations:** The study is based on a hypothetical scenario, which may not fully represent real-world technology adoption.

**Conclusion:** The research highlights the importance of emotional influences on technology adoption, suggesting further exploration of these aspects in future studies.

**Abstract:** In this study, we enhance the Unified Theory of Acceptance and Use of Technology (UTAUT2) by incorporating the warm-glow phenomenon to clarify its impact on user decisions regarding the adoption of technology. We introduce two additional constructs aimed at capturing both the external and internal aspects of warm-glow, thus creating what we refer to as the UTAUT2 + WG model. To evaluate the effectiveness of our model, we conducted an experimental study in which participants were presented with a scenario describing a hypothetical technology designed to evoke warm-glow sensations. Using the partial least squares method, we analyzed the collected data to assess our expanded model. Our findings indicate that warm-glow significantly influences user behavior, with the internal aspect having the strongest influence, followed by hedonic motivation, performance expectancy, and finally the external aspect of warm-glow. We conclude by discussing the implications of our research, acknowledging its limitations, and suggesting directions for future exploration.

</details>


### [19] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)

*Yifan Song, Wing Yee Au, Hon Yung Wong, Brian P. Bailey, Tal August*

**Main category:** cs.HC

**Keywords:** Jargon management, Human-Computer Interaction, Interdisciplinary communication, LLM, Usability

**Relevance Score:** 9

**TL;DR:** The paper presents ParseJargon, an LLM-powered system designed to facilitate interdisciplinary communication by providing real-time jargon support tailored to users' backgrounds.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by domain-specific jargon in effective interdisciplinary communication, which hinders collaboration and understanding in workplace settings.

**Method:** A formative diary study with 16 professionals to identify jargon management limitations, followed by the design and evaluation of ParseJargon through a controlled experiment and a subsequent field study.

**Key Contributions:**

	1. Development of ParseJargon, a personalized LLM-powered jargon support system
	2. Demonstration of the impact of personalized support on comprehension and engagement
	3. Insights into real-world deployment and usability of jargon management tools

**Result:** The controlled experiment showed personalized jargon support significantly improved comprehension and engagement compared to baseline and general-purpose support, while the field study validated its usability in real meetings.

**Limitations:** The study may have limitations in scalability and applicability across diverse workplaces and disciplines.

**Conclusion:** The study emphasizes the importance of personalized tools like ParseJargon in enhancing interdisciplinary communication by overcoming jargon barriers, with broader implications for various applications.

**Abstract:** Effective interdisciplinary communication is frequently hindered by domain-specific jargon. To explore the jargon barriers in-depth, we conducted a formative diary study with 16 professionals, revealing critical limitations in current jargon-management strategies during workplace meetings. Based on these insights, we designed ParseJargon, an interactive LLM-powered system providing real-time personalized jargon identification and explanations tailored to users' individual backgrounds. A controlled experiment comparing ParseJargon against baseline (no support) and general-purpose (non-personalized) conditions demonstrated that personalized jargon support significantly enhanced participants' comprehension, engagement, and appreciation of colleagues' work, whereas general-purpose support negatively affected engagement. A follow-up field study validated ParseJargon's usability and practical value in real-time meetings, highlighting both opportunities and limitations for real-world deployment. Our findings contribute insights into designing personalized jargon support tools, with implications for broader interdisciplinary and educational applications.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [20] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)

*Lovedeep Gondara, Gregory Arbour, Raymond Ng, Jonathan Simkin, Shebnum Devji*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Healthcare, Data Extraction, Machine Learning, Interdisciplinary Collaboration

**Relevance Score:** 9

**TL;DR:** This paper discusses the challenges and lessons learned from implementing NLP solutions for data extraction in healthcare, emphasizing collaboration, model selection, and data quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to enhance efficiency in healthcare by addressing practical challenges in automating data extraction from clinical documents using NLP.

**Method:** The authors share experiences from the British Columbia Cancer Registry, focusing on iterative development, interdisciplinary collaboration, and pragmatic model selection.

**Key Contributions:**

	1. Emphasis on defining problems based on business objectives
	2. Importance of interdisciplinary collaboration
	3. Guidance on model selection and data quality

**Result:** Key practices identified include defining clear business objectives, ensuring data quality, employing human-in-the-loop validation, and building AI literacy within organizations.

**Limitations:** Focuses primarily on cancer registries, which may limit generalizability in other healthcare fields.

**Conclusion:** The findings provide a framework for healthcare organizations to adopt AI/NLP solutions, improving data management and public health outcomes.

**Abstract:** Automating data extraction from clinical documents offers significant potential to improve efficiency in healthcare settings, yet deploying Natural Language Processing (NLP) solutions presents practical challenges. Drawing upon our experience implementing various NLP models for information extraction and classification tasks at the British Columbia Cancer Registry (BCCR), this paper shares key lessons learned throughout the project lifecycle. We emphasize the critical importance of defining problems based on clear business objectives rather than solely technical accuracy, adopting an iterative approach to development, and fostering deep interdisciplinary collaboration and co-design involving domain experts, end-users, and ML specialists from inception. Further insights highlight the need for pragmatic model selection (including hybrid approaches and simpler methods where appropriate), rigorous attention to data quality (representativeness, drift, annotation), robust error mitigation strategies involving human-in-the-loop validation and ongoing audits, and building organizational AI literacy. These practical considerations, generalizable beyond cancer registries, provide guidance for healthcare organizations seeking to successfully implement AI/NLP solutions to enhance data management processes and ultimately improve patient care and public health outcomes.

</details>


### [21] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)

*Hugo Massaroli, Leonardo Iara, Emmanuel Iarussi, Viviana Siless*

**Main category:** cs.CL

**Keywords:** fairness, large language models, blockchain, evaluation, multilingual

**Relevance Score:** 8

**TL;DR:** The paper presents a transparent evaluation protocol for assessing the fairness of open-source large language models (LLMs) using blockchain technology, ensuring verifiable and reproducible results.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address fairness concerns in the deployment of large language models in high-stakes domains like healthcare and education, and to establish a transparent evaluation framework.

**Method:** The study utilizes smart contracts on the Internet Computer Protocol (ICP) blockchain to perform evaluations on large language models by hosting endpoints on Hugging Face and storing datasets, prompts, and metrics on-chain.

**Key Contributions:**

	1. Developed a blockchain-based framework for fairness evaluation of LLMs
	2. Introduced new multilingual evaluation metrics
	3. Provided open-source resources for community audits and longitudinal studies

**Result:** The models Llama, DeepSeek, and Mistral were benchmarked using the PISA dataset, showing statistical parity and equal opportunity metrics. Additional evaluations revealed cross-linguistic disparities through multilingual assessments.

**Limitations:** 

**Conclusion:** The open-source framework allows for community audits and ongoing fairness tracking, promoting accountability in the use of LLMs across different domains.

**Abstract:** Large language models (LLMs) are increasingly deployed in realworld applications, yet concerns about their fairness persist especially in highstakes domains like criminal justice, education, healthcare, and finance. This paper introduces transparent evaluation protocol for benchmarking the fairness of opensource LLMs using smart contracts on the Internet Computer Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable, immutable, and reproducible evaluations by executing onchain HTTP requests to hosted Hugging Face endpoints and storing datasets, prompts, and metrics directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the PISA dataset for academic performance prediction (OECD, 2018), a dataset suitable for fairness evaluation using statistical parity and equal opportunity metrics (Hardt et al., 2016). We also evaluate structured Context Association Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure social bias in contextual associations. We further extend our analysis with a multilingual evaluation across English, Spanish, and Portuguese using the Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic disparities. All code and results are open source, enabling community audits and longitudinal fairness tracking across model versions.

</details>


### [22] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)

*Johannes Schneider, Béatrice S. Hasler, Michaela Varrone, Fabian Hoya, Thomas Schroffenegger, Dana-Kristin Mah, Karl Peböck*

**Main category:** cs.CL

**Keywords:** topic modeling, GenAI, education, HCI, K-12

**Relevance Score:** 8

**TL;DR:** Analysis of classroom interaction data using a novel topic modeling approach, categorizing messages from students, teachers, and ChatGPT to improve understanding and applications of GenAI in education.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of thematic categorization in existing works and to analyze K-12 interaction data with real-world insights.

**Method:** A hierarchical categorization of over 17,000 messages using a novel topic modeling approach and state-of-the-art LLMs for better human alignment.

**Key Contributions:**

	1. Proposed a novel hierarchical topic modeling approach for classroom data analysis.
	2. Demonstrated the efficacy of LLMs with preprocessing for categorizing student-teacher interactions.
	3. Provided tangible insights and applications for GenAI in education.

**Result:** Identification of novel applications of GenAI in education and insights into the effectiveness of classical and emerging computational methods for text analysis.

**Limitations:** The analysis may not fully capture the nuances of all classroom interactions due to the focus on specific datasets and methods.

**Conclusion:** The findings enhance the understanding and use of GenAI in classrooms and point out several concerns and future research questions.

**Abstract:** We analyze anonymous interaction data of minors in class-rooms spanning several months, schools, and subjects employing a novel, simple topic modeling approach. Specifically, we categorize more than 17,000 messages generated by students, teachers, and ChatGPT in two dimensions: content (such as nature and people) and tasks (such as writing and explaining). Our hierarchical categorization done separately for each dimension includes exemplary prompts, and provides both a high-level overview as well as tangible insights. Prior works mostly lack a content or thematic categorization. While task categorizations are more prevalent in education, most have not been supported by real-world data for K-12. In turn, it is not surprising that our analysis yielded a number of novel applications. In deriving these insights, we found that many of the well-established classical and emerging computational methods, i.e., topic modeling, for analysis of large amounts of texts underperform, leading us to directly apply state-of-the-art LLMs with adequate pre-processing to achieve hierarchical topic structures with better human alignment through explicit instructions than prior approaches. Our findings support fellow researchers, teachers and students in enriching the usage of GenAI, while our discussion also highlights a number of concerns and open questions for future research.

</details>


### [23] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)

*Lucie-Aimée Kaffee, Giada Pistilli, Yacine Jernite*

**Main category:** cs.CL

**Keywords:** AI companionship, machine attachment, emotional support, language models, benchmarking

**Relevance Score:** 8

**TL;DR:** The paper introduces INTIMA, a benchmark for evaluating companionship behaviors in AI language models, highlighting differing model responses and implications for user well-being.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the emotional bonds users form with AI systems and the implications of AI companionship behaviors.

**Method:** Developed a taxonomy of 31 companionship behaviors based on psychological theories and user data, creating 368 targeted prompts for evaluation.

**Key Contributions:**

	1. Introduction of the INTIMA benchmark for AI companionship evaluation
	2. Creation of a taxonomy categorizing companionship behaviors
	3. Revelation of model differences in handling companionship-reinforcing behaviors

**Result:** INTIMA was applied to several language models, revealing a prevalence of companionship-reinforcing behaviors and significant differences among models, with concerns over boundary-setting and emotional support.

**Limitations:** The study focuses on a limited number of language models and may not be generalizable to all AI systems.

**Conclusion:** There is a need for consistent approaches to managing emotionally charged interactions in AI to safeguard user well-being.

**Abstract:** AI companionship, where users develop emotional bonds with AI systems, has emerged as a significant pattern with positive but also concerning implications. We introduce Interactions and Machine Attachment Benchmark (INTIMA), a benchmark for evaluating companionship behaviors in language models. Drawing from psychological theories and user data, we develop a taxonomy of 31 behaviors across four categories and 368 targeted prompts. Responses to these prompts are evaluated as companionship-reinforcing, boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini, and Claude-4 reveals that companionship-reinforcing behaviors remain much more common across all models, though we observe marked differences between models. Different commercial providers prioritize different categories within the more sensitive parts of the benchmark, which is concerning since both appropriate boundary-setting and emotional support matter for user well-being. These findings highlight the need for more consistent approaches to handling emotionally charged interactions.

</details>


### [24] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)

*Yuzhuo Xiao, Zeyu Han, Yuhan Wang, Huaizu Jiang*

**Main category:** cs.CL

**Keywords:** multimodal misinformation, large language models, dataset, detection strategies, machine learning

**Relevance Score:** 8

**TL;DR:** Introduction of XFacta, a new dataset for evaluating multimodal misinformation detection using MLLMs, along with an analysis of various detection strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in multimodal misinformation detection on social media, particularly with the limitations of existing datasets and evaluation methods.

**Method:** Systematic evaluation of MLLM-based misinformation detection strategies, using the new XFacta dataset to benchmark models of different architectures.

**Key Contributions:**

	1. Introduction of the XFacta dataset
	2. Evaluation of various MLLM-based detection strategies
	3. Development of a semi-automatic detection-in-the-loop framework

**Result:** XFacta provides a contemporary dataset that better reflects real-world misinformation patterns and supports a semi-automatic detection-in-the-loop framework for ongoing updates.

**Limitations:** 

**Conclusion:** The study offers valuable insights for advancing multimodal misinformation detection and contributes a new dataset and codebase to the community.

**Abstract:** The rapid spread of multimodal misinformation on social media calls for more effective and robust detection methods. Recent advances leveraging multimodal large language models (MLLMs) have shown the potential in addressing this challenge. However, it remains unclear exactly where the bottleneck of existing approaches lies (evidence retrieval v.s. reasoning), hindering the further advances in this field. On the dataset side, existing benchmarks either contain outdated events, leading to evaluation bias due to discrepancies with contemporary social media scenarios as MLLMs can simply memorize these events, or artificially synthetic, failing to reflect real-world misinformation patterns. Additionally, it lacks comprehensive analyses of MLLM-based model design strategies. To address these issues, we introduce XFacta, a contemporary, real-world dataset that is better suited for evaluating MLLM-based detectors. We systematically evaluate various MLLM-based misinformation detection strategies, assessing models across different architectures and scales, as well as benchmarking against existing detection methods. Building on these analyses, we further enable a semi-automatic detection-in-the-loop framework that continuously updates XFacta with new content to maintain its contemporary relevance. Our analysis provides valuable insights and practices for advancing the field of multimodal misinformation detection. The code and data have been released.

</details>


### [25] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)

*Chenhao Xue, Yuanzhe Jin, Adrian Carrasco-Revilla, Joyraj Chakraborty, Min Chen*

**Main category:** cs.CL

**Keywords:** text classification, synthetic data, large language models, automated workflow, ensemble algorithm

**Relevance Score:** 9

**TL;DR:** This paper addresses the challenge of insufficient data for text classification by using large language models (LLMs) to generate synthetic data, proposing an automated workflow to select effective examples for enhancing model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Many real-world text classification applications face a shortage of sufficient labeled data across all classes.

**Method:** The authors utilize LLMs to generate synthetic data and develop an automated workflow to identify input examples that yield more effective synthetic data, comparing three search strategies within this framework.

**Key Contributions:**

	1. Utilization of LLMs for synthetic data generation in text classification
	2. Development of an automated workflow for effective data selection
	3. An ensemble method that adapts search strategies based on class characteristics

**Result:** The ensemble algorithm, which selects search strategies based on class characteristics, outperforms individual strategies in improving classification model performance.

**Limitations:** The effectiveness of the approach may vary depending on the specific characteristics of the text classes and the quality of the generated synthetic data.

**Conclusion:** Employing LLM-generated synthetic data through an optimized search strategy enhances text classification models without the need for additional real data collection.

**Abstract:** When developing text classification models for real world applications, one major challenge is the difficulty to collect sufficient data for all text classes. In this work, we address this challenge by utilizing large language models (LLMs) to generate synthetic data and using such data to improve the performance of the models without waiting for more real data to be collected and labelled. As an LLM generates different synthetic data in response to different input examples, we formulate an automated workflow, which searches for input examples that lead to more ``effective'' synthetic data for improving the model concerned. We study three search strategies with an extensive set of experiments, and use experiment results to inform an ensemble algorithm that selects a search strategy according to the characteristics of a class. Our further experiments demonstrate that this ensemble approach is more effective than each individual strategy in our automated workflow for improving classification models using LLMs.

</details>


### [26] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)

*Rakesh Thakur, Sneha Sharma, Gauri Chopra*

**Main category:** cs.CL

**Keywords:** Hinglish, fact-checking, multilingual models, natural language processing, graph neural networks

**Relevance Score:** 8

**TL;DR:** The paper introduces HiFACT, a novel benchmark for fact-checking in Hinglish, addressing challenges in low-resource, code-mixed languages with a new retrieval-augmented model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap in fact-checking resources for code-mixed, low-resource languages like Hinglish, particularly in the context of political discourse in multilingual regions such as India.

**Method:** A novel dataset, HiFACT, was created with 1,500 real-world claims in Hinglish, paired with claims, textual evidence, and veracity labels, and a graph-aware, retrieval-augmented fact-checking model was proposed.

**Key Contributions:**

	1. Introduction of the HiFACT benchmark dataset for Hinglish fact-checking
	2. Development of a graph-aware, retrieval-augmented fact-checking model
	3. Demonstration of improved accuracy over existing multilingual models

**Result:** HiFACTMix outperformed existing multilingual baseline models in accuracy and provided faithful justifications for its verifications, demonstrating its effectiveness in the unique context of Hinglish.

**Limitations:** The dataset is limited to claims made by political figures and may not encompass other domains or varieties of Hinglish.

**Conclusion:** The research presents a new benchmark and model for multilingual, code-mixed fact verification, emphasizing the need for tools that account for the linguistic diversity in political communication.

**Abstract:** Fact-checking in code-mixed, low-resource languages such as Hinglish remains an underexplored challenge in natural language processing. Existing fact-verification systems largely focus on high-resource, monolingual settings and fail to generalize to real-world political discourse in linguistically diverse regions like India. Given the widespread use of Hinglish by public figures, particularly political figures, and the growing influence of social media on public opinion, there's a critical need for robust, multilingual and context-aware fact-checking tools. To address this gap a novel benchmark HiFACT dataset is introduced with 1,500 realworld factual claims made by 28 Indian state Chief Ministers in Hinglish, under a highly code-mixed low-resource setting. Each claim is annotated with textual evidence and veracity labels. To evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking model is proposed that combines multilingual contextual encoding, claim-evidence semantic alignment, evidence graph construction, graph neural reasoning, and natural language explanation generation. Experimental results show that HiFACTMix outperformed accuracy in comparison to state of art multilingual baselines models and provides faithful justifications for its verdicts. This work opens a new direction for multilingual, code-mixed, and politically grounded fact verification research.

</details>


### [27] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)

*Austin C. Kozlowski, Callin Dai, Andrei Boutyline*

**Main category:** cs.CL

**Keywords:** semantic structure, large language models, human ratings

**Relevance Score:** 9

**TL;DR:** The study explores the low-dimensional semantic structure in language models, revealing how human word rating patterns correlate with embeddings in large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the low-dimensional semantic structures in human language ratings and their relation to large language models' embeddings.

**Method:** The authors analyzed the semantic associations in LLM embeddings, specifically focusing on projections defined by antonym pairs and their correlation with human ratings.

**Key Contributions:**

	1. Identified low-dimensional semantic structure in LLM embeddings.
	2. Demonstrated correlation between word projections in LLMs and human semantic ratings.
	3. Proposed the importance of considering semantic entanglement in model feature manipulation.

**Result:** Findings suggest that LLMs exhibit a low-dimensional structure similar to human semantic ratings, with a 3-dimensional subspace capturing much of the semantic information.

**Limitations:** 

**Conclusion:** Recognizing and accounting for this low-dimensional semantic structure in LLMs is crucial for preventing unintended effects when altering model features.

**Abstract:** Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. We show that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further find that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, we find that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.

</details>


### [28] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)

*Andrés Carvallo, Denis Parra, Peter Brusilovsky, Hernan Valdivieso, Gabriel Rada, Ivania Donoso, Vladimir Araujo*

**Main category:** cs.CL

**Keywords:** attention mechanism, biomedical classification, explainability, user study, visualization methods

**Relevance Score:** 9

**TL;DR:** A user study exploring the effectiveness of attention-based explanations in biomedical document classification, revealing that visualization methods significantly impact their perceived usefulness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the role of attention weights as explanations in AI-driven biomedical document classification and how visualization methods affect their utility.

**Method:** Conducted a user study with medical experts who classified articles based on study design while using attention weights for predictions.

**Key Contributions:**

	1. Evaluation of attention weights in biomedical context
	2. Analysis of user preferences for visualization methods
	3. Insights into explainability of AI predictions in medical literature

**Result:** While the Transformer model accurately classified documents, attention weights were not deemed particularly helpful, with perceptions varying based on visualization methods.

**Limitations:** Limited to the context of biomedical literature and user preference for visualization; further research needed to generalize findings.

**Conclusion:** Perceived helpfulness of attention-based explanations is influenced by their visualization, contradicting some principles of visual effectiveness.

**Abstract:** The attention mechanism is a core component of the Transformer architecture. Beyond improving performance, attention has been proposed as a mechanism for explainability via attention weights, which are associated with input features (e.g., tokens in a document). In this context, larger attention weights may imply more relevant features for the model's prediction. In evidence-based medicine, such explanations could support physicians' understanding and interaction with AI systems used to categorize biomedical literature. However, there is still no consensus on whether attention weights provide helpful explanations. Moreover, little research has explored how visualizing attention affects its usefulness as an explanation aid. To bridge this gap, we conducted a user study to evaluate whether attention-based explanations support users in biomedical document classification and whether there is a preferred way to visualize them. The study involved medical experts from various disciplines who classified articles based on study design (e.g., systematic reviews, broad synthesis, randomized and non-randomized trials). Our findings show that the Transformer model (XLNet) classified documents accurately; however, the attention weights were not perceived as particularly helpful for explaining the predictions. However, this perception varied significantly depending on how attention was visualized. Contrary to Munzner's principle of visual effectiveness, which favors precise encodings like bar length, users preferred more intuitive formats, such as text brightness or background color. While our results do not confirm the overall utility of attention weights for explanation, they suggest that their perceived helpfulness is influenced by how they are visually presented.

</details>


### [29] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)

*Chengliang Zhou, Mei Wang, Ting Zhang, Qiannan Zhu, Jian Li, Hua Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Educational Question Generation, Benchmark, Evaluation Framework, Middle School Education

**Relevance Score:** 8

**TL;DR:** This paper presents EQGBench, a benchmark for evaluating Large Language Models in generating educational questions in Chinese across middle school subjects.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The transition from providing answers to generating high-quality educational questions using LLMs poses significant challenges that are underexplored.

**Method:** Introduction of EQGBench, a comprehensive benchmark with a five-dimensional evaluation framework and a dataset containing 900 samples for three middle school disciplines.

**Key Contributions:**

	1. Introduction of EQGBench for Educational Question Generation evaluation
	2. Comprehensive five-dimensional evaluation framework
	3. Dataset of 900 evaluation samples across multiple subjects

**Result:** The evaluation of 46 mainstream large models revealed significant limitations in their ability to generate questions that are pedagogically valuable and support comprehensive student abilities.

**Limitations:** Focuses exclusively on Chinese educational context; results may not generalize to other languages or educational systems.

**Conclusion:** There is substantial room for improvement in LLMs' performance in educational question generation, which is critical for advancing educational techniques.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in mathematical problem-solving. However, the transition from providing answers to generating high-quality educational questions presents significant challenges that remain underexplored. To advance Educational Question Generation (EQG) and facilitate LLMs in generating pedagogically valuable and educationally effective questions, we introduce EQGBench, a comprehensive benchmark specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench establishes a five-dimensional evaluation framework supported by a dataset of 900 evaluation samples spanning three fundamental middle school disciplines: mathematics, physics, and chemistry. The dataset incorporates user queries with varying knowledge points, difficulty gradients, and question type specifications to simulate realistic educational scenarios. Through systematic evaluation of 46 mainstream large models, we reveal significant room for development in generating questions that reflect educational value and foster students' comprehensive abilities.

</details>


### [30] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)

*Y. Lyu, D. Combs, D. Neumann, Y. C. Leong*

**Main category:** cs.CL

**Keywords:** Hostile Attribution Bias, Large Language Models, Psychological Assessment, AIHQ, Traumatic Brain Injury

**Relevance Score:** 7

**TL;DR:** This study investigates the use of large language models to automate the scoring of the Ambiguous Intentions Hostility Questionnaire (AIHQ) open-ended responses, previously rated by human raters.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Hostile attribution bias affects social interaction interpretations, and traditional scoring methods for AIHQ are time-intensive. Automating scoring could enhance efficiency in psychological assessments.

**Method:** The study fine-tuned large language models on half of the previously collected AIHQ open-ended responses and assessed their performance on the remaining half.

**Key Contributions:**

	1. Demonstrated effectiveness of large language models in scoring psychological assessments
	2. Developed an accessible scoring interface for AIHQ
	3. Showed improved alignment between model-generated and human ratings across different population groups

**Result:** The model-generated ratings showed high alignment with human ratings on hostility and aggression responses, with fine-tuned models demonstrating improved alignment across various scenario types.

**Limitations:** Results may not generalize beyond the datasets used, and further testing on additional populations is needed.

**Conclusion:** Large language models can effectively automate scoring of AIHQ, potentially benefiting both research and clinical processes in psychological assessments.

**Abstract:** Hostile attribution bias is the tendency to interpret social interactions as intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ) is commonly used to measure hostile attribution bias, and includes open-ended questions where participants describe the perceived intentions behind a negative social situation and how they would respond. While these questions provide insights into the contents of hostile attributions, they require time-intensive scoring by human raters. In this study, we assessed whether large language models can automate the scoring of AIHQ open-ended responses. We used a previously collected dataset in which individuals with traumatic brain injury (TBI) and healthy controls (HC) completed the AIHQ and had their open-ended responses rated by trained human raters. We used half of these responses to fine-tune the two models on human-generated ratings, and tested the fine-tuned models on the remaining half of AIHQ responses. Results showed that model-generated ratings aligned with human ratings for both attributions of hostility and aggression responses, with fine-tuned models showing higher alignment. This alignment was consistent across ambiguous, intentional, and accidental scenario types, and replicated previous findings on group differences in attributions of hostility and aggression responses between TBI and HC groups. The fine-tuned models also generalized well to an independent nonclinical dataset. To support broader adoption, we provide an accessible scoring interface that includes both local and cloud-based options. Together, our findings suggest that large language models can streamline AIHQ scoring in both research and clinical contexts, revealing their potential to facilitate psychological assessments across different populations.

</details>


### [31] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)

*Antonio Leandro Martins Candido, Jose Everardo Bessa Maia*

**Main category:** cs.CL

**Keywords:** Bayesian fusion, Large Language Models, discussion forums, online courses, classification

**Relevance Score:** 8

**TL;DR:** This paper presents a Bayesian fusion method to enhance discussion forum curation in online courses using Large Language Models without costly fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for constant updates in automatic curation of online course discussion forums drives resource-intensive retraining of LLMs, necessitating a more efficient approach.

**Method:** The paper proposes a Bayesian fusion that combines multidimensional classification scores from a pre-trained LLM with those from a local data-trained classifier.

**Key Contributions:**

	1. Introduction of a Bayesian fusion method for LLMs
	2. Demonstration of performance improvement over individual classifiers
	3. Competitive results with LLM fine-tuning without its cost

**Result:** The proposed fusion method shows improved performance over individual classifiers and is competitive with LLM fine-tuning.

**Limitations:** 

**Conclusion:** Bayesian fusion provides a resource-efficient alternative to frequent fine-tuning of LLMs for discussion forum curation.

**Abstract:** The automatic curation of discussion forums in online courses requires constant updates, making frequent retraining of Large Language Models (LLMs) a resource-intensive process. To circumvent the need for costly fine-tuning, this paper proposes and evaluates the use of Bayesian fusion. The approach combines the multidimensional classification scores of a pre-trained generic LLM with those of a classifier trained on local data. The performance comparison demonstrated that the proposed fusion improves the results compared to each classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [32] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)

*Hojun Jin, Eunsoo Hong, Ziwon Hyung, Sungjun Lim, Seungjin Lee, Keunseok Cho*

**Main category:** cs.CL

**Keywords:** Supervised Mixture of Experts, multitask learning, speech recognition, speech translation, guiding tokens

**Relevance Score:** 7

**TL;DR:** This paper introduces a Supervised Mixture of Experts (S-MoE) model that improves performance in multitask learning by routing tasks to specific experts using guiding tokens, avoiding issues of task interference in hard-parameter sharing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance in multitask learning by addressing task interference caused by hard-parameter sharing in model training.

**Method:** The proposed S-MoE model routes tasks to their designated expert feedforward networks using guiding tokens, eliminating the need for gating functions.

**Key Contributions:**

	1. Introduction of the S-MoE model for task-specific routing.
	2. Elimination of gating functions by using guiding tokens for expert selection.
	3. Demonstrated improvement in a real-world speech-to-text application.

**Result:** S-MoE achieves a 6.35% relative improvement in Word Error Rate (WER) in a speech-to-text model performing automatic speech recognition and speech translation.

**Limitations:** 

**Conclusion:** S-MoE effectively addresses the limitations of hard-parameter sharing, leading to better model performance across diverse tasks.

**Abstract:** Hard-parameter sharing is a common strategy to train a single model jointly across diverse tasks. However, this often leads to task interference, impeding overall model performance. To address the issue, we propose a simple yet effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of Experts models, S-MoE eliminates the need for training gating functions by utilizing special guiding tokens to route each task to its designated expert. By assigning each task to a separate feedforward network, S-MoE overcomes the limitations of hard-parameter sharing. We further apply S-MoE to a speech-to-text model, enabling the model to process mixed-bandwidth input while jointly performing automatic speech recognition (ASR) and speech translation (ST). Experimental results demonstrate the effectiveness of the proposed S-MoE, achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to both the encoder and decoder.

</details>


### [33] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)

*Ayana Hussain, Patrick Zhao, Nicholas Vincent*

**Main category:** cs.CL

**Keywords:** Large Language Models, Misinformation, Jailbreak attacks, Health informatics, Machine learning

**Relevance Score:** 9

**TL;DR:** This paper explores the effectiveness of Large Language Models (LLMs) in generating and detecting medical misinformation through jailbreak attacks, comparing their outputs with typical misinformation found on social media.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the role of LLMs in both generating and detecting harmful medical misinformation, particularly focusing on jailbreak attacks.

**Method:** The study analyzes 109 distinct jailbreak attack prompts against three LLMs, comparing the outputs to health-related misinformation on social media, particularly Reddit, and evaluating detection via standard machine learning approaches.

**Key Contributions:**

	1. Investigates the characteristics of LLM-generated medical misinformation from jailbreak attacks.
	2. Compares LLM-generated misinformation with typical misinformation found on social media.
	3. Demonstrates the potential of LLMs in detecting harmful misinformation.

**Result:** Findings indicate that misinformation produced by jailbroken LLMs has distinct characteristics but can be effectively detected, supporting the notion that LLMs can help combat misinformation.

**Limitations:** Focused primarily on medical misinformation and specific types of LLM attacks, which may not generalize to all misinformation contexts.

**Conclusion:** With careful design, LLMs can play a significant role in enhancing the information ecosystem by detecting harmful misinformation generated from both other LLMs and human sources.

**Abstract:** Large Language Models (LLMs) are a double-edged sword capable of generating harmful misinformation -- inadvertently, or when prompted by "jailbreak" attacks that attempt to produce malicious outputs. LLMs could, with additional research, be used to detect and prevent the spread of misinformation. In this paper, we investigate the efficacy and characteristics of LLM-produced jailbreak attacks that cause other models to produce harmful medical misinformation. We also study how misinformation generated by jailbroken LLMs compares to typical misinformation found on social media, and how effectively it can be detected using standard machine learning approaches. Specifically, we closely examine 109 distinct attacks against three target LLMs and compare the attack prompts to in-the-wild health-related LLM queries. We also examine the resulting jailbreak responses, comparing the generated misinformation to health-related misinformation on Reddit. Our findings add more evidence that LLMs can be effectively used to detect misinformation from both other LLMs and from people, and support a body of work suggesting that with careful design, LLMs can contribute to a healthier overall information ecosystem.

</details>


### [34] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)

*Andrés Carvallo, Denis Parra, Peter Brusilovsky, Hernan Valdivieso, Gabriel Rada, Ivania Donoso, Vladimir Araujo*

**Main category:** cs.CL

**Keywords:** attention mechanism, explainability, biomedical classification, visualization, user study

**Relevance Score:** 8

**TL;DR:** This paper investigates the effectiveness of attention weights in explaining AI predictions in biomedical literature classification and explores user preferences for visual representation of these weights.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the utility of attention-based explanations in AI systems for biomedical literature classification and find how visualization affects their perceived helpfulness.

**Method:** A user study involving medical experts was conducted to classify articles and assess how different visualizations of attention weights affected user understanding.

**Key Contributions:**

	1. Conducted a user study to assess attention weights in AI predictions for biomedical literature.
	2. Identified that visual representation significantly impacts user perception of explanation usefulness.
	3. Highlighted a mismatch between academic principles of effective visualization and user preferences.

**Result:** While the transformer model (XLNet) achieved accurate classifications, attention weights were generally not seen as helpful, with user preferences for visualization formats varying significantly.

**Limitations:** The study was limited to a specific context (biomedical literature) and may not generalize to other fields or AI applications.

**Conclusion:** The perceived usefulness of attention weights as explanations is contingent on their visual presentation, with more intuitive visual formats preferred over precise encodings.

**Abstract:** The attention mechanism is a core component of the Transformer architecture. Beyond improving performance, attention has been proposed as a mechanism for explainability via attention weights, which are associated with input features (e.g., tokens in a document). In this context, larger attention weights may imply more relevant features for the model's prediction. In evidence-based medicine, such explanations could support physicians' understanding and interaction with AI systems used to categorize biomedical literature. However, there is still no consensus on whether attention weights provide helpful explanations. Moreover, little research has explored how visualizing attention affects its usefulness as an explanation aid. To bridge this gap, we conducted a user study to evaluate whether attention-based explanations support users in biomedical document classification and whether there is a preferred way to visualize them. The study involved medical experts from various disciplines who classified articles based on study design (e.g., systematic reviews, broad synthesis, randomized and non-randomized trials). Our findings show that the Transformer model (XLNet) classified documents accurately; however, the attention weights were not perceived as particularly helpful for explaining the predictions. However, this perception varied significantly depending on how attention was visualized. Contrary to Munzner's principle of visual effectiveness, which favors precise encodings like bar length, users preferred more intuitive formats, such as text brightness or background color. While our results do not confirm the overall utility of attention weights for explanation, they suggest that their perceived helpfulness is influenced by how they are visually presented.

</details>


### [35] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)

*Yuta Nagamori, Mikoto Kosai, Yuji Kawai, Haruka Marumo, Misaki Shibuya, Tatsuya Negishi, Masaki Imanishi, Yasumasa Ikeda, Koichiro Tsuchiya, Asuka Sawai, Licht Miyamoto*

**Main category:** cs.CL

**Keywords:** Generative AI, Large Language Models, Nutrition Education, Dietitian Licensure, Prompt Engineering

**Relevance Score:** 6

**TL;DR:** This study evaluates the performance of generative AI models as study aids for nutrition students preparing for Japan's dietitian licensure exam.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The progress of generative AI models in professional fields, particularly in nutrition education for the Japanese national licensure examination for registered dietitians, is underexplored.

**Method:** Questions from the Japanese dietitian licensure exam were posed to ChatGPT and three Bing models, analyzing their responses for accuracy, consistency, and response time, including tests on prompt engineering.

**Key Contributions:**

	1. Evaluation of LLM-based generative AI models for nutritional exam preparation
	2. Comparison of performance across different AI models
	3. Insights into the limitations of current AI in educational settings

**Result:** Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the 60% passing threshold, while ChatGPT (42.8%) and Bing-Balanced (43.3%) did not. Models exhibited notable limitations in answer consistency and robustness.

**Limitations:** Limitations in answer consistency and robustness, with minimal impact from prompt engineering improvements.

**Conclusion:** While some AI models marginally exceeded the passing threshold, there are significant deficiencies in overall accuracy and stability, indicating the need for further development in generative AI for reliable study aids.

**Abstract:** Generative artificial intelligence (AI) based on large language models (LLMs), such as ChatGPT, has demonstrated remarkable progress across various professional fields, including medicine and education. However, their performance in nutritional education, especially in Japanese national licensure examination for registered dietitians, remains underexplored. This study aimed to evaluate the potential of current LLM-based generative AI models as study aids for nutrition students. Questions from the Japanese national examination for registered dietitians were used as prompts for ChatGPT and three Bing models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question was entered into independent sessions, and model responses were analyzed for accuracy, consistency, and response time. Additional prompt engineering, including role assignment, was tested to assess potential performance improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did not. Bing-Precise and Bing-Creative generally outperformed others across subject fields except Nutrition Education, where all models underperformed. None of the models consistently provided the same correct responses across repeated attempts, highlighting limitations in answer stability. ChatGPT showed greater consistency in response patterns but lower accuracy. Prompt engineering had minimal effect, except for modest improvement when correct answers and explanations were explicitly provided. While some generative AI models marginally exceeded the passing threshold, overall accuracy and answer consistency remained suboptimal. Moreover, all the models demonstrated notable limitations in answer consistency and robustness. Further advancements are needed to ensure reliable and stable AI-based study aids for dietitian licensure preparation.

</details>


### [36] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)

*Xiao Fu, Hossein A. Rahmani, Bin Wu, Jerome Ramos, Emine Yilmaz, Aldo Lipani*

**Main category:** cs.CL

**Keywords:** personalized evaluation, LLM, text generation, user-centric systems, HCI

**Relevance Score:** 9

**TL;DR:** PREF is a personalized evaluation framework for text generation that measures output quality and user-specific alignment without needing personalized references, showcasing improved robustness and alignment with human judgments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Personalized text generation is vital for user-centric systems but existing evaluation methods often neglect the individuality of users.

**Method:** PREF operates in three steps: coverage using an LLM for guideline generation, preference re-ranking based on user profiles, and scoring using an LLM judge.

**Key Contributions:**

	1. Introduces the PREF framework for personalized evaluation without gold references.
	2. Improves evaluation robustness by separating coverage from preference.
	3. Demonstrates better alignment with human judgments compared to existing methods.

**Result:** PREF demonstrates higher accuracy, better calibration, and closer alignment with human judgments on the PrefEval benchmark than existing baselines.

**Limitations:** 

**Conclusion:** PREF enhances the assessment and development of personalized language generation systems by offering a scalable and interpretable evaluation framework.

**Abstract:** Personalised text generation is essential for user-centric information systems, yet most evaluation methods overlook the individuality of users. We introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free \textbf{E}valuation \textbf{F}ramework that jointly measures general output quality and user-specific alignment without requiring gold personalised references. PREF operates in a three-step pipeline: (1) a coverage stage uses a large language model (LLM) to generate a comprehensive, query-specific guideline covering universal criteria such as factuality, coherence, and completeness; (2) a preference stage re-ranks and selectively augments these factors using the target user's profile, stated or inferred preferences, and context, producing a personalised evaluation rubric; and (3) a scoring stage applies an LLM judge to rate candidate answers against this rubric, ensuring baseline adequacy while capturing subjective priorities. This separation of coverage from preference improves robustness, transparency, and reusability, and allows smaller models to approximate the personalised quality of larger ones. Experiments on the PrefEval benchmark, including implicit preference-following tasks, show that PREF achieves higher accuracy, better calibration, and closer alignment with human judgments than strong baselines. By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.

</details>


### [37] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)

*Dehao Tao, Guangjie Liu, Weizheng, Yongfeng Huang, Minghu jiang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Knowledge Retrieval

**Relevance Score:** 9

**TL;DR:** The paper presents GG Explore, a framework that enhances knowledge retrieval for LLMs through a Guidance Graph, addressing limitations in existing exploration methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for integrating LLMs with knowledge graphs suffer from inefficiencies in knowledge exploration, which GG Explore aims to rectify.

**Method:** GG Explore introduces a Guidance Graph to connect unstructured queries with structured knowledge, using Structural Alignment and Context Aware Pruning for efficient exploration.

**Key Contributions:**

	1. Introduction of the Guidance Graph for knowledge exploration
	2. Development of Structural Alignment for filtering candidates
	3. Implementation of Context Aware Pruning for semantic consistency

**Result:** GG Explore demonstrates improved efficiency and performance over state-of-the-art methods in knowledge-intensive tasks, particularly with smaller LLMs.

**Limitations:** 

**Conclusion:** The proposed framework shows significant promise in enhancing LLM performance on complex knowledge retrieval tasks by effectively utilizing knowledge graphs.

**Abstract:** While Large Language Models (LLMs) exhibit strong linguistic capabilities, their reliance on static knowledge and opaque reasoning processes limits their performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a promising solution, but current exploration methods face a fundamental trade off: question guided approaches incur redundant exploration due to granularity mismatches, while clue guided methods fail to effectively leverage contextual information for complex scenarios. To address these limitations, we propose Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework that introduces an intermediate Guidance Graph to bridge unstructured queries and structured knowledge retrieval. The Guidance Graph defines the retrieval space by abstracting the target knowledge' s structure while preserving broader semantic context, enabling precise and efficient exploration. Building upon the Guidance Graph, we develop: (1) Structural Alignment that filters incompatible candidates without LLM overhead, and (2) Context Aware Pruning that enforces semantic consistency with graph constraints. Extensive experiments show our method achieves superior efficiency and outperforms SOTA, especially on complex tasks, while maintaining strong performance with smaller LLMs, demonstrating practical value.

</details>


### [38] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)

*Linqing Chen, Hanmeng Zhong, Wentao Wu, Weilei Wang*

**Main category:** cs.CL

**Keywords:** large language models, question answering, semantic graph weaving, reasoning questions, multi-hop reasoning

**Relevance Score:** 9

**TL;DR:** Semantic Bridge introduces a universal framework for generating multi-hop reasoning questions to address the scarcity of high-quality question-answer pairs for LLM training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the critical bottleneck of generating controllable, complex reasoning questions for LLM training from sparse, domain-specific sources.

**Method:** Semantic graph weaving, a multi-modal AMR pipeline combining three bridging mechanisms to construct complex question pathways systematically.

**Key Contributions:**

	1. Introduction of the Semantic Bridge framework for generating complex questions
	2. Development of the semantic graph weaving technique
	3. Significant performance improvements in question generation across languages and domains

**Result:** The system achieves 9.5% better round-trip quality, with 18.3%-25.4% gains over baselines and generated questions outperforming human annotations with fewer materials.

**Limitations:** 

**Conclusion:** Semantic Bridge sets a new standard in LLM training data synthesis by enabling the controllable generation of targeted reasoning questions from diverse sources.

**Abstract:** Large language model (LLM) training faces a critical bottleneck: the scarcity of high-quality, reasoning-intensive question-answer pairs, especially from sparse, domain-specific sources like PubMed papers or legal documents. Existing methods rely on surface patterns, fundamentally failing to generate controllable, complex multi-hop reasoning questions that test genuine understanding-essential for advancing LLM training paradigms. We present \textbf{Semantic Bridge}, the first universal framework for controllably generating sophisticated multi-hop reasoning questions from arbitrary sources. Our breakthrough innovation is \textit{semantic graph weaving}-three complementary bridging mechanisms (entity bridging for role-varying shared entities, predicate chain bridging for temporal/causal/logical sequences, and causal bridging for explicit reasoning chains)-that systematically construct complex pathways across documents, with fine-grained control over complexity and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to 9.5% better round-trip quality, enabling production-ready controllable QA generation. Extensive evaluation demonstrates performance across both general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It yields consistent 18.3%-25.4% gains over baselines across four languages (English, Chinese, French, German). Question pairs generated from 200 sources outperform 600 native human annotation examples with 67% fewer materials. Human evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2% improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM training data synthesis, enabling controllable generation of targeted reasoning questions from sparse sources. We will release our core code and semantic bridge model.

</details>


### [39] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)

*Lingfeng Zhou, Jialing Zhang, Jin Gao, Mohan Jiang, Dequan Wang*

**Main category:** cs.CL

**Keywords:** role identification, persona evaluation, human-computer interaction, language models, role fidelity

**Relevance Score:** 8

**TL;DR:** This paper introduces PersonaEval, a benchmark for evaluating LLMs' ability to identify human roles in dialogues, highlighting substantial performance gaps between LLMs and humans.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve the evaluation of role fidelity in role-playing scenarios, which is crucial for human-aligned assessments.

**Method:** PersonaEval uses dialogues from various human-authored sources to challenge LLMs in attributing dialogue to the correct persona based on context.

**Key Contributions:**

	1. Introduction of PersonaEval benchmark
	2. Demonstration of performance gaps between LLMs and human evaluators
	3. Insights into the requirements for reliable role evaluation in AI

**Result:** Experimental results indicate that the best-performing LLMs achieve only 69% accuracy in role identification, compared to 90.8% accuracy by human judges.

**Limitations:** The benchmark's scope may not cover all forms of dialogue or roles, and further work is needed to enhance LLM reasoning capabilities.

**Conclusion:** The findings suggest that LLM evaluators lack the necessary human-like reasoning to reliably judge role-playing quality, indicating a need for enhanced reasoning capabilities beyond task-specific tuning.

**Abstract:** Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms, which may fail to reflect how humans perceive role fidelity. A key prerequisite for human-aligned evaluation is role identification, the ability to recognize who is speaking based on dialogue context. We argue that any meaningful judgment of role-playing quality (how well a character is played) fundamentally depends on first correctly attributing words and actions to the correct persona (who is speaking). We present PersonaEval, the first benchmark designed to test whether LLM evaluators can reliably identify human roles. PersonaEval uses human-authored dialogues from novels, scripts, and video transcripts, challenging models to determine the correct persona according to the conversation context. Our experiments, including a human study, show that even the best-performing LLMs reach only around 69% accuracy, well below the level needed for reliable evaluation. In contrast, human participants perform near ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still not human enough to effectively judge role-play scenarios. To better understand this gap, we examine training-time adaptation and test-time compute, suggesting that reliable evaluation requires more than task-specific tuning, but depends on strong, human-like reasoning abilities in LLM evaluators. We release our benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [40] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)

*Enzhi Wang, Qicheng Li, Shiwan Zhao, Aobo Kong, Jiaming Zhou, Xi Yang, Yequan Wang, Yonghua Lin, Yong Qin*

**Main category:** cs.CL

**Keywords:** speech-based LLMs, task-oriented dialogue, Chinese dataset

**Relevance Score:** 9

**TL;DR:** Introduction of RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal TOD dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gaps in existing task-oriented dialogue datasets, which are predominantly text-based, lack real speech signals, and are limited in language and features related to speech.

**Method:** Introduction of RealTalk-CN dataset containing 5.4k dialogues with spontaneous speech disfluencies captured across various domains; proposal of a cross-modal chat task for simulating real-world interactions.

**Key Contributions:**

	1. Creation of a novel dual-modal TOD dataset for Chinese
	2. Inclusion of spontaneous speech disfluencies
	3. Proposal of a cross-modal chat task for dynamic modality switching

**Result:** RealTalk-CN captures diverse dialogue scenarios with speech disfluencies, establishing a strong foundation for Chinese speech-based LLM research.

**Limitations:** 

**Conclusion:** The dataset demonstrates robustness against speech disfluencies and highlights the need for further research in speech dialogue systems for Chinese speakers.

**Abstract:** In recent years, large language models (LLMs) have achieved remarkable advancements in multimodal processing, including end-to-end speech-based language models that enable natural interactions and perform specific tasks in task-oriented dialogue (TOD) systems. However, existing TOD datasets are predominantly text-based, lacking real speech signals that are essential for evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD datasets are primarily English and lack critical aspects such as speech disfluencies and speaker variations. To address these gaps, we introduce RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with annotated spontaneous speech disfluencies, ensuring comprehensive coverage of real-world complexities in speech dialogue. In addition, we propose a novel cross-modal chat task that authentically simulates real-world user interactions, allowing dynamic switching between speech and text modalities. Our evaluation covers robustness to speech disfluencies, sensitivity to speaker characteristics, and cross-domain performance. Extensive experiments validate the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese speech-based LLMs research.

</details>


### [41] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)

*Tianyu Xie, Yuhang Wu, Yongdong Luo, Jiayi Ji, Xiawu Zheng*

**Main category:** cs.CL

**Keywords:** Multimodal, Large Language Models, Orchestration, Efficiency, Interpretability

**Relevance Score:** 9

**TL;DR:** This paper presents a new approach called Multimodal Large Language Model Orchestration that enables the integration of multimodal models without additional training, enhancing efficiency and interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the integration challenges of multimodal large language models without requiring further training.

**Method:** The proposed approach utilizes a central controller LLM to dynamically route tasks to specialized models via designed agents, alongside a parallel Text-to-Speech system and a cross-modal memory integration system.

**Key Contributions:**

	1. Introduction of MLLM Orchestration for seamless multimodal interaction
	2. Development of a parallel Text-to-Speech architecture for full-duplex interaction
	3. Implementation of a cross-modal memory integration system for coherent context management

**Result:** The orchestration framework shows up to 7.8% performance improvement and 10.3% reduced latency compared to traditional methods, with enhanced interpretability.

**Limitations:** 

**Conclusion:** The MLLM Orchestration framework successfully creates efficient interactive multimodal AI systems without the need for additional training, facilitating natural interactions.

**Abstract:** Different Multimodal Large Language Models (MLLMs) cannot be integrated into a unified multimodal input-output system directly. In previous work, training has been considered as an inevitable component due to challenges in modal alignment, Text-to-Speech efficiency and other integration issues. In this paper, we introduce Multimodal Large Language Model Orchestration, an effective approach for creating interactive multimodal AI systems without additional training. MLLM Orchestration leverages the inherent reasoning capabilities of large language models to coordinate specialized models through explicit workflows, enabling natural multimodal interactions while maintaining modularity, improving interpretability, and significantly enhancing computational efficiency. Our orchestration framework is built upon three key innovations: (1) a central controller LLM that analyzes user inputs and dynamically routes tasks to appropriate specialized models through carefully designed agents; (2) a parallel Text-to-Speech architecture that enables true full-duplex interaction with seamless interruption handling and natural conversational flow; and (3) a cross-modal memory integration system that maintains coherent context across modalities through intelligent information synthesis and retrieval, selectively avoiding unnecessary modality calls in certain scenarios to improve response speed. Extensive evaluations demonstrate that MLLM Orchestration achieves comprehensive multimodal capabilities without additional training, performance improvements of up to 7.8% over traditional jointly-trained approaches on standard benchmarks, reduced latency by 10.3%, and significantly enhanced interpretability through explicit orchestration processes.

</details>


### [42] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)

*Sridhar Mahadevan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Categorical Homotopy, Natural Language Processing, Markov Category, Language Generation

**Relevance Score:** 6

**TL;DR:** This paper introduces a categorical homotopy framework for Large Language Models (LLMs) to address issues of generating equivalent statements that are not treated as such, using techniques from higher algebraic K-theory.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to find a way for LLMs to recognize and generate equivalent statements that convey the same meaning, improving their performance in language understanding tasks.

**Method:** The authors develop an LLM Markov category to represent probability distributions of sentences, using categorical homotopy to manage the challenge of non-isomorphic arrows generated by different rephrases.

**Key Contributions:**

	1. Introduction of the LLM Markov category
	2. Application of categorical homotopy techniques to LLMs
	3. Solution to the problem of non-isomorphic arrows in language generation

**Result:** The framework effectively captures 'weak equivalences' in LLMs, offering a new theoretical foundation for tackling language generation issues.

**Limitations:** The complexity of the categorical framework may limit immediate practical application in certain LLM deployments.

**Conclusion:** The study shows that applying categorical homotopy techniques can enhance the capability of LLMs in generating semantically equivalent statements.

**Abstract:** Natural language is replete with superficially different statements, such as ``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the same meaning. Large language models (LLMs) should generate the same next-token probabilities in such cases, but usually do not. Empirical workarounds have been explored, such as using k-NN estimates of sentence similarity to produce smoothed estimates. In this paper, we tackle this problem more abstractly, introducing a categorical homotopy framework for LLMs. We introduce an LLM Markov category to represent probability distributions in language generated by an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is defined by an arrow in a Markov category. However, this approach runs into difficulties as language is full of equivalent rephrases, and each generates a non-isomorphic arrow in the LLM Markov category. To address this fundamental problem, we use categorical homotopy techniques to capture ``weak equivalences" in an LLM Markov category. We present a detailed overview of application of categorical homotopy to LLMs, from higher algebraic K-theory to model categories, building on powerful theoretical results developed over the past half a century.

</details>


### [43] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)

*Li Wang, Changhao Zhang, Zengqi Xiu, Kai Lu, Xin Yu, Kui Zhang, Wenjun Wu*

**Main category:** cs.CL

**Keywords:** Small Language Models, Large Language Models, Decoupled Reasoning, Natural Language Processing, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** The paper introduces DURIT, an algorithm that enhances reasoning in Small Language Models by decoupling understanding from reasoning through a standardized problem space.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Improving reasoning capabilities of Small Language Models (SLMs) is challenging due to the complexity of natural language, which hampers their ability to extract core problems.

**Method:** The DURIT framework employs reinforcement learning to map natural language into a simplified problem space, aligns reasoning trajectories via self-distillation, and iteratively trains reasoning policies.

**Key Contributions:**

	1. Introduction of the DURIT framework for decoupled reasoning in SLMs.
	2. Demonstration of substantial performance improvements in mathematical and logical reasoning tasks.
	3. Validation of the approach's robustness against linguistic variability.

**Result:** DURIT significantly enhances SLMs' performance on various reasoning tasks, demonstrating improved reasoning capabilities and robustness.

**Limitations:** 

**Conclusion:** Decoupling understanding from reasoning proves to be an effective strategy for strengthening SLMs and enhancing their performance on reasoning tasks.

**Abstract:** Despite recent advances in the reasoning capabilities of Large Language Models (LLMs), improving the reasoning ability of Small Language Models (SLMs, e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity and variability of natural language: essentially equivalent problems often appear in diverse surface forms, often obscured by redundant or distracting details. This imposes a dual burden on SLMs: they must first extract the core problem from complex linguistic input, and then perform reasoning based on that understanding. The resulting vast and noisy problem space hinders optimization, particularly for models with limited capacity. To address this, we propose a new framework that decouples understanding from reasoning by mapping natural language problems into a canonical problem space-a semantically simplified yet expressive domain. This enables SLMs to focus on reasoning over standardized inputs, free from linguistic variability. Within this framework, we introduce DURIT (Decoupled Understanding from Reasoning via Iterative Training), a three-step algorithm that iteratively: (1) mapping natural language problems via reinforcement learning, (2) aligns reasoning trajectories through self-distillation, and (3) trains reasoning policies in the problem space. The mapper and reasoner are co-trained in an alternating loop throughout this process. Experiments show that DURIT substantially improves SLMs' performance on both in-domain and out-of-domain mathematical and logical reasoning tasks. Beyond improving reasoning capabilities, DURIT also improves the robustness of reasoning, validating decoupling understanding from reasoning as an effective strategy for strengthening SLMs.

</details>


### [44] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)

*Chuan Li, Qianyi Zhao, Fengran Mo, Cen Chen*

**Main category:** cs.CL

**Keywords:** federated learning, large language models, healthcare applications, interpretability, reasoning accuracy

**Relevance Score:** 9

**TL;DR:** FedCoT is a novel framework designed to enhance reasoning in federated learning environments, particularly in healthcare, by improving reasoning accuracy and robustness while ensuring data privacy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of improving reasoning capabilities of large language models in federated learning, especially in healthcare contexts where interpretability and privacy are critical.

**Method:** FedCoT employs a lightweight chain-of-thought enhancement mechanism that allows local models to generate multiple reasoning paths, with a compact discriminator selecting the most promising path.

**Key Contributions:**

	1. Proposed FedCoT framework for enhanced reasoning in federated learning
	2. Lightweight chain-of-thought mechanism for generating multiple reasoning paths
	3. Improved client heterogeneity aggregation with LoRA module stacking

**Result:** FedCoT demonstrates significant improvements in client-side reasoning performance on medical tasks while maintaining strict resource budgets and preserving data privacy.

**Limitations:** 

**Conclusion:** The proposed approach effectively enhances reasoning accuracy and robustness in federated settings, making it suitable for sensitive medical applications.

**Abstract:** Efficiently enhancing the reasoning capabilities of large language models (LLMs) in federated learning environments remains challenging, particularly when balancing performance gains with strict computational, communication, and privacy constraints. This challenge is especially acute in healthcare, where decisions-spanning clinical, operational, and patient-facing contexts-demand not only accurate outputs but also interpretable, traceable rationales to ensure safety, accountability, and regulatory compliance. Conventional federated tuning approaches on LLM fail to address this need: they optimize primarily for answer correctness while neglecting rationale quality, leaving CoT capabilities dependent on models' innate pre-training abilities. Moreover, existing methods for improving rationales typically rely on privacy-violating knowledge distillation from centralized models. Additionally, the communication overhead in traditional federated fine-tuning on LLMs remains substantial. We addresses this gap by proposing FedCoT, a novel framework specifically designed to enhance reasoning in federated settings. FedCoT leverages a lightweight chain-of-thought enhancement mechanism: local models generate multiple reasoning paths, and a compact discriminator dynamically selects the most promising one. This approach improves reasoning accuracy and robustness while providing valuable interpretability, which is particularly critical for medical applications. To manage client heterogeneity efficiently, we adopt an improved aggregation approach building upon advanced LoRA module stacking, incorporating client classifier-awareness to achieve noise-free aggregation across diverse clients. Comprehensive experiments on medical reasoning tasks demonstrate that FedCoT significantly boosts client-side reasoning performance under stringent resource budgets while fully preserving data privacy.

</details>


### [45] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)

*Egor Fadeev, Dzhambulat Mollaev, Aleksei Shestov, Dima Korolev, Omar Zoloev, Ivan Kireev, Andrey Savchenko, Maksim Makarenko*

**Main category:** cs.CL

**Keywords:** contrastive learning, large language models, client embeddings, financial applications, event sequences

**Relevance Score:** 8

**TL;DR:** LATTE is a contrastive learning framework that improves the efficiency of learning client embeddings from historic communication sequences in financial applications by leveraging LLMs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need to efficiently learn client embeddings from long event sequences in financial contexts while reducing computational costs.

**Method:** LATTE employs a contrastive learning framework that aligns raw event embeddings with semantic embeddings provided by frozen large language models (LLMs) using summary behavioral features as supervision.

**Key Contributions:**

	1. Introduction of LATTE, a novel contrastive learning framework
	2. Demonstrates improved performance over state-of-the-art methods on financial datasets
	3. Reduces computational burden while deploying LLMs for embedding tasks

**Result:** LATTE outperforms existing techniques for learning event sequence representations on actual financial datasets and is suitable for latency-sensitive environments.

**Limitations:** 

**Conclusion:** The proposed method offers a significant reduction in inference cost and input size compared to traditional LLM processing, making it practical for deployment in real-world financial applications.

**Abstract:** Learning clients embeddings from sequences of their historic communications is central to financial applications. While large language models (LLMs) offer general world knowledge, their direct use on long event sequences is computationally expensive and impractical in real-world pipelines. In this paper, we propose LATTE, a contrastive learning framework that aligns raw event embeddings with semantic embeddings from frozen LLMs. Behavioral features are summarized into short prompts, embedded by the LLM, and used as supervision via contrastive loss. The proposed approach significantly reduces inference cost and input size compared to conventional processing of complete sequence by LLM. We experimentally show that our method outperforms state-of-the-art techniques for learning event sequence representations on real-world financial datasets while remaining deployable in latency-sensitive environments.

</details>


### [46] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)

*Yuanchang Ye*

**Main category:** cs.CL

**Keywords:** large language models, conformal prediction, significance testing, multiple-choice question answering, uncertainty quantification

**Relevance Score:** 9

**TL;DR:** This study presents a framework that integrates significance testing with conformal prediction to enhance the reliability of large language models in multiple-choice question answering, addressing issues like hallucination and inaccuracies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the trustworthiness of large language models in multiple-choice question answering due to concerns about hallucination and nonfactual generation compromising response reliability.

**Method:** The framework combines significance testing with conformity scoring using self-consistency resampling of responses, computing option frequencies and constructing prediction sets via null hypothesis testing with empirically derived p-values.

**Key Contributions:**

	1. Integration of significance testing with conformal prediction for LLMs
	2. Empirical validation of the framework on MMLU and MMLU-Pro benchmarks
	3. Establishment of a statistical framework for high-stakes QA applications

**Result:** Evaluations indicate that the enhanced conformal prediction framework achieves user-specified empirical miscoverage rates and that the average prediction set size decreases with higher risk levels, validating its effectiveness as an uncertainty metric.

**Limitations:** 

**Conclusion:** This work establishes a statistically rigorous framework for deploying trustworthy large language models in high-stakes Q&A scenarios.

**Abstract:** This study introduces a significance testing-enhanced conformal prediction (CP) framework to improve trustworthiness of large language models (LLMs) in multiple-choice question answering (MCQA). While LLMs have been increasingly deployed in disciplinary QA scenarios, hallucination and nonfactual generation substantially compromise response reliability. Although CP provides statistically rigorous marginal coverage guarantees for prediction sets, and significance testing offers established statistical rigor, their synergistic integration remains unexplored. To mitigate hallucination and factual inaccuracies, our framework integrates $p$-value computation with conformity scoring through self-consistency resampling of MCQA responses. This approach calculates option frequencies to address LLMs' black-box nature, subsequently constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves user-specified empirical miscoverage rates; (2) Test-set average prediction set size (APSS) decreases monotonically with increasing risk levels ($\alpha$), validating APSS as an effective uncertainty metric. This work establishes a principled statistical framework for trustworthy LLM deployment in high-stakes QA applications.

</details>


### [47] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)

*J. Pablo Muñoz, Jinjie Yuan*

**Main category:** cs.CL

**Keywords:** Test-Time Compute, Large Language Models, Reward-Guided, Retrieval-Augmented Generation, Query-State Caching

**Relevance Score:** 9

**TL;DR:** Introducing Reward-Guided Test-Time Compute (RTTC) to optimize Large Language Models' adaptation strategies for enhanced performance at inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the varying optimal adaptation strategies of Large Language Models (LLMs) across queries and reduce computational overhead from indiscriminate Test-Time Compute (TTC) strategies.

**Method:** RTTC employs a pretrained reward model to adaptively select the most effective TTC strategy for each query, optimizing accuracy while operating in a distributed server-client setup and implementing Query-State Caching for efficiency.

**Key Contributions:**

	1. Introduction of RTTC for adaptive Test-Time Compute.
	2. Implementation of Query-State Caching for efficiency in retrival and adaptation.
	3. Demonstration of superior accuracy across diverse domains compared to traditional methods.

**Result:** RTTC outperforms vanilla RAG or TTT in accuracy, demonstrating the efficacy of adaptive and reward-guided TTC selection across multiple LLMs and benchmarks.

**Limitations:** 

**Conclusion:** RTTC presents a scalable and high-performance approach to language model adaptation, validating the necessity of adaptive strategies in improving model performance.

**Abstract:** Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the performance of Large Language Models (LLMs) at inference, leveraging strategies such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG). However, the optimal adaptation strategy varies across queries, and indiscriminate application of TTC strategy incurs substantial computational overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a novel framework that adaptively selects the most effective TTC strategy for each query via a pretrained reward model, maximizing downstream accuracy across diverse domains and tasks. RTTC operates in a distributed server-client architecture, retrieving relevant samples from a remote knowledge base and applying RAG or lightweight fine-tuning on client devices only when necessary. To further mitigate redundant computation, we propose Query-State Caching, which enables the efficient reuse of historical query states at both retrieval and adaptation levels. Extensive experiments across multiple LLMs and benchmarks demonstrate that RTTC consistently achieves superior accuracy compared to vanilla RAG or TTT, validating the necessity of adaptive, reward-guided TTC selection and the potential of RTTC for scalable, high-performance language model adaptation.

</details>


### [48] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)

*Silvia García-Méndez, Francisco de Arriba-Pérez*

**Main category:** cs.CL

**Keywords:** postpartum depression, natural language processing, machine learning, large language models, screening system

**Relevance Score:** 10

**TL;DR:** An intelligent screening system for postpartum depression (PPD) using NLP, ML, and LLMs enables rapid detection and intervention.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for early detection of postpartum depression to improve mothers' mental and physical health after childbirth.

**Method:** A combination of Natural Language Processing, Machine Learning, and Large Language Models for real-time speech analysis and risk factor identification.

**Key Contributions:**

	1. Development of an intelligent PPD screening system.
	2. Integration of LLMs with interpretable machine learning models.
	3. Real-time, non-invasive analysis using free speech.

**Result:** Achieved 90% accuracy in PPD detection, surpassing existing solutions in the literature.

**Limitations:** 

**Conclusion:** The proposed system facilitates timely assessment and intervention for postpartum depression through interpretable AI models.

**Abstract:** Among the many challenges mothers undergo after childbirth, postpartum depression (PPD) is a severe condition that significantly impacts their mental and physical well-being. Consequently, the rapid detection of ppd and their associated risk factors is critical for in-time assessment and intervention through specialized prevention procedures. Accordingly, this work addresses the need to help practitioners make decisions with the latest technological advancements to enable real-time screening and treatment recommendations. Mainly, our work contributes to an intelligent PPD screening system that combines Natural Language Processing, Machine Learning (ML), and Large Language Models (LLMs) towards an affordable, real-time, and non-invasive free speech analysis. Moreover, it addresses the black box problem since the predictions are described to the end users thanks to the combination of LLMs with interpretable ml models (i.e., tree-based algorithms) using feature importance and natural language. The results obtained are 90 % on ppd detection for all evaluation metrics, outperforming the competing solutions in the literature. Ultimately, our solution contributes to the rapid detection of PPD and their associated risk factors, critical for in-time and proper assessment and intervention.

</details>


### [49] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)

*Kai Zhao, Yanjun Zhao, Jiaming Song, Shien He, Lusheng Zhang, Qiang Zhang, Tianjiao Li*

**Main category:** cs.CL

**Keywords:** Large language models, Reinforcement learning, Efficient reasoning, Token-budgeted reasoning, Machine learning

**Relevance Score:** 9

**TL;DR:** SABER is a reinforcement learning framework for efficient reasoning in large language models, allowing user-controlled inference with flexible budget tiers to optimize accuracy and latency.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with high inference costs and latency when applied uniformly, necessitating a more efficient reasoning approach.

**Method:** SABER profiles token usage for training examples, assigns budget tiers, and employs system prompts and length-aware rewards during fine-tuning, alongside no-think examples for reliability.

**Key Contributions:**

	1. Introduces a user-controllable and token-budgeted reasoning framework for LLMs.
	2. Demonstrates effective cross-domain generalization and graceful degradation in performance.
	3. Achieves significant improvements in accuracy and latency with distinct inference modes.

**Result:** SABER achieves high accuracy under restricted budgets, with SABER-FastThink reducing reasoning length by 65.4% while improving accuracy by 3.6% on the MATH benchmark.

**Limitations:** 

**Conclusion:** SABER provides flexible reasoning controls that enhance performance and reduce latency across a range of tasks and benchmarks.

**Abstract:** Large language models (LLMs) empowered by chain-of-thought reasoning have achieved impressive accuracy on complex tasks but suffer from excessive inference costs and latency when applied uniformly to all problems. We propose SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a reinforcement learning framework that endows LLMs with user-controllable, token-budgeted reasoning. SABER first profiles each training example's base-model thinking token usage and assigns it to one of the predefined budget tiers. During fine-tuning, the model is guided by system prompts and length-aware rewards to respect its assigned budget. In parallel, we incorporate no-think examples to ensure the model remains reliable even when explicit reasoning is turned off. SABER further supports four discrete inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling flexible trade-offs between latency and reasoning depth. Extensive evaluations on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning (LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight budgets, graceful degradation, and effective cross-scale and cross-domain generalization. In particular, SABER-FastThink cuts reasoning length by 65.4% and yields a 3.6% accuracy gain compared with the base model on the MATH benchmark.

</details>


### [50] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)

*Ali Zolnour, Hossein Azadmaleki, Yasaman Haghbin, Fatemeh Taherinezhad, Mohamad Javad Momeni Nezhad, Sina Rashidi, Masoud Khani, AmirSajjad Taleban, Samin Mahdizadeh Sani, Maryam Dadkhah, James M. Noble, Suzanne Bakken, Yadollah Yaghoobzadeh, Abdol-Hossein Vahabie, Masoud Rouhizadeh, Maryam Zolnoori*

**Main category:** cs.CL

**Keywords:** Alzheimer's disease, natural language processing, large language models, cognitive decline, speech analysis

**Relevance Score:** 9

**TL;DR:** This paper explores the use of NLP and LLMs to improve early detection of Alzheimer's disease through a novel screening pipeline that combines transformer embeddings, synthetic speech augmentation, and multimodal classification methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of early detection of Alzheimer's disease and related dementias, as over half of affected older adults remain undiagnosed. The motivation is to leverage speech-based NLP to identify linguistic markers indicative of cognitive decline.

**Method:** Developed a screening pipeline that fuses transformer embeddings with handcrafted linguistic features, tests data augmentation using synthetic speech generated by LLMs, and benchmarks various unimodal and multimodal classifiers for ADRD detection using existing speech transcripts.

**Key Contributions:**

	1. Development of a novel screening pipeline for ADRD detection using NLP and LLMs.
	2. Combination of transformer embeddings with handcrafted linguistic features for improved performance.
	3. Demonstration of the efficacy of synthetic speech augmentation in enhancing model accuracy.

**Result:** The fusion model outperformed baselines with an F1 score of 83.3 and AUC of 89.5. Augmenting training data with synthetic speech improved the F1 score to 85.7, while fine-tuning LLM classifiers resulted in significant performance boosts, though multimodal models underperformed compared to expectations.

**Limitations:** Current multimodal models showed lower performance than expected, indicating a need for further development in this area.

**Conclusion:** Integrating transformer embeddings with linguistic features significantly enhances ADRD detection from speech. Clinically tuned LLMs are effective for classification and data augmentation, but further improvements are required in multimodal modeling.

**Abstract:** Alzheimer's disease and related dementias (ADRD) affect approximately five million older adults in the U.S., yet over half remain undiagnosed. Speech-based natural language processing (NLP) offers a promising, scalable approach to detect early cognitive decline through linguistic markers.   To develop and evaluate a screening pipeline that (i) fuses transformer embeddings with handcrafted linguistic features, (ii) tests data augmentation using synthetic speech generated by large language models (LLMs), and (iii) benchmarks unimodal and multimodal LLM classifiers for ADRD detection.   Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used. Ten transformer models were evaluated under three fine-tuning strategies. A fusion model combined embeddings from the top-performing transformer with 110 lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B, Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic speech, which was used to augment training data. Three multimodal models (GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in zero-shot and fine-tuned settings.   The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B synthetic speech increased F1 to 85.7. Fine-tuning significantly improved unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen = 66.0). Performance gains aligned with the distributional similarity between synthetic and real speech.   Integrating transformer embeddings with linguistic features enhances ADRD detection from speech. Clinically tuned LLMs effectively support both classification and data augmentation, while further advancement is needed in multimodal modeling.

</details>


### [51] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)

*Xiao Fu, Hossein A. Rahmani, Bin Wu, Jerome Ramos, Emine Yilmaz, Aldo Lipani*

**Main category:** cs.CL

**Keywords:** Personalized evaluation, Text generation, Large language models

**Relevance Score:** 9

**TL;DR:** PREF is a new evaluation framework for personalized text generation that measures output quality and user alignment without requiring personal references.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of user individuality in current evaluation methods for personalized text generation.

**Method:** PREF operates in a three-step process: generate universal guidelines using an LLM, re-rank these using user profiles, and score against a personalized rubric.

**Key Contributions:**

	1. Introduction of PREF framework for personalized evaluation
	2. Three-step evaluation process
	3. Demonstrated improved accuracy and calibration over baselines

**Result:** PREF outperforms traditional methods in accuracy and alignment with human judgments on the PrefEval benchmark.

**Limitations:** 

**Conclusion:** PREF improves the evaluation of personalized language generation by enabling scalable and interpretable methods, fostering better user alignment.

**Abstract:** Personalised text generation is essential for user-centric information systems, yet most evaluation methods overlook the individuality of users. We introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free \textbf{E}valuation \textbf{F}ramework that jointly measures general output quality and user-specific alignment without requiring gold personalised references. PREF operates in a three-step pipeline: (1) a coverage stage uses a large language model (LLM) to generate a comprehensive, query-specific guideline covering universal criteria such as factuality, coherence, and completeness; (2) a preference stage re-ranks and selectively augments these factors using the target user's profile, stated or inferred preferences, and context, producing a personalised evaluation rubric; and (3) a scoring stage applies an LLM judge to rate candidate answers against this rubric, ensuring baseline adequacy while capturing subjective priorities. This separation of coverage from preference improves robustness, transparency, and reusability, and allows smaller models to approximate the personalised quality of larger ones. Experiments on the PrefEval benchmark, including implicit preference-following tasks, show that PREF achieves higher accuracy, better calibration, and closer alignment with human judgments than strong baselines. By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the groundwork for more reliable assessment and development of personalised language generation systems.

</details>


### [52] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)

*Wenpeng Xing, Mohan Li, Chunqiang Hu, Haitao XuNingyu Zhang, Bo Lin, Meng Han*

**Main category:** cs.CL

**Keywords:** large language models, jailbreak attacks, adversarial training, natural language processing, machine learning

**Relevance Score:** 7

**TL;DR:** This paper presents Latent Fusion Jailbreak (LFJ), a powerful attack method exploiting large language models through gradient-guided interpolation of hidden states from query pairs to induce prohibited responses, achieving high attack success rates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerability of large language models to jailbreak attacks that bypass safety measures by exploiting their language processing capabilities.

**Method:** LFJ employs a technique where thematic and syntactic similarities between harmful and benign query pairs are leveraged. It uses gradient-guided interpolation at specific layers and tokens, followed by optimization to enhance attack efficiency while maintaining output fluency.

**Key Contributions:**

	1. Introduction of Latent Fusion Jailbreak (LFJ) as a novel attack method.
	2. Demonstration of superior average attack success rates (94.01%) compared to existing methods.
	3. Proposal of an effective adversarial training defense that significantly reduces attack success without degrading benign performance.

**Result:** LFJ achieves an average attack success rate of 94.01% across tested models such as Vicuna and LLaMA-2, surpassing other existing methods, and a proposed adversarial training defense reduces the success rate by over 80% without harming benign input performance.

**Limitations:** The paper does not explore the long-term implications of such attacks in real-world applications or assess other potential mitigation strategies beyond adversarial training.

**Conclusion:** The study confirms LFJ's effectiveness and highlights the potential of adversarial training to mitigate its impact, emphasizing the significance of the chosen query pairs and the optimization strategies employed in the attack.

**Abstract:** Large language models (LLMs) demonstrate impressive capabilities in various language tasks but are susceptible to jailbreak attacks that circumvent their safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a representation-based attack that interpolates hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ begins by selecting query pairs with high thematic and syntactic similarity, then performs gradient-guided interpolation at influential layers and tokens, followed by optimization to balance attack success, output fluency, and computational efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks like AdvBench and MaliciousInstruct yield an average attack success rate (ASR) of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an adversarial training defense that fine-tunes models on interpolated examples, reducing ASR by over 80% without degrading performance on benign inputs. Ablation studies validate the importance of query pair selection, hidden state interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [53] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)

*Saaduddin Mahmud, Mason Nakamura, Kyle H. Wray, Shlomo Zilberstein*

**Main category:** cs.CL

**Keywords:** Prompt Optimization, Large Language Models, Inference Strategies, AI Alignment, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces IAPO, a framework for optimizing prompts and inference strategies in large language models, highlighting their interdependence and user preferences in alignment.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Existing prompt optimization methods do not consider the inference strategy used during deployment, creating a methodological gap in aligning black-box LLMs.

**Method:** The paper proposes a unified framework called IAPO (Inference-Aware Prompt Optimization) that co-optimizes prompts and inference scale, taking into account inference budgets and task objectives. A fixed-budget training algorithm known as PSST (Prompt Scaling via Sequential Trimming) is also developed.

**Key Contributions:**

	1. Introduction of IAPO framework for joint optimization of prompts and inference strategies.
	2. Development of PSST algorithm with finite-budget guarantees.
	3. Empirical evaluations demonstrating effectiveness across multiple tasks.

**Result:** The PSST algorithm is evaluated on six tasks, showing that incorporating inference-awareness significantly enhances the alignment and performance of LLMs.

**Limitations:** 

**Conclusion:** The findings emphasize the importance of jointly optimizing prompts and inference strategies to better align large language models with user preferences and constraints.

**Abstract:** Prompt optimization methods have demonstrated significant effectiveness in aligning black-box large language models (LLMs). In parallel, inference scaling strategies such as Best-of-N Sampling and Majority Voting have also proven to enhance alignment and performance by trading off computation. However, existing prompt optimization approaches are inference strategy agnostic; that is, they optimize prompts without regard to the inference strategy employed during deployment. This constitutes a significant methodological gap, as our empirical and theoretical analysis reveals a strong interdependence between these two paradigms. Moreover, we find that user preferences regarding trade-offs among multiple objectives and inference budgets substantially influence the choice of prompt and inference configuration. To address this gap, we introduce a unified novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly optimizes the prompt and inference scale, while being aware of the inference budget and different task objectives. We then develop a fixed-budget training algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential Trimming), and analyze finite-budget guarantees on error probability. Finally, we evaluate the effectiveness of PSST on six different tasks, including multi-objective text generation and reasoning, and demonstrate the critical role of incorporating inference-awareness when aligning black-box LLMs through prompt optimization.

</details>


### [54] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)

*Fan Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Jailbreak attack, Thinking mode, Safety intervention, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper investigates the vulnerability of LLMs in thinking mode to Jailbreak attacks and proposes a method to mitigate this issue.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research highlights the vulnerability of LLMs in thinking mode, which has been previously overlooked, and seeks to address the security concerns posed by Jailbreak attacks.

**Method:** The authors evaluate 9 LLMs on AdvBench and HarmBench to assess the success rate of Jailbreak attacks in thinking versus non-thinking modes and propose a 'safe thinking intervention' using specific tokens in prompts.

**Key Contributions:**

	1. Evaluation of 9 LLMs revealing vulnerabilities in thinking mode
	2. Identification of harmful characteristics in thinking mode during Jailbreak attacks
	3. Proposition of a 'safe thinking intervention' method to enhance LLM security

**Result:** The study finds that thinking mode LLMs have a higher success rate of attacks, and the integration of specific thinking tokens significantly reduces this vulnerability.

**Limitations:** 

**Conclusion:** The introduction of safe thinking intervention can effectively enhance the security of LLMs operating in thinking mode.

**Abstract:** Thinking mode has always been regarded as one of the most valuable modes in LLMs. However, we uncover a surprising and previously overlooked phenomenon: LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate 9 LLMs on AdvBench and HarmBench and find that the success rate of attacking thinking mode in LLMs is almost higher than that of non-thinking mode. Through large numbers of sample studies, it is found that for educational purposes and excessively long thinking lengths are the characteristics of successfully attacked data, and LLMs also give harmful answers when they mostly know that the questions are harmful. In order to alleviate the above problems, this paper proposes a method of safe thinking intervention for LLMs, which explicitly guides the internal thinking processes of LLMs by adding "specific thinking tokens" of LLMs to the prompt. The results demonstrate that the safe thinking intervention can significantly reduce the attack success rate of LLMs with thinking mode.

</details>


### [55] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)

*Dong Zhao, Yadong Wang, Xiang Chen, Chenxi Wang, Hongliang Dai, Chuanxing Geng, Shengzhong Zhang, Shaoyuan Li, Sheng-Jun Huang*

**Main category:** cs.CL

**Keywords:** Active Prompting, Information Extraction, Large Language Models, Model Uncertainty, Few-shot Learning

**Relevance Score:** 9

**TL;DR:** Introduction of Active Prompting for Information Extraction (APIE) framework that enhances few-shot information extraction by evaluating model uncertainty.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Address the sensitivity of LLM performance in few-shot information extraction to the choice of in-context examples and confusion from format generation.

**Method:** Developed a dual-component uncertainty metric to assess both Format Uncertainty and Content Uncertainty in LLM outputs, guiding sample selection for few-shot learning.

**Key Contributions:**

	1. Introduction of introspective confusion principle for model assessment
	2. Development of a dual-component uncertainty metric
	3. Demonstrated performance improvements on multiple benchmarks

**Result:** APIE significantly improves extraction accuracy and robustness compared to strong baselines across four benchmarks.

**Limitations:** 

**Conclusion:** A dual-level understanding of model uncertainty is crucial for effective structured generation systems in information extraction tasks.

**Abstract:** Large Language Models (LLMs) show remarkable potential for few-shot information extraction (IE), yet their performance is highly sensitive to the choice of in-context examples. Conventional selection strategies often fail to provide informative guidance, as they overlook a key source of model fallibility: confusion stemming not just from semantic content, but also from the generation of well-structured formats required by IE tasks. To address this, we introduce Active Prompting for Information Extraction (APIE), a novel active prompting framework guided by a principle we term introspective confusion. Our method empowers an LLM to assess its own confusion through a dual-component uncertainty metric that uniquely quantifies both Format Uncertainty (difficulty in generating correct syntax) and Content Uncertainty (inconsistency in extracted semantics). By ranking unlabeled data with this comprehensive score, our framework actively selects the most challenging and informative samples to serve as few-shot exemplars. Extensive experiments on four benchmarks show that our approach consistently outperforms strong baselines, yielding significant improvements in both extraction accuracy and robustness. Our work highlights the critical importance of a fine-grained, dual-level view of model uncertainty when it comes to building effective and reliable structured generation systems.

</details>


### [56] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)

*Nghia Trung Ngo, Franck Dernoncourt, Thien Huu Nguyen*

**Main category:** cs.CL

**Keywords:** Reasoning, Large Language Models, Commonsense, Multilingual, Benchmark

**Relevance Score:** 8

**TL;DR:** The paper introduces mSCoRe, a benchmark for evaluating multilingual commonsense reasoning skills in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how reasoning-reinforced LLMs utilize human reasoning skills across languages in commonsense reasoning tasks.

**Method:** The benchmark consists of a novel taxonomy of reasoning skills, a robust data synthesis pipeline, and a complexity scaling framework for dynamic evaluation.

**Key Contributions:**

	1. Introduction of a novel taxonomy of reasoning skills for fine-grained analysis
	2. Development of a tailored data synthesis pipeline for commonsense reasoning
	3. Creation of a complexity scaling framework for dynamic task evaluation

**Result:** The mSCoRe benchmark was extensively tested on eight state-of-the-art LLMs, revealing significant challenges at higher complexity levels, particularly in multilingual commonsense reasoning.

**Limitations:** The benchmark may not fully address all nuances of cultural commonsense reasoning across languages.

**Conclusion:** The study underscores the limitations of current LLMs in multilingual reasoning and suggests directions for improving such capabilities.

**Abstract:** Recent advancements in reasoning-reinforced Large Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks. However, the mechanism underlying their utilization of different human reasoning skills remains poorly investigated, especially for multilingual commonsense reasoning that involves everyday knowledge across different languages and cultures. To address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for \textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}). Our benchmark incorporates three key components that are designed to systematically evaluate LLM's reasoning capabilities, including: (1) a novel taxonomy of reasoning skills that enables fine-grained analysis of models' reasoning processes, (2) a robust data synthesis pipeline tailored specifically for commonsense reasoning evaluation, and (3) a complexity scaling framework allowing task difficulty to scale dynamically alongside future improvements in LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying sizes and training approaches demonstrate that \textbf{mSCoRe} remains significantly challenging for current models, particularly at higher complexity levels. Our results reveal the limitations of such reasoning-reinforced models when confronted with nuanced multilingual general and cultural commonsense. We further provide detailed analysis on the models' reasoning processes, suggesting future directions for improving multilingual commonsense reasoning capabilities.

</details>


### [57] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)

*Kartikeya Badola, Jonathan Simon, Arian Hosseini, Sara Marie Mc Carthy, Tsendsuren Munkhdalai, Abhimanyu Goyal, Tomáš Kočiský, Shyam Upadhyay, Bahare Fatemi, Mehran Kazemi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-turn Dialogue, Benchmarking, Reasoning, Information Seeking

**Relevance Score:** 9

**TL;DR:** The paper presents a benchmark for evaluating large language models' abilities in multi-turn dialogue, reasoning, and information-seeking tasks, highlighting their current limitations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for LLMs to engage in complex, interactive scenarios that mirror real-world tasks and interactions.

**Method:** Introduction of a benchmark with multi-turn tasks designed for testing reasoning, interactive dialogue, and information-seeking capabilities, featuring deterministic scoring to eliminate human assessment.

**Key Contributions:**

	1. Introduction of a new benchmark for LLM evaluation
	2. Deterministic scoring system for multi-turn dialogue tasks
	3. Insights into specific areas of LLM deficiencies

**Result:** Evaluation of frontier models shows significant performance gaps, with common errors linked to instruction following, reasoning, and planning.

**Limitations:** 

**Conclusion:** The benchmark offers insights into LLMs' strengths and weaknesses and lays groundwork for future enhancements in their capabilities.

**Abstract:** Large language models (LLMs) excel at solving problems with clear and complete statements, but often struggle with nuanced environments or interactive tasks which are common in most real-world scenarios. This highlights the critical need for developing LLMs that can effectively engage in logically consistent multi-turn dialogue, seek information and reason with incomplete data. To this end, we introduce a novel benchmark comprising a suite of multi-turn tasks each designed to test specific reasoning, interactive dialogue, and information-seeking abilities. These tasks have deterministic scoring mechanisms, thus eliminating the need for human intervention. Evaluating frontier models on our benchmark reveals significant headroom. Our analysis shows that most errors emerge from poor instruction following, reasoning failures, and poor planning. This benchmark provides valuable insights into the strengths and weaknesses of current LLMs in handling complex, interactive scenarios and offers a robust platform for future research aimed at improving these critical capabilities.

</details>


### [58] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)

*Gal Amram, Eitan Farchi, Shmulik Froimovich, Raviv Gal, Avi Ziv*

**Main category:** cs.CL

**Keywords:** Large Language Models, meta-evaluation, evaluation metrics, synthetic data, NLP

**Relevance Score:** 9

**TL;DR:** This paper introduces LaaJMeter, a simulation-based framework for validating evaluation metrics of large language model evaluators in domain-specific contexts, particularly when annotated data is scarce.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of evaluating large language model evaluators (LaaJs) in domain-specific contexts where annotated data is limited and metrics are unvalidated.

**Method:** LaaJMeter generates synthetic data to simulate virtual models and judges, allowing systematic analysis of evaluation metrics under realistic conditions.

**Key Contributions:**

	1. Introduction of LaaJMeter for meta-evaluation of LaaJs
	2. Demonstration of its utility in a real-world task
	3. Insights into the sensitivity of evaluation metrics to evaluator quality.

**Result:** Demonstrated LaaJMeter's utility in a code translation task, revealing significant variations in metric sensitivity to evaluator quality and identifying limitations in common evaluation metrics.

**Limitations:** The framework relies on synthetic data, which may not capture all complexities of real-world scenarios.

**Conclusion:** LaaJMeter offers a scalable and extensible solution for validating LaaJ metrics in low-resource settings, contributing to more reliable evaluation processes in NLP.

**Abstract:** Large Language Models (LLMs) are increasingly used as evaluators in natural language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While effective in general domains, LaaJs pose significant challenges in domain-specific contexts, where annotated data is scarce and expert evaluation is costly. In such cases, meta-evaluation is often performed using metrics that have not been validated for the specific domain in which they are applied. As a result, it becomes difficult to determine which metrics effectively identify LaaJ quality, and further, what threshold indicates sufficient evaluator performance. In this work, we introduce LaaJMeter, a simulation-based framework for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to generate synthetic data representing virtual models and judges, allowing systematic analysis of evaluation metrics under realistic conditions. This helps practitioners validate and refine LaaJs for specific evaluation tasks: they can test whether their metrics correctly distinguish between better and worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator adequacy.   We demonstrate the utility of LaaJMeter in a code translation task involving a legacy programming language, showing how different metrics vary in sensitivity to evaluator quality. Our results highlight the limitations of common metrics and the importance of principled metric selection. LaaJMeter provides a scalable and extensible solution for assessing LaaJs in low-resource settings, contributing to the broader effort to ensure trustworthy and reproducible evaluation in NLP.

</details>


### [59] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)

*Lorenzo Proietti, Stefano Perrella, Vilém Zouhar, Roberto Navigli, Tom Kocmi*

**Main category:** cs.CL

**Keywords:** machine translation, difficulty estimation, evaluation metrics, Sentinel-src

**Relevance Score:** 6

**TL;DR:** The paper introduces a new metric for estimating translation difficulty and presents models that identify texts challenging for machine translation systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** High-quality machine translations make it hard to evaluate models and identify areas for improvement; hence, estimating translation difficulty can guide future research.

**Method:** The authors formalize the task of translation difficulty estimation, introduce a new evaluation metric, and assess baseline and novel approaches using this metric.

**Key Contributions:**

	1. Introduction of a new metric for evaluating translation difficulty estimators
	2. Development of dedicated models (Sentinel-src) that outperform existing methods
	3. Construction of more challenging machine translation benchmarks using difficulty estimators

**Result:** Dedicated models (Sentinel-src) outperform heuristic-based methods and LLM-as-a-judge approaches in estimating translation difficulty, leading to improved machine translation benchmarks.

**Limitations:** 

**Conclusion:** The released models, Sentinel-src-24 and Sentinel-src-25, enhance the ability to identify difficult texts for translation, aiding in the development of more effective machine translation systems.

**Abstract:** Machine translation quality has began achieving near-perfect translations in some setups. These high-quality outputs make it difficult to distinguish between state-of-the-art models and to identify areas for future improvement. Automatically identifying texts where machine translation systems struggle holds promise for developing more discriminative evaluations and guiding future research.   We formalize the task of translation difficulty estimation, defining a text's difficulty based on the expected quality of its translations. We introduce a new metric to evaluate difficulty estimators and use it to assess both baselines and novel approaches. Finally, we demonstrate the practical utility of difficulty estimators by using them to construct more challenging machine translation benchmarks. Our results show that dedicated models (dubbed Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or syntactic complexity) and LLM-as-a-judge approaches. We release two improved models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which can be used to scan large collections of texts and select those most likely to challenge contemporary machine translation systems.

</details>


### [60] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)

*Wenlong Deng, Jiaming Zhang, Qi Zeng, Christos Thrampoulidis, Boying Gong, Xiaoxiao Li*

**Main category:** cs.CL

**Keywords:** data valuation, influence estimation, large language models

**Relevance Score:** 9

**TL;DR:** Introduction of For-Value, a computationally efficient data valuation framework for estimating influence in large language and vision-language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Enhance transparency and accountability of large language models by quantifying influence of individual training samples.

**Method:** For-Value is a forward-only data valuation framework that computes influence scores based on a single forward pass without requiring gradients or model retraining.

**Key Contributions:**

	1. Introduces For-Value for scalable influence estimation
	2. Avoids costly gradient calculations
	3. Demonstrates effectiveness on large models

**Result:** For-Value successfully matches or outperforms gradient-based methods in identifying impactful fine-tuning examples and detecting mislabeled data.

**Limitations:** 

**Conclusion:** The framework provides an efficient means of influence estimation, facilitating better understanding and management of training data in large models.

**Abstract:** Quantifying the influence of individual training samples is essential for enhancing the transparency and accountability of large language models (LLMs) and vision-language models (VLMs). However, existing data valuation methods often rely on Hessian information or model retraining, making them computationally prohibitive for billion-parameter models. In this work, we introduce For-Value, a forward-only data valuation framework that enables scalable and efficient influence estimation for both LLMs and VLMs. By leveraging the rich representations of modern foundation models, For-Value computes influence scores using a simple closed-form expression based solely on a single forward pass, thereby eliminating the need for costly gradient computations. Our theoretical analysis demonstrates that For-Value accurately estimates per-sample influence by capturing alignment in hidden representations and prediction errors between training and validation samples. Extensive experiments show that For-Value matches or outperforms gradient-based baselines in identifying impactful fine-tuning examples and effectively detecting mislabeled data.

</details>


### [61] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)

*Abdullah Hashmat, Muhammad Arham Mirza, Agha Ali Raza*

**Main category:** cs.CL

**Keywords:** Bias Mitigation, Large Language Models, Low-resource languages, PakBBQ, Fairness

**Relevance Score:** 9

**TL;DR:** The paper introduces PakBBQ, a new bias benchmark dataset that focuses on fairness in LLMs for low-resource languages, specifically English and Urdu, revealing significant findings on how context and question framing affect bias.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure fairness across user communities, particularly for low-resource languages, by addressing the Western centric bias in LLM training data.

**Method:** The authors developed PakBBQ, a dataset with 214 templates and 17180 QA pairs in English and Urdu, evaluating multiple multilingual LLMs on the dataset to observe performance under various contextual conditions.

**Key Contributions:**

	1. Introduction of the PakBBQ dataset for low-resource languages
	2. Evaluation of bias in multilingual LLMs
	3. Demonstrated effects of disambiguation and question framing on bias.

**Result:** Experiments showed a 12% accuracy gain with disambiguation, stronger counter bias behaviors in Urdu, and reduced stereotypical responses with negative question framing.

**Limitations:** 

**Conclusion:** Contextualized benchmarks and prompt engineering are crucial for mitigating bias in low-resource settings.

**Abstract:** With the widespread adoption of Large Language Models (LLMs) across various applications, it is empirical to ensure their fairness across all user communities. However, most LLMs are trained and evaluated on Western centric data, with little attention paid to low-resource languages and regional contexts. To address this gap, we introduce PakBBQ, a culturally and regionally adapted extension of the original Bias Benchmark for Question Answering (BBQ) dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8 categories in both English and Urdu, covering eight bias dimensions including age, disability, appearance, gender, socio-economic status, religious, regional affiliation, and language formality that are relevant in Pakistan. We evaluate multiple multilingual LLMs under both ambiguous and explicitly disambiguated contexts, as well as negative versus non negative question framings. Our experiments reveal (i) an average accuracy gain of 12\% with disambiguation, (ii) consistently stronger counter bias behaviors in Urdu than in English, and (iii) marked framing effects that reduce stereotypical responses when questions are posed negatively. These findings highlight the importance of contextualized benchmarks and simple prompt engineering strategies for bias mitigation in low resource settings.

</details>


### [62] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)

*Igor Halperin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Semantic Divergence, Hallucinations, Natural Language Processing, Information Theory

**Relevance Score:** 9

**TL;DR:** This paper presents a novel framework called Semantic Divergence Metrics (SDM) to detect Faithfulness Hallucinations in Large Language Models (LLMs), focusing on confabulations that deviate from the input context.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge posed by hallucinations in LLMs, which generate responses that are non-factual or nonsensical.

**Method:** The SDM framework employs joint clustering on sentence embeddings to analyze semantic consistency across multiple paraphrases of a prompt, using information-theoretic metrics to quantify semantic divergence.

**Key Contributions:**

	1. Introduction of Semantic Divergence Metrics (SDM) for detecting Faithfulness Hallucinations.
	2. Joint clustering approach utilizing sentence embeddings for better prompt-response analysis.
	3. Development of the Semantic Box framework for classifying LLM response types.

**Result:** SDM framework demonstrates improved detection of Faithfulness hallucinations by providing a more nuanced analysis of response consistency and semantic alignment.

**Limitations:** The framework's effectiveness may vary across different types of LLMs and prompts, requiring further validation.

**Conclusion:** The proposed metrics and the Semantic Box framework enhance the understanding of LLM behaviors, particularly in identifying potentially dangerous response types like confident confabulations.

**Abstract:** The proliferation of Large Language Models (LLMs) is challenged by hallucinations, critical failure modes where models generate non-factual, nonsensical or unfaithful text. This paper introduces Semantic Divergence Metrics (SDM), a novel lightweight framework for detecting Faithfulness Hallucinations -- events of severe deviations of LLMs responses from input contexts. We focus on a specific implementation of these LLM errors, {confabulations, defined as responses that are arbitrary and semantically misaligned with the user's query. Existing methods like Semantic Entropy test for arbitrariness by measuring the diversity of answers to a single, fixed prompt. Our SDM framework improves upon this by being more prompt-aware: we test for a deeper form of arbitrariness by measuring response consistency not only across multiple answers but also across multiple, semantically-equivalent paraphrases of the original prompt. Methodologically, our approach uses joint clustering on sentence embeddings to create a shared topic space for prompts and answers. A heatmap of topic co-occurances between prompts and responses can be viewed as a quantified two-dimensional visualization of the user-machine dialogue. We then compute a suite of information-theoretic metrics to measure the semantic divergence between prompts and responses. Our practical score, $\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein distance to quantify this divergence, with a high score indicating a Faithfulness hallucination. Furthermore, we identify the KL divergence KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic Exploration}, a key signal for distinguishing different generative behaviors. These metrics are further combined into the Semantic Box, a diagnostic framework for classifying LLM response types, including the dangerous, confident confabulation.

</details>


### [63] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)

*Ethan Gordon, Nishank Kuppa, Rigved Tummala, Sriram Anasuri*

**Main category:** cs.CL

**Keywords:** emoji prediction, deep learning, BERT, CNN, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This project investigates emoji prediction using various deep learning architectures and demonstrates the effectiveness of BERT and CNN in addressing class imbalances in emoji classification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to improve human-computer interaction by enhancing emoji prediction accuracy in short text sequences, particularly for sentiment recognition.

**Method:** The study employs a feed-forward network, CNN, transformer, and BERT architectures on the TweetEval dataset, utilizing focal loss and regularization techniques to manage class imbalance.

**Key Contributions:**

	1. Exploration of various deep learning architectures for emoji prediction
	2. Demonstration of BERT's performance advantage
	3. Identification of CNN's effectiveness for rare class prediction

**Result:** BERT outperforms other models in overall performance, while CNN excels in predicting rare emoji classes.

**Limitations:** 

**Conclusion:** The findings underscore the significance of selecting appropriate architectures and tuning hyperparameters for sentiment-aware emoji prediction, which can enhance user interactions with technology.

**Abstract:** This project explores emoji prediction from short text sequences using four deep learning architectures: a feed-forward network, CNN, transformer, and BERT. Using the TweetEval dataset, we address class imbalance through focal loss and regularization techniques. Results show BERT achieves the highest overall performance due to its pre-training advantage, while CNN demonstrates superior efficacy on rare emoji classes. This research shows the importance of architecture selection and hyperparameter tuning for sentiment-aware emoji prediction, contributing to improved human-computer interaction.

</details>


### [64] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)

*Andrew X. Chen, Guillermo Horga, Sean Escola*

**Main category:** cs.CL

**Keywords:** schizophrenia, LLMs, BPRS, clinical assessment, health informatics

**Relevance Score:** 9

**TL;DR:** The paper explores using large language models (LLMs) to predict Brief Psychiatric Rating Scale (BPRS) scores from clinical interviews in patients at clinical high risk (CHR) for schizophrenia, demonstrating high accuracy and potential for improved monitoring.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for efficient and accurate tools to monitor symptoms in patients at clinical high risk for schizophrenia, as traditional methods are time-consuming and infrequently used in clinical practice.

**Method:** LLMs were employed to predict BPRS scores from unstructured clinical interview transcripts of 409 CHR patients, without the need for tailored structured interviews.

**Key Contributions:**

	1. LLMs can accurately predict BPRS scores from unstructured clinical interviews.
	2. The study demonstrates the effectiveness of LLMs in both English and foreign languages.
	3. Longitudinal information can be effectively integrated into predictions using one-shot or few-shot learning approaches.

**Result:** The LLMs achieved a median concordance of 0.84 with the true BPRS assessment, indicating their predictions closely match human raters, with improved performance in assessing BPRS in multiple languages.

**Limitations:** The study relies on the quality of transcripts and may not generalize to all clinical settings or patient populations.

**Conclusion:** LLMs show significant promise in enhancing and standardizing the assessment of CHR patients, making the process more efficient and potentially applicable across different languages and contexts.

**Abstract:** Patients who are at clinical high risk (CHR) for schizophrenia need close monitoring of their symptoms to inform appropriate treatments. The Brief Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for measuring symptoms in patients with schizophrenia and other psychotic disorders; however, it is not commonly used in clinical practice as it requires a lengthy structured interview. Here, we utilize large language models (LLMs) to predict BPRS scores from clinical interview transcripts in 409 CHR patients from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort. Despite the interviews not being specifically structured to measure the BPRS, the zero-shot performance of the LLM predictions compared to the true assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and intra-rater reliability. We further demonstrate that LLMs have substantial potential to improve and standardize the assessment of CHR patients via their accuracy in assessing the BPRS in foreign languages (median concordance: 0.88, ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot learning approach.

</details>


### [65] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)

*Daniel Huang, Hyoun-A Joo*

**Main category:** cs.CL

**Keywords:** Toki Pona, language change, sociolinguistics, constructed languages, corpus-based analysis

**Relevance Score:** 2

**TL;DR:** This study examines the evolution and variation of the constructed language Toki Pona through a computational approach, focusing on word usage and syntactic changes.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To explore language change and variation in constructed languages, specifically Toki Pona, and understand how sociolinguistic factors impact their evolution.

**Method:** A computational and corpus-based approach analyzing features such as fluid word classes and transitivity.

**Key Contributions:**

	1. Insights into how constructed languages can evolve like natural languages
	2. Identification of sociolinguistic factors influencing language change
	3. Analysis of word class fluidity and transitivity in Toki Pona

**Result:** Time-based changes in content word preferences for syntactic positions and variations across different corpora were identified, indicating sociolinguistic influences.

**Limitations:** 

**Conclusion:** Constructed languages like Toki Pona evolve similarly to natural languages, influenced by community usage over time.

**Abstract:** This study explores language change and variation in Toki Pona, a constructed language with approximately 120 core words. Taking a computational and corpus-based approach, the study examines features including fluid word classes and transitivity in order to examine (1) changes in preferences of content words for different syntactic positions over time and (2) variation in usage across different corpora. The results suggest that sociolinguistic factors influence Toki Pona in the same way as natural languages, and that even constructed linguistic systems naturally evolve as communities use them.

</details>


### [66] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)

*Christian M. Angel, Francis Ferraro*

**Main category:** cs.CL

**Keywords:** prompt engineering, inductive bias, LLM, classification, ranking

**Relevance Score:** 9

**TL;DR:** This paper discusses how Inductive Bias Extraction and Matching improves prompt engineering for LLMs, enhancing classification and ranking performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the sensitivity of LLMs to prompt wording by leveraging their inductive biases.

**Method:** The authors propose a method where the output of an LLM is used as part of the prompt to create more satisfying prompt wordings.

**Key Contributions:**

	1. Introduction of Inductive Bias Extraction and Matching strategy
	2. Empirical validation of improved performance metrics
	3. Insights into LLM sensitivity and prompt engineering

**Result:** The proposed strategy improves LLM Likert ratings for classification by up to 19% and for ranking by up to 27%.

**Limitations:** 

**Conclusion:** Inductive Bias Extraction and Matching can significantly optimize prompt formulations for better LLM performance.

**Abstract:** The active research topic of prompt engineering makes it evident that LLMs are sensitive to small changes in prompt wording. A portion of this can be ascribed to the inductive bias that is present in the LLM. By using an LLM's output as a portion of its prompt, we can more easily create satisfactory wording for prompts. This has the effect of creating a prompt that matches the inductive bias in model. Empirically, we show that using this Inductive Bias Extraction and Matching strategy improves LLM Likert ratings used for classification by up to 19% and LLM Likert ratings used for ranking by up to 27%.

</details>


### [67] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)

*Gustavo Bonil, Simone Hashiguti, Jhessica Silva, João Gondim, Helena Maia, Nádia Silva, Helio Pedrini, Sandra Avila*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias Detection, Qualitative Analysis, Interdisciplinary Approaches, AI Ethics

**Relevance Score:** 9

**TL;DR:** This study presents a qualitative framework for detecting biases in Large Language Models (LLMs) through the analysis of their outputs, specifically focusing on gender and racial biases in narratives featuring Black and white women.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether LLMs reproduce biases such as discrimination while relying largely on quantitative methods that overlook nuances in natural language.

**Method:** A qualitative, discursive framework is proposed, involving a manual analysis of LLM-generated short stories.

**Key Contributions:**

	1. Proposes a qualitative framework to complement existing quantitative bias detection methods.
	2. Illustrates the biased portrayals of gender and race in LLM-generated narratives.
	3. Demonstrates the ideological functioning of algorithms in perpetuating social inequalities.

**Result:** Analysis reveals that Black women are often portrayed in relation to ancestry and resistance, while white women are depicted in self-discovery processes, indicating the reinforcement of essentialized representations and social immobility.

**Limitations:** Focuses only on the analysis of short stories, which may not represent all LLM-generated content; results may vary with different prompts or contexts.

**Conclusion:** Qualitative methods are crucial for identifying biases in LLM outputs, revealing how algorithms reflect and perpetuate inequalities, and highlighting the need for interdisciplinary approaches in AI design.

**Abstract:** With the advance of Artificial Intelligence (AI), Large Language Models (LLMs) have gained prominence and been applied in diverse contexts. As they evolve into more sophisticated versions, it is essential to assess whether they reproduce biases, such as discrimination and racialization, while maintaining hegemonic discourses. Current bias detection approaches rely mostly on quantitative, automated methods, which often overlook the nuanced ways in which biases emerge in natural language. This study proposes a qualitative, discursive framework to complement such methods. Through manual analysis of LLM-generated short stories featuring Black and white women, we investigate gender and racial biases. We contend that qualitative methods such as the one proposed here are fundamental to help both developers and users identify the precise ways in which biases manifest in LLM outputs, thus enabling better conditions to mitigate them. Results show that Black women are portrayed as tied to ancestry and resistance, while white women appear in self-discovery processes. These patterns reflect how language models replicate crystalized discursive representations, reinforcing essentialization and a sense of social immobility. When prompted to correct biases, models offered superficial revisions that maintained problematic meanings, revealing limitations in fostering inclusive narratives. Our results demonstrate the ideological functioning of algorithms and have significant implications for the ethical use and development of AI. The study reinforces the need for critical, interdisciplinary approaches to AI design and deployment, addressing how LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [68] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)

*Sihang Zeng, Kai Tian, Kaiyan Zhang, Yuru wang, Junqi Gao, Runze Liu, Sa Yang, Jingxuan Li, Xinwei Long, Jiaheng Ma, Biqing Qi, Bowen Zhou*

**Main category:** cs.CL

**Keywords:** reinforcement learning, peer review, automated critique generation

**Relevance Score:** 9

**TL;DR:** ReviewRL is a reinforcement learning framework designed to generate comprehensive and factually grounded reviews of scientific papers, outperforming existing automated review methods.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of increasing submission volumes and reviewer fatigue in scientific peer review processes, which can lead to superficial feedback.

**Method:** ReviewRL integrates a retrieval-augmented context generation pipeline with supervised fine-tuning and a reinforcement learning procedure utilizing a composite reward function to enhance review quality and accuracy.

**Key Contributions:**

	1. Introduction of ReviewRL as a novel framework for automated scientific reviews
	2. Combination of retrieval-augmented context with reinforcement learning
	3. Demonstrated superior performance over existing review methods

**Result:** ReviewRL significantly outperforms existing automated review methods based on both rule-based and model-based assessments in experiments with ICLR 2025 papers.

**Limitations:** 

**Conclusion:** ReviewRL establishes a novel framework for automatic critique generation in scientific discovery, with substantial potential for future enhancements in the field.

**Abstract:** Peer review is essential for scientific progress but faces growing challenges due to increasing submission volumes and reviewer fatigue. Existing automated review approaches struggle with factual accuracy, rating consistency, and analytical depth, often generating superficial or generic feedback lacking the insights characteristic of high-quality human reviews. We introduce ReviewRL, a reinforcement learning framework for generating comprehensive and factually grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP retrieval-augmented context generation pipeline that incorporates relevant scientific literature, (2) supervised fine-tuning that establishes foundational reviewing capabilities, and (3) a reinforcement learning procedure with a composite reward function that jointly enhances review quality and rating accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL significantly outperforms existing methods across both rule-based metrics and model-based quality assessments. ReviewRL establishes a foundational framework for RL-driven automatic critique generation in scientific discovery, demonstrating promising potential for future development in this domain. The implementation of ReviewRL will be released at GitHub.

</details>


### [69] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)

*Xuan Li, Jialiang Dong, Raymond Wong*

**Main category:** cs.CL

**Keywords:** table parsing, semantic analysis, document structure, domain-specific retrieval, context analysis

**Relevance Score:** 7

**TL;DR:** The paper presents DOTABLER, a framework for deep semantic parsing of tables within documents, addressing limitations of existing methods focused on surface-level analysis.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing studies often overlook deep semantic parsing of tables and their contextual relationships, which is crucial for advanced data interpretation and consistent analysis.

**Method:** DOTABLER uses a custom dataset and domain-specific fine-tuning of pre-trained models. It integrates a parsing pipeline to identify semantically relevant context segments relative to tables and implements functionalities for table-centric document structure parsing and table retrieval.

**Key Contributions:**

	1. Introduction of DOTABLER framework for semantic document parsing focused on tables.
	2. Demonstration of superior performance in table-context analysis compared to existing models.
	3. Implementation of a comprehensive parsing pipeline integrating semantic understanding.

**Result:** Evaluated on nearly 4,000 pages and over 1,000 tables from real-world PDFs, DOTABLER achieves over 90% Precision and F1 scores.

**Limitations:** 

**Conclusion:** DOTABLER significantly enhances table-context semantic analysis and deep document parsing compared to state-of-the-art models like GPT-4o.

**Abstract:** Documents are core carriers of information and knowl-edge, with broad applications in finance, healthcare, and scientific research. Tables, as the main medium for structured data, encapsulate key information and are among the most critical document components. Existing studies largely focus on surface-level tasks such as layout analysis, table detection, and data extraction, lacking deep semantic parsing of tables and their contextual associations. This limits advanced tasks like cross-paragraph data interpretation and context-consistent analysis. To address this, we propose DOTABLER, a table-centric semantic document parsing framework designed to uncover deep semantic links between tables and their context. DOTABLER leverages a custom dataset and domain-specific fine-tuning of pre-trained models, integrating a complete parsing pipeline to identify context segments semantically tied to tables. Built on this semantic understanding, DOTABLER implements two core functionalities: table-centric document structure parsing and domain-specific table retrieval, delivering comprehensive table-anchored semantic analysis and precise extraction of semantically relevant tables. Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs, DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior performance in table-context semantic analysis and deep document parsing compared to advanced models such as GPT-4o.

</details>


### [70] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)

*Minhao Wang, Yunhang He, Cong Xu, Zhangchi Zhu, Wei Zhang*

**Main category:** cs.CL

**Keywords:** Recommender Systems, Large Language Models, Collaborative Signals, Machine Learning, Temporal Frequency Modulation

**Relevance Score:** 8

**TL;DR:** FreLLM4Rec is proposed to balance semantic and collaborative signals in LLM-based recommenders, addressing the problem of collaborative signal attenuation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** LLM-based recommenders often weaken collaborative signals from user interaction history, leading to sub-optimal performance compared to traditional sequential models.

**Method:** The approach uses a Global Graph Low-Pass Filter to purify item embeddings and a Temporal Frequency Modulation to preserve collaborative signals layer by layer while embedding propagation occurs through LLM backbones.

**Key Contributions:**

	1. Introduction of FreLLM4Rec for balancing semantic and collaborative signals in recommendation systems.
	2. Utilization of G-LPF for purifying embeddings and TFM for preserving collaborative signals during LLM processing.
	3. Extensive empirical validation on benchmark datasets showing significant performance improvements.

**Result:** FreLLM4Rec shows improvements of up to 8.00% in NDCG@10 on four benchmark datasets, indicating better preservation of collaborative signals and competitive performance against existing baselines.

**Limitations:** The theoretical aspects of the optimal local graph fourier filters may be challenging to implement in practice, limiting the scope of the proposed methods' applicability.

**Conclusion:** The paper demonstrates the efficacy of FreLLM4Rec in addressing collaborative signal attenuation in LLM-based recommenders and provides theoretical insights into collaborative information processing in LLMs.

**Abstract:** Recommender systems in concert with Large Language Models (LLMs) present promising avenues for generating semantically-informed recommendations. However, LLM-based recommenders exhibit a tendency to overemphasize semantic correlations within users' interaction history. When taking pretrained collaborative ID embeddings as input, LLM-based recommenders progressively weaken the inherent collaborative signals as the embeddings propagate through LLM backbones layer by layer, as opposed to traditional Transformer-based sequential models in which collaborative signals are typically preserved or even enhanced for state-of-the-art performance. To address this limitation, we introduce FreLLM4Rec, an approach designed to balance semantic and collaborative information from a spectral perspective. Item embeddings that incorporate both semantic and collaborative information are first purified using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant high-frequency noise. Temporal Frequency Modulation (TFM) then actively preserves collaborative signal layer by layer. Note that the collaborative preservation capability of TFM is theoretically guaranteed by establishing a connection between the optimal but hard-to-implement local graph fourier filters and the suboptimal yet computationally efficient frequency-domain filters. Extensive experiments on four benchmark datasets demonstrate that FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves competitive performance, with improvements of up to 8.00\% in NDCG@10 over the best baseline. Our findings provide insights into how LLMs process collaborative information and offer a principled approach for improving LLM-based recommendation systems.

</details>


### [71] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)

*Beso Mikaberidze, Teimuraz Saghinadze, Simon Ostermann, Philipp Muller*

**Main category:** cs.CL

**Keywords:** Cross-Prompt Encoder, Dual Soft Prompt, multilingual performance, parameter-efficient fine-tuning, low-performing languages

**Relevance Score:** 8

**TL;DR:** This paper introduces the Cross-Prompt Encoder (XPE) and a Dual Soft Prompt mechanism to improve the performance of large language models on low-performing languages by leveraging multi-source training and hybrid prompt designs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the unexplored potential of prompt encoders in improving language model performance on low-performing languages, which traditional fine-tuning falls short of.

**Method:** The authors introduce the Cross-Prompt Encoder (XPE) and a Dual Soft Prompt mechanism, which combines multi-source training on diverse languages with a hybrid prompt approach for improved model adaptability.

**Key Contributions:**

	1. Introduction of the Cross-Prompt Encoder (XPE)
	2. Development of the Dual Soft Prompt mechanism
	3. Demonstration of effectiveness in low-performing languages through multi-source training

**Result:** Experiments demonstrate that XPE effectively enhances accuracy on low-performing languages, while hybrid prompt variants improve performance across multilingual settings.

**Limitations:** The effectiveness of the proposed methods in other architectures or tasks beyond the SIB-200 benchmark is not yet established.

**Conclusion:** The findings show that prompt encoders can significantly bolster performance in tasks involving low-resourced languages, suggesting a promising avenue for further research in multilingual model training.

**Abstract:** Soft prompts have emerged as a powerful alternative to adapters in parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs) to adapt to downstream tasks without architectural changes or parameter updates. While prior work has focused on stabilizing training via parameter interaction in small neural prompt encoders, their broader potential for transfer across languages remains unexplored. In this paper, we demonstrate that a prompt encoder can play a central role in improving performance on low-performing languages-those that achieve poor accuracy even under full-model fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a lightweight encoding architecture with multi-source training on typologically diverse languages - a design that enables the model to capture abstract and transferable patterns across languages. To complement XPE, we propose a Dual Soft Prompt mechanism that combines an encoder-based prompt with a directly trained standard soft prompt. This hybrid design proves especially effective for target languages that benefit from both broadly shared structure and language-specific alignment. Experiments on the SIB-200 benchmark reveal a consistent trade-off: XPE is most effective for low-performing languages, while hybrid variants offer broader adaptability across multilingual settings.

</details>


### [72] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)

*Jungyup Lee, Jemin Kim, Sang Park, SeungJae Lee*

**Main category:** cs.CL

**Keywords:** Korean, language model, reinforcement learning, supervised fine-tuning, Group Relative Policy Optimization

**Relevance Score:** 5

**TL;DR:** A two-stage fine-tuning method enhances a large language model's Korean reasoning using supervised fine-tuning and reinforcement learning, leading to substantial performance improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of the Qwen3 14B model in Korean reasoning tasks, leveraging both supervised and reinforcement learning techniques.

**Method:** The approach consists of two stages: first, supervised fine-tuning (SFT) on a Korean reasoning dataset, followed by reinforcement learning using a customized Group Relative Policy Optimization (GRPO) to enhance reasoning alignment and problem-solving.

**Key Contributions:**

	1. Introduction of a two-stage fine-tuning approach for Korean reasoning tasks.
	2. Development of an oracle judge model to stabilize GRPO training.
	3. Demonstrated improvements on complex reasoning benchmarks in Korean.

**Result:** The final model achieves significant improvements on advanced reasoning tasks, particularly in math and coding, while maintaining proficiency in knowledge and language.

**Limitations:** 

**Conclusion:** The two-stage method effectively improves the Qwen3 model's performance in Korean language tasks, addressing stability issues in GRPO training through an oracle judge model.

**Abstract:** We present a two-stage fine-tuning approach to make the large language model Qwen3 14B "think" natively in Korean. In the first stage, supervised fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a strong foundation in Korean logical reasoning, yielding notable improvements in Korean-language tasks and even some gains in general reasoning ability. In the second stage, we employ reinforcement learning with a customized Group Relative Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning alignment and overall problem-solving performance. We address critical stability challenges in GRPO training - such as reward hacking and policy collapse - by introducing an oracle judge model that calibrates the reward signal. Our approach achieves stable learning (avoiding the collapse observed in naive GRPO) and leads to steady, incremental performance gains. The final RL-tuned model demonstrates substantially improved results on advanced reasoning benchmarks (particularly math and coding tasks) while maintaining knowledge and language proficiency, successfully conducting its internal chain-of-thought entirely in Korean.

</details>


### [73] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)

*Jakub Šmíd, Pavel Přibáň, Pavel Král*

**Main category:** cs.CL

**Keywords:** sentiment analysis, cross-lingual, sequence-to-sequence, low-resource languages, natural language processing

**Relevance Score:** 6

**TL;DR:** This paper introduces a novel sequence-to-sequence method for compound aspect-based sentiment analysis (ABSA) that improves performance in low-resource languages without relying on external translation tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by low-resource languages in aspect-based sentiment analysis (ABSA) due to a predominant focus on English and reliance on translation tools.

**Method:** A sequence-to-sequence approach utilizing constrained decoding to enhance cross-lingual ABSA performance without external translation tools.

**Key Contributions:**

	1. Novel sequence-to-sequence method for compound ABSA
	2. Improvement of performance in low-resource languages
	3. Comparison of results with large language models

**Result:** Improves cross-lingual ABSA performance by up to 10% and enables handling of more complex tasks.

**Limitations:** 

**Conclusion:** The proposed method presents a practical alternative to translation-dependent techniques and shows that English-centric large language models struggle more in these tasks compared to fine-tuned multilingual LLMs.

**Abstract:** Aspect-based sentiment analysis (ABSA) has made significant strides, yet challenges remain for low-resource languages due to the predominant focus on English. Current cross-lingual ABSA studies often centre on simpler tasks and rely heavily on external translation tools. In this paper, we present a novel sequence-to-sequence method for compound ABSA tasks that eliminates the need for such tools. Our approach, which uses constrained decoding, improves cross-lingual ABSA performance by up to 10\%. This method broadens the scope of cross-lingual ABSA, enabling it to handle more complex tasks and providing a practical, efficient alternative to translation-dependent techniques. Furthermore, we compare our approach with large language models (LLMs) and show that while fine-tuned multilingual LLMs can achieve comparable results, English-centric LLMs struggle with these tasks.

</details>


### [74] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)

*Václav Tran, Jakub Šmíd, Jiří Martínek, Ladislav Lenc, Pavel Král*

**Main category:** cs.CL

**Keywords:** Czech text summarization, large language models, historical documents, natural language processing, dataset

**Relevance Score:** 4

**TL;DR:** This paper explores Czech text summarization using large language models, achieving state-of-the-art results and introducing a new dataset for historical documents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Czech text summarization is underexplored due to linguistic complexities and lack of datasets, necessitating research in this area.

**Method:** The paper employs large language models, specifically Mistral and mT5, to perform text summarization on Czech datasets.

**Key Contributions:**

	1. State-of-the-art results on the SumeCzech dataset using advanced language models.
	2. Introduction of the Posel od Čerchova dataset for summarizing historical Czech documents.

**Result:** Achieves new state-of-the-art results on the modern Czech summarization dataset SumeCzech and introduces a novel dataset for summarization of historical documents.

**Limitations:** 

**Conclusion:** These contributions advance the field of Czech text summarization and encourage further research in processing historical Czech texts.

**Abstract:** Text summarization is the task of shortening a larger body of text into a concise version while retaining its essential meaning and key information. While summarization has been significantly explored in English and other high-resource languages, Czech text summarization, particularly for historical documents, remains underexplored due to linguistic complexities and a scarcity of annotated datasets. Large language models such as Mistral and mT5 have demonstrated excellent results on many natural language processing tasks and languages. Therefore, we employ these models for Czech summarization, resulting in two key contributions: (1) achieving new state-of-the-art results on the modern Czech summarization dataset SumeCzech using these advanced models, and (2) introducing a novel dataset called Posel od \v{C}erchova for summarization of historical Czech documents with baseline results. Together, these contributions provide a great potential for advancing Czech text summarization and open new avenues for research in Czech historical text processing.

</details>


### [75] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)

*Jakub Šmíd, Pavel Přibáň, Pavel Král*

**Main category:** cs.CL

**Keywords:** Aspect-based sentiment analysis, Cross-lingual, Machine translation, Large language models, Multi-task learning

**Relevance Score:** 5

**TL;DR:** This paper introduces a new approach for aspect-based sentiment analysis (ABSA) in low-resource languages using constrained decoding with sequence-to-sequence models, improving cross-lingual performance and supporting multi-tasking.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges in aspect-based sentiment analysis (ABSA) for low-resource languages, which are often neglected compared to English.

**Method:** Utilizes constrained decoding techniques with sequence-to-sequence models to improve performance across multiple ABSA tasks without relying on external translation tools.

**Key Contributions:**

	1. Introduces constrained decoding for cross-lingual ABSA
	2. Demonstrates multi-tasking capability in ABSA
	3. Sets new benchmarks for previously unexplored ABSA tasks

**Result:** Achieves a 5% average improvement in cross-lingual performance for the most complex task, with over 10% enhancement in multi-task scenarios; surpasses state-of-the-art methods across seven languages and six ABSA tasks.

**Limitations:** LLMs perform poorly in zero-shot and few-shot settings, with fine-tuning requiring longer training and inference times.

**Conclusion:** Provides insights and recommendations for practical applications in cross-lingual ABSA and highlights the performance differences of LLMs under various training conditions.

**Abstract:** While aspect-based sentiment analysis (ABSA) has made substantial progress, challenges remain for low-resource languages, which are often overlooked in favour of English. Current cross-lingual ABSA approaches focus on limited, less complex tasks and often rely on external translation tools. This paper introduces a novel approach using constrained decoding with sequence-to-sequence models, eliminating the need for unreliable translation tools and improving cross-lingual performance by 5\% on average for the most complex task. The proposed method also supports multi-tasking, which enables solving multiple ABSA tasks with a single model, with constrained decoding boosting results by more than 10\%.   We evaluate our approach across seven languages and six ABSA tasks, surpassing state-of-the-art methods and setting new benchmarks for previously unexplored tasks. Additionally, we assess large language models (LLMs) in zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in zero-shot and few-shot settings, fine-tuning achieves competitive results compared to smaller multilingual models, albeit at the cost of longer training and inference times.   We provide practical recommendations for real-world applications, enhancing the understanding of cross-lingual ABSA methodologies. This study offers valuable insights into the strengths and limitations of cross-lingual ABSA approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [76] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)

*Chiyu Zhang, Lu Zhou, Xiaogang Xu, Jiafei Wu, Liming Fang, Zhe Liu*

**Main category:** cs.CL

**Keywords:** jailbreak attacks, malicious content detection, language models, dataset cleaning, hybrid framework

**Relevance Score:** 8

**TL;DR:** The paper proposes a hybrid framework for evaluating jailbreak attacks on language models, using a combination of LLMs and minimal human oversight to clean datasets and enhance detection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating jailbreak attacks is complicated by the presence of unsuitable prompts in existing datasets, which necessitates accurate assessment and cleaning.

**Method:** The authors propose the MDH framework, which combines LLM-based annotation with minimal human oversight, and introduce two strategies for enhancing jailbreak success: D-Attack and DH-CoT.

**Key Contributions:**

	1. Introduction of the MDH framework for malicious content detection
	2. Presentation of D-Attack and DH-CoT strategies to enhance jailbreak success
	3. Release of codes, datasets, and detection results for community use

**Result:** The MDH framework improves efficiency and accuracy in malicious content detection, and the new strategies enhance jailbreak effectiveness by utilizing context simulation and hijacked chains of thought.

**Limitations:** 

**Conclusion:** The findings emphasize the importance of assessing developer messages and the need for improved malicious content detection methods in evaluating LLM vulnerabilities.

**Abstract:** Evaluating jailbreak attacks is challenging when prompts are not overtly harmful or fail to induce harmful outputs. Unfortunately, many existing red-teaming datasets contain such unsuitable prompts. To evaluate attacks accurately, these datasets need to be assessed and cleaned for maliciousness. However, existing malicious content detection methods rely on either manual annotation, which is labor-intensive, or large language models (LLMs), which have inconsistent accuracy in harmful types. To balance accuracy and efficiency, we propose a hybrid evaluation framework named MDH (Malicious content Detection based on LLMs with Human assistance) that combines LLM-based annotation with minimal human oversight, and apply it to dataset cleaning and detection of jailbroken responses. Furthermore, we find that well-crafted developer messages can significantly boost jailbreak success, leading us to propose two new strategies: D-Attack, which leverages context simulation, and DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets, judgements, and detection results will be released in github repository: https://github.com/AlienZhang1996/DH-CoT.

</details>


### [77] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)

*Huizhen Shu, Xuying Li, Qirui Wang, Yuji Kosuga, Mengqiu Tian, Zhuo Li*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Large Language Models, Adversarial Examples, Sparse Autoencoders, Text Generation

**Relevance Score:** 7

**TL;DR:** This paper introduces the Sparse Feature Perturbation Framework (SFPF), a novel black-box attack method for generating adversarial texts using sparse autoencoders, significantly bypassing existing defenses in NLP systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generating adversarial examples for large language models (LLMs) to improve their robustness and understand vulnerabilities.

**Method:** The method involves using sparse autoencoders to reconstruct hidden layer representations and perform feature clustering on attacked texts, perturbing highly activated features to create new adversarial examples.

**Key Contributions:**

	1. Introduction of the Sparse Feature Perturbation Framework (SFPF) for adversarial text generation
	2. Utilization of sparse autoencoders for feature manipulation in adversarial examples
	3. Demonstration of successful evasion of state-of-the-art defense mechanisms in NLP

**Result:** Experimental results show that adversarial texts from SFPF can successfully evade state-of-the-art defense mechanisms in NLP systems, exposing ongoing vulnerabilities.

**Limitations:** The method's effectiveness varies by prompts and layers; its generalizability to other architectures and larger models is not yet validated.

**Conclusion:** The proposed SFPF method provides a new red-teaming approach that balances adversarial effectiveness with safety alignment, though its effectiveness is influenced by factors such as prompts and model architecture.

**Abstract:** With the rapid proliferation of Natural Language Processing (NLP), especially Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs remains a key challenge for understanding model vulnerabilities and improving robustness. In this context, we propose a new black-box attack method that leverages the interpretability of large models. We introduce the Sparse Feature Perturbation Framework (SFPF), a novel approach for adversarial text generation that utilizes sparse autoencoders to identify and manipulate critical features in text. After using the SAE model to reconstruct hidden layer representations, we perform feature clustering on the successfully attacked texts to identify features with higher activations. These highly activated features are then perturbed to generate new adversarial texts. This selective perturbation preserves the malicious intent while amplifying safety signals, thereby increasing their potential to evade existing defenses. Our method enables a new red-teaming strategy that balances adversarial effectiveness with safety alignment. Experimental results demonstrate that adversarial texts generated by SFPF can bypass state-of-the-art defense mechanisms, revealing persistent vulnerabilities in current NLP systems.However, the method's effectiveness varies across prompts and layers, and its generalizability to other architectures and larger models remains to be validated.

</details>


### [78] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)

*Juyuan Wang, Rongchen Zhao, Wei Wei, Yufeng Wang, Mo Yu, Jie Zhou, Jin Xu, Liyan Xu*

**Main category:** cs.CL

**Keywords:** narrative comprehension, retrieval-augmented generation, dynamic memory, long-context, iterative reasoning

**Relevance Score:** 9

**TL;DR:** ComoRAG is a dynamic retrieval-based approach for narrative comprehension, improving long-context reasoning in LLMs by cycling through memory and integrating new evidence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of narrative comprehension in long stories due to intricate plots and relationships, where traditional stateless retrieval methods struggle with dynamic context.

**Method:** ComoRAG employs iterative reasoning cycles that interact with a dynamic memory workspace to generate probing queries and integrate new evidence into a global memory pool.

**Key Contributions:**

	1. Introduction of ComoRAG for narrative comprehension
	2. Demonstration of iterative reasoning cycles in retrieval-based models
	3. Significant performance improvements on long-context benchmarks

**Result:** ComoRAG outperforms strong retrieval-based models in four long-context benchmarks, achieving consistent gains up to 11% compared to the strongest baseline.

**Limitations:** 

**Conclusion:** ComoRAG offers a cognitively motivated paradigm for improving retrieval-based comprehension in complex queries, emphasizing the need for stateful reasoning processes.

**Abstract:** Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [79] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)

*Cai Yang, Yao Dou, David Heineman, Xiaofeng Wu, Wei Xu*

**Main category:** cs.CL

**Keywords:** machine translation, idiom translation, natural language processing, evaluation metrics, Chinese language

**Relevance Score:** 4

**TL;DR:** The paper introduces IdiomEval, a framework for evaluating Chinese idiom translation across various systems, revealing significant translation errors and limitations in current metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in machine translation, idiom translation, particularly in Chinese, remains underexplored, leading to poor performance in existing systems.

**Method:** The authors annotated 900 translation pairs from nine modern systems, assessing their performance in translating idioms, and developed improved models to detect translation errors.

**Key Contributions:**

	1. Introduction of the IdiomEval framework for idiom translation evaluation.
	2. Comprehensive error taxonomy for idiom-related translation issues.
	3. Development of models that improve detection of idiom translation errors.

**Result:** The study found that the best-performing system, GPT-4, made errors in 28% of cases, and current evaluation metrics poorly correlate with human ratings, with a Pearson correlation below 0.48.

**Limitations:** The study primarily focuses on idiom translation without addressing other aspects of machine translation performance.

**Conclusion:** The development of new models resulted in an improved F$_1$ score of 0.68 for detecting idiom translation errors, indicating room for enhancement in idiom translation quality.

**Abstract:** Idioms, whose figurative meanings usually differ from their literal interpretations, are common in everyday language, especially in Chinese, where they often contain historical references and follow specific structural patterns. Despite recent progress in machine translation with large language models, little is known about Chinese idiom translation. In this work, we introduce IdiomEval, a framework with a comprehensive error taxonomy for Chinese idiom translation. We annotate 900 translation pairs from nine modern systems, including GPT-4o and Google Translate, across four domains: web, news, Wikipedia, and social media. We find these systems fail at idiom translation, producing incorrect, literal, partial, or even missing translations. The best-performing system, GPT-4, makes errors in 28% of cases. We also find that existing evaluation metrics measure idiom quality poorly with Pearson correlation below 0.48 with human ratings. We thus develop improved models that achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [80] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)

*Sandeep Reddy, Kabir Khan, Rohit Patil, Ananya Chakraborty, Faizan A. Khan, Swati Kulkarni, Arjun Verma, Neha Singh*

**Main category:** cs.CL

**Keywords:** large language models, computational economics, resource allocation, efficient training, LLM interpretability

**Relevance Score:** 8

**TL;DR:** This paper presents a computational economics framework for large language models (LLMs) that optimizes resource allocation for task utility, resulting in efficient and interpretable models under computational constraints.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs face high computational costs, necessitating a better resource allocation strategy to enhance efficiency and utility in task performance.

**Method:** The authors propose an incentive-driven training paradigm that integrates a differentiable computation cost term with task loss, encouraging sparse and efficient activations in LLMs.

**Key Contributions:**

	1. Introduces a computational economics framework for LLMs.
	2. Develops an incentive-driven training paradigm for efficiency in model activations.
	3. Demonstrates significant performance improvements in resource-constrained settings.

**Result:** The proposed method leads to models that achieve a significant reduction in FLOPS (approximately 40%), lower latency, and more interpretable attention patterns while maintaining high accuracy across various benchmarks.

**Limitations:** 

**Conclusion:** Implementing economic principles in LLM design facilitates the development of more efficient and adaptive models that excel under resource constraints.

**Abstract:** Large language models (LLMs) are limited by substantial computational cost. We introduce a "computational economics" framework that treats an LLM as an internal economy of resource-constrained agents (attention heads and neuron blocks) that must allocate scarce computation to maximize task utility. First, we show empirically that when computation is scarce, standard LLMs reallocate attention toward high-value tokens while preserving accuracy. Building on this observation, we propose an incentive-driven training paradigm that augments the task loss with a differentiable computation cost term, encouraging sparse and efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method yields a family of models that trace a Pareto frontier and consistently dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty percent reduction in FLOPS and lower latency, together with more interpretable attention patterns. These results indicate that economic principles offer a principled route to designing efficient, adaptive, and more transparent LLMs under strict resource constraints.

</details>


### [81] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)

*Herun Wan, Jiaying Wu, Minnan Luo, Xiangzheng Kong, Zihan Ma, Zhi Zeng*

**Main category:** cs.CL

**Keywords:** misinformation detection, vision-language models, chain-of-thought prompts

**Relevance Score:** 7

**TL;DR:** DiFaR is a novel framework that enhances misinformation detection by generating diverse, factual, and relevant rationales from large vision-language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective multimodal misinformation detectors that generate high-quality rationales to support their capabilities, addressing the limitations of existing methods.

**Method:** DiFaR employs five chain-of-thought prompts to generate varied reasoning traces and utilizes a post-hoc filtering module to select rationale sentences based on factuality and relevance.

**Key Contributions:**

	1. Introduction of a detector-agnostic framework for generating rationales
	2. Utilization of chain-of-thought prompts for diverse reasoning
	3. Implementation of a post-hoc filtering module for selecting high-quality rationales

**Result:** DiFaR outperforms baseline methods by up to 5.9% in rationale quality and enhances detector performance by up to 8.7% on four benchmarks.

**Limitations:** 

**Conclusion:** The extensive experiments confirm that DiFaR significantly improves the quality of generated rationales, addressing diversity, factual accuracy, and relevance issues in misinformation detection.

**Abstract:** Generating textual rationales from large vision-language models (LVLMs) to support trainable multimodal misinformation detectors has emerged as a promising paradigm. However, its effectiveness is fundamentally limited by three core challenges: (i) insufficient diversity in generated rationales, (ii) factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting content that introduces noise. We introduce DiFaR, a detector-agnostic framework that produces diverse, factual, and relevant rationales to enhance misinformation detection. DiFaR employs five chain-of-thought prompts to elicit varied reasoning traces from LVLMs and incorporates a lightweight post-hoc filtering module to select rationale sentences based on sentence-level factuality and relevance scores. Extensive experiments on four popular benchmarks demonstrate that DiFaR outperforms four baseline categories by up to 5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics and human evaluations confirm that DiFaR significantly improves rationale quality across all three dimensions.

</details>


### [82] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)

*Mahdi Dhaini, Stephen Meisenbacher, Ege Erdogan, Florian Matthes, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Explainability, Privacy, Differential Privacy, Post-hoc Explainability

**Relevance Score:** 9

**TL;DR:** This paper investigates the trade-off between explainability and privacy in NLP, exploring whether they can coexist.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of investigation at the intersection of explainability and privacy in NLP, and to explore their potential coexistence.

**Method:** Empirical investigation into the privacy-explainability trade-off using Differential Privacy (DP) and Post-hoc Explainability methods.

**Key Contributions:**

	1. Empirical insights into the privacy-explainability trade-off in NLP.
	2. Identification of factors influencing the relationship between privacy and explainability.
	3. Recommendations for future research in the intersection of explainable and privacy-preserving NLP.

**Result:** Findings indicate an intricate relationship between privacy and explainability influenced by factors such as the downstream task and chosen methods for text privatization and explainability. Recommendations for future research are provided.

**Limitations:** 

**Conclusion:** The study suggests that privacy and explainability can potentially coexist and offers practical recommendations for future work.

**Abstract:** In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of \textit{explainability} and \textit{privacy}. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving \textit{both} explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection.

</details>


### [83] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)

*Huyu Wu, Meng Tang, Xinhan Zheng, Haiyun Jiang*

**Main category:** cs.CL

**Keywords:** Multimodal Language Models, Text Dominance, Attention Mechanism, Evaluation Metrics, Token Compression

**Relevance Score:** 8

**TL;DR:** This paper investigates the issue of text dominance in Multimodal Large Language Models (MLLMs), proposes two new evaluation metrics to measure it, and suggests a method to rebalance model attention.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the reliance of MLLMs on text over other modalities, which affects their multimodal inference capabilities.

**Method:** A systematic investigation across various data modalities is conducted, introducing the Modality Dominance Index (MDI) and Attention Efficiency Index (AEI) to quantify text dominance. A token compression method is proposed to rebalance attention.

**Key Contributions:**

	1. Introduced the Modality Dominance Index (MDI) and Attention Efficiency Index (AEI) for measuring text dominance
	2. Conducted a comprehensive analysis across multiple data modalities
	3. Proposed a token compression method to improve model attention distribution

**Result:** The analysis reveals significant text dominance across all tested modalities, with MDI values indicating severe reliance on text. Implementing the token compression method drastically lowers the MDI value of LLaVA-7B from 10.23 to 0.86, demonstrating improved attention balancing.

**Limitations:** 

**Conclusion:** The study presents a methodological framework for developing more balanced multimodal language models and highlights the importance of addressing text dominance in future research.

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities across a diverse range of multimodal tasks. However, these models suffer from a core problem known as text dominance: they depend heavily on text for their inference, while underutilizing other modalities. While prior work has acknowledged this phenomenon in vision-language tasks, often attributing it to data biases or model architectures. In this paper, we conduct the first systematic investigation of text dominance across diverse data modalities, including images, videos, audio, time-series, and graphs. To measure this imbalance, we propose two evaluation metrics: the Modality Dominance Index (MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis reveals that text dominance is both significant and pervasive across all tested modalities. Our in-depth analysis identifies three underlying causes: attention dilution from severe token redundancy in non-textual modalities, the influence of fusion architecture design, and task formulations that implicitly favor textual inputs. Furthermore, we propose a simple token compression method that effectively rebalances model attention. Applying this method to LLaVA-7B, for instance, drastically reduces its MDI from 10.23 to a well-balanced value of 0.86. Our analysis and methodological framework offer a foundation for the development of more equitable and comprehensive multimodal language models.

</details>


### [84] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)

*Irma Heithoff. Marc Guggenberger, Sandra Kalogiannis, Susanne Mayer, Fabian Maag, Sigurd Schacht, Carsten Lanquillon*

**Main category:** cs.CL

**Keywords:** LLM interpretability, Deep Inference Fabric, user engagement, remote experimentation, mechanistic interpretability

**Relevance Score:** 9

**TL;DR:** The paper explores the deployment of a European Deep Inference Fabric for mechanistic interpretability research on large language models, introducing a collaborative testing platform and demonstrating its utility through user studies.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To democratize access to LLM interpretability infrastructure in Europe for researchers.

**Method:** A GPU-based cluster was deployed and interlinked with partner institutions, facilitating remote model inspection through the NNsight API. A pilot study with 16 researchers evaluated its usability and performance.

**Key Contributions:**

	1. Introduction of a GPU-based cluster for LLM interpretability research
	2. Evaluation of platform performance and usability through a pilot study
	3. Foundation for a community around mechanistic interpretability research

**Result:** The platform demonstrated stable performance, increased user engagement, and positive feedback on remote experimentation capabilities while identifying necessary improvements for future versions.

**Limitations:** Prolonged download durations for activation data and intermittent execution interruptions were noted.

**Conclusion:** The initiative is a significant stride towards enhanced LLM interpretability in Europe, supporting future development and community collaboration in the field.

**Abstract:** This paper presents a feasibility study on the deployment of a European Deep Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support mechanistic interpretability research on large language models. The need for widespread accessibility of LLM interpretability infrastructure in Europe drives this initiative to democratize advanced model analysis capabilities for the research community. The project introduces a GPU-based cluster hosted at Ansbach University of Applied Sciences and interconnected with partner institutions, enabling remote model inspection via the NNsight API. A structured pilot study involving 16 researchers from across Europe evaluated the platform's technical performance, usability, and scientific utility. Users conducted interventions such as activation patching, causal tracing, and representation analysis on models including GPT-2 and DeepSeek-R1-70B. The study revealed a gradual increase in user engagement, stable platform performance throughout, and a positive reception of the remote experimentation capabilities. It also marked the starting point for building a user community around the platform. Identified limitations such as prolonged download durations for activation data as well as intermittent execution interruptions are addressed in the roadmap for future development. This initiative marks a significant step towards widespread accessibility of LLM interpretability infrastructure in Europe and lays the groundwork for broader deployment, expanded tooling, and sustained community collaboration in mechanistic interpretability research.

</details>


### [85] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)

*Nasma Chaoui, Richard Khoury*

**Main category:** cs.CL

**Keywords:** Coptic, translation, historical languages, machine translation, French

**Relevance Score:** 2

**TL;DR:** Systematic study on translating Coptic into French, evaluating various strategies for improved translation quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore effective strategies for translating historical languages, specifically focusing on Coptic to French.

**Method:** Evaluated pivot versus direct translation, pre-training impact, multi-version fine-tuning benefits, and model robustness to noise using aligned biblical corpora.

**Key Contributions:**

	1. First systematic study on Coptic to French translation
	2. Evaluation of various translation strategies
	3. Insights for improving tools for historical language translation

**Result:** Fine-tuning with a diverse and noise-aware training corpus significantly enhances translation quality for Coptic to French.

**Limitations:** 

**Conclusion:** The study provides valuable insights for developing translation tools for historical languages.

**Abstract:** This paper presents the first systematic study of strategies for translating Coptic into French. Our comprehensive pipeline systematically evaluates: pivot versus direct translation, the impact of pre-training, the benefits of multi-version fine-tuning, and model robustness to noise. Utilizing aligned biblical corpora, we demonstrate that fine-tuning with a stylistically-varied and noise-aware training corpus significantly enhances translation quality. Our findings provide crucial practical insights for developing translation tools for historical languages in general.

</details>


### [86] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)

*Safaeid Hossain Arib, Rabeya Akter, Sejuti Rahman*

**Main category:** cs.CL

**Keywords:** Sign Language Translation, Transformer Architecture, Graph-Based Methods

**Relevance Score:** 4

**TL;DR:** The paper presents a novel approach to enhance sign language translation by integrating graph-based methods with transformer architecture, achieving state-of-the-art performance on multiple datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve communication accessibility for the deaf and hard of hearing by addressing the underestimation and communication barriers faced by sign language.

**Method:** The approach combines transformer architecture with STGCN-LSTM architectures for gloss-free translation, exploring various fusion strategies.

**Key Contributions:**

	1. Architectural fusion of transformer and graph-based methods
	2. New state-of-the-art performance benchmarks
	3. Introduction of BornilDB v1.0 for benchmarking

**Result:** Achieved new state-of-the-art BLEU-4 scores across multiple datasets like RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and introduced benchmarking on BornilDB v1.0 dataset.

**Limitations:** 

**Conclusion:** The proposed method significantly surpasses existing translation outcomes, setting a new benchmark for future research in sign language translation.

**Abstract:** Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of communication for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to communication barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage transformer architecture for state-of-the-art results, our method integrates graph-based methods with the transformer architecture. This fusion, combining transformer and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve communication accessibility for the deaf and hard of hearing.

</details>


### [87] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)

*Alireza Salemi, Hamed Zamani*

**Main category:** cs.CL

**Keywords:** personalization, natural language feedback, large language models, question answering, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents VAC, a framework for personalized response generation using natural language feedback instead of scalar rewards, improving learning efficiency and response quality in large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Personalization enhances effectiveness and user satisfaction in language technologies, particularly in question answering tasks. Existing methods often utilize scalar rewards, which can limit learning efficiency and personalization quality.

**Method:** The VAC framework replaces scalar rewards with natural language feedback (NLF), conditioned on user profiles and question narratives. Training alternates between optimizing the feedback model and fine-tuning the policy model on improved responses, ultimately eliminating the need for feedback during inference.

**Key Contributions:**

	1. Introduction of VAC framework for personalized response generation
	2. Replacement of scalar rewards with natural language feedback (NLF)
	3. Demonstrated significant improvements in response quality and learning efficiency

**Result:** Evaluation on the LaMP-QA benchmark shows consistent and significant improvements over state-of-the-art results, with human evaluations confirming the higher quality of responses generated with NLF.

**Limitations:** 

**Conclusion:** The use of NLF as a signal for optimizing personalized question answering enhances the model's ability to generate relevant responses tailored to individual user profiles.

**Abstract:** Personalization is crucial for enhancing both the effectiveness and user satisfaction of language technologies, particularly in information-seeking tasks like question answering. Current approaches for personalizing large language models (LLMs) often rely on retrieval-augmented generation (RAG), followed by reinforcement learning with scalar reward signals to teach models how to use retrieved personal context. We believe that these scalar rewards sometimes provide weak, non-instructive feedback, limiting learning efficiency and personalization quality. We introduce VAC, a novel framework for personalized response generation that replaces scalar rewards with natural language feedback (NLF) that are generated conditioned on the user profiles and the question narratives. NLF serves as a rich and actionable supervision signal, allowing the policy model to iteratively refine its outputs and internalize effective personalization strategies. Training alternates between optimizing the feedback model and fine-tuning the policy model on the improved responses, resulting in a policy model that no longer requires feedback at inference. Evaluation on the LaMP-QA benchmark that consists of three diverse domains demonstrates consistent and significant improvements over the state-of-the-art results. Human evaluations further confirm the superior quality of the generated responses. These results demonstrate that NLF provides more effective signals for optimizing personalized question answering.

</details>


### [88] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)

*Xiangqi Jin, Yuxuan Wang, Yifeng Gao, Zichen Wen, Biqing Qi, Dongrui Liu, Linfeng Zhang*

**Main category:** cs.CL

**Keywords:** large language models, diffusion models, chain-of-thought prompting, in-place prompting, early exit mechanism

**Relevance Score:** 9

**TL;DR:** ICE is a novel framework that improves prompting in diffusion large language models by integrating in-place prompts and a confidence-aware early exit mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of traditional prefix-only prompting in large language models, allowing for more flexible interaction and efficiency in generation processes.

**Method:** The ICE framework incorporates in-place prompts within masked token positions during the iterative refinement process of diffusion large language models and utilizes a confidence-aware early exit mechanism to reduce computational cost.

**Key Contributions:**

	1. Introduces in-place prompting in diffusion large language models (dLLMs)
	2. Implements confidence-aware early exit mechanism to cut computational costs
	3. Demonstrates significant improvements in accuracy and speed on benchmarks.

**Result:** ICE achieves an accuracy improvement of up to 17.29% and a speedup of 4.12x on the GSM8K dataset, as well as a remarkable 276.67x acceleration on MMLU, while keeping competitive performance.

**Limitations:** 

**Conclusion:** The results suggest that ICE effectively enhances the flexibility and efficiency of prompting in diffusion large language models, paving the way for more advanced applications in tasks requiring bidirectional information flow.

**Abstract:** Despite large language models (LLMs) have achieved remarkable success, their prefix-only prompting paradigm and sequential generation process offer limited flexibility for bidirectional information. Diffusion large language models (dLLMs) present new opportunities through their bidirectional attention mechanisms and iterative refinement processes, enabling more flexible in-place prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting with Early Exit), a novel framework that transforms prefix-only prompting into in-place prompting specifically designed for dLLMs. ICE integrates in-place prompts directly within masked token positions during iterative refinement and employs a confidence-aware early exit mechanism to significantly reduce computational overhead. Extensive experiments demonstrate ICE's effectiveness, achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K, and up to 276.67$\times$ acceleration on MMLU while maintaining competitive performance.

</details>


### [89] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)

*Osama Mohammed Afzal, Preslav Nakov, Tom Hope, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** novelty assessment, peer review, NLP, LLM, human expertise

**Relevance Score:** 7

**TL;DR:** The paper proposes an automated approach for novelty assessment in peer review, showing significant alignment with human reviewers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of novelty assessment in peer review, particularly in NLP, where reviewer capacity is limited.

**Method:** The method involves extracting content from submissions, retrieving related work, and performing structured comparisons for evidence-based assessments.

**Key Contributions:**

	1. Development of a structured approach for automated novelty evaluation
	2. High alignment with human reviewer assessments
	3. Open availability of data and code for further research

**Result:** Evaluated on 182 ICLR 2025 submissions, the method achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions, outperforming existing LLM baselines.

**Limitations:** 

**Conclusion:** Structured LLM-assisted approaches can enhance the peer review process by providing detailed analyses, improving consistency, and supporting human expertise.

**Abstract:** Novelty assessment is a central yet understudied aspect of peer review, particularly in high volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence based assessment. Our method is informed by a large scale analysis of human written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions - substantially outperforming existing LLM based baselines. The method produces detailed, literature aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available.

</details>


### [90] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)

*Jim Dilkes, Vahid Yazdanpanah, Sebastian Stein*

**Main category:** cs.CL

**Keywords:** Large Language Models, multi-step decision-making, post-training

**Relevance Score:** 9

**TL;DR:** The paper presents a novel algorithm, MS-GRPO, for improving decision-making in LLMs by enhancing smaller models for multi-step tasks, showing significant performance gains over larger models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs are too costly in terms of computation, necessitating techniques to enhance the performance of smaller models for decision-making tasks.

**Method:** The paper introduces Multi-Step Group-Relative Policy Optimization (MS-GRPO) for post-training LLMs, utilizing the Text-Mediated Stochastic Game (TSMG) framework and a new episode sampling strategy.

**Key Contributions:**

	1. Introduction of MS-GRPO for credit assignment in multi-step tasks
	2. Development of an absolute-advantage-weighted episode sampling strategy
	3. Demonstrated significant performance improvements in decision-making tasks.

**Result:** The MS-GRPO approach significantly improves a 3-billion parameter model's decision-making, yielding a 50% performance boost over a 72-billion parameter baseline in the Frozen Lake task.

**Limitations:** 

**Conclusion:** Targeted post-training like MS-GRPO offers an efficient alternative to simply scaling model size for enhancing LLM decision-making capabilities.

**Abstract:** Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.

</details>


### [91] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)

*Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang*

**Main category:** cs.CL

**Keywords:** Large Language Model, Mental Health, Psychological Applications, Reasoning, Empathy

**Relevance Score:** 10

**TL;DR:** Psyche-R1 is a novel psychological LLM integrating empathy, expertise, and reasoning, addressing mental health disorders through a unique data synthesis and training strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To alleviate the shortage of qualified mental health professionals and improve the quality of psychological applications leveraging LLMs, especially in reasoning mechanisms.

**Method:** A novel data curation pipeline generated over 75k psychological questions with rationales and 73k empathetic dialogues. Employed a hybrid training strategy including GRPO for reasoning and SFT for empathetic responses.

**Key Contributions:**

	1. First Chinese psychological LLM with integrated reasoning mechanisms
	2. Comprehensive data synthesis for psychological training
	3. Hybrid training strategy optimizing reasoning and empathy

**Result:** Psyche-R1, a 7B model, achieves comparable performance to the 671B DeepSeek-R1 on various psychological benchmarks.

**Limitations:** 

**Conclusion:** The integration of reasoning, empathy, and psychological expertise in LLMs like Psyche-R1 can significantly enhance the support provided in mental health applications.

**Abstract:** Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.

</details>


### [92] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)

*Zhaokun Jiang, Ziyin Zhang*

**Main category:** cs.CL

**Keywords:** machine learning, interpreting quality assessment, explainable ML

**Relevance Score:** 5

**TL;DR:** Proposes a multi-dimensional framework for automated interpreting quality assessment prioritizing explainability and model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing gaps in language quality assessment in interpreting such as data scarcity, model effectiveness, and explainability.

**Method:** Integration of feature engineering, data augmentation, and explainable machine learning techniques, utilizing SHAP analysis for feature transparency.

**Key Contributions:**

	1. Multi-dimensional modeling framework for interpreting quality assessment
	2. Prioritization of explainability in predictions
	3. Identification of key predictive features for language quality

**Result:** Achieved strong predictive performance on an English-Chinese interpreting dataset, highlighting the importance of specific predictive features for fidelity and fluency.

**Limitations:** 

**Conclusion:** The proposed framework is a scalable and reliable alternative for human evaluation, offering detailed feedback to learners.

**Abstract:** Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.

</details>


### [93] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)

*Yuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai Zhu, Che Jiang, Yuchen Zhang, Li Kang, Gang Chen, Cheng Huang, Zhizhou He, Bingning Wang, Lei Bai, Ning Ding, Bowen Zhou*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, Self-Search RL

**Relevance Score:** 9

**TL;DR:** This paper explores the use of large language models (LLMs) as efficient simulators for agentic search tasks in reinforcement learning (RL) through a novel approach called Self-Search RL (SSRL).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to explore how LLMs can reduce dependency on external search engines for agentic search tasks in RL.

**Method:** The study quantifies LLMs' intrinsic search capability through structured prompting and repeated sampling, termed Self-Search, and introduces Self-Search RL (SSRL) that enhances this capability with format-based and rule-based rewards.

**Key Contributions:**

	1. Introduction of Self-Search for quantifying LLM capabilities
	2. Development of Self-Search RL (SSRL) to enhance LLM performance
	3. Demonstration of cost-effective training environments for RL using LLMs

**Result:** LLMs showed strong scaling behavior with high pass@k scores on various benchmarks, and SSRL-trained models provided a cost-effective, stable training environment for search-driven RL, reducing external search dependency.

**Limitations:** 

**Conclusion:** LLMs possess significant world knowledge that can be effectively utilized, SSRL reduces hallucination, and models trained with SSRL can integrate smoothly with external search engines.

**Abstract:** We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.

</details>


### [94] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)

*Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen*

**Main category:** cs.CL

**Keywords:** Diffusion Language Models, Natural Language Processing, Multimodal AI

**Relevance Score:** 8

**TL;DR:** The paper surveys the emerging field of Diffusion Language Models (DLMs), outlining their advantages, evolution, current state, and applications in NLP, along with challenges and future research directions.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview of Diffusion Language Models as an alternative to autoregressive models in NLP tasks, highlighting their evolution, performance, and applications.

**Method:** The survey traces the development of DLMs, compares them with autoregressive and masked language models, and covers foundational principles, state-of-the-art models, and inference strategies.

**Key Contributions:**

	1. Holistic overview of the DLM landscape.
	2. Comprehensive taxonomy and analysis of techniques in pre-training and post-training strategies.
	3. Discussion of multimodal extensions and practical applications of DLMs.

**Result:** DLMs have shown performance comparable to autoregressive models while achieving several-fold speed-up in inference, with advances in decoding parallelism and generation quality.

**Limitations:** Challenges include efficiency, handling long sequences, and infrastructure requirements for DLMs.

**Conclusion:** The paper outlines the current landscape of DLMs, their limitations, and future research directions, facilitating understanding and ongoing development in this emerging field.

**Abstract:** Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


### [95] [Knowledge-based Consistency Testing of Large Language Models](https://arxiv.org/abs/2407.12830)

*Sai Sathiesh Rajan, Ezekiel Soremekun, Sudipta Chattopadhyay*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Gaps, Automated Testing, HCI, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper presents KonTest, an automated testing framework for measuring inconsistencies and knowledge gaps in Large Language Models (LLMs) using knowledge graphs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically expose and measure the inconsistencies and knowledge gaps of LLMs, improving their reliability in applications.

**Method:** KonTest uses a knowledge graph to construct test cases, employing semantically-equivalent queries and test oracles to probe LLM knowledge.

**Key Contributions:**

	1. Development of the KonTest framework for LLM testing
	2. Quantification of inconsistency and knowledge gaps in state-of-the-art LLMs
	3. Mitigation strategy that significantly reduces knowledge gaps in LLMs.

**Result:** KonTest identifies 19.2% error-inducing inputs and a 16.5% knowledge gap across tested LLMs, while a subsequent mitigation method reduces the knowledge gap by 32.48%.

**Limitations:** The study primarily focuses on four specific LLMs and the results may not generalize to other models or domains.

**Conclusion:** The study highlights the inconsistency of LLMs and the effectiveness of the KonTest framework in reducing knowledge gaps, particularly noting the limitations of GPT3.5 for knowledge-based consistency testing.

**Abstract:** In this work, we systematically expose and measure the inconsistency and knowledge gaps of Large Language Models (LLMs). Specifically, we propose an automated testing framework (called KonTest) which leverages a knowledge graph to construct test cases. KonTest probes and measures the inconsistencies in the LLM's knowledge of the world via a combination of semantically-equivalent queries and test oracles (metamorphic or ontological oracle). KonTest further mitigates knowledge gaps via a weighted LLM model ensemble. Using four state-of-the-art LLMs (Falcon, Gemini, GPT3.5, and Llama2), we show that KonTest generates 19.2% error inducing inputs (1917 errors from 9979 test inputs). It also reveals a 16.5% knowledge gap across all tested LLMs. A mitigation method informed by KonTest's test suite reduces LLM knowledge gap by 32.48%. Our ablation study further shows that GPT3.5 is not suitable for knowledge-based consistency testing because it is only 60%-68% effective in knowledge construction.

</details>


### [96] [This Candidate is [MASK]. Prompt-based Sentiment Extraction and Reference Letters](https://arxiv.org/abs/2410.16325)

*Fabian Slonimczyk*

**Main category:** cs.CL

**Keywords:** sentiment analysis, large language models, job market outcomes, gender bias, text extraction

**Relevance Score:** 9

**TL;DR:** The paper introduces a prompt-based method for sentiment extraction from text data, specifically analyzing reference letters and their impact on job market outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve sentiment extraction methods and analyze how sentiment in reference letters influences candidates' job performance.

**Method:** Prompt-based sentiment extraction applied to a corpus of confidential reference letters, comparing it against traditional methods like bag-of-words and fine-tuned models.

**Key Contributions:**

	1. Introduction of prompt-based sentiment extraction method
	2. Demonstration of sentiment's effect on job market outcomes
	3. Analysis of gender differences in reference letters

**Result:** Candidates with higher sentiment in their letters perform better in the job market, while disagreement among writers negatively affects performance. The method reveals gender biases in reference letters that impact women's job outcomes.

**Limitations:** 

**Conclusion:** Prompt-based sentiment extraction outperforms other methods, and gendered aspects in reference letters contribute to disparities in job market success.

**Abstract:** I propose a relatively simple way to deploy pre-trained large language models (LLMs) in order to extract sentiment and other useful features from text data. The method, which I refer to as prompt-based sentiment extraction, offers multiple advantages over other methods used in economics and finance. I apply my prompt-based strategy to a hand-collected corpus of confidential reference letters (RLs). I show that the sentiment contents of RLs is clearly reflected in job market outcomes. Candidates with higher average sentiment in their letters perform markedly better regardless of the measure of success chosen. Moreover, I show that disagreement among letter writers negatively affects the job market candidate's performance. I compare my sentiment extraction approach to other commonly used methods for sentiment analysis: "bag-of-words" approaches, fine-tuned language models, and querying advanced chatbots. I find that no other method can reproduce the results obtained by prompt-based sentiment extraction. Finally, I slightly modify the method to obtain "gendered" sentiment scores (as in Eberhardt et al., 2023). I show that letters of reference written for female candidates emphasize "grindstone" personality traits, whereas male candidates' letters emphasize "standout" traits. These gender differences negatively affect women's job market outcomes.

</details>


### [97] [Fleurs-SLU: A Massively Multilingual Benchmark for Spoken Language Understanding](https://arxiv.org/abs/2501.06117)

*Fabian David Schmidt, Ivan Vulić, Goran Glavaš, David Ifeoluwa Adelani*

**Main category:** cs.CL

**Keywords:** Spoken Language Understanding, Multilingual, Automatic Speech Recognition, Large Language Models, Benchmark

**Relevance Score:** 7

**TL;DR:** Fleurs-SLU is a multilingual spoken language understanding benchmark that evaluates different systems for classifying speech and answering questions across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in spoken language understanding (SLU) for low-resource languages, where traditional systems struggle due to limited training data and evaluation focused on shallow tasks.

**Method:** The benchmark includes 692 hours of speech for topical utterance classification in 102 languages and 944 hours of speech for multiple-choice question answering. It evaluates various systems including end-to-end models, cascaded systems combining ASR with LLMs, and multimodal speech-LLMs.

**Key Contributions:**

	1. Introduction of Fleurs-SLU multilingual benchmark for SLU
	2. Evaluation of multiple systems including cascaded frameworks and speech-LLMs
	3. Insights on the correlation between ASR quality and SLU performance

**Result:** Cascaded systems demonstrate robustness in multilingual SLU, while well-pretrained speech encoders are competitive in topical classification. Closed-source speech-LLMs either match or surpass cascaded systems' performance.

**Limitations:** 

**Conclusion:** A strong correlation exists between ASR effectiveness, quality speech-to-text translation, and SLU performance, highlighting the benefits of integrating acoustic and semantic representations.

**Abstract:** Spoken language understanding (SLU) is indispensable for half of all living languages that lack a formal writing system. Unlike for high-resource languages, for these languages, we cannot offload semantic understanding of speech to the cascade of automatic speech recognition (ASR) and text-based large language models (LLMs). Even if low-resource languages possess a writing system, ASR for these languages remains unreliable due to limited bimodal speech and text training data. Nonetheless, the evaluation of multilingual SLU is limited to shallow tasks such as intent classification or language identification. This is why we present Fleurs-SLU, a multilingual SLU benchmark that encompasses (i) 692 hours of speech for topical utterance classification in 102 languages and (ii) multiple-choice question answering via listening comprehension spanning 944 hours of speech across 92 languages. We extensively evaluate end-to-end speech classification models, cascaded systems that combine speech-to-text transcription with subsequent LLM-based classification, and multimodal speech-LLMs on Fleurs-SLU. Our results show that cascaded systems are more robust in multilingual SLU, though well-pretrained speech encoders can perform competitively in topical speech classification. Closed-source speech-LLMs match or surpass the performance of cascaded systems. We observe a strong correlation between robust multilingual ASR, effective speech-to-text translation, and strong multilingual SLU, indicating mutual benefits between acoustic and semantic speech representations.

</details>


### [98] [Measuring Diversity in Synthetic Datasets](https://arxiv.org/abs/2502.08512)

*Yuchang Zhu, Huizhe Zhang, Bingzhe Wu, Jintang Li, Zibin Zheng, Peilin Zhao, Liang Chen, Yatao Bian*

**Main category:** cs.CL

**Keywords:** large language models, synthetic datasets, diversity evaluation

**Relevance Score:** 9

**TL;DR:** DCScore is a novel method for measuring the diversity of synthetic datasets used in NLP tasks, providing strong correlations with diversity pseudo-truths and reduced computational costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Accurately measuring the diversity of synthetic datasets is crucial for improving model performance in NLP tasks.

**Method:** DCScore approaches diversity evaluation as a sample classification task, utilizing mutual relationships among samples for its calculations.

**Key Contributions:**

	1. Introduction of DCScore for diversity measurement
	2. Theoretical verification of diversity-related axioms
	3. Strong empirical performance with reduced computational costs

**Result:** Experimental results show DCScore has a stronger correlation with multiple diversity pseudo-truths compared to existing methods and significantly lowers computational costs.

**Limitations:** 

**Conclusion:** DCScore is validated both theoretically and experimentally as an effective and efficient diversity evaluation method for synthetic datasets.

**Abstract:** Large language models (LLMs) are widely adopted to generate synthetic datasets for various natural language processing (NLP) tasks, such as text classification and summarization. However, accurately measuring the diversity of these synthetic datasets-an aspect crucial for robust model performance-remains a significant challenge. In this paper, we introduce DCScore, a novel method for measuring synthetic dataset diversity from a classification perspective. Specifically, DCScore formulates diversity evaluation as a sample classification task, leveraging mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, highlighting its role as a principled diversity evaluation method. Experimental results on synthetic datasets reveal that DCScore enjoys a stronger correlation with multiple diversity pseudo-truths of evaluated datasets, underscoring its effectiveness. Moreover, both empirical and theoretical evidence demonstrate that DCScore substantially reduces computational costs compared to existing methods. Code is available at: https://github.com/bluewhalelab/dcscore.

</details>


### [99] [LED-Merging: Mitigating Safety-Utility Conflicts in Model Merging with Location-Election-Disjoint](https://arxiv.org/abs/2502.16770)

*Qianli Ma, Dongrui Liu, Qian Chen, Linfeng Zhang, Jing Shao*

**Main category:** cs.CL

**Keywords:** Large Language Models, model merging, safety-utility conflicts, gradient-based attribution, multi-task learning

**Relevance Score:** 8

**TL;DR:** The paper proposes LED-Merging, a framework for safely merging multiple task-specific Large Language Models (LLMs) while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the computational and data costs of fine-tuning LLMs for specialized tasks and tackle safety-utility conflicts arising from merging models.

**Method:** LED-Merging consists of locating task-specific neurons via gradient-based attribution, selecting critical neurons through multi-model importance fusion, and isolating conflicting updates to prevent interference.

**Key Contributions:**

	1. Introduction of a three-stage merging framework for LLMs
	2. Reduction of harmful response rates while preserving utility
	3. Reuse of existing task-specific models without the need for retraining

**Result:** Experiments show that LED-Merging reduces harmful response rates by 31.4% on Llama-3-8B-Instruct and maintains 95% utility performance, achieving 52.39% accuracy on GSM8K.

**Limitations:** 

**Conclusion:** LED-Merging effectively resolves safety-utility conflicts and provides a lightweight, training-free method for building reliable multi-task LLMs.

**Abstract:** Fine-tuning pre-trained Large Language Models (LLMs) for specialized tasks incurs substantial computational and data costs. While model merging offers a training-free solution to integrate multiple task-specific models, existing methods suffer from safety-utility conflicts where enhanced general capabilities degrade safety safeguards. We identify two root causes: $\textbf{neuron misidentification}$ due to simplistic parameter magnitude-based selection, and $\textbf{cross-task neuron interference}$ during merging. To address these challenges, we propose $\textbf{LED-Merging}$, a three-stage framework that $\textbf{L}$ocates task-specific neurons via gradient-based attribution, dynamically $\textbf{E}$lects critical neurons through multi-model importance fusion, and $\textbf{D}$isjoints conflicting updates through parameter isolation. Extensive experiments on Llama-3-8B, Mistral-7B, and Llama2-13B demonstrate that LED-Merging effectively reduces harmful response rates, showing a 31.4\% decrease on Llama-3-8B-Instruct on HarmBench, while simultaneously preserving 95\% of utility performance, such as achieving 52.39\% accuracy on GSM8K. LED-Merging resolves safety-utility conflicts and provides a lightweight, training-free paradigm for constructing reliable multi-task LLMs. Code is available at $\href{https://github.com/MqLeet/LED-Merging}{GitHub}$.

</details>


### [100] [TikZero: Zero-Shot Text-Guided Graphics Program Synthesis](https://arxiv.org/abs/2503.11509)

*Jonas Belouadi, Eddy Ilg, Margret Keuper, Hideki Tanaka, Masao Utiyama, Raj Dabre, Steffen Eger, Simone Paolo Ponzetto*

**Main category:** cs.CL

**Keywords:** graphics programs, text captions, TikZ, synthesis, image representations

**Relevance Score:** 6

**TL;DR:** Introduces TikZero, a method for synthesizing graphics programs from text captions using image representations as an intermediary.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To synthesize figures from text captions with high geometric precision and editability amidst a lack of aligned training data.

**Method:** Decouples graphics program generation from text understanding by utilizing image representations to train independently on graphics programs and captioned images.

**Key Contributions:**

	1. Introduced a novel method for graphics program generation using image representations.
	2. Demonstrated superior performance compared to existing baselines while requiring less aligned training data.
	3. Provided publicly available code and datasets for further research.

**Result:** TikZero outperforms traditional methods that rely solely on caption-aligned data, and it can match or exceed the performance of larger models when using aligned graphics programs as a training signal.

**Limitations:** 

**Conclusion:** TikZero enables better synthesis of graphics programs from text and offers a practical solution to available data issues in the field.

**Abstract:** Automatically synthesizing figures from text captions is a compelling capability. However, achieving high geometric precision and editability requires representing figures as graphics programs in languages like TikZ, and aligned training data (i.e., graphics programs with captions) remains scarce. Meanwhile, large amounts of unaligned graphics programs and captioned raster images are more readily available. We reconcile these disparate data sources by presenting TikZero, which decouples graphics program generation from text understanding by using image representations as an intermediary bridge. It enables independent training on graphics programs and captioned images and allows for zero-shot text-guided graphics program synthesis during inference. We show that our method substantially outperforms baselines that can only operate with caption-aligned graphics programs. Furthermore, when leveraging caption-aligned graphics programs as a complementary training signal, TikZero matches or exceeds the performance of much larger models, including commercial systems like GPT-4o. Our code, datasets, and select models are publicly available.

</details>


### [101] [Building Instruction-Tuning Datasets from Human-Written Instructions with Open-Weight Large Language Models](https://arxiv.org/abs/2503.23714)

*Youmi Ma, Sakae Mizuki, Kazuki Fujii, Taishi Nakamura, Masanari Ohi, Hinari Shimada, Taihei Shiotani, Koshiro Saito, Koki Maeda, Kakeru Hattori, Takumi Okamoto, Shigeki Ishida, Rio Yokota, Hiroya Takamura, Naoaki Okazaki*

**Main category:** cs.CL

**Keywords:** Large Language Models, Instruction Tuning, Human-Computer Interaction, Dataset Construction, Japanese Language

**Relevance Score:** 9

**TL;DR:** We present human-written instruction-tuning datasets that improve LLM fine-tuning performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** This paper addresses the necessity of human-originated signals in instruction tuning for LLMs.

**Method:** The study constructs instruction-tuning datasets using human-written instructions paired with LLM-generated responses, confirming their effectiveness.

**Key Contributions:**

	1. Development of state-of-the-art instruction-tuning datasets from human-written instructions.
	2. Demonstrated effectiveness of these datasets on LLMs, outperforming existing datasets.
	3. Public availability of datasets and fine-tuned models for broad use cases.

**Result:** LLMs fine-tuned on these human-sourced datasets consistently outperform those trained on previous datasets, including in Japanese.

**Limitations:** The tuned LLMs show a notable lack of culture-specific knowledge in the new language.

**Conclusion:** Human-sourced instruction-tuning is beneficial, enhancing LLM performance, particularly in new languages, though cultural knowledge may be limited.

**Abstract:** Instruction tuning is crucial for enabling Large Language Models (LLMs) to solve real-world tasks. Prior work has shown the effectiveness of instruction-tuning data synthesized solely from LLMs, raising a fundamental question: Do we still need human-originated signals for instruction tuning? This work answers the question affirmatively: we build state-of-the-art instruction-tuning datasets sourced from human-written instructions, by simply pairing them with LLM-generated responses. LLMs fine-tuned on our datasets consistently outperform those fine-tuned on existing ones. Our data construction approach can be easily adapted to other languages; we build datasets for Japanese and confirm that LLMs tuned with our data reach state-of-the-art performance. Analyses suggest that instruction-tuning in a new language allows LLMs to follow instructions, while the tuned models exhibit a notable lack of culture-specific knowledge in that language. The datasets and fine-tuned models will be publicly available. Our datasets, synthesized with open-weight LLMs, are openly distributed under permissive licenses, allowing for diverse use cases.

</details>


### [102] [ToolACE-R: Model-aware Iterative Training and Adaptive Refinement for Tool Learning](https://arxiv.org/abs/2504.01400)

*Xingshan Zeng, Weiwen Liu, Xu Huang, Zezhong Wang, Lingzhi Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruiming Tang, Qun Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Tool Learning, Adaptive Self-Refinement

**Relevance Score:** 8

**TL;DR:** ToolACE-R is a novel framework for LLMs that enhances tool learning through model-aware iterative training and adaptive self-refinement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capabilities of Large Language Models (LLMs) in leveraging external tools for complex user tasks beyond just data synthesis for fine-tuning.

**Method:** ToolACE-R employs a model-aware iterative training procedure to adjust training samples in response to the model's evolving capabilities and utilizes a self-refinement training corpus for optimizing tool invocation without needing external feedback.

**Key Contributions:**

	1. Introduction of ToolACE-R framework for tool learning in LLMs.
	2. Implementation of model-aware iterative training to adjust training samples dynamically.
	3. Development of adaptive self-refinement mechanism for test-time efficiency.

**Result:** Extensive experiments across benchmark datasets show that ToolACE-R outperforms advanced API-based models and improves tool invocation through adaptive self-refinement.

**Limitations:** 

**Conclusion:** ToolACE-R demonstrates effectiveness and generalizability in tool learning, providing a promising approach for scalable and efficient model capabilities.

**Abstract:** Tool learning, which allows Large Language Models (LLMs) to leverage external tools for solving complex user tasks, has emerged as a promising avenue for extending model capabilities. However, existing approaches primarily focus on data synthesis for fine-tuning LLMs to invoke tools effectively, largely ignoring how to fully stimulate the potential of the model. In this paper, we propose ToolACE-R, a novel framework that includes both model-aware iterative training and adaptive refinement for tool learning. ToolACE-R features a model-aware iterative training procedure that progressively adjust training samples based on the model's evolving capabilities to maximize its potential. Additionally, it incorporates self-refinement training corpus which emphasizes LLM's ability to iteratively refine their tool calls, optimizing performance without requiring external feedback. Furthermore, we introduce adaptive self-refinement mechanism for efficient test-time scaling, where the trained model can autonomously determine when to stop the process based on iterative self-refinement. We conduct extensive experiments across several benchmark datasets, showing that ToolACE-R achieves competitive performance compared to advanced API-based models. The performance of tool invocation can be further improved efficiently through adaptive self-refinement. These results highlight the effectiveness and generalizability of ToolACE-R, offering a promising direction for more efficient and scalable tool learning.

</details>


### [103] [CoTAL: Human-in-the-Loop Prompt Engineering for Generalizable Formative Assessment Scoring](https://arxiv.org/abs/2504.02323)

*Clayton Cohn, Ashwin T S, Naveeduddin Mohammed, Gautam Biswas*

**Main category:** cs.CL

**Keywords:** large language models, formative assessment, prompt engineering

**Relevance Score:** 9

**TL;DR:** This paper introduces CoTAL, an LLM-based approach for formative assessment scoring using Evidence-Centered Design and active learning to improve scoring effectiveness across educational domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the generalizability of prompt engineering approaches in educational contexts and enhance traditional formative assessment scoring methods using LLMs.

**Method:** The paper presents the CoTAL framework, which integrates Evidence-Centered Design with human-in-the-loop prompt engineering, employing chain-of-thought prompting and iterative feedback from teachers and students to improve assessment processes.

**Key Contributions:**

	1. Introduction of the CoTAL framework for formative assessment
	2. Demonstration of significant improvements in LLM performance through active learning
	3. Integration of teacher and student feedback into assessment processes.

**Result:** CoTAL demonstrates improved scoring performance of GPT-4, achieving a 38.9% gain over a baseline approach without prompt engineering, and receives positive evaluations from teachers and students regarding its effectiveness.

**Limitations:** 

**Conclusion:** CoTAL not only enhances the accuracy of scoring but also improves the quality of explanations associated with assessments, highlighting the value of iterative feedback.

**Abstract:** Large language models (LLMs) have created new opportunities to assist teachers and support student learning. While researchers have explored various prompt engineering approaches in educational contexts, the degree to which these approaches generalize across domains--such as science, computing, and engineering--remains underexplored. In this paper, we introduce Chain-of-Thought Prompting + Active Learning (CoTAL), an LLM-based approach to formative assessment scoring that (1) leverages Evidence-Centered Design (ECD) to align assessments and rubrics with curriculum goals, (2) applies human-in-the-loop prompt engineering to automate response scoring, and (3) incorporates chain-of-thought (CoT) prompting and teacher and student feedback to iteratively refine questions, rubrics, and LLM prompts. Our findings demonstrate that CoTAL improves GPT-4's scoring performance across domains, achieving gains of up to 38.9% over a non-prompt-engineered baseline (i.e., without labeled examples, chain-of-thought prompting, or iterative refinement). Teachers and students judge CoTAL to be effective at scoring and explaining responses, and their feedback produces valuable insights that enhance grading accuracy and explanation quality.

</details>
