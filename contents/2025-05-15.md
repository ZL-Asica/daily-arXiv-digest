# 2025-05-15

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 19]

- [cs.CL](#cs.CL) [Total: 27]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [WaLLM -- Insights from an LLM-Powered Chatbot deployment via WhatsApp](https://arxiv.org/abs/2505.08894)

*Hiba Eltigani, Rukhshan Haroon, Asli Kocak, Abdullah Bin Faisal, Noah Martin, Fahad Dogar*

**Main category:** cs.HC

**Keywords:** WaLLM, WhatsApp chatbot, digital divide, user engagement, health informatics

**Relevance Score:** 8

**TL;DR:** WaLLM is a custom AI chatbot over WhatsApp designed to improve access to information in developing regions, focusing on user engagement and health topics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the digital divide in education and decision-making in developing regions through a widely used communication platform.

**Method:** Developed a WhatsApp-based chatbot (WaLLM) and analyzed over 6 months of user interaction logs to understand engagement and information-seeking behavior.

**Key Contributions:**

	1. Design and implementation of WaLLM for WhatsApp
	2. Analysis of user interaction logs to derive insights on engagement
	3. Recommendations for UI design and trust calibration in AI systems

**Result:** 55% of queries sought factual information, with 'Health and well-being' being the most popular category (28%), and users engaged more with features like the leaderboard.

**Limitations:** 

**Conclusion:** The study highlights the importance of culture-based customization and the need for careful calibration of user trust in AI systems in developing areas.

**Abstract:** Recent advances in generative AI, such as ChatGPT, have transformed access to information in education, knowledge-seeking, and everyday decision-making. However, in many developing regions, access remains a challenge due to the persistent digital divide. To help bridge this gap, we developed WaLLM - a custom AI chatbot over WhatsApp, a widely used communication platform in developing regions. Beyond answering queries, WaLLM offers several features to enhance user engagement: a daily top question, suggested follow-up questions, trending and recent queries, and a leaderboard-based reward system. Our service has been operational for over 6 months, amassing over 14.7K queries from approximately 100 users. In this paper, we present WaLLM's design and a systematic analysis of logs to understand user interactions. Our results show that 55% of user queries seek factual information. "Health and well-being" was the most popular topic (28%), including queries about nutrition and disease, suggesting users view WaLLM as a reliable source. Two-thirds of users' activity occurred within 24 hours of the daily top question. Users who accessed the "Leaderboard" interacted with WaLLM 3x as those who did not. We conclude by discussing implications for culture-based customization, user interface design, and appropriate calibration of users' trust in AI systems for developing regions.

</details>


### [2] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)

*Lucas McCullum, Pelagie Ami Agassi, Leo Anthony Celi, Daniel K. Ebner, Chrystinne Oliveira Fernandes, Rachel S. Hicklen, Mkliwa Koumbia, Lisa Soleymani Lehmann, David Restrepo*

**Main category:** cs.HC

**Keywords:** LLMs, patient care, human-LLM collaboration, clinical settings, health informatics

**Relevance Score:** 9

**TL;DR:** Explores the need for strategies to effectively collaborate between humans and LLMs in clinical settings, moving beyond mere comparison of LLMs to human experts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the potential risks posed by LLMs in patient care due to their evolving nature and the ill-defined concept of 'expert' in this context.

**Method:** The paper advocates for a shift in research focus towards creating frameworks for safe human-LLM collaboration in healthcare, rather than direct comparison of their capabilities.

**Key Contributions:**

	1. Emphasizes the need for a new approach to human-LLM interaction in healthcare.
	2. Highlights the risks of comparing LLMs to human experts without clear definitions.
	3. Advocates for frameworks that enable safe collaboration rather than competition.

**Result:** Proposes that efficient human-LLM partnerships may enhance patient care while mitigating risks associated with LLM deployment.

**Limitations:** 

**Conclusion:** Future research must prioritize symbiotic relationships between humans and LLMs in clinical contexts to ensure patient safety and effective care.

**Abstract:** Currently, a considerable research effort is devoted to comparing LLMs to a group of human experts, where the term "expert" is often ill-defined or variable, at best, in a state of constantly updating LLM releases. Without proper safeguards in place, LLMs will threaten to cause harm to the established structure of safe delivery of patient care which has been carefully developed throughout history to keep the safety of the patient at the forefront. A key driver of LLM innovation is founded on community research efforts which, if continuing to operate under "humans versus LLMs" principles, will expedite this trend. Therefore, research efforts moving forward must focus on effectively characterizing the safe use of LLMs in clinical settings that persist across the rapid development of novel LLM models. In this communication, we demonstrate that rather than comparing LLMs to humans, there is a need to develop strategies enabling efficient work of humans with LLMs in an almost symbiotic manner.

</details>


### [3] [Tracing the Invisible: Understanding Students' Judgment in AI-Supported Design Work](https://arxiv.org/abs/2505.08939)

*Suchismita Naik, Prakash Shukla, Ike Obi, Jessica Backus, Nancy Rasche, Paul Parsons*

**Main category:** cs.HC

**Keywords:** HCI, generative AI, design judgment, co-creation, trustworthiness

**Relevance Score:** 8

**TL;DR:** The study analyzes reflections from students in an HCI design course on their judgments while using generative AI tools as collaborators in design workflows.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how students incorporate generative AI tools in design and the implications for their creative judgment in HCI.

**Method:** Analysis of reflections from 33 student teams engaged in an HCI design course.

**Key Contributions:**

	1. Introduces new forms of design judgment that encompass the interaction with generative AI.
	2. Provides a conceptual lens for co-creative sensemaking with AI in design contexts.
	3. Highlights the complexities in negotiating creative responsibility between students and AI.

**Result:** Identified established and emergent forms of design judgment, including agency-distribution judgment and reliability judgment, indicating new complexities in creative responsibility and trust in AI outputs.

**Limitations:** 

**Conclusion:** Generative AI complicates design reasoning and encourages students to reflect critically on their co-creative processes with AI.

**Abstract:** As generative AI tools become integrated into design workflows, students increasingly engage with these tools not just as aids, but as collaborators. This study analyzes reflections from 33 student teams in an HCI design course to examine the kinds of judgments students make when using AI tools. We found both established forms of design judgment (e.g., instrumental, appreciative, quality) and emergent types: agency-distribution judgment and reliability judgment. These new forms capture how students negotiate creative responsibility with AI and assess the trustworthiness of its outputs. Our findings suggest that generative AI introduces new layers of complexity into design reasoning, prompting students to reflect not only on what AI produces, but also on how and when to rely on it. By foregrounding these judgments, we offer a conceptual lens for understanding how students engage in co-creative sensemaking with AI in design contexts.

</details>


### [4] [Positioning Monocular Optical See Through Head Worn Displays in Glasses for Everyday Wear](https://arxiv.org/abs/2505.09047)

*Parth Arora, Ethan Kimmel, Katherine Huang, Tyler Kwok, Yukun Song, Sofia Vempala, Georgianna Lin, Ozan Cakmakci, Thad Starner*

**Main category:** cs.HC

**Keywords:** Head-worn displays, User experience, Display positioning, Social perception, Monocular optical see-through

**Relevance Score:** 7

**TL;DR:** This paper explores optimal positioning of head-worn displays (HWDs) in eyeglasses to enhance user experience and reduce social interruption.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of display positioning on user experience with head-worn displays in everyday scenarios, particularly regarding comfort and social perceptions.

**Method:** The paper consolidates findings from recent studies on monocular optical see-through head-worn displays, focusing on performance, comfort, and social perception based on different display positions.

**Key Contributions:**

	1. Provides guidelines for effective visual display positioning in smart eyewear.
	2. Addresses user comfort and societal perceptions of HWDs occupations.
	3. Synthesizes recent research for practical applications in design of HWDs.

**Result:** The study recommends specific display placements for optimal interaction, suggesting a horizontal field of view of 15° with an offset towards the ear for minimal interruption during tasks.

**Limitations:** The study primarily focuses on monocular displays and text-based tasks, limiting applicability to wider use cases.

**Conclusion:** Proper positioning of HWDs can enhance user comfort and performance while mitigating negative social perceptions.

**Abstract:** Head-worn displays for everyday wear in the form of regular eyeglasses are technically feasible with recent advances in waveguide technology. One major design decision is determining where in the user's visual field to position the display. Centering the display in the principal point of gaze (PPOG) allows the user to switch attentional focus between the virtual and real images quickly, and best performance often occurs when the display is centered in PPOG or is centered vertically below PPOG. However, these positions are often undesirable in that they are considered interruptive or are associated with negative social perceptions by users. Offsetting the virtual image may be preferred when tasks involve driving, walking, or social interaction. This paper consolidates findings from recent studies on monocular optical see-through HWDs (OST-HWDs), focusing on potential for interruption, comfort, performance, and social perception. For text-based tasks, which serve as a proxy for many monocular OST-HWD tasks, we recommend a 15{\deg} horizontal field of view (FOV) with the virtual image in the right lens vertically centered but offset to +8.7{\deg} to +23.7{\deg} toward the ear. Glanceable content can be offset up to +30{\deg} for short interactions.

</details>


### [5] [EcoSphere: A Decision-Support Tool for Automated Carbon Emission and Cost Optimization in Sustainable Urban Development](https://arxiv.org/abs/2505.09054)

*Siavash Ghorbany, Ming Hu, Siyuan Yao, Matthew Sisk, Chaoli Wang*

**Main category:** cs.HC

**Keywords:** EcoSphere, carbon emissions, urban planning, sustainable development, computer vision

**Relevance Score:** 4

**TL;DR:** EcoSphere software evaluates embodied and operational carbon in urban planning using computer vision and NLP to assist policymakers in sustainable development.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the construction industry's contribution to greenhouse gas emissions through the evaluation of carbon emissions.

**Method:** Developed EcoSphere software utilizing high-resolution data and applying computer vision along with NLP to classify building materials and emissions.

**Key Contributions:**

	1. EcoSphere categorizes buildings by structural and material characteristics.
	2. It creates a baseline emissions dataset from high-resolution data.
	3. The software simulates policy scenarios to provide actionable insights.

**Result:** EcoSphere produces a baseline emissions dataset and allows the simulation of various policy scenarios for better decision-making in urban planning.

**Limitations:** 

**Conclusion:** The software aids in making data-driven policy decisions that can lead to carbon neutrality in urban environments.

**Abstract:** The construction industry is a major contributor to global greenhouse gas emissions, with embodied carbon being a key component. This study develops EcoSphere, an innovative software designed to evaluate and balance embodied and operational carbon emissions with construction and environmental costs in urban planning. Using high-resolution data from the National Structure Inventory, combined with computer vision and natural language processing applied to Google Street View and satellite imagery, EcoSphere categorizes buildings by structural and material characteristics with a bottom-up approach, creating a baseline emissions dataset. By simulating policy scenarios and mitigation strategies, EcoSphere provides policymakers and non-experts with actionable insights for sustainable development in cities and provide them with a vision of the environmental and financial results of their decisions. Case studies in Chicago and Indianapolis showcase how EcoSphere aids in assessing policy impacts on carbon emissions and costs, supporting data-driven progress toward carbon neutrality.

</details>


### [6] [Display Content, Display Methods and Evaluation Methods of the HCI in Explainable Recommender Systems: A Survey](https://arxiv.org/abs/2505.09065)

*Weiqing Li, Yue Xu, Yuefeng Li, Yinghui Huang*

**Main category:** cs.HC

**Keywords:** Explainable Recommender Systems, Human-Computer Interaction, Video Recommendations

**Relevance Score:** 8

**TL;DR:** This paper presents a unified framework for Explainable Recommender Systems (XRS), addressing algorithmic challenges and highlighting the role of multimedia in explanations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the research on Explainable Recommender Systems (XRS) by providing a unified framework that considers algorithmic aspects as well as human-computer interaction layers.

**Method:** The study synthesizes existing literature on XRS and adopts a lifecycle perspective to summarize technologies and methods in the field, focusing on multimedia, especially video-based explanations.

**Key Contributions:**

	1. Unified framework for XRS research and development
	2. Lifecycle perspective on technologies in XRS
	3. Focus on video-based explanations and evaluation methods

**Result:** The paper emphasizes the importance of considering video-based explanations and provides a structured overview of evaluation methods for XRS.

**Limitations:** 

**Conclusion:** The findings offer insights for systematically designing, progressing, and testing XRS while addressing the HCI layer and challenges in the field.

**Abstract:** Explainable Recommender Systems (XRS) aim to provide users with understandable reasons for the recommendations generated by these systems, representing a crucial research direction in artificial intelligence (AI). Recent research has increasingly focused on the algorithms, display, and evaluation methodologies of XRS. While current research and reviews primarily emphasize the algorithmic aspects, with fewer studies addressing the Human-Computer Interaction (HCI) layer of XRS. Additionally, existing reviews lack a unified taxonomy for XRS and there is insufficient attention given to the emerging area of short video recommendations. In this study, we synthesize existing literature and surveys on XRS, presenting a unified framework for its research and development. The main contributions are as follows: 1) We adopt a lifecycle perspective to systematically summarize the technologies and methods used in XRS, addressing challenges posed by the diversity and complexity of algorithmic models and explanation techniques. 2) For the first time, we highlight the application of multimedia, particularly video-based explanations, along with its potential, technical pathways, and challenges in XRS. 3) We provide a structured overview of evaluation methods from both qualitative and quantitative dimensions. These findings provide valuable insights for the systematic design, progress, and testing of XRS.

</details>


### [7] [PLanet: Formalizing Experimental Design](https://arxiv.org/abs/2505.09094)

*London Bielicke, Anna Zhang, Shruti Tyagi, Emery Berger, Adam Chlipala, Eunice Jun*

**Main category:** cs.HC

**Keywords:** human-computer interaction, domain specific language, experimental design

**Relevance Score:** 6

**TL;DR:** This paper introduces PLanet, a domain-specific language for constructing experimental design plans, enhancing clarity and reducing ambiguities in scientific studies.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in specifying, communicating, and relating experimental design plans due to their complexity.

**Method:** The authors developed a grammar of experimental design that allows for composable operators to create assignment procedures, implemented in the DSL PLanet, which operates in three stages: experimental unit specification, trial-order construction, and order-to-unit mapping.

**Key Contributions:**

	1. Introduction of a grammar of experimental design
	2. Development of PLanet DSL for assignment planning
	3. Evaluation demonstrating PLanet's ability to clarify complex experimental designs

**Result:** PLanet was able to express 11 out of 12 experiments from a sample of recent CHI and UIST publications, revealing ambiguities in the original designs.

**Limitations:** 

**Conclusion:** The use of PLanet facilitates explicit communication of design choices, particularly when researchers fail to provide thorough technical descriptions.

**Abstract:** Carefully constructed experimental designs are essential for drawing valid, generalizable conclusions from scientific studies. Unfortunately, experimental design plans can be difficult to specify, communicate clearly, and relate to alternatives. In response, we introduce a grammar of experimental design that provides composable operators for constructing assignment procedures (e.g., Latin square). We implement this grammar in PLanet, a domain-specific language (DSL) that constructs assignment plans in three stages: experimental unit specification, trial-order construction, and order-to-unit mapping. We evaluate PLanet's expressivity by taking a purposive sample of recent CHI and UIST publications, representing their experiments as programs in PLanet, and identifying ambiguities and alternatives. In our evaluation, PLanet could express 11 out of 12 experiments found in sampled papers. Additionally, we found that PLanet constructs helped make complex design choices explicit when the researchers omit technical language describing their study designs.

</details>


### [8] [PreCare: Designing AI Assistants for Advance Care Planning (ACP) to Enhance Personal Value Exploration, Patient Knowledge, and Decisional Confidence](https://arxiv.org/abs/2505.09115)

*Yu Lun Hsu, Yun-Rung Chou, Chiao-Ju Chang, Yu-Cheng Chang, Zer-Wei Lee, Rokas Gipiškis, Rachel Li, Chih-Yuan Shih, Jen-Kuei Peng, Hsien-Liang Huang, Jaw-Shiun Tsai, Mike Y. Chen*

**Main category:** cs.HC

**Keywords:** Advance Care Planning, AI-driven assistance, usability study

**Relevance Score:** 7

**TL;DR:** The paper presents PreCare, an AI-driven platform for Advance Care Planning (ACP) that enhances user engagement and decision-making compared to traditional online ACP services.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of online Advance Care Planning (ACP) which lacks personalized consultation benefits, this research aims to enhance user understanding and decision making regarding end-of-life treatments.

**Method:** Two formative studies were conducted: one involved shadowing and interviewing ACP teams (3 teams, 18 patients), and the other interviewed users of ACP websites (14 users). Based on these insights, PreCare was designed in collaboration with 6 ACP professionals.

**Key Contributions:**

	1. Design and implementation of PreCare, an AI-driven ACP platform
	2. Demonstrated significant improvement in user engagement and decisional confidence
	3. Collaboration with healthcare professionals to align technology with clinical needs

**Result:** Usability testing of PreCare (n=12) resulted in an excellent System Usability Scale (SUS) rating. A comparative evaluation (n=12) demonstrated that PreCare's AI assistants significantly improved users' exploration of personal values, knowledge, and decisional confidence, with 92% user preference over alternatives.

**Limitations:** 

**Conclusion:** PreCare is an effective tool for enhancing user engagement in Advance Care Planning, leveraging AI to improve understanding and comfort in making end-of-life decisions.

**Abstract:** Advance Care Planning (ACP) allows individuals to specify their preferred end-of-life life-sustaining treatments before they become incapacitated by injury or terminal illness (e.g., coma, cancer, dementia). While online ACP offers high accessibility, it lacks key benefits of clinical consultations, including personalized value exploration, immediate clarification of decision consequences. To bridge this gap, we conducted two formative studies: 1) shadowed and interviewed 3 ACP teams consisting of physicians, nurses, and social workers (18 patients total), and 2) interviewed 14 users of ACP websites. Building on these insights, we designed PreCare in collaboration with 6 ACP professionals. PreCare is a website with 3 AI-driven assistants designed to guide users through exploring personal values, gaining ACP knowledge, and supporting informed decision-making. A usability study (n=12) showed that PreCare achieved a System Usability Scale (SUS) rating of excellent. A comparative evaluation (n=12) showed that PreCare's AI assistants significantly improved exploration of personal values, knowledge, and decisional confidence, and was preferred by 92% of participants.

</details>


### [9] [An Initial Exploration of Default Images in Text-to-Image Generation](https://arxiv.org/abs/2505.09166)

*Hannu Simonen, Atte Kiviniemi, Jonas Oppenlaender*

**Main category:** cs.HC

**Keywords:** text-to-image generation, default images, user satisfaction

**Relevance Score:** 6

**TL;DR:** This paper investigates 'default images' generated by text-to-image models when prompts contain unknown terms, focusing on Midjourney.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of default images on user satisfaction and improve text-to-image generation models and prompt engineering.

**Method:** A systematic approach to create prompts that trigger default images, followed by experiments and a survey on user satisfaction.

**Key Contributions:**

	1. First investigation into default images in TTI
	2. Development of systematic prompting approach
	3. Insights into user satisfaction relating to default images

**Result:** Findings from user surveys indicating that default images affect user satisfaction; results from initial experiments and ablation studies are presented.

**Limitations:** Focused on a specific model (Midjourney), limiting generalizability.

**Conclusion:** The study highlights the challenges posed by default images in text-to-image generation and suggests directions for future research.

**Abstract:** In the creative practice of text-to-image generation (TTI), images are generated from text prompts. However, TTI models are trained to always yield an output, even if the prompt contains unknown terms. In this case, the model may generate what we call "default images": images that closely resemble each other across many unrelated prompts. We argue studying default images is valuable for designing better solutions for TTI and prompt engineering. In this paper, we provide the first investigation into default images on Midjourney, a popular image generator. We describe our systematic approach to create input prompts triggering default images, and present the results of our initial experiments and several small-scale ablation studies. We also report on a survey study investigating how default images affect user satisfaction. Our work lays the foundation for understanding default images in TTI and highlights challenges and future research directions.

</details>


### [10] [Educational impacts of generative artificial intelligence on learning and performance of engineering students in China](https://arxiv.org/abs/2505.09208)

*Lei Fan, Kunyang Deng, Fangxue Liu*

**Main category:** cs.HC

**Keywords:** Generative AI, Engineering Education, Learning Experience, Higher Education, Student Perspectives

**Relevance Score:** 6

**TL;DR:** This study investigates the impact of generative AI on engineering students' learning experiences across China, revealing both benefits and challenges in its adoption.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential applications of generative AI in higher education, particularly in engineering disciplines, and assess its impact on students' learning experiences.

**Method:** Surveys were conducted with 148 students from diverse engineering disciplines and regions in China, focusing on usage frequency, learning impact, challenges, and future prospects of generative AI in education.

**Key Contributions:**

	1. First-hand insights into the benefits and challenges of generative AI for engineering students
	2. Recommendations for integrating generative AI into engineering education
	3. Empirical data on student perceptions of generative AI usage

**Result:** More than half of the students reported positive effects on learning efficiency, initiative, and creativity, while concerns were raised about academic performance, accuracy, and reliability of generative AI.

**Limitations:** The study is based on self-reported data from a specific demographic (engineering students in China), which may not be generalizable to other fields or regions.

**Conclusion:** Generative AI offers both opportunities and challenges for engineering education, necessitating recommendations for effective integration based on student perspectives.

**Abstract:** With the rapid advancement of generative artificial intelligence(AI), its potential applications in higher education have attracted significant attention. This study investigated how 148 students from diverse engineering disciplines and regions across China used generative AI, focusing on its impact on their learning experience and the opportunities and challenges it poses in engineering education. Based on the surveyed data, we explored four key areas: the frequency and application scenarios of AI use among engineering students, its impact on students' learning and performance, commonly encountered challenges in using generative AI, and future prospects for its adoption in engineering education. The results showed that more than half of the participants reported a positive impact of generative AI on their learning efficiency, initiative, and creativity, with nearly half believing it also enhanced their independent thinking. However, despite acknowledging improved study efficiency, many felt their actual academic performance remained largely unchanged and expressed concerns about the accuracy and domain-specific reliability of generative AI. Our findings provide a first-hand insight into the current benefits and challenges generative AI brings to students, particularly Chinese engineering students, while offering several recommendations, especially from the students' perspective, for effectively integrating generative AI into engineering education.

</details>


### [11] [A Note on Semantic Diffusion](https://arxiv.org/abs/2505.09283)

*Alexander P. Ryjov, Alina A. Egorova*

**Main category:** cs.HC

**Keywords:** semantic diffusion, large language models, design applications

**Relevance Score:** 8

**TL;DR:** This paper introduces a hybrid framework combining large language models and semantic diffusion to enhance iterative refinement in design applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The limitation of traditional LLMs and diffusion models in producing convergent design outputs motivated the development of this hybrid framework.

**Method:** The proposed framework enhances the diffusion mechanism by ensuring a convergent search process for design refinements.

**Key Contributions:**

	1. Introduction of a hybrid LLM + semantic diffusion framework
	2. Formalization of the convergence process in design outputs
	3. Improvement of iterative refinement in design applications

**Result:** The hybrid model demonstrated improved capability for localized design refinement by addressing the stochastic nature of output generation in conventional models.

**Limitations:** 

**Conclusion:** This work presents a novel approach that formalizes the design refinement process, potentially improving the usability of models in design settings.

**Abstract:** This paper provides an in-depth examination of the concept of semantic diffusion as a complementary instrument to large language models (LLMs) for design applications. Conventional LLMs and diffusion models fail to induce a convergent, iterative refinement process: each invocation of the diffusion mechanism spawns a new stochastic cycle, so successive outputs do not relate to prior ones and convergence toward a desired design is not guaranteed. The proposed hybrid framework - "LLM + semantic diffusion" - resolves this limitation by enforcing an approximately convergent search procedure, thereby formally addressing the problem of localized design refinement.

</details>


### [12] [AfforDance: Personalized AR Dance Learning System with Visual Affordance](https://arxiv.org/abs/2505.09376)

*Hyunyoung Han, Jongwon Jang, Kitaeg Shim, Sang Ho Yoon*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Dance Education, Personalized Learning

**Relevance Score:** 6

**TL;DR:** AfforDance is an AR-based dance learning system that personalizes education through 3D avatars and interactive visuals.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance dance learning experiences using augmented reality and personalized content.

**Method:** The system converts dance videos into interactive experiences with 3D avatars, audio synchronization, and adaptive visual cues.

**Key Contributions:**

	1. Interactive learning through AR
	2. User-centered design for dance education
	3. Adaptive visual cues for movement guidance

**Result:** AfforDance offers an improved, adaptable learning interface for personalized dance education.

**Limitations:** 

**Conclusion:** The system demonstrates potential for transforming traditional dance education through technology.

**Abstract:** We propose AfforDance, an augmented reality (AR)-based dance learning system that generates personalized learning content and enhances learning through visual affordances. Our system converts user-selected dance videos into interactive learning experiences by integrating 3D reference avatars, audio synchronization, and adaptive visual cues that guide movement execution. This work contributes to personalized dance education by offering an adaptable, user-centered learning interface.

</details>


### [13] [Utilization of Skin Color Change for Image-based Tactile Sensing](https://arxiv.org/abs/2505.09402)

*Seitaro Kaneko, Hiroki Ishizuka, Hidenori Yoshimura, Hiroyuki Kajimoto*

**Main category:** cs.HC

**Keywords:** pressure measurement, skin color change, human-computer interaction, finite element analysis, sensor technology

**Relevance Score:** 7

**TL;DR:** Proposes a direct method to measure fingertip pressure distribution using skin color change due to blood flow.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing pressure measurement methods that do not allow direct contact with textured surfaces.

**Method:** A novel technique measuring pressure distribution through direct touch with a transparent object and monitoring skin color changes via blood flow.

**Key Contributions:**

	1. Direct measurement of pressure distribution via skin color change
	2. Analysis of the relationship between applied pressure and color change
	3. Finite element analysis to understand spatial non-uniformity.

**Result:** Establishes a correlation between applied pressure and skin color change, using finite element analysis to address spatial non-uniformities in color change.

**Limitations:** The contact area and the color change area did not align perfectly.

**Conclusion:** The proposed method can effectively measure internal stress distribution and may serve as a simple sensor in human-computer interactions.

**Abstract:** Measurement of pressure distribution applied to a fingertip is crucial for the teleoperation of robots and human computer interface. Previous studies have acquired pressure distribution by affixing a sensor array to the fingertip or by optically recording the deformation of an object. However, these existing methods inhibit the fingertip from directly contacting the texture, and the pressure applied to the fingertip is measured indirectly. In this study, we propose a method to measure pressure distribution by directly touching a transparent object, focusing on the change in skin color induced by the applied pressure, caused by blood flow. We evaluated the relationship between pressure and skin color change when local pressure is applied, and found a correlation between the pressure and the color change. However, the contact area and the color change area did not align perfectly. We further explored the factor causing the spatial non-uniformity of the color change, by accounting for the stress distribution using finite element analysis. These results suggest that the proposed measurement method can be utilized to measure the internal stress distribution, and it is anticipated to serve as a simple sensor in the field of human computer interface.

</details>


### [14] [Card Sorting Simulator: Augmenting Design of Logical Information Architectures with Large Language Models](https://arxiv.org/abs/2505.09478)

*Eduard Kuric, Peter Demcak, Matus Krajcovic*

**Main category:** cs.HC

**Keywords:** card sorting, human-computer interaction, artificial intelligence, large language models, user research tools

**Relevance Score:** 9

**TL;DR:** This paper presents the development of a Card Sorting Simulator that uses AI to enhance card sorting methodologies in HCI research by generating categorizations, and evaluates its effectiveness against actual participant data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve digital card sorting tools through automated feedback using AI, aiming to enhance the ideation process for understanding user content organization.

**Method:** A prototype tool, the Card Sorting Simulator, harnesses Large Language Models (LLMs) to produce categorizations of cards. A comparative study analyzed data from 28 existing card sorting studies with 1,399 participants to compare AI-generated clusterings with actual participant results.

**Key Contributions:**

	1. Development of an AI-based Card Sorting Simulator
	2. Analysis of AI clustering against real participant data
	3. Provision of insights on improving card sorting methodologies using AI.

**Result:** The study found a good degree of agreement between AI-generated categorizations and real participant data, though there were noted inconsistencies related to the complexity of card labels and their impact on simulation accuracy.

**Limitations:** The study notes inconsistencies from the use of top-down models and the impact of card complexity on simulation accuracy.

**Conclusion:** AI can augment card sorting research by providing valuable preliminary feedback, but further studies are needed to refine and validate AI-driven tools in user research.

**Abstract:** Card sorting is a common ideation technique that elicits information on users' mental organization of content and functionality by having them sort items into categories. For more robust card sorting research, digital card sorting tools could benefit from providing quick automated feedback. Our objective of this research is to advance toward an instrument that applies artificial intelligence (AI) to augment card sorting. For this purpose, we develop the Card Sorting Simulator, a prototype tool that leverages Large Language Models (LLMs) to generate informative categorizations of cards. To illuminate how aligned the simulation is with card sorting by actual participants, and to inform the instrument's design decisions, we conducted a generalizability-focused comparative study. We obtained 28 pre-existing card sorting studies from real practitioners, comprising 1,399 participants, along with diverse contents and origins. With this dataset, we conducted a comprehensive and nuanced analysis of the agreement between actual card sorting results (clusterings of cards) and synthetic clusterings across a multitude of LLMs and prompt designs. Mutual information scores indicate a good degree of agreement to real result clustering, although similarity matrices also demonstrate inconsistencies from mental models, which can be attributed to their top-down nature. Furthermore, the number of cards or complexity of their labels impact the accuracy of its simulation. These findings bolster the case for AI augmentation in card sorting research as a source of meaningful preliminary feedback and highlight the need for further study for the development and validation of intelligent user research tools.

</details>


### [15] [Partnership through Play: Investigating How Long-Distance Couples Use Digital Games to Facilitate Intimacy](https://arxiv.org/abs/2505.09509)

*Nisha Devasia, Adrian Rodriguez, Logan Tuttle, Julie Kientz*

**Main category:** cs.HC

**Keywords:** Long-distance relationships, Multiplayer video games, Relational maintenance, Couple play styles, Game design

**Relevance Score:** 4

**TL;DR:** This study explores how couples in long-distance relationships use multiplayer video games for relational maintenance and affection expression.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the role of multiplayer video games in maintaining long-distance relationships, which are increasingly common among young adults.

**Method:** A mixed-methods approach was employed, collecting data from 13 couples in long-distance relationships who frequently play games together.

**Key Contributions:**

	1. Investigated couple play styles in multiplayer gaming among LDRs.
	2. Highlighted the appropriation of game mechanics for expressing affection.
	3. Developed prototypes addressing the limitations of physical sensations in gaming.

**Result:** The study identified significant differences in couple play styles and how game mechanics are appropriated for virtual affection.

**Limitations:** Limited sample size of 13 couples may not represent all LDR experiences.

**Conclusion:** Prototypes and design implications were created to address the needs of couples regarding physical sensation and memorabilia storage in popular games.

**Abstract:** Long-distance relationships (LDRs) have become more common in the last few decades, primarily among young adults pursuing educational or employment opportunities. A common way for couples in LDRs to spend time together is by playing multiplayer video games, which are often a shared hobby and therefore a preferred joint activity. However, games are relatively understudied in the context of relational maintenance for LDRs. In this work, we used a mixed-methods approach to collect data on the experiences of 13 couples in LDRs who frequently play games together. We investigated different values around various game mechanics and modalities and found significant differences in couple play styles, and also detail how couples appropriate game mechanics to express affection to each other virtually. We also created prototypes and design implications based on couples' needs surrounding the lack of physical sensation and memorabilia storage in most popular games.

</details>


### [16] [Evaluation Metrics for Misinformation Warning Interventions: Challenges and Prospects](https://arxiv.org/abs/2505.09526)

*Hussaini Zubairu, Abdelrahaman Abdou, Ashraf Matrawy*

**Main category:** cs.HC

**Keywords:** misinformation, user-centered interventions, effectiveness metrics, behavioral impact, trust

**Relevance Score:** 4

**TL;DR:** This paper reviews metrics for evaluating the effectiveness of user-centered interventions against misinformation, identifying challenges and proposing future research directions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Misinformation is a significant societal issue that necessitates effective intervention strategies, specifically user-centered interventions like warning systems.

**Method:** The paper categorizes and reviews existing metrics used to assess the effectiveness of misinformation warnings into four groups: behavioral impact, trust and credulity, usability, and cognitive and psychological effects.

**Key Contributions:**

	1. Comprehensive categorization of metrics for misinformation warning effectiveness
	2. Identification of key challenges in current approaches
	3. Proposals for future research directions in the field

**Result:** The review highlights critical challenges in measuring effectiveness, including inconsistency in cognitive metrics, lack of standardized emotional impact metrics, variations in user trust, and the need for inclusive warning designs.

**Limitations:** No systematic review has thoroughly examined these metrics until now, indicating gaps in existing research.

**Conclusion:** The paper outlines the current gaps in research and suggests areas for future studies to improve the design and assessment of misinformation warnings.

**Abstract:** Misinformation has become a widespread issue in the 21st century, impacting numerous areas of society and underscoring the need for effective intervention strategies. Among these strategies, user-centered interventions, such as warning systems, have shown promise in reducing the spread of misinformation. Many studies have used various metrics to evaluate the effectiveness of these warning interventions. However, no systematic review has thoroughly examined these metrics in all studies. This paper provides a comprehensive review of existing metrics for assessing the effectiveness of misinformation warnings, categorizing them into four main groups: behavioral impact, trust and credulity, usability, and cognitive and psychological effects. Through this review, we identify critical challenges in measuring the effectiveness of misinformation warnings, including inconsistent use of cognitive and attitudinal metrics, the lack of standardized metrics for affective and emotional impact, variations in user trust, and the need for more inclusive warning designs. We present an overview of these metrics and propose areas for future research.

</details>


### [17] [Beyond Likes: How Normative Feedback Complements Engagement Signals on Social Media](https://arxiv.org/abs/2505.09583)

*Yuchen Wu, Mingduo Zhao, John Canny*

**Main category:** cs.HC

**Keywords:** prosociial feedback, machine learning, social media, toxic content, expert feedback

**Relevance Score:** 9

**TL;DR:** The study proposes structured prosocial feedback using machine learning and LLMs to counteract the negative impact of conventional engagement signals on content quality in online platforms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional engagement signals like likes and upvotes can promote toxic content and hinder inclusivity. This study aims to find a solution to enhance content quality and user engagement positively.

**Method:** A machine learning feedback system powered by a large language model (LLM) was designed to evaluate user comments based on positive psychology principles, followed by a pre-registered user study to assess its impact.

**Key Contributions:**

	1. Introduction of structured prosocial feedback as a complement to traditional engagement signals.
	2. Implementation of a machine learning system that evaluates content based on psychological principles.
	3. Evidence showing expert feedback reduces toxicity and enhances content quality in social media.

**Result:** The study found that peer feedback increases conformity to popularity cues, while expert feedback led users toward higher-quality content. Combining expert feedback with peer evaluations improved both expert alignment and reduced toxicity in the community.

**Limitations:** The study is limited to the specific social media context tested and may not generalize to all online platforms.

**Conclusion:** Incorporating normative signals like expert scores into feedback systems can foster healthier online environments by addressing the limitations of traditional engagement metrics.

**Abstract:** Many online platforms incorporate engagement signals--such as likes and upvotes--into their content ranking systems and interface design. These signals are designed to boost user engagement. However, they can unintentionally elevate content that is less inclusive and may not support normatively desirable behavior. This issue becomes especially concerning when toxic content correlates strongly with popularity indicators such as likes and upvotes. In this study, we propose structured prosocial feedback as a complementary signal to likes and upvotes--one that highlights content quality based on normative criteria to help address the limitations of conventional engagement signals. We begin by designing and implementing a machine learning feedback system powered by a large language model (LLM), which evaluates user comments based on principles of positive psychology, such as individual well-being, constructive social media use, and character strengths. We then conduct a pre-registered user study to examine how existing peer-based and the new expert-based feedback interact to shape users' selection of comments in a social media setting. Results show that peer feedback increases conformity to popularity cues, while expert feedback shifts preferences toward normatively higher-quality content. Moreover, incorporating expert feedback alongside peer evaluations improves alignment with expert assessments and contributes to a less toxic community environment. This illustrates the added value of normative cues--such as expert scores generated by LLMs using psychological rubrics--and underscores the potential benefits of incorporating such signals into platform feedback systems to foster healthier online environments.

</details>


### [18] [Manifesto for Putting 'Chartjunk' in the Trash 2021!](https://arxiv.org/abs/2109.10132)

*Derya Akbaba, Jack Wilburn, Main T. Nance, Miriah Meyer*

**Main category:** cs.HC

**Keywords:** chartjunk, visualization, research, activism, maintenance art

**Relevance Score:** 4

**TL;DR:** The paper calls on the visualization research community to eliminate the term 'chartjunk' from their discussions and practices, proposing alternative ways to express and design visualizations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge and refine the language used in visualization research, particularly the term 'chartjunk', which is seen as misaligned with effective communication and design.

**Method:** The authors propose three key actions: editing the Wikipedia page on chartjunk, removing instances of chartjunk from IEEE papers, and creating a repository of visualizations with chartjunk eliminated.

**Key Contributions:**

	1. A historical critique of the term 'chartjunk' and its implications for visualization research.
	2. A practical proposal to edit existing literature to eliminate chartjunk.
	3. Creation of a repository enhancing visualization design by showcasing works that discard chartjunk.

**Result:** The authors aim to promote better practices in visualization design by encouraging researchers to reconsider how they talk about and create visualizations, fostering a more constructive dialogue in the community.

**Limitations:** 

**Conclusion:** By removing chartjunk from the research lexicon, the authors hope to inspire a more refined understanding and application of visual communication in research.

**Abstract:** In this provocation we ask the visualization research community to join us in removing chartjunk from our research lexicon. We present an etymology of chartjunk, framing its provocative origins as misaligned, and harmful, to the ways the term is currently used by visualization researchers. We call on the community to dissolve chartjunk from the ways we talk about, write about, and think about the graphical devices we design and study. As a step towards this goal we contribute a performance of maintenance through a trio of acts: editing the Wikipedia page on chartjunk, cutting out chartjunk from IEEE papers, and scanning and posting a repository of the pages with chartjunk removed to invite the community to re-imagine how we describe visualizations. This contribution blurs the boundaries between research, activism, and maintenance art, and is intended to inspire the community to join us in taking out the trash.

</details>


### [19] [Performance Gains of LLMs With Humans in a World of LLMs Versus Humans](https://arxiv.org/abs/2505.08902)

*Lucas McCullum, Pelagie Ami Agassi, Leo Anthony Celi, Daniel K. Ebner, Chrystinne Oliveira Fernandes, Rachel S. Hicklen, Mkliwa Koumbia, Lisa Soleymani Lehmann, David Restrepo*

**Main category:** cs.HC

**Keywords:** LLMs, patient safety, HCI, clinical settings, collaboration

**Relevance Score:** 9

**TL;DR:** This paper discusses the importance of defining the role of LLMs in clinical settings, emphasizing a collaborative approach between humans and LLMs rather than a competitive one.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid evolution of LLMs poses risks to patient care delivery systems, which require careful consideration and safeguards.

**Method:** This communication proposes strategies for enabling collaborative interactions between humans and LLMs to ensure patient safety in clinical environments.

**Key Contributions:**

	1. Identifies the risks of LLMs in patient care settings.
	2. Proposes a new framework for human-LLM collaboration.
	3. Calls for a paradigm shift from competition to collaboration in LLM research.

**Result:** The proposed approach aims to facilitate a symbiotic relationship between healthcare professionals and LLMs, enhancing patient care quality.

**Limitations:** The paper does not provide empirical evidence for the proposed strategies.

**Conclusion:** Future research should prioritize the effective integration of LLMs in clinical practice to maintain patient safety and care standards.

**Abstract:** Currently, a considerable research effort is devoted to comparing LLMs to a group of human experts, where the term "expert" is often ill-defined or variable, at best, in a state of constantly updating LLM releases. Without proper safeguards in place, LLMs will threaten to cause harm to the established structure of safe delivery of patient care which has been carefully developed throughout history to keep the safety of the patient at the forefront. A key driver of LLM innovation is founded on community research efforts which, if continuing to operate under "humans versus LLMs" principles, will expedite this trend. Therefore, research efforts moving forward must focus on effectively characterizing the safe use of LLMs in clinical settings that persist across the rapid development of novel LLM models. In this communication, we demonstrate that rather than comparing LLMs to humans, there is a need to develop strategies enabling efficient work of humans with LLMs in an almost symbiotic manner.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [20] [Human-AI Collaboration or Academic Misconduct? Measuring AI Use in Student Writing Through Stylometric Evidence](https://arxiv.org/abs/2505.08828)

*Eduardo Araujo Oliveira, Madhavi Mohoni, Sonsoles López-Pernas, Mohammed Saqr*

**Main category:** cs.CL

**Keywords:** authorship verification, AI collaboration, academic integrity, LLM-generated texts, educational technology

**Relevance Score:** 8

**TL;DR:** This paper explores the use of authorship verification techniques to measure AI assistance in academic writing, aimed at promoting transparency and student development.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and quantify human-AI collaboration in educational contexts without punitive measures.

**Method:** The research involved dataset selection and expansion, development of AV methods, and systematic evaluation using a new adapted Feature Vector Difference methodology across various datasets.

**Key Contributions:**

	1. Adapted AV methodology for academic writing profiles
	2. Evaluation of AI collaboration in educational settings
	3. Insights into stylometric discrepancies for educational purposes

**Result:** The AV classifier effectively identified differences between student-authored and LLM-generated texts and measured collaboration at word and sentence levels.

**Limitations:** 

**Conclusion:** The study provides a tool for educators to support academic integrity while advancing AV technology in the context of AI-assisted academic writing.

**Abstract:** As human-AI collaboration becomes increasingly prevalent in educational contexts, understanding and measuring the extent and nature of such interactions pose significant challenges. This research investigates the use of authorship verification (AV) techniques not as a punitive measure, but as a means to quantify AI assistance in academic writing, with a focus on promoting transparency, interpretability, and student development. Building on prior work, we structured our investigation into three stages: dataset selection and expansion, AV method development, and systematic evaluation. Using three datasets - including a public dataset (PAN-14) and two from University of Melbourne students from various courses - we expanded the data to include LLM-generated texts, totalling 1,889 documents and 540 authorship problems from 506 students. We developed an adapted Feature Vector Difference AV methodology to construct robust academic writing profiles for students, designed to capture meaningful, individual characteristics of their writing. The method's effectiveness was evaluated across multiple scenarios, including distinguishing between student-authored and LLM-generated texts and testing resilience against LLMs' attempts to mimic student writing styles. Results demonstrate the enhanced AV classifier's ability to identify stylometric discrepancies and measure human-AI collaboration at word and sentence levels while providing educators with a transparent tool to support academic integrity investigations. This work advances AV technology, offering actionable insights into the dynamics of academic writing in an AI-driven era.

</details>


### [21] [Clicking some of the silly options: Exploring Player Motivation in Static and Dynamic Educational Interactive Narratives](https://arxiv.org/abs/2505.08891)

*Daeun Hwang, Samuel Shields, Alex Calderwood, Shi Johnson-Bey, Michael Mateas, Noah Wardrip-Fruin, Edward F. Melcer*

**Main category:** cs.CL

**Keywords:** dynamic narrative, learner motivation, educational games, AI-driven design, interactive narrative

**Relevance Score:** 7

**TL;DR:** This paper examines the effects of dynamic versus static narratives on learner motivation in educational games, using a comparison of two versions of the game Academical.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to investigate how dynamic interactive narratives can influence learner motivation, in light of the positive effects of static narrative games established in previous research.

**Method:** A comparative study was conducted using two versions of Academical: one with a static narrative and another with a dynamic narrative where plots are sequenced based on player choices.

**Key Contributions:**

	1. Investigates the impact of dynamic narratives on learner motivation.
	2. Provides empirical evidence comparing static and dynamic game narratives.
	3. Highlights design implications for integrating AI in educational games.

**Result:** Results show that responsive content and diverse choice options enhance player engagement, emphasizing the challenges of integrating educational goals with dynamic narratives.

**Limitations:** Limited scope of research focused on a specific game type and context; future research could explore broader applications.

**Conclusion:** The findings suggest that AI-driven dynamic narratives hold significant potential for enhancing engagement in educational contexts, leading to important design implications for future development.

**Abstract:** Motivation is an important factor underlying successful learning. Previous research has demonstrated the positive effects that static interactive narrative games can have on motivation. Concurrently, advances in AI have made dynamic and adaptive approaches to interactive narrative increasingly accessible. However, limited work has explored the impact that dynamic narratives can have on learner motivation. In this paper, we compare two versions of Academical, a choice-based educational interactive narrative game about research ethics. One version employs a traditional hand-authored branching plot (i.e., static narrative) while the other dynamically sequences plots during play (i.e., dynamic narrative). Results highlight the importance of responsive content and a variety of choices for player engagement, while also illustrating the challenge of balancing pedagogical goals with the dynamic aspects of narrative. We also discuss design implications that arise from these findings. Ultimately, this work provides initial steps to illuminate the emerging potential of AI-driven dynamic narrative in educational games.

</details>


### [22] [A suite of LMs comprehend puzzle statements as well as humans](https://arxiv.org/abs/2505.08996)

*Adele E Goldberg, Supantho Rakshit, Jennifer Hu, Kyle Mahowald*

**Main category:** cs.CL

**Keywords:** Language Models, Human Comprehension, Pragmatics, Experimental Design, LLM Evaluation

**Relevance Score:** 8

**TL;DR:** This study argues that large language models (LLMs) perform better than previously assumed in understanding English statements, particularly under conditions that simulate natural reading. It critiques prior assessments of human performance and emphasizes the need for improved experimental designs in LLM evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address claims that LLMs underperform compared to humans in comprehending simple English statements and to investigate whether human performance has been overestimated.

**Method:** A preregistered study compared human responses under two conditions: one with rereading allowed and another with rereading restricted to better simulate natural comprehension.

**Key Contributions:**

	1. Demonstrates higher performance of LLMs than previously reported under restricted comprehension conditions
	2. Identifies shared challenges between human and model responses to certain types of queries
	3. Calls for improved experimental design in the evaluation of language models

**Result:** Human accuracy dropped to 73% when rereading was restricted, lower than that of LLMs Falcon-180B-Chat (76%) and GPT-4 (81%); GPT-4o reached perfect accuracy. Both humans and LLMs struggled with queries involving reciprocal actions, indicating similar pragmatic challenges.

**Limitations:** The study is based on specific conditions and types of queries, which may not generalize across all language comprehension tasks.

**Conclusion:** The findings suggest that LLMs may not be inherently weaker than humans in language comprehension and highlight a need for more rigorous experimental design in evaluating LLMs.

**Abstract:** Recent claims suggest that large language models (LMs) underperform humans in comprehending minimally complex English statements (Dentella et al., 2024). Here, we revisit those findings and argue that human performance was overestimated, while LLM abilities were underestimated. Using the same stimuli, we report a preregistered study comparing human responses in two conditions: one allowed rereading (replicating the original study), and one that restricted rereading (a more naturalistic comprehension test). Human accuracy dropped significantly when rereading was restricted (73%), falling below that of Falcon-180B-Chat (76%) and GPT-4 (81%). The newer GPT-o1 model achieves perfect accuracy. Results further show that both humans and models are disproportionately challenged by queries involving potentially reciprocal actions (e.g., kissing), suggesting shared pragmatic sensitivities rather than model-specific deficits. Additional analyses using Llama-2-70B log probabilities, a recoding of open-ended model responses, and grammaticality ratings of other sentences reveal systematic underestimation of model performance. We find that GPT-4o can align with either naive or expert grammaticality judgments, depending on prompt framing. These findings underscore the need for more careful experimental design and coding practices in LLM evaluation, and they challenge the assumption that current models are inherently weaker than humans at language comprehension.

</details>


### [23] [For GPT-4 as with Humans: Information Structure Predicts Acceptability of Long-Distance Dependencies](https://arxiv.org/abs/2505.09005)

*Nicole Cuneo, Eleanor Graves, Supantho Rakshit, Adele E. Goldberg*

**Main category:** cs.CL

**Keywords:** GPT-4, information structure, long-distance dependency, metalinguistic skill, acceptability ratings

**Relevance Score:** 8

**TL;DR:** The paper investigates whether GPT-4 can understand and generate reliable metalinguistic judgments, particularly in relation to the information structure of sentences and their acceptability ratings for long-distance dependency constructions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the capability of language models (LMs) in understanding natural language and their ability to reflect the relationship between form and function as described by linguists.

**Method:** The study probes GPT-4 using tasks that evaluate metalinguistic skills related to information structure and acceptability ratings, replicating the tasks applied to human subjects.

**Key Contributions:**

	1. Demonstrates GPT-4's ability to reflect human-like understanding of information structure in sentences.
	2. Establishes a causal link between information prominence and construction acceptability ratings.
	3. Provides empirical evidence of LMs capturing linguistic relationships previously debated in theoretical linguistics.

**Result:** Results indicate that GPT-4 exhibits metalinguistic skills, successfully capturing a relationship between information structure and acceptability ratings in long-distance dependency constructions across various base constructions.

**Limitations:** The study focuses solely on GPT-4 and may not be generalizable to other LMs; the tasks may not encompass the full complexity of human linguistic judgment.

**Conclusion:** The findings suggest a close connection between natural and GPT-4 generated English, highlighting a need for further exploration into the relationship between information structure and syntax.

**Abstract:** It remains debated how well any LM understands natural language or generates reliable metalinguistic judgments. Moreover, relatively little work has demonstrated that LMs can represent and respect subtle relationships between form and function proposed by linguists. We here focus on a particular such relationship established in recent work: English speakers' judgments about the information structure of canonical sentences predicts independently collected acceptability ratings on corresponding 'long distance dependency' [LDD] constructions, across a wide array of base constructions and multiple types of LDDs. To determine whether any LM captures this relationship, we probe GPT-4 on the same tasks used with humans and new extensions.Results reveal reliable metalinguistic skill on the information structure and acceptability tasks, replicating a striking interaction between the two, despite the zero-shot, explicit nature of the tasks, and little to no chance of contamination [Studies 1a, 1b]. Study 2 manipulates the information structure of base sentences and confirms a causal relationship: increasing the prominence of a constituent in a context sentence increases the subsequent acceptability ratings on an LDD construction. The findings suggest a tight relationship between natural and GPT-4 generated English, and between information structure and syntax, which begs for further exploration.

</details>


### [24] [Atomic Consistency Preference Optimization for Long-Form Question Answering](https://arxiv.org/abs/2505.09039)

*Jingfeng Chen, Raghuveer Thirukovalluru, Junlin Wang, Kaiwei Luo, Bhuwan Dhingra*

**Main category:** cs.CL

**Keywords:** Large Language Models, factoid hallucinations, self-supervised, factual accuracy, model alignment

**Relevance Score:** 9

**TL;DR:** The proposed Atomic Consistency Preference Optimization (ACPO) is a self-supervised method that enhances the factual accuracy of Large Language Models (LLMs) in question-answering without external supervision.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of factoid hallucinations in LLMs while avoiding dependencies on external models or knowledge bases for factual correctness assessment.

**Method:** ACPO uses atomic consistency signals to find high- and low-quality data pairs for model alignment without the need for costly external calls, making the method scalable and efficient.

**Key Contributions:**

	1. Introduction of Atomic Consistency Preference Optimization (ACPO) for self-supervised factual alignment
	2. Demonstration of ACPO's superior performance over a state-of-the-art supervised method (FactAlign)
	3. Establishment of a scalable approach to enhance LLM's factual accuracy without external dependency.

**Result:** Empirical results show that ACPO outperforms the supervised alignment baseline, FactAlign, by 1.95 points on the LongFact and BioGen datasets.

**Limitations:** The study's effectiveness may vary depending on the quality of input data used for training and model architecture.

**Conclusion:** ACPO effectively improves the factual reliability of models in a self-supervised manner, eliminating the need for external knowledge sources.

**Abstract:** Large Language Models (LLMs) frequently produce factoid hallucinations - plausible yet incorrect answers. A common mitigation strategy is model alignment, which improves factual accuracy by training on curated factual and non-factual pairs. However, this approach often relies on a stronger model (e.g., GPT-4) or an external knowledge base to assess factual correctness, which may not always be accessible. To address this, we propose Atomic Consistency Preference Optimization (ACPO), a self-supervised preference-tuning method that enhances factual accuracy without external supervision. ACPO leverages atomic consistency signals, i.e., the agreement of individual facts across multiple stochastic responses, to identify high- and low-quality data pairs for model alignment. By eliminating the need for costly GPT calls, ACPO provides a scalable and efficient approach to improving factoid question-answering. Despite being self-supervised, empirical results demonstrate that ACPO outperforms FactAlign, a strong supervised alignment baseline, by 1.95 points on the LongFact and BioGen datasets, highlighting its effectiveness in enhancing factual reliability without relying on external models or knowledge bases.

</details>


### [25] [A Comprehensive Analysis of Large Language Model Outputs: Similarity, Diversity, and Bias](https://arxiv.org/abs/2505.09056)

*Brandon Smith, Mohamed Reda Bouadjenek, Tahsin Alamgir Kheya, Phillip Dawson, Sunil Aryal*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, Ethics in AI

**Relevance Score:** 8

**TL;DR:** This paper investigates the output similarity, variability, and ethical implications of 12 large language models (LLMs), revealing significant differences in their generated texts and highlighting issues related to similarity and bias.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the similarities and ethical standards of outputs generated by different LLMs and to provide insights for future development.

**Method:** Analyzed approximately 3 million texts generated from 5,000 prompts across 12 LLMs, including proprietary and open-source models.

**Key Contributions:**

	1. Insights into output similarity and variability across different LLMs
	2. Identification of linguistic uniqueness and tone differences among LLM-generated texts
	3. Assessment of gender balance and bias in LLM outputs

**Result:** Outputs from the same LLM are more similar than to human-written texts; some models, like WizardLM-2-8x22b, show high similarity, while GPT-4 is more varied and distinctive in style.

**Limitations:** 

**Conclusion:** The study sheds light on the behavior and diversity of LLM outputs, which is essential for guiding ethical evaluations and future improvements in LLMs.

**Abstract:** Large Language Models (LLMs) represent a major step toward artificial general intelligence, significantly advancing our ability to interact with technology. While LLMs perform well on Natural Language Processing tasks -- such as translation, generation, code writing, and summarization -- questions remain about their output similarity, variability, and ethical implications. For instance, how similar are texts generated by the same model? How does this compare across different models? And which models best uphold ethical standards? To investigate, we used 5{,}000 prompts spanning diverse tasks like generation, explanation, and rewriting. This resulted in approximately 3 million texts from 12 LLMs, including proprietary and open-source systems from OpenAI, Google, Microsoft, Meta, and Mistral. Key findings include: (1) outputs from the same LLM are more similar to each other than to human-written texts; (2) models like WizardLM-2-8x22b generate highly similar outputs, while GPT-4 produces more varied responses; (3) LLM writing styles differ significantly, with Llama 3 and Mistral showing higher similarity, and GPT-4 standing out for distinctiveness; (4) differences in vocabulary and tone underscore the linguistic uniqueness of LLM-generated content; (5) some LLMs demonstrate greater gender balance and reduced bias. These results offer new insights into the behavior and diversity of LLM outputs, helping guide future development and ethical evaluation.

</details>


### [26] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)

*Jennifer Haase, Paul H. P. Hanel, Sebastian Pokutta*

**Main category:** cs.CL

**Keywords:** divergent thinking, multilingual framework, creativity assessment, large language models, automated evaluation

**Relevance Score:** 6

**TL;DR:** S-DAT is a novel multilingual framework for automated assessment of divergent thinking using large language models to compute semantic distance as a language-agnostic measure.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional assessments of divergent thinking are labor-intensive, language-specific, and subjective, making them less scalable and applicable across cultures. S-DAT aims to address these limitations.

**Method:** The framework utilizes large language models and advanced multilingual embeddings to assess divergent thinking by computing semantic distances in a language-agnostic manner across various languages.

**Key Contributions:**

	1. Introduction of S-DAT for multilingual divergent thinking assessment
	2. Cross-linguistic evaluation across eleven languages
	3. Establishment of convergent and discriminant validity with other thinking measures.

**Result:** S-DAT was evaluated in eleven languages, demonstrating consistent and robust scoring for divergent thinking and showing convergent and discriminant validity compared to other measures of thinking.

**Limitations:** 

**Conclusion:** S-DAT provides an innovative and inclusive approach to assessing cognitive flexibility in diverse populations, enabling global-scale creativity research and fair evaluations.

**Abstract:** This paper introduces S-DAT (Synthetic-Divergent Association Task), a scalable, multilingual framework for automated assessment of divergent thinking (DT) -a core component of human creativity. Traditional creativity assessments are often labor-intensive, language-specific, and reliant on subjective human ratings, limiting their scalability and cross-cultural applicability. In contrast, S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance -- a language-agnostic proxy for DT. We evaluate S-DAT across eleven diverse languages, including English, Spanish, German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating robust and consistent scoring across linguistic contexts. Unlike prior DAT approaches, the S-DAT shows convergent validity with other DT measures and correct discriminant validity with convergent thinking. This cross-linguistic flexibility allows for more inclusive, global-scale creativity research, addressing key limitations of earlier approaches. S-DAT provides a powerful tool for fairer, more comprehensive evaluation of cognitive flexibility in diverse populations and can be freely assessed online: https://sdat.iol.zib.de/.

</details>


### [27] [CEC-Zero: Chinese Error Correction Solution Based on LLM](https://arxiv.org/abs/2505.09082)

*Sophie Zhang, Zhiming Lin*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, Chinese spelling correction

**Relevance Score:** 6

**TL;DR:** This paper introduces CEC-Zero, a reinforcement learning framework that allows large language models to self-correct in Chinese text processing without needing external supervision, improving reliability and generalization in applications such as Chinese spelling correction.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the reliability and generalization challenges faced by large language models in Chinese NLP, particularly in the context of Chinese Spelling Correction (CSC).

**Method:** The paper proposes CEC-Zero, a novel reinforcement learning framework that enables large language models to autonomously learn error correction strategies, eliminating the need for annotated data or auxiliary models.

**Key Contributions:**

	1. Introduction of CEC-Zero, a novel RL framework for LLMs.
	2. The elimination of dependency on annotated data for error correction.
	3. Demonstrated improvements in cross-domain generalization for Chinese NLP applications.

**Result:** Experiments demonstrate that RL-enhanced large language models achieve industry-viable accuracy and better cross-domain generalization for Chinese text correction tasks.

**Limitations:** 

**Conclusion:** The proposed framework not only enhances the reliability of LLMs in Chinese NLP applications but also establishes a new paradigm for self-improvement in language models, making them more practical for real-world scenarios.

**Abstract:** Recent advancements in large language models (LLMs) demonstrate exceptional Chinese text processing capabilities, particularly in Chinese Spelling Correction (CSC). While LLMs outperform traditional BERT-based models in accuracy and robustness, challenges persist in reliability and generalization. This paper proposes CEC-Zero, a novel reinforcement learning (RL) framework enabling LLMs to self-correct through autonomous error strategy learning without external supervision. By integrating RL with LLMs' generative power, the method eliminates dependency on annotated data or auxiliary models. Experiments reveal RL-enhanced LLMs achieve industry-viable accuracy and superior cross-domain generalization, offering a scalable solution for reliability optimization in Chinese NLP applications. This breakthrough facilitates LLM deployment in practical Chinese text correction scenarios while establishing a new paradigm for self-improving language models.

</details>


### [28] [How an unintended Side Effect of a Research Project led to Boosting the Power of UML](https://arxiv.org/abs/2505.09269)

*Ulrich Frank, Pierre Maier*

**Main category:** cs.CL

**Keywords:** UML modeling, software architecture, educational tools

**Relevance Score:** 4

**TL;DR:** The paper presents a novel UML modeling tool that integrates class and object diagrams while allowing object execution, enhancing both software architecture and educational experiences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an advanced UML modeling tool that integrates various aspects of software modeling, improving both software architecture and educational tools for teaching.

**Method:** The design and implementation of a new UML modeling tool that integrates class diagrams with object diagrams and supports object execution.

**Key Contributions:**

	1. Integration of class and object diagrams
	2. Execution of objects within the modeling tool
	3. Enhancement of educational practices in software modeling

**Result:** The new tool offers a stimulating learning experience for students and demonstrates the potential of research to yield practical outcomes from an international collaborative project.

**Limitations:** 

**Conclusion:** The integration of software with object models through this UML tool signifies a progression in the capability of modeling tools in the software development field.

**Abstract:** This paper describes the design, implementation and use of a new UML modeling tool that represents a significant advance over conventional tools. Among other things, it allows the integration of class diagrams and object diagrams as well as the execution of objects. This not only enables new software architectures characterized by the integration of software with corresponding object models, but is also ideal for use in teaching, as it provides students with a particularly stimulating learning experience. A special feature of the project is that it has emerged from a long-standing international research project, which is aimed at a comprehensive multi-level architecture. The project is therefore an example of how research can lead to valuable results that arise as a side effect of other work.

</details>


### [29] [A Scalable Unsupervised Framework for multi-aspect labeling of Multilingual and Multi-Domain Review Data](https://arxiv.org/abs/2505.09286)

*Jiin Park, Misuk Kim*

**Main category:** cs.CL

**Keywords:** aspect detection, multilingual, unsupervised learning, review analysis, language models

**Relevance Score:** 7

**TL;DR:** This paper presents an unsupervised, multilingual framework for cross-domain aspect detection in online reviews, leveraging negative sampling and pretrained language models for high-performance automatic labeling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in existing studies that focus on specific domains, languages, and supervised learning approaches requiring large labeled datasets.

**Method:** The proposed framework extracts aspect category candidates through clustering and represents each review as an aspect-aware embedding vector using negative sampling, followed by multi-aspect labeling and fine-tuning of pretrained language models.

**Key Contributions:**

	1. Multilingual and unsupervised framework for aspect detection
	2. High-performance automatic labeling with pretrained models
	3. Validation of automatic labels through human evaluation

**Result:** The framework achieves high performance in generating suitable labels for training and demonstrates superior consistency and scalability compared to other large language models. Human evaluations indicate that automatic labels are comparable to manual ones.

**Limitations:** 

**Conclusion:** This study showcases a robust multi-aspect labeling approach applicable to multilingual and multi-domain review data, with future research directions including automatic review summarization.

**Abstract:** Effectively analyzing online review data is essential across industries. However, many existing studies are limited to specific domains and languages or depend on supervised learning approaches that require large-scale labeled datasets. To address these limitations, we propose a multilingual, scalable, and unsupervised framework for cross-domain aspect detection. This framework is designed for multi-aspect labeling of multilingual and multi-domain review data. In this study, we apply automatic labeling to Korean and English review datasets spanning various domains and assess the quality of the generated labels through extensive experiments. Aspect category candidates are first extracted through clustering, and each review is then represented as an aspect-aware embedding vector using negative sampling. To evaluate the framework, we conduct multi-aspect labeling and fine-tune several pretrained language models to measure the effectiveness of the automatically generated labels. Results show that these models achieve high performance, demonstrating that the labels are suitable for training. Furthermore, comparisons with publicly available large language models highlight the framework's superior consistency and scalability when processing large-scale data. A human evaluation also confirms that the quality of the automatic labels is comparable to those created manually. This study demonstrates the potential of a robust multi-aspect labeling approach that overcomes limitations of supervised methods and is adaptable to multilingual, multi-domain environments. Future research will explore automatic review summarization and the integration of artificial intelligence agents to further improve the efficiency and depth of review analysis.

</details>


### [30] [Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging](https://arxiv.org/abs/2505.09316)

*Hongjin Qian, Zheng Liu*

**Main category:** cs.CL

**Keywords:** large language models, adaptive retrieval, reinforcement learning, information foraging theory, dynamic reasoning

**Relevance Score:** 9

**TL;DR:** InForage is a reinforcement learning framework that enhances LLMs by enabling dynamic, adaptive retrieval during inference, outperforming traditional static methods in complex tasks.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Existing retrieval-augmented generation methods are static and inadequate for complex, evolving information needs. The need for adaptive inference-time retrieval is motivating this research.

**Method:** InForage formalizes retrieval-augmented reasoning as a dynamic information-seeking process using reinforcement learning, rewarding intermediate retrieval quality.

**Key Contributions:**

	1. Proposes a novel reinforcement learning framework (InForage) for adaptive retrieval in LLMs.
	2. Introduces a human-guided dataset for training iterative search and reasoning.
	3. Demonstrates significant performance improvements in reasoning tasks over traditional methods.

**Result:** InForage shows superior performance in general question answering and multi-hop reasoning tasks compared to baseline methods, demonstrating robust and efficient reasoning capabilities.

**Limitations:** 

**Conclusion:** The evaluations underscore InForage's effectiveness in developing LLMs that can adaptively gather and integrate information for complex tasks.

**Abstract:** Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents.

</details>


### [31] [Llama See, Llama Do: A Mechanistic Perspective on Contextual Entrainment and Distraction in LLMs](https://arxiv.org/abs/2505.09338)

*Jingcheng Niu, Xingdi Yuan, Tong Wang, Hamidreza Saghir, Amir H. Abdi*

**Main category:** cs.CL

**Keywords:** contextual entrainment, language models, distraction problem

**Relevance Score:** 8

**TL;DR:** This paper introduces the phenomenon of contextual entrainment in language models, where irrelevant contextual information influences model outputs. It presents a method to identify attention heads responsible for this effect and provides insights for mitigating distractions in language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore how language models are affected by irrelevant contextual information in prompts, termed as contextual entrainment, and its implications for model performance.

**Method:** The authors propose a novel method for discovering 'entrainment heads' in attention mechanisms through differentiable masking, examining their impact on model outputs under different prompt conditions.

**Key Contributions:**

	1. Introduction of the contextual entrainment phenomenon
	2. Identification of attention heads related to contextual distraction
	3. Method to reduce the effect of contextually irrelevant information on model outputs

**Result:** The findings indicate that language models assign higher probabilities to tokens previously encountered in the input context, and that the degree of this effect is influenced by the semantic relevance of the prompt's content. Turning off the identified entrainment heads reduces this distraction effect.

**Limitations:** 

**Conclusion:** The research contributes to understanding the mechanistic factors behind distractions in language models and lays the groundwork for developing strategies to mitigate these issues in practical applications.

**Abstract:** We observe a novel phenomenon, contextual entrainment, across a wide range of language models (LMs) and prompt settings, providing a new mechanistic perspective on how LMs become distracted by ``irrelevant'' contextual information in the input prompt. Specifically, LMs assign significantly higher logits (or probabilities) to any tokens that have previously appeared in the context prompt, even for random tokens. This suggests that contextual entrainment is a mechanistic phenomenon, occurring independently of the relevance or semantic relation of the tokens to the question or the rest of the sentence. We find statistically significant evidence that the magnitude of contextual entrainment is influenced by semantic factors. Counterfactual prompts have a greater effect compared to factual ones, suggesting that while contextual entrainment is a mechanistic phenomenon, it is modulated by semantic factors.   We hypothesise that there is a circuit of attention heads -- the entrainment heads -- that corresponds to the contextual entrainment phenomenon. Using a novel entrainment head discovery method based on differentiable masking, we identify these heads across various settings. When we ``turn off'' these heads, i.e., set their outputs to zero, the effect of contextual entrainment is significantly attenuated, causing the model to generate output that capitulates to what it would produce if no distracting context were provided. Our discovery of contextual entrainment, along with our investigation into LM distraction via the entrainment heads, marks a key step towards the mechanistic analysis and mitigation of the distraction problem.

</details>


### [32] [Qwen3 Technical Report](https://arxiv.org/abs/2505.09388)

*An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu*

**Main category:** cs.CL

**Keywords:** large language models, multilingual support, adaptive resource allocation

**Relevance Score:** 9

**TL;DR:** Qwen3 introduces a series of advanced large language models with enhanced performance, efficiency, and multilingual capabilities, featuring innovative modes of operation and dynamic resource allocation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a versatile series of large language models that improve upon existing LLMs in terms of performance, efficiency, and multilingual capabilities while facilitating user adaptability.

**Method:** The Qwen3 model series implements both dense and Mixture-of-Expert architectures with innovative features such as integrated thinking modes and a thinking budget mechanism for adaptive computational resource allocation.

**Key Contributions:**

	1. Integration of thinking and non-thinking modes for flexible model use
	2. Introduction of a thinking budget for adaptive resource allocation
	3. Expansion of multilingual capabilities from 29 to 119 languages

**Result:** Qwen3 achieves state-of-the-art performance in various benchmarks, including code generation and mathematical reasoning, while expanding multilingual support to 119 languages and dialects.

**Limitations:** 

**Conclusion:** Qwen3 represents a significant advancement in LLM technology, enabling improved accessibility and competitive performance even at smaller scales; all models are publicly available to promote community research.

**Abstract:** In this work, we present Qwen3, the latest version of the Qwen model family. Qwen3 comprises a series of large language models (LLMs) designed to advance performance, efficiency, and multilingual capabilities. The Qwen3 series includes models of both dense and Mixture-of-Expert (MoE) architectures, with parameter scales ranging from 0.6 to 235 billion. A key innovation in Qwen3 is the integration of thinking mode (for complex, multi-step reasoning) and non-thinking mode (for rapid, context-driven responses) into a unified framework. This eliminates the need to switch between different models--such as chat-optimized models (e.g., GPT-4o) and dedicated reasoning models (e.g., QwQ-32B)--and enables dynamic mode switching based on user queries or chat templates. Meanwhile, Qwen3 introduces a thinking budget mechanism, allowing users to allocate computational resources adaptively during inference, thereby balancing latency and performance based on task complexity. Moreover, by leveraging the knowledge from the flagship models, we significantly reduce the computational resources required to build smaller-scale models, while ensuring their highly competitive performance. Empirical evaluations demonstrate that Qwen3 achieves state-of-the-art results across diverse benchmarks, including tasks in code generation, mathematical reasoning, agent tasks, etc., competitive against larger MoE models and proprietary models. Compared to its predecessor Qwen2.5, Qwen3 expands multilingual support from 29 to 119 languages and dialects, enhancing global accessibility through improved cross-lingual understanding and generation capabilities. To facilitate reproducibility and community-driven research and development, all Qwen3 models are publicly accessible under Apache 2.0.

</details>


### [33] [Multilingual Machine Translation with Quantum Encoder Decoder Attention-based Convolutional Variational Circuits](https://arxiv.org/abs/2505.09407)

*Subrit Dikshit, Ritu Tiwari, Priyank Jain*

**Main category:** cs.CL

**Keywords:** multilingual translation, quantum computing, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper presents QEDACVC, a quantum computing-based multilingual translation model that outperforms traditional methods by utilizing quantum architectures.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore multilingual machine translation using quantum computing as an alternative to classical models, which include state-of-the-art techniques like GPT and BERT.

**Method:** QEDACVC uses a quantum encoder-decoder architecture that incorporates quantum convolution, pooling, variational circuits, and attention mechanisms, implemented on quantum hardware.

**Key Contributions:**

	1. Introduction of a quantum encoder-decoder architecture for multilingual translation.
	2. Demonstration of quantum convolution and attention mechanisms in NLP tasks.
	3. Achieved 82% accuracy on multilingual datasets, showcasing the model's effectiveness.

**Result:** The model achieved an accuracy of 82% on the OPUS dataset across English, French, German, and Hindi corpora.

**Limitations:** The implementation requires quantum computing hardware, which may not be widely accessible.

**Conclusion:** QEDACVC demonstrates the feasibility of using quantum computing in multilingual translation and has the potential to enhance performance over classical methods.

**Abstract:** Cloud-based multilingual translation services like Google Translate and Microsoft Translator achieve state-of-the-art translation capabilities. These services inherently use large multilingual language models such as GRU, LSTM, BERT, GPT, T5, or similar encoder-decoder architectures with attention mechanisms as the backbone. Also, new age natural language systems, for instance ChatGPT and DeepSeek, have established huge potential in multiple tasks in natural language processing. At the same time, they also possess outstanding multilingual translation capabilities. However, these models use the classical computing realm as a backend. QEDACVC (Quantum Encoder Decoder Attention-based Convolutional Variational Circuits) is an alternate solution that explores the quantum computing realm instead of the classical computing realm to study and demonstrate multilingual machine translation. QEDACVC introduces the quantum encoder-decoder architecture that simulates and runs on quantum computing hardware via quantum convolution, quantum pooling, quantum variational circuit, and quantum attention as software alterations. QEDACVC achieves an Accuracy of 82% when trained on the OPUS dataset for English, French, German, and Hindi corpora for multilingual translations.

</details>


### [34] [PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning](https://arxiv.org/abs/2505.09519)

*Zongqian Li, Yixuan Su, Nigel Collier*

**Main category:** cs.CL

**Keywords:** Parameter-efficient fine-tuning, Prompt tuning, Mixture of experts

**Relevance Score:** 7

**TL;DR:** PT-MoE integrates matrix decomposition and MoE routing for efficient prompt tuning, achieving state-of-the-art performance on QA and math problem solving tasks with fewer parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient mechanisms to adapt large language models while addressing counter-intuitive performance dynamics observed in existing PEFT methods.

**Method:** PT-MoE framework combines matrix decomposition with mixture-of-experts routing to enhance training efficiency and performance in prompt tuning.

**Key Contributions:**

	1. Introduction of the PT-MoE framework for prompt tuning.
	2. Demonstrated superior performance on 17 datasets compared to existing approaches.
	3. Insights into the interplay between matrix decomposition and MoE for efficient learning.

**Result:** PT-MoE outperforms traditional PT and LoRA methods, improving F1 scores in QA tasks and mathematical accuracy while using 25% fewer parameters than LoRA.

**Limitations:** 

**Conclusion:** The integration of matrix decomposition and MoE routing in PT-MoE yields improved performance and generalization across tasks, indicating potential for future PEFT methods.

**Abstract:** Parameter-efficient fine-tuning (PEFT) methods have shown promise in adapting large language models, yet existing approaches exhibit counter-intuitive phenomena: integrating router into prompt tuning (PT) increases training efficiency yet does not improve performance universally; parameter reduction through matrix decomposition can improve performance in specific domains. Motivated by these observations and the modular nature of PT, we propose PT-MoE, a novel framework that integrates matrix decomposition with mixture-of-experts (MoE) routing for efficient PT. Results across 17 datasets demonstrate that PT-MoE achieves state-of-the-art performance in both question answering (QA) and mathematical problem solving tasks, improving F1 score by 1.49 points over PT and 2.13 points over LoRA in QA tasks, while enhancing mathematical accuracy by 10.75 points over PT and 0.44 points over LoRA, all while using 25% fewer parameters than LoRA. Our analysis reveals that while PT methods generally excel in QA tasks and LoRA-based methods in math datasets, the integration of matrix decomposition and MoE in PT-MoE yields complementary benefits: decomposition enables efficient parameter sharing across experts while MoE provides dynamic adaptation, collectively enabling PT-MoE to demonstrate cross-task consistency and generalization abilities. These findings, along with ablation studies on routing mechanisms and architectural components, provide insights for future PEFT methods.

</details>


### [35] [S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment](https://arxiv.org/abs/2505.09068)

*Jennifer Haase, Paul H. P. Hanel, Sebastian Pokutta*

**Main category:** cs.CL

**Keywords:** divergent thinking, creativity assessment, multilingual, large language models, cognitive flexibility

**Relevance Score:** 6

**TL;DR:** The paper presents S-DAT, a multilingual framework for automated assessment of divergent thinking using language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of traditional creativity assessments which are labor-intensive, language-specific, and subjective.

**Method:** S-DAT leverages large language models and multilingual embeddings to compute a language-agnostic proxy for divergent thinking.

**Key Contributions:**

	1. Multilingual framework for divergent thinking assessment
	2. Scalable and automated evaluation using language models
	3. Cross-linguistic validity and flexibility in creativity research

**Result:** S-DAT was tested across eleven languages and showed robust, consistent scoring and convergent validation with other divergent thinking measures.

**Limitations:** 

**Conclusion:** S-DAT enhances the scalability and inclusivity of creativity research across diverse linguistic contexts.

**Abstract:** This paper introduces S-DAT (Synthetic-Divergent Association Task), a scalable, multilingual framework for automated assessment of divergent thinking (DT) -a core component of human creativity. Traditional creativity assessments are often labor-intensive, language-specific, and reliant on subjective human ratings, limiting their scalability and cross-cultural applicability. In contrast, S-DAT leverages large language models and advanced multilingual embeddings to compute semantic distance -- a language-agnostic proxy for DT. We evaluate S-DAT across eleven diverse languages, including English, Spanish, German, Russian, Hindi, and Japanese (Kanji, Hiragana, Katakana), demonstrating robust and consistent scoring across linguistic contexts. Unlike prior DAT approaches, the S-DAT shows convergent validity with other DT measures and correct discriminant validity with convergent thinking. This cross-linguistic flexibility allows for more inclusive, global-scale creativity research, addressing key limitations of earlier approaches. S-DAT provides a powerful tool for fairer, more comprehensive evaluation of cognitive flexibility in diverse populations and can be freely assessed online: https://sdat.iol.zib.de/.

</details>


### [36] [WorldView-Bench: A Benchmark for Evaluating Global Cultural Perspectives in Large Language Models](https://arxiv.org/abs/2505.09595)

*Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim Ghaznavi, Junaid Qadir*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Inclusivity, Bias Mitigation, Multiplex Worldview, AI Evaluation

**Relevance Score:** 9

**TL;DR:** Introducing WorldView-Bench, a benchmark for assessing Global Cultural Inclusivity in LLMs, focusing on diverse worldviews to mitigate cultural bias.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cultural bias in LLMs resulting from Western-centric training methods that standardize epistemologies, leading to cultural homogenization.

**Method:** Development of WorldView-Bench to evaluate LLMs on their ability to reflect global cultural diversity, using free-form generative evaluations and multiplexity principles.

**Key Contributions:**

	1. Introduction of WorldView-Bench for assessing Global Cultural Inclusivity in LLMs.
	2. Implementation of contextually-implemented and multi-agent system strategies for LLMs.
	3. Demonstrated effectiveness of multiplex-aware approaches in improving cultural diversity metrics in LLMs.

**Result:** WorldView-Bench showed a significant increase in Perspectives Distribution Score entropy from 13% to 94% with the implementation of Multiplex LLMs, along with improved sentiment and cultural balance.

**Limitations:** 

**Conclusion:** Multiplex-aware AI evaluations can effectively reduce cultural bias in LLMs and promote a more inclusive approach to AI system development.

**Abstract:** Large Language Models (LLMs) are predominantly trained and aligned in ways that reinforce Western-centric epistemologies and socio-cultural norms, leading to cultural homogenization and limiting their ability to reflect global civilizational plurality. Existing benchmarking frameworks fail to adequately capture this bias, as they rely on rigid, closed-form assessments that overlook the complexity of cultural inclusivity. To address this, we introduce WorldView-Bench, a benchmark designed to evaluate Global Cultural Inclusivity (GCI) in LLMs by analyzing their ability to accommodate diverse worldviews. Our approach is grounded in the Multiplex Worldview proposed by Senturk et al., which distinguishes between Uniplex models, reinforcing cultural homogenization, and Multiplex models, which integrate diverse perspectives. WorldView-Bench measures Cultural Polarization, the exclusion of alternative perspectives, through free-form generative evaluation rather than conventional categorical benchmarks. We implement applied multiplexity through two intervention strategies: (1) Contextually-Implemented Multiplex LLMs, where system prompts embed multiplexity principles, and (2) Multi-Agent System (MAS)-Implemented Multiplex LLMs, where multiple LLM agents representing distinct cultural perspectives collaboratively generate responses. Our results demonstrate a significant increase in Perspectives Distribution Score (PDS) entropy from 13% at baseline to 94% with MAS-Implemented Multiplex LLMs, alongside a shift toward positive sentiment (67.7%) and enhanced cultural balance. These findings highlight the potential of multiplex-aware AI evaluation in mitigating cultural bias in LLMs, paving the way for more inclusive and ethically aligned AI systems.

</details>


### [37] [LLM-based NLG Evaluation: Current Status and Challenges](https://arxiv.org/abs/2402.01383)

*Mingqi Gao, Xinyu Hu, Jie Ruan, Xiao Pu, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** natural language generation, evaluation metrics, large language models, NLG, machine learning

**Relevance Score:** 9

**TL;DR:** This paper surveys various LLM-based methods for evaluating natural language generation, discussing their advantages and disadvantages, and suggesting future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this survey is to address the limitations of traditional evaluation metrics in natural language generation, especially in light of advancements made by large language models.

**Method:** The paper presents a taxonomy of evaluation methods that utilize LLMs, including metrics derived from LLMs, prompting, fine-tuning, and collaborative evaluation with humans.

**Key Contributions:**

	1. Taxonomy of LLM-based evaluation methods
	2. Evaluation of the advantages and challenges of these methods
	3. Identification of future research directions in NLG evaluation using LLMs.

**Result:** The survey categorizes different LLM-based evaluation methods and provides a discussion on their effectiveness, identifying both strengths and weaknesses in each approach.

**Limitations:** 

**Conclusion:** The authors conclude by highlighting open research questions and proposing future directions for LLM-based natural language generation evaluation.

**Abstract:** Evaluating natural language generation (NLG) is a vital but challenging problem in natural language processing. Traditional evaluation metrics mainly capturing content (e.g. n-gram) overlap between system outputs and references are far from satisfactory, and large language models (LLMs) such as ChatGPT have demonstrated great potential in NLG evaluation in recent years. Various automatic evaluation methods based on LLMs have been proposed, including metrics derived from LLMs, prompting LLMs, fine-tuning LLMs, and human-LLM collaborative evaluation. In this survey, we first give a taxonomy of LLM-based NLG evaluation methods, and discuss their pros and cons, respectively. Lastly, we discuss several open problems in this area and point out future research directions.

</details>


### [38] [Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model](https://arxiv.org/abs/2404.03080)

*Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Bram Hoex, Haofen Wang, Tong Xie, Wenjie Zhang*

**Main category:** cs.CL

**Keywords:** Materials Science, Knowledge Graph, Natural Language Processing, Large Language Models, Data Integration

**Relevance Score:** 7

**TL;DR:** The paper introduces the Materials Knowledge Graph (MKG), which utilizes advanced NLP and LLM techniques to organize extensive materials science research into structured data, enhancing discovery and integration processes.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in discovering and integrating materials science knowledge due to its dispersion across literature and reliance on traditional experimental methods.

**Method:** The MKG employs natural language processing and large language models to extract and organize a decade of research into structured triples, forming a graph with 162,605 nodes and 731,772 edges.

**Key Contributions:**

	1. Development of the Materials Knowledge Graph (MKG)
	2. Integration of NLP and LLM for data extraction and organization
	3. Reduction of reliance on traditional methods for materials discovery

**Result:** MKG enhances data usability by categorizing information into structured labels and facilitates efficient link prediction through network-based algorithms while reducing reliance on experimental methods.

**Limitations:** 

**Conclusion:** The Materials Knowledge Graph streamlines materials research and sets the stage for more advanced science knowledge graphs.

**Abstract:** Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges to the efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated science knowledge graphs.

</details>


### [39] [FAMMA: A Benchmark for Financial Domain Multilingual Multimodal Question Answering](https://arxiv.org/abs/2410.04526)

*Siqiao Xue, Xiaojing Li, Fan Zhou, Qingyang Dai, Zhixuan Chu, Hongyuan Mei*

**Main category:** cs.CL

**Keywords:** financial QA, benchmark, multilingual, multimodal, LLMs

**Relevance Score:** 4

**TL;DR:** Introduction of FAMMA, a benchmark for evaluating LLMs in financial multilingual multimodal question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Developing a benchmark to assess LLMs' capabilities in complex financial reasoning.

**Method:** Two versions of the benchmark: FAMMA-Basic with textbook derived questions and FAMMA-LivePro with expert-created novel questions, both incorporating multimodal data.

**Key Contributions:**

	1. Introduction of a novel benchmark for financial QA
	2. Incorporation of multimodal data in questions
	3. Demonstrated performance improvement through reasoning data fine-tuning

**Result:** FAMMA is challenging for LLMs such as GPT-o1 and DeepSeek-R1, indicating room for improvement in financial QA performance.

**Limitations:** 

**Conclusion:** Fine-tuning models on curated reasoning data enhances performance on the FAMMA-LivePro dataset; resources like leaderboard and data are publicly available.

**Abstract:** In this paper, we introduce FAMMA, an open-source benchmark for \underline{f}in\underline{a}ncial \underline{m}ultilingual \underline{m}ultimodal question \underline{a}nswering (QA). Our benchmark aims to evaluate the abilities of large language models (LLMs) in answering complex reasoning questions that require advanced financial knowledge. The benchmark has two versions: FAMMA-Basic consists of 1,945 questions extracted from university textbooks and exams, along with human-annotated answers and rationales; FAMMA-LivePro consists of 103 novel questions created by human domain experts, with answers and rationales held out from the public for a contamination-free evaluation. These questions cover advanced knowledge of 8 major subfields in finance (e.g., corporate finance, derivatives, and portfolio management). Some are in Chinese or French, while a majority of them are in English. Each question has some non-text data such as charts, diagrams, or tables. Our experiments reveal that FAMMA poses a significant challenge on LLMs, including reasoning models such as GPT-o1 and DeepSeek-R1. Additionally, we curated 1,270 reasoning trajectories of DeepSeek-R1 on the FAMMA-Basic data, and fine-tuned a series of open-source Qwen models using this reasoning data. We found that training a model on these reasoning trajectories can significantly improve its performance on FAMMA-LivePro. We released our leaderboard, data, code, and trained models at https://famma-bench.github.io/famma/.

</details>


### [40] [P-MMEval: A Parallel Multilingual Multitask Benchmark for Consistent Evaluation of LLMs](https://arxiv.org/abs/2411.09116)

*Yidan Zhang, Yu Wan, Boyi Deng, Baosong Yang, Haoran Wei, Fei Huang, Bowen Yu, Junyang Lin, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** Multilingual Models, Benchmark, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper presents P-MMEval, a comprehensive benchmark for evaluating multilingual capabilities across various tasks in large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limits of previous assessments that focused on isolated NLP tasks, providing a broad evaluation of multilingual capabilities in LLMs.

**Method:** Introduction of P-MMEval, a large-scale benchmark covering diverse datasets with consistent language coverage and parallel samples, followed by extensive experiments comparing multilingual models.

**Key Contributions:**

	1. Introduction of the P-MMEval benchmark
	2. Consistent language coverage across datasets
	3. Insights into performance factors for multilingual models

**Result:** The experiments reveal performance comparisons across models and tasks, and insights on the relationship between performance, task types, model sizes, languages, and prompts.

**Limitations:** 

**Conclusion:** The findings provide guidance for future research and the dataset aims to improve the evaluation of multilingual capabilities in LLMs.

**Abstract:** Recent advancements in large language models (LLMs) showcase varied multilingual capabilities across tasks like translation, code generation, and reasoning. Previous assessments often limited their scope to fundamental natural language processing (NLP) or isolated capability-specific tasks. To alleviate this drawback, we aim to present a comprehensive multilingual multitask benchmark. First, we introduce P-MMEval, a large-scale benchmark covering effective fundamental and capability-specialized datasets. Furthermore, P-MMEval delivers consistent language coverage across various datasets and provides parallel samples. Finally, we conduct extensive experiments on representative multilingual model series to compare performances across models and tasks, explore the relationship between multilingual performances and factors such as tasks, model sizes, languages, and prompts, and examine the effectiveness of knowledge transfer from English to other languages. The resulting insights are intended to offer valuable guidance for future research. The dataset is available at https://huggingface.co/datasets/Qwen/P-MMEval.

</details>


### [41] [Principled Data Selection for Alignment: The Hidden Risks of Difficult Examples](https://arxiv.org/abs/2502.09650)

*Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, Zhiqiang Xu*

**Main category:** cs.CL

**Keywords:** large language models, model alignment, data difficulty, Selective DPO, machine learning

**Relevance Score:** 9

**TL;DR:** This paper proposes a new principle regarding the alignment of large language models (LLMs), focusing on the impact of data difficulty on model performance, and introduces Selective DPO to enhance alignment by filtering challenging examples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the assumption that more clean data leads to better alignment in LLMs, focusing on the match between model capacity and example difficulty.

**Method:** Systematic experimentation was conducted to analyze the relationship between data difficulty and model capacity, leading to the development of Selective DPO which filters out overly difficult examples.

**Key Contributions:**

	1. Introduction of Selective DPO to filter overly difficult examples
	2. Demonstrated that challenging examples hinder alignment across multiple LLMs
	3. Illustrated a critical relationship between model capacity and data difficulty

**Result:** The findings indicate that preference examples vary in difficulty, and the performance of models significantly degrades with overly difficult examples. Selective DPO improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark over traditional DPO.

**Limitations:** The study primarily addresses performance in specific datasets and LLMs, leaving other potential impacts unexplored.

**Conclusion:** Aligning data difficulty with model capacity is essential for improving the alignment strategies in large language models.

**Abstract:** The alignment of large language models (LLMs) often assumes that using more clean data yields better outcomes, overlooking the match between model capacity and example difficulty. Challenging this, we propose a new principle: Preference data vary in difficulty, and overly difficult examples hinder alignment, by exceeding the model's capacity. Through systematic experimentation, we validate this principle with three key findings: (1) preference examples vary in difficulty, as evidenced by consistent learning orders across alignment runs; (2) overly difficult examples significantly degrade performance across four LLMs and two datasets; and (3) the capacity of a model dictates its threshold for handling difficult examples, underscoring a critical relationship between data selection and model capacity. Building on this principle, we introduce Selective DPO, which filters out overly difficult examples. This simple adjustment improves alignment performance by 9-16% in win rates on the AlpacaEval 2 benchmark compared to the DPO baseline, suppressing a series of DPO variants with different algorithmic adjustments. Together, these results illuminate the importance of aligning data difficulty with model capacity, offering a transformative perspective for improving alignment strategies in LLMs. Code is available at https://github.com/glorgao/SelectiveDPO.

</details>


### [42] [PropNet: a White-Box and Human-Like Network for Sentence Representation](https://arxiv.org/abs/2502.10725)

*Fei Yang*

**Main category:** cs.CL

**Keywords:** sentence representation, interpretability, cognitive science, natural language processing, transformer models

**Relevance Score:** 7

**TL;DR:** This paper introduces PropNet, a white-box hierarchical network for sentence representation that enhances interpretability compared to traditional black-box transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses issues of bias, trust, and safety in transformer-based embedding methods due to their black-box nature.

**Method:** PropNet is proposed as a purely white-box model that constructs a hierarchical network based on propositions in sentences, inspired by cognitive science.

**Key Contributions:**

	1. Introduction of PropNet for interpretable sentence representation
	2. Basis on cognitive science findings for hierarchical network design
	3. Insights into human cognitive processes for sentence understanding

**Result:** Experiments show that while PropNet lags behind state-of-the-art models in STS tasks, it offers significant interpretability and insights into human cognitive processes.

**Limitations:** PropNet shows a significant performance gap compared to state-of-the-art models in STS tasks.

**Conclusion:** PropNet represents a step towards inherent interpretability in sentence representation models, though further improvements are necessary to enhance its performance in STS tasks.

**Abstract:** Transformer-based embedding methods have dominated the field of sentence representation in recent years. Although they have achieved remarkable performance on NLP missions, such as semantic textual similarity (STS) tasks, their black-box nature and large-data-driven training style have raised concerns, including issues related to bias, trust, and safety. Many efforts have been made to improve the interpretability of embedding models, but these problems have not been fundamentally resolved. To achieve inherent interpretability, we propose a purely white-box and human-like sentence representation network, PropNet. Inspired by findings from cognitive science, PropNet constructs a hierarchical network based on the propositions contained in a sentence. While experiments indicate that PropNet has a significant gap compared to state-of-the-art (SOTA) embedding models in STS tasks, case studies reveal substantial room for improvement. Additionally, PropNet enables us to analyze and understand the human cognitive processes underlying STS benchmarks.

</details>


### [43] [Simulating and Analysing Human Survey Responses with Large Language Models: A Case Study in Energy Stated Preference](https://arxiv.org/abs/2503.10652)

*Han Wang, Jacek Pawlak, Aruna Sivakumar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Stated Preference Surveys, Consumer Choices, Data Collection, Machine Learning

**Relevance Score:** 8

**TL;DR:** This study investigates the use of large language models (LLMs) in simulating consumer choices within stated preference surveys for energy-related scenarios, examining prompt design and integration with traditional choice models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The development aims to improve the efficiency and effectiveness of survey research in capturing consumer preferences through the integration of LLMs, which may alleviate issues with traditional survey methods.

**Method:** The study tests the performance of various LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) in simulating consumer choices, analyzing factors such as prompt design and reasoning capabilities, and compares them with traditional choice models.

**Key Contributions:**

	1. Integration of LLMs into stated preference surveys for energy-related choices
	2. Demonstration of mixed logit models aiding in prompt refinement
	3. Insights into the balance between prompt length and accuracy in LLM simulations

**Result:** DeepSeek-R1 achieved the highest average accuracy (77%) in simulating consumer choices, while LLMs generally showed better accuracy compared to random guessing but were not sufficient for practical simulation use. Mixed logit models helped refine LLM prompts leading to better performance.

**Limitations:** Performance remains insufficient for practical simulation use, and cloud-based models do not consistently outperform smaller local models.

**Conclusion:** Despite the limitations in LLM performance, they offer a scalable solution for survey research with minimal historical data, indicating future directions for refinement in prompt design and reasoning capabilities.

**Abstract:** Survey research plays a crucial role in studies by capturing consumer preferences and informing policy decisions. Stated preference (SP) surveys help researchers understand how individuals make trade-offs in hypothetical, potentially futuristic, scenarios. However, traditional methods are costly, time-consuming, and affected by respondent fatigue and ethical constraints. Large language models (LLMs) have shown remarkable capabilities in generating human-like responses, prompting interest in their use in survey research. This study investigates LLMs for simulating consumer choices in energy-related SP surveys and explores their integration into data collection and analysis workflows. Test scenarios were designed to assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5, DeepSeek-R1) at individual and aggregated levels, considering prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, model types, integration with traditional choice models, and potential biases. While LLMs achieve accuracy above random guessing, performance remains insufficient for practical simulation use. Cloud-based LLMs do not consistently outperform smaller local models. DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Previous SP choices are the most effective input; longer prompts with more factors reduce accuracy. Mixed logit models can support LLM prompt refinement. Reasoning LLMs show potential in data analysis by indicating factor significance, offering a qualitative complement to statistical models. Despite limitations, pre-trained LLMs offer scalability and require minimal historical data. Future work should refine prompts, further explore CoT reasoning, and investigate fine-tuning techniques.

</details>


### [44] [Evaluating Clinical Competencies of Large Language Models with a General Practice Benchmark](https://arxiv.org/abs/2503.17599)

*Zheqing Li, Yiying Yang, Jiping Lang, Wenhao Jiang, Yuhang Zhao, Shuang Li, Dingqian Wang, Zhu Lin, Xuanna Li, Yuze Tang, Jiexian Qiu, Xiaolin Lu, Hongji Yu, Shuang Chen, Yuhua Bi, Xiaofei Zeng, Yixian Chen, Junrong Chen, Lin Yao*

**Main category:** cs.CL

**Keywords:** Large Language Models, general practice, evaluation framework, benchmark, competency

**Relevance Score:** 9

**TL;DR:** This paper introduces a new evaluation framework for assessing Large Language Models (LLMs) in their potential roles as general practitioners (GPs), highlighting the necessity for further development before practical deployment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of competency-based benchmarks for evaluating LLMs in real-world clinical contexts, specifically in general practice.

**Method:** A novel evaluation framework is proposed, alongside the creation of a general practice benchmark (GPBench) that includes expert-annotated data representing routine clinical practice.

**Key Contributions:**

	1. Developed a new evaluation framework for LLMs in general practice
	2. Introduced GPBench, a benchmark with expert-annotated data for LLMs
	3. Evaluated ten LLMs to determine their readiness for clinical use

**Result:** Evaluation of ten state-of-the-art LLMs showed that they are not yet ready for deployment in general practice without human supervision and require further adaptation to GP responsibilities.

**Limitations:** Current LLMs require human oversight and optimization before deployment in clinical settings.

**Conclusion:** Current LLMs need significant optimization to meet the responsibilities of general practitioners effectively.

**Abstract:** Large Language Models (LLMs) have demonstrated considerable potential in general practice. However, existing benchmarks and evaluation frameworks primarily depend on exam-style or simplified question-answer formats, lacking a competency-based structure aligned with the real-world clinical responsibilities encountered in general practice. Consequently, the extent to which LLMs can reliably fulfill the duties of general practitioners (GPs) remains uncertain. In this work, we propose a novel evaluation framework to assess the capability of LLMs to function as GPs. Based on this framework, we introduce a general practice benchmark (GPBench), whose data are meticulously annotated by domain experts in accordance with routine clinical practice standards. We evaluate ten state-of-the-art LLMs and analyze their competencies. Our findings indicate that current LLMs are not yet ready for deployment in such settings without human oversight, and further optimization specifically tailored to the daily responsibilities of GPs is essential.

</details>


### [45] [Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](https://arxiv.org/abs/2503.21696)

*Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, Guiyang Hou, Zhe Zheng, Hang Zhang, Xin Li, Weiming Lu, Peng Li, Yueting Zhuang*

**Main category:** cs.CL

**Keywords:** embodied reasoning, interactive search, machine learning, spatial reasoning, temporal reasoning

**Relevance Score:** 7

**TL;DR:** Embodied Reasoner improves reasoning in interactive tasks requiring spatial and temporal understanding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of deep reasoning models in embodied domains needing interaction with environments through image action trajectories.

**Method:** The paper introduces a model that synthesizes a large dataset of Observation-Thought-Action trajectories and employs a three-stage training pipeline involving imitation learning, self-exploration, and reflection tuning.

**Key Contributions:**

	1. Introduction of the Embodied Reasoner model for interactive embodied search tasks
	2. Creation of a robust dataset of Observation-Thought-Action trajectories for training
	3. Demonstration of improved reasoning capabilities over existing models

**Result:** Embodied Reasoner outperforms various advanced visual reasoning models, demonstrating significant improvements in handling long-horizon tasks and reducing logical inconsistencies.

**Limitations:** 

**Conclusion:** The model shows promise in enhancing reasoning capabilities in embodied search tasks, with further applications in interactive environments.

**Abstract:** Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.

</details>


### [46] [Is analogy enough to draw novel adjective-noun inferences?](https://arxiv.org/abs/2503.24293)

*Hayley Ross, Kathryn Davidson, Najoung Kim*

**Main category:** cs.CL

**Keywords:** analogical reasoning, compositionality, language models

**Relevance Score:** 7

**TL;DR:** This study examines whether inferences from novel adjective-noun combinations can be derived through analogical reasoning rather than compositional mechanisms in both humans and LLMs.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the mechanisms underlying how humans and LLMs generalize meanings from novel adjective-noun combinations.

**Method:** A model of analogical reasoning using lexical similarity was built, and human participants were tasked with reasoning by analogy.

**Key Contributions:**

	1. Developed a model for analogical reasoning in lexical items.
	2. Demonstrated the limits of analogical reasoning in deriving meanings compared to compositional methods.
	3. Provided empirical evidence from human reasoning aligning with LLM performance.

**Result:** The analogical reasoning strategy was effective for a large part of the dataset; however, there were novel combinations where both humans and LLMs derived similar inferences that were not adequately handled by analogy.

**Limitations:** The study may not cover all possible combinations of adjectives and nouns, limiting the generalization of findings.

**Conclusion:** The generalization mechanisms for certain novel adjective-noun combinations involve compositional understanding rather than being solely reliant on analogy.

**Abstract:** Recent work (Ross et al., 2025, 2024) has argued that the ability of humans and LLMs respectively to generalize to novel adjective-noun combinations shows that they each have access to a compositional mechanism to determine the phrase's meaning and derive inferences. We study whether these inferences can instead be derived by analogy to known inferences, without need for composition. We investigate this by (1) building a model of analogical reasoning using similarity over lexical items, and (2) asking human participants to reason by analogy. While we find that this strategy works well for a large proportion of the dataset of Ross et al. (2025), there are novel combinations for which both humans and LLMs derive convergent inferences but which are not well handled by analogy. We thus conclude that the mechanism humans and LLMs use to generalize in these cases cannot be fully reduced to analogy, and likely involves composition.

</details>
