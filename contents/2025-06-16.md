# 2025-06-16

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 107]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Needling Through the Threads: A Visualization Tool for Navigating Threaded Online Discussions](https://arxiv.org/abs/2506.11276)

*Yijun Liu, Frederick Choi, Eshwar Chandrasekharan*

**Main category:** cs.HC

**Keywords:** visual analytics, moderation, threaded discussions, cognitive load, online discourse

**Relevance Score:** 7

**TL;DR:** Needle is an interactive system designed to improve navigation and comprehension in large-scale threaded online discussions by utilizing visual analytics.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Moderators often find it challenging to effectively manage multiple simultaneous discussions due to high volumes of user-generated content, which leads to difficulties in maintaining context and timely moderation.

**Method:** Needle leverages visual analytics to summarize key metrics such as activity, toxicity levels, and voting trends over time, providing both high-level insights and detailed breakdowns of discussions. A user study was conducted with ten Reddit moderators to evaluate its effectiveness.

**Key Contributions:**

	1. Introduction of Needle, an interactive system for moderating threaded discussions
	2. Use of visual analytics to summarize discussion metrics
	3. Provision of design guidelines for future moderation tools

**Result:** The user study found that Needle helps reduce cognitive load for moderators, allows them to prioritize important areas, and aids in decision-making during moderation.

**Limitations:** 

**Conclusion:** Needle not only improves understanding of complex threaded discussions but also sets design guidelines for future visualization-driven moderation tools and sociotechnical systems.

**Abstract:** Navigating large-scale online discussions is difficult due to the rapid pace and large volume of user-generated content. Prior work in CSCW has shown that moderators often struggle to follow multiple simultaneous discussions, track evolving conversations, and maintain contextual understanding--all of which hinder timely and effective moderation. While platforms like Reddit use threaded structures to organize discourse, deeply nested threads can still obscure discussions and make it difficult to grasp the overall trajectory of conversations. In this paper, we present an interactive system called Needle to support better navigation and comprehension of complex discourse within threaded discussions. Needle uses visual analytics to summarize key conversational metrics--such as activity, toxicity levels, and voting trends--over time, offering both high-level insights and detailed breakdowns of discussion threads. Through a user study with ten Reddit moderators, we find that Needle supports moderation by reducing cognitive load in making sense of large discussion, helping prioritize areas that need attention, and providing decision-making supports. Based on our findings, we provide a set of design guidelines to inform future visualization-driven moderation tools and sociotechnical systems. To the best of our knowledge, Needle is one of the first systems to combine interactive visual analytics with human-in-the-loop moderation for threaded online discussions.

</details>


### [2] [Combining Log Data and Collaborative Dialogue Features to Predict Project Quality in Middle School AI Education](https://arxiv.org/abs/2506.11326)

*Conrad Borchers, Xiaoyi Tian, Kristy Elizabeth Boyer, Maya Israel*

**Main category:** cs.HC

**Keywords:** project-based learning, AI education, multimodal learning analytics, dialogue analysis, system interaction logs

**Relevance Score:** 6

**TL;DR:** This study examines how dialogue and system interaction logs can predict the quality of collaborative AI learning projects among middle school students, with a focus on productivity, content richness, and lexical variation in their chatbot training phrases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of tracking project development and assessing success in open-ended project-based learning in computing education.

**Method:** The study analyzed linguistic features from dialogue transcripts and behavioral features from system logs of 94 middle school students working in pairs on AI projects to predict three project quality outcomes.

**Key Contributions:**

	1. Demonstrates the interplay between dialogue and system log data in project-based learning assessments.
	2. Shows the varying predictive power of different data modalities regarding specific learning outcomes.
	3. Highlights the impact of multimodal fusion on prediction effectiveness in educational contexts.

**Result:** Log data was found to be a better predictor of productivity, while dialogue data was more effective for content richness. Multimodal fusion improved predictions for productivity and lexical variation but not for content richness.

**Limitations:** 

**Conclusion:** The effectiveness of multimodal fusion in predicting learning outcomes varies depending on the specific aspect being assessed, contributing to multimodal learning analytics.

**Abstract:** Project-based learning plays a crucial role in computing education. However, its open-ended nature makes tracking project development and assessing success challenging. We investigate how dialogue and system interaction logs predict project quality during collaborative, project-based AI learning of 94 middle school students working in pairs. We used linguistic features from dialogue transcripts and behavioral features from system logs to predict three project quality outcomes: productivity (number of training phrases), content richness (word density), and lexical variation (word diversity) of chatbot training phrases. We compared the predictive accuracy of each modality and a fusion of the modalities. Results indicate log data better predicts productivity, while dialogue data is more effective for content richness. Both modalities modestly predict lexical variation. Multimodal fusion improved predictions for productivity and lexical variation of training phrases but not content richness. These findings suggest that the value of multimodal fusion depends on the specific learning outcome. The study contributes to multimodal learning analytics by demonstrating the nuanced interplay between behavioral and linguistic data in assessing student learning progress in open-ended AI learning environments.

</details>


### [3] [Meeting Patients Where They're At: Toward the Expansion of Chaplaincy Care into Online Spiritual Care Communities](https://arxiv.org/abs/2506.11366)

*Alemitu Bezabih, Shadi Nourriz, Anne-Marie Snider, Rosalie Rauenzahn, C. Estelle Smith*

**Main category:** cs.HC

**Keywords:** spiritual care, online communities, chaplaincy, HCI, CSCW

**Relevance Score:** 7

**TL;DR:** This study explores expanding spiritual care into online spaces, focusing on community roles and implications for design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing need for spiritual care that is currently under-served and not well integrated into technology and online communities.

**Method:** A mixed-methods approach involving interviews and user testing sessions with 22 chaplains, centered around Reddit support communities.

**Key Contributions:**

	1. Model of a 'Care Loop' for integrating formal care and community care in spirituality
	2. Identification of challenges and opportunities for chaplains in online settings
	3. Design implications for future online spiritual care projects

**Result:** The study identified benefits of online spiritual care communities (OSCCs) such as accessibility, scalability, and patient-initiated care, while also highlighting the limitations of technology in mediating spiritual care.

**Limitations:** The study highlights that technology cannot fully mediate spiritual care and indicates additional research is needed for new online interventions.

**Conclusion:** Developing a 'Care Loop' model can enhance access to spiritual care and raise awareness of its utilization, alongside providing design implications for online spiritual care interventions.

**Abstract:** Despite a growing need for spiritual care in the US, it is often under-served, inaccessible, or misunderstood, while almost no prior work in CSCW/HCI research has engaged with professional chaplains and spiritual care providers. This interdisciplinary study aims to develop a foundational understanding of how spiritual care may (or may not) be expanded into online spaces -- especially focusing on anonymous, asynchronous, and text-based online communities. We conducted an exploratory mixed-methods study with chaplains (N=22) involving interviews and user testing sessions centered around Reddit support communities to understand participants' perspectives on technology and their ideations about the role of chaplaincy in prospective Online Spiritual Care Communities (OSCCs). Our Grounded Theory Method analysis highlighted benefits of OSCCs including: meeting patients where they are at; accessibility and scalability; and facilitating patient-initiated care. Chaplains highlighted how their presence in OSCCs could help with shaping peer interactions, moderation, synchronous chats for group care, and redirecting to external resources, while also raising important feasibility concerns, risks, and needs for future design and research. We used an existing taxonomy of chaplaincy techniques to show that some spiritual care strategies may be amenable to online spaces, yet we also exposed the limitations of technology to fully mediate spiritual care and the need to develop new online chaplaincy interventions. Based on these findings, we contribute the model of a ``Care Loop'' between institutionally-based formal care and platform-based community care to expand access and drive greater awareness and utilization of spiritual care. We also contribute design implications to guide future work in online spiritual care.

</details>


### [4] [Co-Designing a Chatbot for Culturally Competent Clinical Communication: Experience and Reflections](https://arxiv.org/abs/2506.11393)

*Sandro Radovanović, Shuangyu Li*

**Main category:** cs.HC

**Keywords:** AI chatbot, cultural competence, communication training, medical education, healthcare

**Relevance Score:** 8

**TL;DR:** This paper explores the use of an AI-driven chatbot to enhance communication training for medical students, focusing on cultural competence.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To address the resource challenges of traditional simulated patient training in medical education, particularly in under-resourced settings.

**Method:** Development and pilot testing of an AI chatbot designed to simulate patient conversations and provide structured feedback based on the ACT Cultural Competence model.

**Key Contributions:**

	1. Exploration of AI in medical communication training
	2. Pilot testing of a novel chatbot for cultural competence
	3. Identification of both benefits and limitations in training using AI tools

**Result:** Initial feedback from third-year medical students indicated that the chatbot facilitated reflection on communication skills, particularly in empathy, but also revealed challenges such as lack of nonverbal cues.

**Limitations:** Absence of nonverbal cues and tendency for virtual patients to be overly agreeable.

**Conclusion:** The use of AI tools like chatbots shows promise for communication training, yet considerable limitations remain that need to be addressed through further research.

**Abstract:** Clinical communication skills are essential for preparing healthcare professionals to provide equitable care across cultures. However, traditional training with simulated patients can be resource intensive and difficult to scale, especially in under-resourced settings. In this project, we explore the use of an AI-driven chatbot to support culturally competent communication training for medical students. The chatbot was designed to simulate realistic patient conversations and provide structured feedback based on the ACT Cultural Competence model. We piloted the chatbot with a small group of third-year medical students at a UK medical school in 2024. Although we did not follow a formal experimental design, our experience suggests that the chatbot offered useful opportunities for students to reflect on their communication, particularly around empathy and interpersonal understanding. More challenging areas included addressing systemic issues and historical context. Although this early version of the chatbot helped surface some interesting patterns, limitations were also clear, such as the absence of nonverbal cues and the tendency for virtual patients to be overly agreeable. In general, this reflection highlights both the potential and the current limitations of AI tools in communication training. More work is needed to better understand their impact and improve the learning experience.

</details>


### [5] [Do Not Immerse and Drive? Prolonged Effects of Cybersickness on Physiological Stress Markers And Cognitive Performance](https://arxiv.org/abs/2506.11536)

*Daniel Zielasko, Ben Rehling, Bernadette von Dawans, Gregor Domes*

**Main category:** cs.HC

**Keywords:** cybersickness, virtual reality, physiological stress, working memory, cognitive performance

**Relevance Score:** 8

**TL;DR:** This study investigates the aftereffects of VR-induced motion sickness, focusing on physiological stress markers and working memory performance, revealing significant discomfort and cognitive decline post-exposure.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of prolonged virtual reality exposure on physiological stress and cognitive performance, particularly aftereffects of motion sickness.

**Method:** Participants were subjected to a carousel simulation to induce cybersickness while assessing subjective discomfort, physiological stress indicators, and cognitive performance through a 90-minute post-exposure analysis.

**Key Contributions:**

	1. Identified prolonged physiological stress response after VR exposure.
	2. Demonstrated decline in working memory performance following cybersickness.
	3. Emphasized the need for extended washout periods in XR research.

**Result:** Significant increases in subjective and physiological stress markers were found, along with a notable decline in working memory performance, with peak symptoms reported up to 90 minutes after VR exposure.

**Limitations:** 

**Conclusion:** The findings suggest a need for longer recovery periods after VR exposure to mitigate the effects on cognitive function and highlight safety risks in professional settings.

**Abstract:** Extended exposure to virtual reality environments can induce motion sickness, often referred to as cybersickness, which may lead to physiological stress responses and impaired cognitive performance. This study investigates the aftereffects of VR-induced motion sickness with a focus on physiological stress markers and working memory performance. Using a carousel simulation to elicit cybersickness, we assessed subjective discomfort (SSQ, FMS), physiological stress (salivary cortisol, alpha-amylase, electrodermal activity, heart rate), and cognitive performance (n-Back task) over a 90-minute post-exposure period. Our findings demonstrate a significant increase in both subjective and physiological stress indicators following VR exposure, accompanied by a decline in working memory performance. Notably, delayed symptom progression was observed in a substantial proportion of participants, with some reporting peak symptoms up to 90 minutes post-stimulation. Salivary cortisol levels remained elevated throughout the observation period, indicating prolonged stress recovery. These results highlight the need for longer washout phases in XR research and raise safety concerns for professional applications involving post-exposure task performance.

</details>


### [6] ["If we misunderstand the client, we misspend 100 hours": Exploring conversational AI and response types for information elicitation](https://arxiv.org/abs/2506.11610)

*Daniel Hove Paludan, Julie Fredsgård, Kasper Patrick Bährentz, Ilhan Aslan*

**Main category:** cs.HC

**Keywords:** Client-Designer Alignment, Conversational Agent, Choice-Based Responses, Design Practice, User Experience

**Relevance Score:** 6

**TL;DR:** This paper investigates how digital technologies, specifically conversational agents and choice-based response formats, can improve client-designer alignment during requirements elicitation in design projects.

**Read time:** 27 min

<details>
  <summary>Details</summary>

**Motivation:** Client-designer alignment is critical for the success of design projects, yet the impact of digital technologies on this alignment is underexplored.

**Method:** A three-phase study involving semi-structured interviews, factorial design experiments with mock clients, and reflections from design companies on a developed digital elicitation tool was conducted.

**Key Contributions:**

	1. Investigates the use of digital technologies in client-designer alignment
	2. Demonstrates the effects of AI and response formats on client input clarity
	3. Provides implications for integrating conversational AI into design practices

**Result:** The integration of conversational AI and choice-based response formats in the elicitation tool led to lower dependability scores on the User Experience Questionnaire but resulted in clearer client input.

**Limitations:** The findings were based on a limited sample of design companies and mock clients, which may not fully represent real-world applications.

**Conclusion:** The paper outlines design implications for using conversational AI and choice-based responses in tools that support mutual understanding in early client-designer collaboration.

**Abstract:** Client-designer alignment is crucial to the success of design projects, yet little research has explored how digital technologies might influence this alignment. To address this gap, this paper presents a three-phase study investigating how digital systems can support requirements elicitation in professional design practice. Specifically, it examines how integrating a conversational agent and choice-based response formats into a digital elicitation tool affects early-stage client-designer collaboration. The first phase of the study inquired into the current practices of 10 design companies through semi-structured interviews, informing the system's design. The second phase evaluated the system using a 2x2 factorial design with 50 mock clients, quantifying the effects of conversational AI and response type on user experience and perceived preparedness. In phase three, the system was presented to seven of the original 10 companies to gather reflections on its value, limitations, and potential integration into practice. Findings show that both conversational AI and choice-based responses lead to lower dependability scores on the User Experience Questionnaire, yet result in client input with greater clarity. We contribute design implications for integrating conversational AI and choice-based responses into elicitation tools to support mutual understanding in early-stage client-designer collaboration.

</details>


### [7] [Perspectives on Explanation Formats From Two Stakeholder Groups in Germany: Software Providers and Dairy Farmers](https://arxiv.org/abs/2506.11665)

*Mengisti Berihu Girmay, Felix Möhrle*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Explainable AI, Agricultural informatics, User acceptance, Dairy farming

**Relevance Score:** 6

**TL;DR:** The paper explores the perceptions of software providers in the German dairy industry regarding the explanation needs of dairy farmers for digital decision support systems, specifically focusing on mastitis detection in cows.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the gap between software providers' assumptions and dairy farmers' actual needs for explanations of digital decision support systems to enhance user acceptance.

**Method:** The research involved creating four explanation formats for mastitis detection and surveying both dairy farmers and software providers to assess their perceptions and preferences regarding these formats.

**Key Contributions:**

	1. Development of explanation formats for mastitis detection
	2. Comparison of views between farmers and software providers
	3. Highlighting the need for user requirements analysis

**Result:** The comparison of feedback indicated significant discrepancies between software providers’ assumptions and the preferences of dairy farmers, suggesting the need for better user requirement analyses.

**Limitations:** The study's findings are not representative due to a small sample size of participants.

**Conclusion:** The findings emphasize the importance of understanding farmer needs to improve the design and acceptance of digital systems in agriculture.

**Abstract:** This paper examines the views of software providers in the German dairy industry with regard to dairy farmers' needs for explanation of digital decision support systems. The study is based on mastitis detection in dairy cows using a hypothetical herd management system. We designed four exemplary explanation formats for mastitis assessments with different types of presentation (textual, rule-based, herd comparison, and time series). In our previous study, 14 dairy farmers in Germany had rated these formats in terms of comprehensibility and the trust they would have in a system providing each format. In this study, we repeat the survey with 13 software providers active in the German dairy industry. We ask them how well they think the formats would be received by farmers. We hypothesized that there may be discrepancies between the views of both groups that are worth investigating, partly to find reasons for the reluctance to adopt digital systems. A comparison of the feedback from both groups supports the hypothesis and calls for further investigation. The results show that software providers tend to make assumptions about farmers' preferences that are not necessarily accurate. Our study, although not representative due to the small sample size, highlights the potential benefits of a thorough user requirements analysis (farmers' needs) to improve software adaptation and user acceptance.

</details>


### [8] [Interaction, Process, Infrastructure: A Unified Architecture for Human-Agent Collaboration](https://arxiv.org/abs/2506.11718)

*Yun Wang, Yan Lu*

**Main category:** cs.HC

**Keywords:** human-agent systems, collaboration, AI tools, interaction, process

**Relevance Score:** 8

**TL;DR:** The paper proposes a layered framework for human-agent systems that focuses on creating a sustainable and adaptive collaboration between humans and AI tools.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragmentation of current AI tools which assist with isolated tasks but lack the ability for sustained collaboration in professional knowledge work.

**Method:** The authors introduce a layered framework integrating interaction, process, and infrastructure dimensions, emphasizing process as a primary focus.

**Key Contributions:**

	1. Introduction of a layered framework for human-agent collaboration
	2. Emphasis on process as a pivotal element
	3. Clarification of current AI tools' limitations and unified design approaches.

**Result:** The model allows for explicit, inspectable, and adaptable processes enabling alignment of evolving goals between humans and agents.

**Limitations:** 

**Conclusion:** This framework clarifies limitations of current AI tools and highlights opportunities for AI system design with an emphasis on structured collaboration.

**Abstract:** As AI tools proliferate across domains, from chatbots and copilots to emerging agents, they increasingly support professional knowledge work. Yet despite their growing capabilities, these systems remain fragmented: they assist with isolated tasks but lack the architectural scaffolding for sustained, adaptive collaboration. We propose a layered framework for human-agent systems that integrates three interdependent dimensions: interaction, process, and infrastructure. Crucially, our architecture elevates process to a primary focus by making it explicit, inspectable, and adaptable, enabling humans and agents to align with evolving goals and coordinate over time. This model clarifies limitations of current tools, unifies emerging system design approaches, and reveals new opportunities for researchers and AI system builders. By grounding intelligent behavior in structured collaboration, we reimagine human-agent collaboration not as task-specific augmentation, but as a form of coherent and aligned system for real-world work.

</details>


### [9] [GeoPandas-AI: A Smart Class Bringing LLM as Stateful AI Code Assistant](https://arxiv.org/abs/2506.11781)

*Gaspard Merten, Gilles Dejaegere, Mahmoud Sakr*

**Main category:** cs.HC

**Keywords:** geospatial data analysis, GeoPandas, LLM integration

**Relevance Score:** 7

**TL;DR:** GeoPandas-AI integrates LLMs into GeoPandas to simplify geospatial data analysis.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the expertise gap in using complex geospatial data manipulation tools like GeoPandas.

**Method:** The paper presents a design for a smart GeoDataFrame class that incorporates LLMs for intelligent data analysis and code generation.

**Key Contributions:**

	1. Integration of LLMs in GeoPandas for geospatial analysis
	2. Development of a smart GeoDataFrame class
	3. Open-source implementation available on PyPI

**Result:** An open-source implementation of GeoPandas-AI is provided via the PyPI package manager, showing effective integration of conversational interfaces and stateful LLMs.

**Limitations:** 

**Conclusion:** GeoPandas-AI represents a new paradigm in geospatial development, enhancing code-copiloting capabilities.

**Abstract:** Geospatial data analysis plays a crucial role in tackling intricate societal challenges such as urban planning and climate modeling. However, employing tools like GeoPandas, a prominent Python library for geospatial data manipulation, necessitates expertise in complex domain-specific syntax and workflows. GeoPandas-AI addresses this gap by integrating LLMs directly into the GeoPandas workflow, transforming the GeoDataFrame class into an intelligent, stateful class for both data analysis and geospatial code development. This paper formalizes the design of such a smart class and provides an open-source implementation of GeoPandas-AI in PyPI package manager. Through its innovative combination of conversational interfaces and stateful exploitation of LLMs for code generation and data analysis, GeoPandas-AI introduces a new paradigm for code-copilots and instantiates it for geospatial development.

</details>


### [10] [Digital Labor: Challenges, Ethical Insights, and Implications](https://arxiv.org/abs/2506.11788)

*ATM Mizanur Rahman, Sharifa Sultana*

**Main category:** cs.HC

**Keywords:** digital labor, crowdsourcing, gig economy, AI, platform design

**Relevance Score:** 4

**TL;DR:** This paper analyzes over 300 research papers on digital labor, focusing on the challenges and trends affecting digital workers on crowdsourcing platforms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the low pay, unfair conditions, and lack of recognition for digital workers on crowdsourcing platforms and to synthesize findings in digital labor research.

**Method:** A systematic literature review was conducted, selecting and analyzing 143 relevant papers on digital gig-labor from a database of over 300 papers published from 2015 to 2024.

**Key Contributions:**

	1. Synthesis of literature on digital labor issues
	2. Identification of key challenges faced by gig workers
	3. Insights for designing better platforms and policies

**Result:** The analysis reveals key challenges and trends in the experiences of digital workers, highlighting persistent issues in representation and governance.

**Limitations:** 

**Conclusion:** The paper offers critical insights for researchers, platform designers, and policymakers to improve the conditions and recognition of digital workers and identifies areas for future research.

**Abstract:** Digital workers on crowdsourcing platforms (e.g., Amazon Mechanical Turk, Appen, Clickworker, Prolific) play a crucial role in training and improving AI systems, yet they often face low pay, unfair conditions, and a lack of recognition for their contributions. To map these issues in the existing literature of computer science, AI, and related scholarship, we selected over 300 research papers on digital labor published between 2015 and 2024, narrowing them down to 143 on digital gig-labor for a detailed analysis. This analysis provides a broad overview of the key challenges, concerns, and trends in the field. Our synthesis reveals how the persistent patterns of representation and voices of gig workers in digital labor are structured and governed. We offer new insights for researchers, platform designers, and policymakers, helping them better understand the experiences of digital workers and pointing to key areas where interventions and future investigations are promptly needed. By mapping the findings from the past ten years' growth of the domain and possible implications, this paper contributes to a more coherent and critical understanding of digital labor in contemporary and future AI ecosystems.

</details>


### [11] [Conversational AI as a Catalyst for Informal Learning: An Empirical Large-Scale Study on LLM Use in Everyday Learning](https://arxiv.org/abs/2506.11789)

*Nađa Terzimehić, Babette Bühler, Enkelejda Kasneci*

**Main category:** cs.HC

**Keywords:** large language models, self-directed learning, user adoption, learning patterns, trust behaviors

**Relevance Score:** 7

**TL;DR:** The study analyzes the adoption of large language models (LLMs) in self-directed learning, revealing that a majority of users, particularly young adults, integrate LLMs into their learning routines, with emerging patterns and trust issues in their use.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the adoption and impact of large language models on informal learning practices.

**Method:** A large-scale survey of 776 participants examining the use of LLMs in self-directed learning and identifying learner types based on their usage patterns.

**Key Contributions:**

	1. Identified four types of learners using LLMs for self-directed learning.
	2. Provided insights into the trust issues users have with LLMs' accuracy and privacy.
	3. Emphasized the importance of inclusive learning design that adapts to different user needs.

**Result:** 88% of respondents utilize LLMs for various learning tasks, with young adults leading this trend, revealing four distinct types of learners and paradoxical trust behaviors regarding LLMs' accuracy and privacy.

**Limitations:** 

**Conclusion:** The findings highlight the need for adaptive learning designs that cater to diverse learner types and incorporate collaborative learning and media variety.

**Abstract:** Large language models have not only captivated the public imagination but have also sparked a profound rethinking of how we learn. In the third year following the breakthrough launch of ChatGPT, everyday informal learning has been transformed as diverse user groups explore these novel tools. Who is embracing LLMs for self-directed learning, and who remains hesitant? What are their reasons for adoption or avoidance? What learning patterns emerge with this novel technological landscape? We present an in-depth analysis from a large-scale survey of 776 participants, showcasing that 88% of our respondents already incorporate LLMs into their everyday learning routines for a wide variety of (learning) tasks. Young adults are at the forefront of adopting LLMs, primarily to enhance their learning experiences independently of time and space. Four types of learners emerge across learning contexts, depending on the tasks they perform with LLMs and the devices they use to access them. Interestingly, our respondents exhibit paradoxical behaviours regarding their trust in LLMs' accuracy and privacy protection measures. Our implications emphasize the importance of including different media types for learning, enabling collaborative learning, providing sources and meeting the needs of different types of learners and learning by design.

</details>


### [12] [Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training](https://arxiv.org/abs/2506.11890)

*Judson Leroy Dean Haynes IV*

**Main category:** cs.HC

**Keywords:** Virtual Reality, AI Avatars, Teacher Training, Pedagogical Framework, Cognitive Load

**Relevance Score:** 8

**TL;DR:** This literature review explores the integration of AI avatars in VR teacher training, proposing a framework called Graduated Realism to optimize avatar realism for effective pedagogy.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To determine the optimal level of avatar realism in AI-powered VR simulators for teacher training and address the gap between technological photorealism and pedagogical needs.

**Method:** Systematic review of literature examining the evolution of avatar realism and application of learning theories like Cognitive Load Theory.

**Key Contributions:**

	1. Proposes the Graduated Realism framework for VR training.
	2. Introduces a novel architecture, Crazy Slots, for efficient avatar response generation.
	3. Synthesize evidence-based principles for AI simulator design.

**Result:** High-fidelity avatars can impose excessive cognitive load on novices; evidence supports the Graduated Realism framework which starts with lower-fidelity avatars and increases complexity as skills develop.

**Limitations:** 

**Conclusion:** A pedagogically grounded approach to realism in VR teacher training is essential for scalability and effectiveness in simulation design.

**Abstract:** Virtual Reality simulators offer a powerful tool for teacher training, yet the integration of AI-powered student avatars presents a critical challenge: determining the optimal level of avatar realism for effective pedagogy. This literature review examines the evolution of avatar realism in VR teacher training, synthesizes its theoretical implications, and proposes a new pedagogical framework to guide future design. Through a systematic review, this paper traces the progression from human-controlled avatars to generative AI prototypes. Applying learning theories like Cognitive Load Theory, we argue that hyper-realism is not always optimal, as high-fidelity avatars can impose excessive extraneous cognitive load on novices, a stance supported by recent empirical findings. A significant gap exists between the technological drive for photorealism and the pedagogical need for scaffolded learning. To address this gap, we propose Graduated Realism, a framework advocating for starting trainees with lower-fidelity avatars and progressively increasing behavioral complexity as skills develop. To make this computationally feasible, we outline a novel single-call architecture, Crazy Slots, which uses a probabilistic engine and a Retrieval-Augmented Generation database to generate authentic, real-time responses without the latency and cost of multi-step reasoning models. This review provides evidence-based principles for designing the next generation of AI simulators, arguing that a pedagogically grounded approach to realism is essential for creating scalable and effective teacher education tools.

</details>


### [13] [Predicting Trust Dynamics Type Using Seven Personal Characteristics](https://arxiv.org/abs/2409.07406)

*Hyesun Chung, X. Jessie Yang*

**Main category:** cs.HC

**Keywords:** trust dynamics, automated technologies, personal characteristics, decision tree, humans-in-the-loop

**Relevance Score:** 8

**TL;DR:** The study investigates the relationship between personal characteristics and trust dynamics in automated technologies, identifying three trust dynamics types through clustering and analyzing differences in behaviors and ratings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the associations between trust dynamics in automated technologies and personal characteristics, and to predict trust dynamics types.

**Method:** Human-subject experiment with 130 participants performing a surveillance task, collecting data through a survey and using k-means clustering to identify trust dynamics types.

**Key Contributions:**

	1. Identification of three distinct trust dynamics types related to personal characteristics.
	2. Significant findings on how personal traits influence trust in automated technologies.
	3. Prediction model for trust dynamics with practical implications.

**Result:** Identified three trust dynamics types: Bayesian decision makers, disbelievers, and oscillators; found significant differences in personal characteristics, behaviors, and performance among these types.

**Limitations:** 

**Conclusion:** Developed a decision tree model with 70% accuracy to predict trust dynamics types based on personal characteristics.

**Abstract:** This study aims to explore the associations between individuals' trust dynamics in automated/autonomous technologies and their personal characteristics, and to further examine whether personal characteristics can be used to predict a user's trust dynamics type. We conducted a human-subject experiment (N=130) in which participants performed a simulated surveillance task assisted by an automated threat detector. Using a pre-experimental survey covering 12 constructs and 28 dimensions, we collected data on participants' personal characteristics. Based on the experimental data, we performed k-means clustering and identified three trust dynamics types. Subsequently, we conducted one-way Analyses of Variance to evaluate differences among the three trust dynamics types in terms of personal characteristics, behaviors, performance, and post-experimental ratings. Participants were clustered into three groups, namely Bayesian decision makers, disbelievers, and oscillators. Results showed that the clusters differ significantly in seven personal characteristics: masculinity, positive affect, extraversion, neuroticism, intellect, performance expectancy, and high expectations. The disbelievers tend to have high neuroticism and low performance expectancy. The oscillators tend to have higher scores in masculinity, positive affect, extraversion, and intellect. We also found significant differences in behaviors, performance, and post-experimental ratings across the three groups. The disbelievers are the least likely to blindly follow the recommendations made by the automated threat detector. Based on the significant personal characteristics, we developed a decision tree model to predict the trust dynamics type with an accuracy of 70%. This model offers promising implications for identifying individuals whose trust dynamics may deviate from a Bayesian pattern.

</details>


### [14] [Examining Human-AI Collaboration for Co-Writing Constructive Comments Online](https://arxiv.org/abs/2411.03295)

*Farhana Shahid, Maximilian Dittgen, Mor Naaman, Aditya Vashistha*

**Main category:** cs.HC

**Keywords:** Large Language Models, Constructive Comments, Online Discourse, Ethical Considerations, Social Issues

**Relevance Score:** 9

**TL;DR:** This paper evaluates the effectiveness of large language models (LLMs) in aiding the writing of constructive comments on divisive social issues and reveals misalignment between LLM and human perceptions of constructiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of expressing constructive disagreement online, especially on divisive social issues like Islamophobia and homophobia.

**Method:** Controlled experiments with 600 participants from India and the US who reviewed and wrote comments, examining their interactions with LLM-generated suggestions.

**Key Contributions:**

	1. Identification of misalignment between LLM and human constructs of constructiveness.
	2. Demonstration that LLMs can significantly improve comment constructiveness and reduce toxicity.
	3. Ethical considerations for using LLMs in online discourse.

**Result:** Participants found that LLM-generated comments were more constructive and positively framed compared to independent human comments, although there were misalignments in the understanding of constructiveness between LLMs and humans.

**Limitations:** LLMs occasionally distorted participants' original views, particularly on non-polarizing stances.

**Conclusion:** While LLMs can enhance the constructiveness of online comments, their tendency to distort original views needs consideration for ethical and design purposes.

**Abstract:** This paper examines if large language models (LLMs) can help people write constructive comments on divisive social issues due to the difficulty of expressing constructive disagreement online. Through controlled experiments with 600 participants from India and the US, who reviewed and wrote constructive comments on threads related to Islamophobia and homophobia, we observed potential misalignment between how LLMs and humans perceive constructiveness in online comments. While the LLM was more likely to prioritize politeness and balance among contrasting viewpoints when evaluating constructiveness, participants emphasized logic and facts more than the LLM did. Despite these differences, participants rated both LLM-generated and human-AI co-written comments as significantly more constructive than those written independently by humans. Our analysis also revealed that LLM-generated comments integrated significantly more linguistic features of constructiveness compared to human-written comments. When participants used LLMs to refine their comments, the resulting comments were more constructive, more positive, less toxic, and retained the original intent. However, occasionally LLMs distorted people's original views -- especially when their stances were not outright polarizing. Based on these findings, we discuss ethical and design considerations in using LLMs to facilitate constructive discourse online.

</details>


### [15] [Towards spatial computing: recent advances in multimodal natural interaction for XR headsets](https://arxiv.org/abs/2502.07598)

*Zhimin Wang, Maohang Rao, Shanghua Ye, Weitao Song, Feng Lu*

**Main category:** cs.HC

**Keywords:** Extended Reality, natural interaction, AI, multimodal interaction, spatial computing

**Relevance Score:** 8

**TL;DR:** This paper reviews recent advancements in multimodal natural interaction techniques for wearable Extended Reality (XR) technology, highlighting AI and LLM's role in enhancing human-computer interaction.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to understand how recent advancements in natural interaction, particularly involving AI and LLMs, influence the design of interaction systems in XR.

**Method:** The authors reviewed papers published between 2022 and 2024 across six top venues, classifying the studies based on application scenarios, operation types, and interaction modalities.

**Key Contributions:**

	1. Comprehensive review of multimodal interaction techniques in XR focused on recent AI advancements
	2. Classification framework for understanding interaction modalities and application scenarios
	3. Identification of future research challenges and directions in natural interaction for XR

**Result:** The review identifies key trends in natural interaction techniques and presents a structured framework for future research directions in XR.

**Limitations:** 

**Conclusion:** The findings underscore the importance of addressing challenges in natural interaction techniques, with implications for the advancement of spatial computing.

**Abstract:** With the widespread adoption of Extended Reality (XR) headsets, spatial computing technologies are gaining increasing attention. Spatial computing enables interaction with virtual elements through natural input methods such as eye tracking, hand gestures, and voice commands, thus placing natural human-computer interaction at its core. While previous surveys have reviewed conventional XR interaction techniques, recent advancements in natural interaction, particularly driven by artificial intelligence (AI) and large language models (LLMs), have introduced new paradigms and technologies. In this paper, we review research on multimodal natural interaction for wearable XR, focusing on papers published between 2022 and 2024 in six top venues: ACM CHI, UIST, IMWUT (Ubicomp), IEEE VR, ISMAR, and TVCG. We classify and analyze these studies based on application scenarios, operation types, and interaction modalities. This analysis provides a structured framework for understanding how researchers are designing advanced natural interaction techniques in XR. Based on these findings, we discuss the challenges in natural interaction techniques and suggest potential directions for future research. This review provides valuable insights for researchers aiming to design natural and efficient interaction systems for XR, ultimately contributing to the advancement of spatial computing.

</details>


### [16] ["It's not a representation of me": Examining Accent Bias and Digital Exclusion in Synthetic AI Voice Services](https://arxiv.org/abs/2504.09346)

*Shira Michel, Sufi Kaur, Sarah Elizabeth Gillespie, Jeffrey Gleason, Christo Wilson, Avijit Ghosh*

**Main category:** cs.HC

**Keywords:** AI speech generation, voice cloning, linguistic privilege, accent discrimination, inclusive design

**Relevance Score:** 8

**TL;DR:** This study evaluates AI speech generation technologies and their socio-technical effects on users' perceptions of accent variations, revealing technical disparities and advocating for inclusive design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of AI speech generation on sociotechnical systems and address issues of linguistic privilege and accent discrimination.

**Method:** A mixed methods approach using surveys and interviews to assess the technical performance of two AI voice services and user experiences.

**Key Contributions:**

	1. Evaluation of technical performance of AI speech technologies across various accents.
	2. Insights into user perceptions of accent variations and their impacts on digital inclusion.
	3. Recommendations for inclusive design and regulation of AI speech technologies.

**Result:** Findings indicate disparities in the technical performance of AI speech technologies across different English accents and highlight issues of digital exclusion related to accent variations.

**Limitations:** The focus is limited to only two AI voice services and may not fully capture all accents or technologies available in the market.

**Conclusion:** The study underscores the importance of inclusive design in AI speech technologies to avoid exacerbating linguistic privilege and discrimination.

**Abstract:** Recent advances in artificial intelligence (AI) speech generation and voice cloning technologies have produced naturalistic speech and accurate voice replication, yet their influence on sociotechnical systems across diverse accents and linguistic traits is not fully understood. This study evaluates two synthetic AI voice services (Speechify and ElevenLabs) through a mixed methods approach using surveys and interviews to assess technical performance and uncover how users' lived experiences influence their perceptions of accent variations in these speech technologies. Our findings reveal technical performance disparities across five regional, English-language accents and demonstrate how current speech generation technologies may inadvertently reinforce linguistic privilege and accent-based discrimination, potentially creating new forms of digital exclusion. Overall, our study highlights the need for inclusive design and regulation by providing actionable insights for developers, policymakers, and organizations to ensure equitable and socially responsible AI speech technologies.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [TeleEval-OS: Performance evaluations of large language models for operations scheduling](https://arxiv.org/abs/2506.11017)

*Yanyan Wang, Yingying Wang, Junli Liang, Yin Xu, Yunlong Liu, Yiming Xu, Zhengwang Jiang, Zhehe Li, Fei Li, Long Zhao, Kuang Xu, Qi Song, Xiangyang Li*

**Main category:** cs.CL

**Keywords:** large language models, telecommunications operation scheduling, evaluation benchmark, TeleEval-OS, NLP

**Relevance Score:** 6

**TL;DR:** This paper introduces TeleEval-OS, a benchmark for evaluating large language models in telecommunications operation scheduling tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the research gap in evaluating large language models' application potential in telecommunications operation scheduling due to complexities and lack of benchmarks.

**Method:** The authors propose the Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS) comprising 15 datasets across 13 subtasks to assess LLM performance using zero-shot and few-shot evaluation methods.

**Key Contributions:**

	1. Introduction of the first benchmark for telecommunications operation scheduling (TeleEval-OS).
	2. Categorization of task complexities for LLM evaluations in this domain.
	3. Demonstration of the performance advantage of open-source LLMs over closed-source ones in specific scenarios.

**Result:** Experimental results indicate that open-source LLMs can outperform closed-source models in specific scenarios within telecommunications operation scheduling.

**Limitations:** 

**Conclusion:** The proposed benchmark enables a systematic evaluation of LLMs, revealing the potential of open-source models in effective telecommunications operation scheduling.

**Abstract:** The rapid advancement of large language models (LLMs) has significantly propelled progress in artificial intelligence, demonstrating substantial application potential across multiple specialized domains. Telecommunications operation scheduling (OS) is a critical aspect of the telecommunications industry, involving the coordinated management of networks, services, risks, and human resources to optimize production scheduling and ensure unified service control. However, the inherent complexity and domain-specific nature of OS tasks, coupled with the absence of comprehensive evaluation benchmarks, have hindered thorough exploration of LLMs' application potential in this critical field. To address this research gap, we propose the first Telecommunications Operation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this benchmark comprises 15 datasets across 13 subtasks, comprehensively simulating four key operational stages: intelligent ticket creation, intelligent ticket handling, intelligent ticket closure, and intelligent evaluation. To systematically assess the performance of LLMs on tasks of varying complexity, we categorize their capabilities in telecommunications operation scheduling into four hierarchical levels, arranged in ascending order of difficulty: basic NLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we leverage zero-shot and few-shot evaluation methods to comprehensively assess 10 open-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o) across diverse scenarios. Experimental results demonstrate that open-source LLMs can outperform closed-source LLMs in specific scenarios, highlighting their significant potential and value in the field of telecommunications operation scheduling.

</details>


### [18] [Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/2506.11063)

*Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** Multimodal RAG, Position Bias, Position Sensitivity Index, Evidence Position, Debiasing Strategies

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of the position of retrieved evidence on the performance of Multimodal Retrieval-Augmented Generation (RAG) systems, revealing significant position bias and introducing the Position Sensitivity Index (PSI_p).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the growing need for robust multimodal RAG systems in knowledge-intensive tasks, highlighting the issues of sensitivity to evidence order and performance instability as evidence complexity increases.

**Method:** The study employs controlled experiments across text-only, image-only, and mixed-modality tasks to analyze the effects of evidence position on performance, introducing the Position Sensitivity Index to quantify bias.

**Key Contributions:**

	1. First comprehensive study of position bias in multimodal RAG systems
	2. Introduction of Position Sensitivity Index (PSI_p)
	3. Development of a visualization framework for attention patterns across decoder layers

**Result:** The findings indicate a U-shaped accuracy curve concerning evidence position, showing that position bias is more pronounced in multimodal interactions and increases logarithmically with the retrieval range.

**Limitations:** The study primarily focuses on experimental settings and may require further validation in real-world applications.

**Conclusion:** The results emphasize the necessity for developing strategies to reorder evidence or mitigate bias in RAG systems to enhance their reliability and fairness.

**Abstract:** Multimodal Retrieval-Augmented Generation (RAG) systems have become essential in knowledge-intensive and open-domain tasks. As retrieval complexity increases, ensuring the robustness of these systems is critical. However, current RAG models are highly sensitive to the order in which evidence is presented, often resulting in unstable performance and biased reasoning, particularly as the number of retrieved items or modality diversity grows. This raises a central question: How does the position of retrieved evidence affect multimodal RAG performance? To answer this, we present the first comprehensive study of position bias in multimodal RAG systems. Through controlled experiments across text-only, image-only, and mixed-modality tasks, we observe a consistent U-shaped accuracy curve with respect to evidence position. To quantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and develop a visualization framework to trace attention allocation patterns across decoder layers. Our results reveal that multimodal interactions intensify position bias compared to unimodal settings, and that this bias increases logarithmically with retrieval range. These findings offer both theoretical and empirical foundations for position-aware analysis in RAG, highlighting the need for evidence reordering or debiasing strategies to build more reliable and equitable generation systems.

</details>


### [19] [Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study](https://arxiv.org/abs/2506.11065)

*Alexey Tikhonov, Sergei Shteiner, Anna Bykova, Ivan P. Yamshchikov*

**Main category:** cs.CL

**Keywords:** Russenorsk, pidgin language, large language models, lexicon analysis, translation agent

**Relevance Score:** 2

**TL;DR:** This paper analyzes the lexicon of the pidgin language Russenorsk using large language models, creating a structured dictionary and formulating hypotheses about its word formation and grammatical structure.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the linguistic structure and principles of word formation in the pidgin language Russenorsk through contemporary NLP techniques.

**Method:** The authors constructed a structured dictionary of Russenorsk based on historical sources and applied LLMs to formulate and test hypotheses regarding its linguistic characteristics.

**Key Contributions:**

	1. Creation of a structured dictionary of Russenorsk
	2. Insights into word formation and grammatical structure using LLMs
	3. Development of a translation agent for Russenorsk renderings.

**Result:** The analysis revealed correspondences between the hypotheses generated by LLMs and those proposed in existing literature, providing insights into the language's structure.

**Limitations:** The analysis is based on surviving literary sources, which may not fully represent the language's historical usage.

**Conclusion:** The study enhances understanding of Russenorsk and demonstrates the effectiveness of LLMs in analyzing historical and lesser-known languages.

**Abstract:** Russenorsk, a pidgin language historically used in trade interactions between Russian and Norwegian speakers, represents a unique linguistic phenomenon. In this paper, we attempt to analyze its lexicon using modern large language models (LLMs), based on surviving literary sources. We construct a structured dictionary of the language, grouped by synonyms and word origins. Subsequently, we use this dictionary to formulate hypotheses about the core principles of word formation and grammatical structure in Russenorsk and show which hypotheses generated by large language models correspond to the hypotheses previously proposed ones in the academic literature. We also develop a "reconstruction" translation agent that generates hypothetical Russenorsk renderings of contemporary Russian and Norwegian texts.

</details>


### [20] [A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes](https://arxiv.org/abs/2506.11067)

*Hieu Nghiem, Hemanth Reddy Singareddy, Zhuqi Miao, Jivan Lamichhane, Abdulaziz Ahmed, Johnson Thomas, Dursun Delen, William Paiva*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Language Models, Healthcare, Clinical NLP, Cost-effective Solutions

**Relevance Score:** 9

**TL;DR:** Development of an LLM-based pipeline to extract Review of Systems entities from clinical notes to reduce documentation burden in healthcare.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a cost-effective solution for extracting Review of Systems entities from clinical notes, addressing the documentation burden in healthcare.

**Method:** The pipeline uses SecTag to extract ROS sections, followed by few-shot LLMs for identifying entity spans, statuses, and associated body systems. Open-source LLMs (Mistral, Llama, Gemma) and ChatGPT were utilized and evaluated on 36 general medicine notes with 341 annotated entities.

**Key Contributions:**

	1. Cost-effective LLM-based pipeline for ROS extraction
	2. Demonstrated high performance with open-source LLMs
	3. Reduced documentation burden in healthcare settings

**Result:** The integration of ChatGPT led to the lowest error rates in detecting ROS entities and their statuses, with open-source LLMs achieving promising performance.

**Limitations:** Limited to evaluation on 36 general medicine notes; results may vary with different clinical contexts.

**Conclusion:** The proposed pipeline is scalable and can be locally deployed in resource-limited environments, providing a viable alternative to commercial models.

**Abstract:** Objective: Develop a cost-effective, large language model (LLM)-based pipeline for automatically extracting Review of Systems (ROS) entities from clinical notes. Materials and Methods: The pipeline extracts ROS sections using SecTag, followed by few-shot LLMs to identify ROS entity spans, their positive/negative status, and associated body systems. We implemented the pipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The evaluation was conducted on 36 general medicine notes containing 341 annotated ROS entities. Results: When integrating ChatGPT, the pipeline achieved the lowest error rates in detecting ROS entity spans and their corresponding statuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable local, cost-efficient execution of the pipeline while delivering promising performance with similarly low error rates (span: 30.5-36.7%; status/system: 24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and locally deployable solution to reduce ROS documentation burden. Open-source LLMs present a viable alternative to commercial models in resource-limited healthcare environments.

</details>


### [21] [Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models](https://arxiv.org/abs/2506.11068)

*Bumjin Park, Jinsil Lee, Jaesik Choi*

**Main category:** cs.CL

**Keywords:** large language models, moral reasoning, deontological keyword bias, AI alignment, linguistic framing

**Relevance Score:** 9

**TL;DR:** This paper explores how large language models (LLMs) misjudge non-obligatory contexts as obligations due to the influence of modal expressions, introducing the concept of Deontological Keyword Bias (DKB) and proposing a strategy to mitigate it.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how large language models (LLMs) make judgments about obligations and to address the underexplored area of moral and ethical reasoning in AI.

**Method:** Empirical analysis of LLM responses to various prompts augmented with modal expressions; introduction of a judgment strategy using few-shot examples and reasoning prompts.

**Key Contributions:**

	1. Introduction of Deontological Keyword Bias (DKB) in LLMs.
	2. Empirical evidence showing LLMs' tendency to misjudge obligations based on prompt wording.
	3. Development of a judgment strategy to mitigate DKB.

**Result:** Over 90% of commonsense scenarios were judged as obligations when modal expressions like 'must' or 'ought to' were present, consistent across different models and question formats.

**Limitations:** 

**Conclusion:** The study highlights the substantial impact of modal expressions on LLMs' judgment processes, emphasizing the need to mitigate biases for better alignment in normative decision-making.

**Abstract:** Large language models (LLMs) are increasingly engaging in moral and ethical reasoning, where criteria for judgment are often unclear, even for humans. While LLM alignment studies cover many areas, one important yet underexplored area is how LLMs make judgments about obligations. This work reveals a strong tendency in LLMs to judge non-obligatory contexts as obligations when prompts are augmented with modal expressions such as must or ought to. We introduce this phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge over 90\% of commonsense scenarios as obligations when modal expressions are present. This tendency is consist across various LLM families, question types, and answer formats. To mitigate DKB, we propose a judgment strategy that integrates few-shot examples with reasoning prompts. This study sheds light on how modal expressions, as a form of linguistic framing, influence the normative decisions of LLMs and underscores the importance of addressing such biases to ensure judgment alignment.

</details>


### [22] [Targeted control of fast prototyping through domain-specific interface](https://arxiv.org/abs/2506.11070)

*Yu-Zhe Shi, Mingchen Liu, Hanlu Ma, Qiao Xu, Huamin Qu, Kun He, Lecheng Ruan, Qining Wang*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Large Language Models, Prototype Control, Design Interfaces, Fast Prototyping

**Relevance Score:** 7

**TL;DR:** The paper presents an interface architecture that allows industrial designers to control prototype models using natural language, mitigating the current gaps between designers' and modeling languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To achieve intuitive control of prototype models using natural language instructions, addressing limitations in current modeling languages and practices.

**Method:** The authors propose an interface architecture that serves as a bridge between natural language and modeling languages, and develop an algorithm for automated domain specification based on design principles from fast prototyping practices.

**Key Contributions:**

	1. Proposed interface architecture for bridging language gaps in design and modeling.
	2. Developed an algorithm for automated domain specification.
	3. Demonstrated effective integration with Large Language Models through evaluations.

**Result:** Machine-based evaluations and human studies indicate that the proposed interface effectively allows targeted control of prototype models through Large Language Models, enhancing their usability in design contexts.

**Limitations:** The study may need further validation across more design domains and contexts.

**Conclusion:** The interface can act as an auxiliary module for LLMs, facilitating precise adjustments to prototypes as intended by designers.

**Abstract:** Industrial designers have long sought a natural and intuitive way to achieve the targeted control of prototype models -- using simple natural language instructions to configure and adjust the models seamlessly according to their intentions, without relying on complex modeling commands. While Large Language Models have shown promise in this area, their potential for controlling prototype models through language remains partially underutilized. This limitation stems from gaps between designers' languages and modeling languages, including mismatch in abstraction levels, fluctuation in semantic precision, and divergence in lexical scopes. To bridge these gaps, we propose an interface architecture that serves as a medium between the two languages. Grounded in design principles derived from a systematic investigation of fast prototyping practices, we devise the interface's operational mechanism and develop an algorithm for its automated domain specification. Both machine-based evaluations and human studies on fast prototyping across various product design domains demonstrate the interface's potential to function as an auxiliary module for Large Language Models, enabling precise and effective targeted control of prototype models.

</details>


### [23] [CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention](https://arxiv.org/abs/2506.11073)

*Zekai Ye, Qiming Li, Xiaocheng Feng, Libo Qin, Yichong Huang, Baohang Li, Kui Jiang, Yang Xiang, Zhirui Zhang, Yunfei Lu, Duyu Tang, Dandan Tu, Bing Qin*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, multilingual object hallucination, cross-modal attention, CLAIM, attention intervention

**Relevance Score:** 7

**TL;DR:** The paper introduces CLAIM, a new method to reduce multilingual object hallucination in LVLMs by aligning cross-modal attention patterns, achieving significant performance improvements with minimal resource usage.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of multilingual object hallucination in Large Vision-Language Models, especially with non-English queries, without the resource demands of pretraining or fine-tuning.

**Method:** CLAIM identifies language-specific cross-modal attention heads and estimates language shift vectors from English to the target language, intervening during inference to align attention outputs.

**Key Contributions:**

	1. Introduces CLAIM for addressing multilingual object hallucination in LVLMs.
	2. Demonstrates performance improvements without extensive resource expenditure.
	3. Highlights the critical role of intermediate layers in multilingual attention divergence.

**Result:** CLAIM achieved an average improvement of 13.56% on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages, with peak improvements of up to 30% in Spanish.

**Limitations:** The approach may still be influenced by the inherent limitations of the existing LVLMs and the extent of language diversity.

**Conclusion:** The study underscores the importance of multilingual attention divergence in intermediate layers and presents CLAIM as an effective near training-free solution for improved cross-lingual visual perception.

**Abstract:** Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal abilities but remain prone to multilingual object hallucination, with a higher likelihood of generating responses inconsistent with the visual input when utilizing queries in non-English languages compared to English. Most existing approaches to address these rely on pretraining or fine-tuning, which are resource-intensive. In this paper, inspired by observing the disparities in cross-modal attention patterns across languages, we propose Cross-Lingual Attention Intervention for Mitigating multilingual object hallucination (CLAIM) in LVLMs, a novel near training-free method by aligning attention patterns. CLAIM first identifies language-specific cross-modal attention heads, then estimates language shift vectors from English to the target language, and finally intervenes in the attention outputs during inference to facilitate cross-lingual visual perception capability alignment. Extensive experiments demonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in Spanish) on the POPE and 21.75% on the hallucination subsets of the MME benchmark across various languages. Further analysis reveals that multilingual attention divergence is most prominent in intermediate layers, highlighting their critical role in multilingual scenarios.

</details>


### [24] [CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling](https://arxiv.org/abs/2506.11077)

*Chongyu Fan, Yihua Zhang, Jinghan Jia, Alfred Hero, Sijia Liu*

**Main category:** cs.CL

**Keywords:** Large reasoning models, Reflection tokens, Cyclical scheduling, Machine learning, Performance improvement

**Relevance Score:** 7

**TL;DR:** Introduction of CyclicReflex, a decoding strategy that optimally modulates reflection tokens to improve performance in large reasoning models during complex problem-solving tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance test-time compute performance of large reasoning models (LRMs) by managing the use of reflection tokens, which are crucial for multi-step reasoning and self-evaluation.

**Method:** The paper proposes cyclical reflection token scheduling (CyclicReflex) that dynamically adjusts the frequency and position of reflection tokens using a triangular waveform to prevent over-reflection and under-reflection.

**Key Contributions:**

	1. Introduction of reflection tokens as a resource in reasoning models
	2. Proposed CyclicReflex for dynamic modulation of reflection tokens
	3. Empirical evidence demonstrating performance improvements across multiple benchmarks

**Result:** CyclicReflex consistently outperforms standard decoding methods and recent approaches like TIP and S1 on benchmarks MATH500, AIME2024/2025, and AMC2023, leading to improved model performance across varying model sizes.

**Limitations:** 

**Conclusion:** CyclicReflex allows for better allocation of reflection tokens, resulting in enhanced reasoning capabilities of large models. The methodology can be beneficial for future research in improving reasoning models.

**Abstract:** Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness test-time scaling to perform multi-step reasoning for complex problem-solving. This reasoning process, executed before producing final answers, is often guided by special juncture tokens or textual segments that prompt self-evaluative reflection. We refer to these transition markers and reflective cues as "reflection tokens" (e.g., "wait", "but", "alternatively"). In this work, we treat reflection tokens as a "resource" and introduce the problem of resource allocation, aimed at improving the test-time compute performance of LRMs by adaptively regulating the frequency and placement of reflection tokens. Through empirical analysis, we show that both excessive and insufficient use of reflection tokens, referred to as over-reflection and under-reflection, can degrade model performance. To better understand and manage this trade-off, we draw an analogy between reflection token usage and learning rate scheduling in optimization. Building on this insight, we propose cyclical reflection token scheduling (termed CyclicReflex), a decoding strategy that dynamically modulates reflection token logits using a position-dependent triangular waveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that CyclicReflex consistently improves performance across model sizes (1.5B-8B), outperforming standard decoding and more recent approaches such as TIP (thought switching penalty) and S1. Codes are available at https://github.com/OPTML-Group/CyclicReflex.

</details>


### [25] [RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs](https://arxiv.org/abs/2506.11078)

*Yuzhou Yang, Yangming Zhou, Zhiying Zhu, Zhenxing Qian, Xinpeng Zhang, Sheng Li*

**Main category:** cs.CL

**Keywords:** Fake News Detection, Large Language Models, Experiential Learning

**Relevance Score:** 8

**TL;DR:** RoE-FND is a framework for Fake News Detection that reinterprets it as a logical deduction task by combining LLMs with experiential learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of deceptive online content requires effective Fake News Detection (FND) systems, but current methods struggle with evidence selection, generalization, and decision-making clarity.

**Method:** RoE-FND includes two stages: self-reflective knowledge building for curating a knowledge base from past errors and dynamic criterion retrieval for synthesizing reasoning guidelines from historical cases during deployment.

**Key Contributions:**

	1. A case-based reasoning framework that addresses existing challenges in FND
	2. A training-free approach for adapting to evolving contexts
	3. Empirical validation showing superior performance over existing methods

**Result:** The framework exhibits superior generalization and effectiveness compared to state-of-the-art methods across three datasets.

**Limitations:** 

**Conclusion:** RoE-FND offers a case-based reasoning approach for FND that adapts to new situations and provides empirical validation for its methodologies.

**Abstract:** The proliferation of deceptive content online necessitates robust Fake News Detection (FND) systems. While evidence-based approaches leverage external knowledge to verify claims, existing methods face critical limitations: noisy evidence selection, generalization bottlenecks, and unclear decision-making processes. Recent efforts to harness Large Language Models (LLMs) for FND introduce new challenges, including hallucinated rationales and conclusion bias. To address these issues, we propose \textbf{RoE-FND} (\textbf{\underline{R}}eason \textbf{\underline{o}}n \textbf{\underline{E}}xperiences FND), a framework that reframes evidence-based FND as a logical deduction task by synergizing LLMs with experiential learning. RoE-FND encompasses two stages: (1) \textit{self-reflective knowledge building}, where a knowledge base is curated by analyzing past reasoning errors, namely the exploration stage, and (2) \textit{dynamic criterion retrieval}, which synthesizes task-specific reasoning guidelines from historical cases as experiences during deployment. It further cross-checks rationales against internal experience through a devised dual-channel procedure. Key contributions include: a case-based reasoning framework for FND that addresses multiple existing challenges, a training-free approach enabling adaptation to evolving situations, and empirical validation of the framework's superior generalization and effectiveness over state-of-the-art methods across three datasets.

</details>


### [26] [MANBench: Is Your Multimodal Model Smarter than Human?](https://arxiv.org/abs/2506.11080)

*Han Zhou, Qitong Xu, Yiheng Dong, Xin Yang*

**Main category:** cs.CL

**Keywords:** Multimodal Learning, Large Language Models, Benchmarking, Human-Computer Interaction, Cross-modal reasoning

**Relevance Score:** 9

**TL;DR:** MANBench introduces a bilingual benchmark for evaluating Multimodal Large Language Models (MLLMs) across various tasks, revealing their strengths and limitations compared to human performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the potential of MLLMs against human performance in multimodal tasks using a comprehensive benchmark that emphasizes intuitive reasoning and real-world complexities.

**Method:** A bilingual benchmark, MANBench, was created with 1,314 questions across nine tasks, involving extensive human experiments to compare human and MLLM performance on knowledge-based and non-knowledge-based domains.

**Key Contributions:**

	1. Introduction of MANBench for evaluating MLLMs
	2. Empirical findings on the strengths and weaknesses of MLLMs
	3. Availability of the code and dataset for community use

**Result:** MLLMs performed well in tasks like Knowledge and Text-Image Understanding but struggled with complex cross-modal reasoning tasks, showing gaps compared to human abilities.

**Limitations:** N/A

**Conclusion:** MANBench highlights that while MLLMs excel in certain areas, they do not yet reach human-level performance in many complex tasks, encouraging further research to improve MLLM capabilities.

**Abstract:** The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited discussions regarding their potential to surpass human performance in multimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms Benchmark), a bilingual benchmark (English and Chinese) comprising 1,314 questions across nine tasks, spanning knowledge-based and non-knowledge-based domains. MANBench emphasizes intuitive reasoning, seamless cross-modal integration, and real-world complexity, providing a rigorous evaluation framework.   Through extensive human experiments involving diverse participants, we compared human performance against state-of-the-art MLLMs. The results indicate that while MLLMs excel in tasks like Knowledge and Text-Image Understanding, they struggle with deeper cross-modal reasoning tasks such as Transmorphic Understanding, Image Consistency, and Multi-image Understanding. Moreover, both humans and MLLMs face challenges in highly complex tasks like Puzzles and Spatial Imagination.   MANBench highlights the strengths and limitations of MLLMs, revealing that even advanced models fall short of achieving human-level performance across many domains. We hope MANBench will inspire efforts to bridge the gap between MLLMs and human multimodal capabilities. The code and dataset are available at https://github.com/micdz/MANBench.

</details>


### [27] [SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs](https://arxiv.org/abs/2506.11081)

*Aditi, Hyunwoo Park, Sicheol Sung, Yo-Sub Han, Sang-Ki Ko*

**Main category:** cs.CL

**Keywords:** Grammar-based test case generation, Context-Free Grammars, Large Language Models, Reinforcement Learning

**Relevance Score:** 6

**TL;DR:** This paper presents an approach to generate Context-Free Grammars with Counters (CCFGs) from natural language specifications using large language models (LLMs) and reinforcement learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Generating valid and general grammars from natural language specifications is challenging, particularly with limited supervision.

**Method:** The proposed method fine-tunes an open-source LLM for specification-to-grammar translation and applies Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality.

**Key Contributions:**

	1. Induction of CCFGs from natural language specifications using LLMs
	2. Use of reinforcement learning for grammar validation
	3. Improved generalization and performance compared to existing methods

**Result:** SAGE, the proposed approach, outperforms 17 other LLMs in grammar quality and test effectiveness, achieving improvements of 15.92% in grammar validity and 12.34% in test effectiveness over the state-of-the-art.

**Limitations:** 

**Conclusion:** The experimental results demonstrate that the SAGE approach effectively induces CCFGs with improved performance metrics.

**Abstract:** Grammar-based test case generation has proven effective for competitive programming problems, but generating valid and general grammars from natural language specifications remains a key challenge, especially under limited supervision. Context-Free Grammars with Counters (CCFGs) have recently been introduced as a formalism to represent such specifications with logical constraints by storing and reusing counter values during derivation. In this work, we explore the use of open-source large language models (LLMs) to induce CCFGs from specifications using a small number of labeled examples and verifiable reward-guided reinforcement learning. Our approach first fine-tunes an open-source LLM to perform specification-to-grammar translation, and further applies Group Relative Policy Optimization (GRPO) to enhance grammar validity and generality. We also examine the effectiveness of iterative feedback for open and closed-source LLMs in correcting syntactic and semantic errors in generated grammars.   Experimental results show that our approach SAGE achieves stronger generalization and outperforms 17 open and closed-source LLMs in both grammar quality and test effectiveness, improving over the state-of-the-art by 15.92%p in grammar validity and 12.34%p in test effectiveness. We provide our implementation and dataset at the following anonymous repository:https://anonymous.4open.science/r/SAGE-5714

</details>


### [28] [PRISM: A Transformer-based Language Model of Structured Clinical Event Data](https://arxiv.org/abs/2506.11082)

*Lionel Levine, John Santerre, Alex S. Young, T. Barry Levine, Francis Campion, Majid Sarrafzadeh*

**Main category:** cs.CL

**Keywords:** clinical decision-making, transformer architecture, predictive modeling, healthcare applications, machine learning

**Relevance Score:** 9

**TL;DR:** PRISM is a transformer-based architecture for modeling clinical decision-making by predicting sequences of medical events.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve clinical decision-making processes by predicting the next steps in patient diagnostics using a sequence-based approach.

**Method:** PRISM uses a transformer architecture to model tokenized clinical event sequences and applies autoregressive training to forecast next diagnostic actions.

**Key Contributions:**

	1. Introduction of PRISM architecture for sequential clinical reasoning
	2. Application of autoregressive training on medical event data
	3. Demonstrated improvements in modeling clinical decision pathways.

**Result:** PRISM shows significant improvements in next-token prediction tasks, accurately reflecting real-world diagnostic pathways and clinician behaviors.

**Limitations:** The model may require extensive customization for different clinical contexts and interactions with real-world data.

**Conclusion:** The framework paves the way for integrating generative modeling techniques into healthcare, enhancing clinical decision support and education.

**Abstract:** We introduce PRISM (Predictive Reasoning in Sequential Medicine), a transformer-based architecture designed to model the sequential progression of clinical decision-making processes. Unlike traditional approaches that rely on isolated diagnostic classification, PRISM frames clinical trajectories as tokenized sequences of events - including diagnostic tests, laboratory results, and diagnoses - and learns to predict the most probable next steps in the patient diagnostic journey. Leveraging a large custom clinical vocabulary and an autoregressive training objective, PRISM demonstrates the ability to capture complex dependencies across longitudinal patient timelines. Experimental results show substantial improvements over random baselines in next-token prediction tasks, with generated sequences reflecting realistic diagnostic pathways, laboratory result progressions, and clinician ordering behaviors. These findings highlight the feasibility of applying generative language modeling techniques to structured medical event data, enabling applications in clinical decision support, simulation, and education. PRISM establishes a foundation for future advancements in sequence-based healthcare modeling, bridging the gap between machine learning architectures and real-world diagnostic reasoning.

</details>


### [29] [Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation](https://arxiv.org/abs/2506.11092)

*Jubin Abhishek Soni, Amit Anand, Rajesh Kumar Pandey, Aniket Abhishek Soni*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Dynamic Context Tuning, Machine Learning, Human-Computer Interaction, AI in Healthcare

**Relevance Score:** 9

**TL;DR:** Dynamic Context Tuning (DCT) enhances Retrieval-Augmented Generation (RAG) to support multi-turn dialogue and evolving tool environments, showing improved accuracy and reduced hallucinations in AI applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing RAG systems are limited to static interactions, making them unsuitable for dynamic domains like healthcare and smart homes where user intent and tool availability change over time.

**Method:** DCT integrates an attention-based context cache for tracking relevant past information, employs LoRA-based retrieval for dynamic tool selection, and uses efficient context compression to stay within LLM limits.

**Key Contributions:**

	1. Introduction of Dynamic Context Tuning (DCT) for evolving contexts in RAG.
	2. Demonstration of improved plan accuracy and reduced hallucinations.
	3. Capability to generalize to previously unseen tools.

**Result:** DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while achieving performance that matches GPT-4 at a lower cost.

**Limitations:** 

**Conclusion:** DCT generalizes to unseen tools, enabling scalable and adaptable AI assistants across various dynamic environments.

**Abstract:** Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.

</details>


### [30] [RedDebate: Safer Responses through Multi-Agent Red Teaming Debates](https://arxiv.org/abs/2506.11083)

*Ali Asad, Stephen Obadinma, Radin Shayanfar, Xiaodan Zhu*

**Main category:** cs.CL

**Keywords:** AI safety, Large Language Models, multi-agent systems, adversarial argumentation, machine learning

**Relevance Score:** 9

**TL;DR:** RedDebate is a multi-agent debate framework using LLMs to enhance AI safety by identifying unsafe behaviors through adversarial argumentation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing AI safety methods that rely on human evaluations and single-model assessments, which are not scalable and are prone to oversight.

**Method:** The proposed framework uses collaborative disagreement among multiple LLMs to critically examine each other's reasoning, uncover blind spots, and improve their responses. Long-term memory is integrated to retain safety insights from these debates.

**Key Contributions:**

	1. Novel multi-agent debate framework for LLMs
	2. First automated method combining debates with red-teaming
	3. Integration of long-term memory for improved safety insights

**Result:** RedDebate effectively reduces unsafe behaviors in AI models, achieving a 17.7% reduction through debate alone, and exceeding 23.5% when combined with long-term memory modules on safety benchmarks like HarmBench.

**Limitations:** 

**Conclusion:** RedDebate is the first fully automated framework combining multi-agent debates and red-teaming to enhance AI safety without human intervention.

**Abstract:** We propose RedDebate, a novel multi-agent debate framework that leverages adversarial argumentation among Large Language Models (LLMs) to proactively identify and mitigate their own unsafe behaviours. Existing AI safety methods often depend heavily on costly human evaluations or isolated single-model assessment, both subject to scalability constraints and oversight risks. RedDebate instead embraces collaborative disagreement, enabling multiple LLMs to critically examine one another's reasoning, and systematically uncovering unsafe blind spots through automated red-teaming, and iteratively improve their responses. We further integrate distinct types of long-term memory that retain learned safety insights from debate interactions. Evaluating on established safety benchmarks such as HarmBench, we demonstrate the proposed method's effectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when combined with long-term memory modules, achieves reductions exceeding 23.5%. To our knowledge, RedDebate constitutes the first fully automated framework that combines multi-agent debates with red-teaming to progressively enhance AI safety without direct human intervention.(Github Repository: https://github.com/aliasad059/RedDebate)

</details>


### [31] [Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)](https://arxiv.org/abs/2506.11112)

*Christine Bauer, Li Chen, Nicola Ferro, Norbert Fuhr, Avishek Anand, Timo Breuer, Guglielmo Faggioli, Ophir Frieder, Hideo Joho, Jussi Karlgren, Johannes Kiesel, Bart P. Knijnenburg, Aldo Lipani, Lien Michiels, Andrea Papenmeier, Maria Soledad Pera, Mark Sanderson, Scott Sanner, Benno Stein, Johanne R. Trippas, Karin Verspoor, Martijn C Willemsen*

**Main category:** cs.CL

**Keywords:** CONIAC, Conversational Agents, Evaluation Framework, User Tasks, Human-Computer Interaction

**Relevance Score:** 6

**TL;DR:** This paper defines and evaluates the concept of Conversational Information Access (CONIAC) through a proposed framework.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a structured evaluation framework to assess the effectiveness of CONIAC systems.

**Method:** The authors developed the Conversational Agents Framework for Evaluation (CAFE), which focuses on six components essential for evaluating CONIAC systems.

**Key Contributions:**

	1. Introduction of the CAFE framework for CONIAC evaluation
	2. Identification of key evaluation components
	3. Discussion on stakeholder goals and user interactions

**Result:** CAFE provides a comprehensive guideline for evaluating user interactions in CONIAC, addressing stakeholders' goals and user tasks.

**Limitations:** 

**Conclusion:** Effective evaluation of CONIAC systems can be achieved through the CAFE framework, aiding in the design of better conversational agents.

**Abstract:** During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.

</details>


### [32] [Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing](https://arxiv.org/abs/2506.11088)

*Pengbo Wang, Chaozhuo Li, Chenxu Wang, Liwen Zheng, Litian Zhang, Xi Zhang*

**Main category:** cs.CL

**Keywords:** LLMs, factuality, faithfulness, hallucinations, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces SPACE, a unified framework for mitigating factuality and faithfulness hallucinations in LLMs by editing shared activation subspaces.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The practical deployment of LLMs is hindered by hallucinations related to factuality and faithfulness, which current methods address independently, leading to performance trade-offs.

**Method:** SPACE employs a dual-task feature modeling approach to establish a geometric foundation for shared subspace existence and uses a hybrid probe strategy that combines spectral clustering and attention head saliency scoring to edit these subspaces.

**Key Contributions:**

	1. Introduces the SPACE framework for joint enhancement of factuality and faithfulness.
	2. Identifies overlapping subspaces in LLM activation dynamics.
	3. Employs a unique hybrid probe strategy for subspace editing.

**Result:** Experimental results show that SPACE enhances both factuality and faithfulness compared to existing methods across multiple benchmark datasets.

**Limitations:** 

**Conclusion:** SPACE offers a novel method of improving LLM performance by concurrently addressing factuality and faithfulness through a shared subspace editing approach.

**Abstract:** LLMs have demonstrated unprecedented capabilities in natural language processing, yet their practical deployment remains hindered by persistent factuality and faithfulness hallucinations. While existing methods address these hallucination types independently, they inadvertently induce performance trade-offs, as interventions targeting one type often exacerbate the other. Through empirical and theoretical analysis of activation space dynamics in LLMs, we reveal that these hallucination categories share overlapping subspaces within neural representations, presenting an opportunity for concurrent mitigation. To harness this insight, we propose SPACE, a unified framework that jointly enhances factuality and faithfulness by editing shared activation subspaces. SPACE establishes a geometric foundation for shared subspace existence through dual-task feature modeling, then identifies and edits these subspaces via a hybrid probe strategy combining spectral clustering and attention head saliency scoring. Experimental results across multiple benchmark datasets demonstrate the superiority of our approach.

</details>


### [33] [Customizing Speech Recognition Model with Large Language Model Feedback](https://arxiv.org/abs/2506.11091)

*Shaoshi Ling, Guoli Ye*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, domain adaptation, reinforcement learning, large language models, named entities

**Relevance Score:** 8

**TL;DR:** This paper proposes a reinforcement learning approach for unsupervised domain adaptation in ASR systems, improving transcription quality of rare named entities using feedback from large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the difficulties ASR systems face with rare named entities and domain mismatches, and improve their transcription performance.

**Method:** The proposed method uses a large language model (LLM) as a reward model to score entity hypotheses from an ASR system, applying reinforcement learning to fine-tune the ASR model based on these scores.

**Key Contributions:**

	1. Introduction of a reinforcement learning framework for ASR domain adaptation.
	2. Use of LLM as a reward model for scoring ASR hypotheses.
	3. Demonstrated improvement in entity recognition performance through unsupervised learning.

**Result:** The method achieves a 21% improvement in entity word error rate compared to conventional self-training methods.

**Limitations:** 

**Conclusion:** The integration of LLM feedback significantly enhances the transcription quality of ASR systems regarding domain-specific challenges.

**Abstract:** Automatic speech recognition (ASR) systems have achieved strong performance on general transcription tasks. However, they continue to struggle with recognizing rare named entities and adapting to domain mismatches. In contrast, large language models (LLMs), trained on massive internet-scale datasets, are often more effective across a wide range of domains. In this work, we propose a reinforcement learning based approach for unsupervised domain adaptation, leveraging unlabeled data to enhance transcription quality, particularly the named entities affected by domain mismatch, through feedback from a LLM. Given contextual information, our framework employs a LLM as the reward model to score the hypotheses from the ASR model. These scores serve as reward signals to fine-tune the ASR model via reinforcement learning. Our method achieves a 21\% improvement on entity word error rate over conventional self-training methods.

</details>


### [34] [Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation](https://arxiv.org/abs/2506.11092)

*Jubin Abhishek Soni, Amit Anand, Rajesh Kumar Pandey, Aniket Abhishek Soni*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Dynamic Context Tuning, Large Language Models, Healthcare, Multi-turn Dialogue

**Relevance Score:** 9

**TL;DR:** Dynamic Context Tuning (DCT) enhances Retrieval-Augmented Generation (RAG) for multi-turn dialogues in dynamic environments like healthcare, improving performance metrics significantly.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing RAG systems that are static and single-turn, particularly in dynamic domains.

**Method:** DCT integrates an attention-based context cache, LoRA-based retrieval for dynamic tool selection, and context compression to adapt to evolving dialogue and tool environments without retraining.

**Key Contributions:**

	1. Introduction of Dynamic Context Tuning (DCT) framework for RAG.
	2. Improvement in dialogue accuracy and reduction in hallucinations.
	3. Generalization to unseen tools in dynamic environments.

**Result:** DCT improves plan accuracy by 14% and reduces hallucinations by 37%, matching GPT-4 performance at a lower cost.

**Limitations:** 

**Conclusion:** DCT enables scalable and adaptable AI assistants by generalizing to new tools in dynamic contexts.

**Abstract:** Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.

</details>


### [35] [The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs](https://arxiv.org/abs/2506.11094)

*Songyang Liu, Chaozhuo Li, Jiameng Qiu, Xi Zhang, Feiran Huang, Litian Zhang, Yiming Hei, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety evaluation, Natural Language Processing, toxicity, bias

**Relevance Score:** 9

**TL;DR:** This survey provides a systematic overview of recent advancements in the safety evaluation of Large Language Models (LLMs), highlighting evaluation motivations, tasks, metrics, methods, and future research directions.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address significant safety concerns associated with LLM-generated content, including toxicity and bias, and to highlight the importance of evaluating their safety before real-world deployment.

**Method:** The paper categorizes safety evaluation tasks based on capabilities like toxicity, ethics, robustness, and bias; reviews existing evaluation metrics, datasets, and evaluation toolkits; and discusses the roles of evaluators in the process.

**Key Contributions:**

	1. Comprehensive categorization of safety evaluation tasks for LLMs
	2. Review of existing metrics and benchmarks used for LLM safety
	3. Identification of challenges and research directions for advancing LLM safety evaluation

**Result:** The survey outlines the current landscape of LLM safety evaluation, identifies existing challenges in the field, and suggests avenues for future research to enhance the evaluation process.

**Limitations:** The survey may not cover all recent advancements and might focus primarily on mainstream evaluation approaches.

**Conclusion:** Prioritizing safety evaluation is crucial for ensuring the responsible deployment of LLMs in various applications.

**Abstract:** With the rapid advancement of artificial intelligence technology, Large Language Models (LLMs) have demonstrated remarkable potential in the field of Natural Language Processing (NLP), including areas such as content generation, human-computer interaction, machine translation, and code generation, among others. However, their widespread deployment has also raised significant safety concerns. In recent years, LLM-generated content has occasionally exhibited unsafe elements like toxicity and bias, particularly in adversarial scenarios, which has garnered extensive attention from both academia and industry. While numerous efforts have been made to evaluate the safety risks associated with LLMs, there remains a lack of systematic reviews summarizing these research endeavors. This survey aims to provide a comprehensive and systematic overview of recent advancements in LLMs safety evaluation, focusing on several key aspects: (1) "Why evaluate" that explores the background of LLMs safety evaluation, how they differ from general LLMs evaluation, and the significance of such evaluation; (2) "What to evaluate" that examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and so on; (3) "Where to evaluate" that summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (4) "How to evaluate" that reviews existing evaluation toolkit, and categorizing mainstream evaluation methods based on the roles of the evaluators. Finally, we identify the challenges in LLMs safety evaluation and propose potential research directions to promote further advancement in this field. We emphasize the importance of prioritizing LLMs safety evaluation to ensure the safe deployment of these models in real-world applications.

</details>


### [36] [Persistent Homology of Topic Networks for the Prediction of Reader Curiosity](https://arxiv.org/abs/2506.11095)

*Manuel D. S. Hopp, Vincent Labatut, Arthur Amalvy, Richard Dufour, Hannah Stone, Hayley Jach, Kou Murayama*

**Main category:** cs.CL

**Keywords:** reader curiosity, information gap theory, topic modeling

**Relevance Score:** 6

**TL;DR:** The paper introduces a framework to model reader curiosity in text by quantifying semantic information gaps using topic modeling and topological analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Reader curiosity is crucial for textual engagement but is underexplored in NLP.

**Method:** The framework uses BERTopic-inspired topic modeling and persistent homology to analyze a dynamic semantic network derived from text segments.

**Key Contributions:**

	1. Introduction of a novel framework to model reader curiosity
	2. Use of topological features to quantify semantic information gaps
	3. Empirical validation of the approach through reader curiosity ratings

**Result:** The proposed method significantly improves curiosity prediction, achieving 73% explained deviance compared to a baseline of 30%.

**Limitations:** 

**Conclusion:** This approach provides a new computational method for analyzing text structure in relation to reader engagement.

**Abstract:** Reader curiosity, the drive to seek information, is crucial for textual engagement, yet remains relatively underexplored in NLP. Building on Loewenstein's Information Gap Theory, we introduce a framework that models reader curiosity by quantifying semantic information gaps within a text's semantic structure. Our approach leverages BERTopic-inspired topic modeling and persistent homology to analyze the evolving topology (connected components, cycles, voids) of a dynamic semantic network derived from text segments, treating these features as proxies for information gaps. To empirically evaluate this pipeline, we collect reader curiosity ratings from participants (n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the topological features from our pipeline as independent variables to predict these ratings, and experimentally show that they significantly improve curiosity prediction compared to a baseline model (73% vs. 30% explained deviance), validating our approach. This pipeline offers a new computational method for analyzing text structure and its relation to reader engagement.

</details>


### [37] [C-SEO Bench: Does Conversational SEO Work?](https://arxiv.org/abs/2506.11097)

*Haritz Puerto, Martin Gubri, Tommaso Green, Seong Joon Oh, Sangdoo Yun*

**Main category:** cs.CL

**Keywords:** Conversational Search, C-SEO, SEO, Benchmark, Search Engine Optimization

**Relevance Score:** 7

**TL;DR:** The paper introduces C-SEO Bench, a benchmark to evaluate Conversational Search Engine Optimization methods, revealing traditional SEO strategies are more effective.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of Conversational Search Engine Optimization (C-SEO) methods across multiple domains and tasks, as existing methods are limited in scope.

**Method:** C-SEO Bench benchmark was developed, which assesses C-SEO methods involving multiple search tasks (question answering and product recommendation) and domains, with a formal evaluation protocol considering various adoption rates.

**Key Contributions:**

	1. Introduction of C-SEO Bench as a new benchmark for evaluating C-SEO methods.
	2. Formalization of a new evaluation protocol for C-SEO.
	3. Demonstration of traditional SEO effectiveness over C-SEO methods.

**Result:** Experiments showed that most current C-SEO methods are largely ineffective, and traditional SEO techniques provided significantly better outcomes.

**Limitations:** Current evaluations do not consider long-term dynamics of C-SEO; only limited domains and tasks were assessed.

**Conclusion:** The study highlights the competitive nature of C-SEO adoption and suggests traditional SEO strategies outperform new C-SEO methods; code and data are made publicly available.

**Abstract:** Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not understand whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are largely ineffective, contrary to reported results in the literature. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at https://github.com/parameterlab/c-seo-bench and https://huggingface.co/datasets/parameterlab/c-seo-bench.

</details>


### [38] [Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey](https://arxiv.org/abs/2506.11102)

*Jiachen Zhu, Menghui Zhu, Renting Rui, Rong Shan, Congmin Zheng, Bo Chen, Yunjia Xi, Jianghao Lin, Weiwen Liu, Ruiming Tang, Yong Yu, Weinan Zhang*

**Main category:** cs.CL

**Keywords:** evaluation frameworks, AI agents, LLM chatbots, benchmarking, natural language processing

**Relevance Score:** 8

**TL;DR:** The paper analyzes evaluation frameworks for AI agents and LLM chatbots, introducing a detailed framework that clarifies their distinctions and categorizes existing benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the confusion among researchers regarding evaluation frameworks for LLM chatbots and AI agents.

**Method:** Systematic analysis of current evaluation approaches, differentiating AI agents from LLM chatbots based on five key aspects and categorizing benchmarks by internal capabilities.

**Key Contributions:**

	1. Systematic analysis of evaluation approaches for AI agents and LLM chatbots
	2. Comprehensive framework differentiating AI agents and LLM chatbots
	3. Categorization of evaluation benchmarks with relevant attributes

**Result:** Introduced a comprehensive analytical framework and categorized existing evaluation benchmarks, providing practical reference tables and guidance for researchers.

**Limitations:** 

**Conclusion:** The study offers actionable insights for the informed selection and application of evaluation benchmarks in AI agent research.

**Abstract:** The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.

</details>


### [39] [You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model](https://arxiv.org/abs/2506.11103)

*Wenchong He, Liqian Peng, Zhe Jiang, Alex Go*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Fine-tuning, Language Models, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper introduces Many-Shot In-Context Fine-tuning (ManyICL), a novel approach that leverages in-context learning to enhance model performance across multiple tasks simultaneously.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the performance gap between in-context learning and dedicated fine-tuning of language models, proposing a more efficient method to improve model generalization.

**Method:** ManyICL extends in-context learning principles to a many-shot framework, treating multiple examples as supervised training targets rather than just prompts.

**Key Contributions:**

	1. Introduction of Many-Shot In-Context Fine-tuning (ManyICL) for LLMs.
	2. Novel training objective that utilizes in-context answers as supervised targets.
	3. Demonstration of improved performance compared to zero/few-shot strategies, approaching dedicated fine-tuning results.

**Result:** ManyICL outperforms traditional zero/few-shot fine-tuning and approaches dedicated fine-tuning performance across diverse tasks such as classification, summarization, and math, while reducing catastrophic forgetting.

**Limitations:** 

**Conclusion:** The proposed ManyICL method significantly boosts the efficiency and effectiveness of language models in handling multiple tasks simultaneously.

**Abstract:** Large language models (LLMs) possess a remarkable ability to perform in-context learning (ICL), which enables them to handle multiple downstream tasks simultaneously without requiring task-specific fine-tuning. Recent studies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma 7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of all tasks at once. However, this approach still lags behind dedicated fine-tuning, where a separate model is trained for each individual task.   In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning (ManyICL), which significantly narrows this performance gap by extending the principles of ICL to a many-shot setting. To unlock the full potential of ManyICL and address the inherent inefficiency of processing long sequences with numerous in-context examples, we propose a novel training objective. Instead of solely predicting the final answer, our approach treats every answer within the context as a supervised training target. This effectively shifts the role of many-shot examples from prompts to targets for autoregressive learning. Through extensive experiments on diverse downstream tasks, including classification, summarization, question answering, natural language inference, and math, we demonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning and approaches the performance of dedicated fine-tuning. Furthermore, ManyICL significantly mitigates catastrophic forgetting issues observed in zero/few-shot fine-tuning. The code will be made publicly available upon publication.

</details>


### [40] [DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration](https://arxiv.org/abs/2506.11104)

*Hanzhi Zhang, Heng Fan, Kewei Sha, Yan Huang, Yunhe Feng*

**Main category:** cs.CL

**Keywords:** sparse attention, NLP, dynamic masking, large language models, efficiency

**Relevance Score:** 7

**TL;DR:** This paper presents a dynamic sparse attention mechanism that improves efficiency in NLP tasks while maintaining adaptability and performance without predefined masks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Long-context understanding in NLP requires efficient self-attention mechanisms due to the limitations of transformers in handling long sequences effectively.

**Method:** The proposed method introduces dynamic masks at the attention-map level, which adaptively choose attention patterns depending on the context, unlike static masks used in traditional sparse attention.

**Key Contributions:**

	1. Dynamic sparse attention mechanism
	2. No need for fine-tuning or predefined masks
	3. Scalable alternative to full attention

**Result:** The dynamic sparse attention mechanism achieves alignment with full-attention models, showing minimal performance degradation and significantly reducing memory and compute overhead in long-sequence NLP tasks.

**Limitations:** 

**Conclusion:** The proposed approach provides a scalable solution for deploying large-scale LLMs while ensuring effective retrieval performance, eliminating the need for fine-tuning or static mask definitions.

**Abstract:** Long-context understanding is crucial for many NLP applications, yet transformers struggle with efficiency due to the quadratic complexity of self-attention. Sparse attention methods alleviate this cost but often impose static, predefined masks, failing to capture heterogeneous attention patterns. This results in suboptimal token interactions, limiting adaptability and retrieval accuracy in long-sequence tasks. This work introduces a dynamic sparse attention mechanism that assigns adaptive masks at the attention-map level, preserving heterogeneous patterns across layers and heads. Unlike existing approaches, our method eliminates the need for fine-tuning and predefined mask structures while maintaining computational efficiency. By learning context-aware attention structures, it achieves high alignment with full-attention models, ensuring minimal performance degradation while reducing memory and compute overhead. This approach provides a scalable alternative to full attention, enabling the practical deployment of large-scale Large Language Models (LLMs) without sacrificing retrieval performance. DAM is available at: https://github.com/HanzhiZhang-Ulrica/DAM.

</details>


### [41] [Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation](https://arxiv.org/abs/2506.11105)

*Uttej Kallakurik, Edward Humes, Rithvik Jonna, Xiaomin Lin, Tinoosh Mohsenin*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical assistant system, model compression, healthcare, edge devices

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel, resource-efficient medical assistant system using compressed LLMs tailored for real-time deployment in edge devices.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enable the deployment of Large Language Models in real-time, resource-constrained environments in healthcare scenarios.

**Method:** A general-purpose compression framework that measures neuron saliency on domain-specific data to prune irrelevant neurons, followed by post-training quantization.

**Key Contributions:**

	1. Introduction of a general-purpose compression framework for LLMs in health informatics.
	2. Demonstration of effective neuron pruning to reduce model size while maintaining performance.
	3. Successful deployment of compressed models on resource-constrained devices.

**Result:** The compressed models were evaluated against medical benchmarks, demonstrating the ability to efficiently deploy significantly reduced models while maintaining performance.

**Limitations:** 

**Conclusion:** Optimized LLMs can achieve real-time, energy-efficient inference for medical applications on edge hardware.

**Abstract:** Large Language Models (LLMs) have significant impact on the healthcare scenarios but remain prohibitively large for deployment in real-time, resource-constrained environments such as edge devices. In this work, we introduce a novel medical assistant system, optimized through our general-purpose compression framework, which tailors Large Language Models (LLMs) for deployment in specialized domains. By measuring neuron saliency on domain-specific data, our method can aggressively prune irrelevant neurons, reducing model size while preserving performance. Following pruning, we apply post-training quantization to further reduce the memory footprint, and evaluate the compressed model across medical benchmarks including MedMCQA, MedQA, and PubMedQA. We also deploy the 50\% compressed Gemma and the 67\% compressed LLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak), achieving real-time, energy-efficient inference under hardware constraints.

</details>


### [42] [Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking](https://arxiv.org/abs/2506.11106)

*Ningyuan Li, Junrui Liu, Yi Shan, Minghui Huang, Tong Li*

**Main category:** cs.CL

**Keywords:** graph-based retrieval, retrieval-augmented generation, dependency-aware, query-resolution, large language models

**Relevance Score:** 8

**TL;DR:** PankRAG enhances graph-based retrieval-augmented generation by addressing the limitations of entity-level extraction through a hierarchical query-resolution strategy and dependency-aware reranking.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the misinterpretation and omission of critical information in graph-based retrieval methods that rely solely on entity extraction, thus improving the fidelity of generated responses.

**Method:** PankRAG uses a multi-level resolution path for structured reasoning, capturing both parallel and sequential interdependencies, followed by a dependency-aware reranking to validate retrieval results.

**Key Contributions:**

	1. Introduces a hierarchical query-resolution strategy that captures interdependencies in queries.
	2. Develops a dependency-aware reranking mechanism to improve the quality of retrieved information.
	3. Demonstrates consistent performance improvements over existing methods across various benchmarks.

**Result:** PankRAG outperforms state-of-the-art approaches across multiple benchmarks in terms of retrieval and generation fidelity.

**Limitations:** 

**Conclusion:** The robust performance of PankRAG demonstrates its effectiveness in enhancing retrieval-augmented generation tasks.

**Abstract:** Contemporary graph-based retrieval-augmented generation (RAG) methods typically begin by extracting entities from user queries and then leverage pre-constructed knowledge graphs to retrieve related relationships and metadata. However, this pipeline's exclusive reliance on entity-level extraction can lead to the misinterpretation or omission of latent yet critical information and relations. As a result, retrieved content may be irrelevant or contradictory, and essential knowledge may be excluded, exacerbating hallucination risks and degrading the fidelity of generated responses. To address these limitations, we introduce PankRAG, a framework that combines a globally aware, hierarchical query-resolution strategy with a novel dependency-aware reranking mechanism. PankRAG first constructs a multi-level resolution path that captures both parallel and sequential interdependencies within a query, guiding large language models (LLMs) through structured reasoning. It then applies its dependency-aware reranker to exploit the dependency structure among resolved sub-questions, enriching and validating retrieval results for subsequent sub-questions. Empirical evaluations demonstrate that PankRAG consistently outperforms state-of-the-art approaches across multiple benchmarks, underscoring its robustness and generalizability.

</details>


### [43] [History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM](https://arxiv.org/abs/2506.11108)

*Andrew Kiruluta, Andreas Lemos, Priscilla Burity*

**Main category:** cs.CL

**Keywords:** CAGSR, vLLM, multi-turn dialogue, chain-of-thought reasoning, reinforcement learning

**Relevance Score:** 8

**TL;DR:** CAGSR-vLLM-MTC enhances the CAGSR framework to support multi-turn dialogue and chain-of-thought reasoning through novel methods of capturing attention signals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve dialogue systems and reasoning capabilities in AI through an advanced reinforcement learning framework.

**Method:** The approach extends the CAGSR framework with a high-performance vLLM runtime, capturing attention weights during generation and accumulating signals over conversation histories and reasoning steps.

**Key Contributions:**

	1. Implementation of an enhanced vLLM runtime for better performance in dialogue.
	2. A self-supervised reward function that aggregates attention signals.
	3. Novel entropy-based clamping mechanism to maintain attention integrity.

**Result:** The extension shows improvements in handling multi-turn dialogues and reasoning processes, addressing practical trade-offs in attention mechanisms.

**Limitations:** 

**Conclusion:** CAGSR-vLLM-MTC can facilitate future advancements in multi-party dialogues and hierarchical reasoning in AI.

**Abstract:** We present CAGSR-vLLM-MTC, an extension of our Self-Supervised Cross-Attention-Guided Reinforcement (CAGSR) framework, now implemented on the high-performance vLLM runtime, to address both multi-turn dialogue and chain-of-thought reasoning. Building upon our original single-turn approach, we first instrumented vLLM's C++/CUDA kernels to asynchronously capture per-layer, per-head cross-attention weights during generation. We then generalized our self-supervised reward function to accumulate attention signals over entire conversation histories and intermediate chain-of-thought steps. We discuss practical trade-offs, including an entropy-based clamping mechanism to prevent attention collapse on early context, and outline future directions for multi-party dialogues and hierarchical reasoning.

</details>


### [44] [Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization](https://arxiv.org/abs/2506.11109)

*Yile Chen, Yicheng Tao, Yue Jiang, Shuai Liu, Han Yu, Gao Cong*

**Main category:** cs.CL

**Keywords:** location-based services, large language models, mobility analytics, tokenization, fine-tuning

**Relevance Score:** 9

**TL;DR:** QT-Mob enhances LLMs for mobility analytics by introducing location tokenization and improved fine-tuning objectives, outperforming existing models in mobility tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in semantic representation of locations and mobility signal modeling in current LLMs used for mobility analytics.

**Method:** QT-Mob includes a location tokenization module for compact and semantically rich location representation and a series of fine-tuning objectives to improve comprehension of movement patterns.

**Key Contributions:**

	1. Introduction of a location tokenization module for better semantic representation of locations.
	2. Development of complementary fine-tuning objectives for aligning tokens with LLM representations.
	3. Demonstration of superior performance on real-world mobility tasks compared to existing methods.

**Result:** Experiments show QT-Mob significantly improves next-location prediction and mobility recovery, outperforming traditional deep learning and LLM-based approaches on real-world datasets.

**Limitations:** 

**Conclusion:** QT-Mob provides a more generalizable framework for mobility analytics with enhanced comprehension of mobility data.

**Abstract:** The widespread adoption of location-based services has led to the generation of vast amounts of mobility data, providing significant opportunities to model user movement dynamics within urban environments. Recent advancements have focused on adapting Large Language Models (LLMs) for mobility analytics. However, existing methods face two primary limitations: inadequate semantic representation of locations (i.e., discrete IDs) and insufficient modeling of mobility signals within LLMs (i.e., single templated instruction fine-tuning). To address these issues, we propose QT-Mob, a novel framework that significantly enhances LLMs for mobility analytics. QT-Mob introduces a location tokenization module that learns compact, semantically rich tokens to represent locations, preserving contextual information while ensuring compatibility with LLMs. Furthermore, QT-Mob incorporates a series of complementary fine-tuning objectives that align the learned tokens with the internal representations in LLMs, improving the model's comprehension of sequential movement patterns and location semantics. The proposed QT-Mob framework not only enhances LLMs' ability to interpret mobility data but also provides a more generalizable approach for various mobility analytics tasks. Experiments on three real-world dataset demonstrate the superior performance in both next-location prediction and mobility recovery tasks, outperforming existing deep learning and LLM-based methods.

</details>


### [45] [AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models](https://arxiv.org/abs/2506.11110)

*Jaeho Lee, Atharv Chowdhary*

**Main category:** cs.CL

**Keywords:** Large Language Models, framing effects, factual consistency, AssertBench, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces AssertBench, a benchmark to study how the framing of factually true statements affects the agreement of Large Language Models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how directional framing of true statements influences model agreement in LLMs, filling the gap in understanding this interaction.

**Method:** AssertBench utilizes evidence-supported facts from the FEVEROUS dataset to create two framing prompts for each fact: one asserting correctness and another asserting incorrectness, measuring model responses.

**Key Contributions:**

	1. Introduction of AssertBench for evaluating LLMs' agreement based on framing.
	2. Methodology for isolating framing effects from factual accuracy.
	3. Insights into the LLMs' consistency in truth evaluation amidst contradictory user statements.

**Result:** The study indicates how LLMs respond differently based on the framing of factual statements, assessing their consistency.

**Limitations:** The study is focused on LLMs and may not generalize to other types of models or tasks; the results are dependent on the training data used.

**Conclusion:** LLMs should ideally maintain consistent evaluations of facts regardless of user-framed prompts, showcasing their factual reliability.

**Abstract:** Recent benchmarks have probed factual consistency and rhetorical robustness in Large Language Models (LLMs). However, a knowledge gap exists regarding how directional framing of factually true statements influences model agreement, a common scenario for LLM users. AssertBench addresses this by sampling evidence-supported facts from FEVEROUS, a fact verification dataset. For each (evidence-backed) fact, we construct two framing prompts: one where the user claims the statement is factually correct, and another where the user claims it is incorrect. We then record the model's agreement and reasoning. The desired outcome is that the model asserts itself, maintaining consistent truth evaluation across both framings, rather than switching its evaluation to agree with the user. AssertBench isolates framing-induced variability from the model's underlying factual knowledge by stratifying results based on the model's accuracy on the same claims when presented neutrally. In doing so, this benchmark aims to measure an LLM's ability to "stick to its guns" when presented with contradictory user assertions about the same fact. The complete source code is available at https://github.com/achowd32/assert-bench.

</details>


### [46] [Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions](https://arxiv.org/abs/2506.11111)

*Kun Zhang, Le Wu, Kui Yu, Guangyi Lv, Dacao Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, robustness, adversarial prompts, out-of-distribution, evaluation metrics

**Relevance Score:** 9

**TL;DR:** This survey reviews the robustness of Large Language Models (LLMs), outlining definitions, evaluation methods, and future research directions.

**Read time:** 60 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing attention on the robustness of LLMs amidst their wide-ranging applications like Agents and Embodied Intelligence.

**Method:** The survey is organized based on the types of perturbated inputs affecting LLM robustness, covering Adversarial Robustness, OOD Robustness, and Evaluation of Robustness frameworks.

**Key Contributions:**

	1. Formal definition of LLM robustness.
	2. Comprehensive classification of robustness issues related to LLMs.
	3. Compilation of evaluation datasets and metrics for LLM robustness.

**Result:** The paper reviews various approaches to LLM robustness, highlighting challenges posed by adversarial prompts, out-of-distribution inputs, and evaluating robustness through new datasets and metrics.

**Limitations:** Limited to the literature reviewed, may not cover all emerging techniques and methods.

**Conclusion:** The authors propose future research opportunities and provide a collection of relevant work to facilitate community efforts in improving LLM robustness.

**Abstract:** Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the community.

</details>


### [47] [Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)](https://arxiv.org/abs/2506.11112)

*Christine Bauer, Li Chen, Nicola Ferro, Norbert Fuhr, Avishek Anand, Timo Breuer, Guglielmo Faggioli, Ophir Frieder, Hideo Joho, Jussi Karlgren, Johannes Kiesel, Bart P. Knijnenburg, Aldo Lipani, Lien Michiels, Andrea Papenmeier, Maria Soledad Pera, Mark Sanderson, Scott Sanner, Benno Stein, Johanne R. Trippas, Karin Verspoor, Martijn C Willemsen*

**Main category:** cs.CL

**Keywords:** Conversational Information Access, evaluation framework, Conversational Agents, HCI, user tasks

**Relevance Score:** 5

**TL;DR:** This paper discusses the definition and framework for evaluating Conversational Information Access (CONIAC) systems through the Conversational Agents Framework for Evaluation (CAFE).

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To define and evaluate the unique features of Conversational Information Access (CONIAC) systems.

**Method:** The paper proposes a world model for CONIAC and outlines the CAFE framework, consisting of six components for evaluation: stakeholder goals, user tasks, user aspects, evaluation criteria, evaluation methodology, and quantitative measures.

**Key Contributions:**

	1. Definition of Conversational Information Access (CONIAC)
	2. Introduction of the CAFE framework for evaluation
	3. Identification of six components essential for evaluating CONIAC systems.

**Result:** The CAFE framework provides a structured approach for evaluating CONIAC systems, focusing on several critical components that influence their effectiveness.

**Limitations:** 

**Conclusion:** The structured evaluation framework aims to enhance understanding and improve the design of CONIAC systems.

**Abstract:** During the workshop, we deeply discussed what CONversational Information ACcess (CONIAC) is and its unique features, proposing a world model abstracting it, and defined the Conversational Agents Framework for Evaluation (CAFE) for the evaluation of CONIAC systems, consisting of six major components: 1) goals of the system's stakeholders, 2) user tasks to be studied in the evaluation, 3) aspects of the users carrying out the tasks, 4) evaluation criteria to be considered, 5) evaluation methodology to be applied, and 6) measures for the quantitative criteria chosen.

</details>


### [48] [Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks](https://arxiv.org/abs/2506.11113)

*Tzu-Ling Lin, Wei-Chih Chen, Teng-Fang Hsiao, Hou-I Liu, Ya-Hsin Yeh, Yu Kai Chan, Wen-Sheng Lien, Po-Yen Kuo, Philip S. Yu, Hong-Han Shuai*

**Main category:** cs.CL

**Keywords:** Large Language Models, Peer Review, Adversarial Attacks, Automated Review, Academic Integrity

**Relevance Score:** 8

**TL;DR:** This paper evaluates the robustness of LLMs in automated peer review against adversarial attacks, highlighting vulnerabilities that affect their reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the burden on human reviewers and explore the potential of LLMs for automated peer review amidst concerns regarding their reliability under adversarial conditions.

**Method:** The study compares the effectiveness of LLM-generated reviews to those of human reviewers and assesses the impact of adversarial attacks on these reviews, identifying mitigation strategies.

**Key Contributions:**

	1. Evaluation of LLMs as automated reviewers
	2. Identification of vulnerabilities to adversarial attacks
	3. Proposition of challenges and mitigation strategies for LLM-based reviews

**Result:** The evaluation shows significant vulnerabilities of LLMs, indicating that adversarial text manipulations can distort their assessments, risking the integrity of the review process.

**Limitations:** 

**Conclusion:** Addressing adversarial risks is crucial for ensuring that AI enhances rather than undermines the quality of academic peer review.

**Abstract:** Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.

</details>


### [49] [KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations](https://arxiv.org/abs/2506.11114)

*Junyu Liu, Kaiqi Yan, Tianyang Wang, Qian Niu, Momoko Nagai-Tanima, Tomoki Aoyama*

**Main category:** cs.CL

**Keywords:** Multimodal benchmark, Healthcare licensing exams, Large language models, Multilingual AI, Clinical reasoning

**Relevance Score:** 9

**TL;DR:** KokushiMD-10 is a multimodal benchmark for evaluating large language models in healthcare, constructed from Japanese national healthcare licensing exams, offering over 11588 questions and highlighting ongoing challenges in medical AI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To comprehensively evaluate large language models across various healthcare roles in clinical scenarios, addressing limitations of existing benchmarks that are text-based and English-centric.

**Method:** Introduction of KokushiMD-10, a benchmark sourced from ten Japanese national healthcare licensing exams, spanning multiple medical fields and incorporating clinical images and rationales.

**Key Contributions:**

	1. Introduction of a comprehensive multimodal benchmark in healthcare.
	2. Inclusion of real exam questions, clinical images, and expert annotations for evaluation.
	3. Assessment of multiple LLMs across text and image modalities.

**Result:** Benchmarking of over 30 state-of-the-art LLMs revealed that no model consistently meets passing thresholds across the assessed domains, indicating challenges in medical AI performance.

**Limitations:** No model consistently passing across domains, indicating ongoing challenges in LLM performance in healthcare.

**Conclusion:** KokushiMD-10 serves as a resource for evaluating and improving reasoning-centric medical AI in multilingual and multimodal contexts.

**Abstract:** Recent advances in large language models (LLMs) have demonstrated notable performance in medical licensing exams. However, comprehensive evaluation of LLMs across various healthcare roles, particularly in high-stakes clinical scenarios, remains a challenge. Existing benchmarks are typically text-based, English-centric, and focus primarily on medicines, which limits their ability to assess broader healthcare knowledge and multimodal reasoning. To address these gaps, we introduce KokushiMD-10, the first multimodal benchmark constructed from ten Japanese national healthcare licensing exams. This benchmark spans multiple fields, including Medicine, Dentistry, Nursing, Pharmacy, and allied health professions. It contains over 11588 real exam questions, incorporating clinical images and expert-annotated rationales to evaluate both textual and visual reasoning. We benchmark over 30 state-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both text and image-based settings. Despite promising results, no model consistently meets passing thresholds across domains, highlighting the ongoing challenges in medical AI. KokushiMD-10 provides a comprehensive and linguistically grounded resource for evaluating and advancing reasoning-centric medical AI across multilingual and multimodal clinical tasks.

</details>


### [50] [Incorporating Domain Knowledge into Materials Tokenization](https://arxiv.org/abs/2506.11115)

*Yerim Oh, Jun-Hyung Park, Junho Kim, SungHo Kim, SangKeun Lee*

**Main category:** cs.CL

**Keywords:** tokenization, materials science, domain knowledge, semantic integrity, NLP

**Relevance Score:** 4

**TL;DR:** This paper presents MATTER, a novel tokenization approach that integrates material knowledge to enhance semantic integrity and reduce fragmentation in materials science text processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional tokenization methods used in NLP do not adequately preserve the structural and semantic integrity of material concepts in scientific texts, leading to issues in downstream tasks.

**Method:** MATTER combines material knowledge from a trained model named MatDetector with a re-ranking technique that focuses on material concepts during token merging.

**Key Contributions:**

	1. Introduction of MATTER, a novel tokenization method for materials science.
	2. Integration of domain-specific knowledge into the tokenization process.
	3. Demonstrated performance improvements over traditional methods in scientific tasks.

**Result:** Experiments show MATTER outperforms existing tokenization methods, achieving a 4% improvement in generation tasks and 2% in classification tasks.

**Limitations:** The study is focused on materials science, which may limit the generalizability of the approach to other domains.

**Conclusion:** The findings emphasize the significance of incorporating domain knowledge in tokenization for effective scientific text processing.

**Abstract:** While language models are increasingly utilized in materials science, typical models rely on frequency-centric tokenization methods originally developed for natural language processing. However, these methods frequently produce excessive fragmentation and semantic loss, failing to maintain the structural and semantic integrity of material concepts. To address this issue, we propose MATTER, a novel tokenization approach that integrates material knowledge into tokenization. Based on MatDetector trained on our materials knowledge base and a re-ranking method prioritizing material concepts in token merging, MATTER maintains the structural integrity of identified material concepts and prevents fragmentation during tokenization, ensuring their semantic meaning remains intact. The experimental results demonstrate that MATTER outperforms existing tokenization methods, achieving an average performance gain of $4\%$ and $2\%$ in the generation and classification tasks, respectively. These results underscore the importance of domain knowledge for tokenization strategies in scientific text processing. Our code is available at https://github.com/yerimoh/MATTER

</details>


### [51] [Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models](https://arxiv.org/abs/2506.11116)

*Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, Yonghua Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Instruction Datasets, Machine Learning, Human-Computer Interaction, Chat Models

**Relevance Score:** 9

**TL;DR:** Infinity-Instruct is a high-quality instruction dataset aimed at improving LLMs' foundational and chat capabilities, curated through a two-phase pipeline.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing open-source instruction datasets that focus on narrow domains and to improve the performance of LLMs in real-world applications.

**Method:** The paper introduces a two-phase pipeline: Phase 1 curates 7.4M foundational instructions from 100M samples using hybrid data selection, and Phase 2 synthesizes 1.5M chat instructions through selection, evolution, and filtering.

**Key Contributions:**

	1. Introduction of a novel instruction dataset, Infinity-Instruct, enhancing LLM performance.
	2. Demonstration of significant performance improvements when fine-tuning with the dataset.
	3. Insights into the relationship between foundational and chat model training.

**Result:** Empirical evaluations show substantial performance gains for models fine-tuned with Infinity-Instruct, surpassing official instruction-tuned models, with InfInstruct-LLaMA3.1-70B outperforming GPT-4-0314 by 8.6% on instruction following tasks.

**Limitations:** 

**Conclusion:** The results suggest a beneficial synergy between foundational and chat training in LLM development, providing new insights into creating effective instruction datasets.

**Abstract:** Large Language Models (LLMs) demonstrate strong performance in real-world applications, yet existing open-source instruction datasets often concentrate on narrow domains, such as mathematics or coding, limiting generalization and widening the gap with proprietary models. To bridge this gap, we introduce Infinity-Instruct, a high-quality instruction dataset designed to enhance both foundational and chat capabilities of LLMs through a two-phase pipeline. In Phase 1, we curate 7.4M high-quality foundational instructions (InfInstruct-F-7.4M) from over 100M samples using hybrid data selection techniques. In Phase 2, we synthesize 1.5M high-quality chat instructions (InfInstruct-G-1.5M) through a two-stage process involving instruction selection, evolution, and diagnostic filtering. We empirically evaluate Infinity-Instruct by fine-tuning several open-source models, including Mistral, LLaMA, Qwen, and Yi, and observe substantial performance gains across both foundational and instruction following benchmarks, consistently surpassing official instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B outperforms GPT-4-0314 by 8.6\% on instruction following tasks while achieving comparable foundational performance. These results underscore the synergy between foundational and chat training and offer new insights into holistic LLM development. Our dataset\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and codes\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly released.

</details>


### [52] [ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research](https://arxiv.org/abs/2506.11117)

*Junyong Lin, Lu Dai, Ruiqian Han, Yijie Sui, Ruilin Wang, Xingliang Sun, Qinglin Wu, Min Feng, Hao Liu, Hui Xiong*

**Main category:** cs.CL

**Keywords:** scientific QA, dataset generation, information retrieval

**Relevance Score:** 8

**TL;DR:** ScIRGen is a framework for generating scientific QA and retrieval datasets that better reflect the actual information needs of researchers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the misalignment between existing scientific retrieval datasets and the complex information needs of researchers, often reflected in their research tasks rather than explicit queries.

**Method:** ScIRGen employs a dataset-oriented information extraction method, utilizes cognitive taxonomy for question generation, and filters synthetic answers using LLM perplexity shifts to create a large-scale scientific retrieval-augmented generation dataset.

**Key Contributions:**

	1. Development of ScIRGen framework for scientific QA and retrieval
	2. Creation of ScIRGen-Geo, a large-scale dataset with 61k QA pairs
	3. Methodology for filtering answers based on LLM perplexity shifts

**Result:** Created the ScIRGen-Geo dataset, containing 61,000 QA pairs, and benchmarked existing methods on this dataset, revealing significant challenges in reasoning complex questions.

**Limitations:** Current methods still struggle with reasoning from complex questions, indicating areas for further improvement.

**Conclusion:** The ScIRGen framework enhances the development of tools for supporting scientific research by generating datasets that closely reflect real-world information needs.

**Abstract:** Scientific researchers need intensive information about datasets to effectively evaluate and develop theories and methodologies. The information needs regarding datasets are implicitly embedded in particular research tasks, rather than explicitly expressed in search queries. However, existing scientific retrieval and question-answering (QA) datasets typically address straightforward questions, which do not align with the distribution of real-world research inquiries. To bridge this gap, we developed ScIRGen, a dataset generation framework for scientific QA \& retrieval that more accurately reflects the information needs of professional science researchers, and uses it to create a large-scale scientific retrieval-augmented generation (RAG) dataset with realistic queries, datasets and papers. Technically, we designed a dataset-oriented information extraction method that leverages academic papers to augment the dataset representation. We then proposed a question generation framework by employing cognitive taxonomy to ensure the quality of synthesized questions. We also design a method to automatically filter synthetic answers based on the perplexity shift of LLMs, which is highly aligned with human judgment of answers' validity. Collectively, these methodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We benchmarked representative methods on the ScIRGen-Geo dataset for their question-answering and retrieval capabilities, finding out that current methods still suffer from reasoning from complex questions. This work advances the development of more sophisticated tools to support the intricate information needs of the scientific community.

</details>


### [53] [Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech](https://arxiv.org/abs/2506.11119)

*Jingyu Li, Lingchao Mao, Hairong Wang, Zhendong Wang, Xi Mao, Xuelei Sherry Ni*

**Main category:** cs.CL

**Keywords:** Alzheimer's Disease, Cognitive Decline, Speech Recognition, Foundation Models, Early Detection

**Relevance Score:** 8

**TL;DR:** This paper benchmarks foundation speech and language models for early detection of Alzheimer's disease and related dementias (ADRD) using audio data from a large dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The importance of early detection of Alzheimer's disease and related dementias (ADRD) to facilitate timely intervention and care, leveraging spontaneous speech as potential biomarkers.

**Method:** Used the PREPARE Challenge dataset comprising audio recordings from over 1,600 participants with varying cognitive statuses (HC, MCI, AD), benchmarked multiple open-source speech and language models for classification of cognitive status.

**Key Contributions:**

	1. Benchmarking framework for foundation models in ADRD detection
	2. Demonstration of ASR-generated embeddings for cognitive status classification
	3. Inclusion of non-semantic features like pause patterns to enhance model performance

**Result:** The Whisper-medium model achieved the highest accuracy at 0.731 and AUC of 0.802, while BERT with pause annotation showed an accuracy of 0.662 and AUC of 0.744. ASR-generated audio embeddings outperformed other methods, and incorporating pause patterns improved classification results.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of acoustic-based approaches for scalable, non-invasive early detection of ADRD, establishing a benchmarking framework with a clinically relevant dataset.

**Abstract:** Background: Alzheimer's disease and related dementias (ADRD) are progressive neurodegenerative conditions where early detection is vital for timely intervention and care. Spontaneous speech contains rich acoustic and linguistic markers that may serve as non-invasive biomarkers for cognitive decline. Foundation models, pre-trained on large-scale audio or text data, produce high-dimensional embeddings encoding contextual and acoustic features.   Methods: We used the PREPARE Challenge dataset, which includes audio recordings from over 1,600 participants with three cognitive statuses: healthy control (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We excluded non-English, non-spontaneous, or poor-quality recordings. The final dataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We benchmarked a range of open-source foundation speech and language models to classify cognitive status into the three categories.   Results: The Whisper-medium model achieved the highest performance among speech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with pause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection using state-of-the-art automatic speech recognition (ASR) model-generated audio embeddings outperformed others. Including non-semantic features like pause patterns consistently improved text-based classification.   Conclusion: This study introduces a benchmarking framework using foundation models and a clinically relevant dataset. Acoustic-based approaches -- particularly ASR-derived embeddings -- demonstrate strong potential for scalable, non-invasive, and cost-effective early detection of ADRD.

</details>


### [54] [SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models](https://arxiv.org/abs/2506.11120)

*Hourun Zhu, Chengchao Shen*

**Main category:** cs.CL

**Keywords:** LLMs, pruning, self-distillation, MLP, compression

**Relevance Score:** 8

**TL;DR:** This paper proposes a self-distillation loss during the pruning phase of LLMs to improve gradient information for pruning MLP modules, leading to significant compression without performance loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the high costs of deploying large language models (LLMs) while maintaining their performance, particularly focusing on the effectiveness of gradient-based pruning methods.

**Method:** Introducing a self-distillation loss during the pruning phase to leverage the original model's predictions, particularly targeting the pruning of multilayer perceptron (MLP) modules in LLMs.

**Key Contributions:**

	1. Self-distillation loss during pruning to enhance gradient information
	2. Focus on MLP module pruning for effective compression
	3. Outperformance of existing pruning methods on zero-shot benchmarks

**Result:** The proposed method outperforms existing pruning methods and achieves competitive performance on benchmarks among 1B-scale open source LLMs.

**Limitations:** 

**Conclusion:** Utilizing self-distillation in the pruning phase allows for significant model compression with minimal impact on performance.

**Abstract:** In spite of strong performance achieved by LLMs, the costs of their deployment are unaffordable. For the compression of LLMs, gradient-based pruning methods present promising effectiveness. However, in these methods, the gradient computation with one-hot labels ignore the potential predictions on other words, thus missing key information for generative capability of the original model. To address this issue, we introduce a self-distillation loss during the pruning phase (rather than post-training) to fully exploit the predictions of the original model, thereby obtaining more accurate gradient information for pruning. Moreover, we find that, compared to attention modules, the predictions of LLM are less sensitive to multilayer perceptron (MLP) modules, which take up more than $5 \times$ parameters (LLaMA3.2-1.2B). To this end, we focus on the pruning of MLP modules, to significantly compress LLM without obvious performance degradation. Experimental results on extensive zero-shot benchmarks demonstrate that our method significantly outperforms existing pruning methods. Furthermore, our method achieves very competitive performance among 1B-scale open source LLMs. The source code and trained weights are available at https://github.com/visresearch/SDMPrune.

</details>


### [55] [MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets](https://arxiv.org/abs/2412.21015)

*Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez*

**Main category:** cs.CL

**Keywords:** Geospatial QA, Large Language Models, Data Annotation

**Relevance Score:** 8

**TL;DR:** MapQaTor is an open-source framework for creating geospatial question answering datasets, improving reliability and speed of the data annotation process.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the handling of natural language geospatial queries by integrating Large Language Models with mapping services.

**Method:** MapQaTor allows seamless integration with maps APIs, centralizes data retrieval and visualization, and caches API responses for consistent data.

**Key Contributions:**

	1. Introduction of an open-source framework for geospatial QA datasets
	2. Integration with various maps APIs for data gathering
	3. Significant speedup in the annotation process by 30 times compared to manual methods

**Result:** The evaluation shows that MapQaTor accelerates the annotation process by at least 30 times compared to manual methods, facilitating the development of complex geospatial datasets.

**Limitations:** 

**Conclusion:** MapQaTor presents a significant advancement in evaluating LLM-based geospatial reasoning and improving their capabilities for geospatial understanding.

**Abstract:** Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.

</details>


### [56] [SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR](https://arxiv.org/abs/2506.11121)

*Wei-Ping Huang, Guan-Ting Lin, Hung-yi Lee*

**Main category:** cs.CL

**Keywords:** ASR, Test-Time Adaptation, language model rescoring, entropy-minimization, domain adaptation

**Relevance Score:** 6

**TL;DR:** This paper introduces SUTA-LM, a novel Test-Time Adaptation method that effectively integrates entropy-minimization with language model rescoring to improve ASR performance across different domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance drops in ASR due to real-world domain mismatches and to explore the interaction between Test-Time Adaptation and language model rescoring.

**Method:** SUTA-LM combines a controlled adaptation process using entropy minimization and an auto-step selection mechanism, followed by language model rescoring to optimize outputs.

**Key Contributions:**

	1. Introduction of SUTA-LM for effective ASR improvement
	2. Demonstration of the challenges between TTA and language model rescoring
	3. Robust experimental results across multiple ASR datasets

**Result:** Experiments demonstrate that SUTA-LM consistently achieves better performance across 18 diverse ASR datasets compared to previous methods.

**Limitations:** Potential limitations of SUTA-LM were not discussed in the abstract.

**Conclusion:** SUTA-LM effectively mitigates the challenges of combining TTA with language model rescoring in ASR applications.

**Abstract:** Despite progress in end-to-end ASR, real-world domain mismatches still cause performance drops, which Test-Time Adaptation (TTA) aims to mitigate by adjusting models during inference. Recent work explores combining TTA with external language models, using techniques like beam search rescoring or generative error correction. In this work, we identify a previously overlooked challenge: TTA can interfere with language model rescoring, revealing the nontrivial nature of effectively combining the two methods. Based on this insight, we propose SUTA-LM, a simple yet effective extension of SUTA, an entropy-minimization-based TTA approach, with language model rescoring. SUTA-LM first applies a controlled adaptation process guided by an auto-step selection mechanism leveraging both acoustic and linguistic information, followed by language model rescoring to refine the outputs. Experiments on 18 diverse ASR datasets show that SUTA-LM achieves robust results across a wide range of domains.

</details>


### [57] [ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams](https://arxiv.org/abs/2506.11125)

*Freddie Grabovski, Gilad Gressel, Yisroel Mirsky*

**Main category:** cs.CL

**Keywords:** Voice Phishing, Adversarial Perturbations, ASR, TTS, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** The paper presents ASRJam, a defense framework against voice phishing scams that disrupts attacker ASR without affecting human callers, utilizing EchoGuard, a novel jammer leveraging natural distortions to ensure usability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Voice phishing scams using LLMs, TTS, and ASR pose a significant security threat; the ASR transcription step is identified as the most vulnerable link.

**Method:** The authors propose ASRJam, which injects adversarial perturbations into victims' audio, and introduce EchoGuard, a jammer that utilizes natural distortions (reverberation and echo) to disrupt ASR while remaining tolerable for humans.

**Key Contributions:**

	1. Introduction of ASRJam as a proactive defense against vishing
	2. Development of EchoGuard utilizing natural audio distortions
	3. User study demonstrating superior effectiveness and usability of EchoGuard

**Result:** EchoGuard achieved the highest overall utility in a user study, providing an effective balance of ASR disruption and human listening experience compared to three state-of-the-art attacks.

**Limitations:** The effectiveness of the approach may vary depending on the specific attack method used and the audio environment.

**Conclusion:** The ASRJam framework and EchoGuard can effectively defend against voice phishing without compromising user experience, marking a significant step in audio security.

**Abstract:** Large Language Models (LLMs), combined with Text-to-Speech (TTS) and Automatic Speech Recognition (ASR), are increasingly used to automate voice phishing (vishing) scams. These systems are scalable and convincing, posing a significant security threat. We identify the ASR transcription step as the most vulnerable link in the scam pipeline and introduce ASRJam, a proactive defence framework that injects adversarial perturbations into the victim's audio to disrupt the attacker's ASR. This breaks the scam's feedback loop without affecting human callers, who can still understand the conversation. While prior adversarial audio techniques are often unpleasant and impractical for real-time use, we also propose EchoGuard, a novel jammer that leverages natural distortions, such as reverberation and echo, that are disruptive to ASR but tolerable to humans. To evaluate EchoGuard's effectiveness and usability, we conducted a 39-person user study comparing it with three state-of-the-art attacks. Results show that EchoGuard achieved the highest overall utility, offering the best combination of ASR disruption and human listening experience.

</details>


### [58] [GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions](https://arxiv.org/abs/2506.11127)

*Wenkang Han, Zhixiong Zeng, Jing Huang, Shu Jiang, Liming Zheng, Longrong Yang, Haibo Qiu, Chang Yao, Jingyuan Chen, Lin Ma*

**Main category:** cs.CL

**Keywords:** Autonomous agents, Graphical User Interfaces, Speech instructions, Human-computer interaction, Mixed-instruction training

**Relevance Score:** 9

**TL;DR:** GUIRoboTron-Speech is the first end-to-end autonomous GUI agent that accepts speech instructions and screenshots, addressing limitations in accessibility and convenience in hands-free interfaces.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility and convenience in human-computer interaction by allowing users to control GUIs via speech, especially in hands-free scenarios.

**Method:** Generated high-quality speech instructions using a text-to-speech model and developed GUIRoboTron-Speech through progressive training stages and a mixed-instruction training strategy to address modality imbalances.

**Key Contributions:**

	1. First end-to-end autonomous GUI agent that uses speech instructions directly
	2. Novel mixed-instruction training strategy to address modality imbalance
	3. Validation on benchmark datasets demonstrating superior performance

**Result:** Experimental validation on benchmark datasets shows that GUIRoboTron-Speech outperforms existing methods in effectively interpreting speech instructions for GUI interactions.

**Limitations:** 

**Conclusion:** The findings indicate the significant potential for using speech as an effective instruction modality for GUI agents, making them more accessible and user-friendly.

**Abstract:** Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing human-computer interaction, yet their reliance on text-based instructions imposes limitations on accessibility and convenience, particularly in hands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the first end-to-end autonomous GUI agent that directly accepts speech instructions and on-device screenshots to predict actions. Confronted with the scarcity of speech-based GUI agent datasets, we initially generated high-quality speech instructions for training by leveraging a random timbre text-to-speech (TTS) model to convert existing text instructions. We then develop GUIRoboTron-Speech's capabilities through progressive grounding and planning training stages. A key contribution is a heuristic mixed-instruction training strategy designed to mitigate the modality imbalance inherent in pre-trained foundation models. Comprehensive experiments on several benchmark datasets validate the robust and superior performance of GUIRoboTron-Speech, demonstrating the significant potential and widespread applicability of speech as an effective instruction modality for driving GUI agents. Our code and datasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech.

</details>


### [59] [Stronger Language Models Produce More Human-Like Errors](https://arxiv.org/abs/2506.11128)

*Andrew Keenan Richardson, Ryan Othniel Kearns, Sean Moss, Vincent Wang-Mascianica, Philipp Koralus*

**Main category:** cs.CL

**Keywords:** language models, human reasoning, cognitive bias, Erotetic Theory of Reasoning, inverse scaling

**Relevance Score:** 8

**TL;DR:** The paper investigates whether language models exhibit human-like reasoning patterns as they improve, revealing an increase in error types that align with human reasoning fallacies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the relationship between the sophistication of language models and their reasoning patterns, particularly if they emulate human-like reasoning fallacies.

**Method:** The study employs the Erotetic Theory of Reasoning (ETR) to generate logical reasoning tasks, analyzing responses from 38 language models on 383 problems to assess error patterns.

**Key Contributions:**

	1. Introduces the concept of inverse scaling in language model reasoning errors.
	2. Applies the Erotetic Theory of Reasoning to language models in a novel context.
	3. Demonstrates that increased sophistication in models correlates with greater alignment to human-like reasoning fallacies.

**Result:** As language models become more capable, their errors increasingly reflect predicted human reasoning fallacies without a corresponding improvement in logical correctness.

**Limitations:** The analysis may be constrained by the specific types of reasoning tasks selected and the particular language models evaluated.

**Conclusion:** The findings suggest that advancements in language models may lead them toward mimicking human cognitive biases rather than achieving normative rationality.

**Abstract:** Do language models converge toward human-like reasoning patterns as they improve? We provide surprising evidence that while overall reasoning capabilities increase with model sophistication, the nature of errors increasingly mirrors predictable human reasoning fallacies: a previously unobserved inverse scaling phenomenon. To investigate this question, we apply the Erotetic Theory of Reasoning (ETR), a formal cognitive framework with empirical support for predicting human reasoning outcomes. Using the open-source package PyETR, we generate logical reasoning problems where humans predictably err, evaluating responses from 38 language models across 383 reasoning tasks. Our analysis indicates that as models advance in general capability (as measured by Chatbot Arena scores), the proportion of their incorrect answers that align with ETR-predicted human fallacies tends to increase ($\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation between model sophistication and logical correctness on these tasks, this shift in error patterns toward human-likeness occurs independently of error rate. These findings challenge the prevailing view that scaling language models naturally obtains normative rationality, suggesting instead a convergence toward human-like cognition inclusive of our characteristic biases and limitations, as we further confirm by demonstrating order-effects in language model reasoning.

</details>


### [60] [Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK](https://arxiv.org/abs/2506.11129)

*Carlos Garcia-Fernandez, Luis Felipe, Monique Shotande, Muntasir Zitu, Aakash Tripathi, Ghulam Rasool, Issam El Naqa, Vivek Rudrapatna, Gilmer Valdes*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, healthcare, clinical trials, machine learning

**Relevance Score:** 9

**TL;DR:** CHECK is a framework that reduces hallucinations in LLMs for healthcare applications, achieving a state-of-the-art performance in clinical question answering.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Hallucinations in large language models pose significant challenges to their reliability and safety in clinical settings.

**Method:** CHECK integrates structured clinical databases with an information theory-based classifier to detect hallucinations, evaluated on clinical trial questions.

**Key Contributions:**

	1. Introduced a continuous-learning framework for LLMs in healthcare.
	2. Achieved state-of-the-art hallucination reduction rates.
	3. Improved performance on clinical benchmarks and USMLE exam.

**Result:** CHECK reduced the hallucination rate of LLama3.3-70B-Instruct from 31% to 0.3% and improved GPT-4o's USMLE passing rate by 5 percentage points to 92.1%.

**Limitations:** 

**Conclusion:** By effectively managing hallucinations, CHECK provides a scalable solution for deploying LLMs in high-stakes medical applications.

**Abstract:** Large language models (LLMs) show promise in healthcare, but hallucinations remain a major barrier to clinical use. We present CHECK, a continuous-learning framework that integrates structured clinical databases with a classifier grounded in information theory to detect both factual and reasoning-based hallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials, CHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% - making an open source model state of the art. Its classifier generalized across medical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE) benchmark and HealthBench realistic multi-turn medical questioning. By leveraging hallucination probabilities to guide GPT-4o's refinement and judiciously escalate compute, CHECK boosted its USMLE passing rate by 5 percentage points, achieving a state-of-the-art 92.1%. By suppressing hallucinations below accepted clinical error thresholds, CHECK offers a scalable foundation for safe LLM deployment in medicine and other high-stakes domains.

</details>


### [61] [A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data](https://arxiv.org/abs/2506.11130)

*Cheng Kang Chou, Chan-Jan Hsu, Ho-Lam Chung, Liang-Hsuan Tseng, Hsi-Chun Cheng, Yu-Kuan Fu, Kuan Po Huang, Hung-Yi Lee*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, self-refining framework, pseudo-labels, text-to-speech, low-resource settings

**Relevance Score:** 6

**TL;DR:** A framework to enhance ASR performance using unlabeled datasets by integrating pseudo-labeling and high-fidelity TTS systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Automatic Speech Recognition (ASR) performance in low-resource or domain-specific settings without the need for labeled data.

**Method:** The proposed method involves using an existing ASR model to generate pseudo-labels from unannotated speech, which are then employed to train a text-to-speech system. The synthesized speech text pairs are fed back into the ASR system for self-improvement.

**Key Contributions:**

	1. Development of Twister, a specialized ASR model for Mandarin
	2. Demonstration of a closed-loop self-improvement cycle using unlabeled datasets
	3. Significant reduction in error rates for specific language contexts

**Result:** The framework enhanced performance on Taiwanese Mandarin speech, achieving a reduction in error rates of up to 20% for Mandarin and 50% for Mandarin-English code-switching compared to the baseline model, Whisper.

**Limitations:** 

**Conclusion:** The self-refining framework provides an effective alternative to traditional pseudo-labeling and self-distillation methods, particularly advantageous in low-resource environments.

**Abstract:** We propose a self-refining framework that enhances ASR performance with only unlabeled datasets. The process starts with an existing ASR model generating pseudo-labels on unannotated speech, which are then used to train a high-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs are bootstrapped into the original ASR system, completing the closed-loop self-improvement cycle. We demonstrated the effectiveness of the framework on Taiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a moderate amount of text data, and synthetic content from the AI models, we adapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error rates by up to 20% on Mandarin and 50% on Mandarin-English code-switching benchmarks compared to Whisper. Results highlight the framework as a compelling alternative to pseudo-labeling self-distillation approaches and provides a practical pathway for improving ASR performance in low-resource or domain-specific settings.

</details>


### [62] [Large Language Models and Emergence: A Complex Systems Perspective](https://arxiv.org/abs/2506.11135)

*David C. Krakauer, John W. Krakauer, Melanie Mitchell*

**Main category:** cs.CL

**Keywords:** emergence, Large Language Models, intelligence, complexity science, emergent capabilities

**Relevance Score:** 7

**TL;DR:** This paper explores the notion of emergence in complexity science as it relates to intelligence, particularly in Large Language Models (LLMs), examining claims of emergent capabilities and intelligence in LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether Large Language Models demonstrate emergent capabilities and intelligence, as described in complexity science.

**Method:** The paper reviews various approaches to quantifying emergence in many-body systems and applies these concepts to analyze LLMs.

**Key Contributions:**

	1. Investigates the relationship between emergence and intelligence in LLMs.
	2. Reviews methods of quantifying emergence relevant to LLMs.
	3. Challenges existing claims of emergent intelligence in LLMs.

**Result:** The analysis reveals complexities in defining and measuring emergent capabilities in LLMs, with varied findings regarding their intelligence.

**Limitations:** The study highlights the difficulties in defining and measuring emergence in complex systems, especially in the context of LLMs.

**Conclusion:** Emergent capabilities in LLMs may not necessarily equate to true emergent intelligence, warranting further investigation into the implications of these findings.

**Abstract:** Emergence is a concept in complexity science that describes how many-body systems manifest novel higher-level properties, properties that can be described by replacing high-dimensional mechanisms with lower-dimensional effective variables and theories. This is captured by the idea "more is different". Intelligence is a consummate emergent property manifesting increasingly efficient -- cheaper and faster -- uses of emergent capabilities to solve problems. This is captured by the idea "less is more". In this paper, we first examine claims that Large Language Models exhibit emergent capabilities, reviewing several approaches to quantifying emergence, and secondly ask whether LLMs possess emergent intelligence.

</details>


### [63] [Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models](https://arxiv.org/abs/2506.11137)

*Chong Shao, Douglas Snyder, Chiran Li, Bowen Gu, Kerry Ngan, Chun-Ting Yang, Jiageng Wu, Richard Wyss, Kueiyu Joshua Lin, Jie Yang*

**Main category:** cs.CL

**Keywords:** medication extraction, EHR, large language models, machine learning, health informatics

**Relevance Score:** 9

**TL;DR:** This paper evaluates the effectiveness of advanced LLMs in extracting medication information from EHR notes, finding strong performance, particularly from GPT-4o and open-sourced models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve patient safety by accurately identifying medication discontinuations in electronic health records (EHRs), which is often complicated by unstructured note formats.

**Method:** The study involved an evaluation of 12 advanced LLMs on three EHR datasets, using various prompting strategies and comparing performance in medication extraction and status classification.

**Key Contributions:**

	1. Evaluation of 12 advanced LLMs for medication extraction from EHRs
	2. Identification of effective prompting strategies for LLMs
	3. Demonstration of the potential of few-shot learning to enhance LLM performance

**Result:** GPT-4o achieved the highest average F1 scores in medication extraction (94.0%) and classification tasks under zero-shot settings, while open-sourced models like Llama-3.1-70B-Instruct also performed competitively in specific categories.

**Limitations:** 

**Conclusion:** Advanced LLMs exhibit strong potential for accurately extracting medication information and identifying discontinuations in EHR notes, with open-sourced models presenting scalable alternatives to proprietary solutions.

**Abstract:** Identifying medication discontinuations in electronic health records (EHRs) is vital for patient safety but is often hindered by information being buried in unstructured notes. This study aims to evaluate the capabilities of advanced open-sourced and proprietary large language models (LLMs) in extracting medications and classifying their medication status from EHR notes, focusing on their scalability on medication information extraction without human annotation. We collected three EHR datasets from diverse sources to build the evaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM prompting strategies. Performance on medication extraction, medication status classification, and their joint task (extraction then classification) was systematically compared across all experiments. We found that LLMs showed promising performance on the medication extraction and discontinuation classification from EHR notes. GPT-4o consistently achieved the highest average F1 scores in all tasks under zero-shot setting - 94.0% for medication extraction, 78.1% for discontinuation classification, and 72.7% for the joint task. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the highest performance in medication status classification on the MIV-Med dataset (68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%) datasets. Medical-specific LLMs demonstrated lower performance compared to advanced general-domain LLMs. Few-shot learning generally improved performance, while CoT reasoning showed inconsistent gains. LLMs demonstrate strong potential for medication extraction and discontinuation identification on EHR notes, with open-sourced models offering scalable alternatives to proprietary systems and few-shot can further improve LLMs' capability.

</details>


### [64] [RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?](https://arxiv.org/abs/2506.11243)

*Santiago Góngora, Ignacio Sastre, Santiago Robaina, Ignacio Remersaro, Luis Chiruzzo, Aiala Rosá*

**Main category:** cs.CL

**Keywords:** NLP, Low-resource models, F1 score, BEA 2025, Educational applications

**Relevance Score:** 7

**TL;DR:** The RETUYT-INCO team achieved competitive results in the BEA 2025 shared task using models under 1B parameters, demonstrating the viability of smaller models in low-resource settings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of small models in natural language processing tasks, particularly in contexts with limited computational resources, typical of institutions in the Global South.

**Method:** The team utilized models with fewer than 1 billion parameters to participate in various tracks of the BEA 2025 shared task, benchmarking against teams with larger models.

**Key Contributions:**

	1. Demonstrated the competitiveness of small models in NLP tasks
	2. Provided benchmarks for models under 1B parameters in shared tasks
	3. Highlighted opportunities for low-resource institutions in NLP research

**Result:** The team's models were competitive, with performance gaps of 6.46 to 13.13 points in F1 scores compared to the winning teams across five tracks.

**Limitations:** The study focuses solely on models under 1B parameters, which may not generalize to other model sizes or settings.

**Conclusion:** Models under 1 billion parameters can effectively compete in specific NLP tasks, proving beneficial for low-resource environments.

**Abstract:** In this paper, we present the RETUYT-INCO participation at the BEA 2025 shared task. Our participation was characterized by the decision of using relatively small models, with fewer than 1B parameters. This self-imposed restriction tries to represent the conditions in which many research labs or institutions are in the Global South, where computational power is not easily accessible due to its prohibitive cost. Even under this restrictive self-imposed setting, our models managed to stay competitive with the rest of teams that participated in the shared task. According to the $exact\ F_1$ scores published by the organizers, the performance gaps between our models and the winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in Track 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the minimum difference with a winner team is $6.46$ points -- and the maximum difference is $13.13$ -- according to the $exact\ F_1$ score, we find that models with a size smaller than 1B parameters are competitive for these tasks, all of which can be run on computers with a low-budget GPU or even without a GPU.

</details>


### [65] [Iterative Multilingual Spectral Attribute Erasure](https://arxiv.org/abs/2506.11244)

*Shun Shao, Yftah Ziser, Zheng Zhao, Yifu Qiu, Shay B. Cohen, Anna Korhonen*

**Main category:** cs.CL

**Keywords:** multilingual representation, debiasing, iterative SVD, bias mitigation, language models

**Relevance Score:** 8

**TL;DR:** This paper introduces IMSAE, a method for debiasing multilingual representations using iterative SVD-based truncation to mitigate bias across languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to transfer debiasing effects between languages to enhance multilingual representations and improve model fairness.

**Method:** IMSAE utilizes iterative SVD-based truncation to identify and mitigate joint bias subspaces across multiple languages, evaluated in both standard and zero-shot settings.

**Key Contributions:**

	1. Introduction of IMSAE for joint bias mitigation in multilingual representations.
	2. Demonstration of effectiveness in standard and zero-shot settings across multiple languages.
	3. Comparative evaluation showing outperforming of traditional approaches.

**Result:** IMSAE demonstrates effectiveness across eight languages and five demographic dimensions, outperforming traditional monolingual and cross-lingual approaches.

**Limitations:** 

**Conclusion:** IMSAE effectively reduces biases in multilingual embeddings while preserving model utility, offering a significant improvement over existing methods.

**Abstract:** Multilingual representations embed words with similar meanings to share a common semantic space across languages, creating opportunities to transfer debiasing effects between languages. However, existing methods for debiasing are unable to exploit this opportunity because they operate on individual languages. We present Iterative Multilingual Spectral Attribute Erasure (IMSAE), which identifies and mitigates joint bias subspaces across multiple languages through iterative SVD-based truncation. Evaluating IMSAE across eight languages and five demographic dimensions, we demonstrate its effectiveness in both standard and zero-shot settings, where target language data is unavailable, but linguistically similar languages can be used for debiasing. Our comprehensive experiments across diverse language models (BERT, LLaMA, Mistral) show that IMSAE outperforms traditional monolingual and cross-lingual approaches while maintaining model utility.

</details>


### [66] [No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning](https://arxiv.org/abs/2506.11246)

*Kushagra Dixit, Abhishek Rajgaria, Harshavardhan Kalalbandi, Dan Roth, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** Large Language Models, prompting techniques, table reasoning, adaptive frameworks, human reasoning

**Relevance Score:** 8

**TL;DR:** This paper investigates the effectiveness of various prompting techniques for table reasoning in Large Language Models (LLMs) and introduces SEAR, an adaptive framework that improves performance across diverse table types.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Temporal Table Reasoning poses a challenge for LLMs, necessitating effective prompting techniques to derive relevant insights from tables, yet existing methods' impacts are largely unexplored.

**Method:** The paper explores multiple prompting techniques across different table types and structures, introducing SEAR as a context-sensitive adaptive framework for improving table reasoning.

**Key Contributions:**

	1. Introduction of SEAR, an adaptive prompting framework for LLMs.
	2. Comprehensive analysis of prompting methods across various table types.
	3. Discovery that a unified table representation improves reasoning performance.

**Result:** SEAR outperforms baseline prompting techniques across all table types, demonstrating that performance is influenced by entity type, table structure, context requirements, and question complexity.

**Limitations:** No single method consistently outperformed others across all tests, highlighting the complexity of table reasoning tasks.

**Conclusion:** An adaptive, structured reasoning approach like SEAR can significantly enhance LLMs' reasoning capabilities with tables, suggesting that there is no one-size-fits-all method, and context plays a crucial role.

**Abstract:** Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective prompting techniques to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, the performance of these models varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique across diverse table types to determine optimal approaches for different scenarios. We find that performance varies based on entity type, table structure, requirement of additional context and question complexity, with NO single method consistently outperforming others. To mitigate these challenges, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts based on context characteristics and integrates a structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to other baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model's reasoning.

</details>


### [67] [Learning a Continue-Thinking Token for Enhanced Test-Time Scaling](https://arxiv.org/abs/2506.11274)

*Liran Ringel, Elad Tolochinsky, Yaniv Romano*

**Main category:** cs.CL

**Keywords:** language models, inference, reasoning, reinforcement learning, accuracy

**Relevance Score:** 8

**TL;DR:** This paper explores a learned continue-thinking token for extending reasoning in language models, showing improved accuracy over fixed-token approaches.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language model performance at inference time by extending reasoning steps using tokens.

**Method:** The authors use a distilled version of DeepSeek-R1, training a learned '<|continue-thinking|>' token's embedding via reinforcement learning with frozen model weights.

**Key Contributions:**

	1. Introduction of a learned continue-thinking token
	2. Comparison with fixed-token strategies
	3. Demonstrated improvements on math benchmarks

**Result:** The learned token shows greater improvement in accuracy on math benchmarks compared to a baseline and a fixed-token approach. For example, a 4.2% improvement on the GSM8K benchmark versus a 1.3% improvement from the fixed token.

**Limitations:** 

**Conclusion:** A dedicated learned continue-thinking token effectively enhances reasoning capabilities and leads to significant gains in model accuracy during inference.

**Abstract:** Test-time scaling has emerged as an effective approach for improving language model performance by utilizing additional compute at inference time. Recent studies have shown that overriding end-of-thinking tokens (e.g., replacing "</think>" with "Wait") can extend reasoning steps and improve accuracy. In this work, we explore whether a dedicated continue-thinking token can be learned to trigger extended reasoning. We augment a distilled version of DeepSeek-R1 with a single learned "<|continue-thinking|>" token, training only its embedding via reinforcement learning while keeping the model weights frozen. Our experiments show that this learned token achieves improved accuracy on standard math benchmarks compared to both the baseline model and a test-time scaling approach that uses a fixed token (e.g., "Wait") for budget forcing. In particular, we observe that in cases where the fixed-token approach enhances the base model's accuracy, our method achieves a markedly greater improvement. For example, on the GSM8K benchmark, the fixed-token approach yields a 1.3% absolute improvement in accuracy, whereas our learned-token method achieves a 4.2% improvement over the base model that does not use budget forcing.

</details>


### [68] [Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning](https://arxiv.org/abs/2506.11300)

*Yang Zhang, Amr Mohamed, Hadi Abdine, Guokan Shang, Michalis Vazirgiannis*

**Main category:** cs.CL

**Keywords:** curriculum learning, language models, training efficiency, pretraining, data ordering

**Relevance Score:** 8

**TL;DR:** This paper investigates the benefits of curriculum learning in pretraining language models, demonstrating improved training efficiency and generalization across multiple settings and benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underutilized potential of curriculum learning in pretraining language models, given its success in other machine learning domains.

**Method:** The study experimented with different forms of curriculum learning, including vanilla curriculum learning, pacing-based sampling, and interleaved curricula, using various difficulty metrics—linguistic and information-theoretic—to train models.

**Key Contributions:**

	1. First systematic investigation of curriculum learning in language model pretraining.
	2. Identification of effective difficulty metrics such as compression ratio, lexical diversity, and readability.
	3. Demonstration of improved training efficiency and generalization through curriculum learning.

**Result:** Curriculum learning was found to improve convergence during the early and mid-training phases of language models, leading to substantial gains in performance, with up to a 3.5% improvement when used as a warmup strategy.

**Limitations:** 

**Conclusion:** The research emphasizes the significance of data ordering in large-scale pretraining, revealing effective difficulty signals and providing guidance for more efficient model training.

**Abstract:** Curriculum learning has shown promise in improving training efficiency and generalization in various machine learning domains, yet its potential in pretraining language models remains underexplored, prompting our work as the first systematic investigation in this area. We experimented with different settings, including vanilla curriculum learning, pacing-based sampling, and interleaved curricula-guided by six difficulty metrics spanning linguistic and information-theoretic perspectives. We train models under these settings and evaluate their performance on eight diverse benchmarks. Our experiments reveal that curriculum learning consistently improves convergence in early and mid-training phases, and can yield lasting gains when used as a warmup strategy with up to $3.5\%$ improvement. Notably, we identify compression ratio, lexical diversity, and readability as effective difficulty signals across settings. Our findings highlight the importance of data ordering in large-scale pretraining and provide actionable insights for scalable, data-efficient model development under realistic training scenarios.

</details>


### [69] [Don't Pay Attention](https://arxiv.org/abs/2506.11305)

*Mohammad Hammoud, Devang Acharya*

**Main category:** cs.CL

**Keywords:** Transformer, RNN, long-range dependencies, NLP benchmarks, neural architecture

**Relevance Score:** 8

**TL;DR:** Avey is a new neural architecture that improves upon Transformers and RNNs by decoupling sequence length from context width, enabling effective processing of long sequences and capturing long-range dependencies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The Transformer is widely used in NLP but struggles with long sequences due to fixed context windows and quadratic attention complexity, leading to a resurgence of interest in RNN-like architectures.

**Method:** Avey introduces a ranker and an autoregressive neural processor that focus on the most relevant tokens for each token in a sequence, independent of their positions.

**Key Contributions:**

	1. Introduction of Avey, a novel neural architecture that circumvents traditional attention and recurrence mechanisms.
	2. Ability to process arbitrarily long sequences by decoupling sequence length from context width.
	3. Experimental validation showing Avey's effectiveness in both short-range and long-range NLP tasks.

**Result:** Avey demonstrates superior performance to Transformers on various short-range NLP benchmarks and shows significant improvement in handling long-range dependencies.

**Limitations:** 

**Conclusion:** The proposed Avey architecture provides an effective alternative to both Transformers and RNNs for processing long sequences in NLP tasks.

**Abstract:** The Transformer has become the de facto standard for large language models and a wide range of downstream tasks across various domains. Despite its numerous advantages like inherent training parallelism, the Transformer still faces key challenges due to its inability to effectively process sequences beyond a fixed context window and the quadratic complexity of its attention mechanism. These challenges have renewed interest in RNN-like architectures, which offer linear scaling with sequence length and improved handling of long-range dependencies, albeit with limited parallelism due to their inherently recurrent nature. In this paper, we propose Avey, a new neural foundational architecture that breaks away from both attention and recurrence. Avey comprises a ranker and an autoregressive neural processor, which collaboratively identify and contextualize only the most relevant tokens for any given token, regardless of their positions in the sequence. Specifically, Avey decouples sequence length from context width, thus enabling effective processing of arbitrarily long sequences. Experimental results show that Avey compares favorably to the Transformer across a variety of standard short-range NLP benchmarks, while notably excelling at capturing long-range dependencies.

</details>


### [70] [Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly](https://arxiv.org/abs/2506.11338)

*Yi-Chien Lin, William Schuler*

**Main category:** cs.CL

**Keywords:** Transformers, surprisal, natural language processing, neural measures, functional MRI

**Relevance Score:** 8

**TL;DR:** This study evaluates the predictive power of surprisal estimates from Transformer-based language models on brain imaging data, finding a continued positive relationship between model perplexity and model fit.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research investigates the predictive power of surprisal from Transformer models beyond latency-based measures in the context of human sentence processing, focusing on brain imaging data.

**Method:** The study assesses 17 pre-trained Transformer-based models across three language families and evaluates their surprisal estimates against two functional magnetic resonance imaging datasets.

**Key Contributions:**

	1. Evaluates Transformer model surprisal in the context of brain imaging data.
	2. Demonstrates generalizability of model perplexity trends beyond reading times.
	3. Explores the relationship between language model parameters and human processing difficulty in a novel way.

**Result:** Results indicate that the positive relationship between model perplexity and predictive power holds true for neural measures as well, suggesting generalizability beyond previous latency measures.

**Limitations:** 

**Conclusion:** Surprisal estimates from larger models are less predictive of human processing difficulty, confirming that trends observed in latency measures apply to brain imaging data.

**Abstract:** As Transformers become more widely incorporated into natural language processing tasks, there has been considerable interest in using surprisal from these models as predictors of human sentence processing difficulty. Recent work has observed a positive relationship between Transformer-based models' perplexity and the predictive power of their surprisal estimates on reading times, showing that language models with more parameters and trained on more data are less predictive of human reading times. However, these studies focus on predicting latency-based measures (i.e., self-paced reading times and eye-gaze durations) with surprisal estimates from Transformer-based language models. This trend has not been tested on brain imaging data. This study therefore evaluates the predictive power of surprisal estimates from 17 pre-trained Transformer-based models across three different language families on two functional magnetic resonance imaging datasets. Results show that the positive relationship between model perplexity and model fit still obtains, suggesting that this trend is not specific to latency-based measures and can be generalized to neural measures.

</details>


### [71] [From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review](https://arxiv.org/abs/2506.11343)

*Yaohui Zhang, Haijing Zhang, Wenlong Ji, Tianyu Hua, Nick Haber, Hancheng Cao, Weixin Liang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Peer Review, Pairwise Comparison, Academic Publishing, Bias

**Relevance Score:** 9

**TL;DR:** This paper explores a novel approach to peer review using LLM agents for pairwise comparisons of manuscripts, demonstrating improved accuracy over traditional methods but highlighting issues of bias.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To rethink the academic peer review process leveraging the capabilities of large language models (LLMs) beyond traditional workflows.

**Method:** Utilizes LLM agents to perform pairwise comparisons among manuscripts, aggregating outcomes to assess relative quality instead of individual scoring.

**Key Contributions:**

	1. Introduces pairwise comparisons using LLMs in academic peer review.
	2. Demonstrates significant performance improvements over traditional methods.
	3. Identifies emergent biases in the peer review selection process.

**Result:** This pairwise evaluation approach shows significant improvements in identifying high-impact papers compared to traditional rating systems.

**Limitations:** Emergent biases related to novelty of research topics and institutional imbalance in selection.

**Conclusion:** The study notes both the potential for transformation in peer review through LLMs and the need to address biases that may emerge, such as reduced novelty in topics and institutional imbalances.

**Abstract:** The advent of large language models (LLMs) offers unprecedented opportunities to reimagine peer review beyond the constraints of traditional workflows. Despite these opportunities, prior efforts have largely focused on replicating traditional review workflows with LLMs serving as direct substitutes for human reviewers, while limited attention has been given to exploring new paradigms that fundamentally rethink how LLMs can participate in the academic review process. In this paper, we introduce and explore a novel mechanism that employs LLM agents to perform pairwise comparisons among manuscripts instead of individual scoring. By aggregating outcomes from substantial pairwise evaluations, this approach enables a more accurate and robust measure of relative manuscript quality. Our experiments demonstrate that this comparative approach significantly outperforms traditional rating-based methods in identifying high-impact papers. However, our analysis also reveals emergent biases in the selection process, notably a reduced novelty in research topics and an increased institutional imbalance. These findings highlight both the transformative potential of rethinking peer review with LLMs and critical challenges that future systems must address to ensure equity and diversity.

</details>


### [72] [Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models](https://arxiv.org/abs/2506.11344)

*Peilin Wu, Jinho D. Choi*

**Main category:** cs.CL

**Keywords:** Speaker Diarization, Text-based model, Semantic understanding

**Relevance Score:** 6

**TL;DR:** A novel text-based approach to Speaker Diarization (SD) that focuses on Sentence-level Speaker Change Detection, using dialogue transcripts instead of audio.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of audio-based speaker diarization systems, which struggle with audio quality and speaker similarity.

**Method:** Development of two models: Single Prediction Model (SPM) and Multiple Prediction Model (MPM) that utilize dialogue transcripts for identifying speaker changes.

**Key Contributions:**

	1. Introduction of a text-based diarization approach
	2. Development of SPM and MPM models
	3. Validation of the MPM's superior performance in short conversations

**Result:** The text-based approach, particularly the MPM model, shows significant improvements in detecting speaker changes, especially in short conversations, and competes well with traditional audio-based systems.

**Limitations:** 

**Conclusion:** This work demonstrates the effectiveness of linguistic features in speaker diarization and emphasizes the integration of semantic understanding, suggesting new research directions for multimodal diarization.

**Abstract:** We present a novel approach to Speaker Diarization (SD) by leveraging text-based methods focused on Sentence-level Speaker Change Detection within dialogues. Unlike audio-based SD systems, which are often challenged by audio quality and speaker similarity, our approach utilizes the dialogue transcript alone. Two models are developed: the Single Prediction Model (SPM) and the Multiple Prediction Model (MPM), both of which demonstrate significant improvements in identifying speaker changes, particularly in short conversations. Our findings, based on a curated dataset encompassing diverse conversational scenarios, reveal that the text-based SD approach, especially the MPM, performs competitively against state-of-the-art audio-based SD systems, with superior performance in short conversational contexts. This paper not only showcases the potential of leveraging linguistic features for SD but also highlights the importance of integrating semantic understanding into SD systems, opening avenues for future research in multimodal and semantic feature-based diarization.

</details>


### [73] [The Biased Samaritan: LLM biases in Perceived Kindness](https://arxiv.org/abs/2506.11361)

*Jack H Fagan, Ruhaan Juyaal, Amy Yue-Ming Yu, Siya Pun*

**Main category:** cs.CL

**Keywords:** Large Language Models, bias assessment, demographic evaluation, generative AI, moral intervention

**Relevance Score:** 9

**TL;DR:** This paper presents a method for evaluating demographic biases in Large Language Models (LLMs) by analyzing their responses to moral interventions across various demographics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding and mitigating biases in LLMs is crucial for responsible AI development, impacting fairness and equity in AI outputs.

**Method:** The authors evaluate LLM biases by prompting models to assess a moral patient's willingness to intervene constructively across different demographics (genders, races, ages).

**Key Contributions:**

	1. Novel method for evaluating LLM demographic biases
	2. Identification of baseline demographic perceptions
	3. Insight into altruism across different demographics

**Result:** The analysis revealed that LLMs perceive the baseline demographic as predominantly white, male, and middle-aged, while non-baseline demographics were generally more altruistic.

**Limitations:** The study primarily focuses on moral intervention scenarios and may not encompass all bias types or contexts.

**Conclusion:** The findings provide insights into LLM biases and suggest methodological approaches for future evaluations, emphasizing the need to address demographic biases in AI training and outputs.

**Abstract:** While Large Language Models (LLMs) have become ubiquitous in many fields, understanding and mitigating LLM biases is an ongoing issue. This paper provides a novel method for evaluating the demographic biases of various generative AI models. By prompting models to assess a moral patient's willingness to intervene constructively, we aim to quantitatively evaluate different LLMs' biases towards various genders, races, and ages. Our work differs from existing work by aiming to determine the baseline demographic identities for various commercial models and the relationship between the baseline and other demographics. We strive to understand if these biases are positive, neutral, or negative, and the strength of these biases. This paper can contribute to the objective assessment of bias in Large Language Models and give the user or developer the power to account for these biases in LLM output or in training future LLMs. Our analysis suggested two key findings: that models view the baseline demographic as a white middle-aged or young adult male; however, a general trend across models suggested that non-baseline demographics are more willing to help than the baseline. These methodologies allowed us to distinguish these two biases that are often tangled together.

</details>


### [74] [A Variational Approach for Mitigating Entity Bias in Relation Extraction](https://arxiv.org/abs/2506.11381)

*Samuel Mensah, Elena Kochkina, Jabez Magomere, Joy Prakash Sain, Simerjot Kaur, Charese Smiley*

**Main category:** cs.CL

**Keywords:** Relation Extraction, Entity Bias, Variational Information Bottleneck, Generalization, Biomedical

**Relevance Score:** 6

**TL;DR:** This paper introduces a Variational Information Bottleneck framework for mitigating entity bias in Relation Extraction, leading to improved generalization and state-of-the-art performance across various domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of entity bias in Relation Extraction, which hampers model generalization.

**Method:** The paper adapts a Variational Information Bottleneck framework that compresses entity-specific information while retaining task-relevant features.

**Key Contributions:**

	1. Introduction of a Variational Information Bottleneck framework for Relation Extraction
	2. Demonstrated state-of-the-art performance in various domains
	3. Robustness and theoretical grounding of the proposed methodology.

**Result:** The proposed method demonstrates state-of-the-art performance on relation extraction datasets in general, financial, and biomedical domains, across both in-domain and out-of-domain settings.

**Limitations:** 

**Conclusion:** The approach provides a robust and interpretable solution to mitigate entity bias in Relation Extraction.

**Abstract:** Mitigating entity bias is a critical challenge in Relation Extraction (RE), where models often rely excessively on entities, resulting in poor generalization. This paper presents a novel approach to address this issue by adapting a Variational Information Bottleneck (VIB) framework. Our method compresses entity-specific information while preserving task-relevant features. It achieves state-of-the-art performance on relation extraction datasets across general, financial, and biomedical domains, in both indomain (original test sets) and out-of-domain (modified test sets with type-constrained entity replacements) settings. Our approach offers a robust, interpretable, and theoretically grounded methodology.

</details>


### [75] [Curriculum-Guided Layer Scaling for Language Model Pretraining](https://arxiv.org/abs/2506.11389)

*Karanpartap Singh, Neil Band, Ehsan Adeli*

**Main category:** cs.CL

**Keywords:** Curriculum learning, Layer scaling, Model pretraining, Generalization, Zero-shot learning

**Relevance Score:** 8

**TL;DR:** Curriculum-Guided Layer Scaling (CGLS) enhances the efficiency of large language model pretraining by synchronizing data difficulty with model growth through progressive layer addition, resulting in improved generalization and performance on downstream benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve learning efficiency in pretraining large language models, inspired by cognitive development in humans.

**Method:** Curriculum-Guided Layer Scaling (CGLS) that progressively adds layers and transitions from simple to complex data during training.

**Key Contributions:**

	1. Introduced Curriculum-Guided Layer Scaling (CGLS) for pretraining efficiency.
	2. Demonstrated improved zero-shot performance using progressively complex datasets.
	3. Showed that model depth can be increased in correlation with data difficulty for better generalization.

**Result:** CGLS outperforms baseline methods on benchmarks like PIQA and ARC, demonstrating improved generalization and zero-shot performance.

**Limitations:** 

**Conclusion:** The findings support CGLS as an effective strategy for enhancing performance on knowledge-intensive tasks through progressive stacking of model layers.

**Abstract:** As the cost of pretraining large language models grows, there is continued interest in strategies to improve learning efficiency during this core training stage. Motivated by cognitive development, where humans gradually build knowledge as their brains mature, we propose Curriculum-Guided Layer Scaling (CGLS), a framework for compute-efficient pretraining that synchronizes increasing data difficulty with model growth through progressive layer stacking (i.e. gradually adding layers during training). At the 100M parameter scale, using a curriculum transitioning from synthetic short stories to general web data, CGLS outperforms baseline methods on the question-answering benchmarks PIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus with a DistilBERT-based classifier and progress from general text to highly technical or specialized content. Our results show that progressively increasing model depth alongside sample difficulty leads to better generalization and zero-shot performance on various downstream benchmarks. Altogether, our findings demonstrate that CGLS unlocks the potential of progressive stacking, offering a simple yet effective strategy for improving generalization on knowledge-intensive and reasoning tasks.

</details>


### [76] [Predicting Early-Onset Colorectal Cancer with Large Language Models](https://arxiv.org/abs/2506.11410)

*Wilson Lau, Youngwon Kim, Sravanthi Parasa, Md Enamul Haque, Anand Oka, Jay Nanduri*

**Main category:** cs.CL

**Keywords:** Colorectal Cancer, Early-Onset Cancer, Machine Learning, Health Informatics, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper presents a study on using machine learning models, including fine-tuned large language models, to predict early-onset colorectal cancer (EoCRC) in a population younger than typical screening ages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing incidence of early-onset colorectal cancer (EoCRC) in younger populations necessitates improving prediction methods for early detection and intervention.

**Method:** Applied 10 different machine learning models to predict EoCRC using patient conditions, lab results, and observations within 6 months prior to CRC diagnosis, comparing their performance against advanced large language models (LLM).

**Key Contributions:**

	1. Demonstration of LLM effectiveness in predicting EoCRC.
	2. Comparison of traditional ML models with LLMs in a healthcare context.
	3. Application of machine learning to a significant, emerging public health issue.

**Result:** The fine-tuned LLM achieved an average sensitivity of 73% and specificity of 91% in predicting EoCRC from data collected prior to CRC diagnosis.

**Limitations:** The study was retrospective, and results may vary across diverse populations and healthcare systems.

**Conclusion:** The study highlights the potential of using advanced LLMs for improving EoCRC prediction, suggesting their utility in clinical settings for early diagnosis.

**Abstract:** The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has increased every year, but this population is younger than the recommended age established by national guidelines for cancer screening. In this paper, we applied 10 different machine learning models to predict EoCRC, and compared their performance with advanced large language models (LLM), using patient conditions, lab results, and observations within 6 months of patient journey prior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients from multiple health systems across the United States. The results demonstrated that the fine-tuned LLM achieved an average of 73% sensitivity and 91% specificity.

</details>


### [77] [Efficient Long-Context LLM Inference via KV Cache Clustering](https://arxiv.org/abs/2506.11418)

*Jie Hu, Shengnan Wang, Yutong He, Ping Gong, Jiawei Yi, Juncheng Zhang, Youhui Bai, Renhai Chen, Gong Zhang, Cheng Li, Kun Yuan*

**Main category:** cs.CL

**Keywords:** large language models, KV cache, online clustering, machine learning, inference acceleration

**Relevance Score:** 8

**TL;DR:** Chelsea introduces a framework for efficient online KV cache clustering in long-context LLMs, achieving significant memory savings and improved inference speed.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address deployment challenges posed by the substantial KV cache required for long-context LLMs, while retaining important information for future generations.

**Method:** The Chelsea framework uses a Chunked Soft Matching approach that segments the sequence into chunks and identifies clusters based on similarity, then merges KV cache within each cluster into a single centroid.

**Key Contributions:**

	1. Introduction of Chelsea framework for KV cache clustering
	2. Chunked Soft Matching for efficient clustering
	3. Theoretical analysis of computational complexity and optimality

**Result:** Chelsea significantly reduces KV cache memory usage by up to 80% and accelerates decoding speed by up to 3.19x with minimal computational overhead.

**Limitations:** 

**Conclusion:** Chelsea provides an efficient solution for optimizing KV cache in long-context LLMs while preserving model performance and reducing latency.

**Abstract:** Large language models (LLMs) with extended context windows have become increasingly prevalent for tackling complex tasks. However, the substantial Key-Value (KV) cache required for long-context LLMs poses significant deployment challenges. Existing approaches either discard potentially critical information needed for future generations or offer limited efficiency gains due to high computational overhead. In this paper, we introduce Chelsea, a simple yet effective framework for online KV cache clustering. Our approach is based on the observation that key states exhibit high similarity along the sequence dimension. To enable efficient clustering, we divide the sequence into chunks and propose Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity. Chelsea then merges the KV cache within each cluster into a single centroid. Additionally, we provide a theoretical analysis of the computational complexity and the optimality of the intra-chunk partitioning strategy. Extensive experiments across various models and long-context benchmarks demonstrate that Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance. Moreover, with minimal computational overhead, Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and reduces end-to-end latency by up to 2.72$\times$.

</details>


### [78] [Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards](https://arxiv.org/abs/2506.11425)

*Jeff Da, Clinton Wang, Xiang Deng, Yuntao Ma, Nikhil Barhate, Sean Hendryx*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human-Computer Interaction, Large Language Models, Software Engineering, Agent Guidance

**Relevance Score:** 8

**TL;DR:** Agent-RLVR introduces a novel framework to enhance Reinforcement Learning from Verifiable Rewards, making it effective in agentic environments with multi-step problem solving, focusing on software engineering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional RLVR methods struggle in complex settings due to sparse reward landscapes. Agent-RLVR aims to improve agent performance in these environments by introducing a guidance mechanism.

**Method:** The framework employs agent guidance, leveraging diverse informational cues to help steer agents toward successful trajectories and actively enhance their problem-solving abilities through a structured training loop.

**Key Contributions:**

	1. Introduces agent guidance for effective reinforcement learning in complex environments.
	2. Demonstrates significant improvement in agent performance on software engineering tasks.
	3. Lays groundwork for future research in training agents with RL in challenging domains.

**Result:** Agent-RLVR significantly improved the performance of the Qwen-2.5-72B-Instruct model, increasing pass rates from 9.4% to 22.4% on the SWE-Bench Verified and further to 27.8% with additional training.

**Limitations:** 

**Conclusion:** Agent-RLVR sets the foundation for more effective training of agents in complex real-world scenarios where traditional RL methods face challenges.

**Abstract:** Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted as the de facto method for enhancing the reasoning capabilities of large language models and has demonstrated notable success in verifiable domains like math and competitive programming tasks. However, the efficacy of RLVR diminishes significantly when applied to agentic environments. These settings, characterized by multi-step, complex problem solving, lead to high failure rates even for frontier LLMs, as the reward landscape is too sparse for effective model training via conventional RLVR. In this work, we introduce Agent-RLVR, a framework that makes RLVR effective in challenging agentic settings, with an initial focus on software engineering tasks. Inspired by human pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively steers the agent towards successful trajectories by leveraging diverse informational cues. These cues, ranging from high-level strategic plans to dynamic feedback on the agent's errors and environmental interactions, emulate a teacher's guidance, enabling the agent to navigate difficult solution spaces and promotes active self-improvement via additional environment exploration. In the Agent-RLVR training loop, agents first attempt to solve tasks to produce initial trajectories, which are then validated by unit tests and supplemented with agent guidance. Agents then reattempt with guidance, and the agent policy is updated with RLVR based on the rewards of these guided trajectories. Agent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4% to 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data is additionally useful for test-time reward model training, shown by further boosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents with RLVR in complex, real-world environments where conventional RL methods struggle.

</details>


### [79] [KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models](https://arxiv.org/abs/2506.11432)

*Taeeun Kim, Semin Jeong, Youngsook Song*

**Main category:** cs.CL

**Keywords:** Korean GEC, NLLB, grammatical error correction, NLP, language models

**Relevance Score:** 4

**TL;DR:** KoGEC is a Korean Grammatical Error Correction system that outperforms larger LLMs like GPT-4 in correcting Korean sentences.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a specialized error correction system for Korean that outperforms general-purpose language models.

**Method:** Fine-tuning of NLLB models for Korean GEC using two social media datasets, evaluated via BLEU scores and LLM-based error classification.

**Key Contributions:**

	1. Introduction of KoGEC as a Korean GEC system
	2. Demonstration of fine-tuned NLLB models outperforming other LLMs in GEC
	3. Development of a Chrome extension for accessibility

**Result:** KoGEC models demonstrated superior performance in Korean grammatical error correction, particularly in balancing different types of errors, compared to GPT-4 and HCX-3.

**Limitations:** Exploration of token vocabulary expansion decreased model performance.

**Conclusion:** KoGEC provides an efficient and specialized GEC solution, showing that compact models can outperform larger LLMs in specific tasks.

**Abstract:** This research introduces KoGEC, a Korean Grammatical Error Correction system using pre\--trained translation models. We fine-tuned NLLB (No Language Left Behind) models for Korean GEC, comparing their performance against large language models like GPT-4 and HCX-3. The study used two social media conversation datasets for training and testing. The NLLB models were fine-tuned using special language tokens to distinguish between original and corrected Korean sentences. Evaluation was done using BLEU scores and an "LLM as judge" method to classify error types. Results showed that the fine-tuned NLLB (KoGEC) models outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a more balanced error correction profile across various error types, whereas the larger LLMs tended to focus less on punctuation errors. We also developed a Chrome extension to make the KoGEC system accessible to users. Finally, we explored token vocabulary expansion to further improve the model but found it to decrease model performance. This research contributes to the field of NLP by providing an efficient, specialized Korean GEC system and a new evaluation method. It also highlights the potential of compact, task-specific models to compete with larger, general-purpose language models in specialized NLP tasks.

</details>


### [80] [AbsenceBench: Language Models Can't Tell What's Missing](https://arxiv.org/abs/2506.11440)

*Harvey Yiyun Fu, Aryan Shrivastava, Jared Moore, Peter West, Chenhao Tan, Ari Holtzman*

**Main category:** cs.CL

**Keywords:** large language models, missing information, AbsenceBench, Transformer attention, benchmark

**Relevance Score:** 8

**TL;DR:** This paper introduces AbsenceBench, a benchmark for assessing LLMs' ability to detect missing information in various contexts.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the limitations of large language models in identifying deliberately omitted information, despite their success in recalling surprising data.

**Method:** AbsenceBench evaluates models by asking them to determine which pieces of text have been removed from documents across three domains: numerical sequences, poetry, and GitHub pull requests, using original and edited contexts.

**Key Contributions:**

	1. Introduction of AbsenceBench for missing information detection
	2. Analysis of LLM performance in identifying absences
	3. Discussion of Transformer model limitations in attention mechanisms

**Result:** State-of-the-art models achieved only a 69.6% F1-score, indicating a significant shortcoming in their ability to detect absent information.

**Limitations:** The benchmark may not cover all potential types of missing information and is limited to the specified domains.

**Conclusion:** The study highlights a fundamental limitation of Transformer mechanisms, suggesting that they struggle to attend to 'gaps' in information due to the nature of their attention mechanism.

**Abstract:** Large language models (LLMs) are increasingly capable of processing long inputs and locating specific information within them, as evidenced by their performance on the Needle in a Haystack (NIAH) test. However, while models excel at recalling surprising information, they still struggle to identify clearly omitted information. We introduce AbsenceBench to assesses LLMs' capacity to detect missing information across three domains: numerical sequences, poetry, and GitHub pull requests. AbsenceBench asks models to identify which pieces of a document were deliberately removed, given access to both the original and edited contexts. Despite the apparent straightforwardness of these tasks, our experiments reveal that even state-of-the-art models like Claude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context length of 5K tokens. Our analysis suggests this poor performance stems from a fundamental limitation: Transformer attention mechanisms cannot easily attend to "gaps" in documents since these absences don't correspond to any specific keys that can be attended to. Overall, our results and analysis provide a case study of the close proximity of tasks where models are already superhuman (NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).

</details>


### [81] [A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems](https://arxiv.org/abs/2506.11467)

*Carlos Rafael Catalan*

**Main category:** cs.CL

**Keywords:** Machine Translation, Low-Resource Languages, Human Evaluation, Natural Language Processing, Gamified Platform

**Relevance Score:** 8

**TL;DR:** This paper proposes a gamified evaluation platform for Machine Translation systems in low-resource languages, addressing the shortage of human evaluators and datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of automated metrics in evaluating Machine Translation systems for low-resource languages, which require expert human evaluators.

**Method:** The paper reviews existing evaluation procedures and proposes a design for a platform aimed at recruiting evaluators and providing datasets for MT systems.

**Key Contributions:**

	1. Comprehensive review of evaluation procedures for MT systems in low-resource contexts
	2. Design proposal for a gamified evaluation platform
	3. Discussion of challenges in its evaluation and applications in NLP

**Result:** The proposed design is a recruitment and gamified evaluation platform that could help fill the resource gap for MT systems targeting low-resource languages.

**Limitations:** Challenges in evaluating the effectiveness of the proposed platform.

**Conclusion:** The platform has potential applications in Natural Language Processing research, but challenges in evaluating its effectiveness are noted.

**Abstract:** Human evaluators provide necessary contributions in evaluating large language models. In the context of Machine Translation (MT) systems for low-resource languages (LRLs), this is made even more apparent since popular automated metrics tend to be string-based, and therefore do not provide a full picture of the nuances of the behavior of the system. Human evaluators, when equipped with the necessary expertise of the language, will be able to test for adequacy, fluency, and other important metrics. However, the low resource nature of the language means that both datasets and evaluators are in short supply. This presents the following conundrum: How can developers of MT systems for these LRLs find adequate human evaluators and datasets? This paper first presents a comprehensive review of existing evaluation procedures, with the objective of producing a design proposal for a platform that addresses the resource gap in terms of datasets and evaluators in developing MT systems. The result is a design for a recruitment and gamified evaluation platform for developers of MT systems. Challenges are also discussed in terms of evaluating this platform, as well as its possible applications in the wider scope of Natural Language Processing (NLP) research.

</details>


### [82] [Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards](https://arxiv.org/abs/2506.11474)

*Jaehoon Yun, Jiwoong Sohn, Jungwoo Park, Hyunjae Kim, Xiangru Tang, Yanjun Shao, Yonghoe Koo, Minhyeok Ko, Qingyu Chen, Mark Gerstein, Michael Moor, Jaewoo Kang*

**Main category:** cs.CL

**Keywords:** large language models, clinical decision-making, retrieval-augmented generation, reasoning errors, medical QA

**Relevance Score:** 9

**TL;DR:** Med-PRM is a framework that improves clinical decision-making by verifying reasoning steps of language models against medical knowledge, achieving state-of-the-art performance in QA and diagnostics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current large language models face challenges in localizing and correcting reasoning errors in clinical decision-making, which is vital for accurate diagnoses and effective patient care.

**Method:** Med-PRM employs retrieval-augmented generation to validate each reasoning step against established medical knowledge bases.

**Key Contributions:**

	1. Introduction of Med-PRM for process reward modeling in clinical decision-making.
	2. Demonstrated state-of-the-art performance on medical QA benchmarks.
	3. Ability to integrate with existing models like Meerkat for improved accuracy.

**Result:** Evaluations demonstrate that Med-PRM outperforms baseline models by up to 13.50% across five medical QA benchmarks and two open-ended diagnostic tasks, achieving over 80% accuracy on MedQA with small models.

**Limitations:** 

**Conclusion:** Med-PRM enhances the reliability of clinical decision-making processes using language models and can be integrated easily with existing policy models.

**Abstract:** Large language models have shown promise in clinical decision making, but current approaches struggle to localize and correct errors at specific steps of the reasoning process. This limitation is critical in medicine, where identifying and addressing reasoning errors is essential for accurate diagnosis and effective patient care. We introduce Med-PRM, a process reward modeling framework that leverages retrieval-augmented generation to verify each reasoning step against established medical knowledge bases. By verifying intermediate reasoning steps with evidence retrieved from clinical guidelines and literature, our model can precisely assess the reasoning quality in a fine-grained manner. Evaluations on five medical QA benchmarks and two open-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art performance, with improving the performance of base models by up to 13.50% using Med-PRM. Moreover, we demonstrate the generality of Med-PRM by integrating it in a plug-and-play fashion with strong policy models such as Meerkat, achieving over 80\% accuracy on MedQA for the first time using small-scale models of 8 billion parameters. Our code and data are available at: https://med-prm.github.io/

</details>


### [83] [ImmunoFOMO: Are Language Models missing what oncologists see?](https://arxiv.org/abs/2506.11478)

*Aman Sinha, Bogdan-Valentin Popescu, Xavier Coubez, Marianne Clausel, Mathieu Constant*

**Main category:** cs.CL

**Keywords:** medical language models, immunotherapy, breast cancer, natural language processing, biomedical research

**Relevance Score:** 8

**TL;DR:** This paper investigates the capabilities of medical language models in identifying specific concepts related to immunotherapy in breast cancer by comparing them with expert clinicians.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is growing interest in the utility of language models (LMs), particularly in biomedical applications, including understanding specific medical concepts.

**Method:** The study compares various language models' performance in identifying hallmarks of immunotherapy in breast cancer abstracts against expert clinical knowledge.

**Key Contributions:**

	1. Comparative analysis of medical language models against clinical expertise.
	2. Highlighting the effectiveness of pre-trained models in identifying specific medical concepts.
	3. Insight into the application of LMs in biomedical NLP tasks.

**Result:** Pre-trained language models demonstrated the potential to outperform larger language models in identifying low-level medical concepts.

**Limitations:** The study is limited to the specific context of immunotherapy in breast cancer and may not generalize to other areas of medicine or broader NLP tasks.

**Conclusion:** The findings suggest that pre-trained LMs can be highly effective in specific biomedical NLP tasks, indicating a need for further exploration in this area.

**Abstract:** Language models (LMs) capabilities have grown with a fast pace over the past decade leading researchers in various disciplines, such as biomedical research, to increasingly explore the utility of LMs in their day-to-day applications. Domain specific language models have already been in use for biomedical natural language processing (NLP) applications. Recently however, the interest has grown towards medical language models and their understanding capabilities. In this paper, we investigate the medical conceptual grounding of various language models against expert clinicians for identification of hallmarks of immunotherapy in breast cancer abstracts. Our results show that pre-trained language models have potential to outperform large language models in identifying very specific (low-level) concepts.

</details>


### [84] [Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models](https://arxiv.org/abs/2506.11485)

*Cole Gawin*

**Main category:** cs.CL

**Keywords:** BERT, relational schemata, semantic tasks, fine-tuning, NLP

**Relevance Score:** 9

**TL;DR:** This paper investigates BERT's ability to encode abstract relational schemata and its classification performance on relation types across various embeddings, highlighting that fine-tuning is necessary for structured understanding of relations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research explores whether BERT's performance in semantic tasks demonstrates genuine conceptual competence or is merely a result of statistical correlations.

**Method:** The study examines internal representations of concept pairs in BERT across different relation types, comparing relational classification performance with the structure of [CLS] token embeddings and assessing the effects of fine-tuning.

**Key Contributions:**

	1. Investigates encoding of relational schemata in BERT
	2. Demonstrates necessity of fine-tuning for relational understanding
	3. Clarifies distinction between empirical performance and structured competence

**Result:** Pretrained BERT shows high classification accuracy, but structured organization of concept pairs by relation type occurs only after fine-tuning on supervised tasks, indicating that the relational understanding is task-induced rather than inherent.

**Limitations:** The paper focuses on BERT and may not generalize to other models; fine-tuning requirements might limit practical applications.

**Conclusion:** Behavioral performance of BERT does not guarantee a structured conceptual understanding; nevertheless, with suitable training, BERT can develop biases that promote grounded relational abstraction.

**Abstract:** While large language models like BERT demonstrate strong empirical performance on semantic tasks, whether this reflects true conceptual competence or surface-level statistical association remains unclear. I investigate whether BERT encodes abstract relational schemata by examining internal representations of concept pairs across taxonomic, mereological, and functional relations. I compare BERT's relational classification performance with representational structure in [CLS] token embeddings. Results reveal that pretrained BERT enables high classification accuracy, indicating latent relational signals. However, concept pairs organize by relation type in high-dimensional embedding space only after fine-tuning on supervised relation classification tasks. This indicates relational schemata are not emergent from pretraining alone but can be induced via task scaffolding. These findings demonstrate that behavioral performance does not necessarily imply structured conceptual understanding, though models can acquire inductive biases for grounded relational abstraction through appropriate training.

</details>


### [85] [Lag-Relative Sparse Attention In Long Context Training](https://arxiv.org/abs/2506.11498)

*Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, Jinlong Li*

**Main category:** cs.CL

**Keywords:** Long-context input, Key-value cache compression, Lag-Relative Sparse Attention

**Relevance Score:** 9

**TL;DR:** The paper presents Lag-Relative Sparse Attention (LRSA) for improving long-context handling in LLMs without added parameters, enhancing efficiency and performance in question-answer tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in handling long-context input due to the quadratic complexity of attention and the issues with key-value cache compression techniques.

**Method:** The proposed Lag-Relative Sparse Attention (LRSA) utilizes LagKV compression for long context post-training, focusing on selecting the top K relevant key-value pairs in a lagging window, processed chunk-by-chunk.

**Key Contributions:**

	1. Introduces Lag-Relative Sparse Attention (LRSA) for long-context post-training.
	2. Demonstrates improved efficiency with top K key-value pair selection.
	3. Achieves better fine-tuned performance for question-answer tasks.

**Result:** Experimental results demonstrate that LRSA significantly improves LLM robustness with key-value compression and achieves superior performance in fine-tuning for question-answer tasks.

**Limitations:** 

**Conclusion:** LRSA fills a critical gap in LLMs' ability to effectively use long-context inputs while maintaining computational efficiency.

**Abstract:** Large Language Models (LLMs) have made significant strides in natural language processing and generation, yet their ability to handle long-context input remains constrained by the quadratic complexity of attention computation and linear-increasing key-value memory footprint. To reduce computational costs and memory, key-value cache compression techniques are commonly applied at inference time, but this often leads to severe performance degradation, as models are not trained to handle compressed context. Although there are more sophisticated compression methods, they are typically unsuitable for post-training because of their incompatibility with gradient-based optimization or high computation overhead. To fill this gap with no additional parameter and little computation overhead, we propose Lag-Relative Sparse Attention(LRSA) anchored by the LagKV compression method for long context post-training. Our method performs chunk-by-chunk prefilling, which selects the top K most relevant key-value pairs in a fixed-size lagging window, allowing the model to focus on salient historical context while maintaining efficiency. Experimental results show that our approach significantly enhances the robustness of the LLM with key-value compression and achieves better fine-tuned results in the question-answer tuning task.

</details>


### [86] [On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval](https://arxiv.org/abs/2506.11499)

*Seongbo Jang, Seonghyeon Lee, Dongha Lee, Hwanjo Yu*

**Main category:** cs.CL

**Keywords:** multimodal chatbots, dialogue systems, response retrieval

**Relevance Score:** 8

**TL;DR:** This paper investigates multimodal chatbots, focusing on how to design systems that can respond using both text and images, comparing various integration methods for response retrieval.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The growing importance of multimodal dialogue systems in both academia and industry necessitates an exploration of effective response generation using diverse modalities.

**Method:** The authors formulate a multimodal dialogue response retrieval task as a combination of three subtasks and propose both a two-step and an end-to-end integration method for multimodal response generation.

**Key Contributions:**

	1. Development of a multimodal dialogue response retrieval task
	2. Introduction of two integration methods (two-step and end-to-end)
	3. Demonstration of performance benefits through parameter sharing across modalities

**Result:** Experimental results demonstrate that the end-to-end method matches the performance of the two-step approach while reducing complexity and improving parameter efficiency through knowledge transfer.

**Limitations:** 

**Conclusion:** The study shows the potential of a streamlined end-to-end approach for multimodal dialogue systems, highlighting the benefits of parameter sharing and improved performance.

**Abstract:** Multimodal chatbots have become one of the major topics for dialogue systems in both research community and industry. Recently, researchers have shed light on the multimodality of responses as well as dialogue contexts. This work explores how a dialogue system can output responses in various modalities such as text and image. To this end, we first formulate a multimodal dialogue response retrieval task for retrieval-based systems as the combination of three subtasks. We then propose three integration methods based on a two-step approach and an end-to-end approach, and compare the merits and demerits of each method. Experimental results on two datasets demonstrate that the end-to-end approach achieves comparable performance without an intermediate step in the two-step approach. In addition, a parameter sharing strategy not only reduces the number of parameters but also boosts performance by transferring knowledge across the subtasks and the modalities.

</details>


### [87] [From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation](https://arxiv.org/abs/2506.11557)

*Chih-Hao Hsu, Ying-Jia Lin, Hung-Yu Kao*

**Main category:** cs.CL

**Keywords:** personalized dialogue generation, discourse relations, graph learning, Large Language Model, coherence

**Relevance Score:** 9

**TL;DR:** The paper introduces MUDI, a new method for generating personalized dialogue responses that maintain coherence and consistency with user traits, using discourse relations graph learning and a specialized graph encoder.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the naturalness and personalization of responses in dialogue generation, addressing the challenges of coherence and user persona alignment.

**Method:** MUDI employs a Large Language Model for discourse relation annotation and transforms dialogue data into structured graphs. The DialogueGAT model captures discourse relations and persona descriptions, with coherence-aware attention strategies for response generation.

**Key Contributions:**

	1. Introduction of MUDI for personalized dialogue generation
	2. Development of the DialogueGAT model for discourse relation graph encoding
	3. Implementation of coherence-aware attention strategies in response generation

**Result:** Experiments show significant improvements in the quality of personalized dialogue, closely resembling human-like interactions.

**Limitations:** 

**Conclusion:** The MUDI approach enhances personalized dialogue generation, effectively capturing and utilizing discourse relations and user personas.

**Abstract:** In dialogue generation, the naturalness of responses is crucial for effective human-machine interaction. Personalized response generation poses even greater challenges, as the responses must remain coherent and consistent with the user's personal traits or persona descriptions. We propose MUDI ($\textbf{Mu}$ltiple $\textbf{Di}$scourse Relations Graph Learning) for personalized dialogue generation. We utilize a Large Language Model to assist in annotating discourse relations and to transform dialogue data into structured dialogue graphs. Our graph encoder, the proposed DialogueGAT model, then captures implicit discourse relations within this structure, along with persona descriptions. During the personalized response generation phase, novel coherence-aware attention strategies are implemented to enhance the decoder's consideration of discourse relations. Our experiments demonstrate significant improvements in the quality of personalized responses, thus resembling human-like dialogue exchanges.

</details>


### [88] [Are LLMs Good Text Diacritizers? An Arabic and Yorùbá Case Study](https://arxiv.org/abs/2506.11602)

*Hawau Olamide Toyin, Samar M. Magdy, Hanan Aldarmaki*

**Main category:** cs.CL

**Keywords:** large language models, LLMs, diacritization, Arabic, Yoruba

**Relevance Score:** 8

**TL;DR:** This paper evaluates the effectiveness of large language models (LLMs) for text diacritization in Arabic and Yoruba, introducing a multilingual dataset and comparing LLM performance against specialized models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of LLMs in diacritization tasks and improve upon specialized diacritization models for Arabic and Yoruba using a novel dataset.

**Method:** The study introduces the MultiDiac dataset and evaluates 14 LLMs against 6 specialized models, including fine-tuning of 4 small models using LoRA for Yoruba.

**Key Contributions:**

	1. Introduction of the MultiDiac dataset for evaluating diacritization in Arabic and Yoruba
	2. Comparative analysis between LLMs and specialized models
	3. Demonstration of fine-tuning techniques to improve performance

**Result:** Many off-the-shelf LLMs outperform specialized diacritization models for both languages, but smaller models exhibit hallucinations; fine-tuning on a small dataset improves performance and reduces hallucinations.

**Limitations:** Smaller models still suffer from hallucinations despite fine-tuning approaches.

**Conclusion:** Fine-tuning smaller models can enhance diacritization performance, suggesting that LLMs can be effectively utilized for this task despite some limitations.

**Abstract:** We investigate the effectiveness of large language models (LLMs) for text diacritization in two typologically distinct languages: Arabic and Yoruba. To enable a rigorous evaluation, we introduce a novel multilingual dataset MultiDiac, with diverse samples that capture a range of diacritic ambiguities. We evaluate 14 LLMs varying in size, accessibility, and language coverage, and benchmark them against 6 specialized diacritization models. Additionally, we fine-tune four small open-source models using LoRA for Yoruba. Our results show that many off-the-shelf LLMs outperform specialized diacritization models for both Arabic and Yoruba, but smaller models suffer from hallucinations. Fine-tuning on a small dataset can help improve diacritization performance and reduce hallucination rates.

</details>


### [89] [SceneGram: Conceptualizing and Describing Tangrams in Scene Context](https://arxiv.org/abs/2506.11631)

*Simeon Junker, Sina Zarrieß*

**Main category:** cs.CL

**Keywords:** SceneGram, multimodal LLMs, human conceptualization, tangram shapes, contextual reference

**Relevance Score:** 7

**TL;DR:** The paper introduces SceneGram, a dataset analyzing how scene context influences human references to tangram shapes, and examines the limitations of multimodal LLMs in capturing these variations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how scene context influences conceptualization and naming of objects, and to highlight shortcomings in current multimodal LLMs.

**Method:** The authors created the SceneGram dataset of human references to tangram shapes in various scene contexts and performed analysis on references generated by multimodal LLMs.

**Key Contributions:**

	1. Introduction of SceneGram dataset for studying the effects of scene context on conceptualization.
	2. Analysis highlighting the limitations of multimodal LLMs in reflecting human reference diversity.
	3. Ins from the dataset can inform improvements in LLM design for better human-like interaction.

**Result:** Analysis revealed that multimodal LLMs do not adequately capture the diversity of human conceptualizations based on scene context.

**Limitations:** The dataset is limited to tangram shapes and may not generalize to other object types or contexts.

**Conclusion:** The findings suggest a gap in LLM performance regarding the richness of human-like reference and naming influenced by contextual factors.

**Abstract:** Research on reference and naming suggests that humans can come up with very different ways of conceptualizing and referring to the same object, e.g. the same abstract tangram shape can be a "crab", "sink" or "space ship". Another common assumption in cognitive science is that scene context fundamentally shapes our visual perception of objects and conceptual expectations. This paper contributes SceneGram, a dataset of human references to tangram shapes placed in different scene contexts, allowing for systematic analyses of the effect of scene context on conceptualization. Based on this data, we analyze references to tangram shapes generated by multimodal LLMs, showing that these models do not account for the richness and variability of conceptualizations found in human references.

</details>


### [90] [LoRA-Gen: Specializing Large Language Model via Online LoRA Generation](https://arxiv.org/abs/2506.11638)

*Yicheng Xiao, Lin Song, Rui Yang, Cheng Cheng, Yixiao Ge, Xiu Li, Ying Shan*

**Main category:** cs.CL

**Keywords:** LoRA, edge-side models, NLP, cloud model, inference efficiency

**Relevance Score:** 8

**TL;DR:** LoRA-Gen leverages a large cloud-side model to generate efficient LoRA parameters for edge-side models, enhancing performance in domain-specific tasks without requiring specialized training.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of effectiveness and efficiency in applying language models to domain-specific tasks, especially for constrained edge-side models.

**Method:** The LoRA-Gen framework generates LoRA parameters from a cloud model based on task descriptions, employing reparameterization to merge parameters into edge models for improved specialization and efficiency.

**Key Contributions:**

	1. Introduction of LoRA-Gen framework for cloud-to-edge model optimization
	2. Demonstrated improvements in inference efficiency and speed
	3. Achieved significant compression ratios on specific tasks

**Result:** LoRA-Gen outperforms conventional LoRA fine-tuning, achieving competitive accuracy and significantly increasing inference speed (2.1x speedup) with TinyLLaMA-1.1B, alongside a compression ratio of 10.1x on intelligent agent tasks with Gemma-2B.

**Limitations:** Limited to the effectiveness of the cloud-side model used for generating parameters; may not generalize across all domain-specific applications.

**Conclusion:** The proposed method allows for effective knowledge transfer and improved inference efficiency, showcasing the potential of cloud-assisted optimization for edge-side language models.

**Abstract:** Recent advances have highlighted the benefits of scaling language models to enhance performance across a wide range of NLP tasks. However, these approaches still face limitations in effectiveness and efficiency when applied to domain-specific tasks, particularly for small edge-side models. We propose the LoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA parameters for edge-side models based on task descriptions. By employing the reparameterization technique, we merge the LoRA parameters into the edge-side model to achieve flexible specialization. Our method facilitates knowledge transfer between models while significantly improving the inference efficiency of the specialized model by reducing the input context length. Without specialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which achieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in reasoning tasks. Besides, our method delivers a compression ratio of 10.1x with Gemma-2B on intelligent agent tasks.

</details>


### [91] [Converting Annotated Clinical Cases into Structured Case Report Forms](https://arxiv.org/abs/2506.11666)

*Pietro Ferrazzi, Alberto Lavelli, Bernardo Magnini*

**Main category:** cs.CL

**Keywords:** Case Report Forms, CRF slot filling, information extraction, large language models, health informatics

**Relevance Score:** 6

**TL;DR:** This paper presents a methodology for converting datasets annotated for information extraction into structured Case Report Forms (CRFs) to address the shortage of well-annotated CRF data. The methodology was tested on the E3C dataset in English and Italian resulting in a new dataset that enables CRF slot filling with reported accuracy metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of publicly available, well-annotated Case Report Form (CRF) datasets limits advancements in CRF slot filling systems that extract information from clinical notes.

**Method:** A semi-automatic conversion methodology was developed to transform existing information extraction datasets into structured CRFs, specifically applied to the E3C dataset in English and Italian.

**Key Contributions:**

	1. Proposed a semi-automatic conversion methodology for CRFs
	2. Created a new dataset for CRF slot filling from the E3C dataset
	3. Demonstrated challenges in filling CRFs using current LLMs

**Result:** The slot filling achieved performance of 67.3% on English and 59.7% on Italian using a closed Large Language Model in a zero-shot manner, while performance on three families of open-source models was lower, indicating the complexity of the task.

**Limitations:** The performance results indicate difficulty in achieving high accuracy for CRF slot filling using recent language models, suggesting limitations in current model capabilities.

**Conclusion:** The study highlights the challenges of CRF slot filling, even with state-of-the-art LLMs, and provides a new high-quality dataset for future research.

**Abstract:** Case Report Forms (CRFs) are largely used in medical research as they ensure accuracy, reliability, and validity of results in clinical studies. However, publicly available, wellannotated CRF datasets are scarce, limiting the development of CRF slot filling systems able to fill in a CRF from clinical notes. To mitigate the scarcity of CRF datasets, we propose to take advantage of available datasets annotated for information extraction tasks and to convert them into structured CRFs. We present a semi-automatic conversion methodology, which has been applied to the E3C dataset in two languages (English and Italian), resulting in a new, high-quality dataset for CRF slot filling. Through several experiments on the created dataset, we report that slot filling achieves 59.7% for Italian and 67.3% for English on a closed Large Language Models (zero-shot) and worse performances on three families of open-source models, showing that filling CRFs is challenging even for recent state-of-the-art LLMs. We release the datest at https://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166

</details>


### [92] [Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE](https://arxiv.org/abs/2506.11673)

*Alicja Dobrzeniecka, Antske Fokkens, Pia Sommerauer*

**Main category:** cs.CL

**Keywords:** amnesic probing, Iterative Nullspace Projection, Mean Projection, behavioral explanations, language models

**Relevance Score:** 7

**TL;DR:** This paper explores new techniques for amnesic probing in model behavior analysis, focusing on targeted information removal to improve behavioral explanations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to understand the impact of specific linguistic information on model performance and limitations in existing removal techniques.

**Method:** The study introduces Mean Projection (MP) and LEACE as alternatives to Iterative Nullspace Projection (INLP) for more precise information removal in amnesic probing.

**Key Contributions:**

	1. Introduction of Mean Projection (MP) and LEACE for amnesic probing
	2. Demonstration of improved targeted information removal
	3. Enhanced potential for behavioral explanations in language models

**Result:** Mean Projection and LEACE effectively eliminate target information without compromising other linguistic data, leading to clearer behavioral explanations than INLP.

**Limitations:** 

**Conclusion:** The proposed methods improve the robustness and clarity of the amnesic probing technique, allowing for better insights into model behavior.

**Abstract:** Amnesic probing is a technique used to examine the influence of specific linguistic information on the behaviour of a model. This involves identifying and removing the relevant information and then assessing whether the model's performance on the main task changes. If the removed information is relevant, the model's performance should decline. The difficulty with this approach lies in removing only the target information while leaving other information unchanged. It has been shown that Iterative Nullspace Projection (INLP), a widely used removal technique, introduces random modifications to representations when eliminating target information. We demonstrate that Mean Projection (MP) and LEACE, two proposed alternatives, remove information in a more targeted manner, thereby enhancing the potential for obtaining behavioural explanations through Amnesic Probing.

</details>


### [93] [LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach](https://arxiv.org/abs/2506.11681)

*Pratibha Zunjare, Michael Hsiao*

**Main category:** cs.CL

**Keywords:** sentence simplification, large language models, multi-agent systems

**Relevance Score:** 6

**TL;DR:** The paper presents a hybrid method using Large Language Models for simplifying complex sentences, achieving a 70% success rate in sentence simplification for video game design applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the difficulty of simplifying complex sentences while maintaining semantic and logical accuracy.

**Method:** A hybrid approach combining advanced prompting techniques with multi-agent architectures to facilitate the sentence simplification process.

**Key Contributions:**

	1. Hybrid method using multi-agent architectures for sentence simplification
	2. Significant performance improvement over single-agent approaches
	3. Potential applications in fields such as game design and beyond.

**Result:** The proposed method successfully simplified 70% of complex sentences in the context of video game design, significantly outperforming a single-agent approach, which achieved a 48% success rate.

**Limitations:** 

**Conclusion:** The hybrid multi-agent system offers a significant improvement in simplifying complex sentences, with potential implications for various applications.

**Abstract:** This paper addresses the challenge of transforming complex sentences into sequences of logical, simplified sentences while preserving semantic and logical integrity with the help of Large Language Models. We propose a hybrid approach that combines advanced prompting with multi-agent architectures to enhance the sentence simplification process. Experimental results show that our approach was able to successfully simplify 70% of the complex sentences written for video game design application. In comparison, a single-agent approach attained a 48% success rate on the same task.

</details>


### [94] [Configurable Preference Tuning with Rubric-Guided Synthetic Data](https://arxiv.org/abs/2506.11702)

*Víctor Gallego*

**Main category:** cs.CL

**Keywords:** human feedback, AI alignment, language models, preference tuning, dynamic adjustment

**Relevance Score:** 9

**TL;DR:** Introduction of Configurable Preference Tuning (CPT) for dynamic adjustment of language model behavior based on human-interpretable directives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Challenge the static nature of traditional human feedback models in AI alignment by introducing a framework that supports dynamic preference adjustment.

**Method:** CPT utilizes synthetically generated preference data based on structured rubrics, allowing prompt-based modulation of LLM outputs without retraining.

**Key Contributions:**

	1. Introduction of a dynamic feedback model for LLMs
	2. Release of experimental artifacts including training code and datasets
	3. Establishment of a structured approach to preference tuning

**Result:** Demonstrated that fine-tuning with rubric-guided preferences enables LLMs to adapt outputs in response to varying prompts, providing nuanced control over language generation.

**Limitations:** 

**Conclusion:** CPT enhances LLM adaptability and provides context-sensitive modeling of human feedback, marking a significant advancement in AI alignment methodologies.

**Abstract:** Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning

</details>


### [95] [The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference](https://arxiv.org/abs/2506.11728)

*Héctor Martínez, Adrián Castelló, Francisco D. Igual, Enrique S. Quintana-Ortí*

**Main category:** cs.CL

**Keywords:** deep learning, mixed-precision, matrix multiplication, gemm, hardware architecture

**Relevance Score:** 4

**TL;DR:** The paper discusses the transition to mixed-precision arithmetic in deep learning, focusing on optimizing general matrix-matrix multiplication (gemm) for various architectures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the shift towards reduced-precision formats in deep learning, optimizing matrix multiplication for modern hardware becomes essential to improve efficiency in resource-constrained environments.

**Method:** The paper reviews traditional gemm implementations and proposes strategies for adapting these to mixed-precision integer arithmetic, showcasing new micro-kernel designs and data layouts optimized for x86_64, ARM, and RISC-V architectures.

**Key Contributions:**

	1. Strategies for adapting gemm to mixed-precision integer arithmetic
	2. Novel micro-kernel designs for enhanced performance
	3. Demonstrated significant performance gains on various architectures

**Result:** The proposed mixed-precision implementations demonstrate significant performance improvements compared to traditional floating-point methods across the evaluated CPU architectures.

**Limitations:** 

**Conclusion:** The findings indicate a new era of optimization for matrix multiplication in deep learning, driven by hardware advancements and the demands of mixed-precision computations.

**Abstract:** Recent advances in deep learning (DL) have led to a shift from traditional 64-bit floating point (FP64) computations toward reduced-precision formats, such as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision arithmetic. This transition enhances computational throughput, reduces memory and bandwidth usage, and improves energy efficiency, offering significant advantages for resource-constrained edge devices. To support this shift, hardware architectures have evolved accordingly, now including adapted ISAs (Instruction Set Architectures) that expose mixed-precision vector units and matrix engines tailored for DL workloads. At the heart of many DL and scientific computing tasks is the general matrix-matrix multiplication gemm, a fundamental kernel historically optimized using axpy vector instructions on SIMD (single instruction, multiple data) units. However, as hardware moves toward mixed-precision dot-product-centric operations optimized for quantized inference, these legacy approaches are being phased out. In response to this, our paper revisits traditional high-performance gemm and describes strategies for adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs, including x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel designs and data layouts that better exploit today's specialized hardware and demonstrate significant performance gains from MIP arithmetic over floating-point implementations across three representative CPU architectures. These contributions highlight a new era of gemm optimization-driven by the demands of DL inference on heterogeneous architectures, marking what we term as the "Cambrian period" for matrix multiplication.

</details>


### [96] [DART: Distilling Autoregressive Reasoning to Silent Thought](https://arxiv.org/abs/2506.11752)

*Nan Jiang, Ziming Wu, De-Chuan Zhan, Fuming Lai, Shaobing Lian*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought, Non-autoregressive reasoning, Efficiency, Self-distillation

**Relevance Score:** 9

**TL;DR:** DART distills autoregressive reasoning in LLMs to a more efficient non-autoregressive framework, improving computational efficiency while maintaining performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational overhead of autoregressive reasoning in LLMs for latency-sensitive applications.

**Method:** DART utilizes a self-distillation framework with two training pathways: CoT for traditional reasoning and ST for direct answer generation using lightweight reasoning evolvement.

**Key Contributions:**

	1. Introduction of DART framework for LLMs
	2. Implementation of self-distillation with two pathways
	3. Demonstration of efficiency gains in reasoning tasks

**Result:** DART achieves comparable reasoning performance to existing models while providing significant efficiency improvements in computation.

**Limitations:** The experimental setup may not cover all possible applications or demonstrate long-term performance.

**Conclusion:** DART serves as a viable alternative for efficient reasoning in LLMs, facilitating quicker inference without sacrificing performance.

**Abstract:** Chain-of-Thought (CoT) reasoning has significantly advanced Large Language Models (LLMs) in solving complex tasks. However, its autoregressive paradigm leads to significant computational overhead, hindering its deployment in latency-sensitive applications. To address this, we propose \textbf{DART} (\textbf{D}istilling \textbf{A}utoregressive \textbf{R}easoning to Silent \textbf{T}hought), a self-distillation framework that enables LLMs to replace autoregressive CoT with non-autoregressive Silent Thought (ST). Specifically, DART introduces two training pathways: the CoT pathway for traditional reasoning and the ST pathway for generating answers directly from a few ST tokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM) to align its hidden states with the CoT pathway, enabling the ST tokens to evolve into informative embeddings. During inference, only the ST pathway is activated, leveraging evolving ST tokens to deliver the answer directly. Extensive experimental results demonstrate that DART achieves comparable reasoning performance to existing baselines while offering significant efficiency gains, serving as a feasible alternative for efficient reasoning.

</details>


### [97] [DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents](https://arxiv.org/abs/2506.11763)

*Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, Zhendong Mao*

**Main category:** cs.CL

**Keywords:** Deep Research Agents, LLM-based agents, benchmarking, information retrieval, citation accuracy

**Relevance Score:** 8

**TL;DR:** Introduction of DeepResearch Bench, a benchmark for evaluating LLM-based Deep Research Agents (DRAs) with 100 PhD-level research tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a systematic method for evaluating the performance of LLM-based agents in conducting research tasks, as existing solutions are absent.

**Method:** Development of two methodologies: a reference-based method for assessing report quality and a framework for evaluating information retrieval and citation effectiveness.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for LLM-based agents
	2. Two novel methodologies aligning DRA evaluation with human judgment
	3. Open-source availability to encourage further research

**Result:** DeepResearch Bench provides a standardized way to evaluate DRAs, achieving strong alignment with human judgment.

**Limitations:** 

**Conclusion:** The open-sourcing of DeepResearch Bench aims to foster advancement in LLM-based agents by offering a comprehensive evaluation framework.

**Abstract:** Deep Research Agents are a prominent category of LLM-based agents. By autonomously orchestrating multistep web exploration, targeted retrieval, and higher-order synthesis, they transform vast amounts of online information into analyst-grade, citation-rich reports--compressing hours of manual desk research into minutes. However, a comprehensive benchmark for systematically evaluating the capabilities of these agents remains absent. To bridge this gap, we present DeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks, each meticulously crafted by domain experts across 22 distinct fields. Evaluating DRAs is inherently complex and labor-intensive. We therefore propose two novel methodologies that achieve strong alignment with human judgment. The first is a reference-based method with adaptive criteria to assess the quality of generated research reports. The other framework is introduced to evaluate DRA's information retrieval and collection capabilities by assessing its effective citation count and overall citation accuracy. We have open-sourced DeepResearch Bench and key components of these frameworks at https://github.com/Ayanami0730/deep_research_bench to accelerate the development of practical LLM-based agents.

</details>


### [98] [Long-Short Alignment for Effective Long-Context Modeling in LLMs](https://arxiv.org/abs/2506.11769)

*Tianqi Du, Haotian Huang, Yifei Wang, Yisen Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Length Generalization, Long-Short Alignment, Regularization, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper addresses the limitations of long-context modeling in large language models (LLMs) by proposing a focus on the output distribution and introducing a new metric to improve length generalization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve effective long-context modeling in LLMs, which is hindered by their fixed context window and challenges with length generalization.

**Method:** The authors conduct case studies on synthetic tasks to analyze long-short alignment in output distributions and propose a metric called Long-Short Misalignment to quantify this relationship, followed by introducing a regularization term during training that encourages long-short alignment.

**Key Contributions:**

	1. Proposes the concept of long-short alignment in LLM output distributions.
	2. Introduces the Long-Short Misalignment metric to measure length generalization.
	3. Develops a regularization term for promoting long-short alignment during model training.

**Result:** Experiments demonstrate the effectiveness of the proposed approach, highlighting a strong correlation between Long-Short Misalignment and length generalization performance in LLMs.

**Limitations:** 

**Conclusion:** The findings offer new insights into achieving better long-context modeling, potentially enhancing the capabilities of LLMs in processing longer sequences.

**Abstract:** Large language models (LLMs) have exhibited impressive performance and surprising emergent properties. However, their effectiveness remains limited by the fixed context window of the transformer architecture, posing challenges for long-context modeling. Among these challenges, length generalization -- the ability to generalize to sequences longer than those seen during training -- is a classical and fundamental problem. In this work, we propose a fresh perspective on length generalization, shifting the focus from the conventional emphasis on input features such as positional encodings or data structures to the output distribution of the model. Specifically, through case studies on synthetic tasks, we highlight the critical role of \textbf{long-short alignment} -- the consistency of output distributions across sequences of varying lengths. Extending this insight to natural language tasks, we propose a metric called Long-Short Misalignment to quantify this phenomenon, uncovering a strong correlation between the metric and length generalization performance. Building on these findings, we develop a regularization term that promotes long-short alignment during training. Extensive experiments validate the effectiveness of our approach, offering new insights for achieving more effective long-context modeling in LLMs. Code is available at https://github.com/PKU-ML/LongShortAlignment.

</details>


### [99] [Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models](https://arxiv.org/abs/2506.11798)

*Maximilian Kreutner, Marlene Lutz, Markus Strohmaier*

**Main category:** cs.CL

**Keywords:** Large Language Models, persona prompting, voting behavior, European Parliament, political discourse

**Relevance Score:** 7

**TL;DR:** This paper analyzes the use of zero-shot persona prompting in Large Language Models to predict individual voting decisions and the stances of European groups on various policies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the progressive bias in LLMs and explore the effectiveness of persona prompts in simulating voting behaviors.

**Method:** The study uses zero-shot persona prompting with minimal information to predict voting decisions and aggregates these predictions for European groups.

**Key Contributions:**

	1. Analysis of zero-shot persona prompting in LLMs
	2. Development of a persona dataset for European politicians
	3. Simulation of voting behavior in European Parliament with high accuracy

**Result:** The model successfully simulates the voting behavior of Members of the European Parliament with a weighted F1 score of approximately 0.793.

**Limitations:** 

**Conclusion:** Zero-shot persona prompting can be effective in predicting individual and group voting decisions in a political context.

**Abstract:** Large Language Models (LLMs) display remarkable capabilities to understand or even produce political discourse, but have been found to consistently display a progressive left-leaning bias. At the same time, so-called persona or identity prompts have been shown to produce LLM behavior that aligns with socioeconomic groups that the base model is not aligned with. In this work, we analyze whether zero-shot persona prompting with limited information can accurately predict individual voting decisions and, by aggregation, accurately predict positions of European groups on a diverse set of policies. We evaluate if predictions are stable towards counterfactual arguments, different persona prompts and generation methods. Finally, we find that we can simulate voting behavior of Members of the European Parliament reasonably well with a weighted F1 score of approximately 0.793. Our persona dataset of politicians in the 2024 European Parliament and our code are available at https://github.com/dess-mannheim/european_parliament_simulation.

</details>


### [100] [Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?](https://arxiv.org/abs/2506.11807)

*Simeon Junker, Manar Ali, Larissa Koch, Sina Zarrieß, Hendrik Buschmeier*

**Main category:** cs.CL

**Keywords:** multimodal large language models, reference resolution, pragmatics, visual stimuli, context-dependent interpretation

**Relevance Score:** 7

**TL;DR:** This paper explores the capabilities of multimodal large language models (MLLMs) in reference resolution tasks using abstract visual stimuli and finds significant challenges in pragmatic interpretation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To probe the pragmatic capabilities of multimodal large language models in interpreting context-dependent color descriptions.

**Method:** The authors conduct experiments involving reference resolution tasks with visual stimuli such as color patches and grids to evaluate the performance of MLLMs.

**Key Contributions:**

	1. New insights into the pragmatic limits of MLLMs
	2. Empirical data from reference resolution tasks with visual stimuli
	3. Identification of context-dependent interpretation challenges

**Result:** The MLLMs demonstrated substantial difficulties in handling context-dependent interpretations of color descriptions, indicating limitations in their pragmatic abilities.

**Limitations:** The study focuses on specific tasks with abstract visual stimuli, which may not capture the full range of linguistic abilities or real-world applications.

**Conclusion:** The findings reveal that despite being straightforward for humans, reference resolution tasks pose significant challenges for current MLLMs, highlighting the need for improved pragmatic understanding.

**Abstract:** We investigate the linguistic abilities of multimodal large language models in reference resolution tasks featuring simple yet abstract visual stimuli, such as color patches and color grids. Although the task may not seem challenging for today's language models, being straightforward for human dyads, we consider it to be a highly relevant probe of the pragmatic capabilities of MLLMs. Our results and analyses indeed suggest that basic pragmatic capabilities, such as context-dependent interpretation of color descriptions, still constitute major challenges for state-of-the-art MLLMs.

</details>


### [101] [Post Persona Alignment for Multi-Session Dialogue Generation](https://arxiv.org/abs/2506.11857)

*Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto*

**Main category:** cs.CL

**Keywords:** dialogue generation, multi-session dialogues, persona alignment

**Relevance Score:** 9

**TL;DR:** Proposes a two-stage framework called Post Persona Alignment (PPA) to improve multi-session persona-based dialogue generation by first generating a response based on context and then aligning it with the speaker's persona.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in maintaining long-term consistency and generating diverse, personalized responses in multi-session dialogue with large language models (LLMs).

**Method:** PPA generates an initial response based on dialogue context and subsequently retrieves relevant persona memories to refine the response for better alignment with the user's persona.

**Key Contributions:**

	1. Introduction of Post Persona Alignment (PPA) framework
	2. Improvement in consistency and diversity of responses
	3. Effective persona relevance in dialogue generation

**Result:** PPA outperforms existing methods in consistency, diversity, and persona relevance on multi-session LLM-generated dialogue data.

**Limitations:** 

**Conclusion:** The post-hoc alignment strategy of PPA offers a more flexible and effective approach to long-term personalized dialogue generation, promoting naturalness and diversity.

**Abstract:** Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speaker's persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation.

</details>


### [102] [Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache](https://arxiv.org/abs/2506.11886)

*Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fourier transform, memory efficiency, transformer architecture, long-context handling

**Relevance Score:** 8

**TL;DR:** FourierAttention is a novel, training-free framework for optimizing long-context handling in Large Language Models by using Fourier bases to enhance memory efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models face increasing memory demands due to growing Key-Value caches and existing methods often compromise accuracy or increase computational overhead.

**Method:** FourierAttention utilizes the varying roles of transformer head dimensions to project long-context-insensitive dimensions onto orthogonal Fourier bases, approximating their evolution with fixed-length spectral coefficients.

**Key Contributions:**

	1. Introduces FourierAttention as a training-free method for improving long-context performance in LLMs.
	2. Utilizes Fourier bases for efficient memory management in transformer architecture.
	3. Demonstrates superior accuracy on long-context tasks using LLaMA models.

**Result:** Evaluations on LLaMA models demonstrate that FourierAttention achieves superior long-context accuracy compared to existing models on benchmarks like LongBench and NIAH.

**Limitations:** 

**Conclusion:** The framework, combined with a custom Triton kernel, offers an efficient deployment strategy without performance loss, addressing memory usage effectively.

**Abstract:** Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.

</details>


### [103] [GeistBERT: Breathing Life into German NLP](https://arxiv.org/abs/2506.11903)

*Raphael Scheible-Schmitt, Johann Frei*

**Main category:** cs.CL

**Keywords:** GeistBERT, German NLP, language models, transformer, machine learning

**Relevance Score:** 3

**TL;DR:** GeistBERT enhances German NLP through improved language models and datasets, achieving state-of-the-art performance in various tasks.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for advanced architectures and datasets suited for the linguistic characteristics of the German language in NLP.

**Method:** GeistBERT was pre-trained using fairseq with standard hyperparameters, initialized from GottBERT weights, and trained on a large-scale German corpus with Whole Word Masking (WWM). Extended-input variants were developed with Nyströmformer and Longformer architectures.

**Key Contributions:**

	1. Development of GeistBERT tailored for German NLP
	2. Enhanced performance in various NLP tasks
	3. Release of models under MIT license for community use

**Result:** GeistBERT achieved strong performance across NER and text classification tasks, leading among base models and setting a new state-of-the-art, outperforming larger models in some cases.

**Limitations:** 

**Conclusion:** The GeistBERT models are made available under the MIT license to benefit the German NLP research community.

**Abstract:** Advances in transformer-based language models have highlighted the benefits of language-specific pre-training on high-quality corpora. In this context, German NLP stands to gain from updated architectures and modern datasets tailored to the linguistic characteristics of the German language. GeistBERT seeks to improve German language processing by incrementally training on a diverse corpus and optimizing model performance across various NLP tasks. It was pre-trained using fairseq with standard hyperparameters, initialized from GottBERT weights, and trained on a large-scale German corpus using Whole Word Masking (WWM). Based on the pre-trained model, we derived extended-input variants using Nystr\"omformer and Longformer architectures with support for sequences up to 8k tokens. While these long-context models were not evaluated on dedicated long-context benchmarks, they are included in our release. We assessed all models on NER (CoNLL 2003, GermEval 2014) and text classification (GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The GeistBERT models achieved strong performance, leading all tasks among the base models and setting a new state-of-the-art (SOTA). Notably, the base models outperformed larger models in several tasks. To support the German NLP research community, we are releasing GeistBERT under the MIT license.

</details>


### [104] [Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study](https://arxiv.org/abs/2506.11919)

*Greta Damo, Elena Cabrio, Serena Villata*

**Main category:** cs.CL

**Keywords:** Counter-speech, Hate Speech, Effectiveness, Computational Framework, Classification

**Relevance Score:** 6

**TL;DR:** A framework for assessing the effectiveness of counter-speech in combating online hate speech, utilizing six core dimensions and advanced classification methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective counter-speech strategies to address the rise of online hate speech and the challenge of evaluating their effectiveness.

**Method:** A computational framework was developed for classifying counter-speech effectiveness, defining six dimensions, and annotating 4,214 instances from benchmark datasets. Two classification strategies were implemented—multi-task and dependency-based.

**Key Contributions:**

	1. Introduction of a novel computational framework for counter-speech effectiveness assessment
	2. Definition of six core dimensions for evaluating counter-speech
	3. Release of a linguistic resource based on annotated counter-speech instances

**Result:** Achieved strong classification results with average F1 scores of 0.94 and 0.96, surpassing standard baselines and demonstrating interdependence among dimensions.

**Limitations:** 

**Conclusion:** The proposed framework and classification strategies provide a valuable resource for evaluating counter-speech effectiveness and can guide future research in online discourse.

**Abstract:** Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS), yet defining the criteria to assess its effectiveness remains an open challenge. We propose a novel computational framework for CS effectiveness classification, grounded in social science concepts. Our framework defines six core dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience Adaptation, and Fairness - which we use to annotate 4,214 CS instances from two benchmark datasets, resulting in a novel linguistic resource released to the community. In addition, we propose two classification strategies, multi-task and dependency-based, achieving strong results (0.94 and 0.96 average F1 respectively on both expert- and user-written CS), outperforming standard baselines, and revealing strong interdependence among dimensions.

</details>


### [105] [Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback](https://arxiv.org/abs/2506.11930)

*Dongwei Jiang, Alvin Zhang, Andrew Wang, Nicholas Andrews, Daniel Khashabi*

**Main category:** cs.CL

**Keywords:** LLM, feedback, machine learning, self-improvement, reasoning

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of LLMs to incorporate external feedback, identifying a limitation termed FEEDBACK FRICTION, and explores strategies to mitigate this issue.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how effectively LLMs can integrate external feedback in order to improve their responses in various reasoning tasks.

**Method:** The study employs a controlled experimental setup where a solver model generates answers, receives targeted feedback from a feedback generator, and then attempts to improve its responses.

**Key Contributions:**

	1. Identification of FEEDBACK FRICTION in LLMs
	2. Evaluation across diverse reasoning tasks
	3. Exploration of strategies to mitigate feedback resistance

**Result:** Despite the near-ideal conditions, solver models showed consistent resistance to the feedback provided, referred to as FEEDBACK FRICTION.

**Limitations:** Resistance to feedback appears persistent even under ideal conditions and common causes like model overconfidence were ruled out.

**Conclusion:** The paper highlights FEEDBACK FRICTION as a significant limitation in LLMs and suggests that future research should focus on overcoming this obstacle for better self-improvement.

**Abstract:** Recent studies have shown LLMs possess some ability to improve their responses when given external feedback. However, it remains unclear how effectively and thoroughly these models can incorporate extrinsic feedback. In an ideal scenario, if LLMs receive near-perfect and complete feedback, we would expect them to fully integrate the feedback and change their incorrect answers to correct ones. In this paper, we systematically investigate LLMs' ability to incorporate feedback by designing a controlled experimental environment. For each problem, a solver model attempts a solution, then a feedback generator with access to near-complete ground-truth answers produces targeted feedback, after which the solver tries again. We evaluate this pipeline across a diverse range of tasks, including math reasoning, knowledge reasoning, scientific reasoning, and general multi-domain evaluations with state-of-the-art language models including Claude 3.7 (with and without extended thinking). Surprisingly, even under these near-ideal conditions, solver models consistently show resistance to feedback, a limitation that we term FEEDBACK FRICTION. To mitigate this limitation, we experiment with sampling-based strategies like progressive temperature increases and explicit rejection of previously attempted incorrect answers, which yield improvements but still fail to help models achieve target performance. We also perform a rigorous exploration of potential causes of FEEDBACK FRICTION, ruling out factors such as model overconfidence and data familiarity. We hope that highlighting this issue in LLMs and ruling out several apparent causes will help future research in self-improvement.

</details>


### [106] [Improving Large Language Model Safety with Contrastive Representation Learning](https://arxiv.org/abs/2506.11938)

*Samuel Simko, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin*

**Main category:** cs.CL

**Keywords:** Large Language Models, adversarial attacks, contrastive representation learning, defense mechanisms, robustness

**Relevance Score:** 8

**TL;DR:** The paper presents a novel defense framework for Large Language Models (LLMs) against adversarial attacks using contrastive representation learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerability of LLMs to adversarial attacks and the limitations of existing defense mechanisms.

**Method:** The proposed method involves fine-tuning a model with a triplet-based loss and adversarial hard negative mining to enhance the separation between benign and harmful representations.

**Key Contributions:**

	1. Introduction of a novel defense framework using contrastive representation learning.
	2. Demonstration of improved robustness against input-level and embedding-space attacks.
	3. Provision of code for the proposed method to enable further research.

**Result:** The experimental results indicate that this approach significantly outperforms previous defenses based on representation engineering, yielding improved robustness against various attack types.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of using contrastive representation learning for enhancing the defense mechanisms of LLMs without sacrificing performance.

**Abstract:** Large Language Models (LLMs) are powerful tools with profound societal impacts, yet their ability to generate responses to diverse and uncontrolled inputs leaves them vulnerable to adversarial attacks. While existing defenses often struggle to generalize across varying attack types, recent advancements in representation engineering offer promising alternatives. In this work, we propose a defense framework that formulates model defense as a contrastive representation learning (CRL) problem. Our method finetunes a model using a triplet-based loss combined with adversarial hard negative mining to encourage separation between benign and harmful representations. Our experimental results across multiple models demonstrate that our approach outperforms prior representation engineering-based defenses, improving robustness against both input-level and embedding-space attacks without compromising standard performance. Our code is available at https://github.com/samuelsimko/crl-llm-defense

</details>


### [107] [code_transformed: The Influence of Large Language Models on Code](https://arxiv.org/abs/2506.12014)

*Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, code style, programming practices, GitHub, empirical study

**Relevance Score:** 8

**TL;DR:** This paper investigates how Large Language Models (LLMs) have influenced programming code style, focusing on naming conventions and maintainability, using data from over 19,000 GitHub repositories.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the transformation of code style due to the advances in LLM capabilities and understand its implications for programming practices.

**Method:** The study involves analyzing coding trends in over 19,000 GitHub repositories linked to arXiv papers from 2020-2025, focusing on measurable aspects like naming conventions and complexity.

**Key Contributions:**

	1. First large-scale empirical evidence on the impact of LLMs on coding style
	2. Identification of measurable trends in coding practices associated with LLM influence
	3. Analysis of LLM reasoning processes on algorithmic problems

**Result:** The study found that the usage of snake_case variable names in Python increased from 47% in Q1 2023 to 51% in Q1 2025, indicating a clear trend influenced by LLMs.

**Limitations:** Difficulty in estimating the exact proportion of code generated or assisted by LLMs due to their diversity and various usage scenarios.

**Conclusion:** The findings provide substantial empirical evidence that LLMs are reshaping real-world coding styles, although estimating the exact amount of LLM-generated code remains challenging.

**Abstract:** Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 19,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake\_case variable names in Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Given the diversity of LLMs and usage scenarios, among other factors, it is difficult or even impossible to precisely estimate the proportion of code generated or assisted by LLMs. Our experimental results provide the first large-scale empirical evidence that LLMs affect real-world programming style.

</details>


### [108] [FlashBack:Efficient Retrieval-Augmented Language Modeling for Long Context Inference](https://arxiv.org/abs/2405.04065)

*Runheng Liu, Xingchen Xiao, Heyan Huang, Zewen Chi, Zhijing Wu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Language Modeling, inference efficiency, Marking Tokens

**Relevance Score:** 9

**TL;DR:** FlashBack is a modular RALM that improves inference efficiency by appending retrieved documents to the context for better KV cache usage, resulting in faster LLM performance without compromising generation quality.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high runtime issues and inefficiencies associated with traditional RALM methods that prepend retrieved content to inputs, affecting inference speed.

**Method:** FlashBack integrates retrieved documents at the end of the context and introduces Marking Tokens for efficient fine-tuning, enhancing KV cache utilization.

**Key Contributions:**

	1. Introduction of FlashBack for efficient RALM
	2. Utilization of appending context pattern for better KV cache performance
	3. Development of Marking Tokens for improved fine-tuning

**Result:** FlashBack achieves up to 4x faster inference speed compared to traditional prepending methods on a 7B LLM while maintaining decent generation quality as measured by perplexity.

**Limitations:** 

**Conclusion:** The proposed method significantly reduces inference costs by improving LLM efficiency during retrieval-augmented generation tasks.

**Abstract:** Retrieval-Augmented Language Modeling (RALM) by integrating large language models (LLM) with relevant documents from an external corpus is a proven method for enabling the LLM to generate information beyond the scope of its pre-training corpus. Previous work utilizing retrieved content by simply prepending it to the input poses a high runtime issue, which degrades the inference efficiency of the LLMs because they fail to use the Key-Value (KV) cache efficiently. In this paper, we propose FlashBack, a modular RALM designed to improve the inference efficiency of RALM with appending context pattern while maintaining decent performance after fine-tuning by Low-Rank Adaption. FlashBack appends retrieved documents at the end of the context for efficiently utilizing the KV cache instead of prepending them. And we introduce Marking Token as two special prompt tokens for marking the boundary of the appending context during fine-tuning. Our experiments on testing generation quality show that FlashBack can remain decent generation quality in perplexity. And the inference speed of FlashBack is up to $4\times$ faster than the prepending counterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing unnecessary re-computation, it demonstrates an advancement by achieving significantly faster inference speed, and this heightened efficiency will substantially reduce inferential cost.

</details>


### [109] [JBBQ: Japanese Bias Benchmark for Analyzing Social Biases in Large Language Models](https://arxiv.org/abs/2406.02050)

*Hitomi Yanaka, Namgi Han, Ryoma Kumon, Jie Lu, Masashi Takeshita, Ryo Sekizawa, Taisei Kato, Hiromi Arai*

**Main category:** cs.CL

**Keywords:** Japanese Bias Benchmark, large language models, social biases, natural language processing, bias mitigation

**Relevance Score:** 9

**TL;DR:** This study constructs the Japanese Bias Benchmark dataset (JBBQ) for analyzing social biases in Japanese large language models, revealing a trade-off between model accuracy and bias scores.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the extent of social biases in Japanese large language models, which has not been fully studied despite the presence of benchmarks in other languages.

**Method:** The study constructs the JBBQ dataset based on the English BBQ benchmark and analyzes the performance of Japanese LLMs on this dataset.

**Key Contributions:**

	1. Development of the Japanese Bias Benchmark dataset (JBBQ) for Question Answering.
	2. Comparison of bias scores in Japanese LLMs with varying parameters.
	3. Evaluation of prompting techniques to reduce social bias effects.

**Result:** The analysis indicates that while larger Japanese LLMs achieve better accuracy on the JBBQ, their bias scores are higher; prompts designed to mitigate bias effects show some effectiveness, though further improvement is required for evidence extraction.

**Limitations:** The ability to extract correct evidence from contexts in Japanese remains an area for improvement.

**Conclusion:** The findings suggest that addressing social biases in Japanese LLMs remains a challenge, and further enhancements in model training and prompting techniques are necessary.

**Abstract:** With the development of large language models (LLMs), social biases in these LLMs have become a pressing issue. Although there are various benchmarks for social biases across languages, the extent to which Japanese LLMs exhibit social biases has not been fully investigated. In this study, we construct the Japanese Bias Benchmark dataset for Question Answering (JBBQ) based on the English bias benchmark BBQ, with analysis of social biases in Japanese LLMs. The results show that while current open Japanese LLMs with more parameters show improved accuracies on JBBQ, their bias scores increase. In addition, prompts with a warning about social biases and chain-of-thought prompting reduce the effect of biases in model outputs, but there is room for improvement in extracting the correct evidence from contexts in Japanese. Our dataset is available at https://github.com/ynklab/JBBQ_data.

</details>


### [110] [Deep Sparse Latent Feature Models for Knowledge Graph Completion](https://arxiv.org/abs/2411.15694)

*Haotian Li, Rui Zhang, Lingzhi Wang, Bin Yu, Youwei Wang, Yuliang Wei, Kai Wang, Richard Yi Da Xu, Bailing Wang*

**Main category:** cs.CL

**Keywords:** knowledge graph completion, stochastic blockmodels, variational autoencoder, link prediction, latent feature models

**Relevance Score:** 6

**TL;DR:** This paper introduces a new probabilistic framework for knowledge graph completion that combines global community structures with local textual features using a deep variational autoencoder.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by text-based approaches in fully incorporating global structural properties of knowledge graphs during completion tasks.

**Method:** A novel KGC framework utilizing sparse latent feature models optimized via a deep variational autoencoder (VAE) that integrates global clustering with local textual features.

**Key Contributions:**

	1. Development of a probabilistic KGC framework using sparse latent feature models.
	2. Integration of global clustering information with local textual features for improved performance.
	3. Enhanced interpretability of underlying latent structures in KGs.

**Result:** The proposed method shows significant performance gains in link prediction tasks across four benchmark datasets compared to existing methods.

**Limitations:** 

**Conclusion:** The framework not only improves missing triples completion but also enhances the interpretability of the latent structures within knowledge graphs.

**Abstract:** Recent advances in knowledge graph completion (KGC) have emphasized text-based approaches to navigate the inherent complexities of large-scale knowledge graphs (KGs). While these methods have achieved notable progress, they frequently struggle to fully incorporate the global structural properties of the graph. Stochastic blockmodels (SBMs), especially the latent feature relational model (LFRM), offer robust probabilistic frameworks for identifying latent community structures and improving link prediction. This paper presents a novel probabilistic KGC framework utilizing sparse latent feature models, optimized via a deep variational autoencoder (VAE). Our proposed method dynamically integrates global clustering information with local textual features to effectively complete missing triples, while also providing enhanced interpretability of the underlying latent structures. Extensive experiments on four benchmark datasets with varying scales demonstrate the significant performance gains achieved by our method.

</details>


### [111] [MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based QA Datasets](https://arxiv.org/abs/2412.21015)

*Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez*

**Main category:** cs.CL

**Keywords:** geospatial, Large Language Models, question answering, mapping services, dataset creation

**Relevance Score:** 8

**TL;DR:** MapQaTor is an open-source framework designed to create reproducible geospatial question answering datasets by integrating with various maps APIs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing mapping and navigation services struggle with natural language geospatial queries; thus, there's a need for better datasets for LLMs to improve understanding in this area.

**Method:** MapQaTor allows users to collect and visualize data from different maps APIs with minimal setup and caches API responses for consistent ground truth.

**Key Contributions:**

	1. Extensible open-source framework for geospatial QA datasets creation
	2. Integration with any maps API for data collection
	3. 30 times faster annotation process compared to manual methods

**Result:** The framework speeds up annotation processes by at least 30 times compared to manual methods, enhancing the creation of geospatial resources like complex datasets.

**Limitations:** 

**Conclusion:** MapQaTor offers a new opportunity for evaluating and advancing LLM-based geospatial reasoning capabilities.

**Abstract:** Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.

</details>


### [112] [Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia Reflect on the Cross-Cultural Sociolinguistic Norms?](https://arxiv.org/abs/2501.03479)

*Sourabrata Mukherjee, Atharva Mehta, Soumya Teotia, Sougata Saha, Akhil Arora, Monojit Choudhury*

**Main category:** cs.CL

**Keywords:** Honorifics, Natural Language Processing, Wikipedia, Bias, Multilingual

**Relevance Score:** 8

**TL;DR:** This paper analyzes honorific usage in Hindi and Bengali Wikipedia, revealing biases and inconsistencies using LLM for annotation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of clear standards for honorific usage in linguistically rich languages on Wikipedia.

**Method:** Conducted large-scale analysis of 10,000 articles per language using LLM to annotate honorifics and socio-demographic features.

**Key Contributions:**

	1. Large-scale analysis of honorifics in Hindi and Bengali Wikipedia
	2. Demonstrated gender bias in honorifics usage
	3. Proposed the need for language-specific editorial guidelines

**Result:** Found significant inconsistencies in honorific usage, with Bengali exhibiting more honorifics than Hindi, and identified gender bias, particularly in Hindi.

**Limitations:** 

**Conclusion:** The study underscores the necessity for Wikipedia to establish language-specific guidelines for honorific usage due to identified biases and inconsistencies.

**Abstract:** Wikipedia, as a massively multilingual, community-driven platform, is a valuable resource for Natural Language Processing (NLP), yet the consistency of honorific usage in honorific-rich languages remains underexplored. Honorifics, subtle yet profound linguistic markers, encode social hierarchies, politeness norms, and cultural values, but Wikipedia's editorial guidelines lack clear standards for their usage in languages where such forms are grammatically and socially prevalent. This paper addresses this gap through a large-scale analysis of third-person honorific pronouns and verb forms in Hindi and Bengali Wikipedia articles. Using Large Language Models (LLM), we automatically annotate 10,000 articles per language for honorific usage and socio-demographic features such as gender, age, fame, and cultural origin. We investigate: (i) the consistency of honorific usage across articles, (ii) how inconsistencies correlate with socio-cultural factors, and (iii) the presence of explicit or implicit biases across languages. We find that honorific usage is consistently more common in Bengali than Hindi, while non-honorific forms are more frequent for infamous, juvenile, and exotic entities in both. Notably, gender bias emerges in both languages, particularly in Hindi, where men are more likely to receive honorifics than women. Our analysis highlights the need for Wikipedia to develop language-specific editorial guidelines for honorific usage.

</details>


### [113] [PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling](https://arxiv.org/abs/2502.01925)

*Avery Ma, Yangchen Pan, Amir-massoud Farahmand*

**Main category:** cs.CL

**Keywords:** many-shot jailbreaking, large language models, adaptive sampling, harmful question-answer pairs, attention analysis

**Relevance Score:** 7

**TL;DR:** This paper describes PANDAS, a technique that enhances many-shot jailbreaking of LLMs by modifying input dialogues and introducing a dataset called ManyHarm.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is to address the vulnerability of large language models (LLMs) to many-shot jailbreaking, which can circumvent safety measures by leveraging the ability of LLMs to handle long input sequences.

**Method:** PANDAS employs a hybrid technique that modifies fabricated dialogues using Positive Affirmations and Negative Demonstrations, combined with an Adaptive Sampling method specific to the target prompt's topic.

**Key Contributions:**

	1. Introduction of PANDAS, a robust technique for enhancing many-shot jailbreaking.
	2. Development of ManyHarm, a dataset for harmful question-answer pairs.
	3. Attention analysis that sheds light on long-context vulnerabilities in LLMs.

**Result:** Experimental results show that PANDAS significantly outperforms baseline methods in scenarios that require long context handling, with detailed attention analysis highlighting how vulnerabilities are exploited.

**Limitations:** The paper may not address the implications of adversarial robustness in real-world applications of LLMs.

**Conclusion:** PANDAS represents a substantial improvement in many-shot jailbreaking, providing insights and methodologies to better understand and mitigate such vulnerabilities in LLMs.

**Abstract:** Many-shot jailbreaking circumvents the safety alignment of LLMs by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational exchanges between the user and the model. These exchanges are randomly sampled from a pool of unsafe question-answer pairs, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with Positive Affirmations, Negative Demonstrations, and an optimized Adaptive Sampling method tailored to the target prompt's topic. We also introduce ManyHarm, a dataset of harmful question-answer pairs, and demonstrate through extensive experiments that PANDAS significantly outperforms baseline methods in long-context scenarios. Through attention analysis, we provide insights into how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking.

</details>


### [114] [TUMLU: A Unified and Native Language Understanding Benchmark for Turkic Languages](https://arxiv.org/abs/2502.11020)

*Jafar Isbarov, Arofat Akhundjanova, Mammad Hajili, Kavsar Huseynova, Dmitry Gaynullin, Anar Rzayev, Osman Tursun, Aizirek Turdubaeva, Ilshat Saetov, Rinat Kharisov, Saule Belginova, Ariana Kenbayeva, Amina Alisheva, Abdullatif Köksal, Samir Rustamov, Duygu Ataman*

**Main category:** cs.CL

**Keywords:** MMLU, multilingual language models, Turkic languages, language understanding benchmarks, TUMLU

**Relevance Score:** 7

**TL;DR:** This paper presents TUMLU, a native language MMLU benchmark for Turkic languages, addressing the limitations of current multilingual models by evaluating their performance using this new dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for high-quality native language benchmarks for multilingual language models, specifically for the under-represented Turkic languages, to enhance the assessment accuracy of their language understanding capabilities.

**Method:** The authors developed two benchmarks: TUMLU for comprehensive evaluation across 11 academic subjects in various Turkic languages, and TUMLU-mini as a concise verified subset. They conducted systematic evaluations of multiple multilingual language models on these benchmarks.

**Key Contributions:**

	1. Development of TUMLU, a comprehensive MMLU benchmark for Turkic languages
	2. Creation of TUMLU-mini, a balanced and manually verified subset
	3. Systematic evaluation of LLMs on newly created benchmarks

**Result:** The evaluation results provided insights into the performance of different LLMs—including Claude, Gemini, GPT, and LLaMA—across languages and subjects, highlighting their strengths and weaknesses in understanding Turkic languages.

**Limitations:** 

**Conclusion:** The introduction of TUMLU and TUMLU-mini facilitates more accurate multilingual language understanding research and supports further development in the field by offering accessible and reliable datasets.

**Abstract:** Being able to thoroughly assess massive multi-task language understanding (MMLU) capabilities is essential for advancing the applicability of multilingual language models. However, preparing such benchmarks in high quality native language is often costly and therefore limits the representativeness of evaluation datasets. While recent efforts focused on building more inclusive MMLU benchmarks, these are conventionally built using machine translation from high-resource languages, which may introduce errors and fail to account for the linguistic and cultural intricacies of the target languages. In this paper, we address the lack of native language MMLU benchmark especially in the under-represented Turkic language family with distinct morphosyntactic and cultural characteristics. We propose two benchmarks for Turkic language MMLU: TUMLU is a comprehensive, multilingual, and natively developed language understanding benchmark specifically designed for Turkic languages. It consists of middle- and high-school level questions spanning 11 academic subjects in Azerbaijani, Crimean Tatar, Karakalpak, Kazakh, Tatar, Turkish, Uyghur, and Uzbek. We also present TUMLU-mini, a more concise, balanced, and manually verified subset of the dataset. Using this dataset, we systematically evaluate a diverse range of open and proprietary multilingual large language models (LLMs), including Claude, Gemini, GPT, and LLaMA, offering an in-depth analysis of their performance across different languages, subjects, and alphabets. To promote further research and development in multilingual language understanding, we release TUMLU-mini and all corresponding evaluation scripts.

</details>


### [115] [Towards Understanding Fine-Tuning Mechanisms of LLMs via Circuit Analysis](https://arxiv.org/abs/2502.11812)

*Xu Wang, Yan Hu, Wenyu Du, Reynold Cheng, Benyou Wang, Difan Zou*

**Main category:** cs.CL

**Keywords:** fine-tuning, Large Language Models, circuit analysis, Low-Rank Adaptation, Mechanistic Interpretability

**Relevance Score:** 8

**TL;DR:** This paper explores the fine-tuning of Large Language Models (LLMs) using circuit analysis to understand its mechanisms, developing a circuit-aware Low-Rank Adaptation (LoRA) method that improves performance.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the mechanisms behind fine-tuning LLMs can enhance their performance in practical applications, which is currently unclear.

**Method:** Circuit analysis is used to study changes in circuits during fine-tuning, and a new circuit-aware Low-Rank Adaptation (LoRA) method is proposed based on findings about circuit dynamics.

**Key Contributions:**

	1. Developed a circuit-aware LoRA method that utilizes circuit changes during fine-tuning
	2. Identified significant changes in circuit edges during fine-tuning
	3. Provided a framework for enhancing fine-tuning in compositional tasks by combining circuits from subtasks.

**Result:** The circuit-based LoRA algorithm shows an average performance improvement of 2.46% over standard LoRA with similar parameter sizes, highlighting significant edge changes in circuits during fine-tuning.

**Limitations:** 

**Conclusion:** This study provides new insights into fine-tuning mechanisms and offers a method to improve LLM performance by understanding and leveraging circuit dynamics.

**Abstract:** Fine-tuning significantly improves the performance of Large Language Models (LLMs), yet its underlying mechanisms remain poorly understood. This paper aims to provide an in-depth interpretation of the fine-tuning process through circuit analysis, a popular tool in Mechanistic Interpretability (MI). Unlike previous studies (Prakash et al. 2024; Chhabra et al. 2024) that focus on tasks where pre-trained models already perform well, we develop a set of mathematical tasks where fine-tuning yields substantial performance gains, which are closer to the practical setting. In our experiments, we identify circuits at various checkpoints during fine-tuning and examine the interplay between circuit analysis, fine-tuning methods, and task complexities. First, we find that while circuits maintain high node similarity before and after fine-tuning, their edges undergo significant changes, in contrast to prior work that shows circuits only add some additional components after fine-tuning. Based on these observations, we develop a circuit-aware Low-Rank Adaptation (LoRA) method, which assigns ranks to layers based on edge changes in the circuits. Experimental results demonstrate that our circuit-based LoRA algorithm achieves an average performance improvement of 2.46% over standard LoRA with similar parameter sizes. Furthermore, we explore how combining circuits from subtasks can enhance fine-tuning in compositional tasks, providing new insights into the design of such tasks and deepening the understanding of circuit dynamics and fine-tuning mechanisms.

</details>


### [116] [Conformal Linguistic Calibration: Trading-off between Factuality and Specificity](https://arxiv.org/abs/2502.19110)

*Zhengping Jiang, Anqi Liu, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** Conformal Linguistic Calibration, uncertainty quantification, abstention, linguistic pragmatics, language models

**Relevance Score:** 9

**TL;DR:** The paper proposes a method called Conformal Linguistic Calibration (CLC) that connects abstention and linguistic calibration in language models, allowing for controllable imprecision in responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Language model outputs can be unreliable, necessitating methods to adapt responses based on uncertainty without losing valuable information or complicating downstream tasks.

**Method:** The proposed unified approach, CLC, integrates linguistic calibration and abstention through a framework of linguistic pragmatics and facilitates controlled imprecision in model outputs.

**Key Contributions:**

	1. Introduction of Conformal Linguistic Calibration (CLC) as a unified approach to linguistic calibration and abstention.
	2. Demonstration of a framework connecting linguistic pragmatics with adaptive language model outputs.
	3. Results show effectiveness in producing reliable outputs while allowing control over response specificity.

**Result:** The CLC method produces calibrated outputs that maintain factual accuracy and supports uncertainty-aware adaptive claim rewriting, balancing factuality and specificity.

**Limitations:** The exploration of CLC is still preliminary, requiring further study to fully evaluate its effectiveness across various applications.

**Conclusion:** The findings suggest that CLC provides an effective way to improve the reliability and usability of model outputs in uncertain situations.

**Abstract:** Language model outputs are not always reliable, thus prompting research into how to adapt model responses based on uncertainty. Common approaches include: \emph{abstention}, where models refrain from generating responses when uncertain; and \emph{linguistic calibration}, where models hedge their statements using uncertainty quantifiers. However, abstention can withhold valuable information, while linguistically calibrated responses are often challenging to leverage in downstream tasks. We propose a unified view, Conformal Linguistic Calibration (CLC), which reinterprets linguistic calibration as \emph{answer set prediction}. First we present a framework connecting abstention and linguistic calibration through the lens of linguistic pragmatics. We then describe an implementation of CLC that allows for controlling the level of imprecision in model responses. Results demonstrate our method produces calibrated outputs with conformal guarantees on factual accuracy. Further, our approach enables fine-tuning models to perform uncertainty-aware adaptive claim rewriting, offering a controllable balance between factuality and specificity.

</details>


### [117] [MEDDxAgent: A Unified Modular Agent Framework for Explainable Automatic Differential Diagnosis](https://arxiv.org/abs/2502.19175)

*Daniel Rose, Chia-Chien Hung, Marco Lepri, Israa Alqassem, Kiril Gashteovski, Carolin Lawrence*

**Main category:** cs.CL

**Keywords:** Differential Diagnosis, Large Language Models, Health Informatics

**Relevance Score:** 9

**TL;DR:** MEDDxAgent enhances differential diagnosis by enabling iterative learning and integrating modular components for better explainability and accuracy in diagnostics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing approaches to differential diagnosis using LLMs have significant limitations, such as reliance on complete patient profiles and single dataset evaluations.

**Method:** The MEDDxAgent framework includes an orchestrator, a history taking simulator, and specialized agents for knowledge retrieval and diagnostic strategy, allowing for interactive and iterative diagnostic processes.

**Key Contributions:**

	1. Introduction of a Modular Explainable DDx Agent (MEDDxAgent) framework for iterative DDx.
	2. Comprehensive benchmark for evaluating DDx across new disease categories.
	3. Demonstration of over 10% accuracy improvement in interactive DDx compared to existing approaches.

**Result:** MEDDxAgent shows over 10% accuracy improvements in interactive differential diagnosis across various disease categories compared to traditional methods.

**Limitations:** 

**Conclusion:** The framework provides not only improved accuracy in diagnostic recommendations but also critical explainability regarding the reasoning behind those recommendations, addressing major gaps in current DDx practices.

**Abstract:** Differential Diagnosis (DDx) is a fundamental yet complex aspect of clinical decision-making, in which physicians iteratively refine a ranked list of possible diseases based on symptoms, antecedents, and medical knowledge. While recent advances in large language models (LLMs) have shown promise in supporting DDx, existing approaches face key limitations, including single-dataset evaluations, isolated optimization of components, unrealistic assumptions about complete patient profiles, and single-attempt diagnosis. We introduce a Modular Explainable DDx Agent (MEDDxAgent) framework designed for interactive DDx, where diagnostic reasoning evolves through iterative learning, rather than assuming a complete patient profile is accessible. MEDDxAgent integrates three modular components: (1) an orchestrator (DDxDriver), (2) a history taking simulator, and (3) two specialized agents for knowledge retrieval and diagnosis strategy. To ensure robust evaluation, we introduce a comprehensive DDx benchmark covering respiratory, skin, and rare diseases. We analyze single-turn diagnostic approaches and demonstrate the importance of iterative refinement when patient profiles are not available at the outset. Our broad evaluation demonstrates that MEDDxAgent achieves over 10% accuracy improvements in interactive DDx across both large and small LLMs, while offering critical explainability into its diagnostic reasoning process.

</details>


### [118] [Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts](https://arxiv.org/abs/2503.09347)

*Hongyu Chen, Seraphina Goldfarb-Tarrant*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety evaluation, artifact sensitivity, automated judging, human alignment

**Relevance Score:** 9

**TL;DR:** This study evaluates the reliability of Large Language Models (LLMs) as automated evaluators for content safety, highlighting biases and the influence of input artifacts on judgments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs as automated evaluators raises concerns about their reliability in assessing content safety.

**Method:** The study evaluates 11 LLM judge models on self-consistency, alignment with human judgments, and susceptibility to input artifacts, particularly examining how artifacts influence judgments.

**Key Contributions:**

	1. Evaluation of 11 LLM judge models across critical safety dimensions.
	2. Demonstration of significant biases affecting LLM evaluations based on input artifacts.
	3. Exploration of jury-based evaluations to enhance robustness and human alignment.

**Result:** Biases in LLM judges notably distort safety evaluations, with apologetic language artifacts skewing preferences by up to 98%. Larger models do not always show greater robustness, while some smaller models are more resistant to specific artifacts.

**Limitations:** Artifact sensitivity persists even in the best jury configurations, indicating ongoing challenges in LLM evaluations.

**Conclusion:** The findings indicate a need for diversified evaluation methodologies that can resist input artifacts to improve safety assessments.

**Abstract:** Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.

</details>


### [119] [LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](https://arxiv.org/abs/2503.21227)

*Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, Qingpei Guo, Si Liu*

**Main category:** cs.CL

**Keywords:** Mixture of Experts, continual learning, large language models, robust knowledge retention, parameter efficiency

**Relevance Score:** 9

**TL;DR:** LLaVA-CMoE is a continual learning framework for LLMs that enables parameter-efficient model expansion and robust knowledge retention without replay data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of parameter growth and catastrophic forgetting in large language models (LLMs) during continual multimodal learning.

**Method:** The paper introduces a Probe-Guided Knowledge Extension mechanism for determining new experts' addition and a Probabilistic Task Locator that assigns lightweight routers for tasks and uses VAE for router identification during inference.

**Key Contributions:**

	1. Introduction of the Probe-Guided Knowledge Extension mechanism
	2. Development of the Probabilistic Task Locator for dynamic expert allocation
	3. Demonstration of effective continual learning performance with reduced forgetting and model size

**Result:** LLaVA-CMoE demonstrates strong performance on the CoIN benchmark, covering eight VQA tasks, achieving reduced forgetting and parameter overhead compared to prior methods.

**Limitations:** 

**Conclusion:** The approach effectively scales LLMs for continual learning while maintaining parameter efficiency and knowledge retention.

**Abstract:** Mixture of Experts (MoE) architectures have recently advanced the scalability and adaptability of large language models (LLMs) for continual multimodal learning. However, efficiently extending these models to accommodate sequential tasks remains challenging. As new tasks arrive, naive model expansion leads to rapid parameter growth, while modifying shared routing components often causes catastrophic forgetting, undermining previously learned knowledge. To address these issues, we propose LLaVA-CMoE, a continual learning framework for LLMs that requires no replay data of previous tasks and ensures both parameter efficiency and robust knowledge retention. Our approach introduces a Probe-Guided Knowledge Extension mechanism, which uses probe experts to dynamically determine when and where new experts should be added, enabling adaptive and minimal parameter expansion tailored to task complexity. Furthermore, we present a Probabilistic Task Locator that assigns each task a dedicated, lightweight router. To handle the practical issue that task labels are unknown during inference, we leverage a VAE-based reconstruction strategy to identify the most suitable router by matching input distributions, allowing automatic and accurate expert allocation. This design mitigates routing conflicts and catastrophic forgetting, enabling robust continual learning without explicit task labels. Extensive experiments on the CoIN benchmark, covering eight diverse VQA tasks, demonstrate that LLaVA-CMoE delivers strong continual learning performance with a compact model size, significantly reducing forgetting and parameter overhead compared to prior methods. These results showcase the effectiveness and scalability of our approach for parameter-efficient continual learning in large language models. Our code will be open-sourced soon.

</details>


### [120] [Deep Binding of Language Model Virtual Personas: a Study on Approximating Political Partisan Misperceptions](https://arxiv.org/abs/2504.11673)

*Minwoo Kang, Suhong Moon, Seung Hyeong Lee, Ayush Raj, Joseph Suh, David M. Chan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human Behavior Simulation, Political Science, In-group/Out-group Bias, Synthetic Backstories

**Relevance Score:** 8

**TL;DR:** This paper explores the depth of Large Language Models' (LLMs) ability to simulate human responses in surveys through the development of synthetic user backstories, enhancing the fidelity and application of LLMs in political science.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs' responses to surveys are deeply representative of in-group members or merely reflect out-group perceptions, which is crucial for studies on polarization and conflict.

**Method:** We developed a novel methodology that constructs virtual personas with rich synthetic backstories via extensive multi-turn interviews, allowing for a more nuanced simulation of human-like responses.

**Key Contributions:**

	1. Novel methodology for creating detailed virtual personas
	2. Significant improvement in replicating human responses
	3. Broader applicability of LLMs in political and social sciences

**Result:** Our method achieved up to an 87% improvement in replicating human response distributions, demonstrating high fidelity in virtual persona responses compared to original studies of in-group/out-group biases.

**Limitations:** 

**Conclusion:** This work allows for broader applications of LLMs in social science research, moving beyond basic estimations of socially understood attitudes.

**Abstract:** Large language models (LLMs) are increasingly capable of simulating human behavior, offering cost-effective ways to estimate user responses to various surveys and polls. However, the questions in these surveys usually reflect socially understood attitudes: the patterns of attitudes of old/young, liberal/conservative, as understood by both members and non-members of those groups. It is not clear whether the LLM binding is \emph{deep}, meaning the LLM answers as a member of a particular in-group would, or \emph{shallow}, meaning the LLM responds as an out-group member believes an in-group member would. To explore this difference, we use questions that expose known in-group/out-group biases. This level of fidelity is critical for applying LLMs to various political science studies, including timely topics on polarization dynamics, inter-group conflict, and democratic backsliding. To this end, we propose a novel methodology for constructing virtual personas with synthetic user ``backstories" generated as extended, multi-turn interview transcripts. Our generated backstories are longer, rich in detail, and consistent in authentically describing a singular individual, compared to previous methods. We show that virtual personas conditioned on our backstories closely replicate human response distributions (up to an 87\% improvement as measured by Wasserstein Distance) and produce effect sizes that closely match those observed in the original studies of in-group/out-group biases. Altogether, our work extends the applicability of LLMs beyond estimating socially understood responses, enabling their use in a broader range of human studies.

</details>


### [121] [D-GEN: Automatic Distractor Generation and Evaluation for Reliable Assessment of Generative Model](https://arxiv.org/abs/2504.13439)

*Grace Byun, Jinho D. Choi*

**Main category:** cs.CL

**Keywords:** Generative Models, Distractor Generation, Multiple-Choice Evaluation

**Relevance Score:** 5

**TL;DR:** D-GEN is an open-source distractor generator that transforms open-ended data into multiple-choice formats and evaluates the quality of distractors using novel methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a challenge in evaluating generative models due to inconsistencies in response formats, and existing methods for generating distractors are labor-intensive.

**Method:** Introduction of D-GEN, which generates distractors from open-ended data and evaluates them using ranking alignment and entropy analysis.

**Key Contributions:**

	1. D-GEN as the first open-source distractor generator model
	2. Proposed novel methods for evaluating distractor quality
	3. High ranking consistency and matching entropy distribution with ground-truth distractors

**Result:** D-GEN achieves high preservation of ranking consistency and matches entropy distributions of ground-truth distractors, supported by positive human evaluations.

**Limitations:** 

**Conclusion:** D-GEN sets a new standard for robust and efficient distractor generation and automated evaluation in multiple-choice settings.

**Abstract:** Evaluating generative models with open-ended generation is challenging due to inconsistencies in response formats. Multiple-choice (MC) evaluation mitigates this issue, but generating high-quality distractors is time-consuming and labor-intensive. We introduce D-GEN, the first open-source distractor generator model that transforms open-ended data into an MC format. To evaluate distractor quality, we propose two novel methods: (1) ranking alignment, ensuring generated distractors retain the discriminatory power of ground-truth distractors, and (2) entropy analysis, comparing model confidence distributions. Our results show that D-GEN preserves ranking consistency (Spearman's rho 0.99, Kendall's tau 0.94) and closely matches the entropy distribution of ground-truth distractors. Human evaluation further confirms the fluency, coherence, distractiveness, and incorrectness. Our work advances robust and efficient distractor generation with automated evaluation, setting a new standard for MC evaluation.

</details>


### [122] [Long-context Non-factoid Question Answering in Indic Languages](https://arxiv.org/abs/2504.13615)

*Ritwik Mishra, Rajiv Ratn Shah, Ponnurangam Kumaraguru*

**Main category:** cs.CL

**Keywords:** Question Answering, Large Language Models, Context-shortening, Indic languages, Open Information Extraction

**Relevance Score:** 8

**TL;DR:** This study investigates context-shortening techniques for improving Question Answering (QA) performance in low-resource Indic languages, demonstrating notable gains in semantic and token-level scores across LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Long contexts in QA tasks are challenging for LLMs, particularly in low-resource Indic languages, necessitating exploration of context-shortening methods to improve effectiveness and efficiency.

**Method:** The study evaluates several context-shortening techniques, including Open Information Extraction (OIE), coreference resolution, and Answer Paragraph Selection (APS), testing their impact on QA performance using standard metrics on four Indic languages.

**Key Contributions:**

	1. Demonstrated effectiveness of context-shortening techniques in improving QA for low-resource Indic languages.
	2. Highlighted the reduction in computational overhead from context-shortening.
	3. Provided insights into the limitations of LLM-based QA systems regarding reasoning and non-factoid questions.

**Result:** The context-shortening techniques result in a 4% average improvement in semantic scores and a 47% improvement in token-level scores without fine-tuning, with additional gains observed when fine-tuning the models.

**Limitations:** LLM-based QA systems struggle with non-factoid questions that require reasoning or debate; verbalizing OIE-generated triples does not improve performance.

**Conclusion:** Context-shortening improves the performance and efficiency of LLM-based QA systems for Indic languages, though limitations in handling non-factoid questions remain.

**Abstract:** Question Answering (QA) tasks, which involve extracting answers from a given context, are relatively straightforward for modern Large Language Models (LLMs) when the context is short. However, long contexts pose challenges due to the quadratic complexity of the self-attention mechanism. This challenge is compounded in Indic languages, which are often low-resource. This study explores context-shortening techniques, including Open Information Extraction (OIE), coreference resolution, Answer Paragraph Selection (APS), and their combinations, to improve QA performance. Compared to the baseline of unshortened (long) contexts, our experiments on four Indic languages (Hindi, Tamil, Telugu, and Urdu) demonstrate that context-shortening techniques yield an average improvement of 4\% in semantic scores and 47\% in token-level scores when evaluated on three popular LLMs without fine-tuning. Furthermore, with fine-tuning, we achieve an average increase of 2\% in both semantic and token-level scores. Additionally, context-shortening reduces computational overhead. Explainability techniques like LIME and SHAP reveal that when the APS model confidently identifies the paragraph containing the answer, nearly all tokens within the selected text receive high relevance scores. However, the study also highlights the limitations of LLM-based QA systems in addressing non-factoid questions, particularly those requiring reasoning or debate. Moreover, verbalizing OIE-generated triples does not enhance system performance. These findings emphasize the potential of context-shortening techniques to improve the efficiency and effectiveness of LLM-based QA systems, especially for low-resource languages. The source code and resources are available at https://github.com/ritwikmishra/IndicGenQA.

</details>


### [123] [Automatic Construction of Multiple Classification Dimensions for Managing Approaches in Scientific Papers](https://arxiv.org/abs/2505.23252)

*Bing Ma, Hai Zhuge*

**Main category:** cs.CL

**Keywords:** approach patterns, multi-dimensional framework, clustering algorithm

**Relevance Score:** 4

**TL;DR:** This paper proposes a multi-dimensional framework for querying scientific approaches by identifying patterns and similarities among them, improving the efficiency of retrieval and management.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Researchers struggle with managing and querying vast numbers of approaches from scientific papers, necessitating a systematic method for efficient retrieval.

**Method:** The paper employs a top-down identification of approach patterns across four linguistic levels, proposes a tree-structure-based similarity measure for approaches, and introduces a bottom-up clustering algorithm to form class trees for better organization.

**Key Contributions:**

	1. Identification of approach patterns at multiple linguistic levels
	2. Development of a tree structure for representing and measuring similarities between approaches
	3. Introduction of a bottom-up clustering algorithm for class trees in a multi-dimensional approach space

**Result:** A multi-dimensional approach space is created, enabling efficient querying that maintains strong relevance between user queries and retrieved approaches, significantly reducing the search space.

**Limitations:** 

**Conclusion:** The proposed structure and methodologies provide a scalable solution for querying approaches in scientific research, enhancing usability and relevance.

**Abstract:** Approaches form the foundation for conducting scientific research. Querying approaches from a vast body of scientific papers is extremely time-consuming, and without a well-organized management framework, researchers may face significant challenges in querying and utilizing relevant approaches. Constructing multiple dimensions on approaches and managing them from these dimensions can provide an efficient solution. Firstly, this paper identifies approach patterns using a top-down way, refining the patterns through four distinct linguistic levels: semantic level, discourse level, syntactic level, and lexical level. Approaches in scientific papers are extracted based on approach patterns. Additionally, five dimensions for categorizing approaches are identified using these patterns. This paper proposes using tree structure to represent step and measuring the similarity between different steps with a tree-structure-based similarity measure that focuses on syntactic-level similarities. A collection similarity measure is proposed to compute the similarity between approaches. A bottom-up clustering algorithm is proposed to construct class trees for approach components within each dimension by merging each approach component or class with its most similar approach component or class in each iteration. The class labels generated during the clustering process indicate the common semantics of the step components within the approach components in each class and are used to manage the approaches within the class. The class trees of the five dimensions collectively form a multi-dimensional approach space. The application of approach queries on the multi-dimensional approach space demonstrates that querying within this space ensures strong relevance between user queries and results and rapidly reduces search space through a class-based query mechanism.

</details>
