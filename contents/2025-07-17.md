# 2025-07-17

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 18]

- [cs.CL](#cs.CL) [Total: 66]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Neuroaesthetics and the Science of Visual Experience](https://arxiv.org/abs/2507.11599)

*Harish Vijayakumar*

**Main category:** cs.HC

**Keywords:** neuroaesthetics, aesthetic experiences, graphic design, interface design, neuroscience

**Relevance Score:** 4

**TL;DR:** This paper explores the neural mechanisms of aesthetic experiences in the field of neuroaesthetics, examining how beauty is processed in the brain and its implications for design.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the interaction between perception, emotion, and cognition in generating aesthetic experiences, and to inform design practices.

**Method:** The paper analyzes neural mechanisms underlying aesthetic experiences through interdisciplinary insights from neuroscience, psychology, and arts.

**Key Contributions:**

	1. Analysis of the neural mechanisms behind aesthetic experiences
	2. Interdisciplinary approach combining neuroscience, psychology, and art
	3. Insights for improving graphic and interface design based on aesthetic principles

**Result:** It reveals that impactful design goes beyond aesthetics; it significantly affects engagement and connection, enhancing user experience.

**Limitations:** 

**Conclusion:** Understanding neuroaesthetics can improve graphic and interface design by emphasizing meaningful visual experiences.

**Abstract:** Neuroaesthetics is an interdisciplinary field that brings together neuroscience, psychology, and the arts to explore how the human brain perceives and responds to visual beauty. This paper examines the neural mechanisms behind aesthetic experiences, aiming to explain why certain designs or artworks feel emotionally or cognitively "right." By analyzing the interaction between perception, emotion, and cognition, neuroaesthetics reveals how beauty is constructed in the brain and how this understanding can inform fields such as graphic and interface design. This paper offers a clear and accessible overview of core neuroaesthetic principles, making the subject approachable to a wide audience. The findings suggest that impactful design is more than surface-level appeal: well-crafted visual experiences can engage, support, and connect people in meaningful ways.

</details>


### [2] [DiaryPlay: AI-Assisted Authoring of Interactive Vignettes for Everyday Storytelling](https://arxiv.org/abs/2507.11628)

*Jiangnan Xu, Haeseul Cha, Gosu Choi, Gyu-cheol Lee, Yeo-Jin Yoon, Zucheul Lee, Konstantinos Papangelis, Dae Hyun Kim, Juho Kim*

**Main category:** cs.HC

**Keywords:** Interactive storytelling, AI authoring, Narrative planning, Human-computer interaction, Everyday storytelling

**Relevance Score:** 7

**TL;DR:** DiaryPlay is an AI-assisted system that simplifies the creation of interactive vignettes for everyday storytelling by extracting core elements from natural language stories.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the potential of interactive vignettes to enhance storytelling, their complexity hinders everyday authors from using them. DiaryPlay aims to streamline the authoring process.

**Method:** DiaryPlay takes a natural language story and extracts the core elements (environment, characters, events) necessary for crafting an interactive vignette. It then uses an LLM-based narrative planner to convert a single-branch story into a branch-and-bottleneck narrative structure.

**Key Contributions:**

	1. Introduction of an AI-assisted authoring system for interactive vignettes.
	2. Use of LLM for narrative planning to facilitate branching story development.
	3. Empirical evaluations of character believability and author support.

**Result:** Technical evaluations demonstrate that the character activities generated by DiaryPlay are perceived as believable as those authored by humans. User studies indicate that the system effectively assists authors and provides engaging experiences.

**Limitations:** Limited to a small number of participants in evaluations.

**Conclusion:** DiaryPlay enables authors to focus on refining their interactive vignettes while ensuring quality viewer interaction and maintaining authorial intent.

**Abstract:** An interactive vignette is a popular and immersive visual storytelling approach that invites viewers to role-play a character and influences the narrative in an interactive environment. However, it has not been widely used by everyday storytellers yet due to authoring complexity, which conflicts with the immediacy of everyday storytelling. We introduce DiaryPlay, an AI-assisted authoring system for interactive vignette creation in everyday storytelling. It takes a natural language story as input and extracts the three core elements of an interactive vignette (environment, characters, and events), enabling authors to focus on refining these elements instead of constructing them from scratch. Then, it automatically transforms the single-branch story input into a branch-and-bottleneck structure using an LLM-powered narrative planner, which enables flexible viewer interactions while freeing the author from multi-branching. A technical evaluation (N=16) shows that DiaryPlay-generated character activities are on par with human-authored ones regarding believability. A user study (N=16) shows that DiaryPlay effectively supports authors in creating interactive vignette elements, maintains authorial intent while reacting to viewer interactions, and provides engaging viewing experiences.

</details>


### [3] [CLAImate: AI-Enabled Climate Change Communication through Personalized and Localized Narrative Visualizations](https://arxiv.org/abs/2507.11677)

*Mashrur Rashik, Jean-Daniel Fekete, Narges Mahyar*

**Main category:** cs.HC

**Keywords:** climate change, personalization, AI, visualization, user experience

**Relevance Score:** 3

**TL;DR:** CLAImate is an AI-enabled prototype that personalizes climate conversation narratives and localizes visualizations based on users' knowledge and location.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Communicating climate change effectively remains difficult due to the abstract nature of climate reports, necessitating a more personalized approach to engage the public.

**Method:** CLAImate personalizes narratives and visualizes climate information tailored to individual users based on their knowledge and geographic context. Its effectiveness was evaluated through factual correctness checks, expert studies, and user pilot tests in the UK.

**Key Contributions:**

	1. Develops personalized narratives for climate communication.
	2. Integrates local visualizations based on user geography.
	3. Evaluates effectiveness through user studies and expert feedback.

**Result:** CLAImate achieved a 66% accuracy in Natural Language Inference (SNLI) and a 70% FACTSCORE. Users reported improved understanding of climate risks, and experts praised the clarity of visualizations.

**Limitations:** Challenges in scalability and accuracy of personalization techniques.

**Conclusion:** While CLAImate demonstrates promise in personalizing climate communication, challenges in personalization, accuracy, and scalability remain, indicating areas for future research.

**Abstract:** Communicating climate change remains challenging, as climate reports, though rich in data and visualizations, often feel too abstract or technical for the public. Although personalization can enhance communication, most tools still lack the narrative and visualization tailoring needed to connect with individual experiences. We present CLAImate, an AI-enabled prototype that personalizes conversation narratives and localizes visualizations based on users' climate knowledge and geographic location. We evaluated CLAImate through internal verification of factual correctness, a formative study with experts, and a pilot with UK residents. CLAImate achieved 66% SNLI accuracy and 70% FACTSCORE. Visualization experts appreciated its clarity and personalization, and seven out of ten UK participants reported better understanding and local relevance of climate risks with CLAImate. We also discuss design challenges in personalization, accuracy, and scalability, and outline future directions for integrating visualizations in personalized conversational interfaces.

</details>


### [4] [GIST: Group Interaction Sensing Toolkit for Mixed Reality](https://arxiv.org/abs/2507.11797)

*Diana Romero, Yasra Chandio, Fatima Anwar, Salma Elmalaki*

**Main category:** cs.HC

**Keywords:** mixed-reality, team collaboration, multi-modal interaction, GIST, behavior patterns

**Relevance Score:** 8

**TL;DR:** This paper presents GIST, a toolkit for capturing multi-modal interaction data in mixed-reality environments to enhance team collaboration insights.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of team dynamics and interactions in mixed-reality applications and move beyond limitations of existing methods.

**Method:** GIST captures data from MR headset sensors, allowing for the analysis of speech, gaze, and proximity to derive interaction networks and behavior patterns.

**Key Contributions:**

	1. Development of a novel toolkit (GIST) for multi-modal data capture in MR.
	2. Evaluation through human subjects study confirming the validity of the approach.
	3. Insight into the dynamics of team interactions during collaborative tasks.

**Result:** The study with 48 participants demonstrated strong correlations between behavior modes and shifts in interaction network structure.

**Limitations:** 

**Conclusion:** GIST effectively captures and analyzes multi-modal interaction data, providing insights into team dynamics in MR environments.

**Abstract:** Understanding how teams coordinate, share work, and negotiate roles in immersive environments is critical for designing effective mixed-reality (MR) applications that support real-time collaboration. However, existing methods either rely on external cameras and offline annotation or focus narrowly on single modalities, limiting their validity and applicability. To address this, we present a novel group interaction sensing toolkit (GIST), a deployable system that passively captures multi-modal interaction data, such as speech, gaze, and spatial proximity from commodity MR headset's sensors and automatically derives both overall static interaction networks and dynamic moment-by-moment behavior patterns. We evaluate GIST with a human subject study with 48 participants across 12 four-person groups performing an open-ended image-sorting task in MR. Our analysis shows strong alignment between the identified behavior modes and shifts in interaction network structure, confirming that momentary changes in speech, gaze, and proximity data are observable through the sensor data.

</details>


### [5] ["Mapping What I Feel": Understanding Affective Geovisualization Design Through the Lens of People-Place Relationships](https://arxiv.org/abs/2507.11841)

*Xingyu Lan, Yutong Yang, Yifan Wang*

**Main category:** cs.HC

**Keywords:** affective visualization, geovisualization, emotion, design taxonomy, human experience

**Relevance Score:** 5

**TL;DR:** This paper explores affective geovisualization design, focusing on how geographic visualizations can influence emotions, providing a design taxonomy and identifying high-level design paradigms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the underexplored area of affective visualization design, specifically within affective geovisualization, and to provide domain-specific insights and frameworks.

**Method:** The study employs the Person-Process-Place (PPP) model from geographic theory to analyze a curated corpus of affective geovisualization designs, leading to the development of a design taxonomy and identification of design paradigms.

**Key Contributions:**

	1. Development of a design taxonomy for affective geovisualization
	2. Identification of four high-level design paradigms
	3. Provision of domain-specific insights for future research

**Result:** The analysis resulted in a design taxonomy that characterizes various methods for eliciting emotions through geographic visualization and identified four high-level design paradigms.

**Limitations:** 

**Conclusion:** The paper extends existing frameworks to include geographic specificity, offering examples and guiding future research in affective geovisualization.

**Abstract:** Affective visualization design is an emerging research direction focused on communicating and influencing emotion through visualization. However, as revealed by previous research, this area is highly interdisciplinary and involves theories and practices from diverse fields and disciplines, thus awaiting analysis from more fine-grained angles. To address this need, this work focuses on a pioneering and relatively mature sub-area, affective geovisualization design, to further the research in this direction and provide more domain-specific insights. Through an analysis of a curated corpus of affective geovisualization designs using the Person-Process-Place (PPP) model from geographic theory, we derived a design taxonomy that characterizes a variety of methods for eliciting and enhancing emotions through geographic visualization. We also identified four underlying high-level design paradigms of affective geovisualization design (e.g., computational, anthropomorphic) that guide distinct approaches to linking geographic information with human experience. By extending existing affective visualization design frameworks with geographic specificity, we provide additional design examples, domain-specific analyses, and insights to guide future research and practices in this underexplored yet highly innovative domain.

</details>


### [6] [Interactive Hybrid Rice Breeding with Parametric Dual Projection](https://arxiv.org/abs/2507.11848)

*Changjian Chen, Pengcheng Wang, Fei Lyu, Zhuo Tang, Li Yang, Long Wang, Yong Cai, Feng Yu, Kenli Li*

**Main category:** cs.HC

**Keywords:** Hybrid Rice Breeding, Genomic Selection, Visual Analysis, Regulatory Genes, Hybrid Selection

**Relevance Score:** 0

**TL;DR:** This paper presents a visual analysis method for improving hybrid rice breeding through regulatory gene identification and hybrid selection.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to enhance the accuracy and efficiency of hybrid rice breeding by integrating genomic prediction models with breeders' experience.

**Method:** A parametric dual projection method was developed for interactive dual analysis tasks, incorporating gene visualization and hybrid visualization techniques.

**Key Contributions:**

	1. Development of a parametric dual projection method for hybrid rice breeding
	2. Creation of visualization tools for regulatory gene and hybrid identification
	3. Demonstration of method effectiveness through case studies and feedback from breeders

**Result:** The proposed method significantly aided in the identification of regulatory genes and desired hybrids, as evidenced by quantitative evaluations and positive user feedback.

**Limitations:** 

**Conclusion:** The study concludes that the visual analysis method can streamline the hybrid rice breeding process and improve selection outcomes for breeders.

**Abstract:** Hybrid rice breeding crossbreeds different rice lines and cultivates the resulting hybrids in fields to select those with desirable agronomic traits, such as higher yields. Recently, genomic selection has emerged as an efficient way for hybrid rice breeding. It predicts the traits of hybrids based on their genes, which helps exclude many undesired hybrids, largely reducing the workload of field cultivation. However, due to the limited accuracy of genomic prediction models, breeders still need to combine their experience with the models to identify regulatory genes that control traits and select hybrids, which remains a time-consuming process. To ease this process, in this paper, we proposed a visual analysis method to facilitate interactive hybrid rice breeding. Regulatory gene identification and hybrid selection naturally ensemble a dual-analysis task. Therefore, we developed a parametric dual projection method with theoretical guarantees to facilitate interactive dual analysis. Based on this dual projection method, we further developed a gene visualization and a hybrid visualization to verify the identified regulatory genes and hybrids. The effectiveness of our method is demonstrated through the quantitative evaluation of the parametric dual projection method, identified regulatory genes and desired hybrids in the case study, and positive feedback from breeders.

</details>


### [7] [Unveiling the Visual Rhetoric of Persuasive Cartography: A Case Study of the Design of Octopus Maps](https://arxiv.org/abs/2507.11903)

*Daocheng Lin, Yifan Wang, Yutong Yang, Xingyu Lan*

**Main category:** cs.HC

**Keywords:** data visualization, persuasion, rhetorical construction, octopus maps, ethical concerns

**Relevance Score:** 4

**TL;DR:** This paper analyzes octopus maps as persuasive visual tools that utilize rhetorical construction, revealing their historical significance and contemporary relevance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in visualization research regarding the persuasive power of rhetorical construction in data visualizations.

**Method:** A focused analysis of 90 octopus maps from the 19th century to contemporary times using rhetorical schema theory to examine their persuasive intents.

**Key Contributions:**

	1. Analysis of persuasive power in octopus maps
	2. Identification of rhetorical strategies in visual metaphors
	3. Discussion of contemporary relevance and ethical issues in data visualization

**Result:** The analysis shows that octopus maps remain relevant in today's digital age, highlighting the dynamic nature of their rhetorical construction and ethical issues in persuasive visualization.

**Limitations:** 

**Conclusion:** Octopus maps illustrate the importance of understanding rhetorical strategies in visualizations, emphasizing the need to consider ethical concerns in their design.

**Abstract:** When designed deliberately, data visualizations can become powerful persuasive tools, influencing viewers' opinions, values, and actions. While researchers have begun studying this issue (e.g., to evaluate the effects of persuasive visualization), we argue that a fundamental mechanism of persuasion resides in rhetorical construction, a perspective inadequately addressed in current visualization research. To fill this gap, we present a focused analysis of octopus maps, a visual genre that has maintained persuasive power across centuries and achieved significant social impact. Employing rhetorical schema theory, we collected and analyzed 90 octopus maps spanning from the 19th century to contemporary times. We closely examined how octopus maps implement their persuasive intents and constructed a design space that reveals how visual metaphors are strategically constructed and what common rhetorical strategies are applied to components such as maps, octopus imagery, and text. Through the above analysis, we also uncover a set of interesting findings. For instance, contrary to the common perception that octopus maps are primarily a historical phenomenon, our research shows that they remain a lively design convention in today's digital age. Additionally, while most octopus maps stem from Western discourse that views the octopus as an evil symbol, some designs offer alternative interpretations, highlighting the dynamic nature of rhetoric across different sociocultural settings. Lastly, drawing from the lessons provided by octopus maps, we discuss the associated ethical concerns of persuasive visualization.

</details>


### [8] [AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding](https://arxiv.org/abs/2507.11911)

*Xiaoqing Chen, Siyang Li, Dongrui Wu*

**Main category:** cs.HC

**Keywords:** EEG decoding, brain-computer interfaces, cross-dataset learning

**Relevance Score:** 4

**TL;DR:** The paper presents a novel calibration-free framework, AFPM, for cross-dataset EEG decoding that improves BCI performance without dataset-specific tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Cross-dataset learning in EEG decoding for BCIs faces challenges due to inconsistencies in channel layouts and non-stationary signal distributions.

**Method:** The framework consists of two components: Spatial Alignment to standardize channels and align EEG distributions, and Frame-Patch Encoding to create unified spatiotemporal patches for decoding.

**Key Contributions:**

	1. Introduction of the first calibration-free framework for EEG decoding
	2. Improved alignment of EEG signals across datasets
	3. Enhanced performance of BCI systems without dataset-specific tuning

**Result:** The AFPM framework outperforms 17 state-of-the-art methods, achieving performance improvements of 4.40% in motor imagery and 3.58% in event-related potential tasks without the need for calibration.

**Limitations:** 

**Conclusion:** AFPM significantly enhances the practicality of BCIs in real-world settings by providing a calibration-free solution for EEG decoding across different datasets.

**Abstract:** Electroencephalogram (EEG) decoding models for brain-computer interfaces (BCIs) struggle with cross-dataset learning and generalization due to channel layout inconsistencies, non-stationary signal distributions, and limited neurophysiological prior integration. To address these issues, we propose a plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has two main components: 1) Spatial Alignment, which selects task-relevant channels based on brain-region priors, aligns EEG distributions across domains, and remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding, which models multi-dataset signals into unified spatiotemporal patches for EEG decoding. Compared to 17 state-of-the-art approaches that need dataset-specific tuning, the proposed calibration-free AFPM achieves performance gains of up to 4.40% on motor imagery and 3.58% on event-related potential tasks. To our knowledge, this is the first calibration-free cross-dataset EEG decoding framework, substantially enhancing the practicalness of BCIs in real-world applications.

</details>


### [9] [d-DQIVAR: Data-centric Visual Analytics and Reasoning for Data Quality Improvement](https://arxiv.org/abs/2507.11960)

*Hyein Hong, Sangbong Yoo, SeokHwan Choi, Jisue Kim, Seongbum Seo, Haneol Cho, Chansoo Kim, Yun Jang*

**Main category:** cs.HC

**Keywords:** data quality, visual analytics, machine learning, data preprocessing

**Relevance Score:** 8

**TL;DR:** The paper presents d-DQIVAR, a visual analytics system aimed at improving data quality to enhance ML model performance by combining data-driven and process-driven approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of traditional batch data preprocessing techniques in optimizing ML model performance and to focus on genuine data quality improvement.

**Method:** The d-DQIVAR system integrates visual analytics techniques to implement both data-driven (imputation, outlier detection, etc.) and process-driven (evaluating DQ and DQI using Kolmogorov-Smirnov test) strategies.

**Key Contributions:**

	1. Introduction of d-DQIVAR as a novel visual analytics system for DQI
	2. Combines data-driven and process-driven strategies for improved ML performance
	3. Case studies and user evaluations showcasing system effectiveness

**Result:** Demonstrates effective improvement in data quality, leading to enhancements in machine learning model performance through user studies and case evaluations.

**Limitations:** 

**Conclusion:** The d-DQIVAR system empowers users by allowing them to utilize their domain knowledge within a structured DQI workflow, significantly contributing to ML outcomes.

**Abstract:** Approaches to enhancing data quality (DQ) are classified into two main categories: data- and process-driven. However, prior research has predominantly utilized batch data preprocessing within the data-driven framework, which often proves insufficient for optimizing machine learning (ML) model performance and frequently leads to distortions in data characteristics. Existing studies have primarily focused on data preprocessing rather than genuine data quality improvement (DQI). In this paper, we introduce d-DQIVAR, a novel visual analytics system designed to facilitate DQI strategies aimed at improving ML model performance. Our system integrates visual analytics techniques that leverage both data-driven and process-driven approaches. Data-driven techniques tackle DQ issues such as imputation, outlier detection, deletion, format standardization, removal of duplicate records, and feature selection. Process-driven strategies encompass evaluating DQ and DQI procedures by considering DQ dimensions and ML model performance and applying the Kolmogorov-Smirnov test. We illustrate how our system empowers users to harness expert and domain knowledge effectively within a practical workflow through case studies, evaluations, and user studies.

</details>


### [10] [Dataset-Adaptive Dimensionality Reduction](https://arxiv.org/abs/2507.11984)

*Hyeon Jeon, Jeongin Park, Soohyun Lee, Dae Hyun Kim, Sungbok Shin, Jinwook Seo*

**Main category:** cs.HC

**Keywords:** dimensionality reduction, structural complexity metrics, dataset-adaptive optimization

**Relevance Score:** 5

**TL;DR:** The paper presents a dataset-adaptive approach to optimize dimensionality reduction (DR) techniques, minimizing trial and error through the use of structural complexity metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to optimize dimensionality reduction techniques efficiently due to the extensive trial and error currently involved, which leads to excessive computational overhead.

**Method:** The approach utilizes structural complexity metrics that quantify the intrinsic complexity of a dataset to determine the necessity of dimensionality, thus guiding the optimization process.

**Key Contributions:**

	1. Introduction of structural complexity metrics for dataset-adaptive DR optimization
	2. Quantitative verification of metrics' effectiveness in approximating dataset complexity
	3. Empirical evidence showing enhanced efficiency in DR optimization processes

**Result:** The proposed metrics were verified to effectively approximate the ground truth complexity of datasets, confirming their effectiveness in a dataset-adaptive DR workflow.

**Limitations:** 

**Conclusion:** The dataset-adaptive workflow significantly improves the efficiency of DR optimization while maintaining accuracy.

**Abstract:** Selecting the appropriate dimensionality reduction (DR) technique and determining its optimal hyperparameter settings that maximize the accuracy of the output projections typically involves extensive trial and error, often resulting in unnecessary computational overhead. To address this challenge, we propose a dataset-adaptive approach to DR optimization guided by structural complexity metrics. These metrics quantify the intrinsic complexity of a dataset, predicting whether higher-dimensional spaces are necessary to represent it accurately. Since complex datasets are often inaccurately represented in two-dimensional projections, leveraging these metrics enables us to predict the maximum achievable accuracy of DR techniques for a given dataset, eliminating redundant trials in optimizing DR. We introduce the design and theoretical foundations of these structural complexity metrics. We quantitatively verify that our metrics effectively approximate the ground truth complexity of datasets and confirm their suitability for guiding dataset-adaptive DR workflow. Finally, we empirically show that our dataset-adaptive workflow significantly enhances the efficiency of DR optimization without compromising accuracy.

</details>


### [11] [Envisage: Towards Expressive Visual Graph Querying](https://arxiv.org/abs/2507.11999)

*Xiaolin Wen, Qishuang Fu, Shuangyue Han, Yichen Guo, Joseph K. Liu, Yong Wang*

**Main category:** cs.HC

**Keywords:** Visual Graph Querying, graph queries, user interaction, query execution, usability

**Relevance Score:** 6

**TL;DR:** Envisage enhances Visual Graph Querying by allowing intuitive construction and execution of complex graph queries without coding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current VGQ tools are limited in allowing users to express complex queries interactively, requiring coding expertise.

**Method:** Envisage introduces four stages: Query Expression for intuitive graph construction, Query Verification for validation, Progressive Query Execution for meaningful results, and Result Analysis for exploration.

**Key Contributions:**

	1. Introduction of a four-stage interactive querying process
	2. Enhanced usability for non-technical users in graph querying
	3. Support for complex query scenarios with flexible parameterization

**Result:** User studies with 14 graph analysts showed that Envisage significantly improves the usability and effectiveness of graph querying.

**Limitations:** 

**Conclusion:** Envisage offers a more expressive and user-friendly approach to visual graph querying, facilitating the handling of complex queries.

**Abstract:** Graph querying is the process of retrieving information from graph data using specialized languages (e.g., Cypher), often requiring programming expertise. Visual Graph Querying (VGQ) streamlines this process by enabling users to construct and execute queries via an interactive interface without resorting to complex coding. However, current VGQ tools only allow users to construct simple and specific query graphs, limiting users' ability to interactively express their query intent, especially for underspecified query intent. To address these limitations, we propose Envisage, an interactive visual graph querying system to enhance the expressiveness of VGQ in complex query scenarios by supporting intuitive graph structure construction and flexible parameterized rule specification. Specifically, Envisage comprises four stages: Query Expression allows users to interactively construct graph queries through intuitive operations; Query Verification enables the validation of constructed queries via rule verification and query instantiation; Progressive Query Execution can progressively execute queries to ensure meaningful querying results; and Result Analysis facilitates result exploration and interpretation. To evaluate Envisage, we conducted two case studies and in-depth user interviews with 14 graph analysts. The results demonstrate its effectiveness and usability in constructing, verifying, and executing complex graph queries.

</details>


### [12] [Tao-Technology for Teen Mobile Use: Harmonizing Adaptation, Autonomy, and Reflection](https://arxiv.org/abs/2507.12204)

*Pengyu Zhu, Janghee Cho*

**Main category:** cs.HC

**Keywords:** Taoist philosophy, mobile technology, self-regulation, adolescents, adaptive governance

**Relevance Score:** 3

**TL;DR:** This paper proposes a self-organizing framework for regulating adolescents' mobile technology use, inspired by Taoist philosophy, which promotes autonomy and adaptive governance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Current regulatory mechanisms for adolescents' mobile technology use are too rigid and do not consider their autonomy or natural usage patterns.

**Method:** The paper introduces the concept of Tao-Technology, drawing on Taoist principles like Wu Wei and Yin-Yang, and integrates insights from Reflective Informatics and Information Ecologies.

**Key Contributions:**

	1. Introduction of Tao-Tech framework based on Taoist philosophy
	2. Integration of Reflective Informatics and Information Ecologies
	3. Focus on dynamic co-adaptive regulation rather than rigid control

**Result:** The proposed framework allows mobile technology to adjust dynamically to the user's context, enhancing self-reflection and meaning-making.

**Limitations:** 

**Conclusion:** By shifting from strict control to a flexible co-adaptive regulation, the framework supports adolescents in developing a balanced relationship with technology.

**Abstract:** Adolescents' mobile technology use is often regulated through rigid control mechanisms that fail to account for their autonomy and natural usage patterns. Drawing on Taoist philosophy, particularly Wu Wei, Yin-Yang, and Zi Ran, this position paper proposes Tao-Technology, a self-organizing, adaptive regulatory framework. Integrating insights from Reflective Informatics and Information Ecologies, we explore how mobile technology can dynamically adjust to context while fostering self-reflection and meaning-making. This approach shifts from external restrictions to dynamic co-adaptative regulation, ensuring technology governance remains flexible yet structured, supporting adolescents in cultivating a balanced and intentional relationship with digital technology.

</details>


### [13] [Draw an Ugly Person An Exploration of Generative AIs Perceptions of Ugliness](https://arxiv.org/abs/2507.12212)

*Garyoung Kim, Huisung Kwon, Seoju Yun, Yu-Won Youn*

**Main category:** cs.HC

**Keywords:** Generative AI, Cultural bias, Ugliness, Ethical AI, Inclusive development

**Relevance Score:** 7

**TL;DR:** This study analyzes how generative AI models understand and represent the concept of ugliness, revealing biases in their outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reproduction of cultural biases in generative AI models, particularly regarding the concept of ugliness.

**Method:** The study extracted 13 adjectives linked to ugliness via a large language model, generating 624 images across four AI models. These images were thematically coded and analyzed for biases.

**Key Contributions:**

	1. Analysis of biases in generative AI representations of ugliness
	2. Identification of how AI models reinforce societal stereotypes
	3. Recommendations for ethical AI training paradigms

**Result:** AI models disproportionately associate ugliness with old white male figures, revealing both entrenched social biases and paradoxical biases affecting various demographic groups.

**Limitations:** 

**Conclusion:** Despite attempts at equitable representation, generative AI perpetuates inherited biases, highlighting the need for ethical AI development frameworks.

**Abstract:** Generative AI does not only replicate human creativity but also reproduces deep-seated cultural biases, making it crucial to critically examine how concepts like ugliness are understood and expressed by these tools. This study investigates how four different generative AI models understand and express ugliness through text and image and explores the biases embedded within these representations. We extracted 13 adjectives associated with ugliness through iterative prompting of a large language model and generated 624 images across four AI models and three prompts. Demographic and socioeconomic attributes within the images were independently coded and thematically analyzed. Our findings show that AI models disproportionately associate ugliness with old white male figures, reflecting entrenched social biases as well as paradoxical biases, where efforts to avoid stereotypical depictions of marginalized groups inadvertently result in the disproportionate projection of negative attributes onto majority groups. Qualitative analysis further reveals that, despite supposed attempts to frame ugliness within social contexts, conventional physical markers such as asymmetry and aging persist as central visual motifs. These findings demonstrate that despite attempts to create more equal representations, generative AI continues to perpetuate inherited and paradoxical biases, underscoring the critical work being done to create ethical AI training paradigms and advance methodologies for more inclusive AI development.

</details>


### [14] [Humans are more gullible than LLMs in believing common psychological myths](https://arxiv.org/abs/2507.12296)

*Bevan Koopman, Guido Zuccon*

**Main category:** cs.HC

**Keywords:** Large Language Models, myth belief, retrieval-augmented generation, debiasing, Machine Psychology

**Relevance Score:** 8

**TL;DR:** This paper studies the behavior of Large Language Models (LLMs) in relation to belief in psychological myths and explores methods to reduce these tendencies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The persistent belief in psychological myths despite debunking prompts a need to investigate how LLMs relate to these myths and the potential for methods to mitigate myth belief in AI systems.

**Method:** The study evaluates myth belief across multiple LLMs using 50 psychological myths and various prompting strategies, including retrieval-augmented generation (RAG) and swaying prompts.

**Key Contributions:**

	1. Examination of LLM behavior concerning psychological myth belief.
	2. Demonstration of the effectiveness of retrieval-augmented generation in reducing myth belief.
	3. Introduction of cognitive science methods in the evaluation of LLMs.

**Result:** LLMs show lower rates of myth belief compared to humans, but user prompts can still significantly influence the models' responses. RAG is particularly effective at reducing myth belief.

**Limitations:** 

**Conclusion:** The findings highlight the potential for cognitive science methods to enhance the development and evaluation of LLM systems, suggesting a latent debiasing capability in LLMs.

**Abstract:** Despite widespread debunking, many psychological myths remain deeply entrenched. This paper investigates whether Large Language Models (LLMs) mimic human behaviour of myth belief and explores methods to mitigate such tendencies. Using 50 popular psychological myths, we evaluate myth belief across multiple LLMs under different prompting strategies, including retrieval-augmented generation and swaying prompts. Results show that LLMs exhibit significantly lower myth belief rates than humans, though user prompting can influence responses. RAG proves effective in reducing myth belief and reveals latent debiasing potential within LLMs. Our findings contribute to the emerging field of Machine Psychology and highlight how cognitive science methods can inform the evaluation and development of LLM-based systems.

</details>


### [15] [TrialCompass: Visual Analytics for Enhancing the Eligibility Criteria Design of Clinical Trials](https://arxiv.org/abs/2507.12298)

*Rui Sheng, Xingbo Wang, Jiachen Wang, Xiaofu Jin, Zhonghua Sheng, Zhenxing Xu, Suraj Rajendran, Huamin Qu, Fei Wang*

**Main category:** cs.HC

**Keywords:** visual analytics, clinical trials, eligibility criteria, electronic health records, sepsis

**Relevance Score:** 7

**TL;DR:** TrialCompass is a visual analytics system designed to enhance the exploration and refinement of eligibility criteria in clinical trials using EHR data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for designing eligibility criteria in clinical trials are limited and fail to integrate detailed EHR characteristics for optimal criteria refinement.

**Method:** TrialCompass employs a knowledge-driven and outcome-driven workflow that enables clinicians to interactively explore eligibility criteria, track changes, and analyze their impact on outcomes.

**Key Contributions:**

	1. Development of TrialCompass for eligibility criteria exploration
	2. Integration of knowledge-driven and outcome-driven approaches
	3. History-tracking feature for iterative refinement of criteria

**Result:** TrialCompass demonstrated effectiveness with real-world datasets, providing significant insights for tailoring eligibility criteria for conditions like septic shock and acute kidney injury.

**Limitations:** 

**Conclusion:** This research opens avenues for using visual analytics to improve the design and refinement of clinical trial eligibility criteria, enhancing overall trial quality and outcomes.

**Abstract:** Eligibility criteria play a critical role in clinical trials by determining the target patient population, which significantly influences the outcomes of medical interventions. However, current approaches for designing eligibility criteria have limitations to support interactive exploration of the large space of eligibility criteria. They also ignore incorporating detailed characteristics from the original electronic health record (EHR) data for criteria refinement. To address these limitations, we proposed TrialCompass, a visual analytics system integrating a novel workflow, which can empower clinicians to iteratively explore the vast space of eligibility criteria through knowledge-driven and outcome-driven approaches. TrialCompass supports history-tracking to help clinicians trace the evolution of their adjustments and decisions when exploring various forms of data (i.e., eligibility criteria, outcome metrics, and detailed characteristics of original EHR data) through these two approaches. This feature can help clinicians comprehend the impact of eligibility criteria on outcome metrics and patient characteristics, which facilitates systematic refinement of eligibility criteria. Using a real-world dataset, we demonstrated the effectiveness of TrialCompass in providing insights into designing eligibility criteria for septic shock and sepsis-associated acute kidney injury. We also discussed the research prospects of applying visual analytics to clinical trials.

</details>


### [16] [An Analysis of Text Functions in Information Visualization](https://arxiv.org/abs/2507.12334)

*Chase Stokes, Anjana Arunkumar, Marti A. Hearst, Lace Padilla*

**Main category:** cs.HC

**Keywords:** text functions, information visualization, design strategies, HCI, factor analysis

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework for understanding the role of text in information visualizations through an analysis of 120 visualizations and 804 text elements, identifying ten distinct text functions and four overarching design strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Text is an integral but understudied component of visualization design, influencing comprehension and preferences; this study aims to deepen understanding of its functional roles.

**Method:** The authors analyzed 120 real-world visualizations and 804 text elements, identifying distinct text functions and conducting factor analysis to reveal overarching design strategies.

**Key Contributions:**

	1. Introduced a new framework for text functions in visualizations.
	2. Identified ten distinct text functions.
	3. Revealed four overarching text-informed design strategies.

**Result:** Ten distinct text functions were identified, and four overarching strategies emerged: Attribution and Variables, Annotation-Centric Design, Visual Embellishments, and Narrative Framing.

**Limitations:** 

**Conclusion:** The findings emphasize the flexibility of text in visualizations and contribute nuanced insight into its diverse roles, enhancing existing frameworks.

**Abstract:** Text is an integral but understudied component of visualization design. Although recent studies have examined how text elements (e.g., titles and annotations) influence comprehension, preferences, and predictions, many questions remain about textual design and use in practice. This paper introduces a framework for understanding text functions in information visualizations, building on and filling gaps in prior classifications and taxonomies. Through an analysis of 120 real-world visualizations and 804 text elements, we identified ten distinct text functions, ranging from identifying data mappings to presenting valenced subtext. We further identify patterns in text usage and conduct a factor analysis, revealing four overarching text-informed design strategies: Attribution and Variables, Annotation-Centric Design, Visual Embellishments, and Narrative Framing. In addition to these factors, we explore features of title rhetoric and text multifunctionality, while also uncovering previously unexamined text functions, such as text replacing visual elements. Our findings highlight the flexibility of text, demonstrating how different text elements in a given design can combine to communicate, synthesize, and frame visual information. This framework adds important nuance and detail to existing frameworks that analyze the diverse roles of text in visualization.

</details>


### [17] [MExplore: an entity-based visual analytics approach for medical expertise acquisition](https://arxiv.org/abs/2507.12337)

*Xiao Pang, Yan Huang, Chang Liu, JiYuan Liu, MingYou Liu*

**Main category:** cs.HC

**Keywords:** medical expertise, visual analytics, medical entity extraction, unstructured texts, interactive exploration

**Relevance Score:** 8

**TL;DR:** MExplore is an interactive visual analytics system designed to enhance the acquisition of medical expertise from unstructured medical texts using a BERT-based model for entity extraction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing studies lack methods for extracting expertise from unstructured medical texts, which are crucial for medical education and professional development.

**Method:** Developed an interactive visual analytics system (MExplore) that integrates a fine-tuned BERT-based model to extract medical entities and a multilevel visual analysis framework for interactive exploration of medical knowledge.

**Key Contributions:**

	1. Introduction of MExplore system for medical expertise acquisition
	2. Development of a fine-tuned BERT model for medical entity extraction
	3. Novel multilevel visual analysis framework for interactive exploration

**Result:** The system significantly enhances the medical expertise acquisition process based on case studies, user study, and expert interviews.

**Limitations:** 

**Conclusion:** MExplore offers an effective interactive approach for acquiring and retaining knowledge from unstructured medical texts.

**Abstract:** Acquiring medical expertise is a critical component of medical education and professional development. While existing studies focus primarily on constructing medical knowledge bases or developing learning tools based on the structured, private healthcare data, they often lack methods for extracting expertise from unstructured medical texts. These texts constitute a significant portion of medical literature and offer greater flexibility and detail compared to structured data formats. Furthermore, many studies fail to provide explicit analytical and learning pathways in this context.   This paper introduces MExplore, an interactive visual analytics system designed to support the acquisition of medical expertise. To address the challenges of the inconsistencies and confidentiality concerns inherent in unstructured medical texts, we propose a workflow that employs a fine-tuned BERT-based model to extract medical entities (MEs) from them. We then present a novel multilevel visual analysis framework that integrates multiple coordinated visualizations, enabling a progressive and interactive exploration of medical knowledge.   To assess the effectiveness of MExplore, we conducted three case studies, a user study, and interviews with domain experts. The results indicate that the system significantly enhances the medical expertise acquisition process, providing an effective interactive approach for acquiring and retaining knowledge from medical texts.

</details>


### [18] [Deconstructing Implicit Beliefs in Visual Data Journalism: Unstable Meanings Behind Data as Truth & Design for Insight](https://arxiv.org/abs/2507.12377)

*Ke Er Amy Zhang, Jodie Jenkinson, Laura Garrison*

**Main category:** cs.HC

**Keywords:** visual data journalism, deconstruction, implicit beliefs, sociotechnical issues, data visualization

**Relevance Score:** 3

**TL;DR:** This paper deconstructs implicit beliefs in visual data journalism using qualitative interviews with journalists, revealing societal influences on notions of objectivity and subjectivity in data storytelling.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the instability of meaning in language and reveal implicit beliefs in visual data journalism through a deconstructive lens.

**Method:** Qualitative analysis of interviews with 17 visual data journalists, applying deconstruction theory and genealogical analysis.

**Key Contributions:**

	1. Application of deconstruction theory to data journalism
	2. Identification of implicit beliefs impacting visualization
	3. Historic contextualization of beliefs through genealogy

**Result:** Identified opposing implicit beliefs in visual data journalism and demonstrated how they are shaped by societal forces over time.

**Limitations:** The study is based on a limited sample of journalists and may not represent all perspectives in visual data journalism.

**Conclusion:** Critical theories can reframe success in visual data storytelling and enhance research outcomes related to sociotechnical issues in data visualization.

**Abstract:** We conduct a deconstructive reading of a qualitative interview study with 17 visual data journalists from newsrooms across the globe. We borrow a deconstruction approach from literary critique to explore the instability of meaning in language and reveal implicit beliefs in words and ideas. Through our analysis we surface two sets of opposing implicit beliefs in visual data journalism: objectivity/subjectivity and humanism/mechanism. We contextualize these beliefs through a genealogical analysis, which brings deconstruction theory into practice by providing a historic backdrop for these opposing perspectives. Our analysis shows that these beliefs held within visual data journalism are not self-enclosed but rather a product of external societal forces and paradigm shifts over time. Through this work, we demonstrate how thinking with critical theories such as deconstruction and genealogy can reframe "success" in visual data storytelling and diversify visualization research outcomes. These efforts push the ways in which we as researchers produce domain knowledge to examine the sociotechnical issues of today's values towards datafication and data visualization.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [19] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)

*Kazuyoshi Otsuka*

**Main category:** cs.CL

**Keywords:** large language models, literary assessment, evaluation patterns, RLHF, aesthetic preferences

**Relevance Score:** 8

**TL;DR:** The study examines large language models (LLMs) as literary critics by evaluating Japanese science fiction stories, revealing distinct evaluation patterns and individual characteristics influenced by RLHF.

**Read time:** 38 min

<details>
  <summary>Details</summary>

**Motivation:** Investigating how LLMs assess literature and their aesthetic preferences may provide insights into LLM behavior and biases, contributing to the understanding of AI's role in cultural contexts.

**Method:** Ten Japanese science fiction short stories were evaluated by six LLMs across seven sessions, utilizing principal component analysis and clustering techniques to analyze evaluation consistency and patterns.

**Key Contributions:**

	1. Positions LLMs as subjective literary critics
	2. Identifies distinct evaluation patterns in literary assessment
	3. Demonstrates influence of RLHF on LLM evaluation characteristics

**Result:** The analysis found five distinct evaluation patterns and significant variance in evaluation consistency among the models, as well as unique evaluation vocabularies for each LLM.

**Limitations:** Results are based on a limited set of science fiction stories, which may not generalize to other genres.

**Conclusion:** LLMs exhibit individual characteristics in literary assessment akin to human critics, challenging the notion that they are purely neutral evaluators.

**Abstract:** This study positions large language models (LLMs) as "subjective literary critics" to explore aesthetic preferences and evaluation patterns in literary assessment. Ten Japanese science fiction short stories were translated into English and evaluated by six state-of-the-art LLMs across seven independent sessions. Principal component analysis and clustering techniques revealed significant variations in evaluation consistency ({\alpha} ranging from 1.00 to 0.35) and five distinct evaluation patterns. Additionally, evaluation variance across stories differed by up to 4.5-fold, with TF-IDF analysis confirming distinctive evaluation vocabularies for each model. Our seven-session within-day protocol using an original Science Fiction corpus strategically minimizes external biases, allowing us to observe implicit value systems shaped by RLHF and their influence on literary judgment. These findings suggest that LLMs may possess individual evaluation characteristics similar to human critical schools, rather than functioning as neutral benchmarkers.

</details>


### [20] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)

*Varun Srivastava, Fan Lei, Srija Mukhopadhyay, Vivek Gupta, Ross Maciejewski*

**Main category:** cs.CL

**Keywords:** multimodal large language models, visual question answering, map visualization

**Relevance Score:** 5

**TL;DR:** This paper introduces MapIQ, a benchmark dataset for visual question answering with maps, focusing on evaluating multimodal large language models (MLLMs) on diverse map types and visual analytical tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in current Map-VQA research, which has largely focused on choropleth maps and a narrow range of thematic categories and visual analytical tasks.

**Method:** The study introduces MapIQ, comprising 14,706 question-answer pairs across choropleth maps, cartograms, and proportional symbol maps, and evaluates multiple MLLMs on six visual analytical tasks against human performance.

**Key Contributions:**

	1. Introduction of MapIQ, a comprehensive benchmark dataset for Map-VQA
	2. Evaluation of MLLM performance across multiple map types and tasks
	3. Insights into the impacts of map design on MLLM robustness

**Result:** MLLMs were evaluated on their performance across three map types and six tasks, with findings indicating varying robustness and sensitivity to map design changes and reliance on geographic knowledge.

**Limitations:** 

**Conclusion:** Insights from these evaluations can guide future improvements in Map-VQA and enhance MLLM performance on diverse visual data.

**Abstract:** Recent advancements in multimodal large language models (MLLMs) have driven researchers to explore how well these models read data visualizations, e.g., bar charts, scatter plots. More recently, attention has shifted to visual question answering with maps (Map-VQA). However, Map-VQA research has primarily focused on choropleth maps, which cover only a limited range of thematic categories and visual analytical tasks. To address these gaps, we introduce MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three map types: choropleth maps, cartograms, and proportional symbol maps spanning topics from six distinct themes (e.g., housing, crime). We evaluate multiple MLLMs using six visual analytical tasks, comparing their performance against one another and a human baseline. An additional experiment examining the impact of map design changes (e.g., altered color schemes, modified legend designs, and removal of map elements) provides insights into the robustness and sensitivity of MLLMs, their reliance on internal geographic knowledge, and potential avenues for improving Map-VQA performance.

</details>


### [21] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)

*Farideh Majidi, Ziaeddin Beheshtifard*

**Main category:** cs.CL

**Keywords:** cross-lingual, sentiment analysis, few-shot learning, incremental learning, Persian

**Relevance Score:** 5

**TL;DR:** This research explores cross-lingual sentiment analysis in Persian using few-shot and incremental learning with multilingual models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a sentiment analysis model for Persian that leverages knowledge from high-resource languages, addressing the challenge of limited data.

**Method:** Utilized three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, DistilBERT) fine-tuned with few-shot and incremental learning methods on small Persian datasets from various sources.

**Key Contributions:**

	1. Development of sentiment analysis model for Persian using few-shot learning
	2. Successful application of incremental learning techniques
	3. High performance achieved with multilingual models in a low-data scenario

**Result:** mDeBERTa and XLM-RoBERTa models achieved 96% accuracy in sentiment analysis on Persian data.

**Limitations:** 

**Conclusion:** The study demonstrates that few-shot and incremental learning combined with multilingual models can effectively improve sentiment analysis performance in low-resource languages.

**Abstract:** This research examines cross-lingual sentiment analysis using few-shot learning and incremental learning methods in Persian. The main objective is to develop a model capable of performing sentiment analysis in Persian using limited data, while getting prior knowledge from high-resource languages. To achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and DistilBERT) were employed, which were fine-tuned using few-shot and incremental learning approaches on small samples of Persian data from diverse sources, including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled the models to learn from a broad range of contexts. Experimental results show that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96% accuracy on Persian sentiment analysis. These findings highlight the effectiveness of combining few-shot learning and incremental learning with multilingual pre-trained models.

</details>


### [22] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)

*Guimin Hu, Yi Xin, Lijie Hu, Zhihong Zhu, Hasti Seifi*

**Main category:** cs.CL

**Keywords:** multimodal learning, uni-modal features, paired-modal features, machine learning, cross-modal interaction

**Relevance Score:** 7

**TL;DR:** The paper proposes a partitioner-guided modal learning framework, PgM, for multimodal learning that effectively segments and learns uni-modal and paired-modal features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve multimodal learning by creating a structured approach that leverages both uni-modal and paired-modal representations for better performance in various tasks.

**Method:** PgM is composed of a modal partitioner, uni-modal learner, paired-modal learner, and uni-paired modal decoder that together allow for effective learning of both types of features and adaptable representations for different downstream tasks.

**Key Contributions:**

	1. Introduction of the PgM framework for multimodal learning
	2. Effective segmentation of modal representations into uni-modal and paired-modal features
	3. Visualization insights into feature distribution across modalities and tasks.

**Result:** PgM demonstrates significant effectiveness in four multimodal tasks and shows robustness and transferability to existing models, improving our understanding of feature distribution across tasks.

**Limitations:** .

**Conclusion:** The developed framework, PgM, provides insights into the contributions of uni-modal and paired-modal features, and offers advantages such as flexible distribution adjustment and varied learning rates.

**Abstract:** Multimodal learning benefits from multiple modal information, and each learned modal representations can be divided into uni-modal that can be learned from uni-modal training and paired-modal features that can be learned from cross-modal interaction. Building on this perspective, we propose a partitioner-guided modal learning framework, PgM, which consists of the modal partitioner, uni-modal learner, paired-modal learner, and uni-paired modal decoder. Modal partitioner segments the learned modal representation into uni-modal and paired-modal features. Modal learner incorporates two dedicated components for uni-modal and paired-modal learning. Uni-paired modal decoder reconstructs modal representation based on uni-modal and paired-modal features. PgM offers three key benefits: 1) thorough learning of uni-modal and paired-modal features, 2) flexible distribution adjustment for uni-modal and paired-modal representations to suit diverse downstream tasks, and 3) different learning rates across modalities and partitions. Extensive experiments demonstrate the effectiveness of PgM across four multimodal tasks and further highlight its transferability to existing models. Additionally, we visualize the distribution of uni-modal and paired-modal features across modalities and tasks, offering insights into their respective contributions.

</details>


### [23] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)

*Maximiliano Hormazbal Lagos, lvaro Bueno Sez, Pedro Alonso Doval, Jorge Alcalde Vesteiro, Hctor Cerezo-Costas*

**Main category:** cs.CL

**Keywords:** ExpliCIT-QA, TableVQA, explainable AI, multimodal systems, healthcare applications

**Relevance Score:** 8

**TL;DR:** ExpliCIT-QA is a multimodal system for tabular question answering that enhances transparency and explainability by providing step-by-step reasoning and code generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the explainability gap in end-to-end TableVQA systems, particularly in sensitive domains like finance and healthcare where auditing results is crucial.

**Method:** The system employs a modular design including: 1) Multimodal Table Understanding with a Chain-of-Thought approach; 2) Language-based Reasoning for natural language explanations; 3) Automatic Code Generation for creating Python/Pandas scripts; 4) Code Execution for final answer computation; 5) Natural Language Explanation detailing how the answer was computed.

**Key Contributions:**

	1. Modular design for improved transparency and explainability in tabular question answering
	2. Step-by-step natural language reasoning and code generation
	3. Evaluation on TableVQA-Bench shows significant interpretability improvements

**Result:** ExpliCIT-QA demonstrated improvements in interpretability and transparency over existing benchmarks, evaluated on the TableVQA-Bench.

**Limitations:** 

**Conclusion:** The modular design and explainability enhancements of ExpliCIT-QA could significantly benefit applications in sensitive areas such as finance and healthcare.

**Abstract:** We present ExpliCIT-QA, a system that extends our previous MRT approach for tabular question answering into a multimodal pipeline capable of handling complex table images and providing explainable answers. ExpliCIT-QA follows a modular design, consisting of: (1) Multimodal Table Understanding, which uses a Chain-of-Thought approach to extract and transform content from table images; (2) Language-based Reasoning, where a step-by-step explanation in natural language is generated to solve the problem; (3) Automatic Code Generation, where Python/Pandas scripts are created based on the reasoning steps, with feedback for handling errors; (4) Code Execution to compute the final answer; and (5) Natural Language Explanation that describes how the answer was computed. The system is built for transparency and auditability: all intermediate outputs, parsed tables, reasoning steps, generated code, and final answers are available for inspection. This strategy works towards closing the explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on the TableVQA-Bench benchmark, comparing it with existing baselines. We demonstrated improvements in interpretability and transparency, which open the door for applications in sensitive domains like finance and healthcare where auditing results are critical.

</details>


### [24] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)

*Meng Li, Timothy M. McPhillips, Dingmin Wang, Shin-Rong Tsai, Bertram Ludscher*

**Main category:** cs.CL

**Keywords:** notebook understanding, Large Language Models, data flow analysis, syntactic parsing, information systems

**Relevance Score:** 8

**TL;DR:** This paper introduces a strategy for better understanding Jupyter notebooks using a combination of syntactic analysis and Large Language Models (LLMs) to resolve ambiguities in data flow and dependencies between cells.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the flow of information in data science and machine learning notebooks is crucial for their effective reuse and adaptation, yet existing LLMs struggle with ambiguities due to hallucinations and long-context issues.

**Method:** The proposed method, CRABS, uses shallow syntactic parsing and analysis of the abstract syntax tree (AST) to interpret notebooks, then employs an LLM to resolve remaining ambiguities in the data inputs and outputs of notebook cells, moving through a cell-by-cell zero-shot learning approach.

**Key Contributions:**

	1. Introduction of the Capture and Resolve Assisted Bounding Strategy (CRABS) for notebook comprehension
	2. Demonstrated effectiveness using a large dataset of annotated notebooks
	3. High accuracy in resolving ambiguities and identifying information flows and dependencies.

**Result:** CRABS achieved an F1 score of 98% in identifying cell-to-cell information flows and 99% in identifying transitive cell execution dependencies when evaluated on a dataset of 50 annotated Kaggle notebooks and was able to resolve 98% of ambiguities present in the notebooks.

**Limitations:** The performance may vary with different notebook formats or unobserved edge cases not present in the training data.

**Conclusion:** The combination of syntactic analysis and LLMs significantly enhances the understanding of information flows and dependencies in Jupyter notebooks, making a valuable contribution to the effective reuse of data science resources.

**Abstract:** Recognizing the information flows and operations comprising data science and machine learning Python notebooks is critical for evaluating, reusing, and adapting notebooks for new tasks. Investigating a notebook via re-execution often is impractical due to the challenges of resolving data and software dependencies. While Large Language Models (LLMs) pre-trained on large codebases have demonstrated effectiveness in understanding code without running it, we observe that they fail to understand some realistic notebooks due to hallucinations and long-context challenges. To address these issues, we propose a notebook understanding task yielding an information flow graph and corresponding cell execution dependency graph for a notebook, and demonstrate the effectiveness of a pincer strategy that uses limited syntactic analysis to assist full comprehension of the notebook using an LLM. Our Capture and Resolve Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and analysis of the abstract syntax tree (AST) to capture the correct interpretation of a notebook between lower and upper estimates of the inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via cell-by-cell zero-shot learning, thereby identifying the true data inputs and outputs of each cell. We evaluate and demonstrate the effectiveness of our approach using an annotated dataset of 50 representative, highly up-voted Kaggle notebooks that together represent 3454 actual cell inputs and outputs. The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves average F1 scores of 98% identifying cell-to-cell information flows and 99% identifying transitive cell execution dependencies.

</details>


### [25] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)

*Matteo Fasulo, Luca Babboni, Luca Tedeschini*

**Main category:** cs.CL

**Keywords:** subjectivity detection, sentiment analysis, multilingual classification, transformer models, class imbalance

**Relevance Score:** 7

**TL;DR:** This paper focuses on improving subjectivity detection in news articles using AI Wizards' approach at CLEF 2025, enhancing transformer classifiers with sentiment scores across multiple languages.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of classifying sentences as subjective or objective in news articles, particularly in multilingual settings where generalization to unseen languages is critical.

**Method:** The proposed method integrates sentiment scores from an auxiliary model with transformer-based classifiers like mDeBERTaV3-base and Llama3.2-1B, using decision threshold calibration to address class imbalance across several languages.

**Key Contributions:**

	1. Development of a sentiment-augmented architecture for subjectivity detection
	2. Demonstration of enhancements across multiple languages, including unseen ones
	3. Optimizing decision thresholds to alleviate class imbalance

**Result:** Results showed that integrating sentiment features significantly improved the performance of subjectivity detection, yielding a notable Macro F1 score of 0.51 for Greek language data.

**Limitations:** 

**Conclusion:** The sentiment-augmented classifier framework demonstrates a robust approach to enhancing subjectivity detection performance and achieving high rankings in multilingual settings.

**Abstract:** This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab Task 1: Subjectivity Detection in News Articles, classifying sentences as subjective/objective in monolingual, multilingual, and zero-shot settings. Training/development datasets were provided for Arabic, German, English, Italian, and Bulgarian; final evaluation included additional unseen languages (e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our primary strategy enhanced transformer-based classifiers by integrating sentiment scores, derived from an auxiliary model, with sentence representations, aiming to improve upon standard fine-tuning. We explored this sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base (English), and Llama3.2-1B. To address class imbalance, prevalent across languages, we employed decision threshold calibration optimized on the development set. Our experiments show sentiment feature integration significantly boosts performance, especially subjective F1 score. This framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


### [26] [Tracing Facts or just Copies? A critical investigation of the Competitions of Mechanisms in Large Language Models](https://arxiv.org/abs/2507.11809)

*Dante Campregher, Yanxu Chen, Sander Hoffman, Maria Heuss*

**Main category:** cs.CL

**Keywords:** Large Language Models, Attention Heads, Factual and Counterfactual Information

**Relevance Score:** 9

**TL;DR:** This study examines how Large Language Models (LLMs) handle conflicting information, focusing on attention heads and their effect on factual outputs.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the competition between factual and counterfactual information in LLMs and understand the role of attention heads in this process.

**Method:** The study reproduces findings from recent literature using Mechanistic Interpretability tools to analyze attention head behaviors and their impact on factual output ratios.

**Key Contributions:**

	1. Reconciliation of findings from multiple studies on LLMs' handling of factual and counterfactual information.
	2. Insights into the role of attention heads in factual output management.
	3. Demonstration of domain specificity in attention head behaviors.

**Result:** The findings indicate that attention heads facilitating factual output operate through general copy suppression rather than selective counterfactual suppression, and the behavior of attention heads is affected by the domain of the information.

**Limitations:** 

**Conclusion:** Attention head behavior is domain-specific, revealing that larger models may demonstrate more specialized patterns that are sensitive to different categories.

**Abstract:** This paper presents a reproducibility study examining how Large Language Models (LLMs) manage competing factual and counterfactual information, focusing on the role of attention heads in this process. We attempt to reproduce and reconcile findings from three recent studies by Ortu et al., Yu, Merullo, and Pavlick and McDougall et al. that investigate the competition between model-learned facts and contradictory context information through Mechanistic Interpretability tools. Our study specifically examines the relationship between attention head strength and factual output ratios, evaluates competing hypotheses about attention heads' suppression mechanisms, and investigates the domain specificity of these attention patterns. Our findings suggest that attention heads promoting factual output do so via general copy suppression rather than selective counterfactual suppression, as strengthening them can also inhibit correct facts. Additionally, we show that attention head behavior is domain-dependent, with larger models exhibiting more specialized and category-sensitive patterns.

</details>


### [27] [ILID: Native Script Language Identification for Indian Languages](https://arxiv.org/abs/2507.11832)

*Yash Ingle, Pruthwik Mishra*

**Main category:** cs.CL

**Keywords:** language identification, Indian languages, dataset, machine learning, deep learning

**Relevance Score:** 8

**TL;DR:** The paper presents a new dataset for language identification involving English and 22 Indian languages, along with baseline models using machine learning and deep learning techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Language identification is a crucial task in NLP and serves as a pre-processing step for many applications. The challenge is heightened in diverse Indian languages with phonetic similarities.

**Method:** The authors released a dataset comprising 230K sentences of English and 22 Indian languages, which includes newly created data. They developed robust baseline models using advanced machine learning and deep learning methods.

**Key Contributions:**

	1. Release of a large dataset (230K sentences) for language identification in Indian languages
	2. Development of robust baseline models for language identification using modern ML techniques
	3. Contribution to the NLP community by addressing challenges present in language identification for Indian languages

**Result:** The baseline models achieved performance comparable to existing state-of-the-art models in the language identification task.

**Limitations:** 

**Conclusion:** This work contributes a valuable resource and models for advancing research in language identification, particularly for the diverse Indian languages.

**Abstract:** The language identification task is a crucial fundamental step in NLP. Often it serves as a pre-processing step for widely used NLP applications such as multilingual machine translation, information retrieval, question and answering, and text summarization. The core challenge of language identification lies in distinguishing languages in noisy, short, and code-mixed environments. This becomes even harder in case of diverse Indian languages that exhibit lexical and phonetic similarities, but have distinct differences. Many Indian languages share the same script making the task even more challenging. In this paper, we release a dataset of 230K sentences consisting of English and all 22 official Indian languages labeled with their language identifiers where data in most languages are newly created. We also develop and release robust baseline models using state-of-the-art approaches in machine learning and deep learning that can aid the research in this field. Our baseline models are comparable to the state-of-the-art models for the language identification task.

</details>


### [28] [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)

*Mohammad Samragh, Arnav Kundu, David Harrison, Kumari Nishu, Devang Naik, Minsik Cho, Mehrdad Farajtabar*

**Main category:** cs.CL

**Keywords:** Autoregressive models, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper presents a novel framework that allows autoregressive language models to predict multiple future tokens simultaneously, significantly improving generation speed without compromising quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the slow inference speed and limited parallelism of autoregressive language models, especially during later stages of generation.

**Method:** The approach includes a masked-input formulation for joint prediction, a gated LoRA to preserve original functionality, a lightweight sampler for coherent sequence generation, auxiliary training losses for coherence and accuracy, and a speculative generation strategy for quadratic token expansion.

**Key Contributions:**

	1. Masked-input formulation for jointly predicting future tokens
	2. Gated LoRA formulation for multi-token prediction
	3. Lightweight learnable sampler for generating coherent sequences

**Result:** The proposed method enables code and math generation nearly 5x faster, and improves general chat and knowledge tasks by almost 2.5x without loss of quality.

**Limitations:** 

**Conclusion:** The framework enhances the performance of autoregressive language models by leveraging knowledge of future tokens for faster and coherent text generation.

**Abstract:** Autoregressive language models are constrained by their inherently sequential nature, generating one token at a time. This paradigm limits inference speed and parallelism, especially during later stages of generation when the direction and semantics of text are relatively certain. In this work, we propose a novel framework that leverages the inherent knowledge of vanilla autoregressive language models about future tokens, combining techniques to realize this potential and enable simultaneous prediction of multiple subsequent tokens. Our approach introduces several key innovations: (1) a masked-input formulation where multiple future tokens are jointly predicted from a common prefix; (2) a gated LoRA formulation that preserves the original LLM's functionality, while equipping it for multi-token prediction; (3) a lightweight, learnable sampler module that generates coherent sequences from the predicted future tokens; (4) a set of auxiliary training losses, including a consistency loss, to enhance the coherence and accuracy of jointly generated tokens; and (5) a speculative generation strategy that expands tokens quadratically in the future while maintaining high fidelity. Our method achieves significant speedups through supervised fine-tuning on pretrained models. For example, it generates code and math nearly 5x faster, and improves general chat and knowledge tasks by almost 2.5x. These gains come without any loss in quality.

</details>


### [29] [Cross-Domain Transfer and Few-Shot Learning for Personal Identifiable Information Recognition](https://arxiv.org/abs/2507.11862)

*Junhong Ye, Xu Yuan, Xinying Qiu*

**Main category:** cs.CL

**Keywords:** PII recognition, cross-domain, data fusion, few-shot learning, text anonymization

**Relevance Score:** 7

**TL;DR:** This paper explores the recognition of personally identifiable information (PII) through model transfer, data fusion, and efficient learning in various domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance automated text anonymization by improving the recognition of PII across different domains.

**Method:** The study employs annotated datasets from healthcare, legal, and biographical sources to evaluate model performance in in-domain and cross-domain settings, along with few-shot learning techniques.

**Key Contributions:**

	1. Analysis of cross-domain model transfer for PII recognition
	2. Insights into data fusion benefits across domains
	3. Demonstration of effective few-shot learning in PII recognition

**Result:** The findings indicate that legal-domain data transfers effectively to biographical contexts, while medical domain data shows resistance to transfer. Additionally, the advantages of data fusion vary by domain, and high-quality recognition is attainable with significantly reduced training data in low-specialization domains.

**Limitations:** The transfer effectiveness is domain-specific, with medical data being less adaptable to other domains.

**Conclusion:** Cross-domain model transfer and data fusion can improve PII recognition, particularly in legal and biographical contexts, even with limited training data.

**Abstract:** Accurate recognition of personally identifiable information (PII) is central to automated text anonymization. This paper investigates the effectiveness of cross-domain model transfer, multi-domain data fusion, and sample-efficient learning for PII recognition. Using annotated corpora from healthcare (I2B2), legal (TAB), and biography (Wikipedia), we evaluate models across four dimensions: in-domain performance, cross-domain transferability, fusion, and few-shot learning. Results show legal-domain data transfers well to biographical texts, while medical domains resist incoming transfer. Fusion benefits are domain-specific, and high-quality recognition is achievable with only 10% of training data in low-specialization domains.

</details>


### [30] [COLA-GEC: A Bidirectional Framework for Enhancing Grammatical Acceptability and Error Correction](https://arxiv.org/abs/2507.11867)

*Xiangyu Yang, Xinying Qiu*

**Main category:** cs.CL

**Keywords:** Grammatical Error Correction, grammatical acceptability judgment, natural language processing, COLA-GEC, mutual knowledge transfer

**Relevance Score:** 7

**TL;DR:** A novel bidirectional framework, COLA-GEC, enhances both Grammatical Error Correction and grammatical acceptability judgment by mutual knowledge transfer, improving performance across languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the independent evolution of GEC and COLA tasks in natural language processing by integrating their foundational grammatical knowledge.

**Method:** Introducing COLA-GEC, we augment grammatical acceptability models with GEC datasets and incorporate grammatical acceptability signals into GEC training with a dynamic loss function.

**Key Contributions:**

	1. Introduction of COLA-GEC framework for mutual knowledge transfer
	2. Significant performance improvement in grammatical acceptability models
	3. State-of-the-art results on multilingual benchmarks

**Result:** Our approach achieves state-of-the-art results on multiple multilingual benchmarks.

**Limitations:** Challenges remain in punctuation error correction.

**Conclusion:** The results indicate the effectiveness of the mutual knowledge transfer approach, though challenges in punctuation error correction remain.

**Abstract:** Grammatical Error Correction (GEC) and grammatical acceptability judgment (COLA) are core tasks in natural language processing, sharing foundational grammatical knowledge yet typically evolving independently. This paper introduces COLA-GEC, a novel bidirectional framework that enhances both tasks through mutual knowledge transfer. First, we augment grammatical acceptability models using GEC datasets, significantly improving their performance across multiple languages. Second, we integrate grammatical acceptability signals into GEC model training via a dynamic loss function, effectively guiding corrections toward grammatically acceptable outputs. Our approach achieves state-of-the-art results on several multilingual benchmarks. Comprehensive error analysis highlights remaining challenges, particularly in punctuation error correction, providing insights for future improvements in grammatical modeling.

</details>


### [31] [DualReward: A Dynamic Reinforcement Learning Framework for Cloze Tests Distractor Generation](https://arxiv.org/abs/2507.11875)

*Tianyou Huang, Xinglu Chen, Jingshen Zhang, Xinying Qiu, Ruiying Niu*

**Main category:** cs.CL

**Keywords:** Reinforcement learning, Distractor generation, Cloze tests, Adaptive scaling

**Relevance Score:** 4

**TL;DR:** Introduction of DualReward, a reinforcement learning framework for automatic distractor generation in cloze tests which outperforms traditional methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve automatic distractor generation in cloze tests by utilizing a reinforcement learning approach rather than conventional supervised learning.

**Method:** Employs a dual reward structure with adaptive scaling to differentiate between human-created gold standard distractors and model-generated candidates.

**Key Contributions:**

	1. Introduction of DualReward framework
	2. Adaptive reward scaling mechanism
	3. Improved performance on diverse datasets

**Result:** Demonstrated consistent improvements over state-of-the-art baselines on both passage-level and sentence-level cloze test datasets.

**Limitations:** 

**Conclusion:** Offers a flexible framework that balances learning from reliable human examples while exploring high-quality distractors for automated test generation.

**Abstract:** This paper introduces DualReward, a novel reinforcement learning framework for automatic distractor generation in cloze tests. Unlike conventional approaches that rely primarily on supervised learning or static generative models, our method employs a dual reward structure with adaptive scaling that differentiates between human-created gold standard distractors and model-generated candidates. The framework dynamically adjusts reward signal intensity based on model performance and confidence. We evaluate our approach on both passage-level (CLOTH-F) and sentence-level (MCQ) cloze test datasets, demonstrating consistent improvements over state-of-the-art baselines. Experimental results show that our adaptive reward scaling mechanism provides modest but consistent benefits on homogeneous datasets (CLOTH-F) and more substantial improvements (3.48-3.86% in P@1) on diverse, cross-domain data (MCQ), suggesting its particular effectiveness for handling varied question types and domains. Our work offers a flexible framework that effectively balances learning from reliable human examples while exploring novel, high-quality distractors for automated test generation.

</details>


### [32] [LLMs Encode Harmfulness and Refusal Separately](https://arxiv.org/abs/2507.11878)

*Jiachen Zhao, Jing Huang, Zhengxuan Wu, David Bau, Weiyan Shi*

**Main category:** cs.CL

**Keywords:** Harmfulness, Refusal, LLMs, AI safety, Latent Guard

**Relevance Score:** 9

**TL;DR:** This paper explores a new dimension of harmfulness in LLMs, offering insights on their internal understanding of harmfulness versus refusal, and proposes a safety application termed Latent Guard.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs truly understand harmfulness beyond merely refusing harmful instructions, and to identify a robust mechanism for AI safety.

**Method:** The researchers analyze the harmfulness and refusal directions in LLMs, employing causal evidence to demonstrate that these two concepts operate independently within the models.

**Key Contributions:**

	1. Identification of a harmfulness direction distinct from refusal direction in LLMs
	2. Development of Latent Guard as a safeguard for detecting unsafe inputs
	3. Empirical evidence showing LLMs have a more robust understanding of harmfulness than refusal

**Result:** The study finds that steering LLMs toward the harmfulness direction can incorrectly label harmless instructions as harmful, while steering toward refusal triggers direct refusal responses. Latent Guard was shown to perform effectively against various jailbreaking methods.

**Limitations:** 

**Conclusion:** LLMs possess a distinct internal representation of harmfulness that can be harnessed for safety applications, providing a more reliable assessment of harmful inputs compared to their refusal behavior.

**Abstract:** LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction. In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. There exists a harmfulness direction that is distinct from the refusal direction. As causal evidence, steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness. Furthermore, using our identified harmfulness concept, we find that certain jailbreak methods work by reducing the refusal signals without reversing the model's internal belief of harmfulness. We also find that adversarially finetuning models to accept harmful instructions has minimal impact on the model's internal belief of harmfulness. These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. For instance, our Latent Guard achieves performance comparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard model, across different jailbreak methods. Our findings suggest that LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety

</details>


### [33] [Marco-Bench-MIF: On Multilingual Instruction-Following Capability of Large Language Models](https://arxiv.org/abs/2507.11882)

*Bo Zeng, Chenyang Lyu, Sinuo Liu, Mingyan Zeng, Minghao Wu, Xuanfan Ni, Tianqi Shi, Yu Zhao, Yefeng Liu, Chenyu Zhu, Ruizhe Li, Jiahui Geng, Qing Li, Yu Tong, Longyue Wang, Weihua Luo, Kaifu Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, multilingual evaluation, instruction-following, localization, benchmark

**Relevance Score:** 9

**TL;DR:** A multilingual extension of the IFEval benchmark for evaluating instruction-following capabilities in Large Language Models (LLMs), addressing linguistic and cultural challenges across 30 languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of Large Language Models in multilingual contexts where existing datasets are limited and often not adequately localized.

**Method:** A hybrid pipeline combining translation with verification was used to create the Marco-Bench-MIF, which was then evaluated on over 20 LLMs, assessing their performance across high and low-resource languages.

**Key Contributions:**

	1. Introduction of Marco-Bench-MIF, a multilingual benchmark
	2. Identification of performance gaps between high and low-resource languages
	3. Insight into the impact of localization on LLM accuracy

**Result:** Findings show a 25-35% accuracy gap between high and low-resource languages, model scales affect performance by 45-60%, and localized data highlights a 7-22% underestimation of machine-translated data accuracy.

**Limitations:** The benchmark may still not fully capture all linguistic and cultural nuances across all 30 languages.

**Conclusion:** The study identifies significant challenges in multilingual instruction following, including keyword consistency and compositional constraints, while providing a new benchmark for future research.

**Abstract:** Instruction-following capability has become a major ability to be evaluated for Large Language Models (LLMs). However, existing datasets, such as IFEval, are either predominantly monolingual and centered on English or simply machine translated to other languages, limiting their applicability in multilingual contexts. In this paper, we present an carefully-curated extension of IFEval to a localized multilingual version named Marco-Bench-MIF, covering 30 languages with varying levels of localization. Our benchmark addresses linguistic constraints (e.g., modifying capitalization requirements for Chinese) and cultural references (e.g., substituting region-specific company names in prompts) via a hybrid pipeline combining translation with verification. Through comprehensive evaluation of 20+ LLMs on our Marco-Bench-MIF, we found that: (1) 25-35% accuracy gap between high/low-resource languages, (2) model scales largely impact performance by 45-60% yet persists script-specific challenges, and (3) machine-translated data underestimates accuracy by7-22% versus localized data. Our analysis identifies challenges in multilingual instruction following, including keyword consistency preservation and compositional constraint adherence across languages. Our Marco-Bench-MIF is available at https://github.com/AIDC-AI/Marco-Bench-MIF.

</details>


### [34] [A Survey of Deep Learning for Geometry Problem Solving](https://arxiv.org/abs/2507.11936)

*Jianzhe Ma, Wenxuan Wang, Qin Jin*

**Main category:** cs.CL

**Keywords:** deep learning, geometry problem solving, multimodal models, evaluation metrics, future directions

**Relevance Score:** 4

**TL;DR:** This paper surveys the applications of deep learning in geometry problem solving, covering relevant tasks, methods, evaluation metrics, and future directions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive and practical reference for deep learning applications in geometry problem solving, promoting further developments in the field.

**Method:** The paper reviews various tasks in geometry problem solving, summarizes deep learning methods used, analyzes evaluation metrics, and discusses current challenges and future research directions.

**Key Contributions:**

	1. Comprehensive summary of tasks in geometry problem solving.
	2. Thorough review of deep learning methods applied to geometry.
	3. Analysis of evaluation metrics for geometry problem solving.

**Result:** The survey identifies key tasks in geometry problem solving and reviews numerous deep learning approaches while highlighting evaluation metrics and challenges in the field.

**Limitations:** The work is in progress and may not cover the latest developments or all aspects of the topic comprehensively.

**Conclusion:** This work aims to serve as a foundational reference for researchers and practitioners interested in applying deep learning to geometry problems and encourages continued exploration.

**Abstract:** Geometry problem solving is a key area of mathematical reasoning, which is widely involved in many important fields such as education, mathematical ability assessment of artificial intelligence, and multimodal ability assessment. In recent years, the rapid development of deep learning technology, especially the rise of multimodal large language models, has triggered a widespread research boom. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our goal is to provide a comprehensive and practical reference of deep learning for geometry problem solving to promote further developments in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.

</details>


### [35] [POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering](https://arxiv.org/abs/2507.11939)

*Yichen Xu, Liangyu Chen, Liang Zhang, Wenxuan Wang, Qin Jin*

**Main category:** cs.CL

**Keywords:** multilingual, chart understanding, question answering, LLM, vision-language models

**Relevance Score:** 8

**TL;DR:** PolyChartQA is a multilingual chart question answering benchmark containing 22,606 charts and 26,151 Q&A pairs in 10 languages, aiming to improve accessibility in chart understanding across diverse languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the English-centric nature of existing chart understanding benchmarks which limits accessibility and applicability for global audiences.

**Method:** A decoupled pipeline separates chart data from rendering code to generate multilingual charts by translating data and reusing code. State-of-the-art LLM-based translation is utilized with strict quality control for linguistic and semantic consistency.

**Key Contributions:**

	1. Introduction of the first multilingual chart question answering benchmark (PolyChartQA)
	2. A robust decoupled pipeline for generating multilingual charts
	3. Insights on performance disparities in chart understanding across different languages

**Result:** Experiments reveal a significant performance gap in chart understanding between English and other languages, particularly low-resource languages with non-Latin scripts.

**Limitations:** As a work in progress, further improvements and validations may be needed in the benchmark design and data quality.

**Conclusion:** PolyChartQA establishes a foundation for enhancing inclusive vision-language models and systematic evaluation of multilingual chart understanding.

**Abstract:** Charts are a universally adopted medium for interpreting and communicating data. However, existing chart understanding benchmarks are predominantly English-centric, limiting their accessibility and applicability to global audiences. In this paper, we present PolyChartQA, the first large-scale multilingual chart question answering benchmark covering 22,606 charts and 26,151 question-answering pairs across 10 diverse languages. PolyChartQA is built using a decoupled pipeline that separates chart data from rendering code, allowing multilingual charts to be flexibly generated by simply translating the data and reusing the code. We leverage state-of-the-art LLM-based translation and enforce rigorous quality control in the pipeline to ensure the linguistic and semantic consistency of the generated multilingual charts. PolyChartQA facilitates systematic evaluation of multilingual chart understanding. Experiments on both open- and closed-source large vision-language models reveal a significant performance gap between English and other languages, especially low-resource ones with non-Latin scripts. This benchmark lays a foundation for advancing globally inclusive vision-language models.

</details>


### [36] [BlockBPE: Parallel BPE Tokenization](https://arxiv.org/abs/2507.11941)

*Amos You*

**Main category:** cs.CL

**Keywords:** tokenization, byte-pair encoding, GPU, batch inference, large language models

**Relevance Score:** 9

**TL;DR:** BlockBPE is a GPU-optimized parallel implementation of byte-pair encoding that enhances throughput in large language model tokenization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of tokenization in large language models, specifically addressing the optimization for batch inference on GPU.

**Method:** BlockBPE replaces regex pre-tokenization with a highly parallelized approach for token merging, achieving linear-time complexity under realistic conditions.

**Key Contributions:**

	1. Introduction of BlockBPE as a parallel GPU implementation of BPE
	2. Elimination of regex pre-tokenization for improved performance
	3. Demonstrated significant throughput improvements over existing tokenizers

**Result:** BlockBPE shows up to 2x higher throughput compared to tiktoken and 2.5x compared to HuggingFace Tokenizers in batch inference workloads.

**Limitations:** 

**Conclusion:** The proposed BlockBPE method significantly enhances performance for tokenization in large language model pipelines, enabling more efficient use of GPU resources.

**Abstract:** Tokenization is a critical preprocessing step in large language model pipelines, yet widely-used implementations remain CPU-bound and suboptimal for batch inference workflows on GPU. We present BlockBPE, a parallel GPU implementation of byte-pair encoding (BPE) that achieves near linear-time complexity under realistic assumptions and is optimized for high-throughput, batch inference. Unlike existing Rust-based tokenizers such as HuggingFace Tokenizers or OpenAI's tiktoken-whose runtimes are dominated by Regex pre-tokenization and exhibit $O(n \log n)$ runtime-BlockBPE eliminates the Regex pre-tokenization which leads to small loss in generation quality, but enables highly parallelized token merges within thread blocks, reducing overall complexity to $O(nd)$ where $d \ll n$. On high-batch inference workloads, BlockBPE achieves up to 2x higher throughput than tiktoken and 2.5x over HuggingFace Tokenizers.

</details>


### [37] [DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression](https://arxiv.org/abs/2507.11942)

*Yi Zhao, Zuchao Li, Hai Zhao, Baoyuan Qi, Guoming Liu*

**Main category:** cs.CL

**Keywords:** prompt compression, dynamic attention, information entropy, natural language processing, large language models

**Relevance Score:** 8

**TL;DR:** DAC is a dynamic attention-aware method for task-agnostic prompt compression that improves information density while reducing computational overhead.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing prompt compression methods lack consideration for attention-critical tokens and changes in information entropy during compression.

**Method:** DAC integrates both entropy and attention information, dynamically adjusting for entropy shifts during the compression process.

**Key Contributions:**

	1. Proposes a dynamic attention-aware approach for prompt compression.
	2. Integrates entropy and attention information for better compression.
	3. Demonstrates significant improvements across multiple tasks and datasets.

**Result:** DAC demonstrates substantial improvements across various domains such as LongBench, GSM8K, and BBH, outperforming prior methods.

**Limitations:** 

**Conclusion:** The proposed DAC method effectively enhances prompt compression in task-agnostic settings, showing robust performance across diverse tasks and LLMs.

**Abstract:** Task-agnostic prompt compression leverages the redundancy in natural language to reduce computational overhead and enhance information density within prompts, especially in long-context scenarios. Existing methods predominantly rely on information entropy as the metric to compress lexical units, aiming to achieve minimal information loss. However, these approaches overlook two critical aspects: (i) the importance of attention-critical tokens at the algorithmic level, and (ii) shifts in information entropy during the compression process. Motivated by these challenges, we propose a dynamic attention-aware approach for task-agnostic prompt compression (DAC). This approach effectively integrates entropy and attention information, dynamically sensing entropy shifts during compression to achieve fine-grained prompt compression. Extensive experiments across various domains, including LongBench, GSM8K, and BBH, show that DAC consistently yields robust and substantial improvements across a diverse range of tasks and LLMs, offering compelling evidence of its efficacy.

</details>


### [38] [IAM: Efficient Inference through Attention Mapping between Different-scale LLMs](https://arxiv.org/abs/2507.11953)

*Yi Zhao, Zuchao Li, Hai Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Attention Mapping, Resource Optimization

**Relevance Score:** 8

**TL;DR:** The paper introduces the IAM framework to optimize large language models (LLMs) by mapping attention between different-scale models, leading to improved efficiency without significant performance loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the inefficiencies in resource consumption of LLMs, particularly with long contexts, and to utilize the similarity in attention matrices across models for optimization.

**Method:** The authors analyze the similarity of attention matrices across different-scale LLMs, select mapping layers, and ensure consistency in mapping. They introduce the IAM framework that enhances attention computation and reduces KV cache usage.

**Key Contributions:**

	1. Introduction of the IAM framework for attention mapping between LLMs.
	2. Demonstrated dual benefits of accelerated attention computation and reduced KV cache usage.
	3. Analysis of attention matrix similarities across different-scale LLMs.

**Result:** IAM achieves a 15% acceleration in prefill and a 22.1% reduction in KV cache usage while maintaining performance across various model series.

**Limitations:** 

**Conclusion:** The IAM framework is a versatile tool to improve LLM efficiency, complementing existing cache optimization methods.

**Abstract:** LLMs encounter significant challenges in resource consumption nowadays, especially with long contexts. Despite extensive efforts dedicate to enhancing inference efficiency, these methods primarily exploit internal sparsity within the models, without leveraging external information for optimization. We identify the high similarity of attention matrices across different-scale LLMs, which offers a novel perspective for optimization. We first conduct a comprehensive analysis of how to measure similarity, how to select mapping Layers and whether mapping is consistency. Based on these insights, we introduce the IAM framework, which achieves dual benefits of accelerated attention computation and reduced KV cache usage by performing attention mapping between small and large LLMs. Our experimental results demonstrate that IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without appreciably sacrificing performance. Experiments on different series of models show the generalizability of IAM. Importantly, it is also orthogonal to many existing KV cache optimization methods, making it a versatile addition to the current toolkit for enhancing LLM efficiency.

</details>


### [39] [The benefits of query-based KGQA systems for complex and temporal questions in LLM era](https://arxiv.org/abs/2507.11954)

*Artem Alekseev, Mikhail Chaichuk, Miron Butko, Alexander Panchenko, Elena Tutubalina, Oleg Somov*

**Main category:** cs.CL

**Keywords:** Knowledge Graph, Question Answering, Multi-hop Reasoning, Temporal Questions, Large Language Models

**Relevance Score:** 7

**TL;DR:** This paper presents a multi-stage query-based framework for knowledge graph question-answering using WikiData, focusing on improving performance for multi-hop and temporal questions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models in multi-hop reasoning and temporal questions, a modular approach using query-based KGQA is explored.

**Method:** The authors propose a multi-stage approach that generates executable queries to enhance QA performance on challenging datasets, evaluated through generalization and rejection studies.

**Key Contributions:**

	1. Introduction of a modular query-based KGQA framework for multi-hop and temporal reasoning.
	2. Development of a novel entity linking and predicate matching method using CoT reasoning.
	3. Demonstration of the frameworks efficacy with small language models on challenging QA benchmarks.

**Result:** The proposed framework shows improved robustness and performance on multi-hop and temporal QA benchmarks, even with small language models.

**Limitations:** The framework's effectiveness may vary depending on the specific characteristics of the datasets used.

**Conclusion:** The study highlights the effectiveness of a query-based multi-stage KGQA framework in enhancing question-answering capabilities for complex inquiries.

**Abstract:** Large language models excel in question-answering (QA) yet still struggle with multi-hop reasoning and temporal questions. Query-based knowledge graph QA (KGQA) offers a modular alternative by generating executable queries instead of direct answers. We explore multi-stage query-based framework for WikiData QA, proposing multi-stage approach that enhances performance on challenging multi-hop and temporal benchmarks. Through generalization and rejection studies, we evaluate robustness across multi-hop and temporal QA datasets. Additionally, we introduce a novel entity linking and predicate matching method using CoT reasoning. Our results demonstrate the potential of query-based multi-stage KGQA framework for improving multi-hop and temporal QA with small language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

</details>


### [40] [PoTPTQ: A Two-step Power-of-Two Post-training for LLMs](https://arxiv.org/abs/2507.11959)

*Xinyu Wang, Vahid Partovi Nia, Peng Lu, Jerry Huang, Xiao-Wen Chang, Boxing Chen, Yufei Cui*

**Main category:** cs.CL

**Keywords:** Power-of-Two quantization, LLM, dequantization, low precision, post-training algorithm

**Relevance Score:** 9

**TL;DR:** This paper proposes an advanced Power-of-Two (PoT) quantization framework for Large Language Model (LLM) weights, improving efficiency and accuracy for low-precision formats while enhancing GPU performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational challenges of deploying large language models due to their resource demands, especially concerning quantization techniques.

**Method:** The authors introduce a novel PoT quantization framework, which includes a two-step post-training algorithm for better accuracy and faster inference using efficient dequantization.

**Key Contributions:**

	1. Introduction of a novel PoT quantization framework for LLM weights.
	2. A two-step post-training method that enhances accuracy and speed.
	3. Demonstrated performance improvements at extremely low precisions.

**Result:** The proposed PoT method improves on state-of-the-art accuracy and achieves significant speedups in dequantization, specifically $3.67\times$ on NVIDIA V100 and $1.63\times$ on NVIDIA RTX 4090.

**Limitations:** 

**Conclusion:** The PoT quantization framework provides an effective solution for low-precision integer quantization of LLMs, enabling faster and more accurate model deployment.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing (NLP) tasks. However, their deployment is challenging due to the substantial computational resources required. Power-of-two (PoT) quantization is a general tool to counteract this difficulty. Albeit previous works on PoT quantization can be efficiently dequantized on CPUs using fixed-point addition, it showed less effectiveness on GPUs. The reason is entanglement of the sign bit and sequential bit manipulations needed for dequantization. We propose a novel POT quantization framework for LLM weights that (i) outperforms state-of-the-art accuracy in extremely low-precision number formats, and (ii) enables faster inference through more efficient dequantization. To maintain the accuracy of the quantized model, we introduce a two-step post-training algorithm: (i) initialize the quantization scales with a robust starting point, and (ii) refine these scales using a minimal calibration set. The performance of our PoT post-training algorithm surpasses the current state-of-the-art in integer quantization, particularly at low precisions such as 2- and 3-bit formats. Our PoT quantization accelerates the dequantization step required for the floating point inference and leads to $3.67\times$ speed up on a NVIDIA V100, and $1.63\times$ on a NVIDIA RTX 4090, compared to uniform integer dequantization.

</details>


### [41] [Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation](https://arxiv.org/abs/2507.11966)

*Ziyu Ge, Gabriel Chua, Leanne Tan, Roy Ka-Wei Lee*

**Main category:** cs.CL

**Keywords:** Toxicity Preservation, Translation Systems, Culturally Sensitive Moderation

**Relevance Score:** 8

**TL;DR:** This paper presents a framework for toxicity-preserving translation in low-resource language pairs, specifically focusing on code-mixed Singlish.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the failure of standard translation systems to preserve local slang and culturally specific markers in under-represented languages.

**Method:** A two-stage framework involving few-shot prompt engineering and model-prompt optimization using various large language models evaluated through semantic similarity and human verification.

**Key Contributions:**

	1. Development of a toxicity-preserving translation framework
	2. Demonstration of few-shot prompt engineering with Singlish
	3. Benchmarking with large language models for enhanced translation quality

**Result:** A demonstrably effective pipeline that enhances translation quality while addressing safety concerns in multicultural large language models.

**Limitations:** The approach is specific to low-resource language pairs and may require further adaptation for broader language use cases.

**Conclusion:** The framework not only improves translation for low-resource contexts but also supports culturally sensitive moderation and highlights the importance of sociolinguistic nuances.

**Abstract:** As online communication increasingly incorporates under-represented languages and colloquial dialects, standard translation systems often fail to preserve local slang, code-mixing, and culturally embedded markers of harmful speech. Translating toxic content between low-resource language pairs poses additional challenges due to scarce parallel data and safety filters that sanitize offensive expressions. In this work, we propose a reproducible, two-stage framework for toxicity-preserving translation, demonstrated on a code-mixed Singlish safety corpus. First, we perform human-verified few-shot prompt engineering: we iteratively curate and rank annotator-selected Singlish-target examples to capture nuanced slang, tone, and toxicity. Second, we optimize model-prompt pairs by benchmarking several large language models using semantic similarity via direct and back-translation. Quantitative human evaluation confirms the effectiveness and efficiency of our pipeline. Beyond improving translation quality, our framework contributes to the safety of multicultural LLMs by supporting culturally sensitive moderation and benchmarking in low-resource contexts. By positioning Singlish as a testbed for inclusive NLP, we underscore the importance of preserving sociolinguistic nuance in real-world applications such as content moderation and regional platform governance.

</details>


### [42] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)

*Liu He, Yuanchao Li, Rui Feng, XinRan Han, Yin-Long Liu, Yuwei Yang, Zude Zhu, Jiahong Yuan*

**Main category:** cs.CL

**Keywords:** Alzheimer's Disease, speech perception, gender bias, acoustic analysis, speech identification

**Relevance Score:** 7

**TL;DR:** This study investigates gender bias in the perception of Alzheimer's Disease speech, finding males' speech is more often identified as AD, particularly in Chinese speech.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the influence of gender bias in speech perception tasks, particularly regarding Alzheimer's Disease (AD).

**Method:** Conducted a perception experiment with 16 Chinese listeners evaluating both Chinese and Greek speech samples for signs of AD.

**Key Contributions:**

	1. Identification of gender bias in the perception of Alzheimer's Disease speech.
	2. Demonstration that shimmer values correlate with AD perception, particularly in male speech.
	3. Call for inclusion of gender factors in AD detection model development.

**Result:** Male speech was more frequently identified as AD, with significant associations found between shimmer values in male speech and AD perception.

**Limitations:** Limited to 16 Chinese listeners and does not account for broader linguistic variations.

**Conclusion:** The findings highlight the importance of addressing gender bias in AD detection models and suggest further research is needed across different languages.

**Abstract:** Gender bias has been widely observed in speech perception tasks, influenced by the fundamental voicing differences between genders. This study reveals a gender bias in the perception of Alzheimer's Disease (AD) speech. In a perception experiment involving 16 Chinese listeners evaluating both Chinese and Greek speech, we identified that male speech was more frequently identified as AD, with this bias being particularly pronounced in Chinese speech. Acoustic analysis showed that shimmer values in male speech were significantly associated with AD perception, while speech portion exhibited a significant negative correlation with AD identification. Although language did not have a significant impact on AD perception, our findings underscore the critical role of gender bias in AD speech perception. This work highlights the necessity of addressing gender bias when developing AD detection models and calls for further research to validate model performance across different linguistic contexts.

</details>


### [43] [Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker](https://arxiv.org/abs/2507.11972)

*Yuhong Zhang, Jialu Li, Shilai Yang, Yuchen Xu, Gert Cauwenberghs, Tzyy-Ping Jung*

**Main category:** cs.CL

**Keywords:** reading comprehension, Large Language Models, graph-based representation, eye-tracking, human-AI co-learning

**Relevance Score:** 9

**TL;DR:** This study explores how humans and LLMs understand reading comprehension through graph-based representations and eye-tracking data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the differences in language comprehension between humans and LLMs, particularly in functional tasks such as inference and information retrieval.

**Method:** An LLM-based AI agent formed a graph-based text representation of a reading passage by grouping words into nodes and edges, followed by comparing eye fixation distributions on these important semantic structures.

**Key Contributions:**

	1. Introduction of a graph-based approach to language comprehension analysis
	2. Comparison of human and LLMs' understanding through eye-tracking data
	3. Insights into human-AI co-learning strategies based on semantic representations

**Result:** LLMs showed high consistency in understanding language based on the graph topological structure, indicating a richer comprehension model than previous studies focused on individual words.

**Limitations:** Focus on graph structures may limit understanding of other linguistic features; needs broader validation across more diverse contexts.

**Conclusion:** The findings suggest effective strategies for human-AI co-learning by leveraging LLMs' understanding of semantic networks.

**Abstract:** Reading comprehension is a fundamental skill in human cognitive development. With the advancement of Large Language Models (LLMs), there is a growing need to compare how humans and LLMs understand language across different contexts and apply this understanding to functional tasks such as inference, emotion interpretation, and information retrieval. Our previous work used LLMs and human biomarkers to study the reading comprehension process. The results showed that the biomarkers corresponding to words with high and low relevance to the inference target, as labeled by the LLMs, exhibited distinct patterns, particularly when validated using eye-tracking data. However, focusing solely on individual words limited the depth of understanding, which made the conclusions somewhat simplistic despite their potential significance. This study used an LLM-based AI agent to group words from a reading passage into nodes and edges, forming a graph-based text representation based on semantic meaning and question-oriented prompts. We then compare the distribution of eye fixations on important nodes and edges. Our findings indicate that LLMs exhibit high consistency in language understanding at the level of graph topological structure. These results build on our previous findings and offer insights into effective human-AI co-learning strategies.

</details>


### [44] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)

*Ana Davila, Jacinto Colan, Yasuhisa Hasegawa*

**Main category:** cs.CL

**Keywords:** Large Language Models, multi-agent debate, ambiguity resolution, human-computer interaction, language understanding

**Relevance Score:** 8

**TL;DR:** Introduces a multi-agent debate framework to enhance LLM performance on ambiguous user requests.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance LLM capabilities in resolving user request ambiguities and improve interaction quality with complex systems.

**Method:** The framework involves three LLM architectures (Llama3-8B, Gemma2-9B, Mistral-7B) and utilizes a dataset with diverse ambiguities to enable structured debates among models.

**Key Contributions:**

	1. Introduced a multi-agent debate framework for LLMs
	2. Demonstrated improved performance in ambiguity resolution
	3. Provided insights for developing adaptive language systems

**Result:** The debate framework significantly improved the performance of Llama3-8B and Mistral-7B, achieving a 76.7% success rate for Mistral-7B in resolving complex ambiguities.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of a debate framework to improve clarity in interactive systems, enhancing robustness in LLM applications.

**Abstract:** Large Language Models (LLMs) have demonstrated significant capabilities in understanding and generating human language, contributing to more natural interactions with complex systems. However, they face challenges such as ambiguity in user requests processed by LLMs. To address these challenges, this paper introduces and evaluates a multi-agent debate framework designed to enhance detection and resolution capabilities beyond single models. The framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and Mistral-7B variants) and a dataset with diverse ambiguities. The debate framework markedly enhanced the performance of Llama3-8B and Mistral-7B variants over their individual baselines, with Mistral-7B-led debates achieving a notable 76.7% success rate and proving particularly effective for complex ambiguities and efficient consensus. While acknowledging varying model responses to collaborative strategies, these findings underscore the debate framework's value as a targeted method for augmenting LLM capabilities. This work offers important insights for developing more robust and adaptive language understanding systems by showing how structured debates can lead to improved clarity in interactive systems.

</details>


### [45] [Value-Based Large Language Model Agent Simulation for Mutual Evaluation of Trust and Interpersonal Closeness](https://arxiv.org/abs/2507.11979)

*Yuki Sakamoto, Takahisa Uchida, Hiroshi Ishiguro*

**Main category:** cs.CL

**Keywords:** Large Language Models, Value Similarity, Human-Like Agents, Trust Building, Social Science

**Relevance Score:** 8

**TL;DR:** This study examines how value similarity influences relationship-building among LLM agents through two experiments, revealing that greater value similarity results in higher mutual trust and closeness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the role of value similarity in building trust and relationships in artificial societies formed by LLM agents, paralleling human social dynamics.

**Method:** Conducted two experiments: a preliminary one to evaluate controllability of values in LLMs, and a main experiment analyzing mutual evaluations of trust and closeness between LLM agent pairs with specific values.

**Key Contributions:**

	1. Investigation of value similarity in LLM agents
	2. Demonstrated controllability of LLM agent values
	3. Provided empirical evidence linking values to trust and relationship-building in artificial entities

**Result:** Higher value similarity among LLM agents led to significantly increased trust and interpersonal closeness after dialogue.

**Limitations:** 

**Conclusion:** LLM agent simulations effectively test social science theories and illuminate how values affect relationship building, offering potential for new theoretical insights.

**Abstract:** Large language models (LLMs) have emerged as powerful tools for simulating complex social phenomena using human-like agents with specific traits. In human societies, value similarity is important for building trust and close relationships; however, it remains unexplored whether this principle holds true in artificial societies comprising LLM agents. Therefore, this study investigates the influence of value similarity on relationship-building among LLM agents through two experiments. First, in a preliminary experiment, we evaluated the controllability of values in LLMs to identify the most effective model and prompt design for controlling the values. Subsequently, in the main experiment, we generated pairs of LLM agents imbued with specific values and analyzed their mutual evaluations of trust and interpersonal closeness following a dialogue. The experiments were conducted in English and Japanese to investigate language dependence. The results confirmed that pairs of agents with higher value similarity exhibited greater mutual trust and interpersonal closeness. Our findings demonstrate that the LLM agent simulation serves as a valid testbed for social science theories, contributes to elucidating the mechanisms by which values influence relationship building, and provides a foundation for inspiring new theories and insights into the social sciences.

</details>


### [46] [Simplifications are Absolutists: How Simplified Language Reduces Word Sense Awareness in LLM-Generated Definitions](https://arxiv.org/abs/2507.11981)

*Lukas Ellinger, Miriam Anschtz, Georg Groh*

**Main category:** cs.CL

**Keywords:** Large Language Models, nLP, simplification, homonyms, educational NLP

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of simplification on the quality of homonym definitions generated by LLMs for different target groups, revealing that simplification can degrade definition completeness but can be improved with fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the simplification of definitions affects different target groups and the quality of information provided by LLMs, particularly for words with multiple meanings.

**Method:** The study tests various LLMs (DeepSeek v3, Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B) using two novel evaluation datasets across different languages, assessing performance through LLM-as-Judge and human annotations.

**Key Contributions:**

	1. Introduced the impact of simplification on homonym definitions for different target groups.
	2. Developed novel evaluation datasets for multi-language assessment.
	3. Demonstrated the effectiveness of fine-tuning Llama 3.1 8B to improve homonym definition quality.

**Result:** Results indicate that simplification significantly reduces definition completeness, exposing users to a higher risk of misunderstanding, particularly for homonyms. In contrast, fine-tuning Llama 3.1 8B with Direct Preference Optimization improved its response quality for all prompt types.

**Limitations:** 

**Conclusion:** Balancing simplicity and completeness in educational NLP applications is crucial to provide reliable definitions that cater to all learners' needs.

**Abstract:** Large Language Models (LLMs) can provide accurate word definitions and explanations for any context. However, the scope of the definition changes for different target groups, like children or language learners. This is especially relevant for homonyms, words with multiple meanings, where oversimplification might risk information loss by omitting key senses, potentially misleading users who trust LLM outputs. We investigate how simplification impacts homonym definition quality across three target groups: Normal, Simple, and ELI5. Using two novel evaluation datasets spanning multiple languages, we test DeepSeek v3, Llama 4 Maverick, Qwen3-30B A3B, GPT-4o mini, and Llama 3.1 8B via LLM-as-Judge and human annotations. Our results show that simplification drastically degrades definition completeness by neglecting polysemy, increasing the risk of misunderstanding. Fine-tuning Llama 3.1 8B with Direct Preference Optimization substantially improves homonym response quality across all prompt types. These findings highlight the need to balance simplicity and completeness in educational NLP to ensure reliable, context-aware definitions for all learners.

</details>


### [47] [Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis](https://arxiv.org/abs/2507.12004)

*Josip Juki*

**Main category:** cs.CL

**Keywords:** neural language models, data efficiency, active learning, weak supervision, representation analysis

**Relevance Score:** 8

**TL;DR:** This thesis presents new optimization techniques and representation analyses to improve data and parameter efficiency in neural language models, focusing on robustness, generalization, and active learning integration.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle challenges related to data and parameter efficiency in neural language models and improve their robustness and generalization capabilities.

**Method:** The thesis introduces innovative approaches based on representation smoothness, regularization strategies using Jacobian and Hessian matrices, active learning strategies combined with parameter-efficient fine-tuning, and weak supervision techniques enhanced by in-context learning.

**Key Contributions:**

	1. Innovative representation smoothness approaches for stabilizing training
	2. Active learning combined with parameter-efficient fine-tuning
	3. Weak supervision improvements through in-context learning

**Result:** The proposed methods demonstrate substantial improvements in performance, stability, and efficiency across various NLP tasks, significantly outperforming traditional methods.

**Limitations:** 

**Conclusion:** The integration of these techniques enables better utilization of data, reduces dependence on extensive labeled datasets, and enhances model adaptability and robustness in low-resource settings.

**Abstract:** This thesis addresses challenges related to data and parameter efficiency in neural language models, with a focus on representation analysis and the introduction of new optimization techniques. The first part examines the properties and dynamics of language representations within neural models, emphasizing their significance in enhancing robustness and generalization. It proposes innovative approaches based on representation smoothness, including regularization strategies that utilize Jacobian and Hessian matrices to stabilize training and mitigate sensitivity to input perturbations. The second part focuses on methods to significantly enhance data and parameter efficiency by integrating active learning strategies with parameter-efficient fine-tuning, guided by insights from representation smoothness analysis. It presents smoothness-informed early-stopping techniques designed to eliminate the need for labeled validation sets and proposes innovative combinations of active learning and parameter-efficient fine-tuning to reduce labeling efforts and computational resources. Extensive experimental evaluations across various NLP tasks demonstrate that these combined approaches substantially outperform traditional methods in terms of performance, stability, and efficiency. The third part explores weak supervision techniques enhanced by in-context learning to effectively utilize unlabeled data, further reducing dependence on extensive labeling. It shows that using in-context learning as a mechanism for weak supervision enables models to better generalize from limited labeled data by leveraging unlabeled examples more effectively during training. Comprehensive empirical evaluations confirm significant gains in model accuracy, adaptability, and robustness, especially in low-resource settings and dynamic data environments.

</details>


### [48] [A Comparative Approach to Assessing Linguistic Creativity of Large Language Models and Humans](https://arxiv.org/abs/2507.12039)

*Anca Dinu, Andra-Maria Florescu, Alina Resceanu*

**Main category:** cs.CL

**Keywords:** Linguistic creativity, Large Language Models, Human creativity, Originality, Metaphorical language

**Relevance Score:** 9

**TL;DR:** This paper presents a linguistic creativity test for humans and LLMs, revealing LLMs outperform humans overall in originality, elaboration, and flexibility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and compare the linguistic creativity of humans and Large Language Models (LLMs) using a structured test.

**Method:** The test included various tasks assessing the generation of new words and phrases based on derivation, compounding, and metaphorical language. 24 humans and 24 LLMs participated, with evaluations done using the OCSAI tool.

**Key Contributions:**

	1. Development of a general linguistic creativity test for humans and LLMs
	2. Demonstration of comparative performance between human creativity and LLM creativity
	3. Insights into the types of creativity exhibited by LLMs versus humans

**Result:** LLMs outperformed humans in all assessed criteria and performed better in 6 out of 8 test tasks, although subtle differences in uniqueness of answers were noted.

**Limitations:** 

**Conclusion:** Humans leaned towards extending creativity, while LLMs preferred fixed creativity, indicating a divergence in creative strategies between the two.

**Abstract:** The following paper introduces a general linguistic creativity test for humans and Large Language Models (LLMs). The test consists of various tasks aimed at assessing their ability to generate new original words and phrases based on word formation processes (derivation and compounding) and on metaphorical language use. We administered the test to 24 humans and to an equal number of LLMs, and we automatically evaluated their answers using OCSAI tool for three criteria: Originality, Elaboration, and Flexibility. The results show that LLMs not only outperformed humans in all the assessed criteria, but did better in six out of the eight test tasks. We then computed the uniqueness of the individual answers, which showed some minor differences between humans and LLMs. Finally, we performed a short manual analysis of the dataset, which revealed that humans are more inclined towards E(extending)-creativity, while LLMs favor F(ixed)-creativity.

</details>


### [49] [Evaluating the Ability of Large Language Models to Reason about Cardinal Directions, Revisited](https://arxiv.org/abs/2507.12059)

*Anthony G Cohn, Robert E Blackwell*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cardinal Directions, Spatial Reasoning, Benchmark, Qualitative Reasoning

**Relevance Score:** 8

**TL;DR:** This paper examines the reasoning abilities of 28 Large Language Models regarding cardinal directions using a specialized benchmark.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the reasoning capabilities of LLMs in determining cardinal directions, given their importance in spatial understanding.

**Method:** An extensive testing setup using templates that vary means of locomotion and perspective (first, second, or third person) to assess LLM performance.

**Key Contributions:**

	1. Development of a benchmark for testing LLMs on cardinal direction reasoning.
	2. Insights into the variations of performance based on different locomotion means and perspectives.
	3. Extension of previous research findings on LLM reasoning abilities.

**Result:** Most LLMs, including newer Large Reasoning Models, struggled to reliably determine the correct cardinal direction across various scenarios.

**Limitations:** Focuses solely on cardinal directions; results may not generalize to other reasoning tasks.

**Conclusion:** The findings indicate limitations in LLMs' spatial reasoning abilities, suggesting a need for improvement in their handling of cardinal direction tasks.

**Abstract:** We investigate the abilities of 28 Large language Models (LLMs) to reason about cardinal directions (CDs) using a benchmark generated from a set of templates, extensively testing an LLM's ability to determine the correct CD given a particular scenario. The templates allow for a number of degrees of variation such as means of locomotion of the agent involved, and whether set in the first, second or third person. Even the newer Large Reasoning Models are unable to reliably determine the correct CD for all questions. This paper summarises and extends earlier work presented at COSIT-24.

</details>


### [50] [StylOch at PAN: Gradient-Boosted Trees with Frequency-Based Stylometric Features](https://arxiv.org/abs/2507.12064)

*Jeremi K. Ochab, Mateusz Matias, Tymoteusz Boba, Tomasz Walkowiak*

**Main category:** cs.CL

**Keywords:** AI detection, stylometric pipeline, spaCy, gradient boosting machines, text classification

**Relevance Score:** 4

**TL;DR:** The paper presents a modular stylometric pipeline for AI detection using spaCy models and light-gradient boosting machines, trained on a large corpus of machine-generated texts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the classification of AI-generated texts using a computationally inexpensive yet effective approach.

**Method:** The paper employs public spaCy models for text preprocessing and feature extraction, followed by applying light-gradient boosting machines as classifiers on a large corpus of over 500,000 machine-generated texts.

**Key Contributions:**

	1. Development of a modular stylometric pipeline for AI detection
	2. Utilization of a large corpus of over 500,000 texts for training
	3. Combining computational efficiency with explainability in AI detection methods.

**Result:** The approach demonstrates enhanced classification capacity by exploring various parameter options, capitalizing on a substantial training dataset.

**Limitations:** 

**Conclusion:** A non-neural and explainable method was found to be effective for the binary AI detection task.

**Abstract:** This submission to the binary AI detection task is based on a modular stylometric pipeline, where: public spaCy models are used for text preprocessing (including tokenisation, named entity recognition, dependency parsing, part-of-speech tagging, and morphology annotation) and extracting several thousand features (frequencies of n-grams of the above linguistic annotations); light-gradient boosting machines are used as the classifier. We collect a large corpus of more than 500 000 machine-generated texts for the classifier's training. We explore several parameter options to increase the classifier's capacity and take advantage of that training set. Our approach follows the non-neural, computationally inexpensive but explainable approach found effective previously.

</details>


### [51] [BOOKCOREF: Coreference Resolution at Book Scale](https://arxiv.org/abs/2507.12075)

*Giuliano Martinelli, Tommaso Bonomo, Pere-Llus Huguet Cabot, Roberto Navigli*

**Main category:** cs.CL

**Keywords:** coreference resolution, long documents, narrative texts, benchmark, machine learning

**Relevance Score:** 8

**TL;DR:** This paper presents a novel pipeline for creating a book-scale benchmark, BOOKCOREF, for assessing coreference resolution systems on long texts exceeding 200,000 tokens.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current coreference resolution benchmarks are inadequate for evaluating systems on long documents, which poses challenges in performance measurement for extensive texts such as books.

**Method:** A new automatic pipeline is developed to generate high-quality coreference resolution annotations for full narrative texts, leading to the creation of the BOOKCOREF benchmark.

**Key Contributions:**

	1. Development of an automatic annotation pipeline for long texts
	2. Creation of the BOOKCOREF benchmark for coreference resolution
	3. Identification of performance challenges in processing large documents

**Result:** Experiments demonstrate that coreference resolution systems can improve performance by up to +20 CoNLL-F1 points when evaluated using the BOOKCOREF benchmark, although challenges arise that reduce effectiveness compared to smaller document benchmarks.

**Limitations:** Current models do not achieve the same performance on book-scale texts as they do on smaller documents.

**Conclusion:** The study highlights the need for novel coreference resolution methods capable of handling book-scale texts and identifies performance gaps in existing models.

**Abstract:** Coreference Resolution systems are typically evaluated on benchmarks containing small- to medium-scale documents. When it comes to evaluating long texts, however, existing benchmarks, such as LitBank, remain limited in length and do not adequately assess system capabilities at the book scale, i.e., when co-referring mentions span hundreds of thousands of tokens. To fill this gap, we first put forward a novel automatic pipeline that produces high-quality Coreference Resolution annotations on full narrative texts. Then, we adopt this pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with an average document length of more than 200,000 tokens. We carry out a series of experiments showing the robustness of our automatic procedure and demonstrating the value of our resource, which enables current long-document coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full books. Moreover, we report on the new challenges introduced by this unprecedented book-scale setting, highlighting that current models fail to deliver the same performance they achieve on smaller documents. We release our data and code to encourage research and development of new book-scale Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

</details>


### [52] [Findings of MEGA: Maths Explanation with LLMs using the Socratic Method for Active Learning](https://arxiv.org/abs/2507.12079)

*Tosin Adewumi, Foteini Simistira Liwicki, Marcus Liwicki, Viktor Gardelli, Lama Alkhaled, Hamam Mokayed*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Machine Learning, Educational Technology, Large Language Models, Gamification

**Relevance Score:** 8

**TL;DR:** The paper investigates the effectiveness of a novel educational approach called MEGA, combining several pedagogical methods enhanced by large language models (LLMs) to improve university students' learning in mathematics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address students' struggles with math and improve learning outcomes by enhancing traditional teaching methods through innovative pedagogical strategies involving LLMs.

**Method:** An intervention study comparing the MEGA approach with traditional Chain of Thought (CoT) methods using a within-group design on two different math datasets with university students.

**Key Contributions:**

	1. Introduction of the MEGA educational approach combining multiple pedagogical methods
	2. Empirical evidence showing the effectiveness of MEGA over traditional teaching methods in math education
	3. Utilization of large language models to enhance student learning experiences.

**Result:** Students reported a significantly better learning experience with the MEGA approach compared to the CoT method, especially in the more challenging MATH dataset, achieving 47.5% versus 26.67%.

**Limitations:** 

**Conclusion:** The MEGA method demonstrates a superior ability to help students understand and learn difficult math problems compared to traditional pedagogical methods.

**Abstract:** This paper presents an intervention study on the effects of the combined methods of (1) the Socratic method, (2) Chain of Thought (CoT) reasoning, (3) simplified gamification and (4) formative feedback on university students' Maths learning driven by large language models (LLMs). We call our approach Mathematics Explanations through Games by AI LLMs (MEGA). Some students struggle with Maths and as a result avoid Math-related discipline or subjects despite the importance of Maths across many fields, including signal processing. Oftentimes, students' Maths difficulties stem from suboptimal pedagogy. We compared the MEGA method to the traditional step-by-step (CoT) method to ascertain which is better by using a within-group design after randomly assigning questions for the participants, who are university students. Samples (n=60) were randomly drawn from each of the two test sets of the Grade School Math 8K (GSM8K) and Mathematics Aptitude Test of Heuristics (MATH) datasets, based on the error margin of 11%, the confidence level of 90%, and a manageable number of samples for the student evaluators. These samples were used to evaluate two capable LLMs at length (Generative Pretrained Transformer 4o (GPT4o) and Claude 3.5 Sonnet) out of the initial six that were tested for capability. The results showed that students agree in more instances that the MEGA method is experienced as better for learning for both datasets. It is even much better than the CoT (47.5% compared to 26.67%) in the more difficult MATH dataset, indicating that MEGA is better at explaining difficult Maths problems.

</details>


### [53] [Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis](https://arxiv.org/abs/2507.12126)

*Payal Bhattad, Sai Manoj Pudukotai Dinakarrao, Anju Gupta*

**Main category:** cs.CL

**Keywords:** data augmentation, NLP, large language models, semantic consistency, topic modeling

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for evaluating text data augmentation methods utilizing large language models, focusing on semantic preservation and diversity in NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address data sparsity in NLP and improve the effectiveness of data augmentation methods, particularly in low-resource settings.

**Method:** The framework includes Scalability Analysis for measuring semantic consistency and Iterative Augmentation with Summarization Refinement (IASR) for tracking semantic drift across paraphrasing cycles.

**Key Contributions:**

	1. Introduction of a principled evaluation framework for LLM-based text augmentation.
	2. Development of Scalability Analysis and IASR for semantic consistency measurement.
	3. Demonstrated effectiveness in enhancing topic granularity and reducing overlaps in a real-world application.

**Result:** Empirical evaluations demonstrated that GPT-3.5 Turbo provided the best balance of semantic fidelity, diversity, and efficiency, resulting in significant improvements in topic modeling tasks.

**Limitations:** 

**Conclusion:** The evaluation framework is validated as useful for structured assessment of LLM-based augmentation in NLP, showing enhanced topic generation capabilities.

**Abstract:** Text data augmentation is a widely used strategy for mitigating data sparsity in natural language processing (NLP), particularly in low-resource settings where limited samples hinder effective semantic modeling. While augmentation can improve input diversity and downstream interpretability, existing techniques often lack mechanisms to ensure semantic preservation during large-scale or iterative generation, leading to redundancy and instability. This work introduces a principled evaluation framework for large language model (LLM) based text augmentation, comprising two components: (1) Scalability Analysis, which measures semantic consistency as augmentation volume increases, and (2) Iterative Augmentation with Summarization Refinement (IASR), which evaluates semantic drift across recursive paraphrasing cycles. Empirical evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the best balance of semantic fidelity, diversity, and generation efficiency. Applied to a real-world topic modeling task using BERTopic with GPT-enhanced few-shot labeling, the proposed approach results in a 400% increase in topic granularity and complete elimination of topic overlaps. These findings validated the utility of the proposed frameworks for structured evaluation of LLM-based augmentation in practical NLP pipelines.

</details>


### [54] [Overview of the Sensemaking Task at the ELOQUENT 2025 Lab: LLMs as Teachers, Students and Evaluators](https://arxiv.org/abs/2507.12143)

*Pavel indel, Ondej Bojar*

**Main category:** cs.CL

**Keywords:** generative language models, evaluation tasks, natural language processing, sensemaking, machine learning

**Relevance Score:** 8

**TL;DR:** The paper discusses ELOQUENT's Sensemaking task for evaluating generative language models through a structured testing process involving question creation, answering, and evaluation across multiple languages.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To establish criteria for assessing the effectiveness of generative language models in comprehending and processing text, specifically through structured tasks that imitate educational evaluations.

**Method:** The Sensemaking task involves three main roles: Teacher systems that create questions, Student systems that answer them, and Evaluator systems that score the answers. The evaluation is based on 7 diverse sources of test materials across 4 languages, and both automatic and manual evaluation methods were applied.

**Key Contributions:**

	1. Outlined a novel structured approach for evaluating generative language models
	2. Identified specific challenges in question answering and evaluation processes using LLMs
	3. Provided baseline comparisons with commercial LLMs in the assessment framework.

**Result:** The paper reports participation from 4 teams, highlighting challenges such as the difficulty in evaluating question quality, the performance of LLMs in question answering, and flawed evaluations when using LLMs to assess answers.

**Limitations:** The study reveals that discerning the quality of question sets and the reliability of LLMs in answer evaluation remains problematic.

**Conclusion:** While the generative models show acceptable performance, improvements are needed in both question creation and evaluation methodologies to enhance accuracy and reliability in assessing answers.

**Abstract:** ELOQUENT is a set of shared tasks that aims to create easily testable high-level criteria for evaluating generative language models. Sensemaking is one such shared task.   In Sensemaking, we try to assess how well generative models ``make sense out of a given text'' in three steps inspired by exams in a classroom setting: (1) Teacher systems should prepare a set of questions, (2) Student systems should answer these questions, and (3) Evaluator systems should score these answers, all adhering rather strictly to a given set of input materials.   We report on the 2025 edition of Sensemaking, where we had 7 sources of test materials (fact-checking analyses of statements, textbooks, transcribed recordings of a lecture, and educational videos) spanning English, German, Ukrainian, and Czech languages.   This year, 4 teams participated, providing us with 2 Teacher submissions, 2 Student submissions, and 2 Evaluator submissions. We added baselines for Teacher and Student using commercial large language model systems. We devised a fully automatic evaluation procedure, which we compare to a minimalistic manual evaluation.   We were able to make some interesting observations. For the first task, the creation of questions, better evaluation strategies will still have to be devised because it is difficult to discern the quality of the various candidate question sets. In the second task, question answering, the LLMs examined overall perform acceptably, but restricting their answers to the given input texts remains problematic. In the third task, evaluation of question answers, our adversarial tests reveal that systems using the LLM-as-a-Judge paradigm erroneously rate both garbled question-answer pairs and answers to mixed-up questions as acceptable.

</details>


### [55] [Toward a Behavioural Translation Style Space: Simulating the Temporal Dynamics of Affect, Behaviour, and Cognition in Human Translation Production](https://arxiv.org/abs/2507.12208)

*Michael Carl, Takanori Mizowaki, Aishvarya Ray, Masaru Yamada, Devi Sri Bandaru, Xinyue Ren*

**Main category:** cs.CL

**Keywords:** Behavioral Translation Style Space, Cognitive Processes, Human Translation, Gaze Data, Keystroke Analysis

**Relevance Score:** 2

**TL;DR:** The paper presents a Behavioural Translation Style Space (BTSS) to analyze translation behavior using keystroke and gaze data, aimed at understanding cognitive processes in translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the relationship between observable translation behavior and higher-order cognitive processes during the act of translation.

**Method:** Analysis of keystrokes and gaze data to identify hidden mental processing structures, organized in a hierarchical multi-layered BTSS.

**Key Contributions:**

	1. Introduction of the BTSS framework
	2. Hierarchical organization of translation behavior
	3. Integration of cognitive processes with observable translation actions

**Result:** Development of BTSS as a foundation for a computational translation agent that simulates the dynamics of affect, behavior, and cognition in human translation.

**Limitations:** 

**Conclusion:** The BTSS framework can enhance understanding and simulation of cognitive processes in translation, revealing insights into the interplay between behavior and translation states.

**Abstract:** The paper introduces a Behavioural Translation Style Space (BTSS) that describes possible behavioural translation patterns. The suggested BTSS is organized as a hierarchical structure that entails various embedded processing layers. We posit that observable translation behaviour - i.e., eye and finger movements - is fundamental when executing the physical act of translation but it is caused and shaped by higher-order cognitive processes and affective translation states. We analyse records of keystrokes and gaze data as indicators of the hidden mental processing structure and organize the behavioural patterns as a multi-layered embedded BTSS. The BTSS serves as the basis for a computational translation agent to simulate the temporal dynamics of affect, automatized behaviour and cognition during human translation production.

</details>


### [56] [Towards few-shot isolated word reading assessment](https://arxiv.org/abs/2507.12217)

*Reuben Smit, Retief Louw, Herman Kamper*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Few-shot Learning, Self-Supervised Learning, Child Speech, Low-Resource Settings

**Relevance Score:** 6

**TL;DR:** This paper presents an ASR-free method for assessing isolated word reading in low-resource settings using few-shot learning and self-supervised models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective reading assessment methods in low-resource environments, particularly for children.

**Method:** The approach involves comparing child speech inputs to adult reference templates using intermediate layers from SSL models, including discretisation of features and barycentre averaging.

**Key Contributions:**

	1. Development of an ASR-free method for word reading assessment
	2. Investigation of SSL model features for child speech
	3. Experimental evidence of performance discrepancies between adult and child speech

**Result:** Idealised experiments reveal reasonable performance for adult speech but significant performance drops for child speech, indicating challenges in processing child data with SSL representations.

**Limitations:** Performance drops significantly for child speech input even when using child templates within the few-shot system.

**Conclusion:** While SSL methods show promise for speech tasks, they struggle with child speech in few-shot classification scenarios, underscoring limitations in current techniques.

**Abstract:** We explore an ASR-free method for isolated word reading assessment in low-resource settings. Our few-shot approach compares input child speech to a small set of adult-provided reference templates. Inputs and templates are encoded using intermediate layers from large self-supervised learned (SSL) models. Using an Afrikaans child speech benchmark, we investigate design options such as discretising SSL features and barycentre averaging of the templates. Idealised experiments show reasonable performance for adults, but a substantial drop for child speech input, even with child templates. Despite the success of employing SSL representations in low-resource speech tasks, our work highlights the limitations of SSL representations for processing child data when used in a few-shot classification system.

</details>


### [57] [Improving Contextual ASR via Multi-grained Fusion with Large Language Models](https://arxiv.org/abs/2507.12252)

*Shilin Zhou, Zhenghua Li*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Keyword Recognition, Large Language Models, Multi-grained Fusion, Late-fusion Strategy

**Relevance Score:** 8

**TL;DR:** This paper introduces a multi-grained fusion approach for ASR that combines token-level and phrase-level keyword recognition using LLMs, achieving state-of-the-art performance in keyword accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accuracy of keyword recognition in ASR systems, particularly for proper nouns and user-specific entities, by integrating different fusion strategies.

**Method:** The proposed method uses a late-fusion strategy that integrates acoustic information from ASR with contextual knowledge from LLMs, utilizing both token-level and phrase-level fusion.

**Key Contributions:**

	1. A novel multi-grained fusion approach that integrates token-level and phrase-level recognition.
	2. Implementation of a late-fusion strategy combining ASR data with LLM context.
	3. Demonstration of state-of-the-art results on keyword-related metrics.

**Result:** Experiments on Chinese and English datasets show that the approach achieves state-of-the-art performance on keyword-related metrics while maintaining high accuracy on non-keyword text.

**Limitations:** 

**Conclusion:** The findings indicate that the combination of token-level and phrase-level components significantly enhances performance and that both are critical in the multi-grained framework.

**Abstract:** While end-to-end Automatic Speech Recognition (ASR) models have shown impressive performance in transcribing general speech, they often struggle to accurately recognize contextually relevant keywords, such as proper nouns or user-specific entities.   Previous approaches have explored leveraging keyword dictionaries in the textual modality to improve keyword recognition, either through token-level fusion that guides token-by-token generation or phrase-level fusion that enables direct copying of keyword phrases.   However, these methods operate at different granularities and have their own limitations.   In this paper, we propose a novel multi-grained fusion approach that jointly leverages the strengths of both token-level and phrase-level fusion with Large Language Models (LLMs).   Our approach incorporates a late-fusion strategy that elegantly combines ASR's acoustic information with LLM's rich contextual knowledge, balancing fine-grained token precision with holistic phrase-level understanding.   Experiments on Chinese and English datasets demonstrate that our approach achieves state-of-the-art performance on keyword-related metrics while preserving high accuracy on non-keyword text.   Ablation studies further confirm that the token-level and phrase-level components both contribute significantly to the performance gains, complementing each other in our joint multi-grained framework.   The code and models will be publicly available at https://github.com/.

</details>


### [58] [Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese](https://arxiv.org/abs/2507.12260)

*Yikang Liu, Wanyang Zhang, Yiming Wang, Jialong Tang, Pei Zhang, Baosong Yang, Fei Huang, Rui Wang, Hai Hu*

**Main category:** cs.CL

**Keywords:** translationese, T-index, language models, human judgment, machine translation

**Relevance Score:** 5

**TL;DR:** Introducing the translationese-index (T-index) for quantitatively measuring translationese using fine-tuned language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a quantitative measure for translationese that captures its nuances and can predict human judgments.

**Method:** Utilization of two contrastively fine-tuned language models to compute a translationese-index (T-index) from likelihood ratios, evaluated on synthetic and real translation datasets.

**Key Contributions:**

	1. First quantitative measure for translationese (T-index)
	2. Robustness demonstrated through human judgment correlation
	3. Complementary to existing MT quality metrics

**Result:** The T-index robustly predicts human annotations of translationese, correlating well with human ratings and showing low correlation with existing MT quality metrics.

**Limitations:** 

**Conclusion:** T-index effectively indicates translationese levels and complements existing translation quality estimation methods like BLEU and COMET.

**Abstract:** In this paper, we propose the first quantitative measure for translationese -- the translationese-index (T-index) for graded and generalizable measurement of translationese, computed from the likelihood ratios of two contrastively fine-tuned language models (LMs). We use a synthesized dataset and a dataset with translations in the wild to evaluate T-index's generalizability in cross-domain settings and its validity against human judgments. Our results show that T-index is both robust and efficient. T-index scored by two 0.5B LMs fine-tuned on only 1-5k pairs of synthetic data can well capture translationese in the wild. We find that the relative differences in T-indices between translations can well predict pairwise translationese annotations obtained from human annotators; and the absolute values of T-indices correlate well with human ratings of degrees of translationese (Pearson's $r = 0.568$). Additionally, the correlation between T-index and existing machine translation (MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting that T-index is not covered by these metrics and can serve as a complementary metric in MT QE.

</details>


### [59] [Infherno: End-to-end Agent-based FHIR Resource Synthesis from Free-form Clinical Notes](https://arxiv.org/abs/2507.12261)

*Johann Frei, Nils Feldhus, Lisa Raithel, Roland Roller, Alexander Meyer, Frank Kramer*

**Main category:** cs.CL

**Keywords:** HL7 FHIR, clinical data integration, LLM agents

**Relevance Score:** 9

**TL;DR:** Proposes Infherno, an end-to-end framework using LLM agents for converting unstructured clinical notes into structured FHIR resources, improving interoperability in healthcare.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve integration and interoperability of clinical data using the HL7 FHIR standard, overcoming limitations of previous rule-based systems and LLMs.

**Method:** Development of an end-to-end framework that leverages LLM agents, code execution, and healthcare terminology databases to convert free-form clinical notes into structured data adhering to the FHIR schema.

**Key Contributions:**

	1. Development of an LLM-powered framework for clinical data integration
	2. Integration with healthcare terminology databases
	3. Competitive performance against human benchmarks for FHIR resource prediction

**Result:** Infherno competes well against a human baseline in predicting FHIR resources from unstructured text, facilitating clinical data integration across institutions.

**Limitations:** 

**Conclusion:** The implementation of Infherno provides significant advances in the automated structuring of clinical notes, promoting interoperability in healthcare settings.

**Abstract:** For clinical data integration and healthcare services, the HL7 FHIR standard has established itself as a desirable format for interoperability between complex health data. Previous attempts at automating the translation from free-form clinical notes into structured FHIR resources rely on modular, rule-based systems or LLMs with instruction tuning and constrained decoding. Since they frequently suffer from limited generalizability and structural inconformity, we propose an end-to-end framework powered by LLM agents, code execution, and healthcare terminology database tools to address these issues. Our solution, called Infherno, is designed to adhere to the FHIR document schema and competes well with a human baseline in predicting FHIR resources from unstructured text. The implementation features a front end for custom and synthetic data and both local and proprietary models, supporting clinical data integration processes and interoperability across institutions.

</details>


### [60] [Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding](https://arxiv.org/abs/2507.12295)

*Feng Xiao, Jicong Fan*

**Main category:** cs.CL

**Keywords:** Text Anomaly Detection, Large Language Models, Embedding Evaluation

**Relevance Score:** 8

**TL;DR:** This paper introduces a comprehensive benchmark for text anomaly detection, utilizing embeddings from various pre-trained language models to evaluate their effectiveness across multiple datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of standardized benchmarks for evaluating text anomaly detection methods hinders progress in the field.

**Method:** The study conducts empirical evaluations using embeddings from early and multiple large language models (LLMs) on diverse text datasets, employing comprehensive metrics.

**Key Contributions:**

	1. Introduced a comprehensive benchmark for text anomaly detection.
	2. Evaluated a variety of embedding models across multiple domains.
	3. Discovered that the quality of embeddings significantly impacts detection efficacy.

**Result:** The research finds that embedding quality is crucial for anomaly detection and that deep learning approaches do not outperform classical methods when using LLM embeddings.

**Limitations:** Limited to the evaluated embedding models and datasets; further studies needed with additional models and domains.

**Conclusion:** The open-sourcing of the benchmark toolkit facilitates future research in text anomaly detection systems.

**Abstract:** Text anomaly detection is a critical task in natural language processing (NLP), with applications spanning fraud detection, misinformation identification, spam detection and content moderation, etc. Despite significant advances in large language models (LLMs) and anomaly detection algorithms, the absence of standardized and comprehensive benchmarks for evaluating the existing anomaly detection methods on text data limits rigorous comparison and development of innovative approaches. This work performs a comprehensive empirical study and introduces a benchmark for text anomaly detection, leveraging embeddings from diverse pre-trained language models across a wide array of text datasets. Our work systematically evaluates the effectiveness of embedding-based text anomaly detection by incorporating (1) early language models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI (small, ada, large)); (3) multi-domain text datasets (news, social media, scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC). Our experiments reveal a critical empirical insight: embedding quality significantly governs anomaly detection efficacy, and deep learning-based approaches demonstrate no performance advantage over conventional shallow algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived embeddings.In addition, we observe strongly low-rank characteristics in cross-model performance matrices, which enables an efficient strategy for rapid model evaluation (or embedding evaluation) and selection in practical applications. Furthermore, by open-sourcing our benchmark toolkit that includes all embeddings from different models and code at https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work provides a foundation for future research in robust and scalable text anomaly detection systems.

</details>


### [61] [Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization](https://arxiv.org/abs/2507.12308)

*Prashanth Vijayaraghavan, Apoorva Nitsure, Charles Mackin, Luyao Shi, Stefano Ambrogio, Arvind Haran, Viresh Paruthi, Ali Elzein, Dan Coops, David Beymer, Tyler Baldwin, Ehsan Degan*

**Main category:** cs.CL

**Keywords:** Large Language Models, VHDL, Code Generation, Summarization, Chain-of-Descriptions

**Relevance Score:** 5

**TL;DR:** The study evaluates the performance of existing code LLMs for VHDL code generation and summarization, highlighting their underperformance in this domain and proposing an improved approach called Chain-of-Descriptions (CoDes).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of research on evaluating LLMs for hardware description languages like VHDL, despite their use in various code-related tasks.

**Method:** The study assesses LLM performance using two datasets (VHDL-Eval and VHDL-Xform) and introduces CoDes, which generates intermediate descriptive steps to improve code generation and summarization.

**Key Contributions:**

	1. Evaluation of LLMs for VHDL tasks using two datasets
	2. Introduction of Chain-of-Descriptions (CoDes) to enhance performance
	3. Empirical results showing significant improvements in generated code quality

**Result:** LLMs consistently underperformed in VHDL tasks across metrics, while the CoDes approach demonstrated significant improvements over standard prompting strategies.

**Limitations:** 

**Conclusion:** CoDes not only enhances VHDL code generation and summarization but also provides a framework for future research on improving code LLMs.

**Abstract:** Large Language Models (LLMs) have become widely used across diverse NLP tasks and domains, demonstrating their adaptability and effectiveness. In the realm of Electronic Design Automation (EDA), LLMs show promise for tasks like Register-Transfer Level (RTL) code generation and summarization. However, despite the proliferation of LLMs for general code-related tasks, there's a dearth of research focused on evaluating and refining these models for hardware description languages (HDLs), notably VHDL. In this study, we evaluate the performance of existing code LLMs for VHDL code generation and summarization using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter, an in-house dataset, aims to gauge LLMs' understanding of functionally equivalent code. Our findings reveal consistent underperformance of these models across different metrics, underscoring a significant gap in their suitability for this domain. To address this challenge, we propose Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of LLMs for VHDL code generation and summarization tasks. CoDes involves generating a series of intermediate descriptive steps based on: (i) the problem statement for code generation, and (ii) the VHDL code for summarization. These steps are then integrated with the original input prompt (problem statement or code) and provided as input to the LLMs to generate the final output. Our experiments demonstrate that the CoDes approach significantly surpasses the standard prompting strategy across various metrics on both datasets. This method not only improves the quality of VHDL code generation and summarization but also serves as a framework for future research aimed at enhancing code LLMs for VHDL.

</details>


### [62] [Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception](https://arxiv.org/abs/2507.12356)

*Liu He, Yuanchao Li, Rui Feng, XinRan Han, Yin-Long Liu, Yuwei Yang, Zude Zhu, Jiahong Yuan*

**Main category:** cs.CL

**Keywords:** gender bias, Alzheimer's Disease, speech perception, acoustic analysis, detection models

**Relevance Score:** 7

**TL;DR:** The study investigates gender bias in the perception of Alzheimer's Disease speech, showing that male speech is more frequently identified as AD, especially in Chinese speech, and emphasizes the need to address this bias in AD detection models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the influence of gender bias on the perception of Alzheimer's Disease speech and heretofore unexamined effects in different linguistic contexts.

**Method:** Conducted a perception experiment with 16 Chinese listeners evaluating both Chinese and Greek speech, with acoustic analysis of male and female speech.

**Key Contributions:**

	1. Demonstrated gender bias in AD speech perception
	2. Presented acoustic analysis linking shimmer values to AD identification
	3. Highlighted the impact of gender bias on detection model performance

**Result:** Identified a significant gender bias where male speech was more frequently perceived as AD; acoustic analysis correlates shimmer values in male speech with AD identification.

**Limitations:** The study involved a limited sample size of listeners and languages, which may affect generalizability.

**Conclusion:** Addressing gender bias is crucial in the development of Alzheimer's Disease detection models and requires further research for validation in various linguistic contexts.

**Abstract:** Gender bias has been widely observed in speech perception tasks, influenced by the fundamental voicing differences between genders. This study reveals a gender bias in the perception of Alzheimer's Disease (AD) speech. In a perception experiment involving 16 Chinese listeners evaluating both Chinese and Greek speech, we identified that male speech was more frequently identified as AD, with this bias being particularly pronounced in Chinese speech. Acoustic analysis showed that shimmer values in male speech were significantly associated with AD perception, while speech portion exhibited a significant negative correlation with AD identification. Although language did not have a significant impact on AD perception, our findings underscore the critical role of gender bias in AD speech perception. This work highlights the necessity of addressing gender bias when developing AD detection models and calls for further research to validate model performance across different linguistic contexts.

</details>


### [63] [Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests through Debate](https://arxiv.org/abs/2507.12370)

*Ana Davila, Jacinto Colan, Yasuhisa Hasegawa*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-agent system, Ambiguity resolution, Human-computer interaction, Natural language understanding

**Relevance Score:** 9

**TL;DR:** The paper presents a multi-agent debate framework to enhance the detection and resolution of ambiguities in user requests processed by large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLMs struggle with ambiguity in user requests, justifying the need for improved interaction frameworks.

**Method:** The study implements a multi-agent debate framework involving three LLM architectures (Llama3-8B, Gemma2-9B, Mistral-7B) evaluated on a dataset rich in ambiguities.

**Key Contributions:**

	1. Introduction of a multi-agent debate framework for LLMs
	2. Demonstration of effective ambiguity resolution using collective model strategies
	3. Empirical evidence of enhanced performance through structured debates

**Result:** Debate framework significantly improved performance; Mistral-7B-led debates achieved a 76.7% success rate in handling complex ambiguities.

**Limitations:** Varied responses of models to collaborative strategies may affect consistency.

**Conclusion:** The findings emphasize the value of structured debates for enhancing LLM capabilities in clarity and user interactions.

**Abstract:** Large Language Models (LLMs) have demonstrated significant capabilities in understanding and generating human language, contributing to more natural interactions with complex systems. However, they face challenges such as ambiguity in user requests processed by LLMs. To address these challenges, this paper introduces and evaluates a multi-agent debate framework designed to enhance detection and resolution capabilities beyond single models. The framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and Mistral-7B variants) and a dataset with diverse ambiguities. The debate framework markedly enhanced the performance of Llama3-8B and Mistral-7B variants over their individual baselines, with Mistral-7B-led debates achieving a notable 76.7% success rate and proving particularly effective for complex ambiguities and efficient consensus. While acknowledging varying model responses to collaborative strategies, these findings underscore the debate framework's value as a targeted method for augmenting LLM capabilities. This work offers important insights for developing more robust and adaptive language understanding systems by showing how structured debates can lead to improved clarity in interactive systems.

</details>


### [64] [Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics](https://arxiv.org/abs/2507.12372)

*Meysam Alizadeh, Fabrizio Gilardi, Zeynab Samei, Mohsen Mosleh*

**Main category:** cs.CL

**Keywords:** Large Language Models, Social Media Analysis, Demographic Inference, Ethical Concerns, Computational Social Science

**Relevance Score:** 8

**TL;DR:** This paper evaluates the ability of web browsing large language models (LLMs) to infer demographic attributes of social media users from usernames, revealing both their potential in computational social science and risks of misuse.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Explore the capacity of LLMs with web browsing capabilities to analyze social media data and infer user demographics, addressing a gap in the current research.

**Method:** Utilized a synthetic dataset of 48 Twitter accounts and a survey dataset of 1,384 international participants to evaluate LLM performance in predicting demographics from social media profiles.

**Key Contributions:**

	1. Demonstrated LLMs' capability to access and analyze social media data for demographic inference.
	2. Identified biases based on user activity in demographic predictions.
	3. Provided guidelines for LLM application in research with societal implications.

**Result:** LLMs demonstrated the ability to accurately predict user demographics from social media content, although biases were observed in accounts with limited activity.

**Limitations:** The study's reliance on synthetic accounts and potential biases in data analysis limit generalizability.

**Conclusion:** While LLMs can contribute to computational social science, their ability to infer demographics raises ethical concerns, necessitating protective measures in their deployment.

**Abstract:** Large language models (LLMs) have traditionally relied on static training data, limiting their knowledge to fixed snapshots. Recent advancements, however, have equipped LLMs with web browsing capabilities, enabling real time information retrieval and multi step reasoning over live web content. While prior studies have demonstrated LLMs ability to access and analyze websites, their capacity to directly retrieve and analyze social media data remains unexplored. Here, we evaluate whether web browsing LLMs can infer demographic attributes of social media users given only their usernames. Using a synthetic dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international participants, we show that these models can access social media content and predict user demographics with reasonable accuracy. Analysis of the synthetic dataset further reveals how LLMs parse and interpret social media profiles, which may introduce gender and political biases against accounts with minimal activity. While this capability holds promise for computational social science in the post API era, it also raises risks of misuse particularly in information operations and targeted advertising underscoring the need for safeguards. We recommend that LLM providers restrict this capability in public facing applications, while preserving controlled access for verified research purposes.

</details>


### [65] [Probing for Arithmetic Errors in Language Models](https://arxiv.org/abs/2507.12379)

*Yucheng Sun, Alessandro Stolfo, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** language models, error detection, internal activations, arithmetic errors, self-correction

**Relevance Score:** 8

**TL;DR:** The paper explores using internal activations in language models to detect arithmetic errors, specifically in 3-digit addition, and demonstrates effective lightweight error detectors for model correctness predictions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how to identify and correct arithmetic errors in language models can enhance their reliability and performance in tasks involving numerical reasoning.

**Method:** The authors utilize probes to decode the model's hidden states for predicting correctness on 3-digit addition and related problems, developing lightweight error detectors that operate with high accuracy.

**Key Contributions:**

	1. Demonstrated effective detection of arithmetic errors in language models using internal activations.
	2. Developed error detectors that achieve over 90% accuracy on a controlled addition task.
	3. Showed generalization of simple arithmetic probes to complex problem-solving scenarios.

**Result:** Lightweight error detectors achieve over 90% accuracy in predicting model correctness and generalize well to more complex problems, providing insights into the internal representations of language models.

**Limitations:** 

**Conclusion:** The study suggests that arithmetic errors can be detected through internal model activations, and probes can effectively guide corrections without heavily disrupting correct outputs.

**Abstract:** We investigate whether internal activations in language models can be used to detect arithmetic errors. Starting with a controlled setting of 3-digit addition, we show that simple probes can accurately decode both the model's predicted output and the correct answer from hidden states, regardless of whether the model's output is correct. Building on this, we train lightweight error detectors that predict model correctness with over 90% accuracy. We then extend our analysis to structured chain-of-thought traces on addition-only GSM8K problems and find that probes trained on simple arithmetic generalize well to this more complex setting, revealing consistent internal representations. Finally, we demonstrate that these probes can guide selective re-prompting of erroneous reasoning steps, improving task accuracy with minimal disruption to correct outputs. Our findings suggest that arithmetic errors can be anticipated from internal activations alone, and that simple probes offer a viable path toward lightweight model self-correction.

</details>


### [66] [Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data](https://arxiv.org/abs/2507.12425)

*Chandana Cheerla*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Enterprise data

**Relevance Score:** 8

**TL;DR:** This paper presents an advanced Retrieval-Augmented Generation (RAG) framework that improves the processing of structured and semi-structured data using hybrid retrieval strategies and metadata-aware filtering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of Large Language Models in handling proprietary enterprise data, including structured and semi-structured formats.

**Method:** The proposed RAG framework employs a combination of dense embeddings and BM25 for retrieval, along with metadata-aware filtering, semantic chunking for coherence, and quantized indexing for efficiency.

**Key Contributions:**

	1. Hybrid retrieval combining dense embeddings and BM25
	2. Metadata-aware filtering with SpaCy NER
	3. Quantized indexing for enhanced retrieval efficiency

**Result:** Experiments indicate a 15% increase in Precision@5, a 13% increase in Recall@5, and a 16% improvement in Mean Reciprocal Rank, alongside qualitative enhancements in Faithfulness, Completeness, and Relevance scores.

**Limitations:** 

**Conclusion:** The framework effectively delivers accurate and contextually relevant responses for enterprise tasks and has potential for future enhancements including multimodal data processing.

**Abstract:** Organizations increasingly rely on proprietary enterprise data, including HR records, structured reports, and tabular documents, for critical decision-making. While Large Language Models (LLMs) have strong generative capabilities, they are limited by static pretraining, short context windows, and challenges in processing heterogeneous data formats. Conventional Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but often struggle with structured and semi-structured data.   This work proposes an advanced RAG framework that combines hybrid retrieval strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by metadata-aware filtering with SpaCy NER and cross-encoder reranking. The framework applies semantic chunking to maintain textual coherence and retains tabular data structures to preserve row-column integrity. Quantized indexing optimizes retrieval efficiency, while human-in-the-loop feedback and conversation memory improve adaptability.   Experiments on enterprise datasets show notable improvements: Precision@5 increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74), and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness (4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale. These results demonstrate the framework's effectiveness in delivering accurate, comprehensive, and contextually relevant responses for enterprise tasks. Future work includes extending to multimodal data and integrating agent-based retrieval. The source code will be released at https://github.com/CheerlaChandana/Enterprise-Chatbot

</details>


### [67] [Can We Predict Alignment Before Models Finish Thinking? Towards Monitoring Misaligned Reasoning Models](https://arxiv.org/abs/2507.12428)

*Yik Siu Chan, Zheng-Xin Yong, Stephen H. Bach*

**Main category:** cs.CL

**Keywords:** chains-of-thought, safety monitoring, language models, predictive analysis, artificial intelligence

**Relevance Score:** 9

**TL;DR:** Investigates the use of chains-of-thought (CoTs) in predicting misalignment in language model responses, showing that model activations are more reliable than text-based methods for safety monitoring.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address alignment risks in open-weights reasoning language models that generate harmful content through chains-of-thought (CoTs).

**Method:** Evaluated various monitoring approaches, including human reviewers, language models, and text classifiers, focusing on CoT texts and activations to predict final response safety.

**Key Contributions:**

	1. Demonstrated that CoT activations are a more reliable predictor of response safety than CoT texts.
	2. Achieved accurate predictions with a lightweight probe even before reasoning completion.
	3. Showed that findings are generalizable across different model sizes and safety benchmarks.

**Result:** A linear probe on CoT activations significantly outperformed text-based methods, providing accurate predictions of response safety even before reasoning completion.

**Limitations:** 

**Conclusion:** Utilizing model latents for monitoring can enable real-time safety interventions during text generation, demonstrating broader applicability across model types and safety metrics.

**Abstract:** Open-weights reasoning language models generate long chains-of-thought (CoTs) before producing a final response, which improves performance but introduces additional alignment risks, with harmful content often appearing in both the CoTs and the final outputs. In this work, we investigate if we can use CoTs to predict final response misalignment. We evaluate a range of monitoring approaches, including humans, highly-capable large language models, and text classifiers, using either CoT text or activations. First, we find that a simple linear probe trained on CoT activations can significantly outperform all text-based methods in predicting whether a final response will be safe or unsafe. CoT texts are often unfaithful and can mislead humans and classifiers, while model latents (i.e., CoT activations) offer a more reliable predictive signal. Second, the probe makes accurate predictions before reasoning completes, achieving strong performance even when applied to early CoT segments. These findings generalize across model sizes, families, and safety benchmarks, suggesting that lightweight probes could enable real-time safety monitoring and early intervention during generation.

</details>


### [68] [S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling](https://arxiv.org/abs/2507.12451)

*Suman Adhya, Debarshi Kumar Sanyal*

**Main category:** cs.CL

**Keywords:** latent representations, topic modeling, variational autoencoder, hyperspherical space, Sliced-Wasserstein

**Relevance Score:** 7

**TL;DR:** The Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM) improves upon variational autoencoder-based neural topic models by addressing posterior collapse through the use of the Spherical Sliced-Wasserstein distance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To resolve the issues of posterior collapse in variational autoencoder-based neural topic models and enhance topic modeling effectiveness in high-dimensional text data.

**Method:** S2WTM employs a latent space with a prior distribution supported on the unit hypersphere and uses the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior.

**Key Contributions:**

	1. Introduction of Spherical Sliced Wasserstein distance in topic modeling
	2. Enhanced coherence and diversity of generated topics
	3. Improved performance on downstream tasks compared to existing models

**Result:** S2WTM demonstrates superior performance compared to state-of-the-art topic models, yielding more coherent and diverse topics and improving performance on subsequent tasks.

**Limitations:** 

**Conclusion:** S2WTM effectively captures hyperspherical structure in the latent space while overcoming challenges associated with posterior collapse, making it a valuable approach for topic modeling.

**Abstract:** Modeling latent representations in a hyperspherical space has proven effective for capturing directional similarities in high-dimensional text data, benefiting topic modeling. Variational autoencoder-based neural topic models (VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical structure. However, VAE-NTMs often suffer from posterior collapse, where the KL divergence term in the objective function highly diminishes, leading to ineffective latent representations. To mitigate this issue while modeling hyperspherical structure in the latent space, we propose the Spherical Sliced Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior distribution supported on the unit hypersphere and leverages the Spherical Sliced-Wasserstein distance to align the aggregated posterior distribution with the prior. Experimental results demonstrate that S2WTM outperforms state-of-the-art topic models, generating more coherent and diverse topics while improving performance on downstream tasks.

</details>


### [69] [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/abs/2507.12466)

*David Mizrahi, Anders Boesen Lindbo Larsen, Jesse Allardice, Suzie Petryk, Yuri Gorokhov, Jeffrey Li, Alex Fang, Josh Gardner, Tom Gunter, Afshin Dehghan*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Machine Learning, Data Selection, Benchmarking, Model Training

**Relevance Score:** 8

**TL;DR:** The paper proposes a method called benchmark-targeted ranking (BETR) for selecting pretraining documents based on their similarity to benchmark training examples, demonstrating significant improvements in model performance and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of making data selection optimization explicit in the context of training machine learning models, contrasting it with benchmark-driven iteration.

**Method:** Benchmark-targeted ranking (BETR) embeds benchmark examples with a sample of pretraining documents in a shared space, scoring the documents by their similarity to benchmarks and training a classifier to predict these scores for the entire corpus.

**Key Contributions:**

	1. Introduction of benchmark-targeted ranking (BETR) for data selection.
	2. Demonstration of a substantial performance improvement across various models and tasks using BETR.
	3. Analysis of the relationship between model size and data filtering requirements.

**Result:** BETR allows for a 2.1x compute multiplier over the DCLM-Baseline and 4.7x over unfiltered data, improving performance on 9 out of 10 tasks across scales.

**Limitations:** The methodology may not generalize perfectly across all domains, and further exploration is needed on the implications of varying benchmark diversity.

**Conclusion:** Directly aligning pretraining data with evaluation benchmarks enhances model capabilities and indicates that selection strategies should adjust with model scale.

**Abstract:** Every data selection method inherently has a target. In practice, these targets often emerge implicitly through benchmark-driven iteration: researchers develop selection strategies, train models, measure benchmark performance, then refine accordingly. This raises a natural question: what happens when we make this optimization explicit? To explore this, we propose benchmark-targeted ranking (BETR), a simple method that selects pretraining documents based on similarity to benchmark training examples. BETR embeds benchmark examples and a sample of pretraining documents in a shared space, scores this sample by similarity to benchmarks, then trains a lightweight classifier to predict these scores for the full corpus. We compare data selection methods by training over 500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to them. From this, we find that simply aligning pretraining data to evaluation benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline (4.7x over unfiltered data) and improves performance on 9 out of 10 tasks across all scales. BETR also generalizes well: when targeting a diverse set of benchmarks disjoint from our evaluation suite, it still matches or outperforms baselines. Our scaling analysis further reveals a clear trend: larger models require less aggressive filtering. Overall, our findings show that directly matching pretraining data to target tasks precisely shapes model capabilities and highlight that optimal selection strategies must adapt to model scale.

</details>


### [70] [RAGGED: Towards Informed Design of Scalable and Stable RAG Systems](https://arxiv.org/abs/2403.09040)

*Jennifer Hsia, Afreen Shaikh, Zhiruo Wang, Graham Neubig*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, RAG, machine learning, evaluation framework, reader robustness

**Relevance Score:** 9

**TL;DR:** RAGGED is a framework for evaluating retrieval-augmented generation systems across various configurations, revealing key insights into reader robustness and system performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges and performance variability of retrieval-augmented generation (RAG) systems and provide a structured approach for evaluation.

**Method:** RAGGED systematically evaluates RAG systems by testing different retriever-reader configurations, retrieval depths, and datasets, focusing on reader robustness to noisy information.

**Key Contributions:**

	1. Introduction of RAGGED framework for evaluating RAG systems
	2. Identification of reader robustness as a key factor for performance stability
	3. Development of new metrics for assessing RAG stability and scalability

**Result:** Analysis of RAG systems indicates that reader robustness is crucial for stability, while the performance impact of retrievers and prompts varies depending on the reader's sensitivity to distraction.

**Limitations:** The framework may not fully account for all potential configurations or external factors affecting RAG performance.

**Conclusion:** RAGGED offers new metrics for assessing RAG system stability and scalability, encouraging optimization of retrieval depths and overall model robustness in future research.

**Abstract:** Retrieval-augmented generation (RAG) enhances language models by integrating external knowledge, but its effectiveness is highly dependent on system configuration. Improper retrieval settings can degrade performance, making RAG less reliable than closed-book generation. In this work, we introduce RAGGED, a framework for systematically evaluating RAG systems across diverse retriever-reader configurations, retrieval depths, and datasets. Our analysis reveals that reader robustness to noise is the key determinant of RAG stability and scalability. Some readers benefit from increased retrieval depth, while others degrade due to their sensitivity to distracting content. Through large-scale experiments on open-domain, multi-hop, and specialized-domain datasets, we show that retrievers, rerankers, and prompts influence performance but do not fundamentally alter these reader-driven trends. By providing a principled framework and new metrics to assess RAG stability and scalability, RAGGED enables systematic evaluation of retrieval-augmented generation systems, guiding future research on optimizing retrieval depth and model robustness.

</details>


### [71] [Protecting Copyrighted Material with Unique Identifiers in Large Language Model Training](https://arxiv.org/abs/2403.15740)

*Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang*

**Main category:** cs.CL

**Keywords:** membership inference, large language models, copyright, ghost sentences, unique identifiers

**Relevance Score:** 8

**TL;DR:** The paper proposes a user-friendly method for detecting copyright membership in large language models using unique identifiers embedded in text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns about copyright infringement and membership inference in large language models, with a focus on simplifying the process for end users.

**Method:** An insert-and-detect methodology utilizing unique identifiers in copyrighted text and introducing methods like ghost sentences and a last-k words test for user engagement.

**Key Contributions:**

	1. Introduction of ghost sentences for membership inference
	2. Development of a last-k words test for user engagement
	3. Comprehensive study on factors affecting memorization and inference in LLMs

**Result:** The study demonstrates the effectiveness of ghost sentences for membership inference, incorporating factors like training data scales and model sizes.

**Limitations:** 

**Conclusion:** The proposed methods provide a feasible approach for users to check copyright membership and show applicability in real scenarios.

**Abstract:** A primary concern regarding training large language models (LLMs) is whether they abuse copyrighted online text. With the increasing training data scale and the prevalence of LLMs in daily lives, two problems arise: \textbf{1)} false positive membership inference results misled by similar examples; \textbf{2)} membership inference methods are usually too complex for end users to understand and use. To address these issues, we propose an alternative \textit{insert-and-detect} methodology, advocating that web users and content platforms employ \textbf{\textit{unique identifiers}} for reliable and independent membership inference. Users and platforms can create their identifiers, embed them in copyrighted text, and independently detect them in future LLMs. As an initial demonstration, we introduce \textit{\textbf{ghost sentences}} and a user-friendly last-$k$ words test, allowing end users to chat with LLMs for membership inference. Ghost sentences consist primarily of unique passphrases of random natural words, which can come with customized elements to bypass possible filter rules. The last-$k$ words test requires a significant repetition time of ghost sentences~($\ge10$). For cases with fewer repetitions, we designed an extra perplexity test, as LLMs exhibit high perplexity when encountering unnatural passphrases. We also conduct a comprehensive study on the memorization and membership inference of ghost sentences, examining factors such as training data scales, model sizes, repetition times, insertion positions, wordlist of passphrases, alignment, \textit{etc}. Our study shows the possibility of applying ghost sentences in real scenarios and provides instructions for the potential application.

</details>


### [72] [Linearly-Interpretable Concept Embedding Models for Text Analysis](https://arxiv.org/abs/2406.14335)

*Francesco De Santis, Philippe Bich, Gabriele Ciravegna, Pietro Barbiero, Danilo Giordano, Tania Cerquitelli*

**Main category:** cs.CL

**Keywords:** Large-Language Models, interpretability, Concept-Bottleneck Models, Linearly Interpretable Concept Embedding Model, health informatics

**Relevance Score:** 8

**TL;DR:** This paper introduces the Linearly Interpretable Concept Embedding Model (LICEM), which improves upon current interpretable models while aligning accuracy with interpretability and reducing the need for extensive annotations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better interpretability in Large-Language Models (LLMs) and the limitations of existing post-hoc interpretation methods.

**Method:** The authors propose LICEM, which offers improved classification accuracy compared to existing interpretable models and maintains consistency and interveneability in its explanations.

**Key Contributions:**

	1. Introduction of LICEM as a novel model for interpretable predictions
	2. Improved classification accuracy that matches black-box models
	3. Automatic prediction of concepts without extensive annotations

**Result:** LICEM achieves better classification accuracy than both existing interpretable models and black-box models, while providing explanations that are more interveneable and causally consistent.

**Limitations:** LICEM remains bound by certain architectural constraints and may still require further validation in diverse real-world applications.

**Conclusion:** LICEM can be trained without concept supervision, predicting concepts automatically using an LLM backbone, thus overcoming the annotation challenges in real-world text data.

**Abstract:** Despite their success, Large-Language Models (LLMs) still face criticism due to their lack of interpretability. Traditional post-hoc interpretation methods, based on attention and gradient-based analysis, offer limited insights as they only approximate the model's decision-making processes and have been proved to be unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been lately proposed in the textual field to provide interpretable predictions based on human-understandable concepts. However, CBMs still exhibit several limitations due to their architectural constraints limiting their expressivity, to the absence of task-interpretability when employing non-linear task predictors and for requiring extensive annotations that are impractical for real-world text data. In this paper, we address these challenges by proposing a novel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the current accuracy-interpretability trade-off. LICEMs classification accuracy is better than existing interpretable models and matches black-box ones. We show that the explanations provided by our models are more interveneable and causally consistent with respect to existing solutions. Finally, we show that LICEMs can be trained without requiring any concept supervision, as concepts can be automatically predicted when using an LLM backbone.

</details>


### [73] [Understanding Language Model Circuits through Knowledge Editing](https://arxiv.org/abs/2406.17241)

*Huaizhi Ge, Frank Rudzicz, Zining Zhu*

**Main category:** cs.CL

**Keywords:** language models, GPT-2, knowledge representation, interpretability, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates the structure and function of critical subnetworks, or circuits, in the GPT-2 language model through knowledge editing experiments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how knowledge is organized and represented within the critical subnetworks of language models.

**Method:** Conducted systematic knowledge editing experiments on the circuits of the GPT-2 language model.

**Key Contributions:**

	1. Introduces insights into knowledge organization within language model circuits
	2. Identifies response patterns of circuits to knowledge editing
	3. Provides directions for future interpretability research in language models

**Result:** Revealed patterns in the circuits' responses to edits, knowledge distribution across components, and the architectural composition of knowledge-bearing circuits.

**Limitations:** 

**Conclusion:** The findings deepen understanding of the complex relationship between model circuits and knowledge representation, providing insights for future interpretability and safety research.

**Abstract:** Recent advances in language model interpretability have identified circuits, critical subnetworks that replicate model behaviors, yet how knowledge is structured within these crucial subnetworks remains opaque. To gain an understanding toward the knowledge in the circuits, we conduct systematic knowledge editing experiments on the circuits of the GPT-2 language model. Our analysis reveals intriguing patterns in how circuits respond to editing attempts, the extent of knowledge distribution across network components, and the architectural composition of knowledge-bearing circuits. These findings offer insights into the complex relationship between model circuits and knowledge representation, deepening the understanding of how information is organized within language models. Our findings offer novel insights into the ``meanings'' of the circuits, and introduce directions for further interpretability and safety research of language models.

</details>


### [74] [How Well Can Knowledge Edit Methods Edit Perplexing Knowledge?](https://arxiv.org/abs/2406.17253)

*Huaizhi Ge, Frank Rudzicz, Zining Zhu*

**Main category:** cs.CL

**Keywords:** large language models, knowledge editing, perplexingness, hierarchical relationships, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the challenges of updating knowledge in large language models (LLMs) using the concept of 'perplexingness', which reflects how conflicting new information is with established conceptual hierarchies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** As large language models (LLMs) struggle with post-training knowledge updates, understanding the factors that affect the editing process is critical for improving their adaptability and accuracy.

**Method:** The authors introduce a concept called perplexingness and create the HierarchyData dataset, which consists of 99 hyponym-hypernym pairs. They conduct controlled experiments to analyze the relationship between perplexingness and the effectiveness of knowledge editing across different models and methods.

**Key Contributions:**

	1. Introduction of the concept of perplexingness as it relates to knowledge editing in LLMs.
	2. Development of HierarchyData, a novel dataset for studying hyponym-hypernym relationships.
	3. Empirical analysis showing the negative correlation between perplexingness and editing effectiveness.

**Result:** The study finds a strong negative correlation between perplexingness and knowledge editing effectiveness, indicating that edits that conflict more with known hierarchies are harder to implement.

**Limitations:** The study is based on a limited dataset and further research is needed to generalize findings across additional categories and models.

**Conclusion:** Knowledge editing in LLMs is fundamentally challenging as conflicting new information becomes increasingly resistant to effective modification, particularly when it involves higher-level abstract concepts.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities, but updating their knowledge post-training remains a critical challenge. While recent model editing techniques like Rank-One Model Editing (ROME) show promise, their effectiveness may vary based on the nature of the knowledge being edited. We introduce the concept of ``perplexingness'': the degree to which new knowledge conflicts with an LLM's learned conceptual hierarchies and categorical relationships. For instance, editing ``British Shorthair is a kind of cat'' to ``British Shorthair is a kind of dog'' represents a low-perplexingness edit within the same taxonomic level, while editing ``A cat is a kind of animal'' to ``A cat is a kind of plant'' represents a high-perplexingness edit that violates fundamental categorical boundaries. To systematically investigate this phenomenon, we introduce HierarchyData, a carefully curated dataset of 99 hyponym-hypernym pairs across diverse categories. Through controlled experiments across three models and four editing methods, we demonstrate a strong negative correlation between the perplexingness of new knowledge and the effectiveness of knowledge editing. Our analysis reveals that edits involving more abstract concepts (hypernyms) generally exhibit higher perplexingness and are more resistant to modification than their specific counterparts (hyponyms). These findings highlight a fundamental challenge in LLM knowledge editing: the more a new fact contradicts an LLM's learned conceptual hierarchies, the harder it becomes to reliably encode that knowledge.

</details>


### [75] [Measuring Spiritual Values and Bias of Large Language Models](https://arxiv.org/abs/2410.11647)

*Songyuan Liu, Ziyang Zhang, Runze Yan, Wei Wu, Carl Yang, Jiaying Lu*

**Main category:** cs.CL

**Keywords:** large language models, spiritual values, bias, hate speech, pre-training

**Relevance Score:** 7

**TL;DR:** This paper examines the influence of spiritual values on large language models (LLMs) and proposes continuing their pre-training on spiritual texts to mitigate biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how the spiritual values reflected in the training data of LLMs affect their behavior and sensitivity in social-fairness scenarios, particularly in hate speech identification.

**Method:** Experimental testing of LLMs' spiritual values and their impact on sensitivity to different hate target groups, followed by a proposal for continuing pre-training with spiritual texts.

**Key Contributions:**

	1. Verification of diverse spiritual values in LLMs
	2. Analysis of LLM sensitivity in social-fairness scenarios
	3. Proposing continued pre-training on spiritual texts to reduce biases

**Result:** Results show that LLMs embody diverse spiritual values and their sensitivity to hate speech varies according to these values. Continuing pre-training on spiritual texts improves LLM performance in mitigating spiritual biases.

**Limitations:** The study may not cover all potential biases present in LLMs and focuses specifically on spiritual values.

**Conclusion:** The study highlights the significance of spiritual values in shaping LLM behavior and suggests a methodology to address associated biases through targeted pre-training.

**Abstract:** Large language models (LLMs) have become integral tool for users from various backgrounds. LLMs, trained on vast corpora, reflect the linguistic and cultural nuances embedded in their pre-training data. However, the values and perspectives inherent in this data can influence the behavior of LLMs, leading to potential biases. As a result, the use of LLMs in contexts involving spiritual or moral values necessitates careful consideration of these underlying biases. Our work starts with verification of our hypothesis by testing the spiritual values of popular LLMs. Experimental results show that LLMs' spiritual values are quite diverse, as opposed to the stereotype of atheists or secularists. We then investigate how different spiritual values affect LLMs in social-fairness scenarios e.g., hate speech identification). Our findings reveal that different spiritual values indeed lead to different sensitivity to different hate target groups. Furthermore, we propose to continue pre-training LLMs on spiritual texts, and empirical results demonstrate the effectiveness of this approach in mitigating spiritual bias.

</details>


### [76] [Rolling the DICE on Idiomaticity: How LLMs Fail to Grasp Context](https://arxiv.org/abs/2410.16069)

*Maggie Mi, Aline Villavicencio, Nafise Sadat Moosavi*

**Main category:** cs.CL

**Keywords:** idiomaticity, LLMs, contextual understanding, natural language processing, collocational frequency

**Relevance Score:** 7

**TL;DR:** This paper investigates LLMs' ability to disambiguate idiomatic meanings using a new controlled dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To test whether LLMs can effectively use context and collocational frequency to resolve idiomaticity in language.

**Method:** A controlled contrastive dataset was constructed for evaluating LLM performance on idiomaticity detection tasks.

**Key Contributions:**

	1. Introduction of a novel controlled dataset for testing idiomaticity in LLMs.
	2. Analysis of the impact of contextual understanding and collocational frequency on model performance.
	3. Public release of the code and dataset for further research.

**Result:** LLMs often struggle to resolve idiomaticity when contextual understanding is necessary; performance improves with higher sentence likelihood and collocational frequency.

**Limitations:** The study may not fully account for all variables influencing idiomaticity resolution beyond context and frequency.

**Conclusion:** The findings indicate significant limitations in LLMs' contextual processing abilities concerning idioms, highlighting the need for better dataset designs.

**Abstract:** Human processing of idioms relies on understanding the contextual sentences in which idioms occur, as well as language-intrinsic features such as frequency and speaker-intrinsic factors like familiarity. While LLMs have shown high performance on idiomaticity detection tasks, this success may be attributed to reasoning shortcuts in existing datasets. To this end, we construct a novel, controlled contrastive dataset designed to test whether LLMs can effectively use context to disambiguate idiomatic meaning. Additionally, we explore how collocational frequency and sentence probability influence model performance. Our findings reveal that LLMs often fail to resolve idiomaticity when it is required to attend to the surrounding context, and that models perform better on sentences that have higher likelihood. The collocational frequency of expressions also impacts performance. We make our code and dataset publicly available.

</details>


### [77] [TRIM: Token Reduction and Inference Modeling for Cost-Effective Language Generation](https://arxiv.org/abs/2412.07682)

*Alfredo Garrachn Ruiz, Toms de la Rosa, Daniel Borrajo*

**Main category:** cs.CL

**Keywords:** Large Language Models, inference cost, distilled outputs, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** TRIM reduces the inference cost of Large Language Models by generating distilled outputs that maintain meaning, then reconstructing them with a smaller model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational demands of LLMs, especially for long-output tasks, by leveraging natural language redundancy for optimization.

**Method:** A pipeline called TRIM generates shorter, distilled outputs from LLMs and reconstructs them into full narratives using a smaller, more efficient model.

**Key Contributions:**

	1. Introduction of TRIM pipeline for LLM optimization
	2. Empirical evidence of token savings
	3. Demonstration of maintaining quality in outputs

**Result:** The approach saved 20.58% in token usage on average while maintaining comparable evaluation metrics in general knowledge tasks.

**Limitations:** 

**Conclusion:** This method shows potential for balancing efficiency and accuracy in language processing applications.

**Abstract:** The inference cost of Large Language Models (LLMs) is a significant challenge due to their computational demands, specially on tasks requiring long outputs. However, natural language often contains redundancy, which presents an opportunity for optimization. We have observed that LLMs can generate distilled language-concise outputs that retain essential meaning, when prompted appropriately. We propose TRIM, a pipeline for saving computational cost in which a shorter distilled output from the LLM is reconstructed into a full narrative by a smaller model with lower inference costs. Our experiments show promising results, particularly in general knowledge domains with 20.58% saved tokens on average with tiny decrease in evaluation metrics, hinting that this approach can effectively balance efficiency and accuracy in language processing tasks.

</details>


### [78] [Labels Generated by Large Language Models Help Measure People's Empathy in Vitro](https://arxiv.org/abs/2501.00691)

*Md Rakibul Hasan, Yue Yao, Md Zakir Hossain, Aneesh Krishna, Imre Rudas, Shafin Rahman, Tom Gedeon*

**Main category:** cs.CL

**Keywords:** Large language models, Empathy computing, Noisy label correction, Data augmentation, Psychology

**Relevance Score:** 9

**TL;DR:** This paper investigates the use of LLM-generated labels to enhance supervised learning in empathy computing, focusing on noisy label correction and data augmentation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues with noisy labels in crowdsourced datasets for empathy computing and improve the accuracy of supervised models.

**Method:** The paper employs two strategies: correcting noisy labels and augmenting training data, using LLMs to generate improved labels for training a RoBERTa model.

**Key Contributions:**

	1. Introduction of LLM-generated labels for noisy label correction
	2. Demonstration of significant accuracy improvement in empathy computing
	3. Analysis of evaluation metrics and demographic biases in model development

**Result:** Replacing or supplementing crowdsourced labels with LLM-generated labels resulted in significant accuracy improvements, achieving a Pearson correlation coefficient of 0.648 on the NewsEmp benchmarks.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs can effectively enhance model training in empathy computing and highlight the need for careful evaluation metric selection due to demographic biases.

**Abstract:** Large language models (LLMs) have revolutionised many fields, with LLM-as-a-service (LLMSaaS) offering accessible, general-purpose solutions without costly task-specific training. In contrast to the widely studied prompt engineering for directly solving tasks (in vivo), this paper explores LLMs' potential for in-vitro applications: using LLM-generated labels to improve supervised training of mainstream models. We examine two strategies - (1) noisy label correction and (2) training data augmentation - in empathy computing, an emerging task to predict psychology-based questionnaire outcomes from inputs like textual narratives. Crowdsourced datasets in this domain often suffer from noisy labels that misrepresent underlying empathy. We show that replacing or supplementing these crowdsourced labels with LLM-generated labels, developed using psychology-based scale-aware prompts, achieves statistically significant accuracy improvements. Notably, the RoBERTa pre-trained language model (PLM) trained with noise-reduced labels yields a state-of-the-art Pearson correlation coefficient of 0.648 on the public NewsEmp benchmarks. This paper further analyses evaluation metric selection and demographic biases to help guide the future development of more equitable empathy computing models. Code and LLM-generated labels are available at https://github.com/hasan-rakibul/LLMPathy.

</details>


### [79] [Learning an Effective Premise Retrieval Model for Efficient Mathematical Formalization](https://arxiv.org/abs/2501.13959)

*Yicheng Tao, Haotian Liu, Shanwen Wang, Hongteng Xu*

**Main category:** cs.CL

**Keywords:** premise retrieval, mathematical formalization, contrastive learning, latent space embeddings, open-source search engine

**Relevance Score:** 5

**TL;DR:** Introduces a novel premise retrieval model that improves accuracy and efficiency using data from Mathlib, addressing challenges in mathematical formalization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist inexperienced users in mathematical formalization through improved premise retrieval methods that do not require high mathematical expertise.

**Method:** The model utilizes a lightweight architecture that embeds queries and premises in a latent space, employing a specifically trained tokenizer on formal corpora and a contrastive learning framework with enhanced similarity calculations and re-ranking.

**Key Contributions:**

	1. Introduction of a lightweight premise retrieval model for formalized mathematics.
	2. Utilization of a tokenizer trained on formal corpora to improve query embedding.
	3. Enhanced retrieval performance through contrastive learning and re-ranking.

**Result:** The proposed model outperforms existing baselines, achieving higher accuracy with a lower computational load in retrieving mathematical premises.

**Limitations:** 

**Conclusion:** The introduction of this retrieval model can significantly enhance the ease of access to formalized mathematics for users lacking advanced expertise, and it is supported by an open-source search engine.

**Abstract:** Formalized mathematics has recently garnered significant attention for its ability to assist mathematicians across various fields. Premise retrieval, as a common step in mathematical formalization, has been a challenge, particularly for inexperienced users. Existing retrieval methods that facilitate natural language queries require a certain level of mathematical expertise from users, while approaches based on formal languages (e.g., Lean) typically struggle with the scarcity of training data, hindering the training of effective and generalizable retrieval models. In this work, we introduce a novel method that leverages data extracted from Mathlib to train a lightweight and effective premise retrieval model. In particular, the proposed model embeds queries (i.e., proof state provided by Lean) and premises in a latent space, featuring a tokenizer specifically trained on formal corpora. The model is learned in a contrastive learning framework, in which a fine-grained similarity calculation method and a re-ranking module are applied to enhance the retrieval performance. Experimental results demonstrate that our model outperforms existing baselines, achieving higher accuracy while maintaining a lower computational load. We have released an open-source search engine based on our retrieval model at https://premise-search.com/. The source code and the trained model can be found at https://github.com/ruc-ai4math/Premise-Retrieval.

</details>


### [80] [Flexible and Efficient Grammar-Constrained Decoding](https://arxiv.org/abs/2502.05111)

*Kanghee Park, Timothy Zhou, Loris D'Antoni*

**Main category:** cs.CL

**Keywords:** Grammar-constrained decoding, Large Language Models, context-free grammar, token masking, NLP

**Relevance Score:** 8

**TL;DR:** This paper presents a new Grammar-constrained decoding (GCD) algorithm that improves offline preprocessing speed for generating structured outputs from Large Language Models (LLMs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing GCD algorithms are slow and require significant time for preprocessing common grammars, which hinders their practical applications.

**Method:** The proposed GCD algorithm achieves 17.71x faster offline preprocessing by optimizing how the LLM subword tokenizer aligns with context-free grammar tokens and efficiently computes token masks.

**Key Contributions:**

	1. Development of a fast GCD algorithm
	2. 17.71x speed improvement in offline preprocessing
	3. State-of-the-art efficiency in online mask computation

**Result:** The new algorithm preserves state-of-the-art efficiency in online mask computation while significantly speeding up the preprocessing phase.

**Limitations:** 

**Conclusion:** The advancements in the new GCD algorithm make it more practical for use in generating structured outputs while adhering to precise syntactic rules.

**Abstract:** Large Language Models (LLMs) are often asked to generate structured outputs that obey precise syntactic rules, such as code snippets or formatted data. Grammar-constrained decoding (GCD) can guarantee that LLM outputs matches such rules by masking out tokens that will provably lead to outputs that do not belong to a specified context-free grammar (CFG). To guarantee soundness, GCD algorithms have to compute how a given LLM subword tokenizer can align with the tokens used   by a given context-free grammar and compute token masks based on this information. Doing so efficiently is challenging and existing GCD algorithms require tens of minutes to preprocess common grammars. We present a new GCD algorithm together with an implementation that offers 17.71x faster offline preprocessing than existing approaches while preserving state-of-the-art efficiency in online mask computation.

</details>


### [81] [DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling](https://arxiv.org/abs/2502.11078)

*Aili Chen, Chengyu Du, Jiangjie Chen, Jinghan Xu, Yikai Zhang, Siyu Yuan, Zulong Chen, Liangyue Li, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** dynamic persona modeling, large language models, reinforcement learning, user behavior prediction, personalized applications

**Relevance Score:** 9

**TL;DR:** DEEPER is a novel approach for dynamic persona modeling using LLMs, focusing on continual optimization through reinforcement learning to improve user behavior prediction accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better persona modeling in personalized applications like recommendation systems and user behavior prediction due to limitations in existing methods.

**Method:** DEEPER employs an iterative reinforcement learning framework to enhance the model's ability to identify effective update directions for persona optimization based on streaming behavior data.

**Key Contributions:**

	1. Introduction of a novel iterative reinforcement learning framework for persona optimization
	2. Demonstrated significant reduction in prediction error compared to existing methods
	3. Validated across a diverse dataset involving 4800 users in 10 domains

**Result:** The proposed approach significantly reduced user behavior prediction error by 32.2% over four update rounds compared to existing baselines.

**Limitations:** 

**Conclusion:** DEEPER demonstrates a robust method for ongoing persona optimization, surpassing traditional methods in accuracy and effectiveness.

**Abstract:** To advance personalized applications such as recommendation systems and user behavior prediction, recent research increasingly adopts large language models (LLMs) for human -readable persona modeling. In dynamic real -world scenarios, effective persona modeling necessitates leveraging streaming behavior data to continually optimize user personas. However, existing methods -whether regenerating personas or incrementally extending them with new behaviors -often fail to achieve sustained improvements in persona quality or future behavior prediction accuracy. To address this, we propose DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization. Specifically, we enhance the model's direction -search capability through an iterative reinforcement learning framework, allowing it to automatically identify effective update directions and optimize personas using discrepancies between user behaviors and model predictions. Extensive experiments on dynamic persona modeling involving 4800 users across 10 domains highlight the superior persona optimization capabilities of DEEPER, delivering an impressive 32.2% average reduction in user behavior prediction error over four update rounds -outperforming the best baseline by a remarkable 22.92%.

</details>


### [82] [Resona: Improving Context Copying in Linear Recurrence Models with Retrieval](https://arxiv.org/abs/2503.22913)

*Xinyu Wang, Linrui Ma, Jerry Huang, Peng Lu, Prasanna Parthasarathi, Xiao-Wen Chang, Boxing Chen, Yufei Cui*

**Main category:** cs.CL

**Keywords:** large language models, linear recurrent models, retrieval-augmented modelling, natural language processing, in-context learning

**Relevance Score:** 8

**TL;DR:** Introducing Resona, a framework that enhances linear recurrent models with retrieval capabilities to improve their performance on language tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gap between linear recurrent models and Transformers in tasks requiring in-context learning.

**Method:** Resona augments linear recurrent models with the ability to retrieve and integrate information from input contexts, enabling better task adaptability.

**Key Contributions:**

	1. Development of the Resona framework for linear recurrent models
	2. Demonstrated significant performance gains on various tasks
	3. Provides a novel approach to augmenting LLM capabilities

**Result:** Experiments reveal substantial performance improvements in linear recurrent models on various synthetic and real-world natural language tasks after applying the Resona framework.

**Limitations:** 

**Conclusion:** Resona serves as a general-purpose method for enhancing in-context learning and language modeling in linear recurrent LLMs.

**Abstract:** Recent shifts in the space of large language model (LLM) research have shown an increasing focus on novel architectures to compete with prototypical Transformer-based models that have long dominated this space. Linear recurrent models have proven to be a viable competitor due to their computational efficiency. However, such models still demonstrate a sizable gap compared to Transformers in terms of in-context learning among other tasks that require recalling information from a context. In this work, we introduce Resona, a simple and scalable framework for augmenting linear recurrent models with retrieval. Resona augments models with the ability to integrate retrieved information from the provided input context, enabling tailored behavior to diverse task requirements. Experiments on a variety of linear recurrent models demonstrate that Resona-augmented models observe significant performance gains on a variety of synthetic as well as real-world natural language tasks, highlighting its ability to act as a general purpose method to improve the in-context learning and language modeling abilities of linear recurrent LLMs.

</details>


### [83] [Semantic Adapter for Universal Text Embeddings: Diagnosing and Mitigating Negation Blindness to Enhance Universality](https://arxiv.org/abs/2504.00584)

*Hongliu Cao*

**Main category:** cs.CL

**Keywords:** negation awareness, universal text embeddings, natural language processing, embedding re-weighting, large language models

**Relevance Score:** 8

**TL;DR:** This study analyzes the negation awareness of universal text embedding models, proposing an efficient method to enhance their understanding of negation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the negation awareness of universal text embeddings, which is underexplored in existing literature, especially in light of prior studies revealing challenges in contextual embedding models.

**Method:** An in-depth analysis is conducted on various cutting-edge universal text embedding models to assess their ability to understand negation, followed by the proposal of a data-efficient and computational-efficient embedding re-weighting method.

**Key Contributions:**

	1. In-depth analysis of negation awareness in universal text embeddings.
	2. Development of a novel embedding re-weighting method to improve negation awareness.
	3. Demonstration of improved performance in negation tasks for language models.

**Result:** The findings indicate a significant lack of negation awareness in universal text embedding models, necessitating a re-weighting approach that significantly improves their performance on negation understanding tasks.

**Limitations:** The analysis primarily focuses on the awareness of negation, which may not account for other semantic aspects of natural language understanding.

**Conclusion:** The proposed embedding re-weighting method enhances negation awareness in text embedding models without altering their parameters and improves performance on various tasks involving negation.

**Abstract:** Negation plays an important role in various natural language processing tasks such as Natural Language Inference and Sentiment Analysis tasks. Numerous prior studies have found that contextual text embedding models such as BERT, ELMO, RoBERTa or XLNet face challenges in accurately understanding negation. Recent advancements in universal text embeddings have demonstrated superior performance over contextual text embeddings in various tasks. However, due to the bias in popular evaluation benchmarks, the negation awareness capacity of these models remains unclear. To bridge the gap in existing literature, an in-depth analysis is initiated in this work to study the negation awareness of cutting-edge universal text embedding models. Our findings reveal a significant lack of negation awareness in these models, often interpreting negated text pairs as semantically similar. To efficiently deal with the conflict that different tasks need different trade-offs between topic and negation information among other semantic information, a data-efficient and computational-efficient embedding re-weighting method is proposed without modifying the parameters of text embedding models. The proposed solution is able to improve text embedding models' negation awareness significantly on both simple negation understanding task and complex negation understanding task. Furthermore, the proposed solution can also significantly improve the negation awareness of Large Language Model based task-specific high dimensional universal text embeddings.

</details>


### [84] [Multi-domain Multilingual Sentiment Analysis in Industry: Predicting Aspect-based Opinion Quadruples](https://arxiv.org/abs/2505.10389)

*Benjamin White, Anastasia Shimorina*

**Main category:** cs.CL

**Keywords:** aspect-based sentiment analysis, large language models, multi-domain, sentiment extraction, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper presents a multi-domain aspect-based sentiment analysis system using LLMs, achieving performance comparable to single-domain models while simplifying operational complexity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve aspect-based sentiment analysis by utilizing LLMs to extract sentiment-related information across various domains and languages.

**Method:** We fine-tuned a single model on multiple domain-specific taxonomies to test its effectiveness in extracting aspect categories, sentiment polarity, targets, and opinion expressions.

**Key Contributions:**

	1. Development of a multi-domain aspect-based sentiment analysis system using LLMs
	2. Demonstration of comparable performance to single-domain models
	3. Insights into handling non-extractive predictions and evaluating model failures.

**Result:** The multi-domain model demonstrated performance on par with specialized single-domain models, effectively reducing operational complexity.

**Limitations:** 

**Conclusion:** The work shows promise for using LLMs in structured prediction tasks across diverse domains, highlighting strategies for handling non-extractive predictions and evaluation of failure modes.

**Abstract:** This paper explores the design of an aspect-based sentiment analysis system using large language models (LLMs) for real-world use. We focus on quadruple opinion extraction -- identifying aspect categories, sentiment polarity, targets, and opinion expressions from text data across different domains and languages. We investigate whether a single fine-tuned model can effectively handle multiple domain-specific taxonomies simultaneously. We demonstrate that a combined multi-domain model achieves performance comparable to specialized single-domain models while reducing operational complexity. We also share lessons learned for handling non-extractive predictions and evaluating various failure modes when developing LLM-based systems for structured prediction tasks.

</details>
