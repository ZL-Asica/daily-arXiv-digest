# 2025-08-11

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 20]

- [cs.CL](#cs.CL) [Total: 72]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Automated Visualization Makeovers with LLMs](https://arxiv.org/abs/2508.05637)

*Siddharth Gangwar, David A. Selby, Sebastian J. Vollmer*

**Main category:** cs.HC

**Keywords:** data visualization, large language models, human-computer interaction, visualization critique, best practices

**Relevance Score:** 8

**TL;DR:** This paper explores using multi-modal large language models (LLMs) to provide constructive criticism on data visualizations to improve them based on best practices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of training in effective data visualization within the data science curriculum and enhance the quality of visual representations of data.

**Method:** The study utilizes a pre-trained LLM, guided by user-defined visualization guidelines, to analyze and critique existing plots, aiding users in improving their charts.

**Key Contributions:**

	1. Utilization of LLMs for constructive visualization critique
	2. Focus on improving existing visualizations rather than generating new ones
	3. Development of a self-hosted applet for practical use

**Result:** The LLM demonstrated sensitivity to various plotting issues across different chart types, successfully providing constructive feedback for improvement.

**Limitations:** 

**Conclusion:** The developed tool serves to educate users on best practices in data visualization and is made available as a user-friendly self-hosted applet.

**Abstract:** Making a good graphic that accurately and efficiently conveys the desired message to the audience is both an art and a science, typically not taught in the data science curriculum. Visualisation makeovers are exercises where the community exchange feedback to improve charts and data visualizations. Can multi-modal large language models (LLMs) emulate this task? Given a plot in the form of an image file, or the code used to generate it, an LLM, primed with a list of visualization best practices, is employed to semi-automatically generate constructive criticism to produce a better plot. Our system is centred around prompt engineering of a pre-trained model, relying on a combination of userspecified guidelines and any latent knowledge of data visualization practices that might lie within an LLMs training corpus. Unlike other works, the focus is not on generating valid visualization scripts from raw data or prompts, but on educating the user how to improve their existing data visualizations according to an interpretation of best practices. A quantitative evaluation is performed to measure the sensitivity of the LLM agent to various plotting issues across different chart types. We make the tool available as a simple self-hosted applet with an accessible Web interface.

</details>


### [2] [A Humanoid Social Robot as a Teaching Assistant in the Classroom](https://arxiv.org/abs/2508.05646)

*Thomas Sievers*

**Main category:** cs.HC

**Keywords:** Child-Robot Interaction, Education Technology, Social Robots, Machine Learning, High School

**Relevance Score:** 7

**TL;DR:** The study explores the use of the social robot Pepper, powered by ChatGPT, in high school education, assessing its impact on learning and student perception.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the integration of child-robot interaction in educational settings to support teachers and enhance the learning experience.

**Method:** The research was conducted in a high school classroom where the social robot Pepper was utilized to present learning material to students. Their acceptance and perceived usefulness of the robot were evaluated through feedback.

**Key Contributions:**

	1. Demonstrating the feasibility of using social robots in education
	2. Evaluating student acceptance of robot-assisted learning
	3. Highlighting the role of LLMs in educational robotics

**Result:** Students generally found the robot's presentation of the learning content to be appropriate and felt that the use of the robot was beneficial.

**Limitations:** 

**Conclusion:** The integration of social robots like Pepper in classrooms can potentially enhance teaching methods and student engagement.

**Abstract:** Although innovation and the support of new technologies are much needed to ease the burden on the education system, social robots in schools to help teachers with educational tasks are rare. Child-Robot Interaction (CRI) could support teachers and add an embodied social component to modern multi-modal and multi-sensory learning environments already in use. The social robot Pepper, connected to the Large Language Model (LLM) ChatGPT, was used in a high school classroom to teach new learning content to groups of students. I tested the technical possibilities with the robot on site and asked the students about their acceptance and perceived usefulness of teaching with the help of a social robot. All participants felt that the robot's presentation of the learning material was appropriate or at least partially appropriate and that its use made sense.

</details>


### [3] [Modeling Interactive Narrative Systems: A Formal Approach](https://arxiv.org/abs/2508.05653)

*Jules Clerc, Domitile Lourdeaux, Mohamed Sallak, Johann Barbier, Marc Ravaine*

**Main category:** cs.HC

**Keywords:** Interactive Narrative Systems, formal representation, storytelling, evaluation, collaboration

**Relevance Score:** 4

**TL;DR:** This paper introduces a formal representation framework for Interactive Narrative Systems (INS) to address challenges in fragmented research and system representations, facilitating analysis and evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome challenges in fragmented research within Interactive Narrative Systems and to improve coherence in their representation.

**Method:** The paper proposes a formal representation framework inspired by various existing approaches, providing a consistent vocabulary and modeling structure for analyzing and comparing INS.

**Key Contributions:**

	1. Introduction of a formal representation framework for INS
	2. Provision of a consistent vocabulary and modeling structure
	3. Empirical validation of the framework on a narrative scenario

**Result:** Experimental results on the 'Little Red Riding Hood' scenario demonstrate the framework's usefulness in enhancing the evaluation of INS.

**Limitations:** 

**Conclusion:** The proposed methodology aims to foster collaboration within the INS community and improve the coherence and analysis of interactive narratives.

**Abstract:** Interactive Narrative Systems (INS) have revolutionized digital experiences by empowering users to actively shape their stories, diverging from traditional passive storytelling. However, the field faces challenges due to fragmented research efforts and diverse system representations. This paper introduces a formal representation framework for INS, inspired by diverse approaches from the state of the art. By providing a consistent vocabulary and modeling structure, the framework facilitates the analysis, the description and comparison of INS properties. Experimental validations on the "Little Red Riding Hood" scenario highlight the usefulness of the proposed formalism and its impact on improving the evaluation of INS. This work aims to foster collaboration and coherence within the INS research community by proposing a methodology for formally representing these systems.

</details>


### [4] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)

*Stefan Pasch, Min Chul Cha*

**Main category:** cs.HC

**Keywords:** Ethical AI, User Satisfaction, Transformers, Sentiment Analysis, User Reviews

**Relevance Score:** 7

**TL;DR:** This study examines the connection between ethical AI principles and user satisfaction by analyzing over 100,000 AI product reviews, revealing that ethical considerations affect satisfaction variably across user types.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates whether ethical principles in AI, such as fairness and transparency, are recognized and valued by users, addressing the lack of empirical evidence in this area.

**Method:** The analysis utilizes transformer-based language models to assess sentiment across seven ethical dimensions defined by the EU Ethics Guidelines for Trustworthy AI, based on over 100,000 user reviews from G2.

**Key Contributions:**

	1. Analysis of over 100,000 user reviews to link ethical AI principles and user satisfaction
	2. Identification of varying impacts of ethical dimensions based on user and product type
	3. Highlighting the importance of ethical AI design in user-centric contexts

**Result:** Findings indicate all seven ethical dimensions are positively correlated with user satisfaction, with variations depending on user type, emphasizing the stronger impact on non-technical users and end-user applications.

**Limitations:** 

**Conclusion:** The results stress the importance of designing ethical AI from the user's perspective and highlight the need to consider different user roles and product types in ethical AI frameworks.

**Abstract:** As AI systems become increasingly embedded in organizational workflows and consumer applications, ethical principles such as fairness, transparency, and robustness have been widely endorsed in policy and industry guidelines. However, there is still scarce empirical evidence on whether these principles are recognized, valued, or impactful from the perspective of users. This study investigates the link between ethical AI and user satisfaction by analyzing over 100,000 user reviews of AI products from G2. Using transformer-based language models, we measure sentiment across seven ethical dimensions defined by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all seven dimensions are positively associated with user satisfaction. Yet, this relationship varies systematically across user and product types. Technical users and reviewers of AI development platforms more frequently discuss system-level concerns (e.g., transparency, data governance), while non-technical users and reviewers of end-user applications emphasize human-centric dimensions (e.g., human agency, societal well-being). Moreover, the association between ethical AI and user satisfaction is significantly stronger for non-technical users and end-user applications across all dimensions. Our results highlight the importance of ethical AI design from users' perspectives and underscore the need to account for contextual differences across user roles and product types.

</details>


### [5] [REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition](https://arxiv.org/abs/2508.05933)

*Xueyuan Xu, Wenjia Dong, Fulin Wei, Li Zhuo*

**Main category:** cs.HC

**Keywords:** affective brain-computer interface, EEG feature selection, multi-dimensional emotion recognition, manifold learning, emotional labels

**Relevance Score:** 8

**TL;DR:** This study proposes a novel EEG feature selection method to improve multi-dimensional emotion recognition in affective brain-computer interfaces, addressing challenges of high dimensionality and missing emotional labels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses challenges in multi-dimensional emotion recognition due to the high dimensionality of EEG features and the sparsity of high-quality samples, which can lead to classifier overfitting and real-time performance issues.

**Method:** The proposed method utilizes adaptive orthogonal non-negative matrix factorization for reconstructing emotional label space, and combines least squares regression with graph-based manifold learning regularization for effective feature selection despite missing information.

**Key Contributions:**

	1. Introduces a feature selection method that utilizes adaptive orthogonal non-negative matrix factorization for label reconstruction.
	2. Employs least squares regression with manifold learning for effective feature selection despite missing data.
	3. Demonstrates robustness compared to existing feature selection approaches through extensive experimentation.

**Result:** Simulation experiments on datasets DREAMER, DEAP, and HDED show that the proposed method outperforms thirteen other advanced feature selection methods in robustness.

**Limitations:** 

**Conclusion:** The novel EEG feature selection method significantly enhances multi-dimensional emotion recognition accuracy, reducing the impact of missing values and outliers in EEG data analysis.

**Abstract:** The affective brain-computer interface is a crucial technology for affective interaction and emotional intelligence, emerging as a significant area of research in the human-computer interaction. Compared to single-type features, multi-type EEG features provide a multi-level representation for analyzing multi-dimensional emotions. However, the high dimensionality of multi-type EEG features, combined with the relatively small number of high-quality EEG samples, poses challenges such as classifier overfitting and suboptimal real-time performance in multi-dimensional emotion recognition. Moreover, practical applications of affective brain-computer interface frequently encounters partial absence of multi-dimensional emotional labels due to the open nature of the acquisition environment, and ambiguity and variability in individual emotion perception. To address these challenges, this study proposes a novel EEG feature selection method for missing multi-dimensional emotion recognition. The method leverages adaptive orthogonal non-negative matrix factorization to reconstruct the multi-dimensional emotional label space through second-order and higher-order correlations, which could reduce the negative impact of missing values and outliers on label reconstruction. Simultaneously, it employs least squares regression with graph-based manifold learning regularization and global feature redundancy minimization regularization to enable EEG feature subset selection despite missing information, ultimately achieving robust EEG-based multi-dimensional emotion recognition. Simulation experiments on three widely used multi-dimensional emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method outperforms thirteen advanced feature selection methods in terms of robustness for EEG emotional feature selection.

</details>


### [6] [ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection](https://arxiv.org/abs/2508.05934)

*Xueyuan Xu, Tianze Yu, Wenjia Dong, Fulin Wei, Li Zhuo*

**Main category:** cs.HC

**Keywords:** Emotion Recognition, Multi-modal Physiological Signals, Feature Selection, Brain-Computer Interfaces, Latent Structure Learning

**Relevance Score:** 7

**TL;DR:** This paper proposes a novel method called Adaptive Shared Latent Structure Learning (ASLSL) for feature selection in incomplete multi-modal physiological signal emotion recognition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in emotion recognition due to high-dimensional, incomplete, and noisy multi-modal physiological features that can lead to overfitting and poor performance.

**Method:** ASLSL leverages adaptive shared latent structure learning to discover a common latent space that correlates incomplete multi-modal data and multi-dimensional emotional labels.

**Key Contributions:**

	1. Introduction of ASLSL for feature selection in incomplete multi-modal data
	2. Demonstration of ASLSL's effectiveness on DEAP and DREAMER datasets
	3. Comparison with seventeen existing feature selection methods.

**Result:** ASLSL outperformed seventeen feature selection methods on two popular datasets, DEAP and DREAMER, demonstrating its effectiveness in handling incomplete data.

**Limitations:** 

**Conclusion:** The proposed ASLSL method is effective in improving emotion classification performance by mitigating the effects of missing information in physiological signals.

**Abstract:** Recently, multi-modal physiological signals based emotion recognition has garnered increasing attention in the field of brain-computer interfaces. Nevertheness, the associated multi-modal physiological features are often high-dimensional and inevitably include irrelevant, redundant, and noisy representation, which can easily lead to overfitting, poor performance, and high computational complexity in emotion classifiers. Feature selection has been widely applied to address these challenges. However, previous studies generally assumed that multi-modal physiological data are complete, whereas in reality, the data are often incomplete due to the openness of the acquisition and operational environment. For example, a part of samples are available in several modalities but not in others. To address this issue, we propose a novel method for incomplete multi-modal physiological signal feature selection called adaptive shared latent structure learning (ASLSL). Based on the property that similar features share similar emotional labels, ASLSL employs adaptive shared latent structure learning to explore a common latent space shared for incomplete multi-modal physiological signals and multi-dimensional emotional labels, thereby mitigating the impact of missing information and mining consensus information. Two most popular multi-modal physiological emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels were utilized to compare the performance between compare ASLSL and seventeen feature selection methods. Comprehensive experimental results on these datasets demonstrate the effectiveness of ASLSL.

</details>


### [7] [It's a Complete Haystack: Understanding Dependency Management Needs in Computer-Aided Design](https://arxiv.org/abs/2508.05940)

*Kathy Cheng, Alison Olechowski, Shurui Zhou*

**Main category:** cs.HC

**Keywords:** CAD, dependency management, collaborative design, hardware development, CSCW

**Relevance Score:** 4

**TL;DR:** The paper explores challenges in CAD dependency management faced by hardware designers and proposes solutions to enhance collaboration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Hardware development teams need efficient processes but struggle with managing design changes and dependencies in CAD models.

**Method:** The study includes thematic analysis of 100 online forum discussions and semi-structured interviews with 10 designers to identify challenges in CAD dependency management.

**Key Contributions:**

	1. Identification of key challenges in CAD dependency management
	2. Proposed design goals to enhance collaborative workflows
	3. Feature recommendations for better awareness of design changes.

**Result:** Nine key challenges related to traceability, navigation, and consistency of CAD dependencies that hinder effective collaboration were identified.

**Limitations:** 

**Conclusion:** The paper proposes design goals and features aimed at improving awareness and management of CAD dependencies among hardware designers.

**Abstract:** In today's landscape, hardware development teams face increasing demands for better quality products, greater innovation, and shorter manufacturing lead times. Despite the need for more efficient and effective processes, hardware designers continue to struggle with a lack of awareness of design changes and other collaborators' actions, a persistent issue in decades of CSCW research. One significant and unaddressed challenge is understanding and managing dependencies between 3D CAD (computer-aided design) models, especially when products can contain thousands of interconnected components. In this two-phase formative study, we explore designers' pain points of CAD dependency management through a thematic analysis of 100 online forum discussions and semi-structured interviews with 10 designers. We identify nine key challenges related to the traceability, navigation, and consistency of CAD dependencies, that harm the effective coordination of hardware development teams. To address these challenges, we propose design goals and necessary features to enhance hardware designers' awareness and management of dependencies, ultimately with the goal of improving collaborative workflows.

</details>


### [8] [Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning](https://arxiv.org/abs/2508.06000)

*Wei Xiang, Ziyue Lei, Haoyuan Che, Fangyuan Ye, Xueting Wu, Lingyun Sun*

**Main category:** cs.HC

**Keywords:** LLM, kinesthetic feedback, operational skill learning, Electrical Muscle Stimulation, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper presents FlightAxis, a tool that combines LLM and Electrical Muscle Stimulation for enhancing operational skill learning through kinesthetic feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in LLM-supported training that neglects kinesthetic feedback, which is essential for operational skill learning.

**Method:** The authors developed FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS) to assist in flight skill acquisition, implementing an 'Align-Analyze-Adjust' strategy.

**Key Contributions:**

	1. Introduction of the FlightAxis tool combining LLM with EMS
	2. Establishment of the 'Align-Analyze-Adjust' strategy
	3. Demonstration of improved task performance and user engagement in operational skill learning.

**Result:** The study showed high user acceptance of LLM-driven body control and significantly reduced task completion times, along with increased trainee engagement and awareness of operational flaws during training.

**Limitations:** 

**Conclusion:** The findings suggest that kinesthetic assistance from LLMs can enhance the training of operational skills by providing necessary physical feedback.

**Abstract:** Operational skill learning, inherently physical and reliant on hands-on practice and kinesthetic feedback, has yet to be effectively replicated in large language model (LLM)-supported training. Current LLM training assistants primarily generate customized textual feedback, neglecting the crucial kinesthetic modality. This gap derives from the textual and uncertain nature of LLMs, compounded by concerns on user acceptance of LLM driven body control. To bridge this gap and realize the potential of collaborative human-LLM action, this work explores human experience of LLM driven kinesthetic assistance. Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS) for flight skill acquisition, a representative operational skill domain. FlightAxis learns flight skills from manuals and guides forearm movements during simulated flight tasks. Our results demonstrate high user acceptance of LLM-mediated body control and significantly reduced task completion times. Crucially, trainees reported that this kinesthetic assistance enhanced their awareness of operation flaws and fostered increased engagement in the training process, rather than relieving perceived load. This work demonstrated the potential of kinesthetic LLM training in operational skill acquisition.

</details>


### [9] [RAGTrace: Understanding and Refining Retrieval-Generation Dynamics in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.06056)

*Sizhe Cheng, Jiaping Li, Huanchen Wang, Yuxin Ma*

**Main category:** cs.HC

**Keywords:** Retrieval-Augmented Generation, RAGTrace, Human-Computer Interaction, Knowledge Retrieval, Generative Models

**Relevance Score:** 9

**TL;DR:** RAGTrace is an interactive evaluation system for analyzing retrieval-generation dynamics in Retrieval-Augmented Generation (RAG) workflows, aimed at improving the understanding and effectiveness of RAG systems in integrating external knowledge with generative capabilities.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in RAG systems, understanding the internal dynamic of retrieval and generation remains opaque, warranting a system that allows for detailed analysis and interaction.

**Method:** RAGTrace offers a multi-level analysis approach, combining high-level performance evaluation with detailed examinations of retrieval relevance and generation fidelity, using case studies and expert evaluations for demonstration.

**Key Contributions:**

	1. Introduction of RAGTrace for interactive evaluation of RAG systems.
	2. Enables comprehensive analysis of retrieval-generation dynamics instead of isolated assessments.
	3. Demonstrated effectiveness through case studies in real-world applications.

**Result:** The system effectively enables users to trace knowledge sources and assess the interactions between retrieval and generation components, resulting in improved understanding and iteration of retrieval processes across various domains.

**Limitations:** 

**Conclusion:** RAGTrace provides a comprehensive tool for enhancing the development and evaluation of RAG applications by allowing integrated exploration of their retrieval and generation relationships.

**Abstract:** Retrieval-Augmented Generation (RAG) systems have emerged as a promising solution to enhance large language models (LLMs) by integrating external knowledge retrieval with generative capabilities. While significant advancements have been made in improving retrieval accuracy and response quality, a critical challenge remains that the internal knowledge integration and retrieval-generation interactions in RAG workflows are largely opaque. This paper introduces RAGTrace, an interactive evaluation system designed to analyze retrieval and generation dynamics in RAG-based workflows. Informed by a comprehensive literature review and expert interviews, the system supports a multi-level analysis approach, ranging from high-level performance evaluation to fine-grained examination of retrieval relevance, generation fidelity, and cross-component interactions. Unlike conventional evaluation practices that focus on isolated retrieval or generation quality assessments, RAGTrace enables an integrated exploration of retrieval-generation relationships, allowing users to trace knowledge sources and identify potential failure cases. The system's workflow allows users to build, evaluate, and iterate on retrieval processes tailored to their specific domains of interest. The effectiveness of the system is demonstrated through case studies and expert evaluations on real-world RAG applications.

</details>


### [10] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)

*Daniel Lee, Nikhil Sharma, Donghoon Shin, DaEun Choi, Harsh Sharma, Jeonghwan Kim, Heng Ji*

**Main category:** cs.HC

**Keywords:** Generative AI, Interactive design, Creative intent, Thematic manipulation, User experience

**Relevance Score:** 6

**TL;DR:** ThematicPlane is a system that allows users to interactively manipulate high-level semantic concepts for generative AI image creation, bridging the gap between creative intent and system control.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by non-experts in aligning generative AI outputs with nuanced creative intent, which often requires externalizing ideas through prompts.

**Method:** An exploratory study involving 6 participants who navigated an interactive thematic design plane, engaging in creative modes that emphasized exploration and manipulation of themes.

**Key Contributions:**

	1. Introduction of ThematicPlane for semantic manipulation in generative AI.
	2. The system facilitates exploratory and iterative creative workflows.
	3. Identifies the necessity of explainable controls in generative tools.

**Result:** Participants found that ThematicPlane allowed for expressive workflows and unexpected results that could serve as inspiration, though there were varying expectations regarding the mapping of themes to image outputs.

**Limitations:** The study involved a small sample size (N=6) and may not represent broader user experiences or needs.

**Conclusion:** ThematicPlane supports iterative creative processes and indicates a need for more explainable controls in generative design tools to better align user expectations with outputs.

**Abstract:** Generative AI has made image creation more accessible, yet aligning outputs with nuanced creative intent remains challenging, particularly for non-experts. Existing tools often require users to externalize ideas through prompts or references, limiting fluid exploration. We introduce ThematicPlane, a system that enables users to navigate and manipulate high-level semantic concepts (e.g., mood, style, or narrative tone) within an interactive thematic design plane. This interface bridges the gap between tacit creative intent and system control. In our exploratory study (N=6), participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. While they grounded their exploration in familiar themes, differing expectations of how themes mapped to outputs revealed a need for more explainable controls. Overall, ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.

</details>


### [11] [A Multimodal Framework for Understanding Collaborative Design Processes](https://arxiv.org/abs/2508.06117)

*Maurice Koch, Nelusa Pathmanathan, Daniel Weiskopf, Kuno Kurzhals*

**Main category:** cs.HC

**Keywords:** collaborative design, multimodal data, visual analysis, AI artifact extraction, workshop methodology

**Relevance Score:** 7

**TL;DR:** The paper presents a framework for analyzing collaborative design processes through multimodal data acquisition and interactive visual analysis using the system reCAPit.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the analysis of collaborative design processes by integrating various data sources and enhancing understanding of decision-making outcomes in design workshops.

**Method:** A modular framework combining workshop setup, AI-based artifact extraction, and interactive visual analysis was developed, illustrated through the implementation of the reCAPit system.

**Key Contributions:**

	1. Development of the reCAPit interactive visual analysis system
	2. Integration of multimodal data sources for enhanced analysis
	3. Case studies showcasing the application of the framework

**Result:** The reCAPit system enables the display of multimodal data and supports analysis through a multimodal streamgraph and topic cards, as demonstrated in case studies addressing urban planning and band-practice visualization workshops.

**Limitations:** 

**Conclusion:** The research introduces a new methodology for data-rich observation and analysis of design workshops, improving collaborative design practices and result dissemination.

**Abstract:** An essential task in analyzing collaborative design processes, such as those that are part of workshops in design studies, is identifying design outcomes and understanding how the collaboration between participants formed the results and led to decision-making. However, findings are typically restricted to a consolidated textual form based on notes from interviews or observations. A challenge arises from integrating different sources of observations, leading to large amounts and heterogeneity of collected data. To address this challenge we propose a practical, modular, and adaptable framework of workshop setup, multimodal data acquisition, AI-based artifact extraction, and visual analysis. Our interactive visual analysis system, reCAPit, allows the flexible combination of different modalities, including video, audio, notes, or gaze, to analyze and communicate important workshop findings. A multimodal streamgraph displays activity and attention in the working area, temporally aligned topic cards summarize participants' discussions, and drill-down techniques allow inspecting raw data of included sources. As part of our research, we conducted six workshops across different themes ranging from social science research on urban planning to a design study on band-practice visualization. The latter two are examined in detail and described as case studies. Further, we present considerations for planning workshops and challenges that we derive from our own experience and the interviews we conducted with workshop experts. Our research extends existing methodology of collaborative design workshops by promoting data-rich acquisition of multimodal observations, combined AI-based extraction and interactive visual analysis, and transparent dissemination of results.

</details>


### [12] [Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models](https://arxiv.org/abs/2508.06300)

*Weihan Zhang, Jun Tao*

**Main category:** cs.HC

**Keywords:** Flow Visualization, Natural Language Interaction, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents an automated framework for explorative flow visualization that aligns flow pattern representations with large language models, enabling natural language interaction for analyzing complex flow structures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of traditional visual interfaces in flow analysis, which are often difficult to learn and use, by introducing natural language interaction as a more intuitive alternative.

**Method:** An automated framework that encodes streamline segments with a denoising autoencoder and maps them to LLM embeddings for semantic matching using an attention mechanism.

**Key Contributions:**

	1. Automatic alignment of flow pattern representations with LLM semantic space
	2. Interactive querying and visualization through natural language
	3. Elimination of manual labeling requirements

**Result:** The proposed framework effectively allows users to extract flow patterns based on natural language queries, enhancing the accessibility of flow visualization.

**Limitations:** 

**Conclusion:** The framework demonstrates promising results in enabling intuitive flow exploration and interaction, informed by test cases.

**Abstract:** Explorative flow visualization allows domain experts to analyze complex flow structures by interactively investigating flow patterns. However, traditional visual interfaces often rely on specialized graphical representations and interactions, which require additional effort to learn and use. Natural language interaction offers a more intuitive alternative, but teaching machines to recognize diverse scientific concepts and extract corresponding structures from flow data poses a significant challenge. In this paper, we introduce an automated framework that aligns flow pattern representations with the semantic space of large language models (LLMs), eliminating the need for manual labeling. Our approach encodes streamline segments using a denoising autoencoder and maps the generated flow pattern representations to LLM embeddings via a projector layer. This alignment empowers semantic matching between textual embeddings and flow representations through an attention mechanism, enabling the extraction of corresponding flow patterns based on textual descriptions. To enhance accessibility, we develop an interactive interface that allows users to query and visualize flow structures using natural language. Through case studies, we demonstrate the effectiveness of our framework in enabling intuitive and intelligent flow exploration.

</details>


### [13] [Emoji Reactions on Telegram Often Reflect Social Approval Over Emotional Resonance](https://arxiv.org/abs/2508.06349)

*Serena Tardelli, Lorenzo Alvisi, Lorenzo Cima, Stefano Cresci, Maurizio Tesconi*

**Main category:** cs.HC

**Keywords:** emoji, sentiment analysis, social dynamics, communication, Telegram

**Relevance Score:** 6

**TL;DR:** This paper investigates the communicative function of emoji reactions on Telegram, finding that emoji reactions may indicate social approval rather than purely emotional responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the broader social dynamics reflected in emoji reactions beyond simple emotional indicators.

**Method:** Analyzed over 650k Telegram messages to examine the relationship between message content and emoji reactions, annotating messages with sentiment and emotional labels, and using language models for inference.

**Key Contributions:**

	1. Analysis of over 650k Telegram messages regarding emoji reactions.
	2. Demonstration of the mismatch between message sentiment and emoji reaction sentiment.
	3. Insights into communicative strategies that increase emoji engagement.

**Result:** There is a systematic mismatch where positive emoji reactions dominate even neutral or negative messages, indicating that reactions may be more about social approval.

**Limitations:** Focused on Telegram, which may limit generalizability to other platforms.

**Conclusion:** Emoji reactions might not be reliable proxies for emotional responses, highlighting the need for refined approaches in sentiment analysis.

**Abstract:** Emoji reactions are a frequently used feature of messaging platforms. Prior work mainly interpreted emojis as indicators of emotional resonance or user sentiment. However, emoji reactions may instead reflect broader social dynamics. Here, we investigate the communicative function of emoji reactions on Telegram by analyzing the relationship between the emotional and rhetorical content of messages and the emoji reactions they receive. We collect and analyze over 650k Telegram messages that received at least one emoji reaction. We annotate each message with sentiment, emotion, persuasion strategy, and speech act labels, and infer the sentiment and emotion of emoji reactions using both lexicons and large languages. We find a systematic mismatch between message sentiment and reaction sentiment, with positive reactions dominating even when the message is neutral or negative. We show that this pattern remains consistent across rhetorical strategies and emotional tones, suggesting that emoji reactions may signal a degree of social approval rather than reflecting emotional resonance. Finally, we shed light on the communicative strategies that predict greater emoji engagement. These findings have methodological implications for sentiment analysis, as interpreting emoji reactions as direct proxies for emotional response may be misleading.

</details>


### [14] [Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems](https://arxiv.org/abs/2508.06354)

*Clara Rigaud*

**Main category:** cs.HC

**Keywords:** obsolete smartphones, interactive systems, musical instrument, open-source toolkit, permacomputing

**Relevance Score:** 6

**TL;DR:** This article discusses reusing obsolete smartphones and tablets to create new interactive systems, particularly a musical instrument controller.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of reusing outdated smartphones and tablets for innovative applications and design new interactive systems that are accessible.

**Method:** The author documents the design process of a musical instrument using obsolete smartphones, including diagnostic stages, creation, and insights gathered from professional musicians.

**Key Contributions:**

	1. Development of a musical instrument controller using obsolete devices.
	2. Documentation of the design process with practical insights for future ventures.
	3. Proposal for an open-source toolkit enabling the creation of interactive systems.

**Result:** The research provides insights into software and hardware design challenges and opportunities, aiming to create an open-source toolkit for building interactive systems from old devices.

**Limitations:** Focuses predominantly on the musical application; may not address broader interactive system needs across other domains.

**Conclusion:** The work suggests a high-level web-based approach to facilitate permacomputing with smartphones and encourages further research and development in this area.

**Abstract:** This article explores the possibilities of reusing obsolete smartphones and tablets to build new interactive systems. Taking the case of a musical instrument, I present my research into the design of a controller made from various of these obsolete smartphones. From the diagnostic stage to the creation of a new autonomous electronic object, I document the process, the barriers and the levers encountered. Based on these explorations and discussions with two professional musicians, I provide several insights into the software and hardware aspects, with a view to continuing this work, towards the creation of an open-source toolkit enabling anyone to build new interactive systems with old devices. I discuss the implication of how a high-level web-based approach could allow designers to enter the black box and foster permacomputing using smartphones.

</details>


### [15] [Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data](https://arxiv.org/abs/2508.06484)

*Yuvraj Virk, Dongyu Liu*

**Main category:** cs.HC

**Keywords:** AI code generation, human oversight, data analysis, LLM errors, business decision-making

**Relevance Score:** 8

**TL;DR:** This paper investigates the ability of non-technical end-users, specifically marketing and sales professionals, to identify errors in AI-generated data analyses and suggests design improvements to enhance critical evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As reliance on AI code generation increases among non-technical users, it's crucial to understand their capability to detect errors in AI outputs, particularly in high-stakes decision-making contexts.

**Method:** The study surveyed marketing and sales professionals, assessing their ability to evaluate LLM-generated analyses through direct engagement with the AI's outputs and feedback on its reliability.

**Key Contributions:**

	1. Identified critical flaws in AI outputs that non-technical users struggle to detect.
	2. Showed that providing structured formats for AI responses can enhance user evaluation but still face limitations.
	3. Called attention to the risks posed by unreliable AI in business decision-making contexts.

**Result:** Participants frequently failed to identify critical flaws in AI-generated analyses, even when prompted. Despite reformatting AI responses to support critical evaluation, reasoning through AI's steps remained challenging for users.

**Limitations:** Still limited in how well the restructured AI outputs could aid user reasoning; further investigation needed to improve understanding.

**Conclusion:** The findings indicate that current AI-generated analyses may present risks for decision-making among business professionals due to their inability to verify the outputs adequately, highlighting the need for better user support in AI designs.

**Abstract:** Non-technical end-users increasingly rely on AI code generation to perform technical tasks like data analysis. However, large language models (LLMs) remain unreliable, and it is unclear whether end-users can effectively identify model errors $\unicode{x2014}$ especially in realistic and domain-specific scenarios. We surveyed marketing and sales professionals to assess their ability to critically evaluate LLM-generated analyses of marketing data. Participants were shown natural language explanations of the AI's code, repeatedly informed the AI often makes mistakes, and explicitly prompted to identify them. Yet, participants frequently failed to detect critical flaws that could compromise decision-making, many of which required no technical knowledge to recognize. To investigate why, we reformatted AI responses into clearly delineated steps and provided alternative approaches for each decision to support critical evaluation. While these changes had a positive effect, participants often struggled to reason through the AI's steps and alternatives. Our findings suggest that business professionals cannot reliably verify AI-generated data analyses on their own and explore reasons why to inform future designs. As non-programmers adopt code-generating AI for technical tasks, unreliable AI and insufficient human oversight poses risks of unsafe or low-quality decisions.

</details>


### [16] [Can LLM "Self-report"?: Evaluating the Validity of Self-report Scales in Measuring Personality Design in LLM-based Chatbots](https://arxiv.org/abs/2412.00207)

*Huiqi Zou, Pengda Wang, Zihan Yan, Tianjun Sun, Ziang Xiao*

**Main category:** cs.HC

**Keywords:** chatbot personality, LLM, self-report, interaction quality, human perceptions

**Relevance Score:** 8

**TL;DR:** This study examines the effectiveness of self-report questionnaires for evaluating the personality design of LLM-based chatbots, revealing weak correlations with human perceptions and interaction quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if LLM-based chatbots can meaningfully self-report their personality traits and assess the validity of self-report methods.

**Method:** Created 500 chatbots with different personality designs and analyzed their self-reported personality scores against human perceptions during interactions.

**Key Contributions:**

	1. Evaluation of personality design in LLM-based chatbots
	2. Investigated the validity of self-report questionnaires for personality assessment
	3. Highlighted implications for more contextualized and interactive evaluation methods

**Result:** The self-reported personality scores showed weak correlations with human-perceived personality traits and overall interaction quality.

**Limitations:** Focus on self-reporting may overlook other relevant aspects of chatbot interaction.

**Conclusion:** There are significant concerns regarding the validity of self-report methods for chatbot personality assessment, highlighting the need for more contextualized evaluations.

**Abstract:** A chatbot's personality design is key to interaction quality. As chatbots evolved from rule-based systems to those powered by large language models (LLMs), evaluating the effectiveness of their personality design has become increasingly complex, particularly due to the open-ended nature of interactions. A recent and widely adopted method for assessing the personality design of LLM-based chatbots is the use of self-report questionnaires. These questionnaires, often borrowed from established human personality inventories, ask the chatbot to rate itself on various personality traits. Can LLM-based chatbots meaningfully "self-report" their personality? We created 500 chatbots with distinct personality designs and evaluated the validity of their self-report personality scores by examining human perceptions formed during interactions with these chatbots. Our findings indicate that the chatbot's answers on human personality scales exhibit weak correlations with both human-perceived personality traits and the overall interaction quality. These findings raise concerns about both the criterion validity and the predictive validity of self-report methods in this context. Further analysis revealed the role of task context and interaction in the chatbot's personality design assessment. We further discuss design implications for creating more contextualized and interactive evaluation.

</details>


### [17] [XR for All: Understanding Developers' Perspectives on Accessibility Integration in Extended Reality](https://arxiv.org/abs/2412.16321)

*Daniel Killough, Tiger F. Ji, Kexin Zhang, Yaxin Hu, Yu Huang, Ruofei Du, Yuhang Zhao*

**Main category:** cs.HC

**Keywords:** Accessibility, Extended Reality, User Experience, Inclusive Design, Immersive Technologies

**Relevance Score:** 7

**TL;DR:** This paper explores the challenges and considerations of incorporating accessibility features in XR applications from the perspective of industry developers.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in accessibility practices for extended reality (XR) applications, given the unique interaction methods used in immersive technologies.

**Method:** Interviews with 25 industry XR developers from various company sizes to gather insights on motivations, techniques, and barriers related to accessibility features in XR.

**Key Contributions:**

	1. Insights into developer attitudes and barriers towards XR accessibility
	2. Compilation and evaluation of accessibility guidelines for 3D virtual worlds in XR
	3. Recommendations for creating effective support methods for developers

**Result:** Identified challenges include conflicting priorities between developers, rapid development culture hindering accessibility, and lack of accessible design considerations early in the development process.

**Limitations:** A comprehensive set of accessibility guidelines for XR is still lacking, and the current guidelines may not be fully applicable to XR contexts.

**Conclusion:** The study highlights the need for established XR accessibility guidelines and informs the creation of effective support methods for developers incorporating accessibly into their XR applications.

**Abstract:** As immersive technologies enable unique, multimodal interaction methods, developers must also use tailored methods to support user accessibility, distinct from traditional software practices. We interviewed 25 industry extended reality (XR) developers, including freelancers, startups, midsize, and big tech companies about their motivations, techniques, barriers, and attitudes towards incorporating accessibility features in their XR apps. Our study revealed a variety of challenges, including conflicting priorities between application and platform developers regarding accessibility infrastructure; rapid development culture hindering accessible development; and the lack of accessible interaction design considerations at the ideation, design, and early prototyping stages. As a comprehensive set of XR accessibility guidelines has yet to be established, we also compiled and evaluated a set of accessibility guidelines for 3D virtual worlds and addressed their limitations when applied to XR. Finally, we inform the creation of effective support methods for industry developers.

</details>


### [18] [AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience](https://arxiv.org/abs/2504.13908)

*Soubhik Barari, Jarret Angbazo, Natalie Wang, Leah M. Christian, Elizabeth Dean, Zoe Slowinski, Brandon Sepulvado*

**Main category:** cs.HC

**Keywords:** AI-assisted interviewing, Conversational AI, Language models, Web surveys, Open-ended responses

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for AI-assisted conversational interviewing using LLMs to enhance the depth and quality of responses in web surveys.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the trade-off between the scalability of standardized surveys and the depth of conversational interviews.

**Method:** The study conducted a web survey experiment with 1,800 participants using AI chatbots that leverage LLMs for dynamic probing and coding of open-ended responses.

**Key Contributions:**

	1. Introduction of an AI-assisted framework for conversational interviewing
	2. Demonstration of LLMs in live coding of survey responses
	3. Evaluation of chatbot efficacy in enhancing open-ended data collection

**Result:** AI chatbots achieved moderate performance in coding accuracy and improved the detail of open-ended responses, despite some biases and mixed respondent experiences.

**Limitations:** Respondent experience was slightly negatively impacted due to depth of responses, and coding accuracy faced issues with false positives due to acquiescence bias.

**Conclusion:** The study demonstrates the potential of LLM-powered AI chatbots to enhance data collection methods in web surveys, although improvements in respondent experience are necessary.

**Abstract:** Standardized surveys scale efficiently but sacrifice depth, while conversational interviews improve response quality at the cost of scalability and consistency. This study bridges the gap between these methods by introducing a framework for AI-assisted conversational interviewing. To evaluate this framework, we conducted a web survey experiment where 1,800 participants were randomly assigned to AI 'chatbots' which use large language models (LLMs) to dynamically probe respondents for elaboration and interactively code open-ended responses to fixed questions developed by human researchers. We assessed the AI chatbot's performance in terms of coding accuracy, response quality, and respondent experience. Our findings reveal that AI chatbots perform moderately well in live coding even without survey-specific fine-tuning, despite slightly inflated false positive errors due to respondent acquiescence bias. Open-ended responses were more detailed and informative, but this came at a slight cost to respondent experience. Our findings highlight the feasibility of using AI methods such as chatbots enhanced by LLMs to enhance open-ended data collection in web surveys.

</details>


### [19] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)

*Stefan Pasch, Min Chul Cha*

**Main category:** cs.HC

**Keywords:** ethical AI, user satisfaction, sentiment analysis, transformer models, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This study examines the relationship between ethical AI principles and user satisfaction in over 100,000 user reviews of AI products, finding a positive association influenced by user type and product type.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of empirical evidence on the recognition and impact of ethical AI principles from the user perspective.

**Method:** Analysis of over 100,000 user reviews from G2, utilizing transformer-based language models to measure sentiment based on seven ethical dimensions outlined by the EU Ethics Guidelines for Trustworthy AI.

**Key Contributions:**

	1. Empirical validation of the relationship between ethical AI principles and user satisfaction.
	2. Identification of variations in ethical AI discussions among technical vs. non-technical users.
	3. Highlighting contextual differences in user experiences across different AI product types.

**Result:** All seven ethical dimensions are positively correlated with user satisfaction, with variations depending on whether the users are technical or non-technical and the type of AI product.

**Limitations:** The study only analyzes user reviews from a specific platform (G2), which may not represent the entire AI product landscape.

**Conclusion:** Designing ethical AI requires considering users' perspectives and the contextual differences related to user roles and product types to enhance user satisfaction.

**Abstract:** As AI systems become increasingly embedded in organizational workflows and consumer applications, ethical principles such as fairness, transparency, and robustness have been widely endorsed in policy and industry guidelines. However, there is still scarce empirical evidence on whether these principles are recognized, valued, or impactful from the perspective of users. This study investigates the link between ethical AI and user satisfaction by analyzing over 100,000 user reviews of AI products from G2. Using transformer-based language models, we measure sentiment across seven ethical dimensions defined by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all seven dimensions are positively associated with user satisfaction. Yet, this relationship varies systematically across user and product types. Technical users and reviewers of AI development platforms more frequently discuss system-level concerns (e.g., transparency, data governance), while non-technical users and reviewers of end-user applications emphasize human-centric dimensions (e.g., human agency, societal well-being). Moreover, the association between ethical AI and user satisfaction is significantly stronger for non-technical users and end-user applications across all dimensions. Our results highlight the importance of ethical AI design from users' perspectives and underscore the need to account for contextual differences across user roles and product types.

</details>


### [20] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)

*Daniel Lee, Nikhil Sharma, Donghoon Shin, DaEun Choi, Harsh Sharma, Jeonghwan Kim, Heng Ji*

**Main category:** cs.HC

**Keywords:** Generative AI, Human-Computer Interaction, Thematic Design, Semantics, User Experience

**Relevance Score:** 6

**TL;DR:** ThematicPlane is a generative AI system designed to help users manipulate high-level semantic concepts to enhance creative exploration in image creation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Generative AI facilitates image creation, but its tools often limit users, especially non-experts, in expressing nuanced creative intent due to reliance on complex prompts and references.

**Method:** We developed ThematicPlane, an interactive thematic design interface that allows manipulation of semantic concepts like mood, style, and narrative tone, tested through an exploratory study with six participants.

**Key Contributions:**

	1. Introduction of ThematicPlane as an innovative generative design tool
	2. Focus on high-level semantic concept manipulation for image creation
	3. Empirical insights revealing user behavior and needs in creative processes.

**Result:** Participants engaging with ThematicPlane found it supported both divergent and convergent creative processes, using unexpected results as inspiration. However, there was a noted gap in expectations regarding the mapping of themes to outputs, highlighting a demand for more explainable controls.

**Limitations:** The exploratory study was limited by a small sample size and a lack of diverse user backgrounds.

**Conclusion:** ThematicPlane enhances iterative workflows and suggests new possibilities for interactive, semantic-driven engagement in generative design tools.

**Abstract:** Generative AI has made image creation more accessible, yet aligning outputs with nuanced creative intent remains challenging, particularly for non-experts. Existing tools often require users to externalize ideas through prompts or references, limiting fluid exploration. We introduce ThematicPlane, a system that enables users to navigate and manipulate high-level semantic concepts (e.g., mood, style, or narrative tone) within an interactive thematic design plane. This interface bridges the gap between tacit creative intent and system control. In our exploratory study (N=6), participants engaged in divergent and convergent creative modes, often embracing unexpected results as inspiration or iteration cues. While they grounded their exploration in familiar themes, differing expectations of how themes mapped to outputs revealed a need for more explainable controls. Overall, ThematicPlane fosters expressive, iterative workflows and highlights new directions for intuitive, semantics-driven interaction in generative design tools.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [21] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)

*Rania Al-Sabbagh*

**Main category:** cs.CL

**Keywords:** English-Arabic corpus, healthcare texts, parallel sentences, natural language processing, translation studies

**Relevance Score:** 7

**TL;DR:** PEACH is a sentence-aligned parallel English-Arabic corpus of healthcare texts, providing resources for translation studies and NLP applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a comprehensive resource for healthcare translation and NLP, particularly for English-Arabic language pairs.

**Method:** The corpus was manually aligned and consists of 51,671 sentence pairs derived from patient information leaflets and educational materials.

**Key Contributions:**

	1. Creation of a gold-standard sentence-aligned corpus for healthcare texts
	2. Support for domain-specific machine translation and bilingual lexicon development
	3. Public accessibility of the corpus for educational and research purposes

**Result:** PEACH contains approximately 590,517 English and 567,707 Arabic word tokens, providing a gold-standard corpus for various research applications.

**Limitations:** 

**Conclusion:** PEACH serves as a valuable tool for researchers in linguistics, translation studies, and natural language processing, especially in the healthcare domain.

**Abstract:** This paper introduces PEACH, a sentence-aligned parallel English-Arabic corpus of healthcare texts encompassing patient information leaflets and educational materials. The corpus contains 51,671 parallel sentences, totaling approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths vary between 9.52 and 11.83 words on average. As a manually aligned corpus, PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics, translation studies, and natural language processing. It can be used to derive bilingual lexicons, adapt large language models for domain-specific machine translation, evaluate user perceptions of machine translation in healthcare, assess patient information leaflets and educational materials' readability and lay-friendliness, and as an educational resource in translation studies. PEACH is publicly accessible.

</details>


### [22] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)

*Chi Zhang, Changjia Zhu, Junjie Xiong, Xiaoran Xu, Lingyao Li, Yao Liu, Zhuo Lu*

**Main category:** cs.CL

**Keywords:** Large Language Models, toxicity, content moderation, adversarial attacks, safety alignment

**Relevance Score:** 9

**TL;DR:** This survey reviews the benefits and harms of Large Language Models (LLMs), focusing on toxicity, adversarial attacks, and moderation techniques, while proposing a unified taxonomy and assessing mitigation strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the dual role of LLMs as both beneficial tools and potential sources of harmful language.

**Method:** Systematic review of recent studies on LLM-related harms, adversarial attacks, and content moderation techniques, including proposing a unified taxonomy of issues.

**Key Contributions:**

	1. Unified taxonomy of LLM-related harms and defenses
	2. Analysis of multimodal and LLM-assisted jailbreak strategies
	3. Assessment of mitigation efforts including RLHF and prompt engineering

**Result:** Identification of key harms and defense strategies related to LLMs, along with an analysis of current evaluation methodologies and emerging techniques.

**Limitations:** Current evaluation methodologies are still limited, suggesting areas for improvement.

**Conclusion:** Highlights the need for improved evaluation methods and future research directions for ethically aligned language technologies.

**Abstract:** Large Language Models (LLMs) have revolutionized content creation across digital platforms, offering unprecedented capabilities in natural language generation and understanding. These models enable beneficial applications such as content generation, question and answering (Q&A), programming, and code reasoning. Meanwhile, they also pose serious risks by inadvertently or intentionally producing toxic, offensive, or biased content. This dual role of LLMs, both as powerful tools for solving real-world problems and as potential sources of harmful language, presents a pressing sociotechnical challenge. In this survey, we systematically review recent studies spanning unintentional toxicity, adversarial jailbreaking attacks, and content moderation techniques. We propose a unified taxonomy of LLM-related harms and defenses, analyze emerging multimodal and LLM-assisted jailbreak strategies, and assess mitigation efforts, including reinforcement learning with human feedback (RLHF), prompt engineering, and safety alignment. Our synthesis highlights the evolving landscape of LLM safety, identifies limitations in current evaluation methodologies, and outlines future research directions to guide the development of robust and ethically aligned language technologies.

</details>


### [23] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)

*Xiangyan Chen, Yufeng Li, Yujian Gan, Arkaitz Zubiaga, Matthew Purver*

**Main category:** cs.CL

**Keywords:** Large Language Models, fact verification, dialogue systems, Chain-of-Thought reasoning, NLP

**Relevance Score:** 9

**TL;DR:** This paper presents FineDialFact, a benchmark for fine-grained dialogue fact verification in the context of hallucinations produced by Large Language Models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the challenge of hallucinations in NLP applications that rely on dialogue systems by providing a more nuanced approach to fact verification.

**Method:** The authors constructed a dataset that focuses on verifying atomic facts extracted from dialogue responses and evaluated it with various baseline methods, highlighting the role of Chain-of-Thought reasoning in improving verification performance.

**Key Contributions:**

	1. Introduction of a fine-grained dialogue fact verification benchmark (FineDialFact)
	2. Construction of a dataset for verifying atomic facts in dialogue responses
	3. Demonstration of improved performance using Chain-of-Thought reasoning

**Result:** Experimental results indicate that incorporating Chain-of-Thought reasoning improves performance; however, the best F1-score achieved is only 0.75, revealing the ongoing challenges in this research area.

**Limitations:** The best performance achieved is still only an F1-score of 0.75, indicating there is significant room for improvement in the task of dialogue fact verification.

**Conclusion:** The FineDialFact benchmark and associated dataset highlight the complexity of fine-grained fact verification in dialogues and encourage further research to improve models in this domain.

**Abstract:** Large Language Models (LLMs) are known to produce hallucinations - factually incorrect or fabricated information - which poses significant challenges for many Natural Language Processing (NLP) applications, such as dialogue systems. As a result, detecting hallucinations has become a critical area of research. Current approaches to hallucination detection in dialogue systems primarily focus on verifying the factual consistency of generated responses. However, these responses often contain a mix of accurate, inaccurate or unverifiable facts, making one factual label overly simplistic and coarse-grained. In this paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact verification, which involves verifying atomic facts extracted from dialogue responses. To support this, we construct a dataset based on publicly available dialogue datasets and evaluate it using various baseline methods. Experimental results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning can enhance performance in dialogue fact verification. Despite this, the best F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is only 0.75, indicating that the benchmark remains a challenging task for future research. Our dataset and code will be public on GitHub.

</details>


### [24] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)

*Abishek Thamma, Micha Heilbron*

**Main category:** cs.CL

**Keywords:** language learning, transformer models, human memory, surprisal prediction, cognitive science

**Relevance Score:** 7

**TL;DR:** Investigation of fleeting memory's role in language learning using transformer models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Explore the paradox that cognitive scientists suggest fleeting memory aids language learning, while Transformer models, which lack this memory, still perform well in language tasks.

**Method:** Conducted experiments comparing transformer models with and without fleeting memory on a realistic training set to assess their language learning capabilities.

**Key Contributions:**

	1. Demonstrated the impact of fleeting memory on transformer models' language learning.
	2. Provided empirical evidence supporting cognitive science theories about memory and language acquisition.
	3. Revealed contradictions between language modeling success and reading time prediction in AI.

**Result:** Fleeting memory improved overall language modeling performance and targeted syntactic evaluation, but adversely affected surprisal-based prediction of human reading times.

**Limitations:** Findings may not generalize beyond language tasks; exploration was limited to specific model configurations and datasets.

**Conclusion:** Memory limitations enhance neural network language learning effectiveness but don't necessarily align with human reading behavior predictions.

**Abstract:** Human memory is fleeting. As words are processed, the exact wordforms that make up incoming sentences are rapidly lost. Cognitive scientists have long believed that this limitation of memory may, paradoxically, help in learning language - an idea supported by classic connectionist modelling work. The rise of Transformers appears to challenge this idea, as these models can learn language effectively, despite lacking memory limitations or other architectural recency biases. Here, we investigate the hypothesized benefit of fleeting memory for language learning in tightly controlled experiments on transformer language models. Training transformers with and without fleeting memory on a developmentally realistic training set, we find that fleeting memory consistently improves language learning (as quantified by both overall language modelling performance and targeted syntactic evaluation) but, unexpectedly, impairs surprisal-based prediction of human reading times. Interestingly, follow up analyses revealed that this discrepancy - better language modeling, yet worse reading time prediction - could not be accounted for by prior explanations of why better language models sometimes fit human reading time worse. Together, these results support a benefit of memory limitations on neural network language learning - but not on predicting behavior.

</details>


### [25] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)

*Tong Li, Rasiq Hussain, Mehak Gupta, Joshua R. Oltmanns*

**Main category:** cs.CL

**Keywords:** depression prediction, language models, criterion contamination, HCI, machine learning

**Relevance Score:** 9

**TL;DR:** This study compares the effectiveness of Mirror models against Non-Mirror models in predicting depression scores using language data, highlighting concerns about bias and generalizability in AI predictions.

**Read time:** 39 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of 'criterion contamination' in LLM-based predictions of depression scores, which affects model reliability and generalizability.

**Method:** A total of 110 participants completed structured diagnostic and life history interviews. GPT-4, GPT-4o, and LLaMA3-70B were utilized to predict depression scores from each type of transcript and compared the performance of Mirror versus Non-Mirror models.

**Key Contributions:**

	1. Introduction of the concept of Non-Mirror models to enhance generalizability in language-based depression prediction.
	2. Empirical comparison showing significant bias in Mirror models through inflated effect sizes.
	3. Identification of semantic clusters that can aid in psychological assessment.

**Result:** Mirror models showed very large effect sizes (R2 = .80), while Non-Mirror models had smaller effect sizes (R2 = .27), but both scores correlating with self-reported depression symptoms were similar (r = ~.54).

**Limitations:** The study is limited to a specific sample size and may not generalize to broader populations or different diagnostic contexts.

**Conclusion:** The study found that Mirror models tend to overinflate effect sizes due to bias from using assessment-mirrored language, suggesting a need for Non-Mirror models to improve generalizability in AI-based psychological evaluations.

**Abstract:** A growing number of studies show near-perfect LLM language-based prediction of depression assessment scores (up to R2 of .70). However, many develop these models directly from language responses to depression assessments. These "Mirror models" suffer from "criterion contamination", which arises when a predicted score depends in part on the predictors themselves. This causes artificial effect size inflation which reduces model generalizability. The present study compares the performance of Mirror models versus "Non-Mirror models", which are developed from language that does not mirror the assessment they are developed to predict. N = 110 research participants completed two different interviews: structured diagnostic and life history interviews. GPT-4, GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic interview depression scores from the two transcripts separately. Mirror models (using structured diagnostic data) showed very large effect sizes (e.g., R2 = .80). As expected, NonMirror models (using life history data) demonstrated smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror and Non-Mirror model-predicted structured interview depression scores were correlated with self-reported depression symptoms, Mirror and NonMirror performed the same (e.g., r = ~.54), indicating that Mirror models contain bias perhaps due to criterion contamination. Topic modeling identified clusters across Mirror and Non-Mirror models, as well as between true-positive and false-positive predictions. In this head-to-head comparison study, Mirror language AI models of depression showed artificially inflated effect sizes and less generalizability. As language AI models for depression continue to evolve, incorporating Non-Mirror models may identify interpretable, and generalizable semantic features that have unique utility in real-world psychological assessment.

</details>


### [26] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)

*Miles Gilberti, Shane Storks, Huteng Dai*

**Main category:** cs.CL

**Keywords:** emergent communication, deep neural networks, natural language, inflectional morphology, morphological properties

**Relevance Score:** 7

**TL;DR:** This paper explores emergent communication in deep neural networks by simulating constraints akin to natural language morphology.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how emergent communication schemes can reflect properties of natural language, particularly in terms of morphology, beyond traditional metrics.

**Method:** The authors reinterpret the attribute-value reconstruction game by imposing a small-vocabulary constraint to simulate double articulation and develop new metrics reflecting properties of natural language inflectional morphology.

**Key Contributions:**

	1. Proposed a new experimental setting simulating double articulation and naturalistic morphology.
	2. Developed novel metrics for evaluating emergent communication based on morphological properties.
	3. Demonstrated that simulated constraints can lead to language behaviors analogous to natural languages.

**Result:** Experiments reveal that phonological constraints lead to concatenative morphology and that emergent languages exhibit patterns similar to natural languages, such as fusing grammatical attributes.

**Limitations:** 

**Conclusion:** The study provides insights into how language-like properties can emerge from neural networks in simulated environments that mimic natural language mechanics.

**Abstract:** Emergent communication (EmCom) with deep neural network-based agents promises to yield insights into the nature of human language, but remains focused primarily on a few subfield-specific goals and metrics that prioritize communication schemes which represent attributes with unique characters one-to-one and compose them syntactically. We thus reinterpret a common EmCom setting, the attribute-value reconstruction game, by imposing a small-vocabulary constraint to simulate double articulation, and formulating a novel setting analogous to naturalistic inflectional morphology (enabling meaningful comparison to natural language communication schemes). We develop new metrics and explore variations of this game motivated by real properties of inflectional morphology: concatenativity and fusionality. Through our experiments, we discover that simulated phonological constraints encourage concatenative morphology, and emergent languages replicate the tendency of natural languages to fuse grammatical attributes.

</details>


### [27] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)

*Sree Bhattacharyya, Lucas Craig, Tharun Dilliraj, Jia Li, James Z. Wang*

**Main category:** cs.CL

**Keywords:** Affective Computing, Cognitive Reasoning, Large Language Models, Cognitive Appraisal Theory, Emotional Intelligence

**Relevance Score:** 8

**TL;DR:** This paper investigates cognitive reasoning in Large Language Models (LLMs) regarding emotions, proposing a benchmark called CoRE to evaluate their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance Affective Computing by going beyond superficial emotion recognition to understand how LLMs reason about emotions.

**Method:** The paper introduces the CoRE benchmark, drawing from cognitive appraisal theory, to evaluate LLMs' cognitive reasoning about emotions through a series of benchmarks and experiments.

**Key Contributions:**

	1. Introduction of CoRE, a benchmark for evaluating cognitive reasoning in LLMs regarding emotions
	2. Insights into how LLMs reason about emotions beyond surface recognition
	3. Identification of cognitive dimensions significant for emotional representation in LLMs.

**Result:** The evaluation revealed diverse reasoning patterns in different LLMs and identified specific cognitive appraisal dimensions that influence emotion characterization.

**Limitations:** 

**Conclusion:** The findings advance our understanding of LLMs' emotional reasoning and contribute to the development of more emotionally intelligent AI systems.

**Abstract:** Affective Computing has been established as a crucial field of inquiry to advance the holistic development of Artificial Intelligence (AI) systems. Foundation models -- especially Large Language Models (LLMs) -- have been evaluated, trained, or instruction-tuned in several past works, to become better predictors or generators of emotion. Most of these studies, however, approach emotion-related tasks in a supervised manner, assessing or training the capabilities of LLMs using discrete emotion labels associated with stimuli (e.g., text, images, video, audio). Evaluation studies, in particular, have often been limited to standard and superficial emotion-related tasks, such as the recognition of evoked or expressed emotions. In this paper, we move beyond surface-level emotion tasks to investigate how LLMs reason about emotions through cognitive dimensions. Drawing from cognitive appraisal theory, we examine whether LLMs produce coherent and plausible cognitive reasoning when reasoning about emotionally charged stimuli. We introduce a large-scale benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal cognitive structures implicitly used by LLMs for emotional reasoning. Through a plethora of evaluation experiments and analysis, we seek to answer: (a) Are models more likely to implicitly rely on specific cognitive appraisal dimensions?, (b) What cognitive dimensions are important for characterizing specific emotions?, and, (c) Can the internal representations of different emotion categories in LLMs be interpreted through cognitive appraisal dimensions? Our results and analyses reveal diverse reasoning patterns across different LLMs. Our benchmark and code will be made publicly available.

</details>


### [28] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)

*Zhanghao Hu, Qinglin Zhu, Siya Qi, Yulan He, Hanqi Yan, Lin Gui*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Spectrum Projection Score, Inference Control, QA Benchmarks

**Relevance Score:** 9

**TL;DR:** This paper introduces Spectrum Projection Score (SPS), a new metric for evaluating retrieval-augmented generation (RAG) in large language models, along with xCompress, a framework for optimizing retrieval summaries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of retrieval-augmented generation by isolating the contributions of the retriever and reader components in large language models.

**Method:** The paper presents SPS, a supervision-free metric that assesses semantic alignment between retrieved summaries and their hidden representations, and describes the xCompress framework for dynamic sampling, ranking, and compressing summary candidates during inference.

**Key Contributions:**

	1. Introduction of the Spectrum Projection Score (SPS) as a metric for evaluating RAG
	2. Development of the xCompress framework for optimizing summary candidates
	3. Demonstration of improved performance across various QA benchmarks using SPS

**Result:** Extensive experiments on five QA benchmarks demonstrate that SPS improves the performance of various LLMs while elucidating the interactions between retrieval and generation.

**Limitations:** 

**Conclusion:** SPS serves as a valuable tool for enhancing RAG performance and provides insights into the roles of retrieval and generation in LLMs.

**Abstract:** Large Language Models (LLMs) have shown improved generation performance through retrieval-augmented generation (RAG) following the retriever-reader paradigm, which supplements model inputs with externally retrieved knowledge. However, prior work often evaluates RAG holistically, assessing the retriever and reader jointly, making it difficult to isolate the true contribution of retrieval, particularly given the prompt sensitivity of LLMs used as readers. We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free metric that allows the reader to gauge the semantic alignment of a retrieved summary with its hidden representation by comparing the area formed by generated tokens from the summary, and the principal directions of subspace in the reader and to measure the relevance. Building on SPS we present xCompress, an inference time controller framework that dynamically samples, ranks, and compresses retrieval summary candidates. Extensive experiments on five QA benchmarks with four open source LLMs show that SPS not only enhances performance across a range of tasks but also provides a principled perspective on the interaction between retrieval and generation.

</details>


### [29] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)

*Rafal Kocielnik, Min Kim, Penphob, Boonyarungsrit, Fereshteh Soltani, Deshawn Sambrano, Animashree Anandkumar, R. Michael Alvarez*

**Main category:** cs.CL

**Keywords:** prosociality, human-AI interaction, LLM, trust and safety, responsible AI

**Relevance Score:** 8

**TL;DR:** A novel three-stage pipeline for classifying prosociality in text, minimizing human effort while achieving high accuracy and reducing costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Detecting prosociality in text communication is an important challenge for trust and safety systems, and current methods suffer from a lack of definitions and labeled data.

**Method:** The approach consists of three stages: identifying an LLM-based labeling strategy, introducing a human-AI refinement loop for task definition clarity, and synthesizing 10k high-quality labels to train a two-stage inference system.

**Key Contributions:**

	1. Scalable pipeline for prosocial content classification
	2. Human-AI refinement loop enhances labeling accuracy
	3. Cost-efficient architecture reduces inference costs significantly

**Result:** The proposed system achieves high precision (approximately 0.90) and reduces inference costs by about 70%.

**Limitations:** 

**Conclusion:** The pipeline showcases how targeted human-AI interaction and thoughtful architecture can facilitate scalable solutions for emerging responsible AI tasks like prosociality detection.

**Abstract:** Detecting prosociality in text--communication intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best LLM-based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\sim$35\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.

</details>


### [30] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)

*Chunyun Zhang, Hongyan Zhao, Chaoran Cui, Qilong Song, Zhiqing Lu, Shuai Gong, Kailin Liu*

**Main category:** cs.CL

**Keywords:** automated essay scoring, topic-aware prompt-tuning, adversarial training, language models, essay evaluation

**Relevance Score:** 5

**TL;DR:** Proposes a novel method Adversarial TOpic-aware Prompt-tuning (ATOP) for improving cross-topic automated essay scoring (AES) by jointly learning topic-shared and topic-specific features.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for cross-topic AES fail to account for topic-specific features, limiting their effectiveness in accurately evaluating essays.

**Method:** ATOP learns topic-shared and topic-specific features using a learnable topic-aware prompt within a unified regression and classification framework; it employs adversarial training and a neighbor-based classifier for improved performance.

**Key Contributions:**

	1. Introduction of the Adversarial TOpic-aware Prompt-tuning (ATOP) method.
	2. Joint learning of topic-shared and topic-specific features for automated essay scoring.
	3. Use of adversarial training and neighbor-based classification to improve essay scoring accuracy.

**Result:** ATOP significantly outperforms state-of-the-art methods on the ASAP++ dataset in both holistic and multi-trait essay scoring.

**Limitations:** 

**Conclusion:** The effectiveness of the ATOP method demonstrates the importance of considering both shared and specific features in cross-topic AES tasks.

**Abstract:** Cross-topic automated essay scoring (AES) aims to develop a transferable model capable of effectively evaluating essays on a target topic. A significant challenge in this domain arises from the inherent discrepancies between topics. While existing methods predominantly focus on extracting topic-shared features through distribution alignment of source and target topics, they often neglect topic-specific features, limiting their ability to assess critical traits such as topic adherence. To address this limitation, we propose an Adversarial TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns topic-shared and topic-specific features to improve cross-topic AES. ATOP achieves this by optimizing a learnable topic-aware prompt--comprising both shared and specific components--to elicit relevant knowledge from pre-trained language models (PLMs). To enhance the robustness of topic-shared prompt learning and mitigate feature scale sensitivity introduced by topic alignment, we incorporate adversarial training within a unified regression and classification framework. In addition, we employ a neighbor-based classifier to model the local structure of essay representations and generate pseudo-labels for target-topic essays. These pseudo-labels are then used to guide the supervised learning of topic-specific prompts tailored to the target topic. Extensive experiments on the publicly available ASAP++ dataset demonstrate that ATOP significantly outperforms existing state-of-the-art methods in both holistic and multi-trait essay scoring. The implementation of our method is publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [31] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)

*Sagar Gandhi, Vishal Gandhi*

**Main category:** cs.CL

**Keywords:** attention sparsity, Transformer models, DistilBERT

**Relevance Score:** 7

**TL;DR:** This paper shows that introducing structured post-hoc sparsity to the attention mechanism of DistilBERT can enhance model accuracy on sentiment analysis tasks, contrary to the conventional belief that sparsity reduces performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that attention sparsity reduces model accuracy in Transformer models and explore its potential as a method for enhancing performance.

**Method:** The authors introduce structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task, evaluating its impact on model accuracy.

**Key Contributions:**

	1. Demonstrated an improvement in accuracy with attention sparsity on a Transformer model
	2. Proposed sparsity as a method for enhancing generalization
	3. Provided empirical evidence against the belief that sparsity harms model performance.

**Result:** The model achieves 80% attention sparsity with a validation accuracy of 91.59%, which is a 0.97% absolute improvement over the dense baseline.

**Limitations:** 

**Conclusion:** Sparsity can act as a powerful implicit regularizer, improving generalization and performance rather than just serving for computational efficiency.

**Abstract:** The quadratic computational cost of the self-attention mechanism is a primary challenge in scaling Transformer models. While attention sparsity is widely studied as a technique to improve computational efficiency, it is almost universally assumed to come at the cost of model accuracy. In this paper, we report a surprising counter-example to this common wisdom. By introducing structured, post-hoc sparsity to the attention mechanism of a DistilBERT model during fine-tuning on the SST-2 sentiment analysis task, we find that model accuracy improves significantly. Our model with 80\% attention sparsity achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over the dense baseline. We hypothesize that this phenomenon is due to sparsity acting as a powerful implicit regularizer, preventing the model from overfitting by forcing it to make predictions with a more constrained and robust set of features. Our work recasts attention sparsity not just as a tool for computational efficiency, but as a potential method for improving the generalization and performance of Transformer models.

</details>


### [32] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)

*Yidong Wang, Xin Wang, Cunxiang Wang, Junfeng Fang, Qiufeng Wang, Jianing Chu, Xuran Meng, Shuxun Yang, Libo Qin, Yue Zhang, Wei Ye, Shikun Zhang*

**Main category:** cs.CL

**Keywords:** Language Models, Self-Rewarding, Preference Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Temporal Self-Rewarding Language Models that improve preference learning in LLMs by coordinating past, present, and future generations, overcoming limitations of existing Self-Rewarding paradigms.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in current Self-Rewarding Language Models which narrow representational differences necessary for effective preference learning.

**Method:** The paper proposes a dual-phase framework, introducing Anchored Rejection and Future-Guided Chosen methodologies to coordinate model generations for improved learning signals.

**Key Contributions:**

	1. Introduction of Temporal Self-Rewarding Language Models that coordinate model generations
	2. Anchored Rejection fixing rejected responses to maintain variability
	3. Future-Guided Chosen dynamically curating responses for improved preference learning

**Result:** Experiments showed significant improvements in performance metrics on various model families (Llama, Qwen, Mistral) and sizes, with Llama3.1-8B achieving a 29.44 win rate on AlpacaEval 2.0, outperforming the Self-Rewarding baseline of 19.69.

**Limitations:** 

**Conclusion:** The proposed method enhances generalization across multiple tasks, including mathematical reasoning and code generation, without requiring specific training data for those tasks.

**Abstract:** Self-Rewarding Language Models propose an architecture in which the Large Language Models(LLMs) both generates responses and evaluates its own outputs via LLM-as-a-Judge prompting, dynamically improving its generative capabilities through iterative Direct Preference Optimization (DPO). However, our analysis reveals a critical limitation in existing Self-Rewarding paradigms: the synchronized improvement of chosen and rejected responses progressively narrows the representational difference between contrasting samples, undermining effective preference learning. We propose \textbf{Temporal Self-Rewarding Language Models} that strategically coordinate past, present, and future model generations to sustain learning signals. Our dual-phase framework introduces: (1) \textit{Anchored Rejection} - fixing rejected responses using the past initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically curating chosen samples using next-generation model predictions. Extensive experiments across three model families (Llama, Qwen, Mistral) and different model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained with our method compared to Self-Rewarding using same computation resources. For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our method also demonstrates superior out-of-distribution generalization across mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code generation (HumanEval) tasks, even though we do not specifically collect such training data.

</details>


### [33] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)

*Kartik Sharma, Yiqiao Jin, Rakshit Trivedi, Srijan Kumar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Estimation, Proxy Embeddings, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper introduces PEEK, a method using proxy embeddings to estimate knowledge in large language models (LLMs) more efficiently than existing probing techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational expense and time consumption of probing large language models (LLMs) for estimating their knowledge, this work aims to create a more efficient alternative.

**Method:** The authors propose PEEK, which leverages pre-trained embedding models by identifying a training set of factual knowledge known to LLMs and adapting embedding models to predict LLM outputs with a linear decoder.

**Key Contributions:**

	1. Introduction of PEEK, a new method for estimating LLM knowledge
	2. Demonstration of up to 90% accuracy in predicting LLM knowledge using embedding models
	3. Identification of sentence embeddings as more suitable than graph embeddings for this task

**Result:** Embeddings can predict LLM knowledge with up to 90% accuracy on a held-out set, demonstrating that embeddings can effectively estimate the knowledge encapsulated in LLMs.

**Limitations:** 

**Conclusion:** Knowledge-adapted embeddings can identify knowledge gaps in LLMs at scale and provide better insights into LLMs' internal inductive biases. The resources used in the research will be available on GitHub.

**Abstract:** Large language models (LLMs) acquire knowledge across diverse domains such as science, history, and geography encountered during generative pre-training. However, due to their stochasticity, it is difficult to predict what LLMs have acquired. Prior work has developed different ways to probe this knowledge by investigating the hidden representations, crafting specific task prompts, curating representative samples, and estimating their uncertainty. However, these methods require making forward passes through the underlying model to probe the LLM's knowledge about a specific fact, making them computationally expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or $\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate $\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models that effectively encode factual knowledge as text or graphs as proxies for LLMs. First, we identify a training set of facts known by LLMs through various probing strategies and then adapt embedding models to predict the LLM outputs with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find that sentence embedding models are more suitable than graph embeddings to predict LLM knowledge, shedding light on the underlying representation of the factual landscape. Thus, we believe that knowledge-adapted embeddings can be used to identify knowledge gaps in LLMs at scale and can provide deeper insights into LLMs' internal inductive bias. The code and data are made available at https://github.com/claws-lab/peek.

</details>


### [34] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)

*Xinda Wang, Zhengxu Hou, Yangshijie Zhang, Bingren Yan, Zhibo Yang, Xingsheng Zhang, Luxi Xing, Qiang Zhou, Chen Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, story evaluation, self-evolving framework, reward model, Chain-of-Thought

**Relevance Score:** 9

**TL;DR:** The Self-Evolving Pairwise Reasoning (EvolvR) framework enhances story evaluation using LLMs by self-synthesizing high-quality comparison data and deploying it as a reward model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of LLMs in open-ended tasks like story evaluation and generation.

**Method:** The EvolvR framework self-synthesizes score-aligned Chain-of-Thought (CoT) data through a multi-persona approach and employs a filtering process for logical rigor before training a reward model.

**Key Contributions:**

	1. Introduction of the EvolvR framework for story evaluation
	2. Self-synthesis of score-aligned CoT data using multi-persona strategy
	3. Demonstration of state-of-the-art performance on evaluation benchmarks

**Result:** EvolvR achieves state-of-the-art performance on three benchmarks (StoryER, HANNA, OpenMEVA) and significantly improves story quality when used as a reward model.

**Limitations:** 

**Conclusion:** The self-evolving approach validates the potential of LLMs as judges in story evaluation and generation tasks.

**Abstract:** Although the effectiveness of Large Language Models (LLMs) as judges (LLM-as-a-judge) has been validated, their performance remains limited in open-ended tasks, particularly in story evaluation. Accurate story evaluation is crucial not only for assisting human quality judgment but also for providing key signals to guide story generation. However, existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation. To address this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework. Grounded in pairwise comparison, the framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task. Experimental results demonstrate that our framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories, thereby fully validating the superiority of our self-evolving approach.

</details>


### [35] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)

*Morris Alper, Moran Yanuka, Raja Giryes, Gaper Begu*

**Main category:** cs.CL

**Keywords:** constructed languages, LLMs, ConlangCrafter, language generation, creativity

**Relevance Score:** 6

**TL;DR:** ConlangCrafter uses LLMs to create constructed languages through a modular and multi-hop process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To utilize large-scale language models in the creative process of constructing languages, enhancing the diversity and coherence of conlangs.

**Method:** ConlangCrafter is a structured pipeline that breaks down language creation into stages: phonology, morphology, syntax, lexicon generation, and translation, using LLMs to facilitate each step.

**Key Contributions:**

	1. Introduction of ConlangCrafter for conlang creation
	2. Multi-hop pipeline that modularizes language design
	3. Utilization of LLMs for enhancing creativity and coherence

**Result:** The evaluation shows that ConlangCrafter can produce coherent and diverse constructed languages without requiring linguistic expertise.

**Limitations:** The approach may not capture all the intricacies of human language due to reliance on LLMs.

**Conclusion:** The method demonstrates the potential of LLMs in creative language generation by allowing non-experts to create functional conlangs.

**Abstract:** Constructed languages (conlangs) such as Esperanto and Quenya have played diverse roles in art, philosophy, and international communication. Meanwhile, large-scale foundation models have revolutionized creative generation in text, images, and beyond. In this work, we leverage modern LLMs as computational creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a multi-hop pipeline that decomposes language design into modular stages -- phonology, morphology, syntax, lexicon generation, and translation. At each stage, our method leverages LLMs' meta-linguistic reasoning capabilities, injecting randomness to encourage diversity and leveraging self-refinement feedback to encourage consistency in the emerging language description. We evaluate ConlangCrafter on metrics measuring coherence and typological diversity, demonstrating its ability to produce coherent and varied conlangs without human linguistic expertise.

</details>


### [36] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)

*Mohamed Basem, Islam Oshallah, Ali Hamdi, Ammar Mohammed*

**Main category:** cs.CL

**Keywords:** Extractive Question Answering, large language models, Arabic, few-shot prompting, post-processing

**Relevance Score:** 3

**TL;DR:** The paper presents two approaches for Extractive Question Answering on the Quran, utilizing instruction-tuned large language models for enhanced performance.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges in Extractive QA related to the Quran's complex language and unique terminology.

**Method:** The study employs few-shot prompting with LLMs (Gemini, DeepSeek) and uses a dedicated Arabic prompt framework alongside a comprehensive post-processing system.

**Key Contributions:**

	1. Development of a specialized Arabic prompt framework for QA
	2. Integration of a robust post-processing system
	3. Demonstration of better performance of LLMs over traditional models

**Result:** Evaluation results indicate that the proposed approach significantly outperforms traditional fine-tuned models, achieving a pAP10 score of 0.637.

**Limitations:** 

**Conclusion:** Prompt-based instruction tuning is shown to be effective for low-resource, semantically rich QA tasks, improving precision and reducing hallucinations in answers.

**Abstract:** This paper presents two effective approaches for Extractive Question Answering (QA) on the Quran. It addresses challenges related to complex language, unique terminology, and deep meaning in the text. The second uses few-shot prompting with instruction-tuned large language models such as Gemini and DeepSeek. A specialized Arabic prompt framework is developed for span extraction. A strong post-processing system integrates subword alignment, overlap suppression, and semantic filtering. This improves precision and reduces hallucinations. Evaluations show that large language models with Arabic instructions outperform traditional fine-tuned models. The best configuration achieves a pAP10 score of 0.637. The results confirm that prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [37] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)

*Shengyuan Chen, Chuang Zhou, Zheng Yuan, Qinggang Zhang, Zeyang Cui, Hao Chen, Yilin Xiao, Jiannong Cao, Xiao Huang*

**Main category:** cs.CL

**Keywords:** large language models, retrieval-augmented generation, logic-aware framework, dynamic retrieval, graph pruning

**Relevance Score:** 9

**TL;DR:** LogicRAG is a novel framework that enhances retrieval-augmented generation in large language models by dynamically extracting reasoning structures at inference time, avoiding the need for costly pre-built graphs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing GraphRAG methods incur high costs in transforming corpora into graphs, leading to token and update inefficiencies, and do not adapt well to varying query complexities.

**Method:** LogicRAG decomposes input queries into subproblems, constructs a directed acyclic graph (DAG) to model dependencies, and employs topological sorting and graph/context pruning for coherent multi-step reasoning.

**Key Contributions:**

	1. Introduces LogicRAG for dynamic reasoning structure extraction at inference time.
	2. Models logical dependencies with directed acyclic graphs (DAGs).
	3. Implements graph and context pruning to reduce token cost.

**Result:** LogicRAG demonstrates superior performance and efficiency over state-of-the-art baselines in retrieval-augmented generation tasks.

**Limitations:** 

**Conclusion:** The proposed framework allows for more adaptive and efficient reasoning in response to complex queries without prior graph construction.

**Abstract:** Large language models (LLMs) often suffer from hallucination, generating factually incorrect statements when handling questions beyond their knowledge and perception. Retrieval-augmented generation (RAG) addresses this by retrieving query-relevant contexts from knowledge bases to support LLM reasoning. Recent advances leverage pre-constructed graphs to capture the relational connections among distributed documents, showing remarkable performance in complex tasks. However, existing Graph-based RAG (GraphRAG) methods rely on a costly process to transform the corpus into a graph, introducing overwhelming token cost and update latency. Moreover, real-world queries vary in type and complexity, requiring different logic structures for accurate reasoning. The pre-built graph may not align with these required structures, resulting in ineffective knowledge retrieval. To this end, we propose a \textbf{\underline{Logic}}-aware \textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented \textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically extracts reasoning structures at inference time to guide adaptive retrieval without any pre-built graph. LogicRAG begins by decomposing the input query into a set of subproblems and constructing a directed acyclic graph (DAG) to model the logical dependencies among them. To support coherent multi-step reasoning, LogicRAG then linearizes the graph using topological sort, so that subproblems can be addressed in a logically consistent order. Besides, LogicRAG applies graph pruning to reduce redundant retrieval and uses context pruning to filter irrelevant context, significantly reducing the overall token cost. Extensive experiments demonstrate that LogicRAG achieves both superior performance and efficiency compared to state-of-the-art baselines.

</details>


### [38] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)

*Sayantan Adak, Pratyush Chatterjee, Somnath Banerjee, Rima Hazra, Somak Aditya, Animesh Mukherjee*

**Main category:** cs.CL

**Keywords:** Language Model, Safety, Process Reward Models, AI Alignment, Logical Coherence

**Relevance Score:** 9

**TL;DR:** AURA is a multi-layered framework utilizing Process Reward Models (PRMs) to enhance safety and logical coherence in LLM outputs by providing step-level evaluations and proactive interventions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Present-day LLMs struggle with managing safety risks due to overlooked logical implications in their outputs, necessitating more effective safety solutions.

**Method:** The paper introduces AURA, a framework that incorporates introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding for safer reasoning in LLMs.

**Key Contributions:**

	1. Introduction of the AURA framework for safety in LLMs
	2. Integration of Process Reward Models for step-level evaluations
	3. Demonstration of superior performance in logical integrity and safety of outputs

**Result:** Empirical evidence shows that AURA significantly outperforms existing safety methodologies, improving logical integrity and safety-awareness in model outputs.

**Limitations:** 

**Conclusion:** This framework marks a key advancement toward safer and more responsible AI, setting new standards for alignment-sensitive applications.

**Abstract:** Present day LLMs face the challenge of managing affordance-based safety risks-situations where outputs inadvertently facilitate harmful actions due to overlooked logical implications. Traditional safety solutions, such as scalar outcome-based reward models, parameter tuning, or heuristic decoding strategies, lack the granularity and proactive nature needed to reliably detect and intervene during subtle yet crucial reasoning steps. Addressing this fundamental gap, we introduce AURA, an innovative, multi-layered framework centered around Process Reward Models (PRMs), providing comprehensive, step level evaluations across logical coherence and safety-awareness. Our framework seamlessly combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to dynamically and proactively guide models toward safer reasoning trajectories. Empirical evidence clearly demonstrates that this approach significantly surpasses existing methods, significantly improving the logical integrity and affordance-sensitive safety of model outputs. This research represents a pivotal step toward safer, more responsible, and contextually aware AI, setting a new benchmark for alignment-sensitive applications.

</details>


### [39] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)

*Vt Gvodiak*

**Main category:** cs.CL

**Keywords:** Pragmatics, Human-Machine Communication, Large Language Models, Probabilistic Pragmatics, Context Frustration

**Relevance Score:** 6

**TL;DR:** The paper reconceptualizes pragmatics in the context of large language models (LLMs), proposing a new Human-Machine Communication framework to address the limitations of traditional theories and the unique nature of machine-centered communication.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To refine the understanding of pragmatics in light of the emergence of LLMs and their impact on human communication.

**Method:** The paper critiques traditional semiotic approaches and introduces the Human-Machine Communication (HMC) framework. It also discusses the limitations of Gricean pragmatics and advocates for probabilistic pragmatics as a more relevant approach for LLMs.

**Key Contributions:**

	1. Proposes the Human-Machine Communication (HMC) framework as an alternative to traditional pragmatics.
	2. Introduces the concept of context frustration in human-machine communication.
	3. Advocates for the use of probabilistic pragmatics over Gricean models in machine contexts.

**Result:** The paper finds that traditional pragmatic theories are inadequately equipped to address the dynamics introduced by LLMs, and proposes that context frustration necessitates a reevaluation of pragmatic frameworks for generative AI communication.

**Limitations:** The paper primarily focuses on theoretical exploration and may lack empirical validation of the proposed frameworks in real-world applications.

**Conclusion:** Pragmatics might require modification or expansion to effectively capture the nuances of communication involving generative AI, particularly in how users interact with LLMs.

**Abstract:** The paper reconceptualizes pragmatics not as a subordinate, third dimension of meaning, but as a dynamic interface through which language operates as a socially embedded tool for action. With the emergence of large language models (LLMs) in communicative contexts, this understanding needs to be further refined and methodologically reconsidered. The first section challenges the traditional semiotic trichotomy, arguing that connectionist LLM architectures destabilize established hierarchies of meaning, and proposes the Human-Machine Communication (HMC) framework as a more suitable alternative. The second section examines the tension between human-centred pragmatic theories and the machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics continue to dominate, it relies on human-specific assumptions ill-suited to predictive systems like LLMs. Probabilistic pragmatics, particularly the Rational Speech Act framework, offers a more compatible teleology by focusing on optimization rather than truth-evaluation. The third section addresses the issue of substitutionalism in three forms - generalizing, linguistic, and communicative - highlighting the anthropomorphic biases that distort LLM evaluation and obscure the role of human communicative subjects. Finally, the paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding, emphasizing how users are compelled to co-construct pragmatic conditions both for the model and themselves. These arguments suggest that pragmatic theory may need to be adjusted or expanded to better account for communication involving generative AI.

</details>


### [40] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)

*Lingyuan Liu, Mengxiang Zhang*

**Main category:** cs.CL

**Keywords:** Knowledge Distillation, Language Models, Data Curation, Curriculum Learning, Efficiency

**Relevance Score:** 9

**TL;DR:** Selective Reflection Distillation (SRD) is a framework for improving Knowledge Distillation (KD) that emphasizes training data quality and model compatibility, leading to enhanced performance of compressed language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing KD methods overlook critical factors such as training data quality and student-model compatibility, which are vital for successful model compression.

**Method:** SRD dynamically evaluates and selects prompt-response pairs from training data using automated ranking based on difficulty, alongside a curriculum scheduling strategy to introduce curated data in phases during distillation.

**Key Contributions:**

	1. Proposes a novel data curation framework for knowledge distillation.
	2. Demonstrates significant performance improvements and reduced computational costs during training.
	3. Offers insights into the importance of data-centric factors in knowledge distillation.

**Result:** SRD shows consistent improvements in distilled model performance and reduces training runtime by up to 39% across various KD methods and model families.

**Limitations:** 

**Conclusion:** The framework highlights the importance of data quality and compatibility in effective LLM distillation and provides a straightforward method to enhance performance without altering underlying KD algorithms.

**Abstract:** Knowledge Distillation (KD) is a fundamental technique for compressing large language models (LLMs) into compact, efficient student models. However, existing white-box KD methods mainly focus on balancing ground truth and student-generated responses while overlooking two critical factors: training data quality and student-model compatibility. To address these limitations, we propose Selective Reflection Distillation (SRD), a novel data curation framework that leverages reflections from student models to systematically refine training data. SRD dynamically evaluates and selects prompt-response pairs by comparing ground truth data with student model outputs, selectively curating high-quality, student-compatible training instances through automated ranking based on difficulty. Furthermore, after selecting the training data, a curriculum scheduling strategy is employed to incrementally introduce these curated subsets into the distillation process at fixed intervals. As a plug-and-play enhancement, SRD consistently improves distillation outcomes across diverse white-box KD approaches and model architectures, as well as decreases computational cost significantly during KD training. Experiments on a range of language model benchmarks demonstrate SRD's consistent improvements in distilled model performance, as well as a reduction in training runtime by up to 39%, under diverse KD methods and model families. Notably, SRD operates as a plug-and-play module, enhancing sample efficiency without modifying underlying KD algorithms. Our findings highlight that data quality and compatibility are pivotal to effective and efficient distillation of LLMs, and SRD provides a principled framework to achieve both. This work advances the understanding of data-centric factors in KD and offers practical insights for enhancing the capability and efficiency of compressed LLMs.

</details>


### [41] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)

*Nizi Nazar, Ehsaneddin Asgari*

**Main category:** cs.CL

**Keywords:** Emotional Intelligence, Large Language Models, Benchmark, Fine-tuning, Human-Aligned AI

**Relevance Score:** 9

**TL;DR:** The paper introduces a four-layer taxonomy of Emotional Intelligence (EI) for LLMs and presents a benchmark called EICAP-Bench to evaluate LLMs on EI capabilities, showing limitations in current models and areas for improvement through fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in integrating Emotional Intelligence into the development of human-aligned LLMs.

**Method:** Introduces a four-layer EI taxonomy and evaluates six LLMs using the EICAP-Bench benchmark, applying LoRA adapters for fine-tuning on a large-scale dialogue dataset.

**Key Contributions:**

	1. Unified four-layer taxonomy of EI for LLMs
	2. Launch of EICAP-Bench benchmark
	3. Fine-tuning insights for enhancing EI in LLMs

**Result:** Qwen2.5-Instruct is identified as the strongest LLM in EI capabilities, and only the Appraisal layer shows significant improvement from fine-tuning.

**Limitations:** Focuses primarily on the Appraisal layer; other layers showed limited improvement.

**Conclusion:** Existing pretraining and instruction-tuning paradigms limit LLMs' emotional reasoning; targeted strategies are required for better EI alignment.

**Abstract:** Emotional Intelligence (EI) is a critical yet underexplored dimension in the development of human-aligned LLMs. To address this gap, we introduce a unified, psychologically grounded four-layer taxonomy of EI tailored for large language models (LLMs), encompassing emotional tracking, cause inference, appraisal, and emotionally appropriate response generation. Building on this framework, we present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to evaluate EI capabilities in open-source LLMs across diverse linguistic and cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma (9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench, identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale, instruction-tuned dialogue dataset, in both English and Arabic. Our statistical analysis reveals that among the five EI layers, only the Appraisal layer shows significant improvement through UC-based fine-tuning. These findings highlight the limitations of existing pretraining and instruction-tuning paradigms in equipping LLMs with deeper emotional reasoning and underscore the need for targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [42] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)

*Gunhee Cho, Yun-Gyung Cheong*

**Main category:** cs.CL

**Keywords:** Big Five, personality traits, large language models, dialogue generation, prompt engineering

**Relevance Score:** 8

**TL;DR:** Big5-Scaler is a prompt-based framework for conditioning LLMs with Big Five personality traits, enabling personality control without additional training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a method for fine-grained personality control in dialogue agents using LLMs without the need for extensive retraining.

**Method:** Embedding numeric Big Five personality trait values into natural language prompts to control dialogue generation and trait expression.

**Key Contributions:**

	1. Introduction of a prompt-based framework for personality conditioning in LLMs
	2. Consistency in trait expression and dialogue generation
	3. Insights on prompt efficiency for personality control

**Result:** The framework induces consistent and distinguishable personality traits across different models, with results influenced by prompt type and scale.

**Limitations:** Limited to controllable personality traits; results may vary across different LLM architectures and data contexts.

**Conclusion:** Big5-Scaler offers an efficient approach to building personality-aware dialogue agents by using concise prompts and lower trait intensities.

**Abstract:** We present Big5-Scaler, a prompt-based framework for conditioning large language models (LLMs) with controllable Big Five personality traits. By embedding numeric trait values into natural language prompts, our method enables fine-grained personality control without additional training. We evaluate Big5-Scaler across trait expression, dialogue generation, and human trait imitation tasks. Results show that it induces consistent and distinguishable personality traits across models, with performance varying by prompt type and scale. Our analysis highlights the effectiveness of concise prompts and lower trait intensities, providing a efficient approach for building personality-aware dialogue agents.

</details>


### [43] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)

*Renhan Zhang, Lian Lian, Zhen Qi, Guiran Liu*

**Main category:** cs.CL

**Keywords:** bias detection, large language models, interpretability, stereotypes, semantic representation

**Relevance Score:** 9

**TL;DR:** This paper proposes a method for detecting implicit biases in large language models using nested semantic representation and contextual contrast, validated on the StereoSet dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of implicit stereotypes that arise in large language models' outputs.

**Method:** The method combines nested semantic representation with a contextual contrast mechanism and uses attention weight perturbation to analyze model sensitivity to social attributes.

**Key Contributions:**

	1. Introduces an interpretable bias detection method for language models
	2. Validates the method using a comprehensive dataset (StereoSet)
	3. Demonstrates high interpretability and strong detection performance across multiple bias dimensions.

**Result:** The proposed method shows strong detection performance across various stereotype dimensions, accurately identifying bias differences while maintaining semantic consistency and output stability.

**Limitations:** 

**Conclusion:** The method provides a transparent and reliable foundation for bias detection within language models, suitable for applications requiring trust in generated content.

**Abstract:** This paper addresses the issue of implicit stereotypes that may arise during the generation process of large language models. It proposes an interpretable bias detection method aimed at identifying hidden social biases in model outputs, especially those semantic tendencies that are not easily captured through explicit linguistic features. The method combines nested semantic representation with a contextual contrast mechanism. It extracts latent bias features from the vector space structure of model outputs. Using attention weight perturbation, it analyzes the model's sensitivity to specific social attribute terms, thereby revealing the semantic pathways through which bias is formed. To validate the effectiveness of the method, this study uses the StereoSet dataset, which covers multiple stereotype dimensions including gender, profession, religion, and race. The evaluation focuses on several key metrics, such as bias detection accuracy, semantic consistency, and contextual sensitivity. Experimental results show that the proposed method achieves strong detection performance across various dimensions. It can accurately identify bias differences between semantically similar texts while maintaining high semantic alignment and output stability. The method also demonstrates high interpretability in its structural design. It helps uncover the internal bias association mechanisms within language models. This provides a more transparent and reliable technical foundation for bias detection. The approach is suitable for real-world applications where high trustworthiness of generated content is required.

</details>


### [44] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)

*Yingfeng Luo, Dingyang Lin, Junxin Wang, Ziqiang Xu, Kaiyan Chang, Tong Zheng, Bei Li, Anxiang Ma, Tong Xiao, Zhengtao Yu, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** model merging, sparsification, multi-task learning, adaptive pruning, machine learning

**Relevance Score:** 5

**TL;DR:** Introducing TADrop, an adaptive sparsification strategy for model merging that optimizes parameter pruning based on the distributional properties of tensors, enhancing performance across various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve model merging by addressing the limitations of uniform sparsity ratios which do not account for the heterogeneity of model parameters.

**Method:** TADrop applies tailored sparsity levels to parameter tensors based on their distributional properties, allowing for aggressive pruning of redundant tensors while preserving critical ones.

**Key Contributions:**

	1. Introduction of TADrop for adaptive sparsification in model merging.
	2. Demonstration of performance improvements across multiple vision and language tasks.
	3. Establishment of a new baseline for high-performance model merging strategies.

**Result:** TADrop consistently enhances performance in diverse tasks and models, achieving an average performance gain of 2.0% across 8 ViT-B/32 tasks when integrated with leading merging methods.

**Limitations:** 

**Conclusion:** TADrop offers a new baseline for effective model merging by adapting sparsification strategies to the inherent structure of the model parameters.

**Abstract:** Model merging has emerged as a compelling data-free paradigm for multi-task learning, enabling the fusion of multiple fine-tuned models into a single, powerful entity. A key technique in merging methods is sparsification, which prunes redundant parameters from task vectors to mitigate interference. However, prevailing approaches employ a ``one-size-fits-all'' strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained. To address this limitation, we introduce \textbf{TADrop} (\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive sparsification strategy that respects this heterogeneity. Instead of a global ratio, TADrop assigns a tailored sparsity level to each parameter tensor based on its distributional properties. The core intuition is that tensors with denser, more redundant distributions can be pruned aggressively, while sparser, more critical ones are preserved. As a simple and plug-and-play module, we validate TADrop by integrating it with foundational, classic, and SOTA merging methods. Extensive experiments across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and significantly boosts their performance. For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0\% across 8 ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging.

</details>


### [45] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)

*Weitao Li, Boran Xiang, Xiaolong Wang, Zhinan Gou, Weizhi Ma, Yang Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Reinforcement Learning, Unified framework, Human-Computer Interaction, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents UR2, a unified framework that integrates Retrieval-Augmented Generation and Reinforcement Learning to enhance reasoning capabilities across various tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the isolated development of RAG and RLVR and improve their applicability to broader domains by unifying them into a single framework.

**Method:** UR2 employs a difficulty-aware curriculum training that invokes retrieval for challenging problems and utilizes a hybrid knowledge access strategy combining offline corpora and LLM-generated summaries.

**Key Contributions:**

	1. Unified framework for RAG and RLVR.
	2. Difficulty-aware curriculum training for selective retrieval.
	3. Hybrid knowledge access strategy that combines offline data with generative summaries.

**Result:** UR2 demonstrates significant performance improvements over existing RAG and RL methods in multiple tasks, achieving results comparable to top models like GPT-4.

**Limitations:** 

**Conclusion:** The integration of retrieval and reasoning through UR2 enhances task adaptability and generalization, making it a robust framework for diverse applications.

**Abstract:** Large Language Models (LLMs) have shown remarkable capabilities through two complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR), which optimizes complex reasoning abilities. However, these two capabilities are often developed in isolation, and existing efforts to unify them remain narrow in scope-typically limited to open-domain QA with fixed retrieval settings and task-specific assumptions. This lack of integration constrains generalization and limits the applicability of RAG-RL methods to broader domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a general framework that unifies retrieval and reasoning through reinforcement learning. UR2 introduces two key contributions: a difficulty-aware curriculum training that selectively invokes retrieval only for challenging problems, and a hybrid knowledge access strategy combining domain-specific offline corpora with LLM-generated summaries. These components are designed to enable dynamic coordination between retrieval and reasoning, improving adaptability across a diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical, and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods, achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several benchmarks. We have released all code, models, and data at https://github.com/Tsinghua-dhy/UR2.

</details>


### [46] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)

*Vt Gvodiak*

**Main category:** cs.CL

**Keywords:** pragmatics, large language models, Human-Machine Communication, context frustration, probabilistic pragmatics

**Relevance Score:** 4

**TL;DR:** This paper redefines pragmatics as a dynamic interface in the context of large language models (LLMs), calling for a methodological reconsideration of human-machine communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To reconcile traditional pragmatic theories with the realities of communicating with LLMs, which challenge established meanings and user interactions.

**Method:** The paper critiques traditional semiotic frameworks and proposes a Human-Machine Communication (HMC) framework, analyzing the compatibility of probabilistic pragmatics with LLMs.

**Key Contributions:**

	1. Proposes the Human-Machine Communication framework as an alternative to traditional pragmatics.
	2. Introduces the concept of context frustration related to LLM interaction.
	3. Critiques reliance on Gricean-inspired pragmatics in the context of predictive systems.

**Result:** It finds that current pragmatics based on human-specific assumptions do not adequately account for the predictive nature of LLMs and introduces context frustration to explore users' experiences.

**Limitations:** The framework may not account for all communicative contexts and could be limited in its applicability across different AI models.

**Conclusion:** Pragmatic theory requires reevaluation to better facilitate interaction with generative AI systems, acknowledging the complexities of human-machine communication.

**Abstract:** The paper reconceptualizes pragmatics not as a subordinate, third dimension of meaning, but as a dynamic interface through which language operates as a socially embedded tool for action. With the emergence of large language models (LLMs) in communicative contexts, this understanding needs to be further refined and methodologically reconsidered. The first section challenges the traditional semiotic trichotomy, arguing that connectionist LLM architectures destabilize established hierarchies of meaning, and proposes the Human-Machine Communication (HMC) framework as a more suitable alternative. The second section examines the tension between human-centred pragmatic theories and the machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics continue to dominate, it relies on human-specific assumptions ill-suited to predictive systems like LLMs. Probabilistic pragmatics, particularly the Rational Speech Act framework, offers a more compatible teleology by focusing on optimization rather than truth-evaluation. The third section addresses the issue of substitutionalism in three forms - generalizing, linguistic, and communicative - highlighting the anthropomorphic biases that distort LLM evaluation and obscure the role of human communicative subjects. Finally, the paper introduces the concept of context frustration to describe the paradox of increased contextual input paired with a collapse in contextual understanding, emphasizing how users are compelled to co-construct pragmatic conditions both for the model and themselves. These arguments suggest that pragmatic theory may need to be adjusted or expanded to better account for communication involving generative AI.

</details>


### [47] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)

*Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Injection, Synthetic Data, Catastrophic Forgetting, Retrieval-Augmented Generation

**Relevance Score:** 9

**TL;DR:** This paper investigates methods for injecting small amounts of unstructured information into large language models (LLMs) and examines the impact on knowledge acquisition and catastrophic forgetting.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of effectively updating LLMs with limited amounts of text data despite their dependency on vast corpora is a key motivation for this research.

**Method:** The authors utilized a dataset of recent news, ensuring no overlap with the model's pre-training data, to probe knowledge acquisition through question-answer pairs and explored various augmentation algorithms for generating synthetic data.

**Key Contributions:**

	1. Investigation of knowledge injection methods into LLMs using limited data
	2. Analysis of catastrophic forgetting in small-data regimes
	3. Demonstration of LLMs generating effective synthetic training data for self-improvement

**Result:** The findings show that merely continuing pre-training on limited data yields modest improvements, while diverse textual variations significantly enhance learning new facts and reveal the effects of catastrophic forgetting.

**Limitations:** The study focuses on a specific methodology and dataset, which may limit the generalizability of the results across different domains or task types.

**Conclusion:** The research demonstrates that self-improvement through synthetic data generation by models themselves is possible and highlights the sensitivity of RAG-based methods compared to parametric approaches.

**Abstract:** Large language models (LLMs) often require vast amounts of text to effectively acquire new knowledge. While continuing pre-training on large corpora or employing retrieval-augmented generation (RAG) has proven successful, updating an LLM with only a few thousand or million tokens remains challenging. In this work, we investigate the task of injecting small, unstructured information into LLMs and its relation to the catastrophic forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap with the model's pre-training data -- to evaluate the knowledge acquisition by probing the model with question-answer pairs related the learned information. Starting from a continued pre-training baseline, we explored different augmentation algorithms to generate synthetic data to improve the knowledge acquisition capabilities. Our experiments show that simply continuing pre-training on limited data yields modest improvements, whereas exposing the model to diverse textual variations significantly improves the learning of new facts -- particularly with methods that induce greater variability through diverse prompting. Furthermore, we shed light on the forgetting phenomenon in small-data regimes, illustrating the delicate balance between learning new content and retaining existing capabilities. We also confirm the sensitivity of RAG-based approaches for knowledge injection, which often lead to greater degradation on control datasets compared to parametric methods. Finally, we demonstrate that models can generate effective synthetic training data themselves, suggesting a pathway toward self-improving model updates. All code and generated data used in our experiments are publicly available, providing a resource for studying efficient knowledge injection in LLMs with limited data at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [48] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)

*Ali Sarabadani, Maryam Abdollahi Shamami, Hamidreza Sadeghsalehi, Borhan Asadi, Saba Hesaraki*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Medical Diagnosis, Personalized Treatment, Adaptive Semantic Fusion

**Relevance Score:** 9

**TL;DR:** The DKG-LLM framework combines a dynamic knowledge graph with LLM for improved medical diagnosis and treatment recommendations, achieving high accuracy on real-world datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance medical diagnosis and personalized treatment recommendations using LLMs and knowledge graphs.

**Method:** Integrating a dynamic knowledge graph with a large language model using the Adaptive Semantic Fusion Algorithm to process heterogeneous medical data and maintain scalability.

**Key Contributions:**

	1. Introduction of the DKG-LLM framework for medical applications
	2. Use of Adaptive Semantic Fusion Algorithm for knowledge graph creation
	3. Demonstrated high accuracy in diagnosing and recommending treatments based on diverse data

**Result:** Achieved a diagnostic accuracy of 84.19% and a treatment recommendation accuracy of 89.63% on real-world datasets like MIMIC-III and PubMed.

**Limitations:** 

**Conclusion:** DKG-LLM is a reliable tool that effectively handles complex medical data and improves diagnosis and treatment recommendation accuracy.

**Abstract:** Large Language Models (LLMs) have grown exponentially since the release of ChatGPT. These models have gained attention due to their robust performance on various tasks, including language processing tasks. These models achieve understanding and comprehension of tasks by training billions of parameters. The development of these models is a transformative force in enhancing natural language understanding and has taken a significant step towards artificial general intelligence (AGI). In this study, we aim to present the DKG-LLM framework. The DKG-LLM framework introduces a groundbreaking approach to medical diagnosis and personalized treatment recommendations by integrating a dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data (including clinical reports and PubMed articles) and patient records dynamically generate a knowledge graph consisting of 15,964 nodes in 13 distinct types (e.g., diseases, symptoms, treatments, patient profiles) and 127,392 edges in 26 relationship types (e.g., causal, therapeutic, association). ASFA utilizes advanced probabilistic models, Bayesian inference, and graph optimization to extract semantic information, dynamically updating the graph with approximately 150 new nodes and edges in each data category while maintaining scalability with up to 987,654 edges. Real-world datasets, including MIMIC-III and PubMed, were utilized to evaluate the proposed architecture. The evaluation results show that DKG-LLM achieves a diagnostic accuracy of 84.19%. The model also has a treatment recommendation accuracy of 89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and transformative tool that handles noisy data and complex multi-symptom diseases, along with feedback-based learning from physician input.

</details>


### [49] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)

*Lai Jiang, Yuekang Li, Xiaohan Zhang, Youtao Ding, Li Pan*

**Main category:** cs.CL

**Keywords:** jailbreak evaluation, LLM red teaming, multi-dimensional framework

**Relevance Score:** 7

**TL;DR:** Introducing SceneJailEval, a scenario-adaptive multi-dimensional framework for jailbreak evaluation that overcomes limitations of existing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for precise jailbreak evaluation methods in LLM red teaming and research, with a focus on quantifying harm intensity rather than binary classifications.

**Method:** SceneJailEval employs a multi-dimensional framework designed to adapt to various scenarios, overcoming the uniform criteria limitation of previous approaches.

**Key Contributions:**

	1. Groundbreaking scenario-adaptive multi-dimensional jailbreak evaluation framework
	2. A comprehensive 14-scenario dataset for diverse jailbreak variants
	3. State-of-the-art performance metrics surpassing existing evaluation methods

**Result:** SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on the full-scenario dataset and 0.995 on JBB, which is an improvement over existing methods.

**Limitations:** 

**Conclusion:** SceneJailEval demonstrates significant advantages in precision and applicability across diverse contexts, filling a critical gap in current evaluation frameworks.

**Abstract:** Precise jailbreak evaluation is vital for LLM red teaming and jailbreak research. Current approaches employ binary classification ( e.g., string matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no" labels without quantifying harm intensity. Existing multi-dimensional frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness) apply uniform evaluation criteria across scenarios, resulting in scenario-specific mismatches--for instance, "Relative Truthfulness" is irrelevant to "hate speech"--which compromise evaluation precision. To tackle these limitations, we introduce SceneJailEval, with key contributions: (1) A groundbreaking scenario-adaptive multi-dimensional framework for jailbreak evaluation, overcoming the critical "one-size-fits-all" constraint of existing multi-dimensional methods, and featuring strong extensibility to flexibly adapt to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset with diverse jailbreak variants and regional cases, filling the long-standing gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3) SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over prior SOTA), surpassing accuracy limits of existing evaluation methods in heterogeneous scenarios and confirming its advantage.

</details>


### [50] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)

*Nizi Nazar, Ehsaneddin Asgari*

**Main category:** cs.CL

**Keywords:** Emotional Intelligence, large language models, benchmark, fine-tuning, human-aligned AI

**Relevance Score:** 8

**TL;DR:** This paper introduces a four-layer taxonomy of Emotional Intelligence (EI) for large language models (LLMs) and presents a benchmark, EICAP-Bench, to evaluate EI capabilities in open-source LLMs, identifying the strengths and weaknesses of various models in emotional reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and improve emotional intelligence in large language models (LLMs) due to its critical yet underdeveloped role in human-aligned AI.

**Method:** Developed a four-layer taxonomy of EI for LLMs and created the EICAP-Bench benchmark to evaluate various LLMs' capabilities in emotional reasoning. Conducted statistical analysis on models fine-tuned with specific datasets.

**Key Contributions:**

	1. Unified taxonomy of EI for LLMs
	2. Creation of the EICAP-Bench benchmark
	3. Evaluation of multiple LLMs to assess EI capabilities

**Result:** Identified that only the Appraisal layer of EI showed significant improvement after fine-tuning models like Qwen2.5 using targeted instruction datasets across linguistic contexts.

**Limitations:** Only one EI layer showed significant improvement; broader improvement across all layers was not observed.

**Conclusion:** Current pretraining methods are inadequate for developing deep emotional reasoning in LLMs, indicating a need for specialized data and targeted modeling approaches to enhance emotional intelligence capabilities.

**Abstract:** Emotional Intelligence (EI) is a critical yet underexplored dimension in the development of human-aligned LLMs. To address this gap, we introduce a unified, psychologically grounded four-layer taxonomy of EI tailored for large language models (LLMs), encompassing emotional tracking, cause inference, appraisal, and emotionally appropriate response generation. Building on this framework, we present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to evaluate EI capabilities in open-source LLMs across diverse linguistic and cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma (9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench, identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale, instruction-tuned dialogue dataset, in both English and Arabic. Our statistical analysis reveals that among the five EI layers, only the Appraisal layer shows significant improvement through UC-based fine-tuning. These findings highlight the limitations of existing pretraining and instruction-tuning paradigms in equipping LLMs with deeper emotional reasoning and underscore the need for targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [51] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)

*Richard Willats, Josh Pennington, Aravind Mohan, Bertie Vidgen*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, content moderation, hate speech detection, policy adaptability, explainability

**Relevance Score:** 9

**TL;DR:** This paper presents a Contextual Policy Engine (CPE) that utilizes Retrieval-Augmented Generation (RAG) for flexible content moderation, allowing dynamic policy updates without retraining.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance content moderation systems' ability to adapt to evolving policies without incurring the costs of retraining.

**Method:** The paper introduces RAG for classification tasks, specifically in hate speech detection, shifting the focus from static classification to contextually evaluating content against dynamic policy guidelines.

**Key Contributions:**

	1. Introduction of Contextual Policy Engine (CPE) for content moderation
	2. Demonstration of dynamic policy updates without retraining
	3. Validation of inherent explainability via policy retrieval

**Result:** The CPE demonstrates classification accuracy comparable to leading systems, provides explainability through policy segment retrieval, and allows for dynamic updates to policies without retraining.

**Limitations:** 

**Conclusion:** RAG can make classification processes more adaptable and transparent, improving content moderation strategies and other classification problems.

**Abstract:** Robust content moderation requires classification systems that can quickly adapt to evolving policies without costly retraining. We present classification using Retrieval-Augmented Generation (RAG), which shifts traditional classification tasks from determining the correct category in accordance with pre-trained parameters to evaluating content in relation to contextual knowledge retrieved at inference. In hate speech detection, this transforms the task from "is this hate speech?" to "does this violate the hate speech policy?"   Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates this approach and offers three key advantages: (1) robust classification accuracy comparable to leading commercial systems, (2) inherent explainability via retrieved policy segments, and (3) dynamic policy updates without model retraining. Through three experiments, we demonstrate strong baseline performance and show that the system can apply fine-grained policy control by correctly adjusting protection for specific identity groups without requiring retraining or compromising overall performance. These findings establish that RAG can transform classification into a more flexible, transparent, and adaptable process for content moderation and wider classification problems.

</details>


### [52] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)

*Keummin Ka, Junhyeong Park, Jahyun Jeon, Youngjae Yu*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Causal Reasoning, Infographics, Benchmark, Multimodal AI

**Relevance Score:** 7

**TL;DR:** Introducing InfoCausalQA, a benchmark for evaluating causal reasoning in Vision-Language Models (VLMs) using infographics that combine visual and textual data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in causal inference capabilities of multimodal AI systems, particularly in Vision-Language Models.

**Method:** The benchmark consists of two tasks: Task 1 evaluates quantitative causal reasoning, while Task 2 assesses semantic causal reasoning involving various causal relations. It includes 494 infographic-text pairs generated and curated for high-quality QA pairs that require real visual understanding.

**Key Contributions:**

	1. Introduction of InfoCausalQA benchmark for causal reasoning evaluation in VLMs
	2. Focus on both quantitative and semantic aspects of causal inference in multimodal contexts
	3. Highlighting the deficiencies of existing VLMs in reasoning with infographic-based information

**Result:** Current VLMs underperform in both computational and semantic causal reasoning compared to human performance, indicating substantial limitations in utilizing infographic information for causal inference.

**Limitations:** The benchmark only evaluates VLM performance on the selected infographics, which may limit the generalizability of the findings to broader multimodal contexts.

**Conclusion:** There is a pressing need to enhance the causal reasoning abilities of multimodal AI systems, as demonstrated by the substantial gap identified in this evaluation.

**Abstract:** Recent advances in Vision-Language Models (VLMs) have demonstrated impressive capabilities in perception and reasoning. However, the ability to perform causal inference -- a core aspect of human cognition -- remains underexplored, particularly in multimodal settings. In this study, we introduce InfoCausalQA, a novel benchmark designed to evaluate causal reasoning grounded in infographics that combine structured visual data with textual context. The benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning based on inferred numerical trends, while Task 2 targets semantic causal reasoning involving five types of causal relations: cause, effect, intervention, counterfactual, and temporal. We manually collected 494 infographic-text pairs from four public sources and used GPT-4o to generate 1,482 high-quality multiple-choice QA pairs. These questions were then carefully revised by humans to ensure they cannot be answered based on surface-level cues alone but instead require genuine visual grounding. Our experimental results reveal that current VLMs exhibit limited capability in computational reasoning and even more pronounced limitations in semantic causal reasoning. Their significantly lower performance compared to humans indicates a substantial gap in leveraging infographic-based information for causal inference. Through InfoCausalQA, we highlight the need for advancing the causal reasoning abilities of multimodal AI systems.

</details>


### [53] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)

*Theresa Pekarek Rosin, Burak Can Kaplan, Stefan Wermter*

**Main category:** cs.CL

**Keywords:** intent recognition, elderly speakers, German speech, synthetic data, large language models

**Relevance Score:** 7

**TL;DR:** This paper presents a novel approach for intent recognition from speech by elderly German speakers, employing an adapted Whisper ASR model and Transformer-based LLMs. It demonstrates significant improvements in classification performance using synthetic data.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of existing intent recognition systems that focus on short commands and predominantly English language, especially for elderly German speakers.

**Method:** Combines an adapted Whisper ASR model fine-tuned on elderly German speech with Transformer-based language models trained on synthetic text datasets generated by LeoLM, Llama3, and ChatGPT.

**Key Contributions:**

	1. Development of a robust intent recognition approach for elderly German speech
	2. Demonstration of the benefits of synthetic data from LLMs
	3. Comparative analysis showing the effectiveness of LeoLM over larger models for this task.

**Result:** Synthetic LLM-generated data improves classification performance and robustness to different speaking styles and unseen vocabulary, with LeoLM outperforming ChatGPT in dataset quality for German intent recognition.

**Limitations:** 

**Conclusion:** Generative AI can effectively bridge data gaps in low-resource domains, and detailed documentation of the data generation and training process is provided for transparency.

**Abstract:** Intent recognition (IR) for speech commands is essential for artificial intelligence (AI) assistant systems; however, most existing approaches are limited to short commands and are predominantly developed for English. This paper addresses these limitations by focusing on IR from speech by elderly German speakers. We propose a novel approach that combines an adapted Whisper ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based language models trained on synthetic text datasets generated by three well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To evaluate the robustness of our approach, we generate synthetic speech with a text-to-speech model and conduct extensive cross-dataset testing. Our results show that synthetic LLM-generated data significantly boosts classification performance and robustness to different speaking styles and unseen vocabulary. Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the much larger ChatGPT (175B) in dataset quality for German intent recognition. Our approach demonstrates that generative AI can effectively bridge data gaps in low-resource domains. We provide detailed documentation of our data generation and training process to ensure transparency and reproducibility.

</details>


### [54] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)

*Ruichong Zhang*

**Main category:** cs.CL

**Keywords:** intellectual property, large language models, plagiarism detection, matrix analysis, statistical significance

**Relevance Score:** 7

**TL;DR:** This paper proposes a novel method called Matrix-Driven Instant Review (MDIR) to detect plagiarism in large language models (LLMs) by leveraging matrix analysis and Large Deviation Theory.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns about intellectual property (IP) in large language models have grown, particularly surrounding plagiarism and authorship issues that can harm original developers.

**Method:** MDIR uses matrix analysis and Large Deviation Theory to achieve accurate reconstruction of weight relationships and rigorous $p$-value estimation, focusing on weight similarity without requiring full model inference.

**Key Contributions:**

	1. Introduction of the Matrix-Driven Instant Review (MDIR) method.
	2. Improved accuracy in reconstructing weight relationships compared to existing methods.
	3. Ability to compute rigorous statistical significance measures for weight similarities.

**Result:** Experiments show that MDIR reliably detects plagiarism even after significant model transformations, such as random permutations and continual pretraining, with all detections performed efficiently on a single PC.

**Limitations:** 

**Conclusion:** MDIR provides a more effective and accessible solution for detecting LLM plagiarism, addressing limitations of existing methods.

**Abstract:** In recent years, concerns about intellectual property (IP) in large language models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct weight copying, upcycling, pruning, or continual pretraining) and claiming authorship without properly attributing to the original license, is a serious misconduct that can lead to significant financial and reputational harm to the original developers. However, existing methods for detecting LLM plagiarism fall short in key areas. They fail to accurately reconstruct weight correspondences, lack the ability to compute statistical significance measures such as $p$-values, and may mistakenly flag models trained on similar data as being related. To address these limitations, we propose Matrix-Driven Instant Review (MDIR), a novel method that leverages matrix analysis and Large Deviation Theory. MDIR achieves accurate reconstruction of weight relationships, provides rigorous $p$-value estimation, and focuses exclusively on weight similarity without requiring full model inference. Experimental results demonstrate that MDIR reliably detects plagiarism even after extensive transformations, such as random permutations and continual pretraining with trillions of tokens. Moreover, all detections can be performed on a single PC within an hour, making MDIR both efficient and accessible.

</details>


### [55] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)

*Yanbin Wei, Jiangyue Yan, Chun Kang, Yang Chen, Hua Liu, James T. Kwok, Yu Zhang*

**Main category:** cs.CL

**Keywords:** Multimodal Models, Graph Question Answering, DynamicTRF

**Relevance Score:** 7

**TL;DR:** The paper introduces DynamicTRF, a framework that tailors Topology Representation Forms (TRFs) for zero-shot graph question-answering (QA) to improve accuracy and conciseness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods using a single type of TRF for graph QA lead to suboptimal responses due to neglecting model/task-specific preferences.

**Method:** The authors analyze existing TRFs and propose a set of tailored TRFs ($F_{ZS}$). They then develop a TRF Preference dataset to rank TRFs by Graph Response Efficiency (GRE) and train a TRF router for adaptive assignment during inference.

**Key Contributions:**

	1. Introduction of tailored TRFs for zero-shot graph QA
	2. Development of a new metric, Graph Response Efficiency (GRE)
	3. Creation of the DynamicTRF framework that improves graph QA accuracy

**Result:** DynamicTRF significantly improves the accuracy of zero-shot graph QA across various tasks based on extensive experiments.

**Limitations:** 

**Conclusion:** The proposed framework enhances graph QA performance of Large Multimodal Models by employing tailored TRFs and a novel router mechanism.

**Abstract:** Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities in diverse domain question-answering (QA) tasks, including graph QA that involves complex graph topologies. However, most current approaches use only a single type of graph representation, namely Topology Representation Form (TRF), such as prompt-unified text descriptions or style-fixed visual styles. Those "one-size-fits-all" approaches fail to consider the specific preferences of different models or tasks, often leading to incorrect or overly long responses. To address this, we first analyze the characteristics and weaknesses of existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency (GRE), which measures the balance between the performance and the brevity in graph QA. Built on these, we develop the DynamicTRF framework, which aims to improve both the accuracy and conciseness of graph QA. To be specific, DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based on their GRE scores, to probe the question-specific TRF preferences. Then it trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from $F_{ZS}$ for each question during the inference. Extensive experiments across 7 in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms of accuracy

</details>


### [56] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)

*Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia*

**Main category:** cs.CL

**Keywords:** cyberbullying detection, large language models, aggression detection, social media, natural language processing

**Relevance Score:** 7

**TL;DR:** The paper explores enhancing large language models' performance in detecting cyberbullying by integrating aggression detection as an auxiliary task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical challenge of cyberbullying detection on social media, which is difficult due to its subtlety and variability.

**Method:** The study conducted experiments on various datasets employing instruction-tuned LLMs, evaluating strategies including zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL), ultimately focusing on an enriched prompt pipeline approach.

**Key Contributions:**

	1. Introduced an enriched prompt pipeline approach for cyberbullying detection.
	2. Showed the value of integrating aggression detection as an auxiliary task.
	3. Demonstrated superior performance of the enriched approach over standard fine-tuning methods.

**Result:** Preliminary results show that the enriched prompt pipeline outperforms standard LoRA fine-tuning, suggesting the effectiveness of using aggression-informed context in enhancing cyberbullying detection.

**Limitations:** 

**Conclusion:** The findings support the idea that auxiliary tasks, like aggression detection, can significantly enhance the generalization and performance of LLMs in sensitive applications.

**Abstract:** Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.

</details>


### [57] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)

*Anubhav Jangra, Bahareh Sarrafzadeh, Adrian de Wynter, Silviu Cucerzan, Sujay Kumar Jauhar*

**Main category:** cs.CL

**Keywords:** style personalized text generation, evaluation metrics, style embeddings, LLM-as-judge, author discrimination

**Relevance Score:** 6

**TL;DR:** This paper explores the evaluation of low-resource author style personalized text generation, questioning traditional metrics like BLEU and ROUGE, and proposes an ensemble of diverse evaluation metrics for better assessment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in effective evaluation methods for low-resource author style personalized text generation and the limitations of existing metrics.

**Method:** The authors develop a style discrimination benchmark encompassing eight writing tasks and evaluate various metrics and their ensembles across domain discrimination, authorship attribution, and LLM discrimination.

**Key Contributions:**

	1. Introduction of a style discrimination benchmark with eight writing tasks.
	2. Critical analysis of traditional evaluation metrics for style personalized text generation.
	3. Proposal of a diverse ensemble of evaluation metrics for better assessment.

**Result:** The study finds that an ensemble of diverse evaluation metrics offers a more holistic evaluation approach compared to traditional metrics like BLEU and ROUGE.

**Limitations:** The paper mainly focuses on low-resource settings, which may not encompass all aspects of text generation evaluation.

**Conclusion:** The paper concludes that adopting an ensemble of evaluation metrics is crucial for effectively assessing style personalized text generation.

**Abstract:** While prior research has built tools and benchmarks towards style personalized text generation, there has been limited exploration of evaluation in low-resource author style personalized text generation space. Through this work, we question the effectiveness of the widely adopted evaluation metrics like BLEU and ROUGE, and explore other evaluation paradigms such as style embeddings and LLM-as-judge to holistically evaluate the style personalized text generation task. We evaluate these metrics and their ensembles using our style discrimination benchmark, that spans eight writing tasks, and evaluates across three settings, domain discrimination, authorship attribution, and LLM personalized vs non-personalized discrimination. We provide conclusive evidence to adopt ensemble of diverse evaluation metrics to effectively evaluate style personalized text generation.

</details>


### [58] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)

*Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He*

**Main category:** cs.CL

**Keywords:** Emotionally Supportive Role-Playing, Large Language Models, Anime Characters, Human-Computer Interaction, Dataset

**Relevance Score:** 9

**TL;DR:** This paper presents ChatAnime, the first dataset designed for evaluating Emotionally Supportive Role-Playing (ESRP) interactions using large language models (LLMs) with anime characters.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in research regarding emotionally supportive interactions with LLMs in the context of role-playing, specifically using anime characters known for their distinct personalities.

**Method:** The authors created the ChatAnime dataset by selecting 20 anime characters and designing 60 emotion-centric questions. They collected dialogue data through interactions between 40 anime enthusiasts and 10 LLMs, structured with a comprehensive evaluation system.

**Key Contributions:**

	1. Introduction of the first ESRP dataset (ChatAnime) for evaluating LLMs with anime characters.
	2. Development of a structured evaluation system featuring fine-grained metrics.
	3. Insights into LLMs' capabilities versus human fans in emotional support and role-playing.

**Result:** Experimental results showed top-performing LLMs outperformed human fans in both role-playing and emotional support, but humans excelled in providing diverse responses.

**Limitations:** The dataset is focused on specific anime characters, which may limit generalizability to other contexts or character types.

**Conclusion:** The study provides valuable datasets for future research on optimizing LLMs for emotionally supportive role-playing, highlighting areas of strength and weakness between humans and LLMs.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at https://github.com/LanlanQiu/ChatAnime.

</details>


### [59] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)

*Haoran Shi, Hongwei Yao, Shuo Shao, Shaopeng Jiao, Ziqi Peng, Zhan Qin, Cong Wang*

**Main category:** cs.CL

**Keywords:** large language models, security, conversation drift, anomaly detection, privacy

**Relevance Score:** 9

**TL;DR:** The paper proposes SecMCP, a secure framework for detecting and quantifying conversation drift in large language models (LLMs) by addressing critical security and privacy risks associated with dynamic tool integration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate security and privacy risks posed by the Model Context Protocol in LLMs, such as adversarial content leading to conversation hijacking and misinformation.

**Method:** SecMCP models LLM activation vectors within a latent polytope space to identify anomalous shifts in conversational dynamics, allowing for the detection of hijacking and misleading information.

**Key Contributions:**

	1. Systematic categorization of MCP security threats
	2. Novel latent polytope-based methodology for quantifying conversation drift
	3. Empirical validation of SecMCP's efficacy

**Result:** SecMCP demonstrated robust detection capabilities with AUROC scores exceeding 0.915 on benchmark datasets while maintaining usability.

**Limitations:** 

**Conclusion:** SecMCP addresses significant gaps in existing defenses against LLM security threats by providing a systematic categorization, a new methodology for quantifying conversation drift, and empirical validation of its effectiveness.

**Abstract:** The Model Context Protocol (MCP) enhances large language models (LLMs) by integrating external tools, enabling dynamic aggregation of real-time data to improve task execution. However, its non-isolated execution context introduces critical security and privacy risks. In particular, adversarially crafted content can induce tool poisoning or indirect prompt injection, leading to conversation hijacking, misinformation propagation, or data exfiltration. Existing defenses, such as rule-based filters or LLM-driven detection, remain inadequate due to their reliance on static signatures, computational inefficiency, and inability to quantify conversational hijacking. To address these limitations, we propose SecMCP, a secure framework that detects and quantifies conversation drift, deviations in latent space trajectories induced by adversarial external knowledge. By modeling LLM activation vectors within a latent polytope space, SecMCP identifies anomalous shifts in conversational dynamics, enabling proactive detection of hijacking, misleading, and data exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA), demonstrating robust detection with AUROC scores exceeding 0.915 while maintaining system usability. Our contributions include a systematic categorization of MCP security threats, a novel latent polytope-based methodology for quantifying conversation drift, and empirical validation of SecMCP's efficacy.

</details>


### [60] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)

*Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Procedural Memory, Lifelong Learning, Artificial Intelligence, Agent Performance

**Relevance Score:** 9

**TL;DR:** This paper presents Memp, a learned procedural memory system for LLM-based agents that allows continuous updating and improves task efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of brittle procedural memory in LLM agents by creating a system that supports lifelong learning and adaptability.

**Method:** Memp distills past agent trajectories into detailed instructions and script-like abstractions, and incorporates strategies for Building, Retrieving, and Updating procedural memory.

**Key Contributions:**

	1. Development of a learnable, updatable procedural memory system for LLM agents
	2. Empirical validation of performance gains through memory refinement
	3. Demonstration of performance retention across models with different strengths.

**Result:** Empirical evaluations show that refined procedural memory leads to increased success rates and efficiency on tasks such as TravelPlanner and ALFWorld.

**Limitations:** 

**Conclusion:** The study demonstrates that procedural memory from a stronger model enhances performance even when migrated to a weaker model, showing the value of learned memory repositories.

**Abstract:** Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.

</details>


### [61] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)

*Andrea Nasuto, Stefano Maria Iacus, Francisco Rowe, Devika Jain*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cross-lingual topic detection, Immigration tweets, Fine-tuning, Open-source LLMs

**Relevance Score:** 9

**TL;DR:** This paper investigates the capability of fine-tuned L LaMA models to perform cross-lingual topic detection in immigration-related discourse, showing that minimal language-specific training can yield significant results across unseen languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how fine-tuning LLMs on limited languages can influence their performance in classifying cross-lingual, culturally-specific social media content.

**Method:** Fine-tuning lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual datasets for classifying immigration-related tweets in 13 languages, evaluating the ability to overcome pre-training biases with minimal fine-tuning.

**Key Contributions:**

	1. Demonstrated cross-lingual capabilities of LLMs with minimal fine-tuning.
	2. Provided open-source models that are efficient and cost-effective for research.
	3. Challenged the belief that extensive multilingual training is necessary for effective classification tasks.

**Result:** Fine-tuned LLMs can reliably classify immigration-related content in unseen languages, with notable performance improvements by using multilingual fine-tuning and minimal exposure to other languages during training.

**Limitations:** The study primarily focuses on immigration-related tweets, which may limit the generalizability of findings to other domains.

**Conclusion:** The study concludes that limited language coverage allows for effective topic-level generalization in LLMs and that biases toward dominant languages can be addressed with minor fine-tuning efforts.

**Abstract:** Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.

</details>


### [62] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)

*Abolfazl Ansari, Delvin Ce Zhang, Nafis Irtiza Tripto, Dongwon Lee*

**Main category:** cs.CL

**Keywords:** Generative AI, journalism, large language models, text detection, linguistic analysis

**Relevance Score:** 6

**TL;DR:** The study analyzes the increasing use of Generative AI in journalism, focusing on AI-generated content in over 40,000 news articles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns regarding journalistic integrity and authorship due to the rise of Generative AI, especially LLMs.

**Method:** The study examines AI-generated content across over 40,000 news articles using three advanced AI-text detectors (Binoculars, Fast-Detect GPT, GPTZero).

**Key Contributions:**

	1. Comprehensive analysis of AI-generated content across diverse news media.
	2. Identification of specific patterns in LLM usage within news articles.
	3. Linguistic impact assessment of GenAI on writing styles in journalism.

**Result:** A substantial increase of GenAI use in local and college news articles, with LLMs primarily used in introductions. Linguistic analysis indicates improved word richness and readability, but decreased formality.

**Limitations:** 

**Conclusion:** While GenAI enhances certain textual qualities, it raises concerns about writing uniformity and authentic journalistic voices in media.

**Abstract:** The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.

</details>


### [63] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)

*Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Inference Optimization, Token Pruning

**Relevance Score:** 8

**TL;DR:** SlimInfer offers a novel framework for accelerating inference in Large Language Models (LLMs) by pruning less critical prompt tokens during the forward pass.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Long-context inference in LLMs is constrained by high computational demands and existing methods that process all hidden states limit efficiency.

**Method:** SlimInfer employs a dynamic fine-grained pruning mechanism that removes redundant tokens during the forward pass, utilizing an information diffusion phenomenon to maintain semantic integrity.

**Key Contributions:**

	1. Introduction of SlimInfer for token pruning during inference
	2. Dynamic layer-wise pruning mechanism
	3. Significant speed and latency improvements for LLMs

**Result:** Achieves up to 2.53x speedup in time-to-first-token (TTFT) and 1.88x reduction in end-to-end latency for LLaMA3.1-8B-Instruct on a single RTX 4090.

**Limitations:** 

**Conclusion:** SlimInfer significantly improves efficiency in LLM inference without performance loss on LongBench, with code release planned upon acceptance.

**Abstract:** Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.

</details>


### [64] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)

*GLM-4. 5 Team, :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang*

**Main category:** cs.CL

**Keywords:** large language model, Mixture-of-Experts, reasoning, agentic AI, open-source

**Relevance Score:** 8

**TL;DR:** GLM-4.5 is a large language model with innovative MoE architecture and hybrid reasoning, showing superior performance across various AI benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To advance research in reasoning and agentic AI systems while providing strong performance in coding and reasoning tasks.

**Method:** The model was trained on 23 trillion tokens using multi-stage training with expert model iteration and reinforcement learning.

**Key Contributions:**

	1. Introduction of MoE architecture with hybrid reasoning methods.
	2. Strong performance metrics with significantly fewer parameters.
	3. Release of open-source code and models for further research.

**Result:** GLM-4.5 achieved 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified, ranking 3rd overall among evaluated models with fewer parameters than competitors.

**Limitations:** 

**Conclusion:** GLM-4.5 and its compact version aim to enhance research in agentic AI and reasoning applications.

**Abstract:** We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [65] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)

*Guimin Hu, Daniel Hershcovich, Hasti Seifi*

**Main category:** cs.CL

**Keywords:** haptic captioning, multimodal sensory language model, LLaMA, reinforcement learning, human feedback

**Relevance Score:** 8

**TL;DR:** HapticLLaMA is a multimodal language model that generates natural language descriptions from haptic signals for applications in accessibility and virtual reality.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Address the underexplored area of haptic signals in multimodal research, particularly for generating language descriptions from vibrations.

**Method:** Developed HapticLLaMA, which uses two types of haptic tokenizers to convert vibration signals into discrete units for integration with the LLaMA model, trained through supervised fine-tuning and reinforcement learning from human feedback.

**Key Contributions:**

	1. Formalization of the haptic captioning task.
	2. Introduction of HapticLLaMA for interpreting haptic signals into language descriptions.
	3. Dual training methodology combining supervised fine-tuning and reinforcement learning.

**Result:** HapticLLaMA achieved a METEOR score of 59.98 and a BLEU-4 score of 32.06, with over 61% of generated captions rated above 3.5 by human evaluators, showing significant improvement with RLHF.

**Limitations:** 

**Conclusion:** The study demonstrates the capability of large language models to effectively interpret haptic signals, suggesting promising applications in VR and accessibility.

**Abstract:** Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMA's captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.

</details>


### [66] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)

*Yilun Hua, Evan Wang, Yoav Artzi*

**Main category:** cs.CL

**Keywords:** Language Models, Convention Formation, Cognitive Interaction, Fine-tuning, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces a targeted fine-tuning process to enhance convention formation in LLMs, demonstrating significant improvements in multi-turn interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLMs' performance in forming ad-hoc conventions during multi-turn interactions, as current models lag behind human capabilities.

**Method:** We developed a post-training fine-tuning process based on heuristically identified demonstrations of convention formation.

**Key Contributions:**

	1. Introduction of a post-training process for fine-tuning LLMs on convention formation.
	2. Development of new benchmarks for evaluating convention formation in LLMs.
	3. Demonstration of significant improvements in LLMs' interaction capabilities through targeted training.

**Result:** Our studies show significantly improved convention formation abilities in post-trained LLMs across two evaluation methods, including a cognitively-motivated interaction benchmark and a document-grounded reference completion task.

**Limitations:** The proposed methods may require substantial data for effective fine-tuning and may not generalize across all contexts of communication.

**Conclusion:** Post-training fine-tuning effectively enhances LLMs' ability to form conventions, bridging the gap between human and machine interaction capabilities.

**Abstract:** Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.

</details>


### [67] [Benchmarking LLMs on the Semantic Overlap Summarization Task](https://arxiv.org/abs/2402.17008)

*John Salvador, Naman Bansal, Mousumi Akter, Souvika Sarkar, Anupam Das, Shubhra Kanti Karmaker*

**Main category:** cs.CL

**Keywords:** Semantic Overlap Summarization, Large Language Models, PrivacyPolicyPairs, dataset, summarization

**Relevance Score:** 8

**TL;DR:** The paper benchmarks large language models on Semantic Overlap Summarization, introduces the PrivacyPolicyPairs dataset, and evaluates LLM-generated summaries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of large language models on the constrained task of Semantic Overlap Summarization and provide a new dataset for benchmarking.

**Method:** The study uses the TELeR prompting taxonomy to generate and evaluate over 905,216 LLM-generated summaries from two SOS datasets across different domains, with human evaluation on 540 samples.

**Key Contributions:**

	1. Introduction of the PrivacyPolicyPairs dataset for SOS benchmarking
	2. Conducted extensive evaluation of LLMs on SOS tasks
	3. Analysis of performance and evaluation methods for LLM-generated summaries

**Result:** The performance of various LLMs on the SOS task is analyzed, alongside the reliability of automatic evaluation methods.

**Limitations:** The focus is exclusively on the SOS task and may not generalize to other summarization forms; the evaluation may be limited by the datasets used.

**Conclusion:** The study provides insights into LLM capabilities for summarization tasks and highlights the need for robust evaluation methods; the datasets and code are available for further research.

**Abstract:** Semantic Overlap Summarization (SOS) is a constrained multi-document summarization task, where the constraint is to capture the common/overlapping information between two alternative narratives. In this work, we perform a benchmarking study of popular Large Language Models (LLMs) exclusively on the SOS task. Additionally, we introduce the PrivacyPolicyPairs (3P) dataset to expand the space of SOS benchmarks in terms of quantity and variety. This dataset provides 135 high-quality SOS data samples sourced from privacy policy documents. We then use a standard prompting taxonomy called TELeR to create and evaluate 905,216 distinct LLM-generated summaries over two SOS datasets from different domains, and we further conduct human evaluation on a subset of 540 samples. We conclude the paper by analyzing models' performances and the reliability of automatic evaluation. The code and datasets used to conduct this study are available at https://anonymous.4open.science/r/llm_eval-E16D.

</details>


### [68] [Extract-and-Abstract: Unifying Extractive and Abstractive Summarization within Single Encoder-Decoder Framework](https://arxiv.org/abs/2409.11827)

*Yuping Wu, Hao Li, Goran Nenadic, Xiao-Jun Zeng*

**Main category:** cs.CL

**Keywords:** abstractive summarization, extractive summarization, saliency mask

**Relevance Score:** 8

**TL;DR:** The paper proposes ExtAbs, a novel method integrating extractive and abstractive summarization in a single model using a saliency mask to enhance performance while reducing error accumulation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and effectiveness of abstractive summarization by addressing the limitations of separately training extractive and abstractive models.

**Method:** ExtAbs employs a parameter-free saliency mask in the encoder-decoder framework, integrating the extraction and abstraction tasks within a single model to enhance focus on salient information during summary generation.

**Key Contributions:**

	1. Introduction of a parameter-free highlight method for summarization
	2. Development of the ExtAbs paradigm for joint extractive and abstractive summarization
	3. Demonstration of superior performance on multiple datasets

**Result:** Experiments demonstrate that ExtAbs outperforms baseline methods in extractive summarization tasks and achieves comparable or superior results in abstractive summarization compared to traditional models.

**Limitations:** 

**Conclusion:** The proposed ExtAbs paradigm effectively reduces error accumulation and training costs, leading to superior summarization performance across multiple datasets.

**Abstract:** Extract-then-Abstract is a naturally coherent paradigm to conduct abstractive summarization with the help of salient information identified by the extractive model. Previous works that adopt this paradigm train the extractor and abstractor separately and introduce extra parameters to highlight the extracted salients to the abstractor, which results in error accumulation and additional training costs. In this paper, we first introduce a parameter-free highlight method into the encoder-decoder framework: replacing the encoder attention mask with a saliency mask in the cross-attention module to force the decoder to focus only on salient parts of the input. A preliminary analysis compares different highlight methods, demonstrating the effectiveness of our saliency mask. We further propose the novel extract-and-abstract paradigm, ExtAbs., which jointly and seamlessly performs Extractive and Abstractive summarization tasks within single encoder-decoder model to reduce error accumulation. In ExtAbs, the vanilla encoder is augmented to extract salients, and the vanilla decoder is modified with the proposed saliency mask to generate summaries. Built upon BART and PEGASUS, experiments on three datasets show that ExtAbs can achieve superior performance than baselines on the extractive task and performs comparable, or even better than the vanilla models on the abstractive task.

</details>


### [69] [Turning Logic Against Itself : Probing Model Defenses Through Contrastive Questions](https://arxiv.org/abs/2501.01872)

*Rachneet Sachdeva, Rima Hazra, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** language models, jailbreak attacks, adversarial templates

**Relevance Score:** 8

**TL;DR:** POATE is a new jailbreak technique targeting language models, exploiting reasoning to produce unethical responses, and proposes methods to counter these attacks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address vulnerabilities in language models that are not detected by existing safety measures, particularly those related to reasoning-driven flaws.

**Method:** Introduced POATE, which uses contrastive reasoning to generate semantic oppositions and adversarial templates to elicit harmful outputs from language models; evaluated attack effectiveness across various model families.

**Key Contributions:**

	1. Introduction of POATE as a novel jailbreak technique
	2. Demonstration of higher attack success rates compared to existing methods
	3. Development of new mitigation strategies for enhancing reasoning robustness

**Result:** Achieved an attack success rate of approximately 44%, higher than existing techniques, revealing significant vulnerability in language model responses to adversarial queries.

**Limitations:** 

**Conclusion:** Proposed new methods (Intent-Aware CoT and Reverse Thinking CoT) enhance model defenses by decomposing queries and evaluating responses to mitigate harmful outputs.

**Abstract:** Large language models, despite extensive alignment with human values and ethical principles, remain vulnerable to sophisticated jailbreak attacks that exploit their reasoning abilities. Existing safety measures often detect overt malicious intent but fail to address subtle, reasoning-driven vulnerabilities. In this work, we introduce POATE (Polar Opposite query generation, Adversarial Template construction, and Elaboration), a novel jailbreak technique that harnesses contrastive reasoning to provoke unethical responses. POATE crafts semantically opposing intents and integrates them with adversarial templates, steering models toward harmful outputs with remarkable subtlety. We conduct extensive evaluation across six diverse language model families of varying parameter sizes to demonstrate the robustness of the attack, achieving significantly higher attack success rates (~44%) compared to existing methods. To counter this, we propose Intent-Aware CoT and Reverse Thinking CoT, which decompose queries to detect malicious intent and reason in reverse to evaluate and reject harmful responses. These methods enhance reasoning robustness and strengthen the model's defense against adversarial exploits.

</details>


### [70] [Neural Contextual Reinforcement Framework for Logical Structure Language Generation](https://arxiv.org/abs/2501.11417)

*Marcus Irvin, William Cooper, Edward Hughes, Jessica Morgan, Christopher Hamilton*

**Main category:** cs.CL

**Keywords:** Neural Contextual Reinforcement, Text Coherence, Large Language Models

**Relevance Score:** 8

**TL;DR:** The Neural Contextual Reinforcement Framework enhances text coherence and structure in LLMs using reinforcement learning, leading to improved semantic flow and clarity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of maintaining long-range dependencies in text generated by large language models.

**Method:** Utilizes reinforcement learning with custom reward functions and dynamic context alignment, incorporating multi-head attention layers and hierarchical encoding.

**Key Contributions:**

	1. Introduces a novel reinforcement learning approach for LLMs
	2. Improves coherence and structural consistency of generated text
	3. Demonstrates scalability and resource efficiency across diverse applications.

**Result:** Significant improvements in coherence metrics, perplexity, and semantic alignment over baseline models, with better narrative clarity and reduced redundancy.

**Limitations:** The paper has been withdrawn due to disputed authorship.

**Conclusion:** The framework demonstrates robustness, scalability, and efficiency for practical applications, adaptable across multiple languages.

**Abstract:** The Neural Contextual Reinforcement Framework introduces an innovative approach to enhancing the logical coherence and structural consistency of text generated by large language models. Leveraging reinforcement learning principles, the framework integrates custom reward functions and dynamic context alignment mechanisms to address challenges inherent in maintaining long-range dependencies across extended sequences. The architecture incorporates multi-head attention layers and hierarchical encoding modules, enabling the model to produce outputs that align closely with human expectations of logical structure and semantic flow. Quantitative evaluations across diverse datasets demonstrate substantial improvements in coherence metrics, perplexity reduction, and semantic alignment, showcasing the framework's ability to outperform baseline models in both general and domain-specific tasks. Qualitative analyses further highlight the framework's capacity to generate text with improved narrative clarity and reduced redundancy, reflecting its effectiveness in balancing fluency with structural precision. In addition to its performance gains, the framework exhibits robustness in handling noisy input data and scalability across varying model sizes, reinforcing its versatility in practical applications. Experimental results reveal that optimal context window sizes significantly influence coherence outcomes, showing the importance of architectural flexibility in adapting to diverse linguistic structures. Cross-lingual performance evaluations affirm the framework's adaptability to multiple languages, extending its utility beyond monolingual contexts. Resource efficiency analyses indicate a reduction in computational overhead compared to traditional approaches, emphasizing the practicality of the framework for large-scale deployment.

</details>


### [71] [Architectural Fusion Through Contextual Partitioning in Large Language Models: A Novel Approach to Parameterized Knowledge Integration](https://arxiv.org/abs/2501.12901)

*Offa Kingsleigh, Alfred Abercrombie, David Woolstencroft, Beorhtric Meadowcroft, Marcus Irvin*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Machine Learning, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** This paper introduces Contextual Partitioning, a method to enhance computational models through dynamic parameter segmentation, improving accuracy and efficiency in language tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to improve architectural design and efficiency of large-scale computational models by introducing a new methodology for parameter segmentation.

**Method:** Contextual Partitioning employs adaptive parameter allocation that aligns with the linguistic features of input data, using gradient-driven segmentation for dynamic recalibration.

**Key Contributions:**

	1. Dynamic segmentation of parameters based on context
	2. Improved accuracy and coherence in linguistic tasks
	3. Reduction in memory usage and training times

**Result:** The method shows significant improvements in accuracy, perplexity, contextual coherence, and reductions in memory usage and training time across various linguistic tasks.

**Limitations:** The paper has been withdrawn due to disputed and unverifiable authorship, limiting the applicability of its findings.

**Conclusion:** Contextual Partitioning offers a scalable and adaptable approach for computational language architectures, eliminating the need for external fine-tuning and enhancing operational efficiency.

**Abstract:** Contextual Partitioning introduces an innovative approach to enhancing the architectural design of large-scale computational models through the dynamic segmentation of parameters into context-aware regions. This methodology emphasizes the importance of task-specific specialization, achieved through adaptive parameter allocation mechanisms that align with the linguistic features of input data. Experimental evaluations demonstrated substantial improvements in accuracy, perplexity, and contextual coherence across a variety of linguistic tasks, highlighting the adaptability and scalability of the proposed framework. By reducing redundancy and enhancing computational efficiency, Contextual Partitioning not only streamlines model operations but also expands the scope of applications for advanced language processing systems. The approach operates autonomously, requiring no external fine-tuning, thereby addressing a significant limitation in conventional parameter optimization techniques. Empirical results demonstrate the effectiveness of gradient-driven segmentation, enabling models to dynamically recalibrate and specialize in response to task-specific demands. Furthermore, resource utilization metrics reveal notable reductions in memory usage and training times, confirming the efficiency of the approach. Observations from qualitative analyses illustrate improved contextual coherence and logical flow in generated outputs, reinforcing the practical value of this technique. The findings collectively demonstrate the potential for Contextual Partitioning to redefine the scalability and adaptability of computational language architectures in diverse and complex domains.

</details>


### [72] [Autonomous Structural Memory Manipulation for Large Language Models Using Hierarchical Embedding Augmentation](https://arxiv.org/abs/2501.14119)

*Derek Yotheringhay, Alistair Kirkland, Humphrey Kirkbride, Josiah Whitesteeple*

**Main category:** cs.CL

**Keywords:** Hierarchical embeddings, Dynamic memory, Contextual understanding, Scalability, Multi-domain generalization

**Relevance Score:** 4

**TL;DR:** This paper discusses hierarchical embedding augmentation and autonomous structural memory manipulation to improve the adaptability and efficiency of token representation in language models, showing promising results in complex tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the representation of tokens through multi-level semantic structures, addressing limitations of traditional static memory architectures.

**Method:** The methodology involves hierarchical embedding techniques and dynamic memory reallocation mechanisms to prioritize contextual features, enhancing scalability and efficiency.

**Key Contributions:**

	1. Introduction of hierarchical embedding techniques for improved token representation.
	2. Dynamic memory reallocation mechanisms that prioritize contextual features.
	3. Demonstrated advantages in accuracy, efficiency, and interpretability for complex tasks.

**Result:** The experimental results indicated substantial improvements in computational efficiency, particularly for longer input sequences, and advantages in accuracy and interpretability over baseline models.

**Limitations:** 

**Conclusion:** The proposed framework effectively combines embedding and memory management strategies, addressing scalability while ensuring task relevance and robustness in complex input scenarios.

**Abstract:** Transformative innovations in model architectures have introduced hierarchical embedding augmentation as a means to redefine the representation of tokens through multi-level semantic structures, offering enhanced adaptability to complex linguistic inputs. Autonomous structural memory manipulation further advances this paradigm through dynamic memory reallocation mechanisms that prioritize critical contextual features while suppressing less relevant information, enabling scalable and efficient performance across diverse tasks. Experimental results reveal substantial improvements in computational efficiency, with marked reductions in processing overhead for longer input sequences, achieved through memory reorganization strategies that adapt to evolving contextual requirements. Hierarchical embeddings not only improved contextual alignment but also facilitated task generalization by capturing relationships at varying semantic granularities, ensuring coherence across layers without introducing significant computational redundancies. Comparative analysis against baseline models demonstrated unique advantages in accuracy, efficiency, and interpretability, particularly in tasks requiring complex contextual understanding or domain-specific adaptability. The ability to dynamically adjust token representations and memory configurations contributed to the model's robustness under varied and unpredictable input conditions. Applications benefiting from these advancements include multi-domain generalization, interactive systems, and scenarios involving real-time decision-making, where traditional static memory architectures often face limitations. The proposed methodology combines advanced embedding and memory management strategies into a cohesive framework that addresses scalability challenges while preserving task-specific relevance.

</details>


### [73] [Contextual Reinforcement in Multimodal Token Compression for Large Language Models](https://arxiv.org/abs/2501.16658)

*Naderdel Piero, Zacharias Cromwell, Nathaniel Wainwright, Matthias Nethercott*

**Main category:** cs.CL

**Keywords:** token compression, contextual reinforcement, machine learning, large-scale models, semantic relevance

**Relevance Score:** 4

**TL;DR:** This paper proposes a contextual reinforcement mechanism for effective token compression in large models across complex datasets, improving accuracy and efficiency while managing token importance dynamically.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical challenge of effective token compression in scaling models for handling diverse datasets.

**Method:** A novel mechanism using contextual reinforcement that dynamically adjusts token importance based on interdependencies and semantic relevance, incorporating graph-based algorithms and adaptive weighting.

**Key Contributions:**

	1. Introduction of a contextual reinforcement mechanism for dynamic token importance adjustment
	2. Use of graph-based algorithms and adaptive weighting for capturing contextual relationships
	3. Demonstrated improvements in accuracy and computational efficiency for multimodal tasks.

**Result:** Substantial reductions in token usage with preserved information quality, significant accuracy improvements, and efficient memory usage across varied domains and tasks.

**Limitations:** Paper has been withdrawn due to disputed and unverifiable authorship, limiting its credibility and availability for further study.

**Conclusion:** The findings emphasize the potential of contextual reinforcement in enhancing token management strategies for large-scale model design.

**Abstract:** Effective token compression remains a critical challenge for scaling models to handle increasingly complex and diverse datasets. A novel mechanism based on contextual reinforcement is introduced, dynamically adjusting token importance through interdependencies and semantic relevance. This approach enables substantial reductions in token usage while preserving the quality and coherence of information representation. Incorporating graph-based algorithms and adaptive weighting, the method captures subtle contextual relationships across textual and multimodal data, ensuring robust alignment and performance in downstream tasks. Evaluations across varied domains reveal significant improvements in accuracy and semantic retention, particularly for tasks requiring detailed cross-modal interactions. Memory usage analyses demonstrate improved computational efficiency, with minimal overhead despite the additional reinforcement processes. Performance gains are further validated through error distribution analyses, showing reduced semantic loss and syntactic inconsistencies compared to baseline models. The modular architecture ensures compatibility with a wide range of open-source frameworks, facilitating scalable implementation for real-world applications. These findings highlight the potential of contextual reinforcement in redefining token management strategies and advancing large-scale model design.

</details>


### [74] [Structural Embedding Projection for Contextual Large Language Model Inference](https://arxiv.org/abs/2501.18826)

*Vincent Enoasmo, Cedric Featherstonehaugh, Xavier Konstantinopoulos, Zacharias Huntington*

**Main category:** cs.CL

**Keywords:** Structured Embedding Projection, language models, embedding transformations, contextual relationships, computational efficiency

**Relevance Score:** 6

**TL;DR:** This paper discusses the Structural Embedding Projection (SEP) as a method to enhance language model inference efficiency and coherence by refining token representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and coherence of language model inference through refined token representations.

**Method:** The paper introduces the Structural Embedding Projection (SEP) which uses projection matrices to integrate hierarchical and relational dependencies in token embeddings.

**Key Contributions:**

	1. Introduction of SEP for structured embedding transformations.
	2. Demonstration of dataset-dependent trade-offs in computational efficiency.
	3. Qualitative improvements in narrative consistency and topic alignment in generated text.

**Result:** Experimental evaluations showed that SEP reduced perplexity and improved contextual coherence across various linguistic datasets, though trade-offs between inference speed and representational richness were noted.

**Limitations:** The paper has been withdrawn due to disputed authorship, limiting the reliability of its conclusions.

**Conclusion:** SEP enhances narrative consistency, topic alignment, and fluency in text generation while requiring careful optimization of embedding layers to maintain stable training dynamics.

**Abstract:** Structured embedding transformations offer a promising approach for enhancing the efficiency and coherence of language model inference. The introduction of Structural Embedding Projection (SEP) provides a mechanism for refining token representations through projection matrices that integrate hierarchical and relational dependencies. The mathematical formulation of SEP enables embedding spaces to capture structured contextual relationships, thereby improving semantic fidelity without significantly increasing computational overhead. Experimental evaluations conducted on a range of linguistic datasets revealed that SEP contributed to reductions in perplexity and enhanced contextual coherence, demonstrating its potential to refine language model outputs. Computational efficiency assessments highlighted variations across different datasets, suggesting that the integration of structured embeddings introduced dataset-dependent trade-offs between inference speed and representational richness. The qualitative analysis of generated responses indicated that SEP enhanced narrative consistency and topic alignment, leading to improved fluency in multi-sentence text generation. The modifications to embedding layers required precise optimization to ensure stable training dynamics, as the introduction of structured transformations altered the traditional representation-learning process. The architectural adjustments necessary for SEP implementation influenced inference latency and memory consumption, requiring a balance between efficiency gains and additional processing demands. The impact of SEP on lexical diversity suggested that embedding modifications influenced the model's vocabulary usage, reflecting a more context-aware selection of generated tokens.

</details>


### [75] [Context-Preserving Tensorial Reconfiguration in Large Language Model Training](https://arxiv.org/abs/2502.00246)

*Larin Tonix, Morgana Baskerville, Nathaniel Stourton, Ophelia Tattershall*

**Main category:** cs.CL

**Keywords:** Long-range dependencies, Neural architectures, Contextual integration, Tensorial operations, Language modeling

**Relevance Score:** 5

**TL;DR:** The paper proposes the Context-Preserving Tensorial Reconfiguration (CPTR) to enhance long-range dependency handling in neural models with improved efficiency and stability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in handling long-range dependencies in neural architectures caused by computational limitations and inefficient contextual retention.

**Method:** The proposed approach, CPTR, utilizes dynamic reorganization of weight tensors via structured factorization and adaptive contraction to enhance contextual integration.

**Key Contributions:**

	1. Introduction of CPTR for effective tensorial reconfiguration
	2. Demonstrated improvements in coherence retention and recall accuracy
	3. Enhanced computational efficiency in language modeling through CPTR

**Result:** Empirical evaluations show that CPTR enhances coherence retention, reduces perplexity, and improves recall accuracy in long-context tasks, along with greater computational efficiency and reduced memory consumption compared to baseline models.

**Limitations:** The paper has been withdrawn due to disputed and unverifiable authorship, raising concerns about its validity.

**Conclusion:** CPTR shows potential in refining neural architectures for long-range contextual understanding and efficient memory utilization, while providing stable and efficient training metrics.

**Abstract:** Handling long-range dependencies in neural architectures has remained a persistent challenge due to computational limitations and inefficient contextual retention mechanisms. Tensorial operations have provided a foundation for restructuring model representations, yet conventional architectures have struggled to incorporate such techniques without introducing excessive complexity. A novel approach, Context-Preserving Tensorial Reconfiguration (CPTR), enables dynamic reorganization of weight tensors through structured factorization and adaptive contraction, allowing for enhanced contextual integration without substantial computational overhead. Empirical evaluations demonstrate that CPTR improves coherence retention across extended sequences, leading to measurable reductions in perplexity and improved recall accuracy for long-context tasks. Performance comparisons reveal that CPTR-enhanced models exhibit greater computational efficiency and reduced memory consumption while maintaining competitive language generation fluency and accuracy. Gradient stability metrics further validate the improved training efficiency, revealing more controlled variance in weight updates. Comparative studies across baseline and CPTR-enhanced models confirm that tensorial reconfiguration contributes to more stable and computationally efficient language modeling. The findings support the potential of CPTR in refining contemporary neural architectures for tasks requiring long-range contextual understanding and efficient memory utilization.

</details>


### [76] [Contextual Morphogenesis in Large Language Models: A Novel Approach to Self-Organizing Token Representations](https://arxiv.org/abs/2502.00301)

*Alistair Dombrowski, Beatrix Engelhardt, Dimitri Fairbrother, Henry Evidail*

**Main category:** cs.CL

**Keywords:** dynamic tokenization, contextual morphogenesis, language models, adaptive tokenization, perplexity reduction

**Relevance Score:** 5

**TL;DR:** The paper proposes a dynamic tokenization approach, contextual morphogenesis, that adjusts token boundaries based on contextual dependencies to improve language model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional tokenization methods hinder language models by enforcing static boundaries that do not adapt to evolving contexts, leading to inefficiencies in representation.

**Method:** The paper introduces contextual morphogenesis, a self-organizing mechanism that dynamically adjusts token boundaries in response to contextual dependencies, evaluated through empirical assessments.

**Key Contributions:**

	1. Introduction of contextual morphogenesis for dynamic tokenization
	2. Demonstration of reduced perplexity and improved interpretability
	3. Analysis of computational trade-offs for hybrid tokenization strategies

**Result:** Adaptive tokenization reduces perplexity and improves representational stability, particularly in linguistically complex domains, while maintaining interpretability and aligning better with contextual cues.

**Limitations:** The paper has been withdrawn due to disputed and unverifiable authorship, limiting further validation of its claims.

**Conclusion:** Contextual morphogenesis serves as a viable alternative to traditional tokenization, with potential advantages for both structural stability and predictive performance in language models.

**Abstract:** Token representations influence the efficiency and adaptability of language models, yet conventional tokenization strategies impose rigid segmentation boundaries that do not adjust dynamically to evolving contextual relationships. The introduction of contextual morphogenesis establishes a self-organizing mechanism that restructures token boundaries based on learned contextual dependencies, allowing embeddings to evolve progressively across iterative processing steps. Empirical evaluations demonstrate that dynamically adjusted tokenization contributes to reductions in perplexity while maintaining representational stability, particularly in linguistically complex domains where static segmentation fails to capture nuanced dependencies. Computational trade-offs associated with self-organizing token structures indicate that additional processing overhead remains within feasible limits, provided that optimization strategies account for segmentation update efficiency. Comparative assessments across different linguistic corpora suggest that adaptive tokenization preserves interpretability while improving alignment with contextual cues, reinforcing the potential of morphogenetic segmentation mechanisms to refine predictive accuracy. Stability analyses confirm that evolving token structures maintain consistent segmentation behaviors across varied text distributions, ensuring that representational adaptations remain linguistically coherent. The effectiveness of contextual morphogenesis in refining structural stability and predictive performance highlights its viability as an alternative to traditional tokenization methods. Further analysis of computational efficiency considerations suggests that hybrid strategies integrating both static and dynamic segmentation techniques may offer a balanced approach to optimizing representational flexibility while maintaining inference efficiency.

</details>


### [77] [Context-Aware Hierarchical Merging for Long Document Summarization](https://arxiv.org/abs/2502.00977)

*Litu Ou, Mirella Lapata*

**Main category:** cs.CL

**Keywords:** Hierarchical Merging, Contextual Augmentation, Summarization, Large Language Models, Hallucinations

**Relevance Score:** 8

**TL;DR:** The paper introduces a technique to improve hierarchical merging for summarizing long texts by incorporating contextual information from the source document, addressing LLM hallucinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate hallucinations in long text summarization by enhancing hierarchical merging with contextual information from the source document.

**Method:** The authors propose contextual augmentation approaches, including replacing, refining, and aligning intermediate summaries with relevant input context, and evaluate these methods on legal and narrative datasets with the Llama 3.1 model family.

**Key Contributions:**

	1. Introduction of contextual augmentation for hierarchical merging
	2. Experimental validation on diverse datasets
	3. Demonstration of superior performance of refinement methods with extractive summarization

**Result:** Experimental results indicate that contextual augmentation methods consistently outperform zero-shot and hierarchical merging baselines, particularly in refining methods paired with extractive summarization.

**Limitations:** Limited to specific domains evaluated (legal and narrative), with potential varied performance in other areas.

**Conclusion:** The study concludes that integrating context into the hierarchical merging process significantly reduces hallucination risks and improves summarization accuracy.

**Abstract:** Hierarchical Merging is a technique commonly used to summarize very long texts ($>$100K tokens) by breaking down the input into smaller sections, summarizing those sections individually, and then merging or combining those summaries into a final coherent summary. Although it helps address the limitations of large language models (LLMs) with fixed input length constraints, the recursive merging process can amplify LLM hallucinations, increasing the risk of factual inaccuracies. In this paper, we seek to mitigate hallucinations by enriching hierarchical merging with context from the source document. Specifically, we propose different approaches to contextual augmentation ranging from \emph{replacing} intermediate summaries with relevant input context, to \emph{refining} them while using the context as supporting evidence, and \emph{aligning} them implicitly (via citations) to the input. Experimental results on datasets representing legal and narrative domains show that contextual augmentation consistently outperforms zero-shot and hierarchical merging baselines for the Llama 3.1 model family. Our analysis further reveals that refinement methods tend to perform best when paired with extractive summarization for identifying relevant input.

</details>


### [78] [Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis](https://arxiv.org/abs/2502.01979)

*Derek Yotheringhay, Beatrix Nightingale, Maximilian Featherstone, Edmund Worthington, Hugo Ashdown*

**Main category:** cs.CL

**Keywords:** Text generation, Latent space modulation, Coherence, Structural consistency, Gradient-based regularization

**Relevance Score:** 4

**TL;DR:** The paper introduces Gradient-Regularized Latent Space Modulation (GRLSM), a novel method for guiding text generation with structured constraints in the latent space, resulting in enhanced coherence and structural integrity in generated outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of conventional text generation methods that rely on rigid rules or fine-tuning strategies, which often lack flexibility and generalizability across diverse tasks.

**Method:** The GRLSM framework utilizes gradient-based regularization within the latent space to impose structured constraints on the text generation process, mitigating abrupt variations and enhancing coherence and structural alignment.

**Key Contributions:**

	1. Introduction of the GRLSM framework for text generation
	2. Demonstration of improved coherence and structural consistency
	3. Empirical validation of reduced perplexity and enhanced interpretability

**Result:** Comparative evaluations show that GRLSM leads to reduced perplexity, increased coherence scores, and improved structural alignment across multiple domains without sacrificing generative flexibility.

**Limitations:** This paper has been withdrawn due to disputed and unverifiable authorship, which may limit its reliability and applicability.

**Conclusion:** The GRLSM framework successfully refines the organization of generated texts, enhances interpretability, and maintains semantic consistency through controlled variations in latent space as demonstrated by performance metrics.

**Abstract:** Generating structured textual content requires mechanisms that enforce coherence, stability, and adherence to predefined constraints while maintaining semantic fidelity. Conventional approaches often rely on rule-based heuristics or fine-tuning strategies that lack flexibility and generalizability across diverse tasks. The incorporation of Gradient-Regularized Latent Space Modulation (GRLSM) introduces a novel paradigm for guiding text generation through the application of structured constraints within the latent space. The integration of gradient-based regularization mitigates abrupt variations in latent representations, ensuring a smoother encoding process that enhances structural consistency and logical progression within generated sequences. Comparative evaluations demonstrate that latent space modulation leads to a reduction in perplexity, increased coherence scores, and improved structural alignment across multiple domains. Stability assessments further indicate that the imposition of spectral norm constraints facilitates more controlled variations in generated text, preserving semantic consistency under input perturbations. Empirical results confirm that structured latent space constraints not only refine the organization of generated outputs but also enhance interpretability through more predictable and reliable synthesis patterns. Performance metrics illustrate that the GRLSM framework substantially reduces structural inconsistencies while preserving the generative flexibility inherent in neural models.

</details>


### [79] [Latent Structure Modulation in Large Language Models Through Stochastic Concept Embedding Transitions](https://arxiv.org/abs/2502.05553)

*Stefan Whitaker, Colin Sisate, Marcel Windsor, Nikolai Fairweather, Tarquin Goldborough, Oskar Lindenfeld*

**Main category:** cs.CL

**Keywords:** stochastic embedding, representation learning, natural language processing

**Relevance Score:** 6

**TL;DR:** This paper discusses stochastic embedding transitions for dynamic token representation during inference, demonstrating improvements in generative performance and lexical diversity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance token representations in natural language processing by allowing embeddings to adjust dynamically through probabilistic mechanisms, thereby improving generative coherence and variability.

**Method:** A framework for stochastic embedding transitions was proposed, where token embeddings evolve through probabilistic updates. Empirical evaluations and statistical analyses were conducted to assess improvements in lexical diversity and model performance.

**Key Contributions:**

	1. Introduction of a stochastic embedding transition framework
	2. Demonstration of improved lexical diversity and generative coherence
	3. Empirical validation of adaptability in token representation

**Result:** Models with stochastic transitions showed greater lexical diversity, improved dialogue coherence, and better retention of low-frequency vocabulary, leading to more varied sentence structures with minor computational overhead.

**Limitations:** The paper has been withdrawn due to disputed authorship, limiting its academic reliability.

**Conclusion:** The study confirms that stochastic embedding transitions enhance representation expressiveness while maintaining coherence, suggesting their feasibility in large-scale NLP applications.

**Abstract:** Stochastic embedding transitions introduce a probabilistic mechanism for adjusting token representations dynamically during inference, mitigating the constraints imposed through static or deterministic embeddings. A transition framework was proposed in which each token embedding evolved through probabilistic updates, ensuring adaptability while preserving semantic integrity across linguistic contexts. Empirical evaluations demonstrated that models incorporating stochastic transitions exhibited greater lexical diversity, improved generative coherence, and enhanced retention of low-frequency vocabulary, contributing to more varied sentence structures and reduced reliance on high-probability token selections. Statistical analyses of embedding drift across transformer layers indicated that representations evolved more flexibly without losing coherence, supporting the hypothesis that controlled stochasticity facilitated context-sensitive representation learning. Experimental results revealed that probabilistic embeddings introduced minor computational overhead while maintaining generative efficiency, reinforcing their feasibility in large-scale applications. A comparative study with traditional embedding approaches highlighted measurable gains in text completion accuracy, dialogue coherence, and structural complexity, confirming the effectiveness of stochastic transitions in enhancing representation expressiveness. Clustering patterns in the embedding space suggested that probabilistic updates preserved meaningful semantic groupings while enabling context-driven shifts, further validating the stability of the transition mechanism. Performance metrics indicated that stochastic transitions balanced adaptability and control, ensuring that generative outputs remained linguistically coherent without excessive randomness.

</details>


### [80] [Structural Perturbation in Large Language Model Representations through Recursive Symbolic Regeneration](https://arxiv.org/abs/2502.05794)

*Kathlyn Eaglewood, Tobias Featherington, Dorian Mayfair, Sylvester Grimshaw, James Pettigrew*

**Main category:** cs.CL

**Keywords:** Symbolic Perturbations, Neural Representations, Text Generation

**Relevance Score:** 7

**TL;DR:** This paper explores symbolic perturbations as a method to influence neural representations for text generation without modifying model parameters, highlighting their potential in maintaining fluency while enhancing adaptability and lexical diversity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how symbolic perturbations can influence neural model outputs without the need for fine-tuning or parameter changes, aiming to enhance text generation quality and adaptability.

**Method:** The approach involves recursive regeneration of symbolic structures that introduce variations in latent embeddings, focusing on attention dynamics and lexical diversity through comparative analysis with traditional fine-tuning.

**Key Contributions:**

	1. Introduction of symbolic perturbations to influence neural representations
	2. Demonstration of maintaining model fluency while enhancing lexical diversity
	3. Empirical evidence of adaptability in domain-specific applications without retraining

**Result:** Findings indicate that symbolic modifications can lead to controlled shifts in attention and refinements in text generation, improving response variability and topic coherence in long-form outputs.

**Limitations:** 

**Conclusion:** Symbolic perturbations provide a novel way to adjust model behavior without retraining, allowing for enhanced adaptability in specific domains and interpretable variations in generated content.

**Abstract:** Symbolic perturbations offer a novel approach for influencing neural representations without requiring direct modification of model parameters. The recursive regeneration of symbolic structures introduces structured variations in latent embeddings, leading to controlled shifts in attention dynamics and lexical diversity across sequential generations. A comparative analysis with conventional fine-tuning techniques reveals that structural modifications at the symbolic level induce distinct variations in contextual sensitivity while maintaining overall model fluency and coherence. Shifts in attention weight distributions highlight the role of symbolic modifications in adjusting token dependencies, influencing response variability, and refining long-form text generation. Experimental findings suggest that symbolic perturbations can enhance adaptability in domain-specific applications, allowing modifications in model behavior without retraining. Evaluations of semantic drift indicate that recursive regeneration alters long-range token dependencies, affecting topic coherence across extended text sequences. Results from lexical variability assessments further support the conclusion that symbolic-level modifications introduce interpretable variations in generated responses, potentially enabling more controlled stylistic adjustments in automated text generation.

</details>


### [81] [Structural Reformation of Large Language Model Neuron Encapsulation for Divergent Information Aggregation](https://arxiv.org/abs/2502.07124)

*Denis Bakushev, Gideon Boultinghouse, Harriet Oppenheimer, Sebastian Gillingwater, Valentina Ashington, Wilfred Stanborough*

**Main category:** cs.CL

**Keywords:** structured neuron encapsulation, deep learning, language generation, modular architectures, parameter efficiency

**Relevance Score:** 7

**TL;DR:** The paper introduces a modular framework for structured neuron encapsulation, enhancing language generation in deep learning models through improved aggregation and specialization of information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of language representation in deep learning architectures by encapsulating neurons in modular frameworks.

**Method:** A modified model incorporating structured neuron encapsulation was evaluated through statistical analyses of generated text and attention weight distributions to assess improvements in language generation.

**Key Contributions:**

	1. Introduction of a modular framework for neuron encapsulation
	2. Demonstration of improved language generation metrics
	3. Mathematical formulation ensuring stable convergence properties of modular architectures

**Result:** The model showed improved perplexity, greater lexical variability, and enhanced logical consistency with reduced redundancy in token selection.

**Limitations:** The paper has been withdrawn due to disputed and unverifiable authorship.

**Conclusion:** Modular encapsulation leads to more effective language generation and improves parameter efficiency, despite some minor increases in processing overhead.

**Abstract:** Structured neuron encapsulation introduces a modular framework that enables more effective aggregation and specialization of information within deep learning architectures. A model modified through this framework demonstrated improved perplexity scores, greater lexical variability, and enhanced consistency in logical reasoning, suggesting that structured parameter distribution contributes to more efficient language representation. Statistical analyses of generated text highlighted a wider range of sentence structures and reduced redundancy in token selection, indicating that encapsulation fosters more adaptable language generation. A detailed evaluation of attention weight distributions revealed that the experimental model exhibited greater divergence in cross-layer activations, supporting the hypothesis that encapsulated neurons assume specialized processing roles. Logical consistency assessments further demonstrated that modular architectures mitigate contradictory outputs, reducing internal conflicts in inferred relationships between linguistic constructs. Computational trade-offs were analyzed, with results showing a minor increase in processing overhead, though improvements in parameter efficiency and structured decision-making compensated for the additional complexity. The mathematical formulation of the encapsulation mechanism confirmed that modular aggregation maintains stable convergence properties while promoting distinct functional roles for different neuron clusters.

</details>


### [82] [Structured Convergence in Large Language Model Representations via Hierarchical Latent Space Folding](https://arxiv.org/abs/2502.08947)

*Fenella Harcourt, Naderdel Piero, Gilbert Sutherland, Daphne Holloway, Harriet Bracknell, Julian Ormsby*

**Main category:** cs.CL

**Keywords:** Hierarchical latent space, Token embeddings, Computational efficiency

**Relevance Score:** 4

**TL;DR:** This paper discusses hierarchical latent space folding to improve token representations in high-dimensional spaces, enhancing computational efficiency and contextual coherence in sequential processing tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses redundancy in token representations that hinders efficiency and coherence in model layers.

**Method:** Introduces dynamic folding operations that adjust token embeddings through structured transformations, refining representational compactness and enhancing dependencies.

**Key Contributions:**

	1. Dynamic folding operations enhance token embeddings.
	2. Improved representational compactness while preserving context.
	3. Efficient allocation of computational resources in deeper layers.

**Result:** Empirical evaluations show reduced representational variance, improved predictive confidence in text generation, and more efficient allocation of computational resources.

**Limitations:** The paper was withdrawn due to disputed authorship, limiting the validity of its findings.

**Conclusion:** Hierarchical latent space folding optimizes model performance through better representation structuring and enhanced computational efficiency, despite minor increases in training time.

**Abstract:** Token representations in high-dimensional latent spaces often exhibit redundancy, limiting computational efficiency and reducing structural coherence across model layers. Hierarchical latent space folding introduces a structured transformation mechanism that enforces a multi-scale organization within learned embeddings, refining representational compactness while preserving essential contextual distinctions. The proposed approach incorporates dynamic folding operations that iteratively adjust token embeddings through structured transformations, influencing both short-range and long-range dependencies in sequential processing tasks. Empirical evaluation demonstrates a reduction in representational variance across layers, contributing to more stable perplexity distributions and enhancing predictive confidence in text generation. The structured redistribution of attention head utilization leads to more efficient allocation of computational resources, particularly in deeper layers, where hierarchical refinements improve contextual abstraction. Comparative analysis of activation sparsity patterns suggests that hierarchical adjustments selectively reinforce critical pathways while reducing computational overhead in non-essential regions of the model. Statistical assessments of token reordering frequencies reveal that hierarchical modifications introduce subtle shifts in sequential dependencies, improving contextual alignment while maintaining syntactic correctness. Computational trade-offs associated with hierarchical folding introduce marginal increases in training time per epoch, yet empirical findings indicate that inference efficiency benefits from the structured representation adjustments. The results highlight the impact of hierarchical latent space folding on optimizing model performance through improved representation structuring and computational efficiency.

</details>


### [83] [Statistical Coherence Alignment for Large Language Model Representation Learning Through Tensor Field Convergence](https://arxiv.org/abs/2502.09815)

*Jonathan Gale, Godfrey Aldington, Harriet Thistlewood, Thomas Tattershall, Basil Wentworth, Vincent Enoasmo*

**Main category:** cs.CL

**Keywords:** Representation Learning, Coherence Alignment, Language Models, Statistical Dependencies, Embedding Optimization

**Relevance Score:** 6

**TL;DR:** This paper introduces Statistical Coherence Alignment, a method for improving the representational consistency of language model embeddings by enforcing structured token representations through tensor field convergence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the coherence and contextual consistency of generated text in language models by improving representation learning.

**Method:** The proposed method uses tensor field convergence to enforce structured token representations, integrated with a loss function that optimizes consistency across training iterations.

**Key Contributions:**

	1. Introduction of Statistical Coherence Alignment for language model embeddings
	2. Development of a mathematical framework for coherence alignment
	3. Demonstration of improvements in perplexity and classification accuracy through empirical evaluations.

**Result:** Empirical evaluations showed improved perplexity, classification accuracy, and rare word embeddings, leading to a more stable representation space and interpretable internal structure.

**Limitations:** The paper has been withdrawn due to disputed and unverifiable authorship, limiting the accessibility of results and their validation.

**Conclusion:** Despite additional memory and training costs, the coherence alignment method provides significant trade-offs in applications needing heightened contextual fidelity.

**Abstract:** Representation learning plays a central role in structuring internal embeddings to capture the statistical properties of language, influencing the coherence and contextual consistency of generated text. Statistical Coherence Alignment is introduced as a method to enforce structured token representations through tensor field convergence, guiding embeddings to reflect statistical dependencies inherent in linguistic data. A mathematical framework is established to quantify coherence alignment, integrating a loss function that optimizes representational consistency across training iterations. Empirical evaluations demonstrate that applying coherence constraints improves perplexity, enhances classification accuracy, and refines rare word embeddings, contributing to a more stable representation space. Comparative analyses with baseline models reveal that the proposed method fosters a more interpretable internal structure, ensuring that embeddings retain contextual dependencies while mitigating representation collapse. The impact on coherence score distributions suggests that the alignment mechanism strengthens semantic integrity across diverse linguistic constructs, leading to a more balanced organization of learned embeddings. Computational assessments indicate that while the method introduces additional memory and training costs, the structured optimization process justifies the trade-offs in applications requiring heightened contextual fidelity. Experimental results validate the effectiveness of coherence alignment in optimizing token representations, providing insights into how statistical dependencies can be leveraged to improve language model training.

</details>


### [84] [Exploring Synaptic Resonance in Large Language Models: A Novel Approach to Contextual Memory Integration](https://arxiv.org/abs/2502.10699)

*George Applegarth, Christian Weatherstone, Maximilian Hollingsworth, Henry Middlebrook, Marcus Irvin*

**Main category:** cs.CL

**Keywords:** synaptic resonance, contextual memory, language models, memory reinforcement, transformer frameworks

**Relevance Score:** 7

**TL;DR:** This paper introduces Synaptic Resonance, a novel mechanism for improving long-range contextual understanding in language models through dynamic memory reinforcement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to address the challenges of maintaining coherence in language models over extended sequences, which traditional memory mechanisms struggle with.

**Method:** The proposed mechanism adjusts synaptic weight matrices based on contextual relevance during training and inference, enhancing memory pathway reinforcement dynamically.

**Key Contributions:**

	1. Introduction of the Synaptic Resonance mechanism
	2. Demonstrated enhancements in long-range contextual consistency
	3. Integration with existing transformer frameworks for scalability

**Result:** The approach leads to reductions in perplexity, improved contextual coherence, and increased resilience to input noise compared to baseline models.

**Limitations:** Not addressed due to withdrawal of the paper; may have limitations related to empirical validation or author disputes.

**Conclusion:** Dynamically reinforced memory pathways can effectively address limitations in traditional memory mechanisms, particularly in applications like dialogue systems and document summarization.

**Abstract:** Contextual memory integration remains a high challenge in the development of language models, particularly in tasks that require maintaining coherence over extended sequences. Traditional approaches, such as self-attention mechanisms and memory-augmented architectures, often prioritize short-term dependencies, leading to fragmentation and inconsistency in long-range contextual understanding. Inspired by principles of synaptic plasticity observed in biological neural systems, a novel mechanism, Synaptic Resonance, is introduced to dynamically reinforce relevant memory pathways during training and inference. Unlike static memory representations, this mechanism continuously adjusts synaptic weight matrices based on contextual relevance, allowing for improved information retention without excessive computational overhead. Evaluations conducted on an open-source language model demonstrate reductions in perplexity, enhancements in contextual coherence, and increased robustness against input noise, highlighting the effectiveness of reinforcement-driven memory modulation. Comparative analysis against baseline models further reveals that the proposed approach achieves higher memory retention efficiency while maintaining computational feasibility. The architectural modifications integrate seamlessly into existing transformer-based frameworks, ensuring stable convergence and efficient inference without sacrificing scalability. Applications benefiting from improved long-term contextual consistency, such as dialogue systems and document summarization, stand to gain from this approach. Empirical findings suggest that dynamically reinforced memory pathways offer a promising alternative to conventional memory mechanisms, addressing longstanding limitations in extended sequence modeling.

</details>


### [85] [Exploring Contextual Flux in Large Language Models: A Novel Approach to Self-Modulating Semantic Networks](https://arxiv.org/abs/2502.10942)

*Henry Evidail, Zachary Mountebank, Alistair Hathersage, Peter Stanhope, Basil Ravenscroft, Tobias Waddingham*

**Main category:** cs.CL

**Keywords:** language models, embedding modulation, text generation, contextual dependencies, generative coherence

**Relevance Score:** 7

**TL;DR:** This paper explores Contextual Flux, a method for dynamic adaptation in language models through embedding modulation, enhancing text generation coherence and flexibility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the dynamic adaptation capabilities of language models and address issues in text generation consistency and thematic retention.

**Method:** The paper introduces an auxiliary gating mechanism within a self-attention framework that adjusts token representations based on contextual dependencies.

**Key Contributions:**

	1. Introduction of Contextual Flux for embedding modulation
	2. Empirical analysis of impact on text generation coherence and flexibility
	3. Assessment of computational demands for real-time embedding adjustments

**Result:** The empirical analysis indicates improvements in coherence and thematic retention with reductions in redundant phrases, but variability in contextual weight computation affects stability during adaptation.

**Limitations:** The paper has been withdrawn due to disputed authorship, limiting its credibility and availability for reference.

**Conclusion:** While adaptive updates enhance coherence in text generation, their effectiveness is dependent on model capacity and input complexity, necessitating optimization for scalable applications.

**Abstract:** Self-modulating mechanisms introduce dynamic adaptation capabilities within language models through contextual realignment strategies that influence token embedding trajectories across extended sequences. Contextual Flux is explored as an approach to embedding modulation, integrating an auxiliary gating mechanism within the self-attention framework to dynamically adjust token representations based on evolving contextual dependencies. The empirical analysis evaluates entropy variations, latent space realignments, and coherence stability to assess the extent to which self-regulation enhances text generation consistency while preserving generative flexibility. Quantitative assessments suggest that embedding shifts contribute to more structured adaptation in long-form sequences, with measured reductions in redundant phrase repetitions and improvements in thematic retention. Variability in contextual weight computation affects modulation stability, leading to differing levels of adaptation across diverse linguistic structures. The computational demands introduced through real-time embedding reconfiguration are examined in relation to model scalability, emphasizing the need for optimization strategies in high-volume generative applications. The findings suggest that while adaptive embedding updates improve certain aspects of coherence, their impact remains contingent on model capacity and input complexity.

</details>


### [86] [Topic Over Source: The Key to Effective Data Mixing for Language Models Pre-training](https://arxiv.org/abs/2502.16802)

*Jiahui Peng, Xinlin Zhuang, Jiantao Qiu, Ren Ma, Jing Yu, He Zhu, Conghui He*

**Main category:** cs.CL

**Keywords:** large language models, data mixing, topic-based, pre-training, model performance

**Relevance Score:** 9

**TL;DR:** This paper proposes a topic-based data mixing strategy for pre-training large language models, enhancing performance by optimizing the integration of diverse data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models by addressing the shortcomings of source-based data mixing and utilizing topic-level characteristics of pre-training data.

**Method:** The authors introduce a multi-stage process combining unsupervised clustering, LLM-based summarization, and supervised classifier training to generate detailed topic labels for data mixing.

**Key Contributions:**

	1. Proposed a novel topic-based data mixing strategy
	2. Conducted a comprehensive comparison between topic-based and source-based partitioning
	3. Provided code and datasets for further research

**Result:** Language models pretrained with topic-based data mixing show consistently lower validation loss and better optimization compared to those using source-based mixing.

**Limitations:** 

**Conclusion:** Topic-based data mixing is superior to source-based methods for training language models, leading to better performance across various mixing strategies.

**Abstract:** The performance of large language models (LLMs) is significantly affected by the quality and composition of their pre-training data, which is inherently diverse, spanning various languages, sources, and topics. Effectively integrating these heterogeneous data groups is crucial for optimizing LLM performance. Previous research has predominantly concentrated on source-based data mixing, often neglecting the nuanced topic-level characteristics of the data. To address this gap, we propose a topic-based data mixing strategy that utilizes detailed topic labels generated through a multi-stage process combining unsupervised clustering, LLM-based summarization, and supervised classifier training. With this strategy, we conduct the first comprehensive comparison of topic-based versus source-based partitioning across multiple mixing strategies. We demonstrate that language models pretrained on data mixed by topics consistently outperform those trained on data mixed by sources across multiple methods including RegMix, DoReMi,temperature-based sampling, and a manual mixing method based on downstream task performance. Our theoretical analysis reveals that topic-based data achieves significantly lower validation loss compared to source-based approaches, creating a better optimization landscape for model training. We will make our code, annotated datasets, and topic classification models publicly available to facilitate further research.

</details>


### [87] [One ruler to measure them all: Benchmarking multilingual long-context language models](https://arxiv.org/abs/2503.01996)

*Yekyung Kim, Jenna Russell, Marzena Karpinska, Mohit Iyyer*

**Main category:** cs.CL

**Keywords:** multilingual, long-context, language models, benchmark, cross-lingual

**Relevance Score:** 7

**TL;DR:** ONERULER is a multilingual benchmark for evaluating long-context language models across 26 languages, revealing significant performance gaps influenced by language resource levels and context lengths.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create a comprehensive assessment tool for long-context language models that considers multilingual capabilities and to identify performance disparities among low- and high-resource languages.

**Method:** Developed through a two-step process: initial creation of English instructions for tasks followed by translation into 25 additional languages with the help of native speakers.

**Key Contributions:**

	1. Introduction of a multilingual benchmark for long-context evaluation
	2. Identification of performance gaps between low- and high-resource languages
	3. Insights into instruction language impact on model performance

**Result:** Experiments indicate performance gaps increase with longer contexts, with Polish outperforming English in long-context tasks, and significant fluctuation in performance based on instruction language in cross-lingual contexts.

**Limitations:** Performance evaluation is limited to the languages included and may not generalize beyond the 26 languages considered.

**Conclusion:** The ONERULER benchmark highlights the need for improved multilingual and cross-lingual long-context training strategies and aims to promote further research in this area.

**Abstract:** We present ONERULER, a multilingual benchmark designed to evaluate long-context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test both retrieval and aggregation, including new variations of the "needle-in-a-haystack" task that allow for the possibility of a nonexistent needle. We create ONERULER through a two-step process, first writing English instructions for each task and then collaborating with native speakers to translate them into 25 additional languages. Experiments with both open-weight and closed LLMs reveal a widening performance gap between low- and high-resource languages as context length increases from 8K to 128K tokens. Surprisingly, English is not the top-performing language on long-context tasks (ranked 6th out of 26), with Polish emerging as the top language. Our experiments also show that many LLMs (particularly OpenAI's o3-mini-high) incorrectly predict the absence of an answer, even in high-resource languages. Finally, in cross-lingual scenarios where instructions and context appear in different languages, performance can fluctuate by up to 20% depending on the instruction language. We hope the release of ONERULER will facilitate future research into improving multilingual and cross-lingual long-context training pipelines.

</details>


### [88] [OpenCodeReasoning: Advancing Data Distillation for Competitive Coding](https://arxiv.org/abs/2504.01943)

*Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** Large language models, Supervised fine-tuning, Coding tasks, Instruction diversity, Open-source

**Relevance Score:** 8

**TL;DR:** The paper discusses the construction and evaluation of a new supervised fine-tuning dataset aimed at improving the coding capabilities of models, achieving state-of-the-art results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in existing reasoning-based large language models (LLMs) that lack transparency in data curation and training processes.

**Method:** Construction of a superior supervised fine-tuning dataset; evaluation on coding tasks using distilled models and analysis of data sources, execution filtering, and instruction diversity.

**Key Contributions:**

	1. Development of a new supervised fine-tuning dataset for reasoning models.
	2. Achievement of state-of-the-art coding performance with distilled models.
	3. Analysis of dataset construction impact on model performance.

**Result:** Achieved 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing performance of models trained with reinforcement learning.

**Limitations:** Execution filtering negatively affected benchmark accuracy; results suggest a trade-off between instruction diversity and solution correctness.

**Conclusion:** Instruction diversity is prioritized over solution correctness for better outcomes; datasets and models will be open-sourced for community access.

**Abstract:** Since the advent of reasoning-based large language models, many have found great success from distilling reasoning capabilities into student models. Such techniques have significantly bridged the gap between reasoning and standard LLMs on coding tasks. Despite this, much of the progress on distilling reasoning models remains locked behind proprietary datasets or lacks details on data curation, filtering and subsequent training. To address this, we construct a superior supervised fine-tuning (SFT) dataset that we use to achieve state-of-the-art coding capability results in models of various sizes. Our distilled models use only SFT to achieve 61.8% on LiveCodeBench and 24.6% on CodeContests, surpassing alternatives trained with reinforcement learning. We then perform analysis on the data sources used to construct our dataset, the impact of code execution filtering, and the importance of instruction/solution diversity. We observe that execution filtering negatively affected benchmark accuracy, leading us to prioritize instruction diversity over solution correctness. Finally, we also analyze the token efficiency and reasoning patterns utilized by these models. We will open-source these datasets and distilled models to the community.

</details>


### [89] [Single-Pass Document Scanning for Question Answering](https://arxiv.org/abs/2504.03101)

*Weili Cao, Jianyou Wang, Youze Zheng, Longtian Bao, Qirui Zheng, Taylor Berg-Kirkpatrick, Ramamohan Paturi, Leon Bergen*

**Main category:** cs.CL

**Keywords:** document scanning, question answering, global coherence, language models, efficient processing

**Relevance Score:** 8

**TL;DR:** Proposes a single-pass document scanning method for efficient question answering over large documents without losing global context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of handling extremely large documents in question answering using traditional chunk-based methods that lose global context.

**Method:** A single-pass scanning approach that processes entire text in linear time, evaluating sentence relevance without breaking context.

**Key Contributions:**

	1. Developed a linear time single-pass document scanning method
	2. Demonstrated performance improvements over chunk-based embedding methods
	3. Maintained global coherence for long documents

**Result:** Outperforms chunk-based methods across 41 QA benchmarks and competes with large language models at lower computational costs.

**Limitations:** 

**Conclusion:** Single-pass document scanning is a promising solution for effective question answering on massive text.

**Abstract:** Handling extremely large documents for question answering is challenging: chunk-based embedding methods often lose track of important global context, while full-context transformers can be prohibitively expensive for hundreds of thousands of tokens. We propose a single-pass document scanning approach that processes the entire text in linear time, preserving global coherence while deciding which sentences are most relevant to the query. On 41 QA benchmarks, our single-pass scanner consistently outperforms chunk-based embedding methods and competes with large language models at a fraction of the computational cost. By conditioning on the entire preceding context without chunk breaks, the method preserves global coherence, which is especially important for long documents. Overall, single-pass document scanning offers a simple solution for question answering over massive text. All code, datasets, and model checkpoints are available at https://github.com/MambaRetriever/MambaRetriever

</details>


### [90] [Self-Steering Language Models](https://arxiv.org/abs/2504.07081)

*Gabriel Grand, Joshua B. Tenenbaum, Vikash K. Mansinghka, Alexander K. Lew, Jacob Andreas*

**Main category:** cs.CL

**Keywords:** self-steering LMs, recursive search procedures, Monte Carlo inference

**Relevance Score:** 8

**TL;DR:** Introduces DisCIPL, a method for self-steering language models to improve reasoning and task-specific inference through recursive search procedures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning capabilities in language models, particularly for complex tasks that involve searching or planning in natural language.

**Method:** DisCIPL utilizes a Planner model to generate inference programs executed by a population of Follower models, allowing for recursive search procedures to guide inference.

**Key Contributions:**

	1. Introduction of self-steering LMs with recursive search capabilities
	2. Demonstration of efficiency improvements over larger models
	3. Opening a design space for Monte Carlo inference strategies

**Result:** DisCIPL matches or outperforms larger models like GPT-4o on challenging constrained generation tasks when using smaller Follower models.

**Limitations:** 

**Conclusion:** The approach proposed creates a new avenue for efficient and verifiable reasoning in language models, leveraging parallelized Monte Carlo inference strategies.

**Abstract:** While test-time reasoning enables language models (LMs) to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for "self-steering" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. Our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.

</details>


### [91] [The Devil Is in the Word Alignment Details: On Translation-Based Cross-Lingual Transfer for Token Classification Tasks](https://arxiv.org/abs/2505.10507)

*Benedikt Ebing, Goran Glava*

**Main category:** cs.CL

**Keywords:** cross-lingual transfer, label projection, word aligners, marker-based methods, token classification

**Relevance Score:** 5

**TL;DR:** This paper examines the effectiveness of word aligners versus marker-based methods for label projection in cross-lingual transfer for token classification tasks, introducing a new ensembling strategy that optimizes performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate low-level design decisions in label projection for cross-lingual transfer and to improve performance in token classification tasks using translation-based strategies.

**Method:** Systematic review of label projection algorithms, filtering strategies, and pre-tokenization effects on translation-based XLT performance, followed by the introduction of an ensembling strategy for predictions.

**Key Contributions:**

	1. Systematic investigation of word aligner design choices for label projection.
	2. Introduction of an ensembling strategy for translate-train and translate-test predictions.
	3. Demonstration of improved performance in XLT with reduced sensitivity to design decisions.

**Result:** Optimized design choices for word aligners provide comparable performance to marker-based methods, and a new ensembling strategy significantly enhances performance in cross-lingual transfer tasks.

**Limitations:** 

**Conclusion:** The proposed ensembling strategy improves robustness and performance in XLT for token classification, reducing sensitivity to design choices in word aligners.

**Abstract:** Translation-based strategies for cross-lingual transfer XLT such as translate-train -- training on noisy target language data translated from the source language -- and translate-test -- evaluating on noisy source language data translated from the target language -- are competitive XLT baselines. In XLT for token classification tasks, however, these strategies include label projection, the challenging step of mapping the labels from each token in the original sentence to its counterpart(s) in the translation. Although word aligners (WAs) are commonly used for label projection, the low-level design decisions for applying them to translation-based XLT have not been systematically investigated. Moreover, recent marker-based methods, which project labeled spans by inserting tags around them before (or after) translation, claim to outperform WAs in label projection for XLT. In this work, we revisit WAs for label projection, systematically investigating the effects of low-level design decisions on token-level XLT: (i) the algorithm for projecting labels between (multi-)token spans, (ii) filtering strategies to reduce the number of noisily mapped labels, and (iii) the pre-tokenization of the translated sentences. We find that all of these substantially impact translation-based XLT performance and show that, with optimized choices, XLT with WA offers performance at least comparable to that of marker-based methods. We then introduce a new projection strategy that ensembles translate-train and translate-test predictions and demonstrate that it substantially outperforms the marker-based projection. Crucially, we show that our proposed ensembling also reduces sensitivity to low-level WA design choices, resulting in more robust XLT for token classification tasks.

</details>


### [92] [AALC: Large Language Model Efficient Reasoning via Adaptive Accuracy-Length Control](https://arxiv.org/abs/2506.20160)

*Ruosen Li, Ziming Luo, Quan Zhang, Ruochen Li, Ben Zhou, Ali Payani, Xinya Du*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Reinforcement Learning, Efficiency, Accuracy, Length Reward

**Relevance Score:** 7

**TL;DR:** AALC is a reinforcement learning approach that improves reasoning efficiency in large reasoning models by integrating a length reward that balances correctness and brevity during training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address high latency and costs associated with lengthy chain-of-thought responses in large reasoning models (LRMs), which do not always yield better accuracy.

**Method:** AALC introduces a lightweight, accuracy-aware length reward that dynamically adjusts during training, incorporating validation accuracy and using a scheduled length penalty that activates only after meeting target performance.

**Key Contributions:**

	1. Introduction of a length reward integrated into reinforcement learning for LRMs
	2. Demonstration of over 50% reduction in response length with maintained accuracy
	3. Insights on balancing efficiency with interpretability in LRM outputs

**Result:** AALC reduces the response length by over 50% while maintaining or even improving accuracy on standard and out-of-distribution math benchmarks, curbing excessive reasoning patterns.

**Limitations:** Reductions in interpretability with some narrative framing and explanatory context being omitted.

**Conclusion:** AALC demonstrates that reward-based strategies can guide LRMs to achieve more efficient and generalizable reasoning, although this may reduce interpretability.

**Abstract:** Large reasoning models (LRMs) achieve impressive reasoning capabilities by generating lengthy chain-of-thoughts, but this "overthinking" incurs high latency and cost without commensurate accuracy gains. In this work, we introduce AALC, a lightweight, accuracy-aware length reward integrated into reinforcement learning that dynamically balances correctness and brevity during training. By incorporating validation accuracy into the reward and employing a smooth, dynamically scheduled length penalty, AALC delays length penalty until target performance is met. Through extensive experiments across standard and out-of-distribution math benchmarks, we show that our approach reduces response length by over 50% while maintaining or even improving the original accuracy. Furthermore, qualitative analysis reveals that our method curbs redundant reasoning patterns such as excessive subgoal setting and verification, leading to structurally refined outputs rather than naive truncation. We also identify that efficiency gains are accompanied by reduced interpretability: models trained with AALC omit some narrative framing and explanatory context. These findings highlight the potential of reward-based strategies to guide LRMs toward more efficient, generalizable reasoning paths.

</details>
