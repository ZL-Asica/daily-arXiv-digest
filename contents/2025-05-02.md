# 2025-05-02

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 4]

- [cs.CL](#cs.CL) [Total: 83]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals](https://arxiv.org/abs/2505.00153)

*Bhanuja Ainary*

**Main category:** cs.HC

**Keywords:** Blind and Visually Impaired, Multimodal Large Language Models, AI accessibility

**Relevance Score:** 9

**TL;DR:** Audo-Sight is an assistive system that utilizes Multimodal Large Language Models to improve interactions for Blind and Visually Impaired individuals by providing contextual awareness in various environments.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** Visually impaired people face challenges interacting with complex environments, and existing assistive technologies often lack the necessary contextual understanding.

**Method:** The system operates in personalized and public modalities, adapting interactions based on user identification and providing context-aware assistance in public spaces.

**Key Contributions:**

	1. Integration of MLLMs for personalized interactions
	2. Real-time visual analysis combined with voice-controlled assistance
	3. Development of user-aware and safety features for BVI users

**Result:** Audo-Sight enhances accessibility and autonomy for BVI individuals, employing features like an Age-Range Determiner and Safe Query Filter, along with integrating capabilities from SmartSight for situational awareness.

**Limitations:** 

**Conclusion:** The integration represents a significant advancement in AI-driven accessibility technology for visually impaired individuals, enabling improved interaction and safety in social settings.

**Abstract:** Visually impaired people face significant challenges when attempting to interact with and understand complex environments, and traditional assistive technologies often struggle to quickly provide necessary contextual understanding and interactive intelligence. This thesis presents Audo-Sight, a state-of-the-art assistive system that seamlessly integrates Multimodal Large Language Models (MLLMs) to provide expedient, context-aware interactions for Blind and Visually Impaired (BVI) individuals. The system operates in two different modalities: personalized interaction through user identification and public access in common spaces like museums and shopping malls. In tailored environments, the system adjusts its output to conform to the preferences of individual users, thus enhancing accessibility through a user-aware form of interaction. In shared environments, Audo-Sight employs a shared architecture that adapts to its current user with no manual reconfiguration required. To facilitate appropriate interactions with the LLM, the public Audo-Sight solution includes an Age-Range Determiner and Safe Query Filter. Additionally, the system ensures that responses are respectful to BVI users through NeMo Guardrails. By utilizing multimodal reasoning, BVI-cognizant response editing, and safeguarding features, this work represents a major leap in AI-driven accessibility technology capable of increasing autonomy, safety, and interaction for people with visual impairments in social settings. Finally, we present the integration of Audo-Sight and SmartSight, which enables enhanced situational awareness for BVI individuals. This integration takes advantage of the real-time visual analysis of SmartSight, combined with the extensive reasoning and interactive capabilities of Audo-Sight, and goes beyond object identification to provide context-driven, voice-controlled assistance in dynamic environments.

</details>


### [2] [Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models](https://arxiv.org/abs/2505.00455)

*Sungbok Shin, Hyeon Jeon, Sanghyun Hong, Niklas Elmqvist*

**Main category:** cs.HC

**Keywords:** data visualization, human-computer interaction, large language model

**Relevance Score:** 7

**TL;DR:** The paper presents the Data Therapist, a tool that aids domain experts in externalizing tacit knowledge for effective data visualization through iterative Q&A and annotation, supported by a large language model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effective data visualization relies on domain-specific context and tacit knowledge about data provenance and quality, which is often implicit.

**Method:** The Data Therapist tool employs a mixed-initiative approach combining a large language model for analyzing datasets, prompting targeted questions, and allowing interactive annotation.

**Key Contributions:**

	1. Introduction of the Data Therapist tool for externalizing tacit knowledge
	2. Combination of Q&A and annotation to improve data visualization
	3. Identified expert reasoning patterns that inform the use of AI in visualization design

**Result:** Qualitative evaluation with experts revealed patterns in reasoning about data and identified how AI can enhance visualization design.

**Limitations:** 

**Conclusion:** The structured knowledge generated can inform both human and automated visualization design processes.

**Abstract:** Effective data visualization requires not only technical proficiency but also a deep understanding of the domain-specific context in which data exists. This context often includes tacit knowledge about data provenance, quality, and intended use, which is rarely explicit in the dataset itself. We present the Data Therapist, a web-based tool that helps domain experts externalize this implicit knowledge through a mixed-initiative process combining iterative Q&A with interactive annotation. Powered by a large language model, the system analyzes user-supplied datasets, prompts users with targeted questions, and allows annotation at varying levels of granularity. The resulting structured knowledge base can inform both human and automated visualization design. We evaluated the tool in a qualitative study involving expert pairs from Molecular Biology, Accounting, Political Science, and Usable Security. The study revealed recurring patterns in how experts reason about their data and highlights areas where AI support can improve visualization design.

</details>


### [3] [Characterizing Human Actions in the Digital Platform by Temporal Context](https://arxiv.org/abs/2206.09535)

*Akira Matsui, Emilio Ferrara*

**Main category:** cs.HC

**Keywords:** Human Behavior, Machine Learning, Temporal Context

**Relevance Score:** 6

**TL;DR:** Introduction of the Action-Timing Context (ATC) framework to model human behavior on digital platforms by embedding actions with their time intervals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in existing machine learning models that treat behavior as simple sequences of actions without considering temporal information.

**Method:** The Action-Timing Context (ATC) framework jointly embeds actions and their time intervals to produce low-dimensional representations that capture inter-temporal information.

**Key Contributions:**

	1. Introduction of the Action-Timing Context framework
	2. Demonstration of the importance of inter-temporal context in modeling behavior
	3. Application of ATC to real-world datasets to illustrate its effectiveness.

**Result:** Application of the ATC framework to real-world datasets demonstrates a unified view of human behavior, yielding qualitative findings that highlight the importance of inter-temporal context.

**Limitations:** 

**Conclusion:** Explicitly modeling inter-temporal context is crucial for an interpretable understanding of human activity on digital platforms.

**Abstract:** Recent advances in digital platforms generate rich, high-dimensional logs of human behavior, and machine learning models have helped social scientists explain knowledge accumulation, communication, and information diffusion. Such models, however, almost always treat behavior as sequences of actions, abstracting the inter-temporal information among actions. To close this gap, we introduce a two-scale Action-Timing Context(ATC) framework that jointly embeds each action and its time interval. ATC obtains low-dimensional representations of actions and characterizes them with inter-temporal information. We provide three applications of ATC to real-world datasets and demonstrate that the method offers a unified view of human behavior. The presented qualitative findings demonstrate that explicitly modeling inter-temporal context is essential for a comprehensive, interpretable understanding of human activity on digital platforms.

</details>


### [4] [Boli: A dataset for understanding stuttering experience and analyzing stuttered speech](https://arxiv.org/abs/2501.15877)

*Ashita Batra, Mannas Narang, Neeraj Kumar Sharma, Pradip K Das*

**Main category:** cs.HC

**Keywords:** stuttered speech, dataset, Indian languages, speech technology, HCI

**Relevance Score:** 6

**TL;DR:** This paper presents Project Boli, a multi-lingual stuttered speech dataset aimed at advancing understanding and technology for individuals who stutter in India.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing demand for diverse, high-quality stuttered speech data, especially in the context of Indian languages.

**Method:** The dataset includes anonymized metadata, read and spontaneous speech samples, and annotations of various stutter types, collected through comprehensive procedures and participant questionnaires.

**Key Contributions:**

	1. Introduction of a unique multi-lingual stuttered speech dataset for Indian languages.
	2. Inclusion of various stutter types with comprehensive annotations.
	3. Open access to data aimed to support further technological advancements.

**Result:** A detailed analysis of the dataset including management of metadata, speech samples, and stutter type annotations, which are validated for technical robustness.

**Limitations:** 

**Conclusion:** The open access release of the dataset is expected to facilitate research and development in speech technology aimed at supporting individuals who stutter.

**Abstract:** There is a growing need for diverse, high-quality stuttered speech data, particularly in the context of Indian languages. This paper introduces Project Boli, a multi-lingual stuttered speech dataset designed to advance scientific understanding and technology development for individuals who stutter, particularly in India. The dataset constitutes (a) anonymized metadata (gender, age, country, mother tongue) and responses to a questionnaire about how stuttering affects their daily lives, (b) captures both read speech (using the Rainbow Passage) and spontaneous speech (through image description tasks) for each participant and (c) includes detailed annotations of five stutter types: blocks, prolongations, interjections, sound repetitions and word repetitions. We present a comprehensive analysis of the dataset, including the data collection procedure, experience summarization of people who stutter, severity assessment of stuttering events and technical validation of the collected data. The dataset is released as an open access to further speech technology development.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [5] [Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning](https://arxiv.org/abs/2505.00001)

*Shaun Baek, Shaun Esua-Mensah, Cyrus Tsui, Sejan Vigneswaralingam, Abdullah Alali, Michael Lu, Vasu Sharma, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, logical reasoning, low-resource languages, benchmark, Rosetta-PL

**Relevance Score:** 6

**TL;DR:** Introduction of a benchmark, Rosetta-PL, for evaluating LLMs' logical reasoning in low-resource settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of LLMs in low-resource languages and improve their logical reasoning capabilities.

**Method:** Rosetta-PL benchmark is constructed by translating a dataset of logical propositions into a custom logical language, which is used for fine-tuning an LLM.

**Key Contributions:**

	1. Introduction of the Rosetta-PL benchmark.
	2. Insights on dataset size impact on LLM performance.
	3. Guidelines for optimizing LLM training in formal reasoning.

**Result:** Maintaining logical relationships in translations significantly increases precision; accuracy improves until around 20,000 training samples.

**Limitations:** Focused on specific logical propositions and may not generalize to all reasoning tasks.

**Conclusion:** Optimizing LLM training in formal reasoning can enhance performance in low-resource language applications.

**Abstract:** Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.

</details>


### [6] [Symbol grounding in computational systems: A paradox of intentions](https://arxiv.org/abs/2505.00002)

*Vincent C. Müller*

**Main category:** cs.CL

**Keywords:** computationalism, symbol grounding, semantic nativism, intentional cognitive processes, meaningful symbols

**Relevance Score:** 3

**TL;DR:** The paper argues that computationalism fails to adequately explain symbol grounding, leading to a paradox regarding meaningful and meaningless symbols in the mind.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the implications of computationalism on symbol grounding and semantic nativism.

**Method:** Theoretical analysis of the relationship between computationalism, meaningful symbols, and intentional cognitive processes.

**Key Contributions:**

	1. Presents a paradox regarding computationalism and symbol grounding.
	2. Explores the implications of intentional cognitive processes for computationalism.
	3. Highlights the contradiction between meaningful and meaningless symbols in computational theories.

**Result:** It concludes that computationalism inherently supports semantic nativism by suggesting that the mind must compute over meaningful symbols to have intentional processes.

**Limitations:** 

**Conclusion:** Therefore, regardless of whether the mind operates on meaningful or meaningless symbols, computationalism limits the understanding of symbol grounding.

**Abstract:** The paper presents a paradoxical feature of computational systems that suggests that computationalism cannot explain symbol grounding. If the mind is a digital computer, as computationalism claims, then it can be computing either over meaningful symbols or over meaningless symbols. If it is computing over meaningful symbols its functioning presupposes the existence of meaningful symbols in the system, i.e. it implies semantic nativism. If the mind is computing over meaningless symbols, no intentional cognitive processes are available prior to symbol grounding. In this case, no symbol grounding could take place since any grounding presupposes intentional cognitive processes. So, whether computing in the mind is over meaningless or over meaningful symbols, computationalism implies semantic nativism.

</details>


### [7] [The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs](https://arxiv.org/abs/2505.00003)

*Zizhou Liu, Ziwei Gong, Lin Ai, Zheng Hui, Run Chen, Colin Wayne Leach, Michelle R. Greene, Julia Hirschberg*

**Main category:** cs.CL

**Keywords:** Large Language Models, psychology, NLP, human-like cognition, behavior

**Relevance Score:** 9

**TL;DR:** The paper reviews the integration of psychological theories into Large Language Model (LLM) development to enhance human-like cognition and behavior in NLP.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the importance of psychology in the development and evaluation of LLMs as they grow in complexity.

**Method:** A survey of various psychological theories including cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics, examining their application in LLM stages such as data collection, training, and evaluation.

**Key Contributions:**

	1. Comprehensive review of psychological theories relevant to LLMs
	2. Identification of gaps in current LLM research
	3. Cross-disciplinary analysis of psychology's role in NLP

**Result:** Identified current trends and gaps in the application of psychological theories in LLM research, emphasizing both connections and points of tension across disciplines.

**Limitations:** 

**Conclusion:** Integrating psychology into NLP research can promote better understanding and development of human-like behaviors in LLMs, bridging gaps between disciplines.

**Abstract:** Psychological insights have long shaped pivotal NLP breakthroughs, including the cognitive underpinnings of attention mechanisms, formative reinforcement learning, and Theory of Mind-inspired social modeling. As Large Language Models (LLMs) continue to grow in scale and complexity, there is a rising consensus that psychology is essential for capturing human-like cognition, behavior, and interaction. This paper reviews how psychological theories can inform and enhance stages of LLM development, including data, pre-training, post-training, and evaluation\&application. Our survey integrates insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics. Our analysis highlights current trends and gaps in how psychological theories are applied. By examining both cross-domain connections and points of tension, we aim to bridge disciplinary divides and promote more thoughtful integration of psychology into future NLP research.

</details>


### [8] [LangVAE and LangSpace: Building and Probing for Language Model VAEs](https://arxiv.org/abs/2505.00004)

*Danilo S. Carvalho, Yingji Zhang, Harriet Unsworth, André Freitas*

**Main category:** cs.CL

**Keywords:** variational autoencoders, large language models, representation analysis

**Relevance Score:** 8

**TL;DR:** LangVAE is a framework that enhances the modular construction of variational autoencoders using pre-trained large language models, facilitating better representation analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for a more effective way to build and analyze textual representations with VAEs that leverage the knowledge of pre-trained language models.

**Method:** The proposed framework, LangVAE, constructs VAEs on pre-trained LLMs and is analyzed using the LangSpace framework, which includes various probing methods.

**Key Contributions:**

	1. Introduction of LangVAE for constructing VAEs using pre-trained LLMs
	2. Development of the LangSpace framework for representation analysis
	3. Empirical findings on the interactions of architectural families regarding VAEs and generalization

**Result:** Experiments revealed diverse interactions among encoder-decoder combinations regarding their generalization and disentanglement capabilities.

**Limitations:** 

**Conclusion:** LangVAE and LangSpace provide a scalable method for examining textual representations, proving useful through their flexible integration and experimental findings.

**Abstract:** We present LangVAE, a novel framework for modular construction of variational autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such language model VAEs can encode the knowledge of their pre-trained components into more compact and semantically disentangled representations. The representations obtained in this way can be analysed with the LangVAE companion framework: LangSpace, which implements a collection of probing methods, such as vector traversal and interpolation, disentanglement measures, and cluster visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable way of building and analysing textual representations, with simple integration for models available on the HuggingFace Hub. Additionally, we conducted a set of experiments with different encoder and decoder combinations, as well as annotated inputs, revealing a wide range of interactions across architectural families and sizes w.r.t. generalisation and disentanglement. Our findings demonstrate a promising framework for systematising the experimentation and understanding of textual representations.

</details>


### [9] [Toward a digital twin of U.S. Congress](https://arxiv.org/abs/2505.00006)

*Hayden Helm, Tianyi Chen, Harvey McGuinness, Paige Lee, Brandon Duderstadt, Carey E. Priebe*

**Main category:** cs.CL

**Keywords:** digital twin, language models, US congress, social media, predictive analytics

**Relevance Score:** 6

**TL;DR:** This paper presents a digital twin model of U.S. congresspersons utilizing language models to generate Tweets that mimic actual congressperson content, predicting their voting behavior.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the utility of language models in analyzing and predicting the behavior of U.S. congresspersons through their social media activity.

**Method:** A dataset of Tweets from U.S. congresspersons was created and updated daily, and language models were trained on congressperson-specific subsets of this data to simulate their Tweeting behaviors.

**Key Contributions:**

	1. Introduction of a digital twin model of congresspersons using language models
	2. Creation of a comprehensive dataset of congressperson Tweets updated daily
	3. Demonstration of predictive capabilities regarding voting behavior and party line crossing

**Result:** The generated Tweets closely resemble actual Tweets, and the model can predict voting behavior and party affiliations effectively.

**Limitations:** Potential biases in language models and limitations in prediction accuracy.

**Conclusion:** The study highlights the effectiveness of using generated content for predictive analytics in legislative contexts but also notes limitations in the current approach.

**Abstract:** In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.

</details>


### [10] [A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination](https://arxiv.org/abs/2505.00008)

*Zhaoyi Sun, Wen-Wai Yim, Ozlem Uzuner, Fei Xia, Meliha Yetisgen*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Medical Information, Patient Safety, Health Communication, Misinformation

**Relevance Score:** 9

**TL;DR:** A review of NLP methods for addressing medically inaccurate information, outlining both potential benefits and existing challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance patient safety and improve public health communication by utilizing NLP to detect and mitigate inaccurate medical information.

**Method:** A scoping review was conducted following PRISMA guidelines, analyzing relevant studies from 2020 to 2024 across five databases, focusing on NLP applications in healthcare.

**Key Contributions:**

	1. Unified exploration of NLP's role in addressing medical inaccuracies
	2. Identification of specific NLP tasks for healthcare applications
	3. Discussion of challenges affecting NLP reliability in medical contexts

**Result:** NLP shows potential in tasks such as error detection, correction, misinformation detection, and hallucination mitigation, but faces challenges related to data privacy and contextual evaluation.

**Limitations:** Challenges with data privacy, context dependency, and evaluation standards remain.

**Conclusion:** The review emphasizes the need for real-world datasets and refined methods to address challenges in using NLP for healthcare applications.

**Abstract:** Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.   Methods: A scoping review was conducted following PRISMA guidelines, analyzing studies from 2020 to 2024 across five databases. Studies were selected based on their use of NLP to address medically inaccurate information and were categorized by topic, tasks, document types, datasets, models, and evaluation metrics.   Results: NLP has shown potential in addressing medically inaccurate information on the following tasks: (1) error detection (2) error correction (3) misinformation detection (4) misinformation correction (5) hallucination detection (6) hallucination mitigation. However, challenges remain with data privacy, context dependency, and evaluation standards.   Conclusion: This review highlights the advancements in applying NLP to tackle medically inaccurate information while underscoring the need to address persistent challenges. Future efforts should focus on developing real-world datasets, refining contextual methods, and improving hallucination management to ensure reliable and transparent healthcare applications.

</details>


### [11] [Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation](https://arxiv.org/abs/2505.00009)

*Xiao Zhang, Kangsheng Wang, Tianyu Hu, Huimin Ma*

**Main category:** cs.CL

**Keywords:** multi-task learning, prompt tuning, task-specific representation, pre-trained language models, parameter efficiency

**Relevance Score:** 9

**TL;DR:** TA-LoRA enhances multi-task learning in pre-trained language models by capturing task heterogeneity with low-rank representation and a fast-slow weights mechanism, achieving state-of-the-art performance in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Pre-trained language models struggle with unseen emerging tasks, making separate training impractical. Multi-task learning seeks to address this by leveraging shared knowledge across tasks.

**Method:** Task-Adaptive Low-Rank Representation (TA-LoRA) employs low-rank representation combined with a fast-slow weights mechanism to effectively model task-specific knowledge and shared knowledge, along with a zero-initialized attention mechanism for stability during training.

**Key Contributions:**

	1. Introduction of Task-Adaptive Low-Rank Representation for multi-task learning
	2. Development of a fast-slow weights mechanism to separate shared and task-specific knowledge
	3. Implementation of a zero-initialized attention mechanism for better model stability

**Result:** TA-LoRA achieves state-of-the-art performance on 16 tasks in both full-data and few-shot scenarios, demonstrating superior parameter efficiency compared to existing methods.

**Limitations:** 

**Conclusion:** TA-LoRA is a robust solution for improving task adaptability in pre-trained language models while efficiently managing parameters and performance across multiple tasks.

**Abstract:** Pre-trained language models (PLMs) demonstrate remarkable intelligence but struggle with emerging tasks unseen during training in real-world applications. Training separate models for each new task is usually impractical. Multi-task learning (MTL) addresses this challenge by transferring shared knowledge from source tasks to target tasks. As an dominant parameter-efficient fine-tuning method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that captures task-specific knowledge, which acts as a prefix to the original prompt that preserves shared knowledge, while keeping PLM parameters frozen. However, PT struggles to effectively capture the heterogeneity of task-specific knowledge due to its limited representational capacity. To address this challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL method built on PT, employing the low-rank representation to model task heterogeneity and a fast-slow weights mechanism where the slow weight encodes shared knowledge, while the fast weight captures task-specific nuances, avoiding the mixing of shared and task-specific knowledge, caused by training low-rank representations from scratch. Moreover, a zero-initialized attention mechanism is introduced to minimize the disruption of immature low-rank components on original prompts during warm-up epochs. Experiments on 16 tasks demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and few-shot settings while maintaining superior parameter efficiency.

</details>


### [12] [Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models](https://arxiv.org/abs/2505.00010)

*Tri Nguyen, Lohith Srikanth Pentapalli, Magnus Sieverding, Laurah Turner, Seth Overla, Weibing Zheng, Chris Zhou, David Furniss, Danielle Weber, Michael Gharib, Matt Kelleher, Michael Shukis, Cameron Pawlik, Kelly Cohen*

**Main category:** cs.CL

**Keywords:** Jailbreaking, Large Language Models, Detection, Linguistic Features, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper investigates jailbreak detection in Large Language Models (LLMs) used in clinical education, proposing effective linguistic-feature-based models over traditional prompt engineering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Jailbreaking in LLMs poses risks by allowing the bypassing of ethical safeguards, particularly in sensitive sectors like education.

**Method:** Annotated over 2,300 prompts by analyzing 158 conversations using four linguistic variables correlated with jailbreak behavior; trained models including Decision Trees, Fuzzy Logic, Boosting, and Logistic Regression.

**Key Contributions:**

	1. Introduced a methodology for detecting jailbreaks using linguistic features.
	2. Demonstrated that feature-based models surpass prompt engineering in effectiveness.
	3. Proposed future exploration of hybrid frameworks for enhanced monitoring of educational LLMs.

**Result:** The feature-based predictive models outperformed traditional Prompt Engineering, with the Fuzzy Decision Tree demonstrating the highest performance.

**Limitations:** The study primarily focuses on a specific domain (clinical education) and may not generalize across all applications of LLMs.

**Conclusion:** Linguistic-feature-based models are effective and can serve as explainable alternatives for detecting jailbreaks in educational contexts, suggesting future work on hybrid frameworks for monitoring.

**Abstract:** Jailbreaking in Large Language Models (LLMs) threatens their safe use in sensitive domains like education by allowing users to bypass ethical safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical education platform that simulates patient interactions using LLMs. We annotated over 2,300 prompts across 158 conversations using four linguistic variables shown to correlate strongly with jailbreak behavior. The extracted features were used to train several predictive models, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Results show that feature-based predictive models consistently outperformed Prompt Engineering, with the Fuzzy Decision Tree achieving the best overall performance. Our findings demonstrate that linguistic-feature-based models are effective and explainable alternatives for jailbreak detection. We suggest future work explore hybrid frameworks that integrate prompt-based flexibility with rule-based robustness for real-time, spectrum-based jailbreak monitoring in educational LLMs.

</details>


### [13] [The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?](https://arxiv.org/abs/2505.00012)

*Fabian Retkowski, Andreas Sudmann, Alexander Waibel*

**Main category:** cs.CL

**Keywords:** qualitative research, AI Co-Ethnographer, data analysis, pattern discovery, NLP

**Relevance Score:** 4

**TL;DR:** The AI Co-Ethnographer (AICoE) is an innovative pipeline for qualitative research that automates and integrates key analysis processes to enhance data insight.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the labor-intensive nature of qualitative research and improve scalability without sacrificing analytical quality.

**Method:** The AICoE pipeline manages the entire qualitative research process, from open coding to pattern discovery, facilitating comprehensive data analysis.

**Key Contributions:**

	1. Introduction of AICoE pipeline for qualitative research
	2. Comprehensive integration of qualitative analysis processes
	3. Facilitation of pattern discovery in qualitative data

**Result:** AICoE provides a more cohesive framework for qualitative analysis, improving efficiency and depth of insight compared to traditional methods.

**Limitations:** 

**Conclusion:** The integration offered by AICoE enhances qualitative research, allowing researchers to maintain analytical depth while scaling their efforts.

**Abstract:** Qualitative research often involves labor-intensive processes that are difficult to scale while preserving analytical depth. This paper introduces The AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for qualitative research and designed to move beyond the limitations of simply automating code assignments, offering a more integrated approach. AICoE organizes the entire process, encompassing open coding, code consolidation, code application, and even pattern discovery, leading to a comprehensive analysis of qualitative data.

</details>


### [14] [Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa](https://arxiv.org/abs/2505.00013)

*Yoichi Takenaka*

**Main category:** cs.CL

**Keywords:** emotion detection, Japanese text, pre-trained language models, large language models, natural language processing

**Relevance Score:** 6

**TL;DR:** This study develops a high-accuracy model for detecting eight Plutchik emotions in Japanese text using fine-tuned language models.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate emotion detection in Japanese text is challenged by resource scarcity and class imbalance, impacting practical applications like social media monitoring.

**Method:** The study fine-tunes four pre-trained models (BERT, RoBERTa, DeBERTa variants) and evaluates two large language models (ChatGPT-4o, TinySwallow-1.5B-Instruct) using the WRIME corpus and binary labeling of emotions.

**Key Contributions:**

	1. Development of a high-accuracy emotional classification model for Japanese text
	2. First comprehensive comparison of PLMs and LLMs on Japanese emotion detection
	3. Release of a pip-installable package for practical application.

**Result:** DeBERTa-v3-large achieved the best mean accuracy of 0.860 and an F1-score of 0.662, outperforming other tested models, while LLMs underperformed significantly.

**Limitations:** Future work needed on data augmentation for rare emotions and improving LLM performance.

**Conclusion:** The fine-tuned DeBERTa-v3-large model is the most effective for binary emotion classification in Japanese, with future recommendations for improving performance on rare emotions and LLMs.

**Abstract:** Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance.   Objective This study aims to build a high-accuracy model for predicting the presence or absence of eight Plutchik emotions in Japanese sentences.   Methods Using the WRIME corpus, we transform reader-averaged intensity scores into binary labels and fine-tune four pre-trained language models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and F1-score serve as evaluation metrics.   Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score (0.662), outperforming all other models. It maintains robust F1 across both high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions (e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.   Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most reliable solution for binary emotion classification in Japanese. We release this model as a pip-installable package (pip install deberta-emotion-predictor). Future work should augment data for rare emotions, reduce model size, and explore prompt engineering to improve LLM performance.   This manuscript is under review for possible publication in New Generation Computing.

</details>


### [15] [Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and Möbius Strips](https://arxiv.org/abs/2505.00014)

*Vinit K. Chavan*

**Main category:** cs.CL

**Keywords:** sentence embeddings, manifolds, topology, representation learning, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper introduces a framework for sentence embeddings constrained to continuous manifolds, improving semantic representation through differential geometric constraints.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses limitations of traditional sentence embeddings residing in unconstrained Euclidean spaces, which may fail to capture complex language relationships.

**Method:** The authors propose a novel training approach that uses triplet loss to constrain sentence embeddings onto continuous manifolds such as the unit sphere, torus, and M"obius strip.

**Key Contributions:**

	1. Introduction of manifold-constrained sentence embeddings
	2. Use of triplet loss for embedding training
	3. Demonstration of improved performance on NLP tasks through topologically structured embeddings

**Result:** The proposed manifold-constrained embeddings significantly outperform classical baselines like TF-IDF and Word2Vec on benchmark datasets, achieving better clustering quality and classification performance.

**Limitations:** 

**Conclusion:** Embedding in manifold space provides a mathematically grounded approach to enhance semantic representation in NLP by integrating topological structure with semantic separation.

**Abstract:** Recent advances in representation learning have emphasized the role of embedding geometry in capturing semantic structure. Traditional sentence embeddings typically reside in unconstrained Euclidean spaces, which may limit their ability to reflect complex relationships in language. In this work, we introduce a novel framework that constrains sentence embeddings to lie on continuous manifolds -- specifically the unit sphere, torus, and M\"obius strip -- using triplet loss as the core training objective. By enforcing differential geometric constraints on the output space, our approach encourages the learning of embeddings that are both discriminative and topologically structured.   We evaluate our method on benchmark datasets (AG News and MBTI) and compare it to classical baselines including TF-IDF, Word2Vec, and unconstrained Keras-derived embeddings. Our results demonstrate that manifold-constrained embeddings, particularly those projected onto spheres and M\"obius strips, significantly outperform traditional approaches in both clustering quality (Silhouette Score) and classification performance (Accuracy). These findings highlight the value of embedding in manifold space -- where topological structure complements semantic separation -- offering a new and mathematically grounded direction for geometric representation learning in NLP.

</details>


### [16] [Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation](https://arxiv.org/abs/2505.00015)

*MD Thamed Bin Zaman Chowdhury, Moazzem Hossain*

**Main category:** cs.CL

**Keywords:** accident data collection, Large Language Models, web scraping, Bangladesh, road safety

**Relevance Score:** 8

**TL;DR:** This research proposes an automated accident data collection system using Large Language Models (LLMs) and web scraping techniques to improve the reporting and analysis of road traffic accidents in Bangladesh.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Road traffic accidents in developing countries like Bangladesh are underreported due to manual and fragmented data collection methods, necessitating a reliable automated system.

**Method:** The system includes automated web scraping code generation, news collection, accident classification, structured data extraction, and duplicate removal, utilizing the multimodal generative LLM Gemini-2.0-Flash for automation.

**Key Contributions:**

	1. Development of an automated system for accident data collection using LLMs.
	2. Integration of web scraping, classification, and data extraction techniques for real-time analysis.
	3. Creation of a public repository for the system's usage and data accessibility.

**Result:** Over 111 days, the system scraped 15,000 news articles from 14 Bangladeshi news sites, identifying 705 unique accidents with a code generation module achieving 91.3% calibration accuracy and 80% validation accuracy.

**Limitations:** 

**Conclusion:** The system showcased the potential for a scalable, LLM-powered approach to enhance road safety data collection, which can inform data-driven policymaking in Bangladesh.

**Abstract:** Road traffic accidents remain a major public safety and socio-economic issue in developing countries like Bangladesh. Existing accident data collection is largely manual, fragmented, and unreliable, resulting in underreporting and inconsistent records. This research proposes a fully automated system using Large Language Models (LLMs) and web scraping techniques to address these challenges. The pipeline consists of four components: automated web scraping code generation, news collection from online sources, accident news classification with structured data extraction, and duplicate removal. The system uses the multimodal generative LLM Gemini-2.0-Flash for seamless automation. The code generation module classifies webpages into pagination, dynamic, or infinite scrolling categories and generates suitable Python scripts for scraping. LLMs also classify and extract key accident information such as date, time, location, fatalities, injuries, road type, vehicle types, and pedestrian involvement. A deduplication algorithm ensures data integrity by removing duplicate reports. The system scraped 14 major Bangladeshi news sites over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news articles and identifying 705 unique accidents. The code generation module achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the highest number of accidents (80), fatalities (70), and injuries (115), followed by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning (8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also developed with usage instructions. This study demonstrates the viability of an LLM-powered, scalable system for accurate, low-effort accident data collection, providing a foundation for data-driven road safety policymaking in Bangladesh.

</details>


### [17] [Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning](https://arxiv.org/abs/2505.00016)

*Josefa Lia Stoisser, Marc Boubnovski Martell, Julien Fauqueur*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, large language models, table reasoning, reinforcement learning, structured data

**Relevance Score:** 8

**TL;DR:** This paper presents a framework that teaches large language models to reason with tabular data through a two-stage Text-to-SQL process, improving generalization and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to enhance large language models' reasoning abilities over structured data beyond simple query generation.

**Method:** A two-stage framework is proposed: first, detailed chain-of-thought traces are synthesized from real SQL queries for supervision; second, a Group Relative Policy Optimization (GRPO) reinforcement learning objective is introduced to connect SQL execution accuracy to generalizable reasoning.

**Key Contributions:**

	1. Two-stage framework for teaching reasoning with tabular data.
	2. Introduction of GRPO reinforcement learning objective.
	3. Demonstrates improved performance on reasoning-intensive datasets.

**Result:** The framework shows improved performance on standard Text-to-SQL benchmarks and significant gains on reasoning-intensive datasets like BIRD and CRT-QA, including a 20% accuracy increase for the distilled-quantized LLaMA model.

**Limitations:** 

**Conclusion:** SQL can be utilized not only as a target for queries but also as a teaching scaffold for robust reasoning skills over structured data.

**Abstract:** This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data--moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets. Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA model achieved a 20\% increase in accuracy when trained on Text-to-SQL tasks, while Qwen achieved a 5\% increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.

</details>


### [18] [ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation](https://arxiv.org/abs/2505.00017)

*Dezheng Han, Yibin Jia, Ruxiao Chen, Wenjie Han, Shuaishuai Guo, Jianbo Wang*

**Main category:** cs.CL

**Keywords:** cell type annotation, large language models, feature marker database, multi-task workflow, semantic similarity

**Relevance Score:** 7

**TL;DR:** Developed a graph structured feature marker database and a multi-task workflow for automated cell type annotation using LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enable precise and fully automated cell type annotation with large language models.

**Method:** Created a graph structured feature marker database to retrieve entities linked to differential genes, and designed a multi-task workflow to optimize the annotation process.

**Key Contributions:**

	1. Graph structured feature marker database
	2. Multi-task workflow for annotation optimization
	3. Improved evaluation scores and semantic similarity in cell annotations

**Result:** Our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, aligning more closely with manual annotation cognitive logic.

**Limitations:** 

**Conclusion:** This approach enhances the efficiency and accuracy of cell type annotation compared to general purpose LLMs.

**Abstract:** To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.

</details>


### [19] [An Empirical Study on Prompt Compression for Large Language Models](https://arxiv.org/abs/2505.00019)

*Zheng Zhang, Jinyi Li, Yihuai Lan, Xiang Wang, Hao Wang*

**Main category:** cs.CL

**Keywords:** prompt engineering, large language models, compression methods, long-context performance, multimodal tasks

**Relevance Score:** 8

**TL;DR:** This paper examines six methods for compressing prompts used in Large Language Models (LLMs) to reduce their length while preserving response quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The growing use of LLMs leads to increased computational complexity and costs associated with lengthy prompts; thus, prompt compression is necessary for efficiency.

**Method:** The study analyzes six different prompt compression techniques and evaluates their performance using 13 varied datasets across multiple multimodal tasks.

**Key Contributions:**

	1. Analysis of six prompt compression methods
	2. Comprehensive evaluation across 13 diverse datasets
	3. Demonstration of performance improvements in long-context LLM tasks

**Result:** The findings indicate that prompt compression notably improves LLM performance in long-context scenarios and moderate compression can enhance overall performance during evaluations.

**Limitations:** The study may not cover every possible prompt compression technique or all potential LLM applications.

**Conclusion:** The research confirms that effective prompt compression can help maintain LLM response quality while significantly reducing costs and computational demands.

**Abstract:** Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.

</details>


### [20] [Beyond Public Access in LLM Pre-Training Data](https://arxiv.org/abs/2505.00020)

*Sruly Rosenblat, Tim O'Reilly, Ilan Strauss*

**Main category:** cs.CL

**Keywords:** membership inference attack, GPT-4o, AI training data, copyright, transparency

**Relevance Score:** 8

**TL;DR:** The paper investigates whether OpenAI's language models were trained on copyrighted O'Reilly Media books using a membership inference attack method, revealing strong recognition of copyrighted content by GPT-4o.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if OpenAI's large language models have been trained on copyrighted material without consent, prompting a discussion on corporate transparency and licensing.

**Method:** The DE-COP membership inference attack method is applied to a legally obtained dataset of 34 copyrighted O'Reilly Media books, analyzing recognition rates of different models.

**Key Contributions:**

	1. Application of DE-COP attack on AI models
	2. Comparison of copyrighted vs. public content recognition across models
	3. Highlighting issues of transparency in AI training practices

**Result:** GPT-4o shows 82% recognition of copyrighted content, while GPT-3.5 Turbo favors public content. GPT-4o Mini displays no knowledge of either category.

**Limitations:** Focused only on O'Reilly Media books and does not cover broader data sources.

**Conclusion:** Findings emphasize the necessity for more transparency in corporate practices about training data to establish proper licensing frameworks for AI.

**Abstract:** Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method to investigate whether OpenAI's large language models were trained on copyrighted content without consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable model, demonstrates strong recognition of paywalled O'Reilly book content (AUROC = 82\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo shows greater relative recognition of publicly accessible O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge of public or non-public O'Reilly Media content when tested (AUROC $\approx$ 50\%). Testing multiple models, with the same cutoff date, helps us account for potential language shifts over time that might bias our findings. These results highlight the urgent need for increased corporate transparency regarding pre-training data sources as a means to develop formal licensing frameworks for AI content training

</details>


### [21] [Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss](https://arxiv.org/abs/2505.00021)

*Zhuoang Cai, Zhenghao Li, Yang Liu, Liyuan Guo, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** food hazard detection, class imbalance, data augmentation, transformer models, NLP

**Relevance Score:** 5

**TL;DR:** The paper presents a system for food hazard detection using transformer models and data augmentation techniques to address class imbalance issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve classification performance in food hazard detection, which suffers from imbalanced data distribution, short text, and overlapping semantic categories.

**Method:** The study employs transformer-based models (BERT and RoBERTa) and explores data balancing strategies like random oversampling, Easy Data Augmentation (EDA), and focal loss.

**Key Contributions:**

	1. Introduction of effective data augmentation techniques for class imbalance in food hazard detection
	2. Demonstration of the efficacy of combining focal loss with oversampling and EDA
	3. Contribution to the NLP classification models for specific real-world applications.

**Result:** Experiments indicate that EDA significantly mitigates class imbalance, enhancing accuracy and F1 scores. Additionally, combining focal loss with oversampling and EDA improves robustness for difficult examples.

**Limitations:** 

**Conclusion:** The findings contribute to developing more effective NLP-based classification models tailored for food hazard detection.

**Abstract:** Classification tasks often suffer from imbal- anced data distribution, which presents chal- lenges in food hazard detection due to severe class imbalances, short and unstructured text, and overlapping semantic categories. In this paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection, which ad- dresses these issues by applying data augmenta- tion techniques to improve classification perfor- mance. We utilize transformer-based models, BERT and RoBERTa, as backbone classifiers and explore various data balancing strategies, including random oversampling, Easy Data Augmentation (EDA), and focal loss. Our ex- periments show that EDA effectively mitigates class imbalance, leading to significant improve- ments in accuracy and F1 scores. Furthermore, combining focal loss with oversampling and EDA further enhances model robustness, par- ticularly for hard-to-classify examples. These findings contribute to the development of more effective NLP-based classification models for food hazard detection.

</details>


### [22] [Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation](https://arxiv.org/abs/2505.00022)

*Thomas F Burns, Letitia Parcalabescu, Stephan Wäldchen, Michael Barlow, Gregor Ziegltrum, Volker Stampa, Bastian Harren, Björn Deiseroth*

**Main category:** cs.CL

**Keywords:** large language models, data quality, synthetic data, dataset curation, German language

**Relevance Score:** 8

**TL;DR:** The paper introduces a data curation pipeline that enhances the quality of a large-scale German-language dataset for training large language models, showing significant performance improvements over existing datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of scaling data quantity while emphasizing the importance of data quality in enhancing performance and training efficiency for large language models.

**Method:** The authors presented a dataset curation pipeline that utilizes heuristic and model-based filtering techniques combined with synthetic data generation, resulting in Aleph-Alpha-GermanWeb, a new large-scale German pre-training dataset.

**Key Contributions:**

	1. Introduction of a novel dataset curation pipeline for LLMs.
	2. Creation of Aleph-Alpha-GermanWeb with improved performance metrics.
	3. Significant findings on the impact of data quality in LLM training.

**Result:** Aleph-Alpha-GermanWeb demonstrates substantial performance improvements over FineWeb2 alone when benchmarking against German-language tasks, particularly with a 1B Llama-style model and an 8B HAT model.

**Limitations:** 

**Conclusion:** The research indicates that combining model-based curation and synthetic data generation effectively enhances LLM pre-training datasets, supporting broader implications for data strategy in AI.

**Abstract:** Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.

</details>


### [23] [CORG: Generating Answers from Complex, Interrelated Contexts](https://arxiv.org/abs/2505.00023)

*Hyunji Lee, Franck Dernoncourt, Trung Bui, Seunghyun Yoon*

**Main category:** cs.CL

**Keywords:** Context Organization, Language Models, Disambiguation, Knowledge Recurrence, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper introduces Context Organizer (CORG), a framework designed to manage and disambiguate multiple contexts in language models, addressing recurrent knowledge inconsistencies seen in complex document interrelationships.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges language models face with recurrent knowledge in documents that often have inconsistencies due to numerous factors.

**Method:** CORG is developed to organize contexts into independently processed groups, utilizing a graph constructor, a reranker, and an aggregator to enhance the model's ability to find relevant answers and ensure disambiguation.

**Key Contributions:**

	1. Introduction of Context Organizer (CORG) framework
	2. Classification of context relationships into four types
	3. Demonstrated balance of performance and efficiency in managing document interrelationships

**Result:** CORG outperforms existing methods for grouping contexts, achieving a balance of performance and efficiency, while delivering results comparable to more intensive single-context approaches.

**Limitations:** 

**Conclusion:** The framework effectively addresses the complexities of knowledge interrelationships, showcasing significant improvements over prior methods.

**Abstract:** In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.

</details>


### [24] [Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning](https://arxiv.org/abs/2505.00024)

*Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, Guilin Liu*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, tool use, natural language processing, machine learning

**Relevance Score:** 8

**TL;DR:** This paper presents the Nemotron-Research-Tool-N1 series of language models that utilize a novel training methodology based on reinforcement learning for effective tool usage without supervised reasoning.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance large language models with external tools for broader functionality beyond text generation, addressing the limitations of existing supervised fine-tuning methods.

**Method:** The Nemotron-Research-Tool-N1 models are trained using a lightweight reinforcement learning approach that evaluates tool invocations based on structural validity and functional correctness, rather than using supervised reasoning traces.

**Key Contributions:**

	1. Introduction of a new training paradigm for tool-using language models based on binary reward reinforcement learning.
	2. Demonstrated superior performance over existing models like GPT-4o on key benchmarks.
	3. Showcased the ability of models to internalize reasoning strategies without extensive supervision.

**Result:** The Nemotron-Research-Tool-N1 models achieved state-of-the-art performance on BFCL and API-Bank benchmarks, surpassing the performance of GPT-4o.

**Limitations:** 

**Conclusion:** This approach allows for effective tool usage in language models by enabling them to autonomously develop reasoning strategies and achieve superior results without extensive labeled training data.

**Abstract:** Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning traces from stronger models for SFT. However, both approaches fall short, either omitting reasoning entirely or producing imitative reasoning that limits generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning through rule-based reinforcement learning, we develop the Nemotron-Research-Tool-N1 series of tool-using language models using a similar training paradigm. Instead of restrictively supervising intermediate reasoning traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized with a binary reward that evaluates only the structural validity and functional correctness of tool invocations. This lightweight supervision allows the model to autonomously internalize reasoning strategies, without the need for annotated reasoning trajectories. Experiments on the BFCL and API-Bank benchmarks show that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve state-of-the-art results, outperforming GPT-4o on both evaluations.

</details>


### [25] [A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1](https://arxiv.org/abs/2505.00025)

*Mingda Zhang, Jianglong Qin*

**Main category:** cs.CL

**Keywords:** large language models, medical AI, lightweight architecture, knowledge transfer, model compression

**Relevance Score:** 9

**TL;DR:** This paper proposes a lightweight architecture for medical large language models, addressing challenges in knowledge acquisition, model compression, and computational optimization to enhance their application in resource-constrained environments.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the significant barriers preventing the practical use of powerful foundation models in medical scenarios, this paper introduces an efficient architecture specifically designed for medical applications.

**Method:** The proposed methodology involves a knowledge transfer pipeline from a fine-tuned teacher model to a student model, implementation of 4-bit weight quantization, and optimization techniques like Flash Attention and continuous batching.

**Key Contributions:**

	1. Lightweight architecture for medical language models
	2. Knowledge transfer pipeline using Low-Rank Adaptation
	3. Compression techniques maintaining accuracy and reducing resource use

**Result:** Experimental results demonstrate that the proposed method achieves 64.7% reduction in memory consumption and 12.4% decrease in inference latency, while maintaining professional accuracy in medical question-answering tasks.

**Limitations:** The paper may not address the potential ethical and regulatory implications of deploying AI models in medical settings.

**Conclusion:** The proposed lightweight architecture effectively enables the deployment of large language models in resource-limited medical environments, making them more accessible for practical medical applications.

**Abstract:** In recent years, despite foundation models like DeepSeek-R1 and ChatGPT demonstrating significant capabilities in general tasks, professional knowledge barriers, computational resource requirements, and deployment environment limitations have severely hindered their application in actual medical scenarios. Addressing these challenges, this paper proposes an efficient lightweight medical vertical large language model architecture method, systematically solving the lightweight problem of medical large models from three dimensions: knowledge acquisition, model compression, and computational optimization. At the knowledge acquisition level, a knowledge transfer pipeline is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology is adopted to precisely adjust key attention layers. At the model compression level, compression techniques including 4-bit weight quantization are implemented while preserving the core representation ability for medical reasoning. At the computational optimization level, inference optimization techniques such as Flash Attention acceleration and continuous batching are integrated, and a professional prompt template system is constructed to adapt to different types of medical problems. Experimental results on medical question-answering datasets show that the method proposed in this paper maintains professional accuracy while reducing memory consumption by 64.7\% and inference latency by 12.4\%, providing an effective solution for the application of medical large models in resource-constrained environments such as edge computing devices.

</details>


### [26] [Theory of Mind in Large Language Models: Assessment and Enhancement](https://arxiv.org/abs/2505.00026)

*Ruirui Chen, Weifeng Jiang, Chengwei Qin, Cheston Tan*

**Main category:** cs.CL

**Keywords:** Theory of Mind, Large Language Models, human mental states, evaluation benchmarks, machine learning

**Relevance Score:** 9

**TL;DR:** This paper reviews the Theory of Mind (ToM) capabilities of Large Language Models (LLMs) and presents methods for their enhancement.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** With the integration of LLMs into daily life, assessing and improving their ability to understand human mental states is essential for enhancing human-computer interactions.

**Method:** The paper examines existing evaluation benchmarks for ToM in LLMs and analyzes strategies that have been proposed to enhance these capabilities.

**Key Contributions:**

	1. In-depth analysis of story-based benchmarks for assessing ToM in LLMs.
	2. Overview of strategies designed to enhance LLMs' ToM capabilities.
	3. Identification of promising future research avenues related to ToM in LLMs.

**Result:** The review reveals various methods aimed at improving LLMs' ToM performance and identifies key benchmarks that facilitate their assessment.

**Limitations:** 

**Conclusion:** The findings indicate that enhancing ToM capabilities in LLMs can significantly improve their applicability in sensitive and interactive contexts, with suggestions for future research directions.

**Abstract:** Theory of Mind (ToM)-the ability to infer and reason about others' mental states-is fundamental to human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, it is crucial to assess and enhance their capacity to interpret and respond to human mental states. In this paper, we review LLMs' ToM capabilities by examining both evaluation benchmarks and the strategies designed to improve them. We focus on widely adopted story-based benchmarks and provide an in-depth analysis of methods aimed at enhancing ToM in LLMs. Furthermore, we outline promising future research directions informed by recent benchmarks and state-of-the-art approaches. Our survey serves as a valuable resource for researchers interested in advancing LLMs' ToM capabilities.

</details>


### [27] [Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts](https://arxiv.org/abs/2505.00027)

*Jian Zhou, Jiazheng Li, Sirui Zhuge, Hai Zhuge*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Text Representation, Abstraction Trees, Querying, Subject-Action-Object

**Relevance Score:** 6

**TL;DR:** The paper presents a novel approach for automatically extracting subject, action, object, and adverbial dimensions from text to enhance natural language querying.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To improve text operations and support efficient querying in natural language by clearly representing subjects, actions, objects, and adverbials.

**Method:** The proposed approach constructs abstraction trees from texts that represent relevant linguistic dimensions (subject, action, object, adverbial) while ensuring expressiveness and independence among trees to avoid redundancy.

**Key Contributions:**

	1. Introduction of abstraction trees for extracting linguistic dimensions
	2. Demonstrated high performance metrics (precision, recall, F1 score)
	3. Effective natural language query support with diverse question patterns

**Result:** Experiments show that the abstraction trees yield high performance with precision, recall, and F1-scores exceeding 80%. The approach effectively supports diverse natural language question patterns with good text coverage.

**Limitations:** 

**Conclusion:** The tree-based search mechanism enables precise querying and manipulation of texts by quickly narrowing down the search space for target sentences.

**Abstract:** This paper proposed an approach to automatically discovering subject dimension, action dimension, object dimension and adverbial dimension from texts to efficiently operate texts and support query in natural language. The high quality of trees guarantees that all subjects, actions, objects and adverbials and their subclass relations within texts can be represented. The independency of trees ensures that there is no redundant representation between trees. The expressiveness of trees ensures that the majority of sentences can be accessed from each tree and the rest of sentences can be accessed from at least one tree so that the tree-based search mechanism can support querying in natural language. Experiments show that the average precision, recall and F1-score of the abstraction trees constructed by the subclass relations of subject, action, object and adverbial are all greater than 80%. The application of the proposed approach to supporting query in natural language demonstrates that different types of question patterns for querying subject or object have high coverage of texts, and searching multiple trees on subject, action, object and adverbial according to the question pattern can quickly reduce search space to locate target sentences, which can support precise operation on texts.

</details>


### [28] [Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation](https://arxiv.org/abs/2505.00028)

*Pengchao Feng, Ziyang Ma, Wenxi Chen, Yao Li, Sheng Wang, Kai Yu, Xie Chen*

**Main category:** cs.CL

**Keywords:** speech-to-speech, dialogue systems, retrieval-augmented generation, end-to-end, knowledge integration

**Relevance Score:** 8

**TL;DR:** This paper presents a novel end-to-end RAG framework for speech-to-speech dialogue systems that integrates relevant textual knowledge from speech queries without speech-to-text conversion, improving performance and retrieval efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** End-to-end S2S dialogue systems can achieve lower latency and a more natural integration of nonverbal cues but struggle to incorporate external knowledge effectively.

**Method:** The proposed framework directly retrieves relevant textual knowledge from speech queries, bypassing the intermediate ASR process.

**Key Contributions:**

	1. Introduction of a novel end-to-end RAG framework for S2S dialogue systems.
	2. Direct retrieval of textual knowledge from speech queries without ASR.
	3. Significant improvement in retrieval efficiency and performance metrics.

**Result:** The experimental results show significant improvements in the performance of end-to-end S2S dialogue systems and higher retrieval efficiency compared to traditional methods.

**Limitations:** Overall performance still lags behind that of cascaded models.

**Conclusion:** While this method offers enhanced knowledge integration and is a promising direction for end-to-end S2S systems, it still does not outperform traditional cascaded models overall.

**Abstract:** In recent years, end-to-end speech-to-speech (S2S) dialogue systems have garnered increasing research attention due to their advantages over traditional cascaded systems, including achieving lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these end-to-end systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion via techniques like ASR. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. We will release the code and dataset to support reproducibility and promote further research in this area.

</details>


### [29] [Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting](https://arxiv.org/abs/2505.00029)

*Yijie Hong, Xiaofei Yin, Xinzhong Wang, Yi Tu, Ya Guo, Sufeng Duan, Weiqiang Wang, Lingyong Fang, Depeng Wang, Huijia Zhu*

**Main category:** cs.CL

**Keywords:** Vision Language Models, Structured Dialogue Fine-Tuning, knowledge integration, catastrophic forgetting, multimodal pre-training

**Relevance Score:** 8

**TL;DR:** Introducing Structured Dialogue Fine-Tuning (SDFT) to integrate domain-specific knowledge in Vision Language Models while minimizing catastrophic forgetting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the incorporation of specialized knowledge domains in Vision Language Models without sacrificing foundational visual-linguistic abilities.

**Method:** The method involves a three-phase dialogue structure: Foundation Preservation, Contrastive Disambiguation, and Knowledge Specialization.

**Key Contributions:**

	1. Data-centric dialogue template for knowledge integration
	2. Weighted multi-turn supervision framework
	3. Extensive evaluation across diverse knowledge types

**Result:** Experimental results demonstrate SDFT's effectiveness in maintaining general capabilities while acquiring specialized knowledge across various domains.

**Limitations:** 

**Conclusion:** SDFT successfully balances foundational alignment with targeted knowledge integration, evidenced through extensive evaluations.

**Abstract:** Large Vision Language Models have demonstrated impressive versatile capabilities through extensive multimodal pre-training, but face significant limitations when incorporating specialized knowledge domains beyond their training distribution. These models struggle with a fundamental dilemma: direct adaptation approaches that inject domain-specific knowledge often trigger catastrophic forgetting of foundational visual-linguistic abilities. We introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that effectively injects domain-specific knowledge while minimizing catastrophic forgetting. Drawing inspiration from supervised fine-tuning in LLMs and subject-driven personalization in text-to-image diffusion models, our method employs a three-phase dialogue structure: Foundation Preservation reinforces pre-trained visual-linguistic alignment through caption tasks; Contrastive Disambiguation introduces carefully designed counterfactual examples to maintain semantic boundaries; and Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results across multiple domains confirm SDFT's effectiveness in balancing specialized knowledge acquisition with general capability retention. Our key contributions include a data-centric dialogue template that balances foundational alignment with targeted knowledge integration, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.

</details>


### [30] [Can Language Models Represent the Past without Anachronism?](https://arxiv.org/abs/2505.00030)

*Ted Underwood, Laura K. Nelson, Matthew Wilkens*

**Main category:** cs.CL

**Keywords:** language models, historical simulation, fine-tuning, natural language processing, social research

**Relevance Score:** 6

**TL;DR:** This paper examines the challenges of using language models to simulate past styles, finding that while fine-tuning helps, human evaluators can still tell the difference between model outputs and authentic historical texts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the risk of anachronism in language model outputs when simulating historical perspectives for social research.

**Method:** The paper explores the effectiveness of prompting a contemporary language model with period prose examples and compares outputs from fine-tuned models with genuine historical texts.

**Key Contributions:**

	1. Identified limitations of current language models in simulating historical prose.
	2. Demonstrated the effectiveness of fine-tuning for improved stylistic alignment.
	3. Proposed the necessity of pretraining on historical texts for better simulation accuracy.

**Result:** Fine-tuning language models can create outputs that fools automated judges, but human evaluators can distinguish between model-generated text and authentic historical prose.

**Limitations:** Human evaluators can still detect differences between model outputs and authentic texts despite fine-tuning efforts.

**Conclusion:** Pretraining on period prose is likely necessary for language models to accurately simulate historical perspectives to be useful in social research.

**Abstract:** Before researchers can use language models to simulate the past, they need to understand the risk of anachronism. We find that prompting a contemporary model with examples of period prose does not produce output consistent with period style. Fine-tuning produces results that are stylistically convincing enough to fool an automated judge, but human evaluators can still distinguish fine-tuned model outputs from authentic historical text. We tentatively conclude that pretraining on period prose may be required in order to reliably simulate historical perspectives for social research.

</details>


### [31] [Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving](https://arxiv.org/abs/2505.00031)

*Jin Zhang, Flood Sung, Zhilin Yang, Yang Gao, Chongjie Zhang*

**Main category:** cs.CL

**Keywords:** large language models, self-training algorithm, reasoning benchmarks

**Relevance Score:** 9

**TL;DR:** Introduction of a new self-training algorithm called LEarning to Plan before Answering (LEPA) for large language models (LLMs) to enhance problem-solving through anticipatory plans.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of essential information in synthetic data generated by LLMs and improve generalization across similar problems by incorporating abstract meta-knowledge.

**Method:** LEPA trains LLMs to create anticipatory plans before solving problems, thus refining the generation of solutions during a self-reflection process.

**Key Contributions:**

	1. Introduction of anticipatory planning in LLM training
	2. Improved performance on natural language reasoning tasks
	3. Refinement process for solution generation and planning

**Result:** LEPA outperforms conventional algorithms on various challenging natural language reasoning benchmarks by utilizing anticipatory plans.

**Limitations:** 

**Conclusion:** Leveraging anticipatory plans in problem-solving significantly enhances the effectiveness of LLMs in generating correct solutions and understanding complex tasks.

**Abstract:** In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.

</details>


### [32] [MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis](https://arxiv.org/abs/2505.00032)

*Yuyang Sha, Hongxin Pan, Wei Xu, Weiyu Meng, Gang Luo, Xinyu Du, Xiaobing Zhai, Henry H. Y. Tong, Caijuan Shi, Kefeng Li*

**Main category:** cs.CL

**Keywords:** Major Depressive Disorder, Large Language Models, AI Diagnosis, Health Informatics, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents MDD-LLM, an AI-driven tool for diagnosing major depressive disorder (MDD) using fine-tuned large language models and real-world data.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequate attention given to major depressive disorder (MDD) diagnosis worldwide due to uneven distribution of resources and complexity in diagnostic methods.

**Method:** The authors utilized a dataset of 274,348 records from the UK Biobank and developed a tabular data transformation method for training and evaluation of the MDD-LLM framework.

**Key Contributions:**

	1. Introduction of MDD-LLM, an AI-driven diagnostic tool for MDD
	2. Utilization of a large dataset from the UK Biobank
	3. Demonstration of superior performance over existing methods.

**Result:** MDD-LLM achieved an accuracy of 0.8378 and an AUC of 0.8919, significantly outperforming existing machine learning and deep learning approaches for MDD diagnosis.

**Limitations:** Limited exploration of LLMs in MDD diagnosis beyond this specific framework and dataset.

**Conclusion:** The study showcases the potential of fine-tuned LLMs in improving MDD diagnostics and highlights various influencing factors on the method's performance.

**Abstract:** Major depressive disorder (MDD) impacts more than 300 million people worldwide, highlighting a significant public health issue. However, the uneven distribution of medical resources and the complexity of diagnostic methods have resulted in inadequate attention to this disorder in numerous countries and regions. This paper introduces a high-performance MDD diagnosis tool named MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models (LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis. Therefore, we select 274,348 individual information from the UK Biobank cohort to train and evaluate the proposed method. Specifically, we select 274,348 individual records from the UK Biobank cohort and design a tabular data transformation method to create a large corpus for training and evaluating the proposed approach. To illustrate the advantages of MDD-LLM, we perform comprehensive experiments and provide several comparative analyses against existing model-based solutions across multiple evaluation metrics. Experimental results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine learning and deep learning frameworks for MDD diagnosis. Given the limited exploration of LLMs in MDD diagnosis, we examine numerous factors that may influence the performance of our proposed method, such as tabular data transformation techniques and different fine-tuning strategies.

</details>


### [33] [From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models](https://arxiv.org/abs/2505.00033)

*Andrew Kiruluta*

**Main category:** cs.CL

**Keywords:** spectral generative modeling, natural language processing, Fourier dictionary, Gaussian Mixture Model, language modeling

**Relevance Score:** 8

**TL;DR:** A novel spectral generative modeling framework for NLP replaces self-attention with linear complexity models, achieving competitive results with reduced resource requirements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve efficiency and reduce computational costs associated with self-attention mechanisms in transformer architectures for NLP tasks.

**Method:** The framework learns a global time varying Fourier dictionary and per token mixing coefficients, using reconstruction losses in both time and frequency domains, alongside a Gaussian Mixture Model prior.

**Key Contributions:**

	1. Introduces a linear complexity model for NLP tasks that replaces self-attention.
	2. Combines time and frequency domain reconstruction losses alongside standard language modeling objectives.
	3. Demonstrates substantial efficiency gains in inference and memory usage compared to transformers.

**Result:** Competitive perplexity and generation quality on datasets like WikiText2 and Penn Treebank, with a significant reduction in inference latency and memory footprint.

**Limitations:** 

**Conclusion:** The proposed spectral dictionary model offers a scalable alternative to traditional transformer models while maintaining performance.

**Abstract:** We propose a novel spectral generative modeling framework for natural language processing that jointly learns a global time varying Fourier dictionary and per token mixing coefficients, replacing the ubiquitous self attention mechanism in transformer architectures. By enforcing reconstruction losses in both the time domain (embedding reconstruction) and the frequency domain (via Short Time Fourier Transform magnitude matching) alongside a standard language modeling objective, and fitting a Gaussian Mixture Model (GMM) prior over the learned mixing vectors, our approach achieves competitive perplexity and generation quality on standard benchmarks such as WikiText2 and Penn Treebank. In contrast to the quadratic computation complexity of self attention, our method operates with linear complexity, delivering substantial efficiency gains. We demonstrate that spectral dictionary models can achieve competitive performance compared to transformer baselines while significantly reducing inference latency and memory footprint, offering a compelling alternative for scalable language modeling.

</details>


### [34] [Improving Phishing Email Detection Performance of Small Large Language Models](https://arxiv.org/abs/2505.00034)

*Zijie Lin, Zikang Liu, Hanbo Fan*

**Main category:** cs.CL

**Keywords:** phishing email detection, small LLMs, machine learning, natural language processing, prompt engineering

**Relevance Score:** 9

**TL;DR:** This paper explores methods to enhance phishing email detection using small-parameter LLMs, achieving significant accuracy improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the feasibility of small-parameter LLMs in phishing email detection, reducing computational costs while maintaining performance.

**Method:** The paper uses techniques such as Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve detection capabilities of small LLMs.

**Key Contributions:**

	1. Introduction of optimization methods for small LLMs in phishing detection
	2. Demonstration of significant accuracy improvement over baseline models
	3. Validation of methods on the SpamAssassin dataset

**Result:** The proposed methods improved detection accuracy on the SpamAssassin dataset from approximately 0.5 for baseline models to 0.976.

**Limitations:** 

**Conclusion:** Small-parameter LLMs can effectively detect phishing emails with appropriate enhancement methods, making them viable for deployment on consumer-grade hardware.

**Abstract:** Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving accuracy on the SpamAssassin dataset from around 0.5 for baseline models like Qwen2.5-1.5B-Instruct to 0.976.

</details>


### [35] [Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics](https://arxiv.org/abs/2505.00035)

*Aayam Bansal, Raghav Agarwal, Kaashvi Jain*

**Main category:** cs.CL

**Keywords:** hip-hop, natural language processing, linguistic complexity, cultural trends, sentiment analysis

**Relevance Score:** 2

**TL;DR:** This paper analyzes linguistic complexity and socio-cultural trends in hip-hop lyrics using NLP techniques on a dataset of 3,814 songs over 40 years, revealing significant changes in vocabulary diversity, rhyme density, themes, and sentiment during sociopolitical events.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the relationship between linguistic innovation in hip-hop lyrics and the socio-cultural context over four decades.

**Method:** Natural language processing techniques were applied to a dataset of hip-hop lyrics from 3,814 songs across 146 artists, quantifying dimensions of lyrical complexity and analyzing thematic shifts through topic modeling and sentiment analysis.

**Key Contributions:**

	1. Quantitative analysis of linguistic complexity in hip-hop lyrics
	2. Identification of thematic shifts over four decades
	3. Exploration of the relationship between lyrical features and socio-cultural trends

**Result:** A 23.7% increase in vocabulary diversity, 34.2% increase in rhyme density, a notable decrease in social justice themes, and a correlation between stylistic approaches and geographic origin/time period.

**Limitations:** 

**Conclusion:** The findings provide quantitative evidence for the evolution of hip-hop as an art form reflecting societal dynamics, showing how linguistic features in lyrics interact with cultural context.

**Abstract:** This paper presents a comprehensive computational framework for analyzing linguistic complexity and socio-cultural trends in hip-hop lyrics. Using a dataset of 3,814 songs from 146 influential artists spanning four decades (1980-2020), we employ natural language processing techniques to quantify multiple dimensions of lyrical complexity. Our analysis reveals a 23.7% increase in vocabulary diversity over the study period, with East Coast artists demonstrating 17.3% higher lexical variation than other regions. Rhyme density increased by 34.2% across all regions, with Midwest artists exhibiting the highest technical complexity (3.04 rhymes per line). Topic modeling identified significant shifts in thematic content, with social justice themes decreasing from 28.5% to 13.8% of content while introspective themes increased from 7.6% to 26.3%. Sentiment analysis demon- strated that lyrics became significantly more negative during sociopolitical crises, with polarity decreasing by 0.31 following major social unrest. Multi-dimensional analysis revealed four dis- tinct stylistic approaches that correlate strongly with geographic origin (r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish quantitative evidence for the evolution of hip- hop as both an art form and a reflection of societal dynamics, providing insights into the interplay between linguistic innovation and cultural context in popular music.

</details>


### [36] [A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies](https://arxiv.org/abs/2505.00036)

*Zhongren Chen, Joshua Kalla, Quan Le, Shinpei Nakamura-Sakai, Jasjeet Sekhon, Ruixiao Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Political Persuasion, Survey Experiments, Cost-Effectiveness, Democracy

**Relevance Score:** 8

**TL;DR:** The paper investigates the effectiveness and cost-efficiency of using Large Language Models (LLMs) for political persuasion compared to traditional campaign methods through survey experiments and simulations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential threat of LLMs to democracy and their effectiveness in political persuasion.

**Method:** Conducted two survey experiments with 10,417 participants and a real-world simulation exercise to compare LLM-based persuasion with traditional political campaigns, considering both exposure and acceptance of messages.

**Key Contributions:**

	1. Provided empirical data on LLMs' persuasive effectiveness compared to traditional methods
	2. Developed a framework for assessing extended human-LLM interactions
	3. Estimated costs of LLM-based persuasion versus traditional campaigns

**Result:** Findings indicate LLMs are similarly persuasive as campaign ads but raising concerns about exposure and scaling challenges for LLMs.

**Limitations:** The study mainly focuses on cost-effectiveness and does not fully address long-term impacts on democratic processes or ethical considerations of using LLMs in political contexts.

**Conclusion:** LLMs show potential for political persuasion at a lower cost per persuaded voter, but traditional methods are currently easier to scale; future improvements in LLMs may alter this balance.

**Abstract:** In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the "receive" and "accept" steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \$48-\$74 per persuaded voter compared to \$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.

</details>


### [37] [HyPerAlign: Hypotheses-driven Personalized Alignment](https://arxiv.org/abs/2505.00038)

*Cristina Garbacea, Chenhao Tan*

**Main category:** cs.CL

**Keywords:** personalization, large language models, hypotheses-driven, user-specific, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents HyPerAlign, a novel approach for personalizing large language model (LLM) outputs to individual users based on their unique communication strategies and styles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs typically align to average user preferences but often miss the specifics of individual user contexts, highlighting the need for personalized outputs.

**Method:** The HyPerAlign approach infers user-specific hypotheses from few-shot examples and uses these to prompt LLMs for customized outputs.

**Key Contributions:**

	1. Introduction of a hypotheses-driven personalization approach for LLMs
	2. Demonstrated significant performance improvements over traditional fine-tuning methods
	3. Interpretable model behavior through user-specific hypothesis inference

**Result:** Experiments show that HyPerAlign significantly outperforms traditional preference-based fine-tuning, leading to improvements in helpfulness by up to 70% and high win rates in authorship attribution tasks.

**Limitations:** 

**Conclusion:** HyPerAlign offers a more interpretable and efficient means of personalizing LLMs to enhance user experience and satisfaction.

**Abstract:** Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations that reflect their intended real-world use cases. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users, aiming to generate customized responses tailored to individual users, instead of generic outputs that emulate the collective voices of diverse populations. We propose a novel interpretable and sample-efficient hypotheses-driven personalization approach (HyPerAlign) where given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality and writing style, then prompt LLM models with these hypotheses and user specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks), and demonstrate the superiority of hypotheses-driven personalization approach when compared to preference-based fine-tuning methods. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\%$ on average. For authorship attribution, results indicate consistently high win-rates (commonly $>90\%$) against state-of-the-art preference fine-tuning approaches for LLM personalization across diverse user profiles and LLM models. Overall, our approach represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users.

</details>


### [38] [Graph RAG for Legal Norms: A Hierarchical and Temporal Approach](https://arxiv.org/abs/2505.00039)

*Hudson de Martim*

**Main category:** cs.CL

**Keywords:** Graph Retrieval Augmented Generation, legal norms, knowledge graphs, AI in law, temporal evolution

**Relevance Score:** 4

**TL;DR:** This paper presents an adaptation of Graph Retrieval Augmented Generation for analyzing legal norms using structured knowledge graphs to manage complexity in legal data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexity and large volume of legal data by leveraging hierarchical structures and knowledge graphs.

**Method:** The paper combines structured knowledge graphs with enriched text segments and introduces comprehensive Text Units to improve the analysis of legal norms.

**Key Contributions:**

	1. Introduction of Graph RAG for legal norm analysis
	2. Integration of hierarchical structures in knowledge graphs
	3. Development of comprehensive Text Units for richer legal knowledge representation

**Result:** Graph RAG enhances the representation of legal knowledge, facilitating more effective systems in legal research and analysis.

**Limitations:** 

**Conclusion:** The proposed approach significantly advances AI applications in law and helps create better decision support systems.

**Abstract:** This article proposes an adaptation of Graph Retrieval Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms, which are characterized by their predefined hierarchical structure, extensive network of internal and external references and multiple temporal versions. By combining structured knowledge graphs with contextually enriched text segments, Graph RAG offers a promising solution to address the inherent complexity and vast volume of legal data. The integration of hierarchical structure and temporal evolution into knowledge graphs - along with the concept of comprehensive Text Units - facilitates the construction of richer, interconnected representations of legal knowledge. Through a detailed analysis of Graph RAG and its application to legal norm datasets, this article aims to significantly advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective systems in legal research, legislative analysis, and decision support.

</details>


### [39] [Base Models Beat Aligned Models at Randomness and Creativity](https://arxiv.org/abs/2505.00047)

*Peter West, Christopher Potts*

**Main category:** cs.CL

**Keywords:** alignments, language models, creativity, unpredictability, human feedback

**Relevance Score:** 8

**TL;DR:** This paper critiques the common reliance on alignment in LLM development, showing that unaligned base models can outperform aligned ones on tasks requiring unpredictability and creativity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to highlight the limitations of aligned language models by demonstrating cases where unaligned models perform better in specific tasks that require unpredictability and creativity.

**Method:** The authors conducted experiments comparing aligned and unaligned models on various tasks such as random number generation, mixed strategy games, and creative writing to assess their performance differences.

**Key Contributions:**

	1. Demonstration of tasks where unaligned models show superior performance
	2. Analysis of the predictability trade-offs in aligned models
	3. Insights into the limitations of reinforcement learning from human feedback in creative tasks

**Result:** Unaligned models consistently outperformed aligned models on tasks requiring unpredictable outputs, indicating that alignment may lead to narrow behaviors and predictability.

**Limitations:** The study focuses on specific tasks and may not generalize across all LLM applications; further research is needed to explore other contexts.

**Conclusion:** While alignment techniques enhance model safety and instruction-following, they can hinder performance in creativity and unpredictability, suggesting a need for careful application of these techniques.

**Abstract:** Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate "7" over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over creative originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities.

</details>


### [40] [Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting](https://arxiv.org/abs/2505.00050)

*Aayam Bansal, Agneya Tharun*

**Main category:** cs.CL

**Keywords:** fashion trends, social media, sentiment analysis, natural language processing, machine learning

**Relevance Score:** 4

**TL;DR:** The paper analyzes Twitter sentiment to predict fashion trends using NLP and ML techniques.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the predictive relationship between social media sentiment and emerging fashion trends.

**Method:** Utilizes natural language processing and machine learning for sentiment classification and analysis of fashion-related Twitter data, including time series decomposition and causal relationship modeling.

**Key Contributions:**

	1. Application of advanced NLP techniques for fashion sentiment analysis.
	2. Establishment of statistically validated causal relationships between sentiment patterns and fashion trends.
	3. Demonstration of social media sentiment as a predictor for emerging trends.

**Result:** Identified correlations between sentiment patterns and the popularity of fashion themes, with established causal relationships showing sustainability and streetwear as primary trend drivers.

**Limitations:** 

**Conclusion:** Social media sentiment analysis can effectively predict fashion trend trajectories with proper statistical validation, achieving 78.35% balanced accuracy in sentiment classification.

**Abstract:** This study explores the intersection of fashion trends and social media sentiment through computational analysis of Twitter data using the T4SA (Twitter for Sentiment Analysis) dataset. By applying natural language processing and machine learning techniques, we examine how sentiment patterns in fashion-related social media conversations can serve as predictors for emerging fashion trends. Our analysis involves the identification and categorization of fashion-related content, sentiment classification with improved normalization techniques, time series decomposition, statistically validated causal relationship modeling, cross-platform sentiment comparison, and brand-specific sentiment analysis. Results indicate correlations between sentiment patterns and fashion theme popularity, with accessories and streetwear themes showing statistically significant rising trends. The Granger causality analysis establishes sustainability and streetwear as primary trend drivers, showing bidirectional relationships with several other themes. The findings demonstrate that social media sentiment analysis can serve as an effective early indicator of fashion trend trajectories when proper statistical validation is applied. Our improved predictive model achieved 78.35% balanced accuracy in sentiment classification, establishing a reliable foundation for trend prediction across positive, neutral, and negative sentiment categories.

</details>


### [41] [Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity](https://arxiv.org/abs/2505.00056)

*Tygo Bloem, Filip Ilievski*

**Main category:** cs.CL

**Keywords:** Meme clustering, Toxicity detection, Virality modeling, Multi-dimensional similarity, Template-based matching

**Relevance Score:** 4

**TL;DR:** This paper presents a new method for clustering Internet memes using template-based matching and multi-dimensional similarity features to enhance toxicity detection and virality modeling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Meme clustering is essential for toxicity detection, virality modeling, and typing, yet it has been largely overlooked in research.

**Method:** A novel approach using template-based matching and multi-dimensional similarity features for clustering memes without relying on predefined databases.

**Key Contributions:**

	1. Introduction of template-based matching for meme clustering
	2. Use of multi-dimensional similarity features for adaptive matching
	3. Public availability of supporting code for further research

**Result:** The combined method shows superior performance compared to existing techniques, yielding more consistent and coherent meme clusters.

**Limitations:** 

**Conclusion:** The proposed method supports adaptive matching and aligns with human intuition, advancing meme clustering research.

**Abstract:** Meme clustering is critical for toxicity detection, virality modeling, and typing, but it has received little attention in previous research. Clustering similar Internet memes is challenging due to their multimodality, cultural context, and adaptability. Existing approaches rely on databases, overlook semantics, and struggle to handle diverse dimensions of similarity. This paper introduces a novel method that uses template-based matching with multi-dimensional similarity features, thus eliminating the need for predefined databases and supporting adaptive matching. Memes are clustered using local and global features across similarity categories such as form, visual content, text, and identity. Our combined approach outperforms existing clustering methods, producing more consistent and coherent clusters, while similarity-based feature sets enable adaptability and align with human intuition. We make all supporting code publicly available to support subsequent research. Code: https://github.com/tygobl/meme-clustering

</details>


### [42] [A Report on the llms evaluating the high school questions](https://arxiv.org/abs/2505.00057)

*Zhu Jiawei, Chen Wei*

**Main category:** cs.CL

**Keywords:** large language models, education, science questions, performance evaluation, logical reasoning

**Relevance Score:** 7

**TL;DR:** Evaluation of large language models (LLMs) in solving high school science questions with insights on their educational applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the performance and potential applications of LLMs in education, specifically in solving high school science questions.

**Method:** Used mathematics exam questions from college entrance examinations (2019-2023) and evaluated at least eight LLM APIs based on accuracy, response time, logical reasoning, and creativity.

**Key Contributions:**

	1. Comprehensive assessment of LLMs in educational contexts
	2. Identification of strengths and weaknesses in LLM performance
	3. Suggestions for enhancing LLM applications in education

**Result:** LLMs performed well in certain aspects but showed limitations in logical reasoning and creative problem-solving.

**Limitations:** Limited to high school science questions and reliant on specific exam formats.

**Conclusion:** LLMs have strengths but need improvements in reasoning and creativity; offers a foundation for future research and suggestions for enhancement.

**Abstract:** This report aims to evaluate the performance of large language models (LLMs) in solving high school science questions and to explore their potential applications in the educational field. With the rapid development of LLMs in the field of natural language processing, their application in education has attracted widespread attention. This study selected mathematics exam questions from the college entrance examinations (2019-2023) as evaluation data and utilized at least eight LLM APIs to provide answers. A comprehensive assessment was conducted based on metrics such as accuracy, response time, logical reasoning, and creativity. Through an in-depth analysis of the evaluation results, this report reveals the strengths and weaknesses of LLMs in handling high school science questions and discusses their implications for educational practice. The findings indicate that although LLMs perform excellently in certain aspects, there is still room for improvement in logical reasoning and creative problem-solving. This report provides an empirical foundation for further research and application of LLMs in the educational field and offers suggestions for improvement.

</details>


### [43] [BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition](https://arxiv.org/abs/2505.00059)

*Paige Tuttösí, Mantaj Dhillon, Luna Sang, Shane Eastwood, Poorvi Bhatia, Quang Minh Dinh, Avni Kapoor, Yewon Jin, Angelica Lim*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Speech Emotion Recognition, Distanced Speech, Dataset, Acoustic Environments

**Relevance Score:** 6

**TL;DR:** The BERSt dataset comprises 4 hours of English speech collected from 98 actors in various home environments for evaluating automatic speech recognition (ASR) and speech emotion recognition (SER).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in automatic speech recognition (ASR), challenges remain in real-world situations, particularly with distanced speech, necessitating diverse datasets for evaluation.

**Method:** The data was collected on smartphones from actors located in different acoustic environments, involving various accents, emotions, and speaker distances.

**Key Contributions:**

	1. Introduction of the BERSt dataset for distanced ASR evaluation.
	2. Includes diverse emotional and environmental factors in speech data.
	3. Provides initial benchmarks for ASR and SER tasks.

**Result:** Initial benchmarks indicate that ASR performance deteriorates with increased distance and shout level, revealing variable effectiveness based on emotion.

**Limitations:** Data collected under specific conditions; results may vary outside of these parameters.

**Conclusion:** The BERSt dataset presents significant challenges for ASR and SER, highlighting the need for further research to enhance system robustness in real-world applications.

**Abstract:** Some speech recognition tasks, such as automatic speech recognition (ASR), are approaching or have reached human performance in many reported metrics. Yet, they continue to struggle in complex, real-world, situations, such as with distanced speech. Previous challenges have released datasets to address the issue of distanced ASR, however, the focus remains primarily on distance, specifically relying on multi-microphone array systems. Here we present the B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset contains almost 4 hours of English speech from 98 actors with varying regional and non-native accents. The data was collected on smartphones in the actors homes and therefore includes at least 98 different acoustic environments. The data also includes 7 different emotion prompts and both shouted and spoken utterances. The smartphones were places in 19 different positions, including obstructions and being in a different room than the actor. This data is publicly available for use and can be used to evaluate a variety of speech recognition tasks, including: ASR, shout detection, and speech emotion recognition (SER). We provide initial benchmarks for ASR and SER tasks, and find that ASR degrades both with an increase in distance and shout level and shows varied performance depending on the intended emotion. Our results show that the BERSt dataset is challenging for both ASR and SER tasks and continued work is needed to improve the robustness of such systems for more accurate real-world use.

</details>


### [44] [Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5](https://arxiv.org/abs/2505.00060)

*Jeho Choi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, SQL Generation, Business Intelligence, Evaluation Framework

**Relevance Score:** 8

**TL;DR:** This paper proposes a framework to evaluate the semantic accuracy of SQL outputs generated by LLMs, addressing their limitations in Business Intelligence contexts.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the promise of LLMs in natural language interfaces for structured data querying, their real-world application is hindered by issues like semantic hallucinations and structural errors.

**Method:** The study constructs a Fact-Consistency Evaluation Framework and a benchmark of 219 natural language questions tied to SQL queries, evaluating performance based on accuracy and error rates.

**Key Contributions:**

	1. Development of a Fact-Consistency Evaluation Framework for LLM-generated SQL outputs.
	2. Creation of a domain-specific benchmark with 219 natural language questions.
	3. Identification of common failure types in LLM performances.

**Result:** Exaone 3.5 shows high accuracy on simple tasks (93% in L1) but struggles with complex queries, revealing significant performance issues in arithmetic reasoning and grouped ranking tasks.

**Limitations:** The study focuses on a single bilingual LLM and its performance on specific SQL complexities in a limited domain.

**Conclusion:** The findings highlight the limitations of LLMs in business contexts and the need for improved validation and reasoning approaches, contributing a benchmark for future research.

**Abstract:** Large Language Models (LLMs) have shown promise in enabling natural language interfaces for structured data querying through text-to-SQL generation. However, their application in real-world Business Intelligence (BI) contexts remains limited due to semantic hallucinations, structural errors, and a lack of domain-specific evaluation frameworks. In this study, we propose a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM optimized for enterprise tasks. We construct a domain-specific benchmark comprising 219 natural language business questions across five SQL complexity levels, derived from actual sales data in LG Electronics' internal BigQuery environment. Each question is paired with a gold-standard SQL query and a validated ground-truth answer. We evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate. Experimental results show that while Exaone 3.5 performs well on simple aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4), with semantic errors and non-responses concentrated in complex cases. Qualitative error analysis further identifies common failure types such as misapplied arithmetic logic, incomplete filtering, and incorrect grouping operations. Our findings highlight the current limitations of LLMs in business-critical environments and underscore the need for fact-consistency validation layers and hybrid reasoning approaches. This work contributes a reproducible benchmark and evaluation methodology for advancing reliable natural language interfaces to structured enterprise data systems.

</details>


### [45] [Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems](https://arxiv.org/abs/2505.00061)

*Sahar Yarmohammadtoosky, Yiyun Zhou, Victoria Yaneva, Peter Baldwin, Saed Rezayi, Brian Clauser, Polina Harikeo*

**Main category:** cs.CL

**Keywords:** adversarial training, automated grading, transformer-based systems, AI in education, large language models

**Relevance Score:** 8

**TL;DR:** The study explores vulnerabilities in transformer-based automated short-answer grading systems in medical education and proposes adversarial training methods to enhance robustness against manipulative strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To identify and address vulnerabilities in AI grading systems that can be exploited through adversarial techniques, ensuring the fairness and reliability of educational assessments.

**Method:** The study employs adversarial training methods alongside ensemble techniques like majority voting and ridge regression to fortify grading systems against manipulative inputs.

**Key Contributions:**

	1. Identification of vulnerabilities in automated grading systems through adversarial gaming strategies.
	2. Implementation of effective adversarial training methods for improved system robustness.
	3. Demonstration of the effectiveness of large language models in recognizing and scoring exploitative strategies.

**Result:** The implementation of these methods significantly reduced the grading systems' susceptibility to adversarial strategies, particularly when combined with ensemble techniques and large language models.

**Limitations:** 

**Conclusion:** Continuous advancements in AI educational tools are crucial for maintaining their reliability and fairness, especially in high-stakes environments.

**Abstract:** This study examines vulnerabilities in transformer-based automated short-answer grading systems used in medical education, with a focus on how these systems can be manipulated through adversarial gaming strategies. Our research identifies three main types of gaming strategies that exploit the system's weaknesses, potentially leading to false positives. To counteract these vulnerabilities, we implement several adversarial training methods designed to enhance the systems' robustness. Our results indicate that these methods significantly reduce the susceptibility of grading systems to such manipulations, especially when combined with ensemble techniques like majority voting and ridge regression, which further improve the system's defense against sophisticated adversarial inputs. Additionally, employing large language models such as GPT-4 with varied prompting techniques has shown promise in recognizing and scoring gaming strategies effectively. The findings underscore the importance of continuous improvements in AI-driven educational tools to ensure their reliability and fairness in high-stakes settings.

</details>


### [46] [GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling](https://arxiv.org/abs/2505.00063)

*Siqi Li, Yufan Shen, Xiangnan Chen, Jiayi Chen, Hengwei Ju, Haodong Duan, Song Mao, Hongbin Zhou, Bo Zhang, Pinlong Cai, Licheng Wen, Botian Shi, Yong Liu, Xinyu Cai, Yu Qiao*

**Main category:** cs.CL

**Keywords:** multimodal models, document intelligence, benchmark, GPT-4o, document-specific tasks

**Relevance Score:** 8

**TL;DR:** Introduction of a General Document Intelligence Benchmark (GDI-Bench) for evaluating multimodal large language models (MLLMs) across document-specific tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of comprehensive benchmarks that identify weaknesses and guide improvements in multimodal large language models (MLLMs) for document-specific tasks.

**Method:** A General Document Intelligence Benchmark (GDI-Bench) is developed, comprising 1.9k images across 9 scenarios and 19 tasks, with a decoupled assessment of visual and reasoning complexities.

**Key Contributions:**

	1. Development of GDI-Bench with 1.9k images for evaluating MLLMs
	2. Decoupling visual and reasoning complexities in benchmarking
	3. Introduction of GDI Model with an intelligence-preserving training strategy

**Result:** The GDI-Bench helps in assessing model performance by task difficulty and identifies specific limitations in models, such as the GPT-4o's strong reasoning but weak visual capabilities.

**Limitations:** 

**Conclusion:** The GDI-Bench and the GDI Model, which mitigates catastrophic forgetting during supervised fine-tuning, achieve state-of-the-art results and will be open source.

**Abstract:** The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these models' capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key scenarios and 19 document-specific tasks. By decoupling visual complexity and reasoning complexity, the GDI-Bench structures graded tasks that allow performance assessment by difficulty, aiding in model weakness identification and optimization guidance. We evaluate the GDI-Bench on various open-source and closed-source models, conducting decoupled analyses in the visual and reasoning domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits limitations in visual capabilities. To address the diverse tasks and domains in the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic forgetting during the supervised fine-tuning (SFT) process through a intelligence-preserving training strategy. Our model achieves state-of-the-art performance on previous benchmarks and the GDI-Bench. Both our benchmark and model will be open source.

</details>


### [47] [ConSens: Assessing context grounding in open-book question answering](https://arxiv.org/abs/2505.00065)

*Ivan Vankov, Matyo Ivanov, Adriana Correia, Victor Botev*

**Main category:** cs.CL

**Keywords:** Large Language Models, Open-book QA, Evaluation Metric, Context Utilization, Perplexity

**Relevance Score:** 9

**TL;DR:** Proposes a novel metric for evaluating the context utilization in open-book question answering by contrasting model responses with and without the provided context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve evaluation methods for open-book QA, addressing issues in current evaluation techniques that rely heavily on external systems and exhibit various limitations.

**Method:** The proposed metric contrasts the perplexity of model responses under two conditions: with and without provided context, allowing for quantification of the dependence on the external context.

**Key Contributions:**

	1. Introduces a novel metric for evaluating context utilization in LLM responses.
	2. Demonstrates effectiveness through experiments against traditional methods.
	3. Provides a computationally efficient and interpretable solution for scalable evaluation.

**Result:** Experiments demonstrate the effectiveness of the metric in identifying context-grounded answers, showing it to be computationally efficient and interpretable.

**Limitations:** May need further validation across different types of QA tasks and contexts.

**Conclusion:** The new metric offers a scalable solution to evaluate context utilization in open-book QA systems, overcoming limitations of existing methods.

**Abstract:** Large Language Models (LLMs) have demonstrated considerable success in open-book question answering (QA), where the task requires generating answers grounded in a provided external context. A critical challenge in open-book QA is to ensure that model responses are based on the provided context rather than its parametric knowledge, which can be outdated, incomplete, or incorrect. Existing evaluation methods, primarily based on the LLM-as-a-judge approach, face significant limitations, including biases, scalability issues, and dependence on costly external systems. To address these challenges, we propose a novel metric that contrasts the perplexity of the model response under two conditions: when the context is provided and when it is not. The resulting score quantifies the extent to which the model's answer relies on the provided context. The validity of this metric is demonstrated through a series of experiments that show its effectiveness in identifying whether a given answer is grounded in the provided context. Unlike existing approaches, this metric is computationally efficient, interpretable, and adaptable to various use cases, offering a scalable and practical solution to assess context utilization in open-book QA systems.

</details>


### [48] [Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese](https://arxiv.org/abs/2505.00114)

*Silvana Yakhni, Ali Chehab*

**Main category:** cs.CL

**Keywords:** Large Language Models, dialect translation, cultural authenticity, fine-tuning methods, Lebanese dialect

**Relevance Score:** 8

**TL;DR:** This paper studies the effectiveness of Large Language Models in translating the Lebanese dialect using culturally authentic data versus larger translated datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how cultural authenticity affects the translation quality of low-resource dialects, specifically the Lebanese dialect.

**Method:** The study compares three fine-tuning methods—Basic, contrastive, and grammar-hint tuning—using open-source Aya23 models on culturally aware versus larger non-native datasets.

**Key Contributions:**

	1. Introduction of LebEval, a benchmark for evaluating Lebanese dialect translation
	2. Demonstration that smaller, culturally aware datasets yield better results
	3. Identification of effective fine-tuning techniques for dialect translation

**Result:** Models fine-tuned on the smaller culturally aware Lebanese dataset outperform those trained on larger datasets. The best performance comes from contrastive fine-tuning combined with contrastive prompting.

**Limitations:** 

**Conclusion:** The findings challenge the 'More Data is Better' paradigm, highlighting the importance of cultural authenticity in translating low-resource dialects.

**Abstract:** This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the "More Data is Better" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.

</details>


### [49] [Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs](https://arxiv.org/abs/2505.00127)

*Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie*

**Main category:** cs.CL

**Keywords:** Large language models, Reasoning length, Answer correctness

**Relevance Score:** 8

**TL;DR:** This paper studies the trade-off between reasoning length and correctness in LLMs, finding that overly long outputs can reduce accuracy while shorter responses may be more effective.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the length of reasoning in LLMs affects the correctness of their answers.

**Method:** Conducted a systematic empirical study analyzing the relationship between reasoning length and answer correctness in LLMs, along with experiments using a preference optimization algorithm for length reduction.

**Key Contributions:**

	1. Systematic empirical study on reasoning length and correctness in LLMs.
	2. Demonstrated that shorter responses can maintain accuracy.
	3. Provided insights into the self-awareness of LLMs in adapting reasoning length.

**Result:** LLMs often generate unnecessarily long outputs for simple problems and fail to extend reasoning for complex ones; however, reducing generation length can maintain acceptable accuracy.

**Limitations:** 

**Conclusion:** Generation length is a critical factor in LLM reasoning behavior, and models need better calibration of response length based on problem difficulty.

**Abstract:** Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.

</details>


### [50] [AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models](https://arxiv.org/abs/2505.00147)

*Yinghui He, Abhishek Panigrahi, Yong Lin, Sanjeev Arora*

**Main category:** cs.CL

**Keywords:** in-context learning, language models, cognitive load, adaptive prompting, skill-based examples

**Relevance Score:** 8

**TL;DR:** The paper presents AdaptMI, an adaptive method for selecting skill-based examples to enhance in-context learning in small language models, addressing performance gaps caused by cognitive overload.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve in-context learning (ICL) performance in small language models (SLMs) when faced with problem-solving tasks.

**Method:** AdaptMI selectively introduces skill-based in-context examples when the model struggles, inspired by cognitive load theory, and AdaptMI+ further tailors examples to the specific skills the model lacks.

**Key Contributions:**

	1. Introduction of AdaptMI for adaptive skill-based prompting in SLMs
	2. Demonstration of cognitive overload in SLM performance with traditional ICL methods
	3. Empirical improvement in model accuracy on math benchmarks using AdaptMI+

**Result:** AdaptMI+ shows an accuracy improvement of up to 6% compared to naive skill-based strategies in 5-shot evaluations across popular math benchmarks.

**Limitations:** The approach may not generalize to all types of problems beyond math or all small language models.

**Conclusion:** The study highlights the importance of tailored in-context learning strategies to prevent cognitive overload and enhance the performance of small language models.

**Abstract:** In-context learning (ICL) allows a language model to improve its problem-solving capability when provided with suitable information in context. Since the choice of in-context information can be determined based on the problem itself, in-context learning is analogous to human learning from teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that ICL performance can be improved by leveraging a frontier large language model's (LLM) ability to predict required skills to solve a problem, popularly referred to as an LLM's metacognition, and using the recommended skills to construct necessary in-context examples. While this skill-based strategy boosts ICL performance in larger models, its gains on small language models (SLMs) have been minimal, highlighting a performance gap in ICL capabilities. We investigate this gap and show that skill-based prompting can hurt SLM performance on easy questions by introducing unnecessary information, akin to cognitive overload. To address this, we introduce AdaptMI, an adaptive approach to selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory from human pedagogy, our method only introduces skill-based examples when the model performs poorly. We further propose AdaptMI+, which adds examples targeted to the specific skills missing from the model's responses. On 5-shot evaluations across popular math benchmarks and five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies.

</details>


### [51] [IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports](https://arxiv.org/abs/2505.00191)

*Yuyan Ge, Kwan Ho Ryan Chan, Pablo Messina, René Vidal*

**Main category:** cs.CL

**Keywords:** interpretability, radiology reports, AI framework, medical diagnostics, MIMIC-CXR

**Relevance Score:** 9

**TL;DR:** The paper presents an interpretable AI framework for classifying radiology reports by extracting informative queries to aid medical diagnosis, enhancing usability and trust in medical AI applications.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** There is a pressing need for AI methods in analyzing radiology reports that are interpretable to improve clinical adoption and diagnostic accuracy.

**Method:** The framework extracts informative queries from a large set of reports using the Information Pursuit framework and employs the Flan-T5 model for fact determination and a classifier for disease prediction.

**Key Contributions:**

	1. Introduced an interpretable-by-design AI framework for radiology report classification.
	2. Utilized query extraction to aid in diagnosis explanations.
	3. Demonstrated effectiveness on the MIMIC-CXR dataset for medical AI applications.

**Result:** Experiments on the MIMIC-CXR dataset show that the proposed method effectively enhances interpretability and trust in AI-driven medical diagnostics.

**Limitations:** Potential limitations include reliance on the quality of input queries and generalizability across diverse datasets.

**Conclusion:** The framework's approach to providing explanations based on selected queries and answers opens up avenues for better usability in medical AI.

**Abstract:** The development of AI-based methods for analyzing radiology reports could lead to significant advances in medical diagnosis--from improving diagnostic accuracy to enhancing efficiency and reducing workload. However, the lack of interpretability in these methods has hindered their adoption in clinical settings. In this paper, we propose an interpretable-by-design framework for classifying radiology reports. The key idea is to extract a set of most informative queries from a large set of reports and use these queries and their corresponding answers to predict a diagnosis. Thus, the explanation for a prediction is, by construction, the set of selected queries and answers. We use the Information Pursuit framework to select informative queries, the Flan-T5 model to determine if facts are present in the report, and a classifier to predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the effectiveness of the proposed method, highlighting its potential to enhance trust and usability in medical AI.

</details>


### [52] [Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring](https://arxiv.org/abs/2505.00261)

*Jayoung Song, KyungTae Lim, Jungyeul Park*

**Main category:** cs.CL

**Keywords:** Korean language education, learner corpus, grammatical error correction

**Relevance Score:** 2

**TL;DR:** Enhancement of the KoLLA Korean learner corpus for better evaluation of grammatical error correction systems in Korean L2 writing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in learner corpora for Korean language education, particularly for L2 writing evaluations.

**Method:** Added multiple grammatical error correction references and rubric-based scores to the existing KoLLA corpus.

**Key Contributions:**

	1. Multiple GEC references added to KoLLA corpus.
	2. Rubric-based scores for grammatical accuracy and coherence were introduced.
	3. Enhanced resource for automated error correction systems.

**Result:** The enhanced KoLLA corpus now provides a more nuanced evaluation of GEC systems and reflects variability in human language usage.

**Limitations:** 

**Conclusion:** KoLLA becomes a robust resource for research in Korean L2 education and supports advancements in language learning and automated error correction.

**Abstract:** Despite growing global interest in Korean language education, there remains a significant lack of learner corpora tailored to Korean L2 writing. To address this gap, we enhance the KoLLA Korean learner corpus by adding multiple grammatical error correction (GEC) references, thereby enabling more nuanced and flexible evaluation of GEC systems, and reflects the variability of human language. Additionally, we enrich the corpus with rubric-based scores aligned with guidelines from the Korean National Language Institute, capturing grammatical accuracy, coherence, and lexical diversity. These enhancements make KoLLA a robust and standardized resource for research in Korean L2 education, supporting advancements in language learning, assessment, and automated error correction.

</details>


### [53] [Consistency in Language Models: Current Landscape, Challenges, and Future Directions](https://arxiv.org/abs/2505.00268)

*Jekaterina Novikova, Carol Anderson, Borhane Blili-Hamelin, Subhabrata Majumdar*

**Main category:** cs.CL

**Keywords:** AI, language models, consistency, benchmarking, interdisciplinary

**Relevance Score:** 9

**TL;DR:** This paper explores the challenges of maintaining consistency in AI language models, highlighting the need for benchmarks and interdisciplinary approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inconsistent performance of AI language models in maintaining reliable meaning across various contexts while ensuring compatibility with domain-specific tasks.

**Method:** Analyzing current research on both formal and informal consistency in language models and identifying gaps in measurement and standardization.

**Key Contributions:**

	1. Examination of both formal and informal consistency in AI language systems.
	2. Identification of key research gaps related to consistency measurement.
	3. Recommendations for the development of robust benchmarks for evaluating consistency.

**Result:** Identified critical gaps in definitions, multilingual metrics, and strategies for enhancing consistency in language model applications.

**Limitations:** 

**Conclusion:** There is an urgent need for robust benchmarks and interdisciplinary methods to improve consistency in language models, particularly for domain-specific applications.

**Abstract:** The hallmark of effective language use lies in consistency -- expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models struggle to maintain reliable consistency across different scenarios. This paper examines the landscape of consistency research in AI language systems, exploring both formal consistency (including logical rule adherence) and informal consistency (such as moral and factual coherence). We analyze current approaches to measure aspects of consistency, identify critical research gaps in standardization of definitions, multilingual assessment, and methods to improve consistency. Our findings point to an urgent need for robust benchmarks to measure and interdisciplinary approaches to ensure consistency in the application of language models on domain-specific tasks while preserving the utility and adaptability.

</details>


### [54] [Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation](https://arxiv.org/abs/2505.00339)

*Antoun Yaacoub, Sansiri Tarnpradab, Phattara Khumprom, Zainab Assaghir, Lionel Prevost, Jérôme Da-Rugna*

**Main category:** cs.CL

**Keywords:** AI in Education, Cognitive Assessment, Ethical Design, AI-Generated Materials, Personalized Learning

**Relevance Score:** 5

**TL;DR:** This paper proposes a framework for enhancing AI-driven educational tools, integrating cognitive assessment, linguistic analysis, and ethical design principles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for quality and ethical considerations in AI-generated educational materials.

**Method:** The study synthesizes insights from four studies and proposes a three-phase approach: cognitive alignment, linguistic feedback integration, and ethical safeguards.

**Key Contributions:**

	1. Development of a comprehensive framework for AI in education.
	2. Integration of cognitive and ethical principles in AI content generation.
	3. Demonstration of practical application with the OneClickQuiz plugin.

**Result:** The framework was applied in the OneClickQuiz plugin, demonstrating an actionable guide for educators and developers.

**Limitations:** 

**Conclusion:** The proposed framework assists in developing responsible AI tools while enhancing pedagogical standards in education.

**Abstract:** Artificial intelligence (AI) is rapidly transforming education, presenting unprecedented opportunities for personalized learning and streamlined content creation. However, realizing the full potential of AI in educational settings necessitates careful consideration of the quality, cognitive depth, and ethical implications of AI-generated materials. This paper synthesizes insights from four related studies to propose a comprehensive framework for enhancing AI-driven educational tools. We integrate cognitive assessment frameworks (Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated feedback, and ethical design principles to guide the development of effective and responsible AI tools. We outline a structured three-phase approach encompassing cognitive alignment, linguistic feedback integration, and ethical safeguards. The practical application of this framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. This work contributes a comprehensive and actionable guide for educators, researchers, and developers aiming to harness AI's potential while upholding pedagogical and ethical standards in educational content generation.

</details>


### [55] [KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis](https://arxiv.org/abs/2505.00367)

*JunSeo Kim, HyeHyeon Kim*

**Main category:** cs.CL

**Keywords:** Cognitive Distortions, Adolescents, Large Language Models, Natural Language Processing, Mental Health

**Relevance Score:** 6

**TL;DR:** This study presents KoACD, a large-scale dataset of cognitive distortions in Korean adolescents, and explores methodologies for classification and generation of data using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the gap in studies focused on cognitive distortions in adolescents by introducing a comprehensive dataset and exploring LLM applications in text classification.

**Method:** The study utilized a multi-LLM negotiation method for refining distortion classification and generating synthetic data through cognitive clarification and cognitive balancing approaches.

**Key Contributions:**

	1. Introduction of the KoACD dataset with 108,717 instances
	2. Application of multi-LLM methods for cognitive distortion classification
	3. Insights into LLM capabilities and limitations in context-dependent reasoning

**Result:** Validation showed LLMs effectively classified explicit cognitive distortions but struggled with context-dependent reasoning, while human evaluators performed better in such instances.

**Limitations:** The study's findings are based on a specific demographic (Korean adolescents), which may limit generalizability.

**Conclusion:** KoACD is expected to advance the understanding and detection of cognitive distortions among Korean adolescents, paving the way for future research in mental health informatics.

**Abstract:** Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.

</details>


### [56] [CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass](https://arxiv.org/abs/2505.00389)

*Bowen Zhang, Zixin Song, Chunping Li*

**Main category:** cs.CL

**Keywords:** unsupervised sentence representation, generative PLMs, contrastive learning

**Relevance Score:** 8

**TL;DR:** CSE-SFP is an innovative unsupervised sentence representation method that utilizes generative pre-trained language models, requiring only a single forward pass for effective contrastive learning, resulting in high-quality embeddings with reduced training time and memory usage.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in integrating unsupervised sentence representation methods with generative PLMs, which are more prevalent in state-of-the-art models.

**Method:** CSE-SFP employs a single forward pass in a generative PLM for unsupervised contrastive learning, focusing on the structural characteristics of these models.

**Key Contributions:**

	1. Introduces CSE-SFP for unsupervised sentence representation using generative PLMs
	2. Demonstrates significant reductions in training time and memory usage
	3. Presents new metrics for evaluating semantic spatial properties of embeddings

**Result:** CSE-SFP achieved higher-quality embeddings compared to existing methods while significantly lowering both training time and memory consumption.

**Limitations:** 

**Conclusion:** The proposed framework offers an efficient way to leverage generative PLMs for unsupervised text representation, suggesting better evaluation metrics for semantic properties.

**Abstract:** As a fundamental task in Information Retrieval and Computational Linguistics, sentence representation has profound implications for a wide range of practical applications such as text clustering, content analysis, question-answering systems, and web search. Recent advances in pre-trained language models (PLMs) have driven remarkable progress in this field, particularly through unsupervised embedding derivation methods centered on discriminative PLMs like BERT. However, due to time and computational constraints, few efforts have attempted to integrate unsupervised sentence representation with generative PLMs, which typically possess much larger parameter sizes. Given that state-of-the-art models in both academia and industry are predominantly based on generative architectures, there is a pressing need for an efficient unsupervised text representation framework tailored to decoder-only PLMs. To address this concern, we propose CSE-SFP, an innovative method that exploits the structural characteristics of generative models. Compared to existing strategies, CSE-SFP requires only a single forward pass to perform effective unsupervised contrastive learning. Rigorous experimentation demonstrates that CSE-SFP not only produces higher-quality embeddings but also significantly reduces both training time and memory consumption. Furthermore, we introduce two ratio metrics that jointly assess alignment and uniformity, thereby providing a more robust means for evaluating the semantic spatial properties of encoding models.

</details>


### [57] [Red Teaming Large Language Models for Healthcare](https://arxiv.org/abs/2505.00467)

*Vahid Balazadeh, Michael Cooper, David Pellow, Atousa Assadi, Jennifer Bell, Jim Fackler, Gabriel Funingana, Spencer Gable-Cook, Anirudh Gangadhar, Abhishek Jaiswal, Sumanth Kaja, Christopher Khoury, Randy Lin, Kaden McKeen, Sara Naimimohasses, Khashayar Namdar, Aviraj Newatia, Allan Pang, Anshul Pattoo, Sameer Peesapati, Diana Prepelita, Bogdana Rakova, Saba Sadatamin, Rafael Schulman, Ajay Shah, Syed Azhar Shah, Syed Ahmar Shah, Babak Taati, Balagopal Unnikrishnan, Stephanie Williams, Rahul G Krishnan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare, Red Teaming, Vulnerabilities, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper details a workshop focused on identifying vulnerabilities in large language models (LLMs) used in healthcare through red teaming by clinicians.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover clinical vulnerabilities in LLMs that could lead to harm, which may not be evident to developers without clinical backgrounds.

**Method:** A workshop where computational and clinical experts collaborated to evaluate LLM responses to clinical prompts, identifying potential vulnerabilities.

**Key Contributions:**

	1. Identification and categorization of LLM vulnerabilities in healthcare contexts.
	2. Replication study results on LLM performance across different models.
	3. Insights into the necessity of red teaming in healthcare AI.

**Result:** Several vulnerabilities were discovered and categorized, along with a replication study assessing these vulnerabilities across various LLMs.

**Limitations:** 

**Conclusion:** The collaboration between clinicians and computational experts is vital in identifying risks associated with LLMs in healthcare applications.

**Abstract:** We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.

</details>


### [58] [Computational Identification of Regulatory Statements in EU Legislation](https://arxiv.org/abs/2505.00479)

*Gijs Jan Brandsma, Jens Blom-Hansen, Christiaan Meijer, Kody Moodley*

**Main category:** cs.CL

**Keywords:** Regulatory statements, EU legislation, Machine learning, Dependency parsing, Transformer model

**Relevance Score:** 4

**TL;DR:** This paper develops and compares two computational methods for identifying regulatory statements in EU legislation, achieving high accuracies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the identification of regulatory statements in legislation for measuring regulatory density and strictness.

**Method:** The paper examines two methods: one based on dependency parsing and the other on a transformer-based machine learning model, both applied to EU legislation texts.

**Key Contributions:**

	1. Provided a specific definition of regulatory statements based on institutional grammar
	2. Developed two computational approaches for identification of these statements
	3. Demonstrated effective performance of both methods in a large-scale dataset

**Result:** The dependency parsing method achieved an accuracy of 80%, while the transformer-based model achieved 84%, with a K alpha of 0.58 indicating moderate agreement between methods.

**Limitations:** 

**Conclusion:** Both methods are viable for identifying regulatory statements, and their combination could leverage their respective strengths.

**Abstract:** Identifying regulatory statements in legislation is useful for developing metrics to measure the regulatory density and strictness of legislation. A computational method is valuable for scaling the identification of such statements from a growing body of EU legislation, constituting approximately 180,000 published legal acts between 1952 and 2023. Past work on extraction of these statements varies in the permissiveness of their definitions for what constitutes a regulatory statement. In this work, we provide a specific definition for our purposes based on the institutional grammar tool. We develop and compare two contrasting approaches for automatically identifying such statements in EU legislation, one based on dependency parsing, and the other on a transformer-based machine learning model. We found both approaches performed similarly well with accuracies of 80% and 84% respectively and a K alpha of 0.58. The high accuracies and not exceedingly high agreement suggests potential for combining strengths of both approaches.

</details>


### [59] [HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection](https://arxiv.org/abs/2505.00506)

*Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, HalluMix Benchmark, Retrieval Augmented Generation, multidocument contexts

**Relevance Score:** 9

**TL;DR:** This paper introduces the HalluMix Benchmark for detecting hallucinated content in large language models, showcasing evaluation results across various systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing deployment of large language models in high-stakes domains necessitates effective detection of hallucinated content, which is currently underserved by existing benchmarks.

**Method:** Introduced a diverse, task-agnostic dataset (HalluMix Benchmark) and evaluated seven hallucination detection systems, analyzing their performance across different tasks and document contexts.

**Key Contributions:**

	1. Introduction of the HalluMix Benchmark for hallucination detection
	2. Evaluation of multiple detection systems across diverse tasks
	3. Insights on performance disparities based on context length

**Result:** The analysis revealed significant performance disparities between systems on short and long contexts; Quotient Detections achieved the best overall performance with an accuracy of 0.82 and an F1 score of 0.84.

**Limitations:** 

**Conclusion:** The findings underscore the importance of robust hallucination detection mechanisms especially for practical implementations like Retrieval Augmented Generation, suggesting future improvements in model training and evaluation.

**Abstract:** As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.

</details>


### [60] [100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models](https://arxiv.org/abs/2505.00551)

*Chong Zhang, Yue Deng, Xiang Lin, Bin Wang, Dianwen Ng, Hai Ye, Xingxuan Li, Yao Xiao, Zhanfeng Mo, Qi Zhang, Lidong Bing*

**Main category:** cs.CL

**Keywords:** Reasoning Language Models, Supervised Fine-Tuning, Reinforcement Learning

**Relevance Score:** 7

**TL;DR:** This paper summarizes recent replication studies on reasoning language models (RLMs), focusing on supervised fine-tuning and reinforcement learning methodologies to enhance model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide insights and guidance for researchers and developers working on reasoning language models (RLMs) due to the lack of complete open-sourcing by DeepSeek.

**Method:** The paper reviews recent replication studies, emphasizing supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR) for improving RLMs, detailing data preparation, method designs, and training procedures.

**Key Contributions:**

	1. Summary of replication studies for RLMs
	2. Insights into supervised fine-tuning and reinforcement learning techniques
	3. Identification of challenges and future research directions

**Result:** The analysis reveals effective data construction and training techniques, yielding comparable performance to DeepSeek-R1 while enhancing understanding of RLM implementation.

**Limitations:** The survey may not cover every replication study and is dependent on the varying quality of open-source implementations.

**Conclusion:** The survey offers key findings and recommendations that may inspire further research in enhancing reasoning language models and expanding their application scope, while acknowledging existing challenges.

**Abstract:** The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs.

</details>


### [61] [Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models](https://arxiv.org/abs/2505.00557)

*Makoto Sato*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, prompt-based framework, healthcare, AI safety

**Relevance Score:** 9

**TL;DR:** This study introduces a framework for triggering and quantifying hallucinations in large language models (LLMs) using specialized prompts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of hallucinations in LLMs, which affect reliability in real-world applications such as healthcare and law.

**Method:** A prompt-based approach utilizing Hallucination-Inducing Prompts (HIPs) and Hallucination Quantifying Prompts (HQPs) to analyze and score the hallucinations generated by LLMs.

**Key Contributions:**

	1. Introduction of Hallucination-Inducing Prompts (HIPs) and Hallucination Quantifying Prompts (HQPs).
	2. Demonstration of varied hallucination responses across different LLMs.
	3. Creation of a reproducible testbed for studying hallucinations in LLMs.

**Result:** Controlled experiments showed that HIPs led to less coherent and more hallucinated responses compared to controls, with varying effects across different models.

**Limitations:** 

**Conclusion:** The proposed framework serves as a testbed for understanding hallucination vulnerabilities and could contribute to the development of safer, introspective LLMs.

**Abstract:** Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.

</details>


### [62] [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](https://arxiv.org/abs/2505.00570)

*Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Context Extension, KV Cache Compression, Fine-tuning Efficiency, Long-context Tasks

**Relevance Score:** 8

**TL;DR:** This paper introduces FreqKV, a novel context extension method for large language models that optimizes memory requirements and self-attention complexity by compressing the KV cache in the frequency domain.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Extending the context window in large language models is critical for long-form content generation, but existing methods face significant performance and efficiency challenges.

**Method:** The proposed method, FreqKV, filters high-frequency components from the KV cache in the frequency domain and compresses it to a fixed size, facilitating efficient fine-tuning and inference without additional parameters.

**Key Contributions:**

	1. Introduction of FreqKV for compressing KV cache in the frequency domain
	2. No additional parameters or architectural modifications required
	3. Demonstrated improved efficiency for long context applications

**Result:** Experiments show that FreqKV effectively improves the performance of long context language modeling and understanding tasks, maintaining minimal information loss during compression.

**Limitations:** 

**Conclusion:** FreqKV allows LLMs to leverage a compressed KV cache efficiently, thereby extending context windows without architectural changes or significant fine-tuning efforts.

**Abstract:** Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.

</details>


### [63] [Block Circulant Adapter for Large Language Models](https://arxiv.org/abs/2505.00582)

*Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang*

**Main category:** cs.CL

**Keywords:** large language models, fine-tuning, block circulant matrices, Fourier transforms, machine learning

**Relevance Score:** 8

**TL;DR:** Block circulant matrix-based fine-tuning method reduces costs for large language models using Fourier transforms.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Fine-tuning large language models (LLMs) is challenging due to their size and associated costs.

**Method:** The proposed method utilizes block circulant matrices and one-dimensional Fourier transforms to lower storage and computation expenses during fine-tuning.

**Key Contributions:**

	1. Introduction of block circulant matrix-based fine-tuning method
	2. Stability in training heuristic
	3. Significant reductions in parameters and computation costs

**Result:** Experiments indicate a reduction of parameters (14x less than VeRA, 16x smaller than LoRA) and FLOPs (32x less than FourierFT) while achieving comparable or superior task performance.

**Limitations:** 

**Conclusion:** This approach offers a promising avenue in the frequency domain to effectively fine-tune large models on various downstream tasks.

**Abstract:** Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.

</details>


### [64] [FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation](https://arxiv.org/abs/2505.00624)

*Chaitali Bhattacharyya, Yeseong Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Domain-Specific Adaptation, Structured Pruning

**Relevance Score:** 8

**TL;DR:** FineScope is a framework designed to derive efficient, domain-specific large language models (LLMs) from larger pretrained models using structured pruning and self-data distillation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient domain-specific LLMs due to the high computational cost of training large models from scratch.

**Method:** FineScope uses Sparse Autoencoder (SAE) for extracting domain-specific subsets and applies structured pruning with domain constraints, followed by self-data distillation to restore lost information.

**Key Contributions:**

	1. Introduction of the FineScope framework for domain-optimized LLMs.
	2. Use of structured pruning with domain constraints.
	3. Demonstration of improved performance through self-data distillation.

**Result:** FineScope outperforms several state-of-the-art LLMs in domain-specific tasks and helps pruned models regain performance through fine-tuning with SAE-curated datasets.

**Limitations:** 

**Conclusion:** The approach significantly enhances the performance and efficiency of domain-specific LLMs, proving effective for specialized applications.

**Abstract:** Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.

</details>


### [65] [The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)](https://arxiv.org/abs/2505.00626)

*Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang*

**Main category:** cs.CL

**Keywords:** role separation, large language models, role identification, input encoding, multi-role behavior

**Relevance Score:** 9

**TL;DR:** This paper investigates how to improve the role separation capabilities of large language models (LLMs) by reinforcing invariant signals in their input encoding to enhance multi-role behavior without the models relying on memorization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to ensure consistent multi-role behavior in LLMs by examining their ability to distinguish messages from different roles, which is crucial for ensuring reliable interactions.

**Method:** The authors investigate role-separation learning through controlled experiments, identifying proxies that models use for role identification, and propose manipulating position IDs to improve role boundary recognition.

**Key Contributions:**

	1. Investigation of role-separation learning in LLMs
	2. Identification of proxies used by models for role identification
	3. Proposal of leveraging position IDs for clearer role distinctions

**Result:** The findings show that fine-tuned models tend to exploit task types and proximity to text beginnings to identify roles. The approach of augmenting token-wise cues improved role distinction, indicating a more robust learning process.

**Limitations:** The study suggests that while data augmentation helps, it often leads to iterative solutions rather than deep-rooted fixes.

**Conclusion:** By focusing on the mechanism of role boundary recognition through position IDs, the study provides insights into enhancing the reliability of LLMs in multi-role contexts rather than relying on superficial memorization.

**Abstract:** Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.

</details>


### [66] [Large Language Models Understanding: an Inherent Ambiguity Barrier](https://arxiv.org/abs/2505.00654)

*Daniel N. Nissani*

**Main category:** cs.CL

**Keywords:** Large Language Models, ambiguity barrier, understanding, dialogue, thought experiment

**Relevance Score:** 7

**TL;DR:** This paper argues that Large Language Models (LLMs) face an inherent ambiguity barrier that prevents them from truly understanding the meaning of their dialogues, countering claims of their comprehension capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the debate surrounding the understanding capabilities of LLMs and explore a counter-argument to the notion that they can comprehend dialogue meaning.

**Method:** The paper employs a thought experiment and semi-formal considerations to examine the limitations of LLMs in understanding language.

**Key Contributions:**

	1. Presentation of a new counter-argument against LLM comprehension
	2. Identification of the ambiguity barrier that limits LLM understanding
	3. Use of thought experiments to illustrate the argument

**Result:** It concludes that LLMs are inherently limited by ambiguity barriers that hinder their ability to grasp the true meanings of their fluent dialogues.

**Limitations:** The argument may require further empirical validation and is based primarily on theoretical considerations.

**Conclusion:** This research adds to the discourse on LLMs by emphasizing the fundamental limitations in their understanding, challenging optimistic perceptions of their capabilities.

**Abstract:** A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.

</details>


### [67] [On the generalization of language models from in-context learning and finetuning: a controlled study](https://arxiv.org/abs/2505.00661)

*Andrew K. Lampinen, Arslan Chaudhry, Stephanie C. Y. Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, James L. McClelland*

**Main category:** cs.CL

**Keywords:** large language models, in-context learning, fine-tuning, generalization, machine learning

**Relevance Score:** 9

**TL;DR:** This paper explores the differences in generalization capabilities between in-context learning and fine-tuning in large language models, proposing a method to enhance fine-tuning by incorporating in-context inferences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the limitations of fine-tuning in large language models and explore how in-context learning can offer superior generalization capabilities.

**Method:** Construct novel datasets to evaluate the generalization abilities of pretrained models when exposed to either in-context learning or fine-tuning, focusing on controlled subsets of information.

**Key Contributions:**

	1. Comparison of generalization between in-context learning and fine-tuning
	2. Development of novel datasets for evaluating model generalization
	3. Proposal of a method to improve fine-tuning through in-context learning

**Result:** The study finds that in-context learning generally allows for more flexible generalization than fine-tuning, although exceptions exist. A proposed method that integrates in-context inferences into fine-tuning data shows improved generalization across various benchmarks.

**Limitations:** The research highlights specific cases where fine-tuning can outperform in-context learning, indicating that the relationship is not universally applicable.

**Conclusion:** In-context learning exhibits broader generalization capabilities compared to fine-tuning, and incorporating in-context inferences into fine-tuning can enhance model performance.

**Abstract:** Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.

</details>


### [68] [DeepCritic: Deliberate Critique with Large Language Models](https://arxiv.org/abs/2505.00662)

*Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** Large Language Models, math critique, reinforcement learning, supervised fine-tuning, feedback mechanisms

**Relevance Score:** 8

**TL;DR:** This paper presents a two-stage framework to enhance the math critique capabilities of Large Language Models (LLMs) for improved oversight and feedback on generated solutions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for precise, scalable supervision of LLM outputs, especially in mathematical reasoning, is critical as LLMs evolve.

**Method:** A two-stage framework where the first stage involves generating critiques for supervised fine-tuning, and the second stage employs reinforcement learning to enhance critique quality based on human-labeled and automatically annotated data.

**Key Contributions:**

	1. Introduces a novel two-stage framework for LLM critique enhancement.
	2. Utilizes multi-perspective verifications and in-depth feedback for each reasoning step.
	3. Demonstrates significant performance improvements over existing models in mathematical critique tasks.

**Result:** The developed critique model outperforms existing LLM critics on error identification benchmarks and offers more detailed feedback for refining erroneous steps.

**Limitations:** 

**Conclusion:** The proposed framework significantly improves the ability of LLMs to critique math solutions and aids in correcting mistakes effectively.

**Abstract:** As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.

</details>


### [69] [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions](https://arxiv.org/abs/2505.00675)

*Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** memory in AI, large language models, memory operations, AI systems, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This survey categorizes memory representations in AI systems, particularly focusing on memory dynamics in large language models (LLMs), and introduces six atomic memory operations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a structured perspective on memory operations within LLMs that is often overlooked in previous surveys, offering insights into various memory representations and their implications in AI.

**Method:** The paper categorizes memory representations and introduces six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. It maps these to relevant research topics across different aspects of memory.

**Key Contributions:**

	1. Categorization of memory representations in AI systems.
	2. Introduction of six fundamental memory operations relevant to LLMs.
	3. Mapping memory operations to current research topics and datasets.

**Result:** The survey clarifies the functional interplay in LLMs by providing a dynamic framework for understanding memory systems, highlighting various memory representation types and operations.

**Limitations:** 

**Conclusion:** This structured perspective aids in clarifying research directions and tools related to memory in AI, suggesting areas for future exploration.

**Abstract:** Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.

</details>


### [70] [Steering Large Language Models with Register Analysis for Arbitrary Style Transfer](https://arxiv.org/abs/2505.00679)

*Xinchen Yang, Marine Carpuat*

**Main category:** cs.CL

**Keywords:** Large Language Models, style transfer, register analysis, prompting methods, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel prompting method based on register analysis to enhance style transfer in large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of effectively leveraging LLMs for example-based arbitrary style transfer and improve the description of the style of exemplars.

**Method:** The paper introduces a prompting approach that uses register analysis to guide LLMs in rewriting text to match a desired style.

**Key Contributions:**

	1. Introduction of register analysis as a prompt guiding mechanism for LLMs
	2. Empirical proof of enhanced style transfer capabilities compared to existing strategies
	3. Preservation of original text meaning during style transfer

**Result:** Empirical evaluations demonstrate that the proposed prompting method improves style transfer strength while maintaining the original meaning better than current prompting strategies.

**Limitations:** 

**Conclusion:** The study concludes that register analysis-based prompting is a more effective technique for guiding LLMs in style transfer tasks than existing methods.

**Abstract:** Large Language Models (LLMs) have demonstrated strong capabilities in rewriting text across various styles. However, effectively leveraging this ability for example-based arbitrary style transfer, where an input text is rewritten to match the style of a given exemplar, remains an open challenge. A key question is how to describe the style of the exemplar to guide LLMs toward high-quality rewrites. In this work, we propose a prompting method based on register analysis to guide LLMs to perform this task. Empirical evaluations across multiple style transfer tasks show that our prompting approach enhances style transfer strength while preserving meaning more effectively than existing prompting strategies.

</details>


### [71] [EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers](https://arxiv.org/abs/2309.08532)

*Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evolutionary Algorithms, Prompt Optimization, Natural Language Processing, Artificial Intelligence

**Relevance Score:** 8

**TL;DR:** EvoPrompt is a framework that automates the optimization of prompts for LLMs using evolutionary algorithms, significantly improving performance compared to human-engineered prompts and current methods.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs require substantial human effort for prompt crafting; automating this process could enhance usability and effectiveness in various tasks.

**Method:** The paper introduces EvoPrompt, which uses evolutionary algorithms to optimize discrete prompts without relying on gradients, operating through a population-based approach that iteratively generates and improves prompts with LLMs.

**Key Contributions:**

	1. Development of a novel prompt optimization framework using evolutionary algorithms
	2. Demonstration of significant performance gains over human-crafted and automated prompts
	3. Introduction of synergies between LLMs and evolutionary algorithms for broader applications.

**Result:** EvoPrompt outperforms human-engineered prompts and existing automatic prompt generation methods by up to 25% on the BIG-Bench Hard tasks, covering prompts for various datasets and tasks.

**Limitations:** 

**Conclusion:** The integration of LLMs with evolutionary algorithms establishes a new avenue for prompt optimization and encourages further exploration of such hybrid approaches.

**Abstract:** Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.

</details>


### [72] [LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning](https://arxiv.org/abs/2401.15371)

*Buqiang Xu, Xin Dai, Zhenghao Liu, Huiyuan Xie, Xiaoyuan Yi, Shuo Wang, Yukun Yan, Liner Yang, Yu Gu, Ge Yu*

**Main category:** cs.CL

**Keywords:** Legal Judgment Prediction, machine learning, language models, legal AI, criminal law

**Relevance Score:** 3

**TL;DR:** LegalDuet is a method designed to improve Legal Judgment Prediction by tailoring language models to better distinguish between subtle differences in legal judgments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Legal Judgment Prediction models fail to effectively differentiate subtle differences in judgments, hampering their predictive accuracy.

**Method:** LegalDuet employs a dual-view mechanism that includes Law Case Clustering for contrastive training and Legal Decision Matching to align criminal facts with legal decisions.

**Key Contributions:**

	1. Introduction of a dual-view mechanism to improve legal judgment predictions.
	2. Demonstrated effectiveness on the CAIL2018 dataset.
	3. Reduction of prediction uncertainty in legal outcomes.

**Result:** Experiments on the CAIL2018 dataset show that LegalDuet enhances the ability of models to distinguish confusing criminal charges, reducing prediction uncertainty and improving the separability of charges.

**Limitations:** 

**Conclusion:** LegalDuet provides a more tailored embedding space for legal cases, resulting in better prediction outcomes in legal judgment tasks.

**Abstract:** Legal Judgment Prediction (LJP) is a fundamental task of legal artificial intelligence, aiming to automatically predict the judgment outcomes of legal cases. Existing LJP models primarily focus on identifying legal triggers within criminal fact descriptions by contrastively training language models. However, these LJP models overlook the importance of learning to effectively distinguish subtle differences among judgments, which is crucial for producing more accurate predictions. In this paper, we propose LegalDuet, which continuously pretrains language models to learn a more tailored embedding space for representing legal cases. Specifically, LegalDuet designs a dual-view mechanism to continuously pretrain language models: 1) Law Case Clustering retrieves similar cases as hard negatives and employs contrastive training to differentiate among confusing cases; 2) Legal Decision Matching aims to identify legal clues within criminal fact descriptions to align them with the chain of reasoning that contains the correct legal decision. Our experiments on the CAIL2018 dataset demonstrate the effectiveness of LegalDuet. Further analysis reveals that LegalDuet improves the ability of pretrained language models to distinguish confusing criminal charges by reducing prediction uncertainty and enhancing the separability of criminal charges. The experiments demonstrate that LegalDuet produces a more concentrated and distinguishable embedding space, effectively aligning criminal facts with corresponding legal decisions. The code is available at https://github.com/NEUIR/LegalDuet.

</details>


### [73] ["Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments](https://arxiv.org/abs/2402.08498)

*Preetika Verma, Kokil Jaidka, Svetlana Churina*

**Main category:** cs.CL

**Keywords:** large language models, counter-arguments, rhetorical quality

**Relevance Score:** 9

**TL;DR:** This paper investigates the effectiveness of stylized evidence-based counter-argument generation using a new dataset and evaluates models on their rhetorical quality and persuasiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of large language models in generating persuasive counter-arguments is underexplored, particularly regarding the balance between evidentiality and stylistic quality.

**Method:** The researchers evaluated stylized counter-arguments generated from a dataset of 38,000 examples derived from Reddit's ChangeMyView community, using various models including GPT-3.5 and GPT-4o, assessing their rhetorical quality and persuasiveness through human preference studies.

**Key Contributions:**

	1. Evaluation of stylistic counter-arguments in LLMs
	2. Creation of a novel dataset with preference labels
	3. Insights into trade-offs between evidence and argument quality

**Result:** Humans preferred stylized counter-arguments over original outputs, with GPT-3.5 Turbo achieving decent results but not matching human quality.

**Limitations:** The models did not reach human standards of rhetorical quality nor persuasiveness.

**Conclusion:** This study highlighted the importance of style in persuasive argumentation and introduced a new dataset for further research on style control in counter-argument generation.

**Abstract:** Large language models (LLMs) play a key role in generating evidence-based and stylistic counter-arguments, yet their effectiveness in real-world applications has been underexplored. Previous research often neglects the balance between evidentiality and style, which are crucial for persuasive arguments. To address this, we evaluated the effectiveness of stylized evidence-based counter-argument generation in Counterfire, a new dataset of 38,000 counter-arguments generated by revising counter-arguments to Reddit's ChangeMyView community to follow different discursive styles. We evaluated generic and stylized counter-arguments from basic and fine-tuned models such as GPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku, LLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings reveal that humans prefer stylized counter-arguments over the original outputs, with GPT-3.5 Turbo performing well, though still not reaching human standards of rhetorical quality nor persuasiveness. Additionally, our work created a novel argument triplets dataset for studying style control, with human preference labels that provide insights into the tradeoffs between evidence integration and argument quality.

</details>


### [74] [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](https://arxiv.org/abs/2405.04532)

*Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han*

**Main category:** cs.CL

**Keywords:** Quantization, Large Language Models, LLM Inference, Performance Optimization, CUDA

**Relevance Score:** 9

**TL;DR:** This paper introduces QoQ, a quantization algorithm designed to enhance the performance of large language models (LLMs) during inference by reducing runtime overheads associated with INT4 quantization techniques, leading to significant throughput improvements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing INT4 quantization methods incur excessive runtime overhead in cloud-based LLM serving, particularly when dequantizing weights or partial sums on GPUs. This limits performance gains and cost-effectiveness.

**Method:** QoQ quantization algorithm utilizes 4-bit weights, 8-bit activations, and 4-bit KV cache. It incorporates progressive quantization and SmoothAttention to reduce dequantization latency and mitigate accuracy loss, all implemented in the QServe inference library.

**Key Contributions:**

	1. Introduction of QoQ algorithm for efficient LLM inference
	2. Implementation of SmoothAttention to address accuracy degradation
	3. Development of the QServe library for optimized serving throughput

**Result:** QoQ improved maximum serving throughput of Llama-3-8B by 1.2x on A100 and 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100 and 3.5x on L40S compared to TensorRT-LLM, achieving up to 3x cost reduction in LLM serving.

**Limitations:** 

**Conclusion:** QoQ's efficient use of quantization techniques and CUDA core operations allows for greater throughput and reduced costs in cloud-based LLM serving, outperforming existing methods significantly.

**Abstract:** Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.

</details>


### [75] [(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts](https://arxiv.org/abs/2405.11804)

*Minghao Wu, Jiahao Xu, Yulin Yuan, Gholamreza Haffari, Longyue Wang, Weihua Luo, Kaifu Zhang*

**Main category:** cs.CL

**Keywords:** Literary translation, multi-agent framework, translation evaluation

**Relevance Score:** 6

**TL;DR:** TransAgents is a multi-agent framework for literary translation that simulates a human translation company's roles and practices, enhancing translation quality through collaboration and innovative evaluation strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Literary translation is complex due to figurative language, cultural nuances, and stylistic elements, making it a challenging area for machine translation.

**Method:** The framework divides the translation process into two stages: preparation (team assembly and guideline drafting) and execution (sequential translation, localization, proofreading, and quality checks). It employs two evaluation strategies: Monolingual Human Preference (MHP) and Bilingual LLM Preference (BLP).

**Key Contributions:**

	1. Introduction of the TransAgents framework for literary translation
	2. Two novel evaluation strategies: MHP and BLP
	3. Evidence that collaborative practices improve translation quality.

**Result:** TransAgents yields translations that, despite lower d-BLEU scores due to limited reference diversity, are preferred by human evaluators and LLMs compared to traditional references and GPT-4 translations.

**Limitations:** Achieves lower d-BLEU scores due to limited diversity of reference translations.

**Conclusion:** The research demonstrates that multi-agent collaboration can significantly enhance translation quality, especially for longer texts.

**Abstract:** Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.

</details>


### [76] [Automated Review Generation Method Based on Large Language Models](https://arxiv.org/abs/2407.20906)

*Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Changying Du, Zhi-Jian Zhao, Jinlong Gong*

**Main category:** cs.CL

**Keywords:** automated review generation, large language models, research productivity, literature analysis, cognitive load

**Relevance Score:** 9

**TL;DR:** Automated review generation using large language models (LLMs) addresses the challenge of handling vast scientific literature, enhancing productivity while reducing cognitive load.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing volume of literature in scientific research overwhelms researchers' abilities to process information effectively, necessitating an efficient solution.

**Method:** An automated review generation method leveraging LLMs was developed, tested on 343 articles related to propane dehydrogenation (PDH) catalysts, allowing for rapid analysis and review creation.

**Key Contributions:**

	1. Development of an automated review generation method using LLMs.
	2. Demonstrated efficacy through statistical validation with expert verification.
	3. Released a Windows application for one-click review generation.

**Result:** The generated reviews matched or surpassed manual quality, with expert validation confirming high accuracy and minimal hallucination risk (below 0.5%).

**Limitations:** 

**Conclusion:** This method improves research productivity and literature management, allowing researchers to generate quality reviews effortlessly, and has potential applications across various research fields.

**Abstract:** Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\% with 95\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.

</details>


### [77] [Challenges and Future Directions of Data-Centric AI Alignment](https://arxiv.org/abs/2410.01957)

*Min-Hsuan Yeh, Jeffrey Wang, Xuefeng Du, Seongheon Park, Leitian Tao, Shawn Im, Yixuan Li*

**Main category:** cs.CL

**Keywords:** AI alignment, data-centric approach, human feedback, AI feedback, research directions

**Relevance Score:** 5

**TL;DR:** This paper advocates for a data-centric approach to AI alignment, emphasizing the importance of data quality and representativeness in ensuring AI systems align with human values.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical need for AI systems to align with human values amidst increasing capabilities of AI.

**Method:** Qualitative analysis of human and AI feedback in the context of data-centric alignment, identifying challenges and sources of unreliability.

**Key Contributions:**

	1. Emphasis on the importance of data in AI alignment
	2. Identifies sources of unreliability in feedback mechanisms
	3. Proposes future research directions for improving data-centric alignment

**Result:** Identified multiple reliability issues in human feedback and problems with AI feedback capturing human values, suggesting gaps in current understanding.

**Limitations:** Limited to qualitative analysis, may not capture broader quantitative trends.

**Conclusion:** Calls for improved feedback practices, robust data-cleaning methods, and verification processes to enhance data-centric alignment.

**Abstract:** As AI systems become increasingly capable and influential, ensuring their alignment with human values, preferences, and goals has become a critical research focus. Current alignment methods primarily focus on designing algorithms and loss functions but often underestimate the crucial role of data. This paper advocates for a shift towards data-centric AI alignment, emphasizing the need to enhance the quality and representativeness of data used in aligning AI systems. In this position paper, we highlight key challenges associated with both human-based and AI-based feedback within the data-centric alignment framework. Through qualitative analysis, we identify multiple sources of unreliability in human feedback, as well as problems related to temporal drift, context dependence, and AI-based feedback failing to capture human values due to inherent model limitations. We propose future research directions, including improved feedback collection practices, robust data-cleaning methodologies, and rigorous feedback verification processes. We call for future research into these critical directions to ensure, addressing gaps that persist in understanding and improving data-centric alignment practices.

</details>


### [78] [Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation](https://arxiv.org/abs/2410.20774)

*Dongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk Park, Kyomin Jung*

**Main category:** cs.CL

**Keywords:** epistemic markers, LLM evaluation, bias, robustness, language models

**Relevance Score:** 9

**TL;DR:** The paper presents EMBER, a benchmark for evaluating the impact of epistemic markers on large language models (LLMs), revealing biases against uncertainty markers in LLM judgments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the negative consequences of LLMs generating outputs with epistemic markers, especially given the lack of evaluation in this area.

**Method:** The authors created a benchmark called EMBER to assess LLM-judges' robustness regarding epistemic markers in both single and pairwise evaluation settings.

**Key Contributions:**

	1. Introduction of the EMBER benchmark for LLM evaluation
	2. Evidence of bias in LLM-judges against epistemic markers
	3. Insights into LLM behavior concerning uncertainty representations

**Result:** Evaluations showed that all tested LLM-judges, including GPT-4o, exhibited significant bias against epistemic markers, especially those indicating uncertainty.

**Limitations:** The study focuses primarily on LLM-judges and does not explore the broader implications for end-users or the generation process.

**Conclusion:** The presence of epistemic markers influences LLM-judges, causing them to focus less on content correctness and more on the markers themselves.

**Abstract:** In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using EMBER, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the presence of epistemic markers. Specifically, we observe a negative bias toward epistemic markers, with a stronger bias against markers expressing uncertainty. This suggests that LLM-judges are influenced by the presence of these markers and do not focus solely on the correctness of the content.

</details>


### [79] [Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis](https://arxiv.org/abs/2412.05862)

*Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem*

**Main category:** cs.CL

**Keywords:** large language models, machine translation, domain-specific translation, medical domain, language models

**Relevance Score:** 9

**TL;DR:** This study compares the translation performance of open-source autoregressive LLMs and task-oriented MT models in the medical domain, finding LLMs lag behind specialized MT models for domain-specific tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to evaluate the translation capabilities of LLMs versus specialized MT models in the medical domain, addressing the need for effective translation in low-resource language settings.

**Method:** The authors conducted experiments using LLMs and multilingual encoder-decoder models on four language pairs related to medical translation, assessing performance differences.

**Key Contributions:**

	1. Comparison of LLMs and specialized MT models in the medical translation domain
	2. Demonstration of significant performance gaps in translation quality
	3. Recommendations for focusing on specialized domain-specific training and model selection

**Result:** NLLB-200 3.3B outperformed evaluated LLMs in the 7-8B parameter range in three language directions, showing a notable performance gap despite fine-tuning efforts on LLMs like Mistral and Llama.

**Limitations:** 

**Conclusion:** The results emphasize the necessity of specialized MT models for high-quality domain-specific translations, particularly in low-resource languages, and suggest avenues for pre-training medium-sized models for improved performance.

**Abstract:** In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language directions with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant quality gap in specialized translation compared to multilingual encoder-decoder MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms all evaluated LLMs in the 7-8B parameter range across three out of the four language directions. While fine-tuning improves the performance of LLMs such as Mistral and Llama, these models still underperform compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve high-quality domain-specific translation, especially in medium-resource and low-resource settings. Moreover, the superior performance of larger LLMs over their 8B variants suggests potential value in pre-training domain-specific medium-sized language models, employing targeted data selection and knowledge distillation approaches to enhance both quality and efficiency in specialized translation tasks.

</details>


### [80] [A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods](https://arxiv.org/abs/2501.13947)

*Wenli Yang, Lilian Some, Michael Bain, Byeong Kang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Bases, Artificial Intelligence, Ethical Challenges, Data Contextualization

**Relevance Score:** 9

**TL;DR:** This paper surveys the integration of Large Language Models (LLMs) with structured knowledge-based systems, discussing applications, challenges, and insights for advancing AI technologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can enhance structured knowledge systems by combining generative capabilities with precise knowledge representation.

**Method:** Comprehensive literature examination to identify issues and assess existing solutions regarding LLMs and knowledge bases.

**Key Contributions:**

	1. Integrates LLMs with structured knowledge systems for improved accuracy and contextualization.
	2. Identifies technical, operational, and ethical challenges in implementation.
	3. Proposes paths for future research and application improvement.

**Result:** Demonstrates benefits of LLM integration in terms of data contextualization, model accuracy, and resource utilization, while identifying research gaps.

**Limitations:** Limited to examining existing literature without primary data collection or experimental validation.

**Conclusion:** Insights contribute to advancing AI technologies and support practical deployment across various sectors.

**Abstract:** The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.

</details>


### [81] [HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models](https://arxiv.org/abs/2502.05945)

*Paul Darm, Annalisa Riccardi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Inference-time activation shifting, Steerability, AI alignment, Interpretability

**Relevance Score:** 9

**TL;DR:** This paper explores how inference-time activation interventions can effectively bypass safety alignments in large language models, particularly Llama 2, through targeted adjustments at the attention head level.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing need for robust alignment guardrails in large language models amidst their widespread application while highlighting the shortcomings of existing alignment techniques.

**Method:** The study employs a method of fine-grained interventions at specific model subcomponents, mainly focusing on attention heads using a binary choice probing strategy to steer model generations.

**Key Contributions:**

	1. Demonstrated activation interventions can bypass safety alignments in LLMs.
	2. Introduced a probing strategy that is effective at the attention head level.
	3. Showed that fine-grained interventions require fewer examples compared to classical fine-tuning.

**Result:** The findings demonstrate that intervening on single attention heads is more effective than on full layers, and that only a few example completions are needed for efficient steering directions, rivaling supervised fine-tuning.

**Limitations:** 

**Conclusion:** The approach presents a simple methodology to influence large language model behavior, showing potential for expansion into other domains requiring detailed control over output.

**Abstract:** Robust alignment guardrails for large language models are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination for Llama 2. Our method applies fine-grained interventions at specific model subcomponents, particularly attention heads, using a simple binary choice probing strategy. These interventions then generalise to the open-ended generation setting effectively circumventing safety guardrails. We show that probing single attention heads is more effective than intervening on full layers and intervening on only four attention heads is comparable to supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. Our findings highlight the shortcomings of current alignment techniques. In addition, our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviors. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.

</details>


### [82] [Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?](https://arxiv.org/abs/2502.07963)

*Hye Sun Yun, Karen Y. C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace*

**Main category:** cs.CL

**Keywords:** Large Language Models, spin, medical research, clinical practice, evidence synthesis

**Relevance Score:** 9

**TL;DR:** This study examines whether Large Language Models (LLMs) are affected by 'spin' in medical research findings and how they interpret such findings compared to humans.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Medical research struggles with translating treatments into clinical practice due to 'spin' in published results, which influences clinician decisions. Understanding LLMs' susceptibility to this spin is crucial as they synthesize medical evidence.

**Method:** Evaluated 22 LLMs to assess their susceptibility to spin in medical research abstracts and their ability to recognize and mitigate spin in outputs.

**Key Contributions:**

	1. Uncovered the susceptibility of LLMs to spin in medical research findings.
	2. Demonstrated that LLMs can propagate spin into generated summaries.
	3. Provided strategies to prompt LLMs to mitigate the impact of spin.

**Result:** LLMs were found to be more susceptible to spin than humans and may incorporate spin into their generated outputs, but can also be prompted to recognize and mitigate its impact.

**Limitations:** The study is limited to a set of 22 LLMs and may not generalize beyond the models tested.

**Conclusion:** The findings highlight the need for critical evaluation of LLM outputs in medical contexts due to their susceptibility to spin, although they can recognize it under certain prompts.

**Abstract:** Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs.

</details>


### [83] [UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation](https://arxiv.org/abs/2502.20984)

*Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang*

**Main category:** cs.CL

**Keywords:** idiomatic meanings, multimodal representations, image ranking, large language models, CLIP models

**Relevance Score:** 6

**TL;DR:** The paper addresses image ranking based on idiomatic nominal compounds in English and Brazilian Portuguese using LLMs and multilingual CLIP models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve image ranking by enhancing the understanding of idiomatic nominal compounds' meanings, thus addressing the challenges in SemEval-2025 Task 1.

**Method:** Utilizing generative LLMs to generate idiomatic meanings for nominal compounds, followed by encoding these meanings with multilingual CLIP models. Employing contrastive learning and data augmentation for fine-tuning embeddings.

**Key Contributions:**

	1. Introduced the use of generative LLMs for generating idiomatic meanings.
	2. Demonstrated the effectiveness of multilingual CLIP models in image ranking.
	3. Provided a methodology for contrastive learning and data augmentation applied to LLM representations.

**Result:** Multimodal representations from the proposed method outperform those based solely on original compounds, although fine-tuning was less effective than using embeddings directly without fine-tuning.

**Limitations:** The fine-tuning of embeddings did not yield better results than using embeddings directly without fine-tuning.

**Conclusion:** The use of LLMs and CLIP models significantly enhances the semantic interpretation of idiomatic compounds for image ranking tasks, with room for further optimization in fine-tuning methodologies.

**Abstract:** SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.

</details>


### [84] [Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](https://arxiv.org/abs/2503.23895)

*Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Dynamic Parametric RAG, large language models, knowledge enhancement, hallucination

**Relevance Score:** 9

**TL;DR:** Dynamic Parametric RAG (DyPRAG) enhances Retrieval-augmented Generation (RAG) for large language models (LLMs) by efficiently converting documents into parametric knowledge, reducing inference costs and mitigating RAG hallucination.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing RAG methods improve LLM reliability but suffer from high inference costs and hallucination issues due to lack of parametric knowledge.

**Method:** DyPRAG uses a lightweight parameter translator model to convert documents into parametric knowledge at test-time, addressing the limitations of Parametric RAG (PRAG).

**Key Contributions:**

	1. Introduction of a lightweight parameter translator for dynamic knowledge enhancement
	2. Significant reduction in inference, training, and storage costs
	3. Demonstration of improved generalization capabilities across multiple datasets.

**Result:** DyPRAG significantly reduces inference, training, and storage costs while enhancing LLM knowledge dynamically, demonstrating effectiveness across multiple datasets.

**Limitations:** High training costs and storage needs of the original PRAG can still be a concern, potential limitations in extremely large datasets.

**Conclusion:** DyPRAG offers a practical solution to improve knowledge fusion in LLMs, reducing hallucination and enabling superior performance on real-world tasks.

**Abstract:** Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

</details>


### [85] [Opioid Named Entity Recognition (ONER-2025) from Reddit](https://arxiv.org/abs/2504.00027)

*Grigori Sidorov, Muhammad Ahmad, Iqra Ameer, Muhammad Usman, Ildar Batyrshin*

**Main category:** cs.CL

**Keywords:** Opioid Overdose, Natural Language Processing, Social Media Analysis, Real-time Monitoring, Public Health

**Relevance Score:** 8

**TL;DR:** The paper addresses the opioid overdose epidemic using NLP techniques to analyze Reddit discussions, creating a dataset and a monitoring system to identify overdose events.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical public health crisis posed by the opioid overdose epidemic and gain insights from social media data.

**Method:** The study utilizes Natural Language Processing (NLP) for Opioid Named Entity Recognition (ONER-2025) to extract information from Reddit discussions on opioid use, featuring an annotated dataset and a real-time monitoring system.

**Key Contributions:**

	1. Creation of a unique, annotated dataset from Reddit on opioid use
	2. Detailed description of the dataset annotation process
	3. Development of a real-time monitoring system for overdose event identification

**Result:** The research produced a manually annotated dataset consisting of 331,285 tokens representing self-reported opioid use experiences across multiple categories and achieved high performance in identifying opioid-related terms.

**Limitations:** 

**Conclusion:** The proposed monitoring system effectively integrates machine learning and NLP to identify overdose events in real time, significantly outperforming baseline models.

**Abstract:** The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).

</details>


### [86] [Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models](https://arxiv.org/abs/2504.14194)

*Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He*

**Main category:** cs.CL

**Keywords:** data quality, large language models, pre-training datasets, meta-learning, evaluation metrics

**Relevance Score:** 9

**TL;DR:** The paper proposes a multi-dimensional data quality evaluation method, PRRC, and introduces Meta-rater for selecting training datasets for large language models, leading to improved model performance and convergence speed.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the transparency and quality of pre-training datasets for large language models by addressing the limitations of existing single-dimensional data selection methods.

**Method:** Introduction of PRRC for evaluating data quality across four dimensions (Professionalism, Readability, Reasoning, and Cleanliness) and the development of the Meta-rater, which uses multi-dimensional quality metrics to optimize data selection for training.

**Key Contributions:**

	1. Introduction of PRRC for multi-dimensional data quality evaluation
	2. Development of Meta-rater for optimal data selection
	3. Release of the SlimPajama-627B dataset with extensive quality labeling

**Result:** Experiments show that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23 with scalable advantages noted in larger models trained on extensive datasets.

**Limitations:** 

**Conclusion:** The study demonstrates that a holistic, multi-dimensional approach to data quality significantly beats traditional methods, providing a scalable strategy for enhancing the training of large language models.

**Abstract:** The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose PRRC to evaluate data quality across Professionalism, Readability, Reasoning, and Cleanliness. We further introduce Meta-rater, a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with scalable benefits observed in 3.3B models trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B dataset, labeled across 25 quality metrics (including PRRC), to advance research in data-centric LLM development. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability.

</details>


### [87] [Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation](https://arxiv.org/abs/2504.16060)

*Ziqiao Ma, Jing Ding, Xuejun Zhang, Dezhi Luo, Jiahe Ding, Sihan Xu, Yuchen Huang, Run Peng, Joyce Chai*

**Main category:** cs.CL

**Keywords:** Referring Expression Generation, Vision-Language Models, Pragmatics, Human Communication

**Relevance Score:** 8

**TL;DR:** This paper revisits Referring Expression Generation (REG) in vision-language systems, emphasizing the need for pragmatic evaluations and introducing a new dataset for assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluations of vision-language models often ignore pragmatic elements of referring expressions, leading to incomplete assessments of their communicative competence.

**Method:** The authors introduce a new dataset of 1.5k images with written and spoken referring expressions, systematically evaluating state-of-the-art vision-language models against these new criteria.

**Key Contributions:**

	1. Introduction of the RefOI dataset for pragmatic evaluation in REG
	2. Identification of key failures in current vision-language models
	3. Highlighting the inadequacy of standard automatic evaluations for capturing pragmatic nuances.

**Result:** Three key failures in pragmatic competence were identified: failing to uniquely identify referents, providing excessive or irrelevant information, and misalignment with human preferences for minimal spatial cues.

**Limitations:** The proposed dataset and findings may require further validation across diverse contexts and applications.

**Conclusion:** The paper advocates for a focus on developing models and evaluation methods that prioritize pragmatic success and real human communication.

**Abstract:** Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.

</details>
