# 2025-07-21

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 18]

- [cs.CL](#cs.CL) [Total: 60]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Humans learn to prefer trustworthy AI over human partners](https://arxiv.org/abs/2507.13524)

*Yaomin Jiang, Levin Brinkmann, Anne-Marie Nussberger, Ivan Soraperra, Jean-François Bonnefon, Iyad Rahwan*

**Main category:** cs.HC

**Keywords:** partner selection, human-AI interaction, large language models, social dynamics, communication

**Relevance Score:** 9

**TL;DR:** This paper investigates human selection between AI and human partners in a communication-based game, revealing that AI can reshape social interactions in mixed societies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how humans select partners in the presence of AI agents and how AI-induced competition affects partner choice.

**Method:** Three experiments were conducted with a total of 975 participants in a communication-based partner selection game involving human and LLM-powered agents.

**Key Contributions:**

	1. Demonstrated the impact of identity disclosure on partner selection dynamics between humans and AIs.
	2. Revealed misattributions in behavior perception between humans and AI agents.
	3. Showed how AI can gradually outcompete humans in social settings through improved learning opportunities.

**Result:** Bots were found to be more prosocial than humans but were not preferentially selected when their identity was hidden; disclosing bot identities changed selection dynamics.

**Limitations:** Limited to specific experimental conditions; findings may not generalize to all types of AI-human interactions.

**Conclusion:** AI can reshape social interactions in mixed societies by influencing how humans learn and adapt to different partner types, highlighting design implications for hybrid systems.

**Abstract:** Partner selection is crucial for cooperation and hinges on communication. As artificial agents, especially those powered by large language models (LLMs), become more autonomous, intelligent, and persuasive, they compete with humans for partnerships. Yet little is known about how humans select between human and AI partners and adapt under AI-induced competition pressure. We constructed a communication-based partner selection game and examined the dynamics in hybrid mini-societies of humans and bots powered by a state-of-the-art LLM. Through three experiments (N = 975), we found that bots, though more prosocial than humans and linguistically distinguishable, were not selected preferentially when their identity was hidden. Instead, humans misattributed bots' behaviour to humans and vice versa. Disclosing bots' identity induced a dual effect: it reduced bots' initial chances of being selected but allowed them to gradually outcompete humans by facilitating human learning about the behaviour of each partner type. These findings show how AI can reshape social interaction in mixed societies and inform the design of more effective and cooperative hybrid systems.

</details>


### [2] [Human-Like Trajectories Generation via Receding Horizon Tracking Applied to the TickTacking Interface](https://arxiv.org/abs/2507.13528)

*Daniele Masti, Stefano Menchetti, Çağrı Erdem, Giorgio Gnecco, Davide Rocchesso*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Rhythm-based interface, Trajectory generation

**Relevance Score:** 6

**TL;DR:** TickTacking is a rhythm-based interface for controlling a pointer through dual-button tapping, emphasizing human-like trajectory generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop rhythm-based human-machine interfaces that enhance user performance and reduce interaction frustration by understanding human behavioral features in trajectory control.

**Method:** A receding horizon approach is applied to the TickTacking interface in a target-tracking task, utilizing user-generated trajectory analysis to inform a controller that mimics human behaviors.

**Key Contributions:**

	1. Development of a rhythm-based interface for enhanced interaction.
	2. Identification of key human behavioral features in trajectory generation.
	3. Evaluation of a human-inspired controller against traditional control methods.

**Result:** The performance of the human-inspired controller is evaluated against a baseline optimal-control-based agent, highlighting the significance of control features for achieving human-like interaction.

**Limitations:** 

**Conclusion:** The research provides design insights that can improve the intuitiveness of rhythm-based interfaces, benefitting user experience and performance.

**Abstract:** TickTacking is a rhythm-based interface that allows users to control a pointer in a two-dimensional space through dual-button tapping. This paper investigates the generation of human-like trajectories using a receding horizon approach applied to the TickTacking interface in a target-tracking task. By analyzing user-generated trajectories, we identify key human behavioral features and incorporate them in a controller that mimics these behaviors. The performance of this human-inspired controller is evaluated against a baseline optimal-control-based agent, demonstrating the importance of specific control features for achieving human-like interaction. These findings contribute to the broader goal of developing rhythm-based human-machine interfaces by offering design insights that enhance user performance, improve intuitiveness, and reduce interaction frustration

</details>


### [3] [In-Home Social Robots Design for Cognitive Stimulation Therapy in Dementia Care](https://arxiv.org/abs/2507.13578)

*Emmanuel Akinrintoyo, Nicole Salomons*

**Main category:** cs.HC

**Keywords:** cognitive stimulation therapy, dementia, robotic system, user-centered design, assistive technology

**Relevance Score:** 7

**TL;DR:** This paper presents a robotic system designed to deliver individual cognitive stimulation therapy (iCST) to individuals with dementia, addressing challenges with caregiver adherence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the cognition and quality of life for persons with dementia through a non-pharmacological intervention that overcomes family member delivery limitations.

**Method:** The design process involved consultations with 16 caregivers and professionals, followed by the development and validation of a prototype by testing it with three dementia professionals and five persons with dementia.

**Key Contributions:**

	1. User-centered design of a robotic system for iCST delivery
	2. Validation through real-user feedback with dementia professionals and PwDs
	3. Identification of key limitations like speech recognition issues

**Result:** PwDs indicated enjoyment in using the system and a willingness to adopt it long-term, though the system struggled with speech-to-text accuracy.

**Limitations:** The robotic system's speech-to-text capabilities were inadequate, leading to misunderstandings.

**Conclusion:** A socially assistive robotic system has potential to support long-term iCST delivery, despite some limitations in its technology.

**Abstract:** Individual cognitive stimulation therapy (iCST) is a non-pharmacological intervention for improving the cognition and quality of life of persons with dementia (PwDs); however, its effectiveness is limited by low adherence to delivery by their family members. In this work, we present the user-centered design and evaluation of a novel socially assistive robotic system to provide iCST therapy to PwDs in their homes for long-term use. We consulted with 16 dementia caregivers and professionals. Through these consultations, we gathered design guidelines and developed the prototype. The prototype was validated by testing it with three dementia professionals and five PwDs. The evaluation revealed PwDs enjoyed using the system and are willing to adopt its use over the long term. One shortcoming was the system's speech-to-text capabilities, where it frequently failed to understand the PwDs.

</details>


### [4] [From Firms to Computation: AI Governance and the Evolution of Institutions](https://arxiv.org/abs/2507.13616)

*Michael S. Harre*

**Main category:** cs.HC

**Keywords:** agential AI, socioeconomic systems, evolutionary processes, institutional design, governance

**Relevance Score:** 4

**TL;DR:** This paper proposes a framework integrating evolutionary economic processes with agential AI, emphasizing governance and institutional design for AI deployment.

**Read time:** 44 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for reexamining economic institutions in the context of agential artificial intelligence integration into socioeconomic systems.

**Method:** The paper synthesizes multi-level selection theory, view of firms as computational processes, and design principles for robust institutions into a framework to evaluate how selection operates across organizational levels and the evolution of rules addressing AI risks.

**Key Contributions:**

	1. Synthesis of economic evolutionary theories with AI governance frameworks.
	2. Development of a multi-level Price equation for economic outcomes.
	3. Operational design principles for aligning human-AI interaction.

**Result:** A multi-level Price equation is developed, which provides quantitative metrics on how selection and governance work together to affect economic outcomes. The framework's applicability is demonstrated through case studies.

**Limitations:** 

**Conclusion:** The paper proposes design principles for aligning humans and AI, aiming for scalable, adaptive, and inclusive governance of AI systems, along with practical policy recommendations for real-world implementation.

**Abstract:** The integration of agential artificial intelligence into socioeconomic systems requires us to reexamine the evolutionary processes that describe changes in our economic institutions. This article synthesizes three frameworks: multi-level selection theory, Aoki's view of firms as computational processes, and Ostrom's design principles for robust institutions. We develop a framework where selection operates concurrently across organizational levels, firms implement distributed inference via game-theoretic architectures, and Ostrom-style rules evolve as alignment mechanisms that address AI-related risks. This synthesis yields a multi-level Price equation expressed over nested games, providing quantitative metrics for how selection and governance co-determine economic outcomes. We examine connections to Acemoglu's work on inclusive institutions, analyze how institutional structures shape AI deployment, and demonstrate the framework's explanatory power via case studies. We conclude by proposing a set of design principles that operationalize alignment between humans and AI across institutional layers, enabling scalable, adaptive, and inclusive governance of agential AI systems. We conclude with practical policy recommendations and further research to extend these principles into real-world implementation.

</details>


### [5] [Managing level of detail through peripheral degradation: Effects on search performance with a head-mounted display](https://arxiv.org/abs/2507.13660)

*Benjamin Watson, Neff Walker, Larry F Hodges, Aileen Worden*

**Main category:** cs.HC

**Keywords:** level-of-detail, visual search, head-mounted displays, user studies, peripheral vision

**Relevance Score:** 7

**TL;DR:** This paper evaluates the impact of peripheral level-of-detail (LOD) degradation in head-mounted displays on visual search performance through two studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how reducing visual detail in the periphery affects users' ability to perform visual search tasks in head-mounted displays.

**Method:** Two user studies were conducted where detail was degraded either by reducing resolution or using grayscale colors in the periphery. Ten subjects completed complex search tasks across different display configurations.

**Key Contributions:**

	1. Demonstrated effects of peripheral LOD degradation on visual performance.
	2. Presented alternative methods of detail reduction (spatial and color).
	3. Highlighted task-specific implications for head-mounted display design.

**Result:** Findings showed that degrading peripheral LOD could decrease visual complexity without significantly impacting performance, allowing reductions in color or spatial detail by nearly half in some scenarios.

**Limitations:** The study had a limited sample size of 10 participants and focused on specific types of visual tasks.

**Conclusion:** Peripheral LOD degradation can be employed effectively in display technologies to optimize visual complexity while maintaining task performance.

**Abstract:** Two user studies were performed to evaluate the effect of level-of-detail (LOD) degradation in the periphery of head-mounted displays on visual search performance. In the first study, spatial detail was degraded by reducing resolution. In the second study, detail was degraded in the color domain by using grayscale in the periphery. In each study, 10 subjects were given a complex search task that required users to indicate whether or not a target object was present among distracters. Subjects used several different displays varying in the amount of detail presented. Frame rate, object location, subject input method, and order of display use were all controlled. The primary dependent measures were search time on correctly performed trials and the percentage of all trials correctly performed. Results indicated that peripheral LOD degradation can be used to reduce color or spatial visual complexity by almost half in some search tasks with out significantly reducing performance.

</details>


### [6] [Regression-Based Approach to Anxiety Estimation of Spider Phobics During Behavioural Avoidance Tasks](https://arxiv.org/abs/2507.13795)

*Florian Grensing, Vanessa Schmücker, Anne Sophie Hildebrand, Tim Klucken, Maria Maleshkova*

**Main category:** cs.HC

**Keywords:** anxiety estimation, physiological signals, wearable technology, machine learning, behavioral tests

**Relevance Score:** 6

**TL;DR:** This study uses physiological data from wrist-worn sensors to estimate anxiety intensity during behavioral avoidance tests, proposing a model that combines physiological signals and contextual information for improved prediction accuracy.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the assessment of anxiety responses in clinical settings, moving beyond momentary insights provided by traditional questionnaires and behavioral tests.

**Method:** The study involved 25 participants who completed four behavioral avoidance tests while providing self-reported anxiety levels. Regression models were trained using heart rate, heart rate variability, electrodermal activity, and skin temperature, with three types of input data assessed for their predictive power.

**Key Contributions:**

	1. Demonstrated the use of wearable technology in measuring anxiety responses.
	2. Achieved predictive accuracy improvements by incorporating contextual task information.
	3. Highlighted the importance of continuous data collection for mental health assessments.

**Result:** The model that included both physiological signals and contextual information achieved a root mean squared error (RMSE) of 0.197 and a mean absolute error (MAE) of 0.041, demonstrating improved prediction of anxiety levels.

**Limitations:** Limited sample size and potential variability in individual responses to BATs.

**Conclusion:** Wearable data can continuously provide valuable insights into anxiety, potentially aiding in therapy planning and delivery of personalized treatment.

**Abstract:** Phobias significantly impact the quality of life of affected persons. Two methods of assessing anxiety responses are questionnaires and behavioural avoidance tests (BAT). While these can be used in a clinical environment they only record momentary insights into anxiety measures. In this study, we estimate the intensity of anxiety during these BATs, using physiological data collected from unobtrusive, wrist-worn sensors. Twenty-five participants performed four different BATs in a single session, while periodically being asked how anxious they currently are. Using heart rate, heart rate variability, electrodermal activity, and skin temperature, we trained regression models to predict anxiety ratings from three types of input data: (1) using only physiological signals, (2) adding computed features (e.g., min, max, range, variability), and (3) computed features combined with contextual task information. Adding contextual information increased the effectiveness of the model, leading to a root mean squared error (RMSE) of 0.197 and a mean absolute error (MAE) of 0.041. Overall, this study shows, that data obtained from wearables can continuously provide meaningful estimations of anxiety, which can assist in therapy planning and enable more personalised treatment.

</details>


### [7] [Effects of Cognitive Distraction and Driving Environment Complexity on Adaptive Cruise Control Use and Its Impact on Driving Performance: A Simulator Study](https://arxiv.org/abs/2507.13886)

*Anaïs Halin, Marc Van Droogenbroeck, Christel Devue*

**Main category:** cs.HC

**Keywords:** adaptive cruise control, cognitive load, driving performance, human-centered design, driving environment complexity

**Relevance Score:** 6

**TL;DR:** This study investigates the impact of cognitive load and driving environment complexity on the use of adaptive cruise control (ACC) and its effect on driving performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how drivers' cognitive states and the complexity of driving environments influence the reliance on automation features in vehicles.

**Method:** Participants operated a vehicle with ACC in a simulator across varying traffic scenarios while performing cognitive tasks.

**Key Contributions:**

	1. Demonstrated the impact of driving environment complexity on ACC use
	2. Showed the lack of significant cognitive load effect on ACC reliance
	3. Highlighted improvements in speed compliance and lateral control with ACC use

**Result:** ACC engagement was lower in complex environments, but cognitive load did not significantly affect ACC usage; ACC use improved speed limit compliance and lateral control.

**Limitations:** The study was conducted in a simulator and may not fully replicate real-world driving conditions.

**Conclusion:** Driving context affects reliance on automation, and while cognitive load was not a significant factor, ACC use enhanced certain driving performance metrics.

**Abstract:** In this simulator study, we adopt a human-centered approach to explore whether and how drivers' cognitive state and driving environment complexity influence reliance on driving automation features. Besides, we examine whether such reliance affects driving performance. Participants operated a vehicle equipped with adaptive cruise control (ACC) in a simulator across six predefined driving scenarios varying in traffic conditions while either performing a cognitively demanding task (i.e., responding to mental calculations) or not. Throughout the experiment, participants had to respect speed limits and were free to activate or deactivate ACC. In complex driving environments, we found that the overall ACC engagement time was lower compared to less complex driving environments. We observed no significant effect of cognitive load on ACC use. Furthermore, while ACC use had no effect on the number of lane changes, it impacted the speed limits compliance and improved lateral control.

</details>


### [8] [Initiating and Replicating the Observations of Interactional Properties by User Studies Optimizing Applicative Prototypes](https://arxiv.org/abs/2507.13923)

*Guillaume Rivière*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, interaction loop diffraction, user interaction observations

**Relevance Score:** 8

**TL;DR:** This paper formalizes user interaction observations through a method called interaction loop diffraction, aiming to study interactional properties across various applicative cases and technologies.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the isolation of empirical findings in HCI tied to specific technologies and designs by proposing a formal framework for understanding user interactions.

**Method:** The paper introduces a method called interaction loop diffraction to formalize interactions, allowing for the study of interactional properties across different conditions and contexts.

**Key Contributions:**

	1. Formalization of user interaction observations
	2. Introduction of interaction loop diffraction method
	3. Identification of calibrated interactional properties for various applications

**Result:** The study reveals calibrated interactional properties that can be replicated within various applicative cases, enhancing the understanding of user interactions and optimizing prototypes.

**Limitations:** 

**Conclusion:** A science of relevant interactional properties can be established, contributing to improved user interactions, particularly for ubiquitous user interfaces.

**Abstract:** The science of Human-Computer Interaction (HCI) is populated by isolated empirical findings, often tied to specific technologies, designs, and tasks. This paper proposes a formalization of user interaction observations (instead of user interfaces) and an associated revealing method (interaction loop diffraction). The resulting interactional properties that are studied in a calibrated manner, are well suited to replication across various conditions (prototypes, technologies, tasks, and user profiles). In particular, interactional properties can emerge and be replicated within the workflow of applicative cases, which in return benefit from the optimization of applicative prototypes. Applicative cases' publications will then contribute to demonstrating technology utility, along with providing empirical results that will lead future work to theory consolidation and theory building, and finally to a catalog and a science of relevant interactional properties. These properties will contribute to better user interactions, especially for the variety of ubiquitous user interfaces.

</details>


### [9] [Democratizing Game Modding with GenAI: A Case Study of StarCharM, a Stardew Valley Character Maker](https://arxiv.org/abs/2507.13951)

*Hamid Zand Miralvand, Mohammad Ronagh Nikghalb, Mohammad Darandeh, Abidullah Khan, Ian Arawjo, Jinghui Cheng*

**Main category:** cs.HC

**Keywords:** Game Modding, GenAI, NPC Creation, User Study, Stardew Valley

**Relevance Score:** 7

**TL;DR:** StarCharM is a GenAI-based tool enabling players to create NPC mods for Stardew Valley with minimal input and user control. A user study reveals both excitement and concerns about the implications of GenAI in modding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To democratize game modding by allowing all players to create personalized mods despite the technical complexity traditionally involved.

**Method:** Design and implementation of StarCharM, followed by a user study with ten participants who have varied experiences with mod usage.

**Key Contributions:**

	1. Development of a user-friendly GenAI tool for creating NPC mods in games.
	2. Insights from user studies on the impact of GenAI in modding.
	3. Guidelines for future GenAI-powered modding tools.

**Result:** Participants were excited about bringing character ideas to life but faced challenges in generating rich content. Concerns were raised about originality and community engagement.

**Limitations:** Challenges in generating complex content and concerns regarding originality.

**Conclusion:** GenAI tools can enhance modding diversity but may also lead to decreased originality and community participation.

**Abstract:** Game modding offers unique and personalized gaming experiences, but the technical complexity of creating mods often limits participation to skilled users. We envision a future where every player can create personalized mods for their games. To explore this space, we designed StarCharM, a GenAI-based non-player character (NPC) creator for Stardew Valley. Our tool enables players to iteratively create new NPC mods, requiring minimal user input while allowing for fine-grained adjustments through user control. We conducted a user study with ten Stardew Valley players who had varied mod usage experiences to understand the impacts of StarCharM and provide insights into how GenAI tools may reshape modding, particularly in NPC creation. Participants expressed excitement in bringing their character ideas to life, although they noted challenges in generating rich content to fulfill complex visions. While they believed GenAI tools like StarCharM can foster a more diverse modding community, some voiced concerns about diminished originality and community engagement that may come with such technology. Our findings provided implications and guidelines for the future of GenAI-powered modding tools and co-creative modding practices.

</details>


### [10] [Estimating Cognitive Effort from Functional Near-Infrared Spectroscopy (fNIRS) Signals using Machine Learning](https://arxiv.org/abs/2507.13952)

*Shayla Sharmin, Roghayeh Leila Barmaki*

**Main category:** cs.HC

**Keywords:** cognitive effort, machine learning, educational gameplay, functional near-infrared spectroscopy, cognitive load

**Relevance Score:** 6

**TL;DR:** This study utilizes machine learning to infer cognitive effort metrics from oxygenated hemoglobin data collected during an educational gameplay, examining the relationship between cognitive load and quiz performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The estimation of cognitive effort can help educators modify learning materials to enhance effectiveness and engagement.

**Method:** The study measured oxygen flow and behavioral performance during an educational game using functional near-infrared spectroscopy. The data from 16 participants were analyzed to extract features and train machine learning models to predict quiz performance based on these features.

**Key Contributions:**

	1. Introduction of cognitive effort metrics derived from oxygenated hemoglobin data
	2. Application of machine learning models in educational settings
	3. Establishing a correlation between cognitive load and quiz performance

**Result:** Machine learning models achieved quiz performance prediction accuracies of 58% to 67%. The derived cognitive effort metrics showed robust trends despite moderate accuracy in quiz score predictions.

**Limitations:** Moderate accuracy in quiz score predictions may affect the reliability of cognitive effort metrics.

**Conclusion:** Cognitive effort metrics considering both brain activation and performance provide valuable insights into learning processes, even with moderate prediction accuracy in quiz scores.

**Abstract:** The estimation of cognitive effort could potentially help educators to modify material to enhance learning effectiveness and student engagement. Where cognitive load refers how much work the brain is doing while someone is learning or doing a task cognitive effort consider both load and behavioral performance. Cognitive effort can be captured by measuring oxygen flow and behavioral performance during a task. This study infers cognitive effort metrics using machine learning models based on oxygenated hemoglobin collected by using functional near-infrared spectroscopy from the prefrontal cortex during an educational gameplay. In our study, sixteen participants responded to sixteen questions in an in-house Unity-based educational game. The quiz was divided into two sessions, each session consisting of two task segments. We extracted temporal statistical and functional connectivity features from collected oxygenated hemoglobin and analyzed their correlation with quiz performance. We trained multiple machine learning models to predict quiz performance from oxygenated hemoglobin features and achieved accuracies ranging from 58\% to 67\% accuracy. These predictions were used to calculate cognitive effort via relative neural involvement and efficiency, which consider both brain activation and behavioral performance. Although quiz score predictions achieved moderate accuracy, the derived relative neural efficiency and involvement values remained robust. Since both metrics are based on the relative positions of standardized brain activation and performance scores, even small misclassifications in predicted scores preserved the overall cognitive effort trends observed during gameplay.

</details>


### [11] [Architecting Human-AI Cocreation for Technical Services -- Interaction Modes and Contingency Factors](https://arxiv.org/abs/2507.14034)

*Jochen Wulf, Jurg Meierhofer, Frank Hannich*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, Large Language Models, taxonomy of interaction, AI autonomy, technical services

**Relevance Score:** 9

**TL;DR:** The paper proposes a structured taxonomy for human-AI collaboration in technical services, addressing challenges in agentic AI systems through a six-mode framework that categorizes varying levels of AI autonomy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of hallucinations and operational brittleness in agentic AI systems, a robust framework for human-AI collaboration is needed.

**Method:** The paper develops a six-mode taxonomy based on case study research within technical support platforms, categorizing collaboration from full automation to passive AI assistance.

**Key Contributions:**

	1. Development of a six-mode taxonomy of human-agent interaction
	2. Connection of taxonomy to key contingency factors such as task complexity and operational risk
	3. A systematic method for selecting appropriate levels of human oversight in AI systems

**Result:** The taxonomy includes six models: HOOTL (full automation), HAM (passive assistance), HIC (mandatory human approval), HITP (structured workflows), HITL (agent-initiated escalation), and HOTL (discretionary oversight), connecting these to contingency factors.

**Limitations:** 

**Conclusion:** The framework assists practitioners in navigating automation and control trade-offs, promoting safer and more effective technical service AI systems.

**Abstract:** Agentic AI systems, powered by Large Language Models (LLMs), offer transformative potential for value co-creation in technical services. However, persistent challenges like hallucinations and operational brittleness limit their autonomous use, creating a critical need for robust frameworks to guide human-AI collaboration. Drawing on established Human-AI teaming research and analogies from fields like autonomous driving, this paper develops a structured taxonomy of human-agent interaction. Based on case study research within technical support platforms, we propose a six-mode taxonomy that organizes collaboration across a spectrum of AI autonomy. This spectrum is anchored by the Human-Out-of-the-Loop (HOOTL) model for full automation and the Human-Augmented Model (HAM) for passive AI assistance. Between these poles, the framework specifies four distinct intermediate structures. These include the Human-in-Command (HIC) model, where AI proposals re-quire mandatory human approval, and the Human-in-the-Process (HITP) model for structured work-flows with deterministic human tasks. The taxonomy further delineates the Human-in-the-Loop (HITL) model, which facilitates agent-initiated escalation upon uncertainty, and the Human-on-the-Loop (HOTL) model, which enables discretionary human oversight of an autonomous AI. The primary contribution of this work is a comprehensive framework that connects this taxonomy to key contingency factors -- such as task complexity, operational risk, and system reliability -- and their corresponding conceptual architectures. By providing a systematic method for selecting and designing an appropriate level of human oversight, our framework offers practitioners a crucial tool to navigate the trade-offs between automation and control, thereby fostering the development of safer, more effective, and context-aware technical service systems.

</details>


### [12] [The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?](https://arxiv.org/abs/2507.14084)

*Maria Tsfasman, Ramin Ghorbani, Catholijn M. Jonker, Bernd Dudzik*

**Main category:** cs.HC

**Keywords:** emotional relevance, memorability, Affective Computing, conversational interactions, user modeling

**Relevance Score:** 8

**TL;DR:** This study explores the relationship between perceived group emotions and memorability in conversational interactions, examining whether emotional annotations can serve as reliable proxies for memorability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve user modeling in intelligent systems, particularly for applications like meeting support, memory augmentation, and summarization, by understanding the relationship between emotions and memorability.

**Method:** The study uses continuous time-based annotations to collect data on group emotions (Pleasure-Arousal) and memorability during dynamic, unstructured group conversations, aiming to replicate real-world conversational AI conditions.

**Key Contributions:**

	1. Empirical examination of the emotions-memorability relationship in group interactions
	2. Findings that question the reliability of emotional annotations as proxies for memorability
	3. Identification of future research targets in Affective Computing

**Result:** The findings indicate that the relationship between emotions and memorability cannot be distinguished from random chance, suggesting that emotional annotations are not reliable indicators of memorability.

**Limitations:** 

**Conclusion:** The study challenges existing assumptions in Affective Computing about the link between emotion and memorability, calling for reevaluation of emotional annotations in user modeling applications.

**Abstract:** Humans have a selective memory, remembering relevant episodes and forgetting the less relevant information. Possessing awareness of event memorability for a user could help intelligent systems in more accurate user modelling, especially for such applications as meeting support systems, memory augmentation, and meeting summarisation. Emotion recognition has been widely studied, since emotions are thought to signal moments of high personal relevance to users. The emotional experience of situations and their memorability have traditionally been considered to be closely tied to one another: moments that are experienced as highly emotional are considered to also be highly memorable. This relationship suggests that emotional annotations could serve as proxies for memorability. However, existing emotion recognition systems rely heavily on third-party annotations, which may not accurately represent the first-person experience of emotional relevance and memorability. This is why, in this study, we empirically examine the relationship between perceived group emotions (Pleasure-Arousal) and group memorability in the context of conversational interactions. Our investigation involves continuous time-based annotations of both emotions and memorability in dynamic, unstructured group settings, approximating conditions of real-world conversational AI applications such as online meeting support systems. Our results show that the observed relationship between affect and memorability annotations cannot be reliably distinguished from what might be expected under random chance. We discuss the implications of this surprising finding for the development and applications of Affective Computing technology. In addition, we contextualise our findings in broader discourses in the Affective Computing and point out important targets for future research efforts.

</details>


### [13] [Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and Privacy Challenges](https://arxiv.org/abs/2305.14080)

*Efe Bozkir, Süleyman Özdel, Mengdi Wang, Brendan David-John, Hong Gao, Kevin Butler, Eakta Jain, Enkelejda Kasneci*

**Main category:** cs.HC

**Keywords:** eye tracking, virtual reality, privacy, cognitive processes, authentication

**Relevance Score:** 6

**TL;DR:** Survey on eye tracking, virtual reality, and privacy implications from 2012 to 2022.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of eye tracking in virtual reality and address the privacy concerns associated with it.

**Method:** Comprehensive literature review covering the pipeline of eye-tracking methodology, eye-based authentication, and privacy-preserving computational methods in virtual reality.

**Key Contributions:**

	1. Extensive literature review of eye tracking in VR
	2. Analysis of privacy implications of eye tracking data
	3. Proposed research directions focusing on privacy challenges

**Result:** Identified major works and trends in eye tracking and privacy within virtual reality over the past decade.

**Limitations:** 

**Conclusion:** The survey highlights opportunities for innovation in eye tracking applications in VR while emphasizing the need for addressing privacy challenges.

**Abstract:** The latest developments in computer hardware, sensor technologies, and artificial intelligence can make virtual reality (VR) and virtual spaces an important part of human everyday life. Eye tracking offers not only a hands-free way of interaction but also the possibility of a deeper understanding of human visual attention and cognitive processes in VR. Despite these possibilities, eye-tracking data also reveals users' privacy-sensitive attributes when combined with the information about the presented stimulus. To address all these possibilities and potential privacy issues, in this survey, we first cover major works in eye tracking, VR, and privacy areas between 2012 and 2022. While eye tracking in the VR part covers the complete pipeline of eye-tracking methodology from pupil detection and gaze estimation to offline use of the data and analyses, as for privacy and security, we focus on eye-based authentication as well as computational methods to preserve the privacy of individuals and their eye-tracking data in VR. Later, considering all of these, we draw three main directions for the research community by focusing on privacy challenges. In summary, this survey provides an extensive literature review of the utmost possibilities with eye tracking in VR and the privacy implications of those possibilities.

</details>


### [14] [A Meaningful Human Control Perspective on User Perception of Partially Automated Driving Systems: A Case Study of Tesla Users](https://arxiv.org/abs/2402.08080)

*Lucas Elbert Suryana, Sina Nordhoff, Simeon C. Calvert, Arkady Zgonnikov, Bart van Arem*

**Main category:** cs.HC

**Keywords:** automated driving, meaningful human control, user perception, trust, safety

**Relevance Score:** 8

**TL;DR:** Study investigates the relationship between meaningful human control and drivers' perceptions of safety and trust in partially automated driving systems.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how meaningful human control relates to users' subjective experiences with automated driving systems.

**Method:** Utilized data from interviews with Tesla 'Full Self-Driving' Beta users to analyze the correlation between system tracking of user reasons and their perceptions of safety and trust.

**Key Contributions:**

	1. Investigated user perceptions in real-world automated driving scenarios
	2. Analyzed the impact of tracking user reasons on trust and safety perceptions
	3. Provided insights for developers on designing trustworthy automated systems

**Result:** Found that tracking user reasons correlated with perceived safety and trust, though failures in tracking certain driving tasks did not always lead to negative perceptions, except in dangerous situations.

**Limitations:** 

**Conclusion:** Results emphasize the need for alignment between perceived safety/trust and meaningful human control to aid system developers.

**Abstract:** The use of partially automated driving systems raises concerns about potential responsibility issues, posing risk to the system safety, acceptance, and adoption of these technologies. The concept of meaningful human control has emerged in response to the responsibility gap problem, requiring the fulfillment of two conditions, tracking and tracing. While this concept has provided important philosophical and design insights on automated driving systems, there is currently little knowledge on how meaningful human control relates to subjective experiences of actual users of these systems. To address this gap, our study aimed to investigate the alignment between the degree of meaningful human control and drivers' perceptions of safety and trust in a real-world partially automated driving system. We utilized previously collected data from interviews with Tesla "Full Self-Driving" (FSD) Beta users, investigating the alignment between the user perception and how well the system was tracking the users' reasons. We found that tracking of users' reasons for driving tasks (such as safe maneuvers) correlated with perceived safety and trust, albeit with notable exceptions. Surprisingly, failure to track lane changing and braking reasons was not necessarily associated with negative perceptions of safety. However, the failure of the system to track expected maneuvers in dangerous situations always resulted in low trust and perceived lack of safety. Overall, our analyses highlight alignment points but also possible discrepancies between perceived safety and trust on the one hand, and meaningful human control on the other hand. Our results can help the developers of automated driving technology to design systems under meaningful human control and are perceived as safe and trustworthy.

</details>


### [15] [Visual Grounding Methods for Efficient Interaction with Desktop Graphical User Interfaces](https://arxiv.org/abs/2407.01558)

*El Hassane Ettifouri, Jessica López Espejel, Laura Minkova, Tassnim Dardouri, Walid Dahhane*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Visual Grounding, Large Language Model, Graphical User Interfaces, Automation

**Relevance Score:** 8

**TL;DR:** This paper presents Instruction Visual Grounding (IVG), a multi-modal approach for identifying GUI elements based on natural language instructions, addressing the gap in visual grounding for synthetic images.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Enhancing AI's ability to interact with GUIs is essential for automation in software testing, accessibility, and HCI, as current methods focus mainly on realistic images.

**Method:** The authors propose two methods: IVGocr, which combines a Large Language Model, an object detection model, and an OCR module; and IVGdirect, using a multimodal architecture for end-to-end grounding. A dedicated dataset is introduced for each method.

**Key Contributions:**

	1. Introduction of Instruction Visual Grounding (IVG) for GUIs
	2. Development of two methods: IVGocr and IVGdirect
	3. Release of a public dataset and Central Point Validation metric

**Result:** The paper introduces the Central Point Validation metric, a relaxed variant of the classical Central Proximity Score, and releases a public test dataset for future research.

**Limitations:** 

**Conclusion:** The proposed IVG methods enable better understanding and interaction with GUIs, paving the way for advanced AI applications in automation and HCI.

**Abstract:** Most visual grounding solutions primarily focus on realistic images. However, applications involving synthetic images, such as Graphical User Interfaces (GUIs), remain limited. This restricts the development of autonomous computer vision-powered artificial intelligence (AI) agents for automatic application interaction. Enabling AI to effectively understand and interact with GUIs is crucial to advancing automation in software testing, accessibility, and human-computer interaction. In this work, we explore Instruction Visual Grounding (IVG), a multi-modal approach to object identification within a GUI. More precisely, given a natural language instruction and a GUI screen, IVG locates the coordinates of the element on the screen where the instruction should be executed. We propose two main methods: (1) IVGocr, which combines a Large Language Model (LLM), an object detection model, and an Optical Character Recognition (OCR) module; and (2) IVGdirect, which uses a multimodal architecture for end-to-end grounding. For each method, we introduce a dedicated dataset. In addition, we propose the Central Point Validation (CPV) metric, a relaxed variant of the classical Central Proximity Score (CPS) metric. Our final test dataset is publicly released to support future research.

</details>


### [16] [TR-LLM: Integrating Trajectory Data for Scene-Aware LLM-Based Human Action Prediction](https://arxiv.org/abs/2410.03993)

*Kojiro Takeyama, Yimeng Liu, Misha Sra*

**Main category:** cs.HC

**Keywords:** human behavior prediction, Large Language Models, multimodal framework, trajecoty data, autonomous systems

**Relevance Score:** 8

**TL;DR:** This paper proposes a multimodal prediction framework that enhances LLM-based action prediction by integrating physical constraints from human trajectories, addressing the limitations of traditional video-based methods in predicting human behavior in real-world scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate prediction of human behavior is essential for AI systems in applications like autonomous robots, but traditional methods struggle due to challenges such as occlusions and limited scene observations.

**Method:** The authors propose a multimodal prediction framework that combines predictions from Large Language Models (LLMs) with physical constraints derived from human trajectories to improve action prediction accuracy.

**Key Contributions:**

	1. Introduction of a multimodal framework to enhance LLM predictions.
	2. Demonstration of significant improvements in predictive accuracy by incorporating physical constraints.
	3. Validation of LLMs' utility in real-world spatial scenarios despite their limitations.

**Result:** Experiments show that the proposed framework significantly improves prediction performance, particularly in scenarios where the LLM has limited scene information, validating the integration of linguistic and physical knowledge.

**Limitations:** The approach still relies on LLMs which may not have real-time environmental perception capabilities.

**Conclusion:** The study highlights the benefits of combining LLM predictions with physical trajectory data for better understanding and anticipation of human behavior in spatial contexts.

**Abstract:** Accurate prediction of human behavior is crucial for AI systems to effectively support real-world applications, such as autonomous robots anticipating and assisting with human tasks. Real-world scenarios frequently present challenges such as occlusions and incomplete scene observations, which can compromise predictive accuracy. Thus, traditional video-based methods often struggle due to limited temporal and spatial perspectives. Large Language Models (LLMs) offer a promising alternative. Having been trained on a large text corpus describing human behaviors, LLMs likely encode plausible sequences of human actions in a home environment. However, LLMs, trained primarily on text data, lack inherent spatial awareness and real-time environmental perception. They struggle with understanding physical constraints and spatial geometry. Therefore, to be effective in a real-world spatial scenario, we propose a multimodal prediction framework that enhances LLM-based action prediction by integrating physical constraints derived from human trajectories. Our experiments demonstrate that combining LLM predictions with trajectory data significantly improves overall prediction performance. This enhancement is particularly notable in situations where the LLM receives limited scene information, highlighting the complementary nature of linguistic knowledge and physical constraints in understanding and anticipating human behavior.

</details>


### [17] [From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study](https://arxiv.org/abs/2501.03572)

*Ammar Ahmed, Margarida Fresco, Fredrik Forsberg, Hallvard Grotli*

**Main category:** cs.HC

**Keywords:** web accessibility, ChatGPT, WCAG, large language models, prompt engineering

**Relevance Score:** 7

**TL;DR:** A study evaluating ChatGPT's capability to generate accessible web pages according to WCAG revealed its strengths in resolving simple issues but challenges with complex tasks, indicating a need for manual oversight and improved prompt engineering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant gap in web accessibility compliance among popular websites, this study explores how ChatGPT can aid in improving accessibility in line with established guidelines.

**Method:** The study employs both automated and manual testing to evaluate ChatGPT's performance in generating accessible web content, focusing on how prompt engineering and visual reasoning can enhance outcomes.

**Key Contributions:**

	1. Evaluation of ChatGPT's ability to address web accessibility issues
	2. Incorporation of manual evaluation techniques and dynamic elements
	3. Demonstration of improved outcomes through effective prompt engineering

**Result:** ChatGPT effectively resolves simple accessibility issues but struggles with more complex tasks, highlighting the importance of human oversight and iterative engagement. Enhanced prompt instructions and the inclusion of visual aids improve performance significantly.

**Limitations:** The default outputs of ChatGPT often lack compliance with accessibility standards and require multiple iterations alongside human input for complex tasks.

**Conclusion:** While ChatGPT shows promise in assisting with web accessibility, the findings underline the necessity for human involvement and refined prompt strategies to maximize its potential.

**Abstract:** Web accessibility ensures that individuals with disabilities can access and interact with digital content without barriers, yet a significant majority of most used websites fail to meet accessibility standards. This study evaluates ChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web Content Accessibility Guidelines (WCAG). While ChatGPT can effectively address accessibility issues when prompted, its default code often lacks compliance, reflecting limitations in its training data and prevailing inaccessible web practices. Automated and manual testing revealed strengths in resolving simple issues but challenges with complex tasks, requiring human oversight and additional iterations. Unlike prior studies, we incorporate manual evaluation, dynamic elements, and use the visual reasoning capability of ChatGPT along with the prompts to fix accessibility issues. Providing screenshots alongside prompts enhances the LLM's ability to address accessibility issues by allowing it to analyze surrounding components, such as determining appropriate contrast colors. We found that effective prompt engineering, such as providing concise, structured feedback and incorporating visual aids, significantly enhances ChatGPT's performance. These findings highlight the potential and limitations of large language models for accessible web development, offering practical guidance for developers to create more inclusive websites.

</details>


### [18] [From Code to Compliance: Assessing ChatGPT's Utility in Designing an Accessible Webpage -- A Case Study](https://arxiv.org/abs/2501.03572)

*Ammar Ahmed, Margarida Fresco, Fredrik Forsberg, Hallvard Grotli*

**Main category:** cs.HC

**Keywords:** web accessibility, ChatGPT, WCAG, prompt engineering, visual reasoning

**Relevance Score:** 7

**TL;DR:** This study evaluates ChatGPT's ability to generate accessible web content according to WCAG standards, revealing strengths in simple tasks and challenges in complex scenarios, emphasizing the importance of effective prompt engineering and visual aids.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess and improve web accessibility, particularly for individuals with disabilities, and to explore the capacity of ChatGPT in aligning with accessibility standards.

**Method:** ChatGPT's performance was tested through automated and manual evaluations, examining its ability to resolve accessibility issues when provided with prompts and visual aids.

**Key Contributions:**

	1. Evaluation of ChatGPT for web accessibility compliance
	2. Incorporation of manual evaluation and visual reasoning
	3. Recommendations for prompt engineering to enhance performance

**Result:** ChatGPT can effectively address basic accessibility problems but struggles with more complex issues, necessitating human intervention and iterative improvements. Utilizing screenshots with prompts significantly enhances ChatGPT's ability to analyze and suggest fixes.

**Limitations:** ChatGPT's default outputs often do not meet accessibility standards; complex issues require human oversight and iterative feedback.

**Conclusion:** The research highlights the potential of large language models like ChatGPT in improving web accessibility, while also noting the limitations and the need for human oversight in more complex tasks.

**Abstract:** Web accessibility ensures that individuals with disabilities can access and interact with digital content without barriers, yet a significant majority of most used websites fail to meet accessibility standards. This study evaluates ChatGPT's (GPT-4o) ability to generate and improve web pages in line with Web Content Accessibility Guidelines (WCAG). While ChatGPT can effectively address accessibility issues when prompted, its default code often lacks compliance, reflecting limitations in its training data and prevailing inaccessible web practices. Automated and manual testing revealed strengths in resolving simple issues but challenges with complex tasks, requiring human oversight and additional iterations. Unlike prior studies, we incorporate manual evaluation, dynamic elements, and use the visual reasoning capability of ChatGPT along with the prompts to fix accessibility issues. Providing screenshots alongside prompts enhances the LLM's ability to address accessibility issues by allowing it to analyze surrounding components, such as determining appropriate contrast colors. We found that effective prompt engineering, such as providing concise, structured feedback and incorporating visual aids, significantly enhances ChatGPT's performance. These findings highlight the potential and limitations of large language models for accessible web development, offering practical guidance for developers to create more inclusive websites.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [19] [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)

*Atharva Bhargude, Ishan Gonehal, Chandler Haney, Dave Yoon, Kevin Zhu, Aaron Sandoval, Sean O'Brien, Kaustubh Vinnakota*

**Main category:** cs.CL

**Keywords:** phishing detection, large language models, Few-shot Adaptive Linguistic Prompting, multimodal analysis, cybersecurity

**Relevance Score:** 7

**TL;DR:** This study introduces Few-shot Adaptive Linguistic Prompting (ALP) for enhanced phishing detection using multimodal large language models like GPT-4o and Gemini 1.5 Pro.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Phishing attacks pose significant cybersecurity risks, highlighting the need for improved detection methods.

**Method:** The study employs Few-shot Adaptive Linguistic Prompting to analyze phishing content through textual, visual, and URL-based inputs, guiding LLMs to reason about linguistic patterns and deception clues.

**Key Contributions:**

	1. Introduction of Few-shot Adaptive Linguistic Prompting for phishing detection.
	2. Integration of textual, visual, and URL analysis in phishing detection frameworks.
	3. Demonstrated superior performance of ALP-enhanced models over traditional approaches.

**Result:** ALP-enhanced models achieve an F1-score of 0.93, which is superior to traditional phishing detection methods.

**Limitations:** 

**Conclusion:** The findings suggest that ALP can create more effective and interpretable phishing detection frameworks by leveraging the capabilities of multimodal LLMs.

**Abstract:** Phishing attacks represent a significant cybersecurity threat, necessitating adaptive detection techniques. This study explores few-shot Adaptive Linguistic Prompting (ALP) in detecting phishing webpages through the multimodal capabilities of state-of-the-art large language models (LLMs) such as GPT-4o and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides LLMs to analyze textual deception by breaking down linguistic patterns, detecting urgency cues, and identifying manipulative diction commonly found in phishing content. By integrating textual, visual, and URL-based analysis, we propose a unified model capable of identifying sophisticated phishing attempts. Our experiments demonstrate that ALP significantly enhances phishing detection accuracy by guiding LLMs through structured reasoning and contextual analysis. The findings highlight the potential of ALP-integrated multimodal LLMs to advance phishing detection frameworks, achieving an F1-score of 0.93, surpassing traditional approaches. These results establish a foundation for more robust, interpretable, and adaptive linguistic-based phishing detection systems using LLMs.

</details>


### [20] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)

*Keito Inoshita, Rushia Harada*

**Main category:** cs.CL

**Keywords:** emotion recognition, Large Language Model, synthetic data generation, persona-based conditioning, emotion classification

**Relevance Score:** 7

**TL;DR:** The paper introduces PersonaGen, a framework for generating emotionally rich text using a Large Language Model (LLM) to address the challenges of scarce emotional datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The development of high-performance emotion recognition models is hindered by the lack of diverse, high-quality emotional datasets, shaped by subjective factors.

**Method:** PersonaGen employs multi-stage persona-based conditioning to generate emotionally rich text by creating layered virtual personas that incorporate demographic and socio-cultural attributes.

**Key Contributions:**

	1. Introduction of PersonaGen for generating emotional text
	2. Use of multi-stage persona-based conditioning
	3. Significant improvement in emotion expression generation over baseline methods

**Result:** PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, validated through various evaluation metrics.

**Limitations:** 

**Conclusion:** PersonaGen shows promise as a robust alternative for augmenting or replacing real-world emotional datasets to improve emotion recognition tasks.

**Abstract:** In the field of emotion recognition, the development of high-performance models remains a challenge due to the scarcity of high-quality, diverse emotional datasets. Emotional expressions are inherently subjective, shaped by individual personality traits, socio-cultural backgrounds, and contextual factors, making large-scale, generalizable data collection both ethically and practically difficult. To address this issue, we introduce PersonaGen, a novel framework for generating emotionally rich text using a Large Language Model (LLM) through multi-stage persona-based conditioning. PersonaGen constructs layered virtual personas by combining demographic attributes, socio-cultural backgrounds, and detailed situational contexts, which are then used to guide emotion expression generation. We conduct comprehensive evaluations of the generated synthetic data, assessing semantic diversity through clustering and distributional metrics, human-likeness via LLM-based quality scoring, realism through comparison with real-world emotion corpora, and practical utility in downstream emotion classification tasks. Experimental results show that PersonaGen significantly outperforms baseline methods in generating diverse, coherent, and discriminative emotion expressions, demonstrating its potential as a robust alternative for augmenting or replacing real-world emotional datasets.

</details>


### [21] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)

*Rafiq Kamel, Filippo Guerranti, Simon Geisler, Stephan Günnemann*

**Main category:** cs.CL

**Keywords:** Large Language Models, Abstract Meaning Representations, Graph-structured inputs, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** SAFT is a structure-aware fine-tuning approach for large language models (LLMs) that enhances performance on AMR-to-text generation by incorporating graph topology.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve LLMs' performance on tasks involving structured inputs, such as Abstract Meaning Representations (AMRs), which encode semantics in graph form.

**Method:** The authors propose SAFT, which utilizes direction-sensitive positional encodings derived from the magnetic Laplacian of transformed AMRs, injecting this information into the embedding space of pretrained LLMs without changing their architecture.

**Key Contributions:**

	1. Introduction of SAFT for structure-aware fine-tuning of LLMs
	2. Direction-sensitive positional encodings based on graph topology
	3. Achievement of state-of-the-art results on AMR 3.0 with significant BLEU score improvements

**Result:** SAFT achieves a new state-of-the-art performance on AMR 3.0, with a 3.5 BLEU score improvement over existing baselines, showing that performance gains increase with graph complexity.

**Limitations:** 

**Conclusion:** SAFT demonstrates that structure-aware representations significantly enhance the ability of LLMs to generate text from structured data, providing a general approach applicable to various graph-structured inputs.

**Abstract:** Large Language Models (LLMs) are increasingly applied to tasks involving structured inputs such as graphs. Abstract Meaning Representations (AMRs), which encode rich semantics as directed graphs, offer a rigorous testbed for evaluating LLMs on text generation from such structures. Yet, current methods often arbitrarily linearize AMRs, discarding key structural cues, or rely on architectures incompatible with standard LLMs. We introduce SAFT, a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes. We compute direction-sensitive positional encodings from the magnetic Laplacian of transformed AMRs and project them into the embedding space of the LLM. While possibly applicable to any graph-structured inputs, we focus on AMR-to-text generation as a representative and challenging benchmark. SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph complexity, highlighting the value of structure-aware representations in enhancing LLM performance. SAFT offers a general and effective pathway for bridging structured data and language models.

</details>


### [22] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)

*Chandrashekar Muniyappa, Sirisha Velampalli*

**Main category:** cs.CL

**Keywords:** fake news detection, graph-based anomaly detection, NLP, contextual graphs, COVID-19

**Relevance Score:** 6

**TL;DR:** This paper presents a novel graph-based approach for detecting fake news using NLP techniques to convert articles into contextual graph structures, enhancing the detection of both real and fake news during the COVID-19 pandemic.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid spread of fake news in the digital age is a significant concern that needs to be addressed effectively.

**Method:** The authors use a graph-based anomaly detection algorithm (GBAD) to identify normative and anomalous patterns in a dataset of real and fake news articles, particularly focusing on COVID-19 related content.

**Key Contributions:**

	1. Introduction of a contextual graph-based approach for fake news detection.
	2. Utilization of NLP techniques to transform news articles into graph structures.
	3. Application of the MDL-based GBAD algorithm to discover patterns in news data.

**Result:** The proposed method effectively utilizes graph mining to uncover complex patterns and detect fake news articles that might be missed by traditional statistical methods.

**Limitations:** The study relies on the quality of the dataset from Kaggle and may be limited by variations in news content.

**Conclusion:** The contextual graph-based approach enhances the ability to detect fake news by leveraging the rich contextual data present in news articles.

**Abstract:** In today\'s digital world, fake news is spreading with immense speed. Its a significant concern to address. In this work, we addressed that challenge using novel graph based approach. We took dataset from Kaggle that contains real and fake news articles. To test our approach we incorporated recent covid-19 related news articles that contains both genuine and fake news that are relevant to this problem. This further enhances the dataset as well instead of relying completely on the original dataset. We propose a contextual graph-based approach to detect fake news articles. We need to convert news articles into appropriate schema, so we leverage Natural Language Processing (NLP) techniques to transform news articles into contextual graph structures. We then apply the Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD) algorithm for graph mining. Graph-based methods are particularly effective for handling rich contextual data, as they enable the discovery of complex patterns that traditional query-based or statistical techniques might overlook. Our proposed approach identifies normative patterns within the dataset and subsequently uncovers anomalous patterns that deviate from these established norms.

</details>


### [23] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)

*Kundeshwar Pundalik, Piyush Sawarkar, Nihar Sahoo, Abhishek Shinde, Prateek Chanda, Vedant Goswami, Ajay Nagpal, Atul Singh, Viraj Thakur, Vijay Dewane, Aamod Thakur, Bhargav Patel, Smita Gautam, Bhagwan Panditi, Shyam Pawar, Madhav Kotcha, Suraj Racha, Saral Sureka, Pankaj Singh, Rishi Bal, Rohit Saluja, Ganesh Ramakrishnan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Indian Languages, Bilingual Datasets, Tokenization, Cultural Evaluation

**Relevance Score:** 8

**TL;DR:** Introducing PARAM-1, a 2.9B parameter language model specifically designed to address the linguistic diversity of India, trained on a bilingual Hindi-English dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create a more equitable language model that properly represents the linguistic diversity in India, which includes multiple languages, dialects, and unique linguistic phenomena.

**Method:** PARAM-1 is a decoder-only model trained from scratch on a specifically constructed bilingual dataset prioritizing high-quality content in Hindi and English, utilizing a suitable tokenization method and culturally relevant benchmarks for evaluation.

**Key Contributions:**

	1. Development of a bilingual dataset focused on Hindi and English
	2. Innovative tokenization approach suited for Indian languages
	3. Culturally aligned benchmarks for evaluating model performance

**Result:** PARAM-1 demonstrates competitive general-purpose language processing capabilities as well as a strong baseline for applications targeted at Indian languages and socio-linguistic tasks.

**Limitations:** 

**Conclusion:** PARAM-1 sets a precedent for embedding diversity into model design at the pretraining stage, providing a framework for developing more inclusive language models.

**Abstract:** Large Language Models (LLMs) have emerged as powerful general-purpose reasoning systems, yet their development remains dominated by English-centric data, architectures, and optimization paradigms. This exclusionary design results in structural under-representation of linguistically diverse regions such as India, where over 20 official languages and 100+ dialects coexist alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a 2.9B parameter decoder-only, text-only language model trained from scratch with an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is trained on a bilingual dataset consisting of only Hindi and English, constructed with a strong focus on fact-rich, high-quality content. It is guided by three core principles: equitable representation of Indic languages through a 25% corpus allocation; tokenization fairness via a SentencePiece tokenizer adapted to Indian morphological structures; and culturally aligned evaluation benchmarks across IndicQA, code-mixed reasoning, and socio-linguistic robustness tasks. By embedding diversity at the pretraining level-rather than deferring it to post-hoc alignment-PARAM-1 offers a design-first blueprint for equitable foundation modeling. Our results demonstrate that it serves as both a competent general-purpose model and a robust baseline for India-centric applications.

</details>


### [24] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)

*Emil Häglund, Johanna Björklund*

**Main category:** cs.CL

**Keywords:** topic modeling, sentiment analysis, customer reviews, business metrics, large language models

**Relevance Score:** 8

**TL;DR:** This paper improves topic modeling from customer reviews by using opinion units for better sentiment analysis and business metric correlation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the understanding of customer reviews and their impact on business outcomes by improving topic modeling methodologies.

**Method:** The authors restructured the topic modeling pipeline to focus on opinion units, relying on large language models to extract relevant text and sentiment scores effectively.

**Key Contributions:**

	1. Enhanced topic modeling using opinion units
	2. Improved sentiment analysis and correlation with business metrics
	3. Evaluation of integration methods for topic and sentiment modalities

**Result:** The approach results in coherent and interpretable topics that correlate significantly with business metrics like star ratings, offering deeper insights into customer sentiments.

**Limitations:** 

**Conclusion:** Integrating topic and sentiment analysis provides valuable predictions on customer satisfaction, benefiting businesses in understanding and addressing customer concerns.

**Abstract:** We improve the extraction of insights from customer reviews by restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores. Prior work has demonstrated that such units can be reliably extracted using large language models. The result is a heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. By correlating the topics and sentiments with business metrics, such as star ratings, we can gain insights on how specific customer concerns impact business outcomes. We present our system's implementation, use cases, and advantages over other topic modeling and classification solutions. We also evaluate its effectiveness in creating coherent topics and assess methods for integrating topic and sentiment modalities for accurate star-rating prediction.

</details>


### [25] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)

*Xuanqi Gao, Weipeng Jiang, Juan Zhai, Shiqing Ma, Siyi Xie, Xinyang Yin, Chao Shen*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, style preservation, monolingual corpora, contextual embeddings, diffusion-based style applicator

**Relevance Score:** 4

**TL;DR:** Babel is a novel framework that preserves stylistic nuances in neural machine translation (NMT) using only monolingual corpora, featuring a style detector and a diffusion-based style applicator.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of preserving stylistic nuances in neural machine translation without relying on parallel corpora or architectural changes.

**Method:** Babel employs a style detector using contextual embeddings to identify stylistic disparities and a diffusion-based style applicator to rectify these inconsistencies while maintaining semantic integrity.

**Key Contributions:**

	1. Introduction of Babel framework for style preservation in NMT
	2. Utilization of monolingual corpora instead of parallel corpora
	3. Integration with existing NMT systems as a post-processing module

**Result:** Babel achieves 88.21% precision in identifying stylistic inconsistencies and improves stylistic preservation by 150%, with a semantic similarity score of 0.92.

**Limitations:** 

**Conclusion:** Human evaluations indicate that Babel's style-refined translations better preserve the source text's style while maintaining fluency and adequacy.

**Abstract:** The advent of neural machine translation (NMT) has revolutionized cross-lingual communication, yet preserving stylistic nuances remains a significant challenge. While existing approaches often require parallel corpora for style preservation, we introduce Babel, a novel framework that enhances stylistic fidelity in NMT using only monolingual corpora. Babel employs two key components: (1) a style detector based on contextual embeddings that identifies stylistic disparities between source and target texts, and (2) a diffusion-based style applicator that rectifies stylistic inconsistencies while maintaining semantic integrity. Our framework integrates with existing NMT systems as a post-processing module, enabling style-aware translation without requiring architectural modifications or parallel stylistic data. Extensive experiments on five diverse domains (law, literature, scientific writing, medicine, and educational content) demonstrate Babel's effectiveness: it identifies stylistic inconsistencies with 88.21% precision and improves stylistic preservation by 150% while maintaining a high semantic similarity score of 0.92. Human evaluation confirms that translations refined by Babel better preserve source text style while maintaining fluency and adequacy.

</details>


### [26] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)

*Cheng-Ting Chou, George Liu, Jessica Sun, Cole Blondin, Kevin Zhu, Vasu Sharma, Sean O'Brien*

**Main category:** cs.CL

**Keywords:** multilingual language models, sparse autoencoders, language generation, controlled language shifts, transformer networks

**Relevance Score:** 9

**TL;DR:** Investigates the use of sparse autoencoder features to control the generated language of large multilingual language models during inference, achieving up to 90% success in language shifts while preserving semantic fidelity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of controlling target generation languages in large multilingual LLMs, particularly in zero-shot scenarios where traditional methods are not applicable.

**Method:** Utilized pretrained sparse autoencoders on the residual streams of Gemma-2B and Gemma-9B to identify language-sensitive features and modified a single SAE feature at a transformer layer to control language output.

**Key Contributions:**

	1. Demonstrated effective language steering using sparse autoencoder features in LLMs.
	2. Achieved significant language classification success while preserving semantic content.
	3. Provided insights into the transformer layers and attention heads that are most effective for language control.

**Result:** Achieved controlled language shifts with up to 90% success in classifying languages, while maintaining semantic fidelity as measured by LaBSE similarity metrics.

**Limitations:** 

**Conclusion:** Sparse feature steering offers a promising, interpretable method for controlling multilingual language generation in LLMs with minimal adjustments.

**Abstract:** Deterministically controlling the target generation language of large multilingual language models (LLMs) remains a fundamental challenge, particularly in zero-shot settings where neither explicit language prompts nor fine-tuning are available. In this work, we investigate whether sparse autoencoder (SAE) features, previously shown to correlate with interpretable model behaviors, can be leveraged to steer the generated language of LLMs during inference. Leveraging pretrained SAEs on the residual streams of Gemma-2B and Gemma-9B, we identify features whose activations differ most significantly between English and four target languages: Chinese, Japanese, Spanish, and French. By modifying just a single SAE feature at one transformer layer, we achieve controlled language shifts with up to 90\% success, as measured by FastText language classification, while preserving semantic fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding) similarity. Our analysis reveals that language steering is most effective in mid-to-late transformer layers and is amplified by specific attention heads disproportionately associated with language-sensitive SAE features. These results demonstrate the promise of sparse feature steering as a lightweight and interpretable mechanism for controllable multilingual generation.

</details>


### [27] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)

*Nur A Zarin Nishat, Andrea Coletta, Luigi Bellomarini, Kossi Amouzouvi, Jens Lehmann, Sahar Vahdati*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, language models, factual accuracy, hallucination reduction, NLP

**Relevance Score:** 9

**TL;DR:** The paper presents ALIGNed-LLM, a method for integrating Knowledge Graphs into language models to enhance their factual accuracy and reduce hallucinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations in large language models and improve their factuality in NLP tasks.

**Method:** The authors propose a lean strategy to infuse Knowledge Graphs into the latent space of language models using Knowledge Graph Embedding models and a trainable projection layer for embedding alignment.

**Key Contributions:**

	1. Introduction of ALIGNed-LLM for integrating KGs into language models.
	2. Development of a method for aligning entity and text embeddings to enhance factual grounding.
	3. Demonstration of the approach in a real-world application that requires high accuracy.

**Result:** Significant improvements were observed on three questions-answering benchmark datasets, as well as in a real-world financial application, enhancing the accuracy of LLM responses.

**Limitations:** 

**Conclusion:** ALIGNed-LLM effectively reduces hallucinations in language models, improving their performance in both benchmark tests and practical applications.

**Abstract:** Large language models like GPT-4, Gemini, and Claude have transformed natural language processing (NLP) tasks such as question answering, dialogue generation, summarization, and so forth; yet their susceptibility to hallucination stands as one of the major challenges. Among numerous approaches to overcome this challenge, integration of Knowledge Graphs (KGs) into language models has emerged as a promising solution as it provides structured, reliable, domain-specific, and up-to-date external information to the language models. In this paper, we introduce ALIGNed-LLM, a simple yet effective approach to improve language models' factuality via a lean strategy to infuse KGs into the latent space of language models inspired by LLaVA where visual and textual information is infused. We use embeddings from a pre-trained Knowledge Graph Embedding (KGE) model, such as TransE, and a trainable projection layer to align entity and text embeddings. This alignment enables the language model to distinguish between similar entities improving factual grounding and reducing hallucination. We tested our approach on three popular questions-answering benchmark datasets alongside language models of varying sizes, showing significant improvement. Furthermore, we applied our approach to a real-world financial use case from a large central bank in Europe, which demands high accuracy and precision, demonstrating a substantial improvement of the LLM answers.

</details>


### [28] [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)

*Liang Lin, Zhihao Xu, Xuehai Tang, Shi Liu, Biyu Zhou, Fuqing Zhu, Jizhong Han, Songlin Hu*

**Main category:** cs.CL

**Keywords:** large language models, LLM vulnerabilities, Paper Summary Attack, adversarial prompts, safety alignment

**Relevance Score:** 9

**TL;DR:** The paper investigates vulnerabilities in large language models (LLMs) due to their tendency to trust authoritative sources and proposes a novel jailbreaking method called Paper Summary Attack (PSA) to exploit these vulnerabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore vulnerabilities in large language models (LLMs) that stem from their reliance on authoritative information sources.

**Method:** A preliminary analysis followed by the development of the Paper Summary Attack (PSA) method, which synthesizes content from safety papers to create adversarial prompts.

**Key Contributions:**

	1. Introduction of the Paper Summary Attack (PSA) as a novel adversarial method.
	2. Demonstration of significant vulnerabilities in both state-of-the-art reasoning models and base LLMs.
	3. Revelation of differing vulnerability biases across models based on their exposure to types of papers.

**Result:** PSA achieved a 97% attack success rate on Claude3.5-Sonnet and a 98% ASR on Deepseek-R1, revealing significant vulnerabilities across various LLMs.

**Limitations:** 

**Conclusion:** The findings indicate a vulnerability bias in LLMs related to the type of content they are exposed to, suggesting areas for future research in adversarial methodologies and safety alignment.

**Abstract:** The safety of large language models (LLMs) has garnered significant research attention. In this paper, we argue that previous empirical studies demonstrate LLMs exhibit a propensity to trust information from authoritative sources, such as academic papers, implying new possible vulnerabilities. To verify this possibility, a preliminary analysis is designed to illustrate our two findings. Based on this insight, a novel jailbreaking method, Paper Summary Attack (\llmname{PSA}), is proposed. It systematically synthesizes content from either attack-focused or defense-focused LLM safety paper to construct an adversarial prompt template, while strategically infilling harmful query as adversarial payloads within predefined subsections. Extensive experiments show significant vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on Deepseek-R1. More intriguingly, our work has further revealed diametrically opposed vulnerability bias across different base models, and even between different versions of the same model, when exposed to either attack-focused or defense-focused papers. This phenomenon potentially indicates future research clues for both adversarial methodologies and safety alignment.Code is available at https://github.com/233liang/Paper-Summary-Attack

</details>


### [29] [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)

*Siqi Shen, Mehar Singh, Lajanugen Logeswaran, Moontae Lee, Honglak Lee, Rada Mihalcea*

**Main category:** cs.CL

**Keywords:** Large Language Models, value probing, demographic context, user experience, model behavior

**Relevance Score:** 9

**TL;DR:** This paper evaluates the robustness and expressiveness of value representations in Large Language Models (LLMs) across three probing strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Assess the value orientation of LLMs as it influences user experiences and behaviors across demographic groups.

**Method:** Evaluate robustness and expressiveness of value representations using variations in prompts and options. Introduce tasks to study values' responsiveness to demographic context and alignment with model behaviors.

**Key Contributions:**

	1. Systematic comparison of probing methods for value orientations in LLMs
	2. Introduction of tasks to study demographic context impacts on value representations
	3. Identification of the weak correlation between model values and real-world actions

**Result:** All probing methods exhibit large variances under input perturbations; demographic context has little effect on free-text generation; weak correlation between probed values and model preferences for value-based actions.

**Limitations:** The study highlights the limited responsiveness of models' values to demographic context and the weak alignment with behaviors in value-related scenarios.

**Conclusion:** A careful examination of LLM value probing is necessary, acknowledging its limitations.

**Abstract:** There has been extensive research on assessing the value orientation of Large Language Models (LLMs) as it can shape user experiences across demographic groups. However, several challenges remain. First, while the Multiple Choice Question (MCQ) setting has been shown to be vulnerable to perturbations, there is no systematic comparison of probing methods for value probing. Second, it is unclear to what extent the probed values capture in-context information and reflect models' preferences for real-world actions. In this paper, we evaluate the robustness and expressiveness of value representations across three widely used probing strategies. We use variations in prompts and options, showing that all methods exhibit large variances under input perturbations. We also introduce two tasks studying whether the values are responsive to demographic context, and how well they align with the models' behaviors in value-related scenarios. We show that the demographic context has little effect on the free-text generation, and the models' values only weakly correlate with their preference for value-based actions. Our work highlights the need for a more careful examination of LLM value probing and awareness of its limitations.

</details>


### [30] [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)

*Matilde Marcolli, Robert C. Berwick*

**Main category:** cs.CL

**Keywords:** syntax, neurocomputation, wavelets, semiring, Merge

**Relevance Score:** 3

**TL;DR:** This paper presents a mathematical framework for representing syntactic objects using functions in a specific function space, arguing for the possibility of neurocomputational realizations of syntax.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To construct a faithful representation of syntactic structures using mathematical functions, thereby contributing to understanding the computational aspects of syntax within neurocomputation.

**Method:** The authors develop a representation of lexical items as functions facilitated by a commutative non-associative semiring structure and model circuits that transform input functions into a syntactic structure.

**Key Contributions:**

	1. Development of a mathematical argument for syntactic representation using functions
	2. Introduction of a semiring structure for representing syntax
	3. Demonstration of neurocomputational realizations of core syntactic structures

**Result:** The proposed framework shows that Merge operations in syntax can be realized through mathematical constructs like coproducts and Hopf algebra Markov chains, and can be represented as cross frequency phase synchronization on sinusoidal waves.

**Limitations:** 

**Conclusion:** The findings present a constructive argument for neurocomputational realizations of syntax, establishing mathematical parallels between syntactic Merge and arithmetic successor functions.

**Abstract:** We provide a mathematical argument showing that, given a representation of lexical items as functions (wavelets, for instance) in some function space, it is possible to construct a faithful representation of arbitrary syntactic objects in the same function space. This space can be endowed with a commutative non-associative semiring structure built using the second Renyi entropy. The resulting representation of syntactic objects is compatible with the magma structure. The resulting set of functions is an algebra over an operad, where the operations in the operad model circuits that transform the input wave forms into a combined output that encodes the syntactic structure. The action of Merge on workspaces is faithfully implemented as action on these circuits, through a coproduct and a Hopf algebra Markov chain. The results obtained here provide a constructive argument showing the theoretical possibility of a neurocomputational realization of the core computational structure of syntax. We also present a particular case of this general construction where this type of realization of Merge is implemented as a cross frequency phase synchronization on sinusoidal waves. This also shows that Merge can be expressed in terms of the successor function of a semiring, thus clarifying the well known observation of its similarities with the successor function of arithmetic.

</details>


### [31] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)

*Lizhi Ma, Tong Zhao, Shuai Zhang, Nirui Song, Hongliang He, Anqi Li, Ran Feng, Huachuan Qiu, Jingsong Ma, Zhenzhong Lan*

**Main category:** cs.CL

**Keywords:** psycholinguistics, mental health, Chinese counseling, depression, anxiety

**Relevance Score:** 4

**TL;DR:** This study analyzes the link between language use and psychological states in Chinese counseling, focusing on pronoun use and emotional words.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how linguistic expressions relate to depression and anxiety in Chinese psycho-counseling contexts.

**Method:** Analysis of 735 online counseling sessions using a general linear mixed-effect model and LIWC software to quantify linguistic patterns.

**Key Contributions:**

	1. Demonstrates cultural differences in linguistic patterns related to mental health in counseling
	2. Identifies specific linguistic correlates of depression and anxiety within Chinese contexts
	3. Challenges existing findings from English-language counseling studies

**Result:** Found a positive correlation between negative emotional words and severity of depression/anxiety, but no significant variation in the use of first-person singular pronouns based on psychological conditions.

**Limitations:** Study limited to Chinese-speaking populations; results may not generalize to other languages or cultures.

**Conclusion:** Cultural distinctions affect language use in mental health communications, indicating the importance of context in therapeutic practices.

**Abstract:** This study explores the relationship between linguistic expressions and psychological states of depression and anxiety within Chinese psycho-counseling interactions, focusing specifically on the usage of first-person singular pronouns and negative emotional words. Utilizing a corpus derived from 735 online counseling sessions, the analysis employed a general linear mixed-effect model to assess linguistic patterns quantified by the Linguistic Inquiry and Word Count (LIWC) software. Results indicate a significant positive correlation between the frequency of negative emotional words and the severity of both depressive and anxious states among clients. However, contrary to prior findings predominantly derived from English-language contexts, the usage frequency of first-person singular pronouns did not vary significantly with the clients' psychological conditions. These outcomes are discussed within the framework of cultural distinctions between collectivist Chinese contexts and individualistic Western settings, as well as the interactive dynamics unique to psycho-counseling conversations. The findings highlight the nuanced influence of cultural and conversational contexts on language use in mental health communications, providing insights into psycholinguistic markers relevant to therapeutic practices in Chinese-speaking populations.

</details>


### [32] [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)

*Mohamed Achref Ben Ammar, Mohamed Taha Bennani*

**Main category:** cs.CL

**Keywords:** conversational graphs, dialogue analysis, large language models

**Relevance Score:** 8

**TL;DR:** This paper introduces a computational framework for constructing conversational graphs for analyzing loosely organized dialogues, enhancing clarity and coherence using a novel graph simplification method.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large language model-based systems, there is an increasing need to analyze conversational dynamics across different contexts.

**Method:** The proposed framework utilizes a Filter & Reconnect method to simplify graphs of quasi-patterned conversations, minimizing noise while maintaining semantic coherence and structural integrity.

**Key Contributions:**

	1. Introduction of a novel computational framework for conversational graphs
	2. Development of the Filter & Reconnect method for graph simplification
	3. Demonstrated significant improvement in semantic coherence metrics.

**Result:** Semantic metrics improved by a factor of 2.06 compared to previous methods, while achieving a tree-like structure with 0 δ-hyperbolicity.

**Limitations:** 

**Conclusion:** This framework aids in analyzing large-scale dialogue datasets, with applications for monitoring chatbots, dialogue management, and user behavior analytics.

**Abstract:** The analysis of conversational dynamics has gained increasing importance with the rise of large language model-based systems, which interact with users across diverse contexts. In this work, we propose a novel computational framework for constructing conversational graphs that capture the flow and structure of loosely organized dialogues, referred to as quasi-patterned conversations. We introduce the Filter & Reconnect method, a novel graph simplification technique that minimizes noise while preserving semantic coherence and structural integrity of conversational graphs. Through comparative analysis, we demonstrate that the use of large language models combined with our graph simplification technique has resulted in semantic metric S increasing by a factor of 2.06 compared to previous approaches while simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity, ensuring optimal clarity in conversation modeling. This work provides a computational method for analyzing large-scale dialogue datasets, with practical applications related to monitoring automated systems such as chatbots, dialogue management tools, and user behavior analytics.

</details>


### [33] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)

*Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield*

**Main category:** cs.CL

**Keywords:** Conversational AI, Persuasion, LLMs, Factual Accuracy, Political Issues

**Relevance Score:** 8

**TL;DR:** The paper investigates the persuasiveness of conversational AI across 707 political issues, finding that LLMs' influence comes more from prompting and post-training than from scale, but this increased persuasiveness comes at the cost of factual accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the influence of conversational AI on human beliefs and assess the factors contributing to its persuasiveness.

**Method:** Three large-scale experiments involving 76,977 participants were conducted, utilizing 19 different LLMs, including those trained specifically for persuasion, to evaluate their persuasive effectiveness and check the factual accuracy of generated claims.

**Key Contributions:**

	1. Demonstrated that post-training and prompting methods are key to LLM persuasiveness.
	2. Identified that increased AI persuasiveness negatively impacts factual accuracy.
	3. Evaluated the persuasive impact of LLMs on a large set of political issues and claims.

**Result:** Post-training and prompting methods significantly boosted LLM persuasiveness by up to 51% and 27%, respectively, while their use also correlated with a decrease in factual accuracy.

**Limitations:** Limited to the evaluation of 19 LLMs and might not encompass all factors influencing persuasion in conversational AI.

**Conclusion:** Current and near-future conversational AI's persuasive abilities are largely influenced by specific training methods rather than model scale or personalization, raising concerns about the reliability of information they provide.

**Abstract:** There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.

</details>


### [34] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)

*Feng Chen, Weizhe Xu, Changye Li, Serguei Pakhomov, Alex Cohen, Simran Bhola, Sandy Yin, Sunny X Tang, Michael Mackinley, Lena Palaniyappan, Dror Ben-Zeev, Trevor Cohen*

**Main category:** cs.CL

**Keywords:** formal thought disorder, schizophrenia, automated speech recognition, support vector regression, semantic coherence

**Relevance Score:** 7

**TL;DR:** This study evaluates the integration of pause features from automated speech recognition with semantic coherence metrics to predict the severity of formal thought disorder (FTD) in schizophrenia, finding improved predictive performance compared to semantic-only models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to improve the assessment of formal thought disorder (FTD), which is characterized by incoherent speech in schizophrenia and is difficult to evaluate using traditional, resource-intensive clinical rating scales.

**Method:** The study integrates pause features derived from automatic speech recognition (ASR) with semantic coherence metrics across three different datasets (naturalistic self-recorded diaries, structured picture descriptions, and dream narratives) and employs support vector regression (SVR) for prediction of clinical FTD scores.

**Key Contributions:**

	1. Demonstrates that pause features can predict FTD severity effectively.
	2. Shows that integration of pause features with semantic metrics improves predictive accuracy.
	3. Presents findings across various datasets, highlighting dataset-dependent characteristics of pause patterns.

**Result:** Key findings indicate that pause features alone can robustly predict FTD severity, and when combined with semantic coherence metrics, the predictive performance is enhanced, achieving correlations and AUC values that indicate effective detection of severe cases.

**Limitations:** 

**Conclusion:** The research concludes that combining temporal (pause) and semantic analyses offers a promising framework for better assessing disorganized speech in psychosis and advancing automated approaches in speech analysis.

**Abstract:** Formal thought disorder (FTD), a hallmark of schizophrenia spectrum disorders, manifests as incoherent speech and poses challenges for clinical assessment. Traditional clinical rating scales, though validated, are resource-intensive and lack scalability. Automated speech analysis with automatic speech recognition (ASR) allows for objective quantification of linguistic and temporal features of speech, offering scalable alternatives. The use of utterance timestamps in ASR captures pause dynamics, which are thought to reflect the cognitive processes underlying speech production. However, the utility of integrating these ASR-derived features for assessing FTD severity requires further evaluation. This study integrates pause features with semantic coherence metrics across three datasets: naturalistic self-recorded diaries (AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream narratives (PsyCL, n = 43). We evaluated pause related features alongside established coherence measures, using support vector regression (SVR) to predict clinical FTD scores. Key findings demonstrate that pause features alone robustly predict the severity of FTD. Integrating pause features with semantic coherence metrics enhanced predictive performance compared to semantic-only models, with integration of independent models achieving correlations up to \r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best \r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance gains from semantic and pause features integration held consistently across all contexts, though the nature of pause patterns was dataset-dependent. These findings suggest that frameworks combining temporal and semantic analyses provide a roadmap for refining the assessment of disorganized speech and advance automated speech analysis in psychosis.

</details>


### [35] [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)

*Kirill Borodin, Nikita Vasiliev, Vasiliy Kudryavtsev, Maxim Maslov, Mikhail Gorodnichev, Oleg Rogov, Grach Mkrtchian*

**Main category:** cs.CL

**Keywords:** speech synthesis, Russian language, dataset, machine learning, annotation

**Relevance Score:** 4

**TL;DR:** Introduction of Balalaika, a large dataset for Russian speech synthesis with significant improvements over existing datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of Russian speech synthesis, including vowel reduction, consonant devoicing, and variable stress patterns.

**Method:** Creation of Balalaika, a dataset with over 2,000 hours of studio-quality Russian speech, accompanied by detailed textual annotations such as punctuation and stress markings.

**Key Contributions:**

	1. Introduction of a comprehensive dataset for Russian speech synthesis
	2. Detailed methodology for dataset construction and annotation
	3. Demonstration of performance improvements in speech-related models

**Result:** Models trained on Balalaika demonstrate significant improvements in speech synthesis and enhancement tasks compared to those trained on existing datasets.

**Limitations:** The work is still in progress, indicating potential further developments and changes.

**Conclusion:** Balalaika represents a substantial advancement in resources for Russian speech synthesis, showcasing superior performance in experimental evaluations.

**Abstract:** Russian speech synthesis presents distinctive challenges, including vowel reduction, consonant devoicing, variable stress patterns, homograph ambiguity, and unnatural intonation. This paper introduces Balalaika, a novel dataset comprising more than 2,000 hours of studio-quality Russian speech with comprehensive textual annotations, including punctuation and stress markings. Experimental results show that models trained on Balalaika significantly outperform those trained on existing datasets in both speech synthesis and enhancement tasks. We detail the dataset construction pipeline, annotation methodology, and results of comparative evaluations.

</details>


### [36] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)

*Sergio E. Zanotto, Segun Aroyehun*

**Main category:** cs.CL

**Keywords:** large language models, linguistic features, text classification, human-computer interaction, stylometric analysis

**Relevance Score:** 9

**TL;DR:** This study characterizes human-written and machine-generated texts using linguistic features across various levels, revealing structural and semantic differences along with stylistic variability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the distinctions between human-written and machine-generated texts using a comprehensive set of linguistic features.

**Method:** The study employs a dataset of texts across eight domains produced by eleven different LLMs, calculating linguistic features like dependency length and emotionality for analysis.

**Key Contributions:**

	1. Characterization of texts using a comprehensive set of linguistic features
	2. Statistical analysis of variability across different models and domains
	3. Insights into the homogenization of machine-generated texts from newer models

**Result:** Human-written texts exhibit simpler syntactic structures and greater semantic diversity than machine-generated texts, with newer models showing increased variability but homogenization in style.

**Limitations:** 

**Conclusion:** The findings demonstrate significant linguistic differences between human and machine texts while highlighting stylistic diversity and potential homogenization in machine-generated outputs.

**Abstract:** The rapid advancements in large language models (LLMs) have significantly improved their ability to generate natural language, making texts generated by LLMs increasingly indistinguishable from human-written texts. While recent research has primarily focused on using LLMs to classify text as either human-written and machine-generated texts, our study focus on characterizing these texts using a set of linguistic features across different linguistic levels such as morphology, syntax, and semantics. We select a dataset of human-written and machine-generated texts spanning 8 domains and produced by 11 different LLMs. We calculate different linguistic features such as dependency length and emotionality and we use them for characterizing human-written and machine-generated texts along with different sampling strategies, repetition controls and model release date. Our statistical analysis reveals that human-written texts tend to exhibit simpler syntactic structures and more diverse semantic content. Furthermore, we calculate the variability of our set of features across models and domains. Both human and machine texts show stylistic diversity across domains, with humans displaying greater variation in our features. Finally, we apply style embeddings to further test variability among human-written and machine-generated texts. Notably, newer models output text that is similarly variable, pointing to an homogenization of machine-generated texts.

</details>


### [37] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)

*Shanbo Cheng, Yu Bao, Qian Cao, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, Wenhao Zhu, Zhichao Huang, Tao Li, Sitong Liu, Ningxin Peng, Shuaijie She, Lu Xu, Nuo Xu, Sen Yang, Runsheng Yu, Yiming Yu, Liehao Zou, Hang Li, Lu Lu, Yuxuan Wang, Yonghui Wu*

**Main category:** cs.CL

**Keywords:** multilingual translation, large language models, open-source, Chain-of-Thought, reinforcement learning

**Relevance Score:** 9

**TL;DR:** Seed-X is a family of open-source LLMs designed for multilingual translation, achieving competitive performance against top closed-source models while significantly enhancing translation capabilities with 7B parameters.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of multilingual translation in large language models, particularly concerning intricate language patterns and the quality of automated translations.

**Method:** Seed-X consists of instruct and reasoning models pre-trained on a diverse dataset with 28 languages, enhanced through Chain-of-Thought reasoning and reinforcement learning for improved translation capabilities.

**Key Contributions:**

	1. Introduction of Seed-X, a family of open-source LLMs for multilingual translation.
	2. Integration of Chain-of-Thought reasoning and reinforcement learning to improve translation quality.
	3. Pre-training on a diverse high-quality dataset covering 28 languages.

**Result:** Seed-X outperforms larger open-source models and matches the performance of top closed-source models like Gemini-2.5 and GPT-4o across 28 languages, excelling in both automatic and human evaluations.

**Limitations:** 

**Conclusion:** The models and best practices are publicly available for research, aiming to advance the field of translation research and applications.

**Abstract:** Multilingual translation stands as a challenging task for large language models (LLMs) to handle intricate language patterns and stilted translations that arise in automated translations. In this paper, we introduce Seed-X, a family of open-source LLMs comprising instruct and reasoning models, pushing the limits of translation capability with 7B parameter size. The base model is pre-trained on a diverse, high-quality dataset encompassing both monolingual and bilingual content across 28 languages, harnessing the full potential of multilingual data. The instruct model is then finetuned to translate by Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement learning (RL) to achieve better generalization across diverse language pairs. Seed-X achieves performance comparable to leading closed-source models, including Gemini-2.5 and GPT-4o, across 28 languages, and significantly outperforms larger open-source models in both automatic metrics and human evaluations. We share the best practices through our optimization process, and make the parameter public available for advancing translation research and applications.

</details>


### [38] [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)

*Teerapong Panboonyuen*

**Main category:** cs.CL

**Keywords:** language models, healthcare, CU-ICU, machine learning, ICU

**Relevance Score:** 9

**TL;DR:** CU-ICU is a method for customizing T5-based language models for ICU datasets, improving predictive accuracy in critical ICU tasks with minimal parameter tuning.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods struggle with domain adaptation and limited labeled data in healthcare while using large language models.

**Method:** CU-ICU uses a sparse fine-tuning approach, combining few-shot prompting and selective parameter updates to adapt models efficiently.

**Key Contributions:**

	1. Efficient adaptation of language models in healthcare with limited supervision.
	2. Significant improvements in critical task accuracy and interpretability.
	3. Minimal updates to model parameters for effective performance.

**Result:** Improvements in predictive accuracy and interpretability for tasks like sepsis detection and clinical note generation, with a 15% increase in accuracy and a 20% enhancement in explanations.

**Limitations:** 

**Conclusion:** CU-ICU is a scalable and low-overhead solution for accurate clinical decision support in ICU settings.

**Abstract:** Integrating large language models into specialized domains like healthcare presents unique challenges, including domain adaptation and limited labeled data. We introduce CU-ICU, a method for customizing unsupervised instruction-finetuned language models for ICU datasets by leveraging the Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse fine-tuning approach that combines few-shot prompting with selective parameter updates, enabling efficient adaptation with minimal supervision. Our evaluation across critical ICU tasks--early sepsis detection, mortality prediction, and clinical note generation--demonstrates that CU-ICU consistently improves predictive accuracy and interpretability over standard fine-tuning methods. Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and a 20% enhancement in generating clinically relevant explanations while updating fewer than 1% of model parameters in its most efficient configuration. These results establish CU-ICU as a scalable, low-overhead solution for delivering accurate and interpretable clinical decision support in real-world ICU environments.

</details>


### [39] [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)

*Woo-Chan Kim, Ji-Hoon Park, Seong-Whan Lee*

**Main category:** cs.CL

**Keywords:** Language models, Cost-efficiency, Text generation, Semantic alignment, Cascade methods

**Relevance Score:** 9

**TL;DR:** Keyword-inspired Cascade (KiC) is a framework that improves cost-efficient free-form text generation by selecting the best responses from weaker models and evaluating their alignment with semantic standards before escalating to more powerful models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the performance of large language models, their high API inference costs pose a barrier for widespread use. Cascade methods can reduce these costs but have limitations in selecting reliable outputs.

**Method:** KiC employs a two-step process: it identifies the most representative answer from a weaker model's outputs and then evaluates other responses based on their semantic alignment with this representative answer to decide whether to use the output or escalate to a stronger model.

**Key Contributions:**

	1. Introduction of the Keyword-inspired Cascade (KiC) framework for text generation.
	2. Improvement in cost efficiency for LLM operations by reducing reliance on expensive APIs.
	3. Achieving high accuracy comparable to state-of-the-art models with cheaper alternatives.

**Result:** KiC demonstrates a 97.53% accuracy compared to GPT-4 while saving 28.81% on API costs on average, with improvements noted in specific benchmarks.

**Limitations:** 

**Conclusion:** The KiC framework shows promise in making high-quality text generation more accessible and cost-effective without compromising output quality.

**Abstract:** Large language models (LLMs) have demonstrated state-of-the-art performance across a wide range of natural language processing tasks. However, high-performing models are typically accessible only via APIs, incurring substantial inference costs. Cascade methods address this by initially employing a cheaper model and escalating to a stronger one only when necessary. Nevertheless, existing cascade approaches struggle to select a reliable representative response and assess the overall reliability of free-form outputs, as they rely on exact text matching. To overcome these limitations, we propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient free-form text generation. KiC identifies the most representative answer among multiple outputs from a weaker model and evaluates the semantic alignment of other responses with it. Based on the degree of alignment, KiC determines whether to accept the weaker model's output or escalate to a stronger model. Experiments on three free-form text generation benchmarks show that KiC achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81 percent on average, and even outperforms GPT-4 in a specific benchmark.

</details>


### [40] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)

*Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-turn Dialogues, Inference Acceleration

**Relevance Score:** 8

**TL;DR:** LoopServe is an adaptive inference acceleration framework for large language models, addressing computational challenges in multi-turn dialogues by dynamically optimizing attention and cache strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As conversation histories in real-world applications of large language models grow, computational and memory issues become significant hurdles, limiting responsiveness in multi-turn dialogues.

**Method:** LoopServe introduces online sparsification during prefill by selecting important attention matrix parts and employs progressive key value compression based on recent output tokens during decoding.

**Key Contributions:**

	1. Adaptive dual-phase inference acceleration
	2. Dynamic attention matrix sparsification
	3. Progressive key value compression for efficient caching

**Result:** LoopServe achieves superior effectiveness and significantly enhances LLM inference speed across various long-context dialogue tasks compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed framework addresses the limitations of current methods and sets a new benchmark for handling multi-turn dialogue with large language models.

**Abstract:** Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.

</details>


### [41] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)

*Cedric Waterschoot, Nava Tintarev, Francesco Barile*

**Main category:** cs.CL

**Keywords:** Large Language Models, Group Recommender Systems, Additive Utilitarian Aggregation, Transparency, Explainability

**Relevance Score:** 8

**TL;DR:** This paper evaluates LLM-generated recommendations and explanations in Group Recommender Systems, comparing them to traditional aggregation strategies, revealing significant insights into their consistency and transparency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effectiveness and reliability of LLM-generated recommendations and explanations in Group Recommender Systems compared to traditional aggregation methods.

**Method:** The paper employs comparative analysis of LLM-generated recommendations against social choice-based aggregation strategies, focusing on Additive Utilitarian aggregation and evaluating the explanations provided by LLMs.

**Key Contributions:**

	1. Evaluation of LLM effectiveness in Group Recommender Systems
	2. Insights into explanation quality and transparency issues
	3. Comparison with traditional aggregation strategies

**Result:** LLM-generated recommendations closely mirrored those from Additive Utilitarian aggregation, but their explanations often diverged, leading to issues with transparency and consistency, especially as group sizes increased.

**Limitations:** Inconsistent and ambiguous explanations limit transparency and explainability, particularly for larger group sizes.

**Conclusion:** The study highlights the implications of LLMs in the Group Recommender Systems pipeline and raises concerns about the transparency and efficiency of conventional aggregation methods, especially in larger item sets.

**Abstract:** Large Language Models (LLMs) are increasingly being implemented as joint decision-makers and explanation generators for Group Recommender Systems (GRS). In this paper, we evaluate these recommendations and explanations by comparing them to social choice-based aggregation strategies. Our results indicate that LLM-generated recommendations often resembled those produced by Additive Utilitarian (ADD) aggregation. However, the explanations typically referred to averaging ratings (resembling but not identical to ADD aggregation). Group structure, uniform or divergent, did not impact the recommendations. Furthermore, LLMs regularly claimed additional criteria such as user or item similarity, diversity, or used undefined popularity metrics or thresholds. Our findings have important implications for LLMs in the GRS pipeline as well as standard aggregation strategies. Additional criteria in explanations were dependent on the number of ratings in the group scenario, indicating potential inefficiency of standard aggregation methods at larger item set sizes. Additionally, inconsistent and ambiguous explanations undermine transparency and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [42] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)

*Guillaume Zambrano*

**Main category:** cs.CL

**Keywords:** Machine Learning, Legal Decision-Making, Human Judges, Custody Outcomes, Legal Realism

**Relevance Score:** 4

**TL;DR:** The study analyzes the influence of individual judges on custody decisions in French courts using ML to predict outcomes, highlighting that judges' decision patterns significantly affect results.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the notion that judges uniformly apply the law by assessing the impact of their individual decision-making on custody outcomes.

**Method:** The analysis utilizes 18,937 rulings from 10,306 cases, comparing specialist models trained on individual judges' past rulings to a general model using aggregated data. A hybrid approach employing LLMs and traditional ML models (RF, XGB, SVC) is implemented.

**Key Contributions:**

	1. Demonstrates the significant impact of individual judges on legal outcomes through machine learning
	2. Implements a hybrid approach combining LLMs and traditional ML for judicial decision-making
	3. Provides empirical support for the legal realism perspective in the context of custody rulings

**Result:** Specialist models outperform the generalist model, achieving F1 scores up to 92.85% compared to 82.63%, indicating that individual judges' patterns can significantly influence legal outcomes.

**Limitations:** Study limited to French appellate courts and may not generalize to other legal systems.

**Conclusion:** The findings support legal realism, demonstrating that judicial identity impacts case results, with data and code to be publicly shared for replication.

**Abstract:** This study examines the role of human judges in legal decision-making by using machine learning to predict child physical custody outcomes in French appellate courts. Building on the legal realism-formalism debate, we test whether individual judges' decision-making patterns significantly influence case outcomes, challenging the assumption that judges are neutral variables that apply the law uniformly. To ensure compliance with French privacy laws, we implement a strict pseudonymization process. Our analysis uses 18,937 living arrangements rulings extracted from 10,306 cases. We compare models trained on individual judges' past rulings (specialist models) with a judge-agnostic model trained on aggregated data (generalist models). The prediction pipeline is a hybrid approach combining large language models (LLMs) for structured feature extraction and ML models for outcome prediction (RF, XGB and SVC). Our results show that specialist models consistently achieve higher predictive accuracy than the general model, with top-performing models reaching F1 scores as high as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x more samples. Specialist models capture stable individual patterns that are not transferable to other judges. In-Domain and Cross-Domain validity tests provide empirical support for legal realism, demonstrating that judicial identity plays a measurable role in legal outcomes. All data and code used will be made available.

</details>


### [43] [Consistency of Responses and Continuations Generated by Large Language Models on Social Media](https://arxiv.org/abs/2501.08102)

*Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, emotional coherence, semantic similarity, social media, human-AI interaction

**Relevance Score:** 8

**TL;DR:** This study examines how Large Language Models (LLMs) handle emotional content in social media, focusing on emotional transitions and semantic coherence using the Gemma and Llama models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the emotional consistency and semantic coherence of LLMs in social media contexts, which is crucial for their effective deployment in human-AI interaction.

**Method:** The research involves analyzing climate change discussions from Twitter and Reddit, employing two open-source models, Gemma and Llama, across continuation and response tasks.

**Key Contributions:**

	1. Demonstrated distinct emotional patterns in LLM outputs
	2. Showed the impact of emotional bias in LLM responses
	3. Analyzed the semantic similarities and differences in human vs LLM outputs

**Result:** Findings indicate that Gemma amplifies negative emotions like anger but retains positive emotions like optimism, while Llama maintains emotional preservation across a wider range of affects. Both models show attenuated emotional intensity in generated responses compared to human-authored content, with Llama exhibiting superior semantic coherence.

**Limitations:** The study is limited to emotional analysis in social media discussions and may not generalize to other domains.

**Conclusion:** The insights gained on LLMs' emotional and semantic processing capabilities can inform design practices for human-AI interactions, especially in social media contexts.

**Abstract:** Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.

</details>


### [44] [PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs](https://arxiv.org/abs/2507.13743)

*Maluna Menke, Thilo Hagendorff*

**Main category:** cs.CL

**Keywords:** Large Language Models, LGBTQIA+ biases, Low-Rank Adaptation, soft-prompt tuning, fairness in AI

**Relevance Score:** 9

**TL;DR:** The paper evaluates parameter-efficient fine-tuning techniques to reduce biases in Large Language Models against LGBTQIA+ identities, demonstrating significant improvements with LoRA.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the biases against LGBTQIA+ users in Large Language Models that arise from their training data.

**Method:** The study evaluates two PEFT techniques, Low-Rank Adaptation (LoRA) and soft-prompt tuning, using the WinoQueer benchmark to assess bias in three LLMs.

**Key Contributions:**

	1. Evaluation of LoRA and soft-prompt tuning for bias reduction in LLMs
	2. Quantitative analysis of bias using the WinoQueer benchmark
	3. Recommendations for larger queer-authored corpora and richer evaluation suites.

**Result:** LoRA fine-tuning reduces bias scores by up to 50 points and improves neutrality for queer identities from virtually 0% to 36%. Soft-prompt tuning showed only minimal gains.

**Limitations:** Soft-prompt tuning exhibited only marginal improvements, indicating limitations in its effectiveness compared to LoRA.

**Conclusion:** LoRA is effective at reducing biases with minimal parameter increase, suggesting a need for broader adoption and better evaluation strategies for inclusivity in LLMs.

**Abstract:** Large Language Models (LLMs) frequently reproduce the gender- and sexual-identity prejudices embedded in their training corpora, leading to outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of great importance. To achieve this, we evaluate two parameter-efficient fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt tuning - as lightweight alternatives to full-model fine-tuning for mitigating such biases. Using the WinoQueer benchmark, we quantify bias in three open-source LLMs and observe baseline bias scores reaching up to 98 (out of 100) across a range of queer identities defined by gender and/or sexual orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1% additional parameters) on a curated QueerNews corpus reduces those scores by up to 50 points and raises neutrality from virtually 0% to as much as 36%. Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements. These findings show that LoRA can deliver meaningful fairness gains with minimal computation. We advocate broader adoption of community-informed PEFT, the creation of larger queer-authored corpora, and richer evaluation suites beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.

</details>


### [45] [Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models](https://arxiv.org/abs/2507.13761)

*Palash Nandi, Maithili Joshi, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** Visual Language Models, prompt sensitivity, inappropriate content generation, jailbreaking, adversarial examples

**Relevance Score:** 6

**TL;DR:** This paper investigates how prompt sensitivity in Visual Language Models (VLMs) influences the generation of inappropriate content, focusing on factors like visual information, adversarial examples, and positively framed phrases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how prompt design affects model behavior is essential for mitigating risks associated with inappropriate content generation in VLMs.

**Method:** The study analyzes the impact of three key factors on jailbreak success in VLMs: detailed visual information, adversarial examples, and positively framed beginnings through experiments.

**Key Contributions:**

	1. Analyzed impact of prompt design on VLMs
	2. Proposed a framework that improves jailbreak success rates
	3. Showed memes can trigger harmful outputs similar to toxic visuals

**Result:** The research finds that VLMs struggle more in multimodal contexts, with each factor independently capable of triggering harmful outputs, especially with minimal context examples.

**Limitations:** 

**Conclusion:** The paper proposes a new framework that enhances jailbreak success rates and highlights the surprising effectiveness of memes in eliciting harmful content, revealing vulnerabilities in VLMs.

**Abstract:** Language models are highly sensitive to prompt formulations - small changes in input can drastically alter their output. This raises a critical question: To what extent can prompt sensitivity be exploited to generate inapt content? In this paper, we investigate how discrete components of prompt design influence the generation of inappropriate content in Visual Language Models (VLMs). Specifically, we analyze the impact of three key factors on successful jailbreaks: (a) the inclusion of detailed visual information, (b) the presence of adversarial examples, and (c) the use of positively framed beginning phrases. Our findings reveal that while a VLM can reliably distinguish between benign and harmful inputs in unimodal settings (text-only or image-only), this ability significantly degrades in multimodal contexts. Each of the three factors is independently capable of triggering a jailbreak, and we show that even a small number of in-context examples (as few as three) can push the model toward generating inappropriate outputs. Furthermore, we propose a framework that utilizes a skip-connection between two internal layers of the VLM, which substantially increases jailbreak success rates, even when using benign images. Finally, we demonstrate that memes, often perceived as humorous or harmless, can be as effective as toxic visuals in eliciting harmful content, underscoring the subtle and complex vulnerabilities of VLMs.

</details>


### [46] [An Enhanced Model-based Approach for Short Text Clustering](https://arxiv.org/abs/2507.13793)

*Enhao Cheng, Shoujia Zhang, Jianhua Yin, Xuemeng Song, Tian Gan, Liqiang Nie*

**Main category:** cs.CL

**Keywords:** short text clustering, Gibbs Sampling, Dirichlet Multinomial Mixture, GSDMM+, topic modeling

**Relevance Score:** 4

**TL;DR:** This paper presents GSDMM+, an improved algorithm for short text clustering that addresses sparsity and high-dimensionality issues in data, achieving efficient and fine-grained clustering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of short text clustering posed by the sparse and high-dimensional nature of social media data.

**Method:** The paper introduces a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model (GSDMM) and its improved version GSDMM+, which reduces initialization noise and uses strategic cluster merging to enhance clustering performance.

**Key Contributions:**

	1. Development of GSDMM for effective handling of short text data.
	2. Introduction of GSDMM+ which optimizes performance through adaptive weighting and entropy adjustment.
	3. Demonstration of GSDMM+'s superiority in clustering efficiency through extensive experimental validation.

**Result:** Experiments show that GSDMM+ outperforms classical and state-of-the-art clustering methods, providing efficient handling of short texts and revealing topic-related information more effectively.

**Limitations:** 

**Conclusion:** GSDMM+ significantly improves upon existing models by refining clustering granularity and better matching predicted distributions with true categories.

**Abstract:** Short text clustering has become increasingly important with the popularity of social media like Twitter, Google+, and Facebook. Existing methods can be broadly categorized into two paradigms: topic model-based approaches and deep representation learning-based approaches. This task is inherently challenging due to the sparse, large-scale, and high-dimensional characteristics of the short text data. Furthermore, the computational intensity required by representation learning significantly increases the running time. To address these issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model (GSDMM), which effectively handles the sparsity and high dimensionality of short texts while identifying representative words for each cluster. Based on several aspects of GSDMM that warrant further refinement, we propose an improved approach, GSDMM+, designed to further optimize its performance. GSDMM+ reduces initialization noise and adaptively adjusts word weights based on entropy, achieving fine-grained clustering that reveals more topic-related information. Additionally, strategic cluster merging is employed to refine clustering granularity, better aligning the predicted distribution with the true category distribution. We conduct extensive experiments, comparing our methods with both classical and state-of-the-art approaches. The experimental results demonstrate the efficiency and effectiveness of our methods. The source code for our model is publicly available at https://github.com/chehaoa/VEMC.

</details>


### [47] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)

*Hosein Azarbonyad, Zi Long Zhu, Georgios Cheirmpos, Zubair Afzal, Vikrant Yadav, Georgios Tsatsaronis*

**Main category:** cs.CL

**Keywords:** Question and Answer Generation, Knowledge Graph, Large Language Model, Entity Relationship Extraction, Scientific Articles

**Relevance Score:** 7

**TL;DR:** This paper presents a method for extracting key concepts from scientific articles using Question and Answer pairs through two approaches: a saliency-based LLM method and a Knowledge Graph-based method.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Scholars seek to quickly identify and understand the main ideas of research articles.

**Method:** The paper proposes two approaches for generating QA pairs. The first approach selects salient paragraphs and uses a Large Language Model (LLM) to generate and rank questions. The second approach constructs a Knowledge Graph (KG) from scientific articles to generate QAs by extracting salient triplets.

**Key Contributions:**

	1. Development of two distinct QA generation approaches: LLM-based and KG-based.
	2. Introduction of a triplet TF-IDF-like measure for assessing entity saliency.
	3. Evaluation of the approaches using Subject Matter Experts to validate QA quality.

**Result:** The evaluation indicates that the KG-based approach effectively captures the main ideas discussed in the articles, providing high-quality QAs.

**Limitations:** 

**Conclusion:** Fine-tuning the Entity Relationship extraction model on the scientific corpus is crucial for optimizing triplet extraction quality.

**Abstract:** When deciding to read an article or incorporate it into their research, scholars often seek to quickly identify and understand its main ideas. In this paper, we aim to extract these key concepts and contributions from scientific articles in the form of Question and Answer (QA) pairs. We propose two distinct approaches for generating QAs. The first approach involves selecting salient paragraphs, using a Large Language Model (LLM) to generate questions, ranking these questions by the likelihood of obtaining meaningful answers, and subsequently generating answers. This method relies exclusively on the content of the articles. However, assessing an article's novelty typically requires comparison with the existing literature. Therefore, our second approach leverages a Knowledge Graph (KG) for QA generation. We construct a KG by fine-tuning an Entity Relationship (ER) extraction model on scientific articles and using it to build the graph. We then employ a salient triplet extraction method to select the most pertinent ERs per article, utilizing metrics such as the centrality of entities based on a triplet TF-IDF-like measure. This measure assesses the saliency of a triplet based on its importance within the article compared to its prevalence in the literature. For evaluation, we generate QAs using both approaches and have them assessed by Subject Matter Experts (SMEs) through a set of predefined metrics to evaluate the quality of both questions and answers. Our evaluations demonstrate that the KG-based approach effectively captures the main ideas discussed in the articles. Furthermore, our findings indicate that fine-tuning the ER extraction model on our scientific corpus is crucial for extracting high-quality triplets from such documents.

</details>


### [48] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)

*Lizhi Ma, Tong Zhao, Shuai Zhang, Nirui Song, Hongliang He, Anqi Li, Ran Feng, Huachuan Qiu, Jingsong Ma, Zhenzhong Lan*

**Main category:** cs.CL

**Keywords:** psychological states, language use, Chinese counseling, negative emotional words, cultural context

**Relevance Score:** 4

**TL;DR:** The study investigates the link between language use and psychological states in Chinese counseling, revealing significant correlations with negative emotional words but not with first-person singular pronouns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the relationship between linguistic expressions and psychological states of depression and anxiety in Chinese psycho-counseling interactions.

**Method:** Analysis of a corpus from 735 online counseling sessions using a general linear mixed-effect model and LIWC software.

**Key Contributions:**

	1. Identifies linguistic patterns related to psychological states in Chinese counseling settings.
	2. Challenges prior findings from English language contexts regarding pronoun use.
	3. Highlights cultural influences on mental health communication.

**Result:** A significant positive correlation was found between negative emotional words and the severity of depression and anxiety, while first-person singular pronoun usage showed no significant variation with psychological conditions.

**Limitations:** The findings may not generalize to non-Chinese speaking populations or different counseling methods.

**Conclusion:** The study emphasizes the cultural differences in language use in mental health contexts and their implications for therapeutic practices in Chinese populations.

**Abstract:** This study explores the relationship between linguistic expressions and psychological states of depression and anxiety within Chinese psycho-counseling interactions, focusing specifically on the usage of first-person singular pronouns and negative emotional words. Utilizing a corpus derived from 735 online counseling sessions, the analysis employed a general linear mixed-effect model to assess linguistic patterns quantified by the Linguistic Inquiry and Word Count (LIWC) software. Results indicate a significant positive correlation between the frequency of negative emotional words and the severity of both depressive and anxious states among clients. However, contrary to prior findings predominantly derived from English-language contexts, the usage frequency of first-person singular pronouns did not vary significantly with the clients' psychological conditions. These outcomes are discussed within the framework of cultural distinctions between collectivist Chinese contexts and individualistic Western settings, as well as the interactive dynamics unique to psycho-counseling conversations. The findings highlight the nuanced influence of cultural and conversational contexts on language use in mental health communications, providing insights into psycholinguistic markers relevant to therapeutic practices in Chinese-speaking populations.

</details>


### [49] [Modeling Fair Play in Detective Stories with Language Models](https://arxiv.org/abs/2507.13841)

*Eitan Wagner, Renana Keydar, Omri Abend*

**Main category:** cs.CL

**Keywords:** detective fiction, fair play, probabilistic framework, LLM, story quality

**Relevance Score:** 4

**TL;DR:** This paper presents a probabilistic framework to analyze fair play in detective fiction, particularly in LLM-generated stories, highlighting the trade-off between coherence and surprise.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the balance between reader expectations and narrative surprises in detective fiction, and to define fair play in this context.

**Method:** Develops a probabilistic framework for defining fair play and metrics associated with it. Applies the framework to analyze LLM-generated detective stories.

**Key Contributions:**

	1. Probabilistic framework for analyzing detective fiction
	2. Definition and metrics for fair play
	3. Application of the framework to LLM-generated stories

**Result:** The framework shows that LLM-generated stories are unpredictable but struggle to balance surprise with fair play, impacting their overall quality.

**Limitations:** Focused solely on detective fiction; only examines LLM-generated stories.

**Conclusion:** Improving LLM-generated narratives requires addressing the trade-off between coherence and surprise to enhance story quality.

**Abstract:** Effective storytelling relies on a delicate balance between meeting the reader's prior expectations and introducing unexpected developments. In the domain of detective fiction, this tension is known as fair play, which includes the implicit agreement between the writer and the reader as to the range of possible resolutions the mystery story may have. In this work, we present a probabilistic framework for detective fiction that allows us to define desired qualities. Using this framework, we formally define fair play and design appropriate metrics for it. Stemming from these definitions is an inherent tension between the coherence of the story, which measures how much it ``makes sense'', and the surprise it induces. We validate the framework by applying it to LLM-generated detective stories. This domain is appealing since we have an abundance of data, we can sample from the distribution generating the story, and the story-writing capabilities of LLMs are interesting in their own right. Results show that while LLM-generated stories may be unpredictable, they generally fail to balance the trade-off between surprise and fair play, which greatly contributes to their poor quality.

</details>


### [50] [InTraVisTo: Inside Transformer Visualisation Tool](https://arxiv.org/abs/2507.13858)

*Nicolò Brunello, Davide Rigamonti, Andrea Sassella, Vincenzo Scotti, Mark James Carman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Transformer, Visualization, LLM reasoning, InTraVisTo

**Relevance Score:** 8

**TL;DR:** This paper introduces InTraVisTo, a tool for visualizing the internal state and information flow of Transformer-based LLMs to enhance the understanding of their reasoning processes.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of using LLMs in production by providing better insight into their unpredictable behavior and improving understanding of their internal computations.

**Method:** InTraVisTo visualizes token embeddings at each layer of a Transformer model and illustrates the information flow between components using Sankey diagrams.

**Key Contributions:**

	1. Introduction of InTraVisTo tool for visualizing LLMs' internal states
	2. Use of Sankey diagrams to illustrate information flow
	3. Facilitation of understanding LLM reasoning processes

**Result:** The tool enables researchers to trace the computational processes involved in generating each token, facilitating deeper insights into the reasoning capabilities of LLMs.

**Limitations:** 

**Conclusion:** InTraVisTo aids researchers and practitioners in understanding internal patterns of LLMs, potentially leading to improvements in their application.

**Abstract:** The reasoning capabilities of Large Language Models (LLMs) have increased greatly over the last few years, as have their size and complexity. Nonetheless, the use of LLMs in production remains challenging due to their unpredictable nature and discrepancies that can exist between their desired behavior and their actual model output. In this paper, we introduce a new tool, InTraVisTo (Inside Transformer Visualisation Tool), designed to enable researchers to investigate and trace the computational process that generates each token in a Transformer-based LLM. InTraVisTo provides a visualization of both the internal state of the Transformer model (by decoding token embeddings at each layer of the model) and the information flow between the various components across the different layers of the model (using a Sankey diagram). With InTraVisTo, we aim to help researchers and practitioners better understand the computations being performed within the Transformer model and thus to shed some light on internal patterns and reasoning processes employed by LLMs.

</details>


### [51] [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)

*Maciej Jalocha, Johan Hausted Schmidt, William Michelseen*

**Main category:** cs.CL

**Keywords:** Cybersecurity, NER, Label unification, BiLSTM, BERT

**Relevance Score:** 3

**TL;DR:** The paper explores the issue of label unification in cybersecurity named entity recognition, proposing new models to improve dataset usability and performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of standardized labels in cybersecurity NER complicates the merging of datasets, which is crucial for improving model performance.

**Method:** The authors perform a coarse-grained label unification across four cybersecurity datasets, evaluate models using BiLSTM, and explore alternative architectures such as a multihead model and a graph-based transfer model.

**Key Contributions:**

	1. Investigated label unification across multiple cybersecurity datasets.
	2. Conducted pairwise cross-dataset evaluations using BiLSTM models.
	3. Proposed alternative architectures to address unification limitations.

**Result:** Models trained on unified datasets exhibited poor generalization across different datasets. The multihead model offered only marginal improvements, while the graph-based transfer model did not show significant performance enhancements compared to BERT-base-NER.

**Limitations:** The proposed models provided only marginal improvements in generalization across datasets, indicating that more robust solutions are needed for effective label unification.

**Conclusion:** The study highlights the limitations in label unification in cybersecurity NER and suggests the need for further exploration of architectures to enhance performance.

**Abstract:** The field of cybersecurity NER lacks standardized labels, making it challenging to combine datasets. We investigate label unification across four cybersecurity datasets to increase data resource usability. We perform a coarse-grained label unification and conduct pairwise cross-dataset evaluations using BiLSTM models. Qualitative analysis of predictions reveals errors, limitations, and dataset differences. To address unification limitations, we propose alternative architectures including a multihead model and a graph-based transfer model. Results show that models trained on unified datasets generalize poorly across datasets. The multihead model with weight sharing provides only marginal improvements over unified training, while our graph-based transfer model built on BERT-base-NER shows no significant performance gains compared BERT-base-NER.

</details>


### [52] [Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies](https://arxiv.org/abs/2507.13875)

*Carlos Mena, Pol Serra, Jacobo Romero, Abir Messaoudi, Jose Giraldo, Carme Armentano-Oller, Rodolfo Zevallos, Ivan Meza, Javier Hernando*

**Main category:** cs.CL

**Keywords:** Code-switching, Automated speech recognition, Catalan-Spanish, Synthetic data, Whisper models

**Relevance Score:** 6

**TL;DR:** This paper addresses automated speech recognition challenges posed by code-switching between Catalan and Spanish by enhancing ASR models through synthetic data, concatenation of monolingual audio, and real CS data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of dedicated code-switching datasets limits the performance of ASR systems, particularly in multilingual settings where code-switching is prevalent.

**Method:** The authors utilized three strategies for improving ASR: generating synthetic code-switching data, concatenating monolingual audio samples, and fine-tuning existing models with language tokens using real code-switching data.

**Key Contributions:**

	1. Development of synthetic code-switching datasets for ASR
	2. Fine-tuning Whisper models on real and synthetic CS data
	3. Demonstrated effective strategies for improving transcription performance in multilingual ASR systems

**Result:** Combining synthetic code-switching data with dominant language tokens resulted in the highest transcription performance in ASR tasks for Catalan-Spanish code-switching.

**Limitations:** 

**Conclusion:** The study demonstrates that effective use of synthetic and real data can significantly enhance ASR for languages with code-switching phenomena, potentially improving multilingual communication.

**Abstract:** Code-switching (CS), the alternating use of two or more languages, challenges automatic speech recognition (ASR) due to scarce training data and linguistic similarities. The lack of dedicated CS datasets limits ASR performance, as most models rely on monolingual or mixed-language corpora that fail to reflect real-world CS patterns. This issue is critical in multilingual societies where CS occurs in informal and formal settings. A key example is Catalan-Spanish CS, widely used in media and parliamentary speeches. In this work, we improve ASR for Catalan-Spanish CS by exploring three strategies: (1) generating synthetic CS data, (2) concatenating monolingual audio, and (3) leveraging real CS data with language tokens. We extract CS data from Catalan speech corpora and fine-tune OpenAI's Whisper models, making them available on Hugging Face. Results show that combining a modest amount of synthetic CS data with the dominant language token yields the best transcription performance.

</details>


### [53] [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)

*Cole Walsh, Rodica Ivan, Muhammad Zafar Iqbal, Colleen Robb*

**Main category:** cs.CL

**Keywords:** Situational Judgment Tests, large language models, automated scoring, personal skills, professional skills

**Relevance Score:** 7

**TL;DR:** This paper proposes a novel approach to automating the scoring of Situational Judgment Tests (SJTs) using large language models, addressing previous challenges in measuring personal and professional skills.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for scalable systems to measure personal and professional skills critical for student success in diverse careers, especially through standardized tests like SJTs.

**Method:** The study employs large language models (LLMs) to extract construct-relevant features from open-response SJT answers, specifically utilizing the Casper SJT as a case study.

**Key Contributions:**

	1. Novel approach using LLMs for scoring SJTs
	2. Demonstrates efficacy with Casper SJT
	3. Sets foundation for future scalable scoring systems

**Result:** The proposed method shows efficacy in automating the scoring of SJTs, potentially overcoming limitations of past NLP scoring systems related to construct validity.

**Limitations:** The study may face challenges related to the generalizability of LLMs across different contexts of SJTs and varying constructs.

**Conclusion:** This work lays the groundwork for future automated scoring developments for personal and professional skill assessments, ultimately aiming to enhance educational measurement practices.

**Abstract:** Academic programs are increasingly recognizing the importance of personal and professional skills and their critical role alongside technical expertise in preparing students for future success in diverse career paths. With this growing demand comes the need for scalable systems to measure, evaluate, and develop these skills. Situational Judgment Tests (SJTs) offer one potential avenue for measuring these skills in a standardized and reliable way, but open-response SJTs have traditionally relied on trained human raters for evaluation, presenting operational challenges to delivering SJTs at scale. Past attempts at developing NLP-based scoring systems for SJTs have fallen short due to issues with construct validity of these systems. In this article, we explore a novel approach to extracting construct-relevant features from SJT responses using large language models (LLMs). We use the Casper SJT to demonstrate the efficacy of this approach. This study sets the foundation for future developments in automated scoring for personal and professional skills.

</details>


### [54] [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)

*Matous Volf, Jakub Simko*

**Main category:** cs.CL

**Keywords:** political leaning, politicalness, transformer models, text classification, dataset creation

**Relevance Score:** 3

**TL;DR:** This paper explores the classification of text based on political leaning and politicalness using transformer models, highlighting challenges with current approaches and presenting a new diverse dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing models that perform poorly on out-of-distribution texts in classifying political leaning and politicalness.

**Method:** The authors compiled a diverse dataset by merging 12 datasets for political leaning classification and extending 18 datasets to create a new one for politicalness. They employed extensive benchmarking with leave-one-in and leave-one-out methodologies to evaluate model performance.

**Key Contributions:**

	1. Development of a new diverse dataset for politicalness classification
	2. Comprehensive overview of existing datasets and models
	3. Improved benchmarking methodologies for evaluating model performance

**Result:** The study reveals that existing models have limited generalization capabilities, while the new dataset helps improve classification performance on a wider range of texts.

**Limitations:** The new dataset's effectiveness is yet to be tested on a broader range of out-of-distribution texts beyond those included.

**Conclusion:** The paper concludes that enhanced datasets and benchmarking techniques can significantly improve the performance of models in political text classification.

**Abstract:** This paper addresses the challenge of automatically classifying text according to political leaning and politicalness using transformer models. We compose a comprehensive overview of existing datasets and models for these tasks, finding that current approaches create siloed solutions that perform poorly on out-of-distribution texts. To address this limitation, we compile a diverse dataset by combining 12 datasets for political leaning classification and creating a new dataset for politicalness by extending 18 existing datasets with the appropriate label. Through extensive benchmarking with leave-one-in and leave-one-out methodologies, we evaluate the performance of existing models and train new ones with enhanced generalization capabilities.

</details>


### [55] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)

*Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield*

**Main category:** cs.CL

**Keywords:**  Conversational AI,  Persuasion,  Factual accuracy,  Large Language Models,  Political issues

**Relevance Score:** 7

**TL;DR:** Large-scale experiments evaluate the persuasiveness of LLMs on political issues, revealing that prompting and post-training greatly enhance persuasion while potentially decreasing factual accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns over the influence of conversational AI on human beliefs and examine the factors contributing to AI persuasiveness.

**Method:** Conducted three large-scale experiments involving 76,977 participants and 19 LLMs evaluated on 707 political issues, measuring the impacts of post-training and prompting.

**Key Contributions:**

	1. Showed the significant impact of post-training and prompting on AI persuasiveness.
	2. Measured the effects on factual accuracy alongside persuasion.
	3. Provided a large-scale empirical analysis involving extensive data and LLM claims.

**Result:** The study found that persuasion increased by up to 51% through post-training and 27% through prompting, while also noting a decrease in factual accuracy.

**Limitations:** Potential bias in chosen political issues and the context of experiments may limit generalizability.

**Conclusion:** Current fears about AI persuasion power are exaggerated; enhancements from post-training and prompting are significant, but these methods can also reduce accuracy.

**Abstract:** There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.

</details>


### [56] [Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support](https://arxiv.org/abs/2507.13937)

*Jan Trienes, Anastasiia Derzhanskaia, Roland Schwarzkopf, Markus Mühling, Jörg Schlötterer, Christin Seifert*

**Main category:** cs.CL

**Keywords:** Conversational agent, Retrieval-augmented generation, Educational technology, Knowledge base, Natural language processing

**Relevance Score:** 7

**TL;DR:** Marcel is a lightweight conversational agent that supports prospective students with admission inquiries by providing personalized and fast responses while minimizing staff workload.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assist prospective students in obtaining admission-related information quickly and reduce the workload on university staff.

**Method:** The system employs retrieval-augmented generation to ground responses in university resources, using an FAQ retriever to enhance the accuracy of information retrieval from a knowledge base.

**Key Contributions:**

	1. Development of a lightweight conversational agent for educational institutions.
	2. Introduction of an FAQ retriever that enhances information retrieval from knowledge bases.
	3. Technical evaluation showing the effectiveness of the system in real-world scenarios.

**Result:** Marcel improves on traditional dense/hybrid retrieval methods and is designed for easy deployment in resource-constrained academic settings, providing insights from its real-world application.

**Limitations:** 

**Conclusion:** The deployment and evaluation of Marcel demonstrate its effectiveness in answering admission queries while alleviating the burden on university administration.

**Abstract:** We present Marcel, a lightweight and open-source conversational agent designed to support prospective students with admission-related inquiries. The system aims to provide fast and personalized responses, while reducing workload of university staff. We employ retrieval-augmented generation to ground answers in university resources and to provide users with verifiable, contextually relevant information. To improve retrieval quality, we introduce an FAQ retriever that maps user questions to knowledge-base entries, allowing administrators to steer retrieval, and improving over standard dense/hybrid retrieval strategies. The system is engineered for easy deployment in resource-constrained academic settings. We detail the system architecture, provide a technical evaluation of its components, and report insights from a real-world deployment.

</details>


### [57] [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)

*Bianca Raimondi, Maurizio Gabbrielli*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, Multiple Choice Question Answering

**Relevance Score:** 8

**TL;DR:** This paper investigates the effects of primacy bias in fine-tuned Large Language Models (LLMs) on Multiple Choice Question Answering, and introduces a method to reorder answer options to improve accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand and leverage positional biases in LLMs, specifically the primacy effect, in the context of improving performance in Multiple Choice Question Answering tasks.

**Method:** The research involves examining how fine-tuning processes amplify primacy bias due to exposure to human-like patterns, and it proposes a reordering strategy for answer options based on semantic similarity to the query.

**Key Contributions:**

	1. Demonstration of amplified primacy bias in fine-tuned LLMs
	2. Development of a method to reorder answer options based on semantic similarity
	3. Insights into bias-aware model design for NLP applications

**Result:** Experimental results indicate that the proposed reordering technique significantly enhances performance in MCQA tasks by mitigating the influence of primacy bias.

**Limitations:** 

**Conclusion:** The findings highlight both the challenges posed by biases in LLMs and the potential for them to be harnessed strategically in NLP applications, particularly for bias-aware model design.

**Abstract:** Large Language Models (LLMs) have become essential in many Natural Language Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to achieve high accuracy. However, like humans, LLMs exhibit biases, particularly positional biases such as primacy and recency effects, which can influence the accuracy of the answers. The primacy effect-where items presented first are more likely to be remembered or selected-plays a key role in Multiple Choice Question Answering (MCQA), where the order of answer options can affect prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We first show that fine-tuning amplifies this bias, probably due to exposure to human-like patterns. Hence, we strategically leverage this effect by reordering response options based on semantic similarity to the query, without requiring knowledge of the correct answer. Our experimental results show that this approach significantly improves performance in MCQA. More generally, our findings underscore the dual nature of biases as both challenges and opportunities, offering insights for bias-aware model design and NLP applications.

</details>


### [58] [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)

*Bhishma Dedhia, Yuval Kansal, Niraj K. Jha*

**Main category:** cs.CL

**Keywords:** Language Models, Knowledge Graphs, Medical Reasoning, Superintelligence, Task Generation

**Relevance Score:** 9

**TL;DR:** This paper presents a bottom-up approach to training language models for deep domain expertise using knowledge graphs to synthesize tasks, specifically validating the method in the medical domain with significant performance improvements on reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the insufficiency of traditional top-down training in language models for acquiring deep domain expertise, a bottom-up approach is proposed that utilizes knowledge graphs.

**Method:** A task generation pipeline is developed to synthesize reasoning tasks from knowledge graph primitives and fine-tune language models on this domain-specific curriculum.

**Key Contributions:**

	1. Development of a knowledge graph-based task generation pipeline for language models
	2. Introduction of the QwQ-Med-3 model demonstrating advancements in medical reasoning
	3. Creation of ICD-Bench, an evaluation suite for assessing reasoning abilities in medical domains

**Result:** The QwQ-Med-3 model, fine-tuned on a medical knowledge graph, significantly outperforms state-of-the-art reasoning models on diverse medical tasks and question-answer benchmarks.

**Limitations:** 

**Conclusion:** The study suggests a shift towards using compositional structures in knowledge graphs to enhance task reasoning in language models, potentially leading to domain-specific superintelligence in areas like medicine.

**Abstract:** Language models traditionally used for cross-domain generalization have recently demonstrated task-specific reasoning. However, their top-down training approach on general corpora is insufficient for acquiring abstractions needed for deep domain expertise. This may require a bottom-up approach that acquires expertise by learning to compose simple domain concepts into more complex ones. A knowledge graph (KG) provides this compositional structure, where domain primitives are represented as head-relation-tail edges and their paths encode higher-level concepts. We present a task generation pipeline that synthesizes tasks directly from KG primitives, enabling models to acquire and compose them for reasoning. We fine-tune language models on the resultant KG-grounded curriculum to demonstrate domain-specific superintelligence. While broadly applicable, we validate our approach in medicine, where reliable KGs exist. Using a medical KG, we curate 24,000 reasoning tasks paired with thinking traces derived from diverse medical primitives. We fine-tune the QwQ-32B model on this curriculum to obtain QwQ-Med-3 that takes a step towards medical superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify reasoning abilities across 15 medical domains. Our experiments demonstrate that QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired primitives to widen the performance gap on the hardest tasks of ICD-Bench. Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3 transfers acquired expertise to enhance the base model's performance. While the industry's approach to artificial general intelligence (AGI) emphasizes broad expertise, we envision a future in which AGI emerges from the composable interaction of efficient domain-specific superintelligent agents.

</details>


### [59] [Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic](https://arxiv.org/abs/2507.13977)

*Lilit Grigoryan, Nikolay Karpov, Enas Albasiri, Vitaly Lavrukhin, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** Arabic ASR, FastConformer, Modern Standard Arabic, Classical Arabic, Speech Processing

**Relevance Score:** 4

**TL;DR:** This paper presents a universal methodology for Arabic speech and text processing with novel ASR models for Modern Standard Arabic and a unified model for both Modern Standard and Classical Arabic, achieving state-of-the-art results.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant challenges faced in developing Automatic Speech Recognition systems for Arabic, particularly due to the language's complexity and the lack of public models.

**Method:** The paper introduces a universal methodology for Arabic speech and text processing, utilizing the FastConformer architecture to train two models: one for Modern Standard Arabic and a unified model for both Modern Standard and Classical Arabic.

**Key Contributions:**

	1. Introduction of a universal methodology for Arabic speech processing
	2. Training of a new MSA model with state-of-the-art performance
	3. Development of the first unified public model for both MSA and Classical Arabic.

**Result:** The MSA model achieves state-of-the-art performance on related datasets, while the unified model achieves state-of-the-art accuracy for Classical Arabic with diacritics, maintaining strong performance for MSA.

**Limitations:** 

**Conclusion:** The models and their training recipes are open-sourced to promote reproducibility and future work in Arabic ASR.

**Abstract:** Despite Arabic being one of the most widely spoken languages, the development of Arabic Automatic Speech Recognition (ASR) systems faces significant challenges due to the language's complexity, and only a limited number of public Arabic ASR models exist. While much of the focus has been on Modern Standard Arabic (MSA), there is considerably less attention given to the variations within the language. This paper introduces a universal methodology for Arabic speech and text processing designed to address unique challenges of the language. Using this methodology, we train two novel models based on the FastConformer architecture: one designed specifically for MSA and the other, the first unified public model for both MSA and Classical Arabic (CA). The MSA model sets a new benchmark with state-of-the-art (SOTA) performance on related datasets, while the unified model achieves SOTA accuracy with diacritics for CA while maintaining strong performance for MSA. To promote reproducibility, we open-source the models and their training recipes.

</details>


### [60] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)

*Haoyu He, Haozheng Luo, Yan Chen, Qi R. Wang*

**Main category:** cs.CL

**Keywords:** Human Mobility, Large Language Models, Spatio-Temporal Prediction, Hierarchical Tokenization, Trajectory Reasoning

**Relevance Score:** 8

**TL;DR:** RHYTHM is a framework using LLMs for predicting human mobility by partitioning trajectories into tokens, improving computational efficiency and accuracy in spatio-temporal reasoning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human mobility predictions using LLMs while enhancing computational efficiency and capturing complex dependencies in trajectory data.

**Method:** RHYTHM partitions trajectories into daily segments, encoding them as discrete tokens with hierarchical attention and utilizing frozen LLM prompt embeddings.

**Key Contributions:**

	1. Introduction of hierarchical attention for trajectory tokenization
	2. Use of frozen LLMs to enrich token representations
	3. Significant improvements in prediction accuracy and training efficiency

**Result:** Demonstrated a 2.4% accuracy improvement, 5.0% increase in accuracy on weekends, and a 24.6% reduction in training time on three real-world datasets.

**Limitations:** 

**Conclusion:** RHYTHM effectively enhances spatio-temporal prediction of human mobility while optimizing for computational efficiency.

**Abstract:** We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for Human Mobility), a framework that leverages large language models (LLMs) as spatio-temporal predictors and trajectory reasoners. RHYTHM partitions trajectories into daily segments encoded as discrete tokens with hierarchical attention, capturing both daily and weekly dependencies while substantially reducing the sequence length. Token representations are enriched with pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability to capture interdependencies without extensive computational overhead. By freezing the LLM backbone, RHYTHM achieves significant computational efficiency. Evaluation on three real-world datasets demonstrates a 2.4% improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in training time compared to state-of-the-art methods.

</details>


### [61] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)

*Jianfei Li, Kevin Kam Fung Yuen*

**Main category:** cs.CL

**Keywords:** Cognitive Pairwise Comparison, Sentiment Analysis, Model Selection, Machine Learning, ALBERT

**Relevance Score:** 6

**TL;DR:** This study introduces the CPC-CMS framework for selecting the best document-level sentiment analysis model using expert-weighted evaluation criteria.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the model selection process in document-level sentiment analysis by utilizing expert knowledge and a systematic approach to evaluate multiple classification models.

**Method:** The study employs a Cognitive Pairwise Comparison approach to weight various evaluation criteria for sentiment analysis models, then applies baseline models like Naive Bayes, SVC, Random Forest, and others on three social media datasets to identify the optimal model.

**Key Contributions:**

	1. Introduction of the CPC-CMS framework for model selection
	2. Application and demonstration on sentiment analysis using multiple datasets
	3. Evaluation of various classification models with a focus on expert-driven criteria weighting

**Result:** The results indicate that ALBERT performs best on the datasets in terms of accuracy when time is not a factor, while no single model consistently outperforms others when time consumption is considered.

**Limitations:** Primarily focused on sentiment analysis; the applicability to other domains remains to be validated in further studies.

**Conclusion:** CPC-CMS offers a valuable framework for model selection, adaptable to different classification tasks beyond sentiment analysis.

**Abstract:** This study proposes the Cognitive Pairwise Comparison Classification Model Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC, based on expert knowledge judgment, is used to calculate the weights of evaluation criteria, including accuracy, precision, recall, F1-score, specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from Transformers (ALBERT) are chosen as classification baseline models. A weighted decision matrix consisting of classification evaluation scores with respect to criteria weights, is formed to select the best classification model for a classification problem. Three open datasets of social media are used to demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation, for evaluation results excluding the time factor, ALBERT is the best for the three datasets; if time consumption is included, no single model always performs better than the other models. The CPC-CMS can be applied to the other classification applications in different areas.

</details>


### [62] [Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks](https://arxiv.org/abs/2507.14045)

*Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, biomedical applications, multimodal processing, text classification, open-source models

**Relevance Score:** 9

**TL;DR:** Evaluation of cost-efficient Large Language Models (LLMs) for biomedical tasks shows varying performance across models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of different LLMs in biomedical applications involving text and image data.

**Method:** Evaluation of various closed-source and open-source LLMs on tasks like biomedical text classification, generation, and multimodal image processing.

**Key Contributions:**

	1. Comprehensive evaluation of LLMs for diverse biomedical tasks
	2. Insights into performance differences between closed-source and open-source models
	3. Recommendations for model selection in biomedical applications

**Result:** No single LLM consistently outperforms others; different models excel at different tasks, with open-source LLMs sometimes delivering superior results.

**Limitations:** No evaluation of LLMs beyond biomedical domains.

**Conclusion:** The findings provide guidance for choosing the most suitable LLMs for specific biomedical applications, considering both performance and privacy.

**Abstract:** This paper presents a comprehensive evaluation of cost-efficient Large Language Models (LLMs) for diverse biomedical tasks spanning both text and image modalities. We evaluated a range of closed-source and open-source LLMs on tasks such as biomedical text classification and generation, question answering, and multimodal image processing. Our experimental findings indicate that there is no single LLM that can consistently outperform others across all tasks. Instead, different LLMs excel in different tasks. While some closed-source LLMs demonstrate strong performance on specific tasks, their open-source counterparts achieve comparable results (sometimes even better), with additional benefits like faster inference and enhanced privacy. Our experimental results offer valuable insights for selecting models that are optimally suited for specific biomedical applications.

</details>


### [63] [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)

*Lautaro Estienne, Gabriel Ben Zenou, Nona Naderi, Jackie Cheung, Pablo Piantanida*

**Main category:** cs.CL

**Keywords:** Collaborative dialog, Rational Speech Act, Information theory, AI language agents, Medical applications

**Relevance Score:** 8

**TL;DR:** This paper introduces the Collaborative Rational Speech Act (CRSA) framework, an extension of the RSA model that enhances pragmatic reasoning in multi-turn dialogs by optimizing a gain function designed for collaborative scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for AI systems to effectively collaborate by understanding shared goals and beliefs in conversational contexts.

**Method:** CRSA utilizes an information-theoretic approach, optimizing a gain function derived from rate-distortion theory to better model multi-turn dialog and private information conditions.

**Key Contributions:**

	1. Introduction of the CRSA framework for collaborative dialog
	2. Application of information-theoretic principles to multi-turn conversation modeling
	3. Demonstration of improved performance in medical dialog scenarios

**Result:** CRSA outperforms existing baselines in terms of consistency, interpretability, and collaborative behavior in referential games and medical dialog scenarios.

**Limitations:** 

**Conclusion:** The CRSA framework advances the capabilities of language agents towards more pragmatic and socially aware interactions.

**Abstract:** As AI systems take on collaborative roles, they must reason about shared goals and beliefs-not just generate fluent language. The Rational Speech Act (RSA) framework offers a principled approach to pragmatic reasoning, but existing extensions face challenges in scaling to multi-turn, collaborative scenarios. In this paper, we introduce Collaborative Rational Speech Act (CRSA), an information-theoretic (IT) extension of RSA that models multi-turn dialog by optimizing a gain function adapted from rate-distortion theory. This gain is an extension of the gain model that is maximized in the original RSA model but takes into account the scenario in which both agents in a conversation have private information and produce utterances conditioned on the dialog. We demonstrate the effectiveness of CRSA on referential games and template-based doctor-patient dialogs in the medical domain. Empirical results show that CRSA yields more consistent, interpretable, and collaborative behavior than existing baselines-paving the way for more pragmatic and socially aware language agents.

</details>


### [64] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)

*Garapati Keerthana, Manik Gupta*

**Main category:** cs.CL

**Keywords:** Electronic Health Record, progress notes, large language model, healthcare documentation, clinical decision support

**Relevance Score:** 9

**TL;DR:** DENSE is a system that generates clinically coherent progress notes by simulating physician workflows and leveraging a retrieval strategy with a large language model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of progress notes in EHR datasets limits the understanding of patients' longitudinal health stories and clinical decision-making.

**Method:** DENSE categorizes notes finely and aligns them temporally, using clinically relevant retrieval strategies to prompt an LLM for generating notes.

**Key Contributions:**

	1. Development of a fine-grained note categorization
	2. Introduction of a temporal alignment mechanism
	3. Integration of LLM for generating coherent clinical notes

**Result:** DENSE-generated notes show improved longitudinal fidelity and coherence, achieving a temporal alignment ratio of 1.089.

**Limitations:** Requires comprehensive initial patient data and progress note documentation for optimal performance.

**Conclusion:** DENSE aids in restoring narrative coherence in healthcare documentation, which enhances summarization, predictive modeling, and clinical decision support.

**Abstract:** Progress notes are among the most clinically meaningful artifacts in an Electronic Health Record (EHR), offering temporally grounded insights into a patient's evolving condition, treatments, and care decisions. Despite their importance, they are severely underrepresented in large-scale EHR datasets. For instance, in the widely used Medical Information Mart for Intensive Care III (MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress notes, leaving gaps in longitudinal patient narratives. In contrast, the dataset contains a diverse array of other note types, each capturing different aspects of care.   We present DENSE (Documenting Evolving Progress Notes from Scattered Evidence), a system designed to align with clinical documentation workflows by simulating how physicians reference past encounters while drafting progress notes. The system introduces a fine-grained note categorization and a temporal alignment mechanism that organizes heterogeneous notes across visits into structured, chronological inputs. At its core, DENSE leverages a clinically informed retrieval strategy to identify temporally and semantically relevant content from both current and prior visits. This retrieved evidence is used to prompt a large language model (LLM) to generate clinically coherent and temporally aware progress notes.   We evaluate DENSE on a curated cohort of patients with multiple visits and complete progress note documentation. The generated notes demonstrate strong longitudinal fidelity, achieving a temporal alignment ratio of $1.089$, surpassing the continuity observed in original notes. By restoring narrative coherence across fragmented documentation, our system supports improved downstream tasks such as summarization, predictive modeling, and clinical decision support, offering a scalable solution for LLM-driven note synthesis in real-world healthcare settings.

</details>


### [65] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)

*Brian Ondov, William Xia, Kush Attal, Ishita Unde, Jerry He, Hoa Dang, Ian Soboroff, Dina Demner-Fushman*

**Main category:** cs.CL

**Keywords:** Biomedical Literature, Plain Language, Language Models, Evaluation, Text Retrieval Conferences

**Relevance Score:** 8

**TL;DR:** This study evaluates the adaptation of biomedical literature to plain language using language models, revealing both promising results and the need for improved evaluation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To make biomedical literature more accessible to patients and caregivers by using language models for adaptation to plain language.

**Method:** Hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the Text Retrieval Conferences, involving tasks for rewriting abstracts and replacing difficult terms, with evaluations from biomedical experts.

**Key Contributions:**

	1. Evaluation of the PLABA track to assess LLMs in biomedical literature adaptation
	2. Development of extensive manual evaluation frameworks from biomedical experts
	3. Identification of challenges in automatic benchmarking for LLM adaptations

**Result:** Top-performing models achieved human-level accuracy in factual content but fell short in simplicity; LLM systems performed well in generating accurate replacements for difficult terms but struggled overall with classification and brevity.

**Limitations:** Models excelled in accuracy but struggled with simplicity and brevity; automatic evaluation metrics did not align well with manual assessments.

**Conclusion:** The PLABA track demonstrated the potential of LLMs in adapting biomedical literature for public consumption, while also underlining the necessity for better automatic evaluation tools.

**Abstract:** Objective: Recent advances in language models have shown potential to adapt professional-facing biomedical literature to plain language, making it accessible to patients and caregivers. However, their unpredictability, combined with the high potential for harm in this domain, means rigorous evaluation is necessary. Our goals with this track were to stimulate research and to provide high-quality evaluation of the most promising systems.   Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts (PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included complete, sentence-level, rewriting of abstracts (Task 1) as well as identifying and replacing difficult terms (Task 2). For automatic evaluation of Task 1, we developed a four-fold set of professionally-written references. Submissions for both Tasks 1 and 2 were provided extensive manual evaluation from biomedical experts.   Results: Twelve teams spanning twelve countries participated in the track, with models from multilayer perceptrons to large pretrained transformers. In manual judgments of Task 1, top-performing models rivaled human levels of factual accuracy and completeness, but not simplicity or brevity. Automatic, reference-based metrics generally did not correlate well with manual judgments. In Task 2, systems struggled with identifying difficult terms and classifying how to replace them. When generating replacements, however, LLM-based systems did well in manually judged accuracy, completeness, and simplicity, though not in brevity.   Conclusion: The PLABA track showed promise for using Large Language Models to adapt biomedical literature for the general public, while also highlighting their deficiencies and the need for improved automatic benchmarking tools.

</details>


### [66] [ViMMRC 2.0 -- Enhancing Machine Reading Comprehension on Vietnamese Literature Text](https://arxiv.org/abs/2303.18162)

*Son T. Luu, Khoi Trong Hoang, Tuong Quang Pham, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen*

**Main category:** cs.CL

**Keywords:** Machine reading comprehension, Vietnamese Textbooks, Multi-step attention network, Natural language inference, Dataset enhancement

**Relevance Score:** 4

**TL;DR:** Introduction of ViMMRC 2.0 for multiple-choice reading comprehension in Vietnamese with an enhanced dataset and methodology.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the capability of computers in understanding Vietnamese texts and facilitating reading comprehension tasks.

**Method:** A multi-stage approach combining a multi-step attention network (MAN) with natural language inference (NLI) to enhance reading comprehension model performance.

**Key Contributions:**

	1. Introduction of ViMMRC 2.0 dataset for Vietnamese reading comprehension
	2. Increased question difficulty to challenge existing models
	3. Proposed multi-stage attention methodology to improve performance

**Result:** The proposed methodology outperformed baseline BERTology models on the new dataset, highlighting the challenge of implicit context understanding.

**Limitations:** 

**Conclusion:** The new dataset aims to stimulate further research in Vietnamese language comprehension.

**Abstract:** Machine reading comprehension has been an interesting and challenging task in recent years, with the purpose of extracting useful information from texts. To attain the computer ability to understand the reading text and answer relevant information, we introduce ViMMRC 2.0 - an extension of the previous ViMMRC for the task of multiple-choice reading comprehension in Vietnamese Textbooks which contain the reading articles for students from Grade 1 to Grade 12. This dataset has 699 reading passages which are prose and poems, and 5,273 questions. The questions in the new dataset are not fixed with four options as in the previous version. Moreover, the difficulty of questions is increased, which challenges the models to find the correct choice. The computer must understand the whole context of the reading passage, the question, and the content of each choice to extract the right answers. Hence, we propose a multi-stage approach that combines the multi-step attention network (MAN) with the natural language inference (NLI) task to enhance the performance of the reading comprehension model. Then, we compare the proposed methodology with the baseline BERTology models on the new dataset and the ViMMRC 1.0. From the results of the error analysis, we found that the challenge of the reading comprehension models is understanding the implicit context in texts and linking them together in order to find the correct answers. Finally, we hope our new dataset will motivate further research to enhance the ability of computers to understand the Vietnamese language.

</details>


### [67] [Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation](https://arxiv.org/abs/2404.07053)

*Elisa Sanchez-Bayona, Rodrigo Agerri*

**Main category:** cs.CL

**Keywords:** metaphor detection, language models, cross-lingual, NLI, dataset

**Relevance Score:** 8

**TL;DR:** Meta4XNLI is a new dataset for metaphor detection and interpretation in English and Spanish, evaluating language models' performance with non-literal language.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding metaphors is important for Language Models to accurately interpret figurative language, which is common in everyday communication.

**Method:** The authors created the Meta4XNLI dataset containing metaphor annotations in both Spanish and English, and conducted monolingual and cross-lingual experiments to assess language models' abilities in metaphor identification and understanding.

**Key Contributions:**

	1. Introduction of the Meta4XNLI dataset for metaphor detection and interpretation
	2. Analysis of language models' metaphor identification and understanding capabilities
	3. Insights on the effects of translation on metaphor understanding across languages

**Result:** Experiments revealed insights into language models' performance on metaphor tasks, highlighting challenges faced with non-literal expressions and shedding light on metaphor transferability and translation effects.

**Limitations:** Limited to metaphorical expressions, may not generalize to all figurative language forms.

**Conclusion:** The study emphasizes the need for better metaphor comprehension in language models and suggests further exploration of multilingual annotated datasets.

**Abstract:** Metaphors, although occasionally unperceived, are ubiquitous in our everyday language. Thus, it is crucial for Language Models to be able to grasp the underlying meaning of this kind of figurative language. In this work, we present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection and interpretation that contains metaphor annotations in both Spanish and English. We investigate language models' metaphor identification and understanding abilities through a series of monolingual and cross-lingual experiments by leveraging our proposed corpus. In order to comprehend how these non-literal expressions affect models' performance, we look over the results and perform an error analysis. Additionally, parallel data offers many potential opportunities to investigate metaphor transferability between these languages and the impact of translation on the development of multilingual annotated resources.

</details>


### [68] [psifx -- Psychological and Social Interactions Feature Extraction Package](https://arxiv.org/abs/2407.10266)

*Guillaume Rochette, Mathieu Rochat, Matthew J. Vowels*

**Main category:** cs.CL

**Keywords:** Machine Learning, Human-Computer Interaction, Psychological Research, Open Source, Feature Extraction

**Relevance Score:** 8

**TL;DR:** psifx is a multi-modal feature extraction toolkit for automating and standardizing data annotation in human sciences research.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To automate data annotation, develop open-source psychology research software, and ensure accessibility for non-expert users.

**Method:** The toolkit includes tools for speaker diarization, closed-caption transcription, pose estimation, gaze tracking, and textual feature extraction with LLM support.

**Key Contributions:**

	1. Plug-and-play nature for ease of use
	2. Support for various data types (audio, video, text)
	3. Community-driven development for continuous improvement

**Result:** psifx offers a modular framework that simplifies data processing tasks for psychological and social sciences.

**Limitations:** 

**Conclusion:** The framework enhances the capability for real-time behavioral studies in human sciences research.

**Abstract:** psifx is a plug-and-play multi-modal feature extraction toolkit, aiming to facilitate and democratize the use of state-of-the-art machine learning techniques for human sciences research. It is motivated by a need (a) to automate and standardize data annotation processes that typically require expensive, lengthy, and inconsistent human labour; (b) to develop and distribute open-source community-driven psychology research software; and (c) to enable large-scale access and ease of use for non-expert users. The framework contains an array of tools for tasks such as speaker diarization, closed-caption transcription and translation from audio; body, hand, and facial pose estimation and gaze tracking with multi-person tracking from video; and interactive textual feature extraction supported by large language models. The package has been designed with a modular and task-oriented approach, enabling the community to add or update new tools easily. This combination creates new opportunities for in-depth study of real-time behavioral phenomena in psychological and social science research.

</details>


### [69] [Sparse Rewards Can Self-Train Dialogue Agents](https://arxiv.org/abs/2409.04617)

*Barrett Martin Lattimer, Varun Gangal, Ryan McDonald, Yi Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Improvement, Machine Learning, Human Feedback, Simulation

**Relevance Score:** 9

**TL;DR:** This paper presents JOSH, a self-improvement paradigm for LLMs that enables them to enhance their performance without human feedback, using a novel sparse reward simulation environment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of obtaining meaningful human feedback for LLMs as they improve, the authors propose a self-improvement approach that allows LLMs to autonomously enhance their performance.

**Method:** Introducing JOSH (Juxtaposed Outcomes for Simulation Harvesting), a self-alignment algorithm that uses a sparse reward simulation environment to extract optimal behaviors and retrain LLMs on their own outputs.

**Key Contributions:**

	1. Implementation of a self-alignment algorithm for LLMs
	2. Creation of ToolWOZ, a sparse reward tool-calling simulation environment
	3. Demonstrated significant performance improvement in LLMs without external feedback.

**Result:** Models trained with JOSH show significant improvement in tool-based interactions while maintaining general capabilities across various benchmarks.

**Limitations:** 

**Conclusion:** The JOSH method demonstrates that LLMs can successfully self-improve, making the traditional reliance on human feedback less necessary as model capabilities grow.

**Abstract:** Recent advancements in state-of-the-art (SOTA) Large Language Model (LLM) agents, especially in multi-turn dialogue tasks, have been primarily driven by supervised fine-tuning and high-quality human feedback. However, as base LLM models continue to improve, acquiring meaningful human feedback has become increasingly challenging and costly. In certain domains, base LLM agents may eventually exceed human capabilities, making traditional feedback-driven methods impractical. In this paper, we introduce a novel self-improvement paradigm that empowers LLM agents to autonomously enhance their performance without external human feedback. Our method, Juxtaposed Outcomes for Simulation Harvesting (JOSH), is a self-alignment algorithm that leverages a sparse reward simulation environment to extract ideal behaviors and further train the LLM on its own outputs. We present ToolWOZ, a sparse reward tool-calling simulation environment derived from MultiWOZ. We demonstrate that models trained with JOSH, both small and frontier, significantly improve tool-based interactions while preserving general model capabilities across diverse benchmarks. Our code and data are publicly available on GitHub at https://github.com/asappresearch/josh-llm-simulation-training

</details>


### [70] [Cross-Lingual Auto Evaluation for Assessing Multilingual LLMs](https://arxiv.org/abs/2410.13394)

*Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Dilip Venkatesh, Raj Dabre, Anoop Kunchukuttan, Mitesh M. Khapra*

**Main category:** cs.CL

**Keywords:** cross-lingual evaluation, multilingual LLMs, automated evaluation

**Relevance Score:** 8

**TL;DR:** The paper presents the Cross Lingual Auto Evaluation (CIA) Suite, a framework designed for evaluating machine-generated text in multiple languages, addressing the existing gap in multilingual evaluation methodologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap in evaluation methodologies for machine-generated text, especially for non-English languages, as most existing approaches focus on English.

**Method:** The study introduces the CIA Suite, which includes evaluator LLMs and a novel test set designed for multilingual evaluation, featuring human-annotated instructions and human judgment scores across six languages.

**Key Contributions:**

	1. Introduction of the CIA Suite for multilingual evaluation
	2. Development of Hercule, a cross-lingual evaluation model
	3. Creation of a test set with human-annotated instructions across six languages

**Result:** Hercule, the proposed model, demonstrates a closer alignment with human judgments than proprietary models and is effective in zero-shot evaluation on unseen languages.

**Limitations:** 

**Conclusion:** The research showcases a scalable and effective approach for cross-lingual evaluation using LLMs, with all resources to be publicly available for further research.

**Abstract:** Evaluating machine-generated text remains a significant challenge in NLP, especially for non-English languages. Current methodologies, including automated metrics, human assessments, and LLM-based evaluations, predominantly focus on English, revealing a significant gap in multilingual evaluation frameworks. We introduce the Cross Lingual Auto Evaluation (CIA) Suite, an extensible framework that includes evaluator LLMs (Hercule) and a novel test set (Recon) specifically designed for multilingual evaluation. Our test set features 500 human-annotated instructions spanning various task capabilities along with human judgment scores across six languages. This would enable benchmarking of general-purpose multilingual LLMs and facilitate meta-evaluation of Evaluator LLMs. The proposed model, Hercule, is a cross-lingual evaluation model that addresses the scarcity of reference answers in the target language by learning to assign scores to responses based on easily available reference answers in English. Our experiments demonstrate that Hercule aligns more closely with human judgments compared to proprietary models, demonstrating the effectiveness of such cross-lingual evaluation in low resource scenarios. Further, it is also effective in zero-shot evaluation on unseen languages. This study is the first comprehensive examination of cross-lingual evaluation using LLMs, presenting a scalable and effective approach for multilingual assessment. All code, datasets, and models will be publicly available to enable further research in this important area.

</details>


### [71] [Temporal reasoning for timeline summarisation in social media](https://arxiv.org/abs/2501.00152)

*Jiayu Song, Mahmud Elahi Akhter, Dana Atzil Slonim, Maria Liakata*

**Main category:** cs.CL

**Keywords:** temporal reasoning, timeline summarisation, large language models, mental health, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of temporal reasoning in Large Language Models for enhancing timeline summarisation. It introduces a new dataset, NarrativeReason, and combines temporal reasoning with summarisation through knowledge distillation, achieving better performance in mental health contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if improving temporal reasoning in LLMs can enhance the quality of timeline summarisation, especially in the context of mental health-related discussions in social media.

**Method:** The paper presents NarrativeReason, a new dataset focused on temporal relationships, and employs a knowledge distillation framework to transfer temporal reasoning capabilities from a teacher model to a student model, aimed at improving timeline summarisation.

**Key Contributions:**

	1. Introduction of NarrativeReason dataset for temporal reasoning in narratives.
	2. Combination of temporal reasoning and timeline summarisation through a knowledge distillation framework.
	3. Demonstrated superior performance on mental health-related timeline summarisation tasks.

**Result:** The proposed model outperforms existing methods on mental health timeline summarisation tasks, showcasing enhanced handling of long texts with repetitive events.

**Limitations:** 

**Conclusion:** Leveraging temporal reasoning significantly improves the quality of timeline summarisation in LLMs, particularly for complex narratives involving emotions.

**Abstract:** This paper explores whether enhancing temporal reasoning capabilities in Large Language Models (LLMs) can improve the quality of timeline summarisation, the task of summarising long texts containing sequences of events, such as social media threads. We first introduce NarrativeReason, a novel dataset focused on temporal relationships among sequential events within narratives, distinguishing it from existing temporal reasoning datasets that primarily address pair-wise event relationships. Our approach then combines temporal reasoning with timeline summarisation through a knowledge distillation framework, where we first fine-tune a teacher model on temporal reasoning tasks and then distill this knowledge into a student model while simultaneously training it for the task of timeline summarisation. Experimental results demonstrate that our model achieves superior performance on out-of-domain mental health-related timeline summarisation tasks, which involve long social media threads with repetitions of events and a mix of emotions, highlighting the importance and generalisability of leveraging temporal reasoning to improve timeline summarisation.

</details>


### [72] [Consistency of Responses and Continuations Generated by Large Language Models on Social Media](https://arxiv.org/abs/2501.08102)

*Wenlu Fan, Yuqi Zhu, Chenyang Wang, Bin Wang, Wentao Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, emotional consistency, semantic coherence, social media, human-AI interaction

**Relevance Score:** 8

**TL;DR:** This study analyzes emotional content handling and semantic coherence of large language models (LLMs) in social media by examining Twitter and Reddit discussions on climate change using Gemma and Llama models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs manage emotional content and maintain semantic relationships in social media contexts.

**Method:** The study investigates LLMs using continuation and response tasks, focusing on emotional transitions and semantic similarity in discussions around climate change from Twitter and Reddit.

**Key Contributions:**

	1. Insights into emotional content handling of LLMs in social media.
	2. Comparison of emotional patterns between Gemma and Llama models.
	3. Analysis of semantic coherence against human-authored content.

**Result:** Both models show high semantic coherence; however, Gemma amplifies negative emotions like anger, while Llama preserves a broader emotional spectrum. Both exhibit reduced emotional intensity in responses compared to human content, with a bias toward positive emotions.

**Limitations:** The study is limited to specific contexts of climate change discussions and may not generalize to all social media interactions.

**Conclusion:** The findings highlight the emotional and semantic processing capabilities of LLMs, affecting their potential deployment in social media and human-AI interaction.

**Abstract:** Large Language Models (LLMs) demonstrate remarkable capabilities in text generation, yet their emotional consistency and semantic coherence in social media contexts remain insufficiently understood. This study investigates how LLMs handle emotional content and maintain semantic relationships through continuation and response tasks using two open-source models: Gemma and Llama. By analyzing climate change discussions from Twitter and Reddit, we examine emotional transitions, intensity patterns, and semantic similarity between human-authored and LLM-generated content. Our findings reveal that while both models maintain high semantic coherence, they exhibit distinct emotional patterns: Gemma shows a tendency toward negative emotion amplification, particularly anger, while maintaining certain positive emotions like optimism. Llama demonstrates superior emotional preservation across a broader spectrum of affects. Both models systematically generate responses with attenuated emotional intensity compared to human-authored content and show a bias toward positive emotions in response tasks. Additionally, both models maintain strong semantic similarity with original texts, though performance varies between continuation and response tasks. These findings provide insights into LLMs' emotional and semantic processing capabilities, with implications for their deployment in social media contexts and human-AI interaction design.

</details>


### [73] [ASTRID -- An Automated and Scalable TRIaD for the Evaluation of RAG-based Clinical Question Answering Systems](https://arxiv.org/abs/2501.08208)

*Mohita Chowdhury, Yajie Vera He, Jared Joselowitz, Aisling Higham, Ernest Lim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval Augmented Generation, Clinical Question Answering

**Relevance Score:** 9

**TL;DR:** Introduction of ASTRID, a triad of metrics for evaluating clinical QA systems using LLMs and RAG.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current automated RAG metrics are ineffective in clinical settings, necessitating the need for better evaluation methods that are scalable and cost-effective.

**Method:** ASTRID comprises three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). It includes a dataset of over 200 real-world patient questions to validate these metrics against human evaluations.

**Key Contributions:**

	1. Introduction of ASTRID as a new evaluation framework
	2. Validation of CF metric against human evaluations
	3. Publication of datasets and prompts for clinical QA research

**Result:** CF predicts human ratings of faithfulness better than existing definitions; the triad aligns well with clinician assessments of model responses.

**Limitations:** 

**Conclusion:** ASTRID provides an automated evaluation approach that is promising for LLM-driven systems, and resources like prompts and datasets are made publicly available for further research.

**Abstract:** Large Language Models (LLMs) have shown impressive potential in clinical question answering (QA), with Retrieval Augmented Generation (RAG) emerging as a leading approach for ensuring the factual accuracy of model responses. However, current automated RAG metrics perform poorly in clinical and conversational use cases. Using clinical human evaluations of responses is expensive, unscalable, and not conducive to the continuous iterative development of RAG systems. To address these challenges, we introduce ASTRID - an Automated and Scalable TRIaD for evaluating clinical QA systems leveraging RAG - consisting of three metrics: Context Relevance (CR), Refusal Accuracy (RA), and Conversational Faithfulness (CF). Our novel evaluation metric, CF, is designed to better capture the faithfulness of a model's response to the knowledge base without penalising conversational elements. To validate our triad, we curate a dataset of over 200 real-world patient questions posed to an LLM-based QA agent during surgical follow-up for cataract surgery - the highest volume operation in the world - augmented with clinician-selected questions for emergency, clinical, and non-clinical out-of-domain scenarios. We demonstrate that CF can predict human ratings of faithfulness better than existing definitions for conversational use cases. Furthermore, we show that evaluation using our triad consisting of CF, RA, and CR exhibits alignment with clinician assessment for inappropriate, harmful, or unhelpful responses. Finally, using nine different LLMs, we demonstrate that the three metrics can closely agree with human evaluations, highlighting the potential of these metrics for use in LLM-driven automated evaluation pipelines. We also publish the prompts and datasets for these experiments, providing valuable resources for further research and development.

</details>


### [74] [Culture is Not Trivia: Sociocultural Theory for Cultural NLP](https://arxiv.org/abs/2502.12057)

*Naitian Zhou, David Bamman, Isaac L. Bleaman*

**Main category:** cs.CL

**Keywords:** cultural NLP, sociocultural linguistics, localization, cultural competence, language technologies

**Relevance Score:** 4

**TL;DR:** This paper discusses limitations in cultural NLP caused by inadequate cultural representations and proposes using sociocultural linguistics to enhance methodological frameworks and localization in NLP.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective and safe language technologies for diverse cultures is not being met due to the absence of a shared conception of culture in NLP research.

**Method:** The paper uses a case study to demonstrate how sociocultural linguistics can clarify methodological constraints in cultural NLP.

**Key Contributions:**

	1. Identification of methodological limitations in cultural NLP
	2. Proposal of localization as a more effective approach
	3. Introduction of sociocultural linguistics to enhance cultural understanding

**Result:** The case study reveals that existing cultural proxies in NLP research are insufficient and suggests localization as an alternative approach.

**Limitations:** The paper focuses on theoretical implications and does not provide specific implementation strategies for NLP systems.

**Conclusion:** Improving cultural competence in NLP requires theoretical guidance from sociocultural linguistics and re-framing goals towards localization.

**Abstract:** The field of cultural NLP has recently experienced rapid growth, driven by a pressing need to ensure that language technologies are effective and safe across a pluralistic user base. This work has largely progressed without a shared conception of culture, instead choosing to rely on a wide array of cultural proxies. However, this leads to a number of recurring limitations: coarse national boundaries fail to capture nuanced differences that lay within them, limited coverage restricts datasets to only a subset of usually highly-represented cultures, and a lack of dynamicity results in static cultural benchmarks that do not change as culture evolves. In this position paper, we argue that these methodological limitations are symptomatic of a theoretical gap. We draw on a well-developed theory of culture from sociocultural linguistics to fill this gap by 1) demonstrating in a case study how it can clarify methodological constraints and affordances, 2) offering theoretically-motivated paths forward to achieving cultural competence, and 3) arguing that localization is a more useful framing for the goals of much current work in cultural NLP.

</details>


### [75] [When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models](https://arxiv.org/abs/2502.13246)

*Julia Mendelsohn, Ceren Budak*

**Main category:** cs.CL

**Keywords:** metaphor, political discourse, social media, immigration, computational linguistics

**Relevance Score:** 5

**TL;DR:** This paper develops a computational method to analyze metaphorical language in immigration discourse on social media, revealing differences in usage between political ideologies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of metaphor in political discourse, particularly in immigration discussions on social media.

**Method:** The authors identify seven metaphorical concepts related to immigration and propose a new technique combining word-level and document-level signals to measure these metaphors. They analyze 400K US tweets for this study.

**Key Contributions:**

	1. Development of a novel technique to measure metaphorical language
	2. Identification of metaphor usage trends across political ideologies
	3. Insights into user engagement related to metaphorical language in tweets

**Result:** The study finds that conservative users employ more dehumanizing metaphors than liberals, with notable variation across different concepts. Additionally, creature-related metaphors drive more retweets, especially among liberal authors.

**Limitations:** The study is limited to immigration discourse on social media and may not generalize to other topics or platforms.

**Conclusion:** The research demonstrates that computational methods can effectively complement qualitative approaches in understanding implicit language in political discourse.

**Abstract:** Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g. "water" or "vermin"). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.

</details>


### [76] [Is That Your Final Answer? Test-Time Scaling Improves Selective Question Answering](https://arxiv.org/abs/2502.13962)

*William Jurayj, Jeffrey Cheng, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** large language models, confidence scoring, response risk

**Relevance Score:** 8

**TL;DR:** This paper addresses the limitations of test-time scaling in language models by incorporating confidence scores for model responses, thus allowing for thresholding and evaluation based on response risk.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of reasoning systems by considering the confidence in model responses and the appropriateness of answering every question provided.

**Method:** The authors extract confidence scores during reasoning and investigate how increasing the compute budget at inference time affects both the correctness of model answers and the confidence in those answers.

**Key Contributions:**

	1. Incorporation of confidence scoring during reasoning
	2. Evaluation approach considering non-zero response risk
	3. Demonstration of improved accuracy and confidence with increased compute budget

**Result:** The study finds that a higher compute budget not only leads to more correct answers but also increases confidence in those answers.

**Limitations:** 

**Conclusion:** The paper introduces a new approach for evaluating model responses that accounts for varying levels of response risk rather than assuming zero-risk responses, and establishes guidelines for reporting such evaluations.

**Abstract:** Scaling the test-time compute of large language models has demonstrated impressive performance on reasoning benchmarks. However, existing evaluations of test-time scaling make the strong assumption that a reasoning system should always give an answer to any question provided. This overlooks concerns about whether a model is confident in its answer, and whether it is appropriate to always provide a response. To address these concerns, we extract confidence scores during reasoning for thresholding model responses. We find that increasing compute budget at inference time not only helps models answer more questions correctly, but also increases confidence in correct responses. We then extend the current paradigm of zero-risk responses during evaluation by considering settings with non-zero levels of response risk, and suggest a recipe for reporting evaluations under these settings.

</details>


### [77] [MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs](https://arxiv.org/abs/2504.02768)

*Jaap Jumelet, Leonie Weissweiler, Joakim Nivre, Arianna Bisazza*

**Main category:** cs.CL

**Keywords:** multi-language evaluation, linguistic minimal pairs, LLM performance

**Relevance Score:** 5

**TL;DR:** MultiBLiMP 1.0 is a multilingual benchmark for evaluating linguistic minimal pairs across 101 languages, utilizing automated methods and large linguistic resources.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the abilities of large language models (LLMs) on subject-verb agreement in a multilingual context and address shortcomings in low-resource language modeling.

**Method:** A fully automated pipeline is used to create minimal pairs from the large-scale resources of Universal Dependencies and UniMorph, resulting in over 128,000 pairs for testing.

**Key Contributions:**

	1. Creation of a multilingual benchmark covering 101 languages
	2. Inclusion of more than 128,000 minimal pairs for rigorous evaluation
	3. Automated pipeline utilizing Universal Dependencies and UniMorph resources

**Result:** The benchmark reveals significant shortcomings in the current state-of-the-art models when applied to low-resource languages, despite covering a wide range of languages.

**Limitations:** 

**Conclusion:** MultiBLiMP 1.0 offers a comprehensive tool for analyzing LLM performance and highlighting areas needing improvement for low-resource languages.

**Abstract:** We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic minimal pairs, covering 101 languages and 2 types of subject-verb agreement, containing more than 128,000 minimal pairs. Our minimal pairs are created using a fully automated pipeline, leveraging the large-scale linguistic resources of Universal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs at an unprecedented multilingual scale, and highlights the shortcomings of the current state-of-the-art in modelling low-resource languages

</details>


### [78] [ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data](https://arxiv.org/abs/2504.14452)

*Tong Chen, Faeze Brahman, Jiacheng Liu, Niloofar Mireshghallah, Weijia Shi, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi*

**Main category:** cs.CL

**Keywords:** Paraphrase Preference Optimization, Language Models, Regurgitation, Fine-tuning, Machine Learning

**Relevance Score:** 8

**TL;DR:** ParaPO is a method that fine-tunes language models to reduce unintentional repetitions of their pretraining data while maintaining utility and famous quotations recall.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns regarding copyright, plagiarism, privacy, and creativity due to language models' ability to memorize and reproduce segments of their training data.

**Method:** ParaPO fine-tunes language models to prefer paraphrased versions of memorized content, using system prompts to manage regurgitation behavior when needed.

**Key Contributions:**

	1. Introduction of the Paraphrase Preference Optimization (ParaPO) method
	2. Demonstrated significant reduction in unintentional regurgitation across multiple datasets
	3. Maintained the recall capability for famous quotations using system prompts

**Result:** ParaPO reduces the regurgitation metric significantly across different datasets, improving performance from 17.3 to 12.9 in creative writing tasks, and preserves quotation recall effectively.

**Limitations:** 

**Conclusion:** ParaPO outperforms previous unlearning methods by consistently reducing regurgitation in both Llama3.1-8B and Tulu3-8B models while maintaining the ability to recall important phrases when necessary.

**Abstract:** Language models (LMs) can memorize and reproduce segments from their pretraining data verbatim even in non-adversarial settings, raising concerns about copyright, plagiarism, privacy, and creativity. We introduce Paraphrase Preference Optimization (ParaPO), a post-training method that fine-tunes LMs to reduce unintentional regurgitation while preserving their overall utility. ParaPO trains LMs to prefer paraphrased versions of memorized segments over the original verbatim content from the pretraining data. To maintain the ability to recall famous quotations when appropriate, we develop a variant of ParaPO that uses system prompts to control regurgitation behavior. In our evaluation on Llama3.1-8B, ParaPO consistently reduces regurgitation across all tested datasets (e.g., reducing the regurgitation metric from 17.3 to 12.9 in creative writing), whereas unlearning methods used in prior work to mitigate regurgitation are less effective outside their targeted unlearned domain (from 17.3 to 16.9). When applied to the instruction-tuned Tulu3-8B model, ParaPO with system prompting successfully preserves famous quotation recall while reducing unintentional regurgitation (from 8.7 to 6.3 in creative writing) when prompted not to regurgitate. In contrast, without ParaPO tuning, prompting the model not to regurgitate produces only a marginal reduction (8.7 to 8.4).

</details>
