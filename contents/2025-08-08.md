# 2025-08-08

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 26]

- [cs.CL](#cs.CL) [Total: 66]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [AI Should Be More Human, Not More Complex](https://arxiv.org/abs/2508.04713)

*Carlo Esposito*

**Main category:** cs.HC

**Keywords:** Large Language Models, User engagement, AI communication

**Relevance Score:** 9

**TL;DR:** LLMs in search applications often produce verbose responses that users find unsatisfactory. Users prefer concise and sourced answers, revealing issues in current AI communication trends.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate user preferences in AI-powered search responses and the impact of response complexity on user satisfaction.

**Method:** Conducted a comprehensive study with approximately 10,000 participants comparing responses from five major AI-powered search systems.

**Key Contributions:**

	1. Demonstrated user preference for concise responses
	2. Identified the impact of verbose AI responses on user trust
	3. Challenged existing paradigms of AI response complexity

**Result:** Users overwhelmingly preferred concise, source-attributed responses to verbose explanations, indicating a misalignment between AI development and user needs.

**Limitations:** 

**Conclusion:** Optimal AI communication should prioritize brevity and transparency, as more complex responses do not equate to better performance.

**Abstract:** Large Language Models (LLMs) in search applications increasingly prioritize verbose, lexically complex responses that paradoxically reduce user satisfaction and engagement. Through a comprehensive study of 10.000 (est.) participants comparing responses from five major AI-powered search systems, we demonstrate that users overwhelmingly prefer concise, source-attributed responses over elaborate explanations. Our analysis reveals that current AI development trends toward "artificial sophistication" create an uncanny valley effect where systems sound knowledgeable but lack genuine critical thinking, leading to reduced trust and increased cognitive load. We present evidence that optimal AI communication mirrors effective human discourse: direct, properly sourced, and honest about limitations. Our findings challenge the prevailing assumption that more complex AI responses indicate better performance, instead suggesting that human-like brevity and transparency are key to user engagement and system reliability.

</details>


### [2] [Evaluating the Impact of LLM-guided Reflection on Learning Outcomes with Interactive AI-Generated Educational Podcasts](https://arxiv.org/abs/2508.04787)

*Vishnu Menon, Andy Cherney, Elizabeth B. Cloude, Li Zhang, Tiffany D. Do*

**Main category:** cs.HC

**Keywords:** LLM, reflection prompts, user experience, interactive podcasts, learning outcomes

**Relevance Score:** 7

**TL;DR:** The study investigates the impact of LLM-guided reflection prompts in AI-generated podcasts on learning and user experience.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how interactive AI elements can enhance educational outcomes and user engagement in podcast formats.

**Method:** An experimental design with 36 undergraduates comparing a podcast with LLM-guided reflection prompts to one without.

**Key Contributions:**

	1. Examined the role of LLM-guided reflection prompts in education
	2. Provided insights on user experience in podacasts
	3. Indentified the need for refining reflective interactivity design

**Result:** Learning outcomes were similar, but reflection prompts negatively affected perceived attractiveness of the podcast.

**Limitations:** The study has a small sample size and limited scope of application.

**Conclusion:** Further research is needed to improve reflective interactivity design to maintain user engagement.

**Abstract:** This study examined whether embedding LLM-guided reflection prompts in an interactive AI-generated podcast improved learning and user experience compared to a version without prompts. Thirty-six undergraduates participated, and while learning outcomes were similar across conditions, reflection prompts reduced perceived attractiveness, highlighting a call for more research on reflective interactivity design.

</details>


### [3] [At a Glance to Your Fingertips: Enabling Direct Manipulation of Distant Objects Through SightWarp](https://arxiv.org/abs/2508.04821)

*Yang Liu, Thorbjørn Mikkelsen, Zehai Liu, Gengchen Tian, Diako Mardanbegi, Qiushi Zhou, Hans Gellersen, Ken Pfeuffer*

**Main category:** cs.HC

**Keywords:** 3D user interfaces, interaction technique, eye-hand coordination

**Relevance Score:** 8

**TL;DR:** Introducing SightWarp, an interaction technique enabling direct manipulation of distant objects in 3D interfaces by using eye-hand coordination to summon object proxies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a solution for manipulating distant objects in 3D UIs, maintaining the inherent benefits of direct gestures.

**Method:** SightWarp creates a scaled near-space proxy of a distant object and its context when users shift their gaze or hand, allowing for immediate object interaction.

**Key Contributions:**

	1. Introduction of the SightWarp interaction technique
	2. Demonstration of improved performance in 3D tasks
	3. Application examples for diverse manipulation scenarios

**Result:** User studies indicate that SightWarp improves performance in 3D object docking tasks compared to traditional gaze and pinch techniques.

**Limitations:** 

**Conclusion:** SightWarp enhances usability in 3D environments by allowing natural interaction with distant objects, thus supporting more expressive object manipulation.

**Abstract:** In 3D user interfaces, reaching out to grab and manipulate something works great until it is out of reach. Indirect techniques like gaze and pinch offer an alternative for distant interaction, but do not provide the same immediacy or proprioceptive feedback as direct gestures. To support direct gestures for faraway objects, we introduce SightWarp, an interaction technique that exploits eye-hand coordination to seamlessly summon object proxies to the user's fingertips. The idea is that after looking at a distant object, users either shift their gaze to the hand or move their hand into view-triggering the creation of a scaled near-space proxy of the object and its surrounding context. The proxy remains active until the eye-hand pattern is released. The key benefit is that users always have an option to immediately operate on the distant object through a natural, direct hand gesture. Through a user study of a 3D object docking task, we show that users can easily employ SightWarp, and that subsequent direct manipulation improves performance over gaze and pinch. Application examples illustrate its utility for 6DOF manipulation, overview-and-detail navigation, and world-in-miniature interaction. Our work contributes to expressive and flexible object interactions across near and far spaces.

</details>


### [4] [Charts-of-Thought: Enhancing LLM Visualization Literacy Through Structured Data Extraction](https://arxiv.org/abs/2508.04842)

*Amit Kumar Das, Mohammad Tarun, Klaus Mueller*

**Main category:** cs.HC

**Keywords:** Large Language Models, Visualization Literacy, Charts-of-Thought, Human-Computer Interaction, Accessibility

**Relevance Score:** 9

**TL;DR:** The paper evaluates Large Language Models' (LLMs) visualization literacy using a novel prompting technique called Charts-of-Thought and establishes that these models can exceed human performance in visualization tasks with structured prompting.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To assess and improve the visualization literacy of modern LLMs, providing a benchmark and a method for better visual interpretation.

**Method:** Three LLMs (Claude-3.7-sonnet, GPT-4.5 preview, and Gemini-2.0-pro) were tested using the Visualization Literacy Assessment Test (VLAT) with standard prompts and a structured method called Charts-of-Thought.

**Key Contributions:**

	1. Introduction of the Charts-of-Thought prompting technique
	2. Demonstration of LLMs surpassing human visualization literacy benchmarks
	3. Implications for improving accessibility of visualizations for individuals with lower literacy

**Result:** Claude-3.7-sonnet scored 50.17—well above the human baseline of 28.82—with significant score increases across models using the Charts-of-Thought method.

**Limitations:** 

**Conclusion:** Modern multimodal LLMs, when guided by a systematic analytical framework, can outperform humans in visualization literacy tasks, establishing a new standard for evaluating and enhancing LLM capabilities.

**Abstract:** This paper evaluates the visualization literacy of modern Large Language Models (LLMs) and introduces a novel prompting technique called Charts-of-Thought. We tested three state-of-the-art LLMs (Claude-3.7-sonnet, GPT-4.5 preview, and Gemini-2.0-pro) on the Visualization Literacy Assessment Test (VLAT) using standard prompts and our structured approach. The Charts-of-Thought method guides LLMs through a systematic data extraction, verification, and analysis process before answering visualization questions. Our results show Claude-3.7-sonnet achieved a score of 50.17 using this method, far exceeding the human baseline of 28.82. This approach improved performance across all models, with score increases of 21.8% for GPT-4.5, 9.4% for Gemini-2.0, and 13.5% for Claude-3.7 compared to standard prompting. The performance gains were consistent across original and modified VLAT charts, with Claude correctly answering 100% of questions for several chart types that previously challenged LLMs. Our study reveals that modern multimodal LLMs can surpass human performance on visualization literacy tasks when given the proper analytical framework. These findings establish a new benchmark for LLM visualization literacy and demonstrate the importance of structured prompting strategies for complex visual interpretation tasks. Beyond improving LLM visualization literacy, Charts-of-Thought could also enhance the accessibility of visualizations, potentially benefiting individuals with visual impairments or lower visualization literacy.

</details>


### [5] [An Implementation of a Visual Stepper in the GRASP Programming System](https://arxiv.org/abs/2508.04859)

*Panicz Maciej Godek*

**Main category:** cs.HC

**Keywords:** GRASP, visual evaluator, Scheme, programming system, extension mechanisms

**Relevance Score:** 2

**TL;DR:** This paper discusses the implementation of the visual evaluator extension in the GRASP programming system and provides a design tutorial around GRASP's architecture and extension mechanisms.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To present the implementation of the visual evaluator in GRASP and to offer insights into the design and architecture of its extension mechanism, addressing unresolved challenges in system capabilities.

**Method:** The paper outlines the design and architectural considerations of GRASP, detailing how the visual evaluator extension is integrated into the system.

**Key Contributions:**

	1. Implementation details of the visual evaluator extension in GRASP
	2. Tutorial on GRASP's architecture and extension mechanisms
	3. Identification of key problems for building similar systems

**Result:** The details of the visual evaluator's implementation and the design principles of GRASP are shared, highlighting the ongoing development and the type of problems faced in achieving desired system capabilities.

**Limitations:** Details may change before final release; the system is not complete as of the writing.

**Conclusion:** While the implementation details are not final, the identified problems in building a system like GRASP are relevant to the Scheme community and may influence future developments.

**Abstract:** The direct purpose of this paper - as its title suggests - is to present how the visual evaluator extension is implemented in the GRASP programming system. The indirect purpose is to provide a tutorial around the design of GRASP, and in particular - around the architecture of its extension mechanism. Neither GRASP nor its extension mechanisms are, at the moment of writing this paper, final or complete, and we are certain that some details of the solutions described in here will change even before the first release. What will not change, though, is the set of problems that need to be solved in order to build a system with capabilities similar to those of GRASP. We believe that these problems might be of interest to the Scheme community.

</details>


### [6] [Learning AI Auditing: A Case Study of Teenagers Auditing a Generative AI Model](https://arxiv.org/abs/2508.04902)

*Luis Morales-Navarro, Michelle Gan, Evelyn Yu, Lauren Vogelstein, Yasmin B. Kafai, Danaé Metaxa*

**Main category:** cs.HC

**Keywords:** Algorithm Auditing, AI Literacy, Youth Engagement, Biases in AI, Participatory Design

**Relevance Score:** 8

**TL;DR:** This study explores how high school students engage in algorithm auditing to identify biases in AI tools, particularly through a workshop where they audited TikTok's generative AI model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the need for equipping youth with AI literacy skills and understanding the social impacts of AI/ML technologies they encounter in their daily lives.

**Method:** Conducted a two-week participatory design workshop with 14 teenagers, guiding them to audit a generative AI tool used for creating TikTok filters.

**Key Contributions:**

	1. Demonstrates non-expert participation in AI auditing
	2. Highlights unique perspectives on biases from youth
	3. Shows the potential of workshops to enhance AI literacy

**Result:** Participants were engaged and creative, uncovering unique biases such as age-related issues, and their findings aligned with professional audits in some aspects.

**Limitations:** 

**Conclusion:** The research emphasizes the importance of algorithm auditing in fostering AI literacy among teenagers and encouraging them to critically analyze AI systems.

**Abstract:** This study investigates how high school-aged youth engage in algorithm auditing to identify and understand biases in artificial intelligence and machine learning (AI/ML) tools they encounter daily. With AI/ML technologies being increasingly integrated into young people's lives, there is an urgent need to equip teenagers with AI literacies that build both technical knowledge and awareness of social impacts. Algorithm audits (also called AI audits) have traditionally been employed by experts to assess potential harmful biases, but recent research suggests that non-expert users can also participate productively in auditing. We conducted a two-week participatory design workshop with 14 teenagers (ages 14-15), where they audited the generative AI model behind TikTok's Effect House, a tool for creating interactive TikTok filters. We present a case study describing how teenagers approached the audit, from deciding what to audit to analyzing data using diverse strategies and communicating their results. Our findings show that participants were engaged and creative throughout the activities, independently raising and exploring new considerations, such as age-related biases, that are uncommon in professional audits. We drew on our expertise in algorithm auditing to triangulate their findings as a way to examine if the workshop supported participants to reach coherent conclusions in their audit. Although the resulting number of changes in race, gender, and age representation uncovered by the teens were slightly different from ours, we reached similar conclusions. This study highlights the potential for auditing to inspire learning activities to foster AI literacies, empower teenagers to critically examine AI systems, and contribute fresh perspectives to the study of algorithmic harms.

</details>


### [7] [Root Cause Analysis Training for Healthcare Professionals With AI-Powered Virtual Simulation: A Proof-of-Concept](https://arxiv.org/abs/2508.04904)

*Yuqi Hu, Qiwen Xiong, Zhenzhen Qin, Brandon Watanabe, Yujing Wang, Mirjana Prpa, Ilmi Yoon*

**Main category:** cs.HC

**Keywords:** Root Cause Analysis, 3D simulation, healthcare training, large language models, immersive learning

**Relevance Score:** 9

**TL;DR:** An AI-based 3D simulation game is developed to enhance Root Cause Analysis (RCA) training for healthcare professionals, providing an interactive and immersive learning experience.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing RCA training programs have high resource demands, leading to inadequate training and inconsistent implementation in healthcare settings.

**Method:** The paper presents a prototype of a 3D simulation game that allows users to conduct RCA investigations in a simulated ICU environment by interacting with virtual avatars using LLMs and other AI technologies.

**Key Contributions:**

	1. Development of an AI-powered 3D simulation game for RCA training
	2. Integration of large language models for realistic interactions
	3. Cost-effective and scalable alternative to traditional RCA training

**Result:** The prototype facilitates life-like interactions and supports formative feedback, improving the learning process for healthcare professionals.

**Limitations:** 

**Conclusion:** Plans are discussed for empirical evaluation of the efficacy of the AI-powered simulation training system.

**Abstract:** Root Cause Analysis (RCA) is a critical tool for investigating adverse events in healthcare and improving patient safety. However, existing RCA training programs are often limited by high resource demands, leading to insufficient training and inconsistent implementation. To address this challenge, we present an AI-powered 3D simulation game that helps healthcare professionals develop RCA skills through interactive, immersive simulations. This approach offers a cost-effective, scalable, and accessible alternative to traditional training. The prototype simulates an RCA investigation following a death in the ICU, where learners interview five virtual avatars representing ICU team members to investigate the incident and complete a written report. The system enables natural, life-like interactions with avatars via large language models (LLMs), emotional text-to-speech, and AI-powered animations. An additional LLM component provides formative and summative feedback to support continual improvement. We conclude by outlining plans to empirically evaluate the system's efficacy.

</details>


### [8] [Toward Supporting Narrative-Driven Data Exploration: Barriers and Design Opportunities](https://arxiv.org/abs/2508.04920)

*Oliver Huang, Carolina Nobre*

**Main category:** cs.HC

**Keywords:** Narrative-driven exploration, Data analysis, Design opportunities

**Relevance Score:** 7

**TL;DR:** This paper explores challenges in narrative-driven data exploration and proposes design opportunities to improve support for such analyses.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this study stems from the need to enhance narrative-driven inquiries in data exploration, which are becoming increasingly common among analysts.

**Method:** The research involved a formative study with 48 participants to identify barriers in narrative-driven exploration.

**Key Contributions:**

	1. Identification of key barriers in narrative-driven data exploration
	2. Insights into maintaining context and tracing reasoning in data analysis
	3. Proposed design opportunities for improving narrative analysis tools

**Result:** Key barriers identified include difficulties in maintaining context across views, tracing reasoning paths, and externalizing evolving interpretations.

**Limitations:** 

**Conclusion:** The findings highlight design opportunities to better support narrative-driven analysis.

**Abstract:** Analysts increasingly explore data through evolving, narrative-driven inquiries, moving beyond static dashboards and predefined metrics as their questions deepen and shift. As these explorations progress, insights often become dispersed across views, making it challenging to maintain context or clarify how conclusions arise. Through a formative study with 48 participants, we identify key barriers that hinder narrative-driven exploration, including difficulty maintaining context across views, tracing reasoning paths, and externalizing evolving interpretations. Our findings surface design opportunities to support narrative-driven analysis better.

</details>


### [9] [Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge](https://arxiv.org/abs/2508.04995)

*Matthew Kelly*

**Main category:** cs.HC

**Keywords:** Large Language Models, knowledge production, AI governance, epistemology, infrastructure studies

**Relevance Score:** 8

**TL;DR:** The paper introduces the Situated Epistemic Infrastructures (SEI) framework to analyze how knowledge is validated in hybrid human-machine systems, focusing on coordination over classification in knowledge production.

**Read time:** 27 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragility of current knowledge infrastructures exposed by Large Language Models and to propose a framework for understanding how authority is established in hybrid systems.

**Method:** The Situated Epistemic Infrastructures (SEI) framework is presented as a diagnostic tool that integrates insights from various fields to analyze the mediation of credibility in knowledge production.

**Key Contributions:**

	1. Introduction of the SEI framework for hybrid human-machine systems.
	2. Focus on coordination over classification in knowledge validation.
	3. Contribution to AI governance and ethical design discussions.

**Result:** The SEI framework emphasizes the importance of coordination in knowledge validation rather than strict classification, which can enhance our understanding of AI governance and the design of information systems.

**Limitations:** 

**Conclusion:** The framework offers a novel approach to understanding the complexities of epistemic stewardship in the age of AI, positioning itself as a robust alternative to traditional models of scholarly communication.

**Abstract:** Large Language Models (LLMs) such as ChatGPT have rendered visible the fragility of contemporary knowledge infrastructures by simulating coherence while bypassing traditional modes of citation, authority, and validation. This paper introduces the Situated Epistemic Infrastructures (SEI) framework as a diagnostic tool for analyzing how knowledge becomes authoritative across hybrid human-machine systems under post-coherence conditions. Rather than relying on stable scholarly domains or bounded communities of practice, SEI traces how credibility is mediated across institutional, computational, and temporal arrangements. Integrating insights from infrastructure studies, platform theory, and epistemology, the framework foregrounds coordination over classification, emphasizing the need for anticipatory and adaptive models of epistemic stewardship. The paper contributes to debates on AI governance, knowledge production, and the ethical design of information systems by offering a robust alternative to representationalist models of scholarly communication.

</details>


### [10] [Human-AI Schema Discovery and Application for Creative Problem Solving](https://arxiv.org/abs/2508.05045)

*Sitong Wang*

**Main category:** cs.HC

**Keywords:** human-AI collaboration, schema discovery, creative problem solving

**Relevance Score:** 7

**TL;DR:** The research develops a framework for human-AI schema discovery to enhance creative problem solving and collaboration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist users in recognizing and applying schemas for creative tasks, improving the accessibility of implicit knowledge in complex domains.

**Method:** Designing systems that facilitate sensemaking of examples and integrating schemas into workflows for human-AI co-creation.

**Key Contributions:**

	1. Framework for human-AI schema discovery
	2. Support for sensemaking over examples
	3. Operationalization of schemas in co-creative workflows

**Result:** The developed framework supports users in discovering and operationalizing schemas, potentially leading to more effective human-AI collaborations.

**Limitations:** 

**Conclusion:** Schema-guided interaction can enhance creativity and collaboration between humans and AI by making implicit knowledge more accessible.

**Abstract:** Humans often rely on underlying structural patterns-schemas-to create, whether by writing stories, designing software, or composing music. Schemas help organize ideas and guide exploration, but they are often difficult to discover and apply, especially in complex or unfamiliar domains. My Ph.D. research develops a framework for human-AI schema discovery and application to support creative problem solving. I design systems that support users in sensemaking over examples to abstract schemas, and in operationalizing schemas into human-AI co-creative workflows for application. This research offers insights into how schema-guided interaction can make implicit knowledge more accessible and actionable, advancing more transparent and collaborative human-AI systems.

</details>


### [11] [Accessibility Beyond Accommodations: A Systematic Redesign of Introduction to Computer Science for Students with Visual Impairments](https://arxiv.org/abs/2508.05056)

*Vaanee Tripathi, Aalok Thakkar*

**Main category:** cs.HC

**Keywords:** accessibility, computer science education, visual impairments, inclusive education, universal design

**Relevance Score:** 6

**TL;DR:** The paper presents a framework for redesigning introductory computer science curricula to enhance accessibility for students with visual impairments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address systemic barriers preventing students with visual impairments from fully participating in computer science education.

**Method:** The framework comprises five components: accessible learning resources, in-class learning kits, structured support systems, an online tool repository, and psychosocial support.

**Key Contributions:**

	1. Comprehensive framework for curriculum redesign
	2. Integration of technical and pedagogical dimensions
	3. Grounded in universal design principles

**Result:** The framework offers a systematic approach to accessibility in education that does not rely on complex technical infrastructure.

**Limitations:** Limited to introductory computer science curricula; empirical evaluation of learning outcomes still required.

**Conclusion:** This comprehensive framework aims to integrate accessibility into curriculum design, emphasizing practical implementation for equitable learning experiences.

**Abstract:** Computer science education has evolved extensively; however, systemic barriers still prevent students with visual impairments from fully participating. While existing research has developed specialized programming tools and assistive technologies, these solutions remain fragmented and often require complex technical infrastructure, which limits their classroom implementation. Current approaches treat accessibility as individual accommodations rather than integral curriculum design, creating gaps in holistic educational support. This paper presents a comprehensive framework for redesigning introductory computer science curricula to provide equitable learning experiences for students with visual impairments without requiring specialized technical infrastructure. The framework outlines five key components that together contribute a systematic approach to curriculum accessibility: accessible learning resources with pre-distributed materials and tactile diagrams, in-class learning kits with hands-on demonstrations, structured support systems with dedicated teaching assistance, an online tool repository, and psychosocial support for classroom participation. Unlike existing tool-focused solutions, this framework addresses both technical and pedagogical dimensions of inclusive education while emphasizing practical implementation in standard university settings. The design is grounded in universal design principles and validated through expert consultation with accessibility specialists and disability services professionals, establishing foundations for future empirical evaluation of learning outcomes and student engagement while serving as a template for broader institutional adoption.

</details>


### [12] [A Desktop-Centric Design Space for Direct Object Examination and Visualization in Mixed-Reality Environments](https://arxiv.org/abs/2508.05088)

*Sam Johnson-Lacoss, Santiago V. Lombeyda, S. George Djorgovski*

**Main category:** cs.HC

**Keywords:** mixed reality, 3D data analysis, human-computer interaction, design framework, immersive technology

**Relevance Score:** 7

**TL;DR:** The paper discusses the impact of mixed reality (MR) on data analysis and communication in research and clinical settings, proposing a design framework for MR applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how MR technologies can enhance 3D data analysis and workspace ergonomics for clinicians and researchers currently limited by 2D interfaces.

**Method:** The authors create a design space breakdown, categorizing different interaction zones and modalities for MR technology application.

**Key Contributions:**

	1. Development of a design framework for MR applications
	2. Identification of interaction zones and modalities
	3. Enhancement of object-centric data analysis capabilities

**Result:** Mixed reality applications can significantly improve the understanding of complex 3D phenomena by providing immersive 3D representations of data within natural spaces.

**Limitations:** Potential challenges in ergonomic integration and user adaptation to MR environments.

**Conclusion:** The integration of MR technologies into research and clinical workflows can lead to better data analysis and communication through enhanced spatial contextualization.

**Abstract:** Mixed reality (MR) environments are bound to become ubiquitous as MR technology becomes lighter, higher resolution, more affordable, and overall becomes a seamless extension of our current work and living spaces. For research scientists and clinicians focused on understanding 3D phenomena or patient pathologies within the context of the larger human anatomy, that means a necessary evolution of their workstations currently only utilizing 2D interfaces for everyday communication, logistics and data analysis. MR technologies bring forth immersive 3D representations coexisting in our natural spaces, while allowing for richer interconnected information displays, where 3D representations greatly aid in the detailed understanding of physical structures, spatial relationships, and 3D contextualization of 2D measurements, projections, abstractions, and other data details. We present a breakdown of the different interaction zones and modalities into a design space that best accommodates the creation of applications for users engaged through MR technologies in precise object-centric data analysis within the ergonomic confines of their desktop physical spaces.

</details>


### [13] [SparseEMG: Computational Design of Sparse EMG Layouts for Sensing Gestures](https://arxiv.org/abs/2508.05098)

*Anand Kumar, Antony Albert Raj Irudayaraj, Ishita Chandra, Adwait Sharma, Aditya Shekhar Nittala*

**Main category:** cs.HC

**Keywords:** EMG, gesture recognition, electrode selection, machine learning, SparseEMG

**Relevance Score:** 8

**TL;DR:** This paper analyzes the impact of electrode selection on gesture recognition accuracy in EMG systems and introduces SparseEMG, a tool for generating efficient electrode layouts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overlooked impact of electrode selection on the accuracy of gesture recognition systems using EMG.

**Method:** The study evaluates 28 combinations of selection schemes and classifiers across six datasets to find an optimal balance between electrode count and classification accuracy.

**Key Contributions:**

	1. First data-driven analysis of electrode selection in EMG gesture recognition.
	2. Introduction of SparseEMG, a design tool for electrode layout generation.
	3. Demonstration of the transferability of generated layouts across users.

**Result:** The analysis reveals that using Permutation Importance with Random Forest can reduce the number of electrodes by 53.5% while maintaining accuracy, and SparseEMG generates effective layouts validated in real-world applications.

**Limitations:** The study may have limitations regarding the generalizability of results to all possible hardware setups.

**Conclusion:** SparseEMG effectively supports gesture recognition in various scenarios while minimizing the number of electrodes required for accurate performance.

**Abstract:** Gesture recognition with electromyography (EMG) is a complex problem influenced by gesture sets, electrode count and placement, and machine learning parameters (e.g., features, classifiers). Most existing toolkits focus on streamlining model development but overlook the impact of electrode selection on classification accuracy. In this work, we present the first data-driven analysis of how electrode selection and classifier choice affect both accuracy and sparsity. Through a systematic evaluation of 28 combinations (4 selection schemes, 7 classifiers), across six datasets, we identify an approach that minimizes electrode count without compromising accuracy. The results show that Permutation Importance (selection scheme) with Random Forest (classifier) reduces the number of electrodes by 53.5\%. Based on these findings, we introduce SparseEMG, a design tool that generates sparse electrode layouts based on user-selected gesture sets, electrode constraints, and ML parameters while also predicting classification performance. SparseEMG supports 50+ unique gestures and is validated in three real-world applications using different hardware setups. Results from our multi-dataset evaluation show that the layouts generated from the SparseEMG design tool are transferable across users with only minimal variation in gesture recognition performance.

</details>


### [14] [Metacognition and self-regulated learning in manipulative robotic problem-solving task](https://arxiv.org/abs/2508.05112)

*Margarida Romero, George Kalmpourtzis*

**Main category:** cs.HC

**Keywords:** metacognition, creative problem solving, educational robots, exploration, knowledge regulation

**Relevance Score:** 4

**TL;DR:** This chapter explores the role of metacognition in creative problem solving (CPS) with a focus on educational robots, highlighting a case study involving robot cubes and the regulation of exploration and exploitation of knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The analysis of meta-reasoning in CPS activities is crucial for understanding how learners monitor and regulate their problem-solving processes.

**Method:** The chapter examines the CPS process through a case study where participants engage with robot cubes to develop both technological knowledge and conceptual understanding of system-level behavior.

**Key Contributions:**

	1. Analysis of metacognitive processes in CPS with educational robots
	2. Case study illustrating interactions with robot cubes
	3. Identification of knowledge emergence through metacognitive regulation

**Result:** Findings reveal how metacognitive regulation influences the exploration and exploitation of knowledge, guiding learners toward solutions in CPS tasks.

**Limitations:** 

**Conclusion:** Metacognitive regulation plays a vital role in enhancing learners' problem-solving capabilities, particularly in ill-defined problem contexts.

**Abstract:** Metacognition is an important aspect in creative problem solving (CPS) and through this chapter we analyse the meta-reasoning aspects applied in the different processes of monitoring the progress of learners' reasoning and CPS activities. Meta-reasoning monitors the way that problem-solving processes advance and regulate time and efforts towards a solution. In the context of an ill-defined problem, exploration is required to develop a better-defined problem space and advance towards the solution space. The way learners engage in exploration and exploitations is regulated by the meta-reasoning within the CPS activity. The objective of this chapter is to examine and identify the CPS process with educational robots through a metacognitive and interactionist approach. This chapter presents a case study, where, to solve a problem, a participant had to explore a set of robot cubes to develop the technological knowledge associated with each single component of the system, but also conceptualize a system-level behaviour of the cubes when they are assembled. The chapter presents the emergence of knowledge through the metacognitive regulation of the process of exploration and exploitation of prior knowledge and emergent knowledge until finding a solution

</details>


### [15] [AI Conversational Tutors in Foreign Language Learning: A Mixed-Methods Evaluation Study](https://arxiv.org/abs/2508.05156)

*Nikolaos Avouris*

**Main category:** cs.HC

**Keywords:** AI tutors, language learning, user experience, natural language understanding, data privacy

**Relevance Score:** 4

**TL;DR:** The paper evaluates state-of-the-art AI tutors for foreign language learning through a mixed-method empirical study, focusing on user experience and conversation functionality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language skills through AI tutors and establish quality assessment criteria for future tools.

**Method:** Mixed-method empirical study evaluating state-of-the-art AI tutors with an emphasis on conversation functionality and analysis of chat transcripts.

**Key Contributions:**

	1. User experience evaluation of AI tutors for language learning
	2. Criteria for assessing the quality of AI tutoring systems
	3. Insights into conversation functionality and data privacy concerns

**Result:** Findings include user experience insights and quality assessment criteria for AI tutors based on their conversation capabilities.

**Limitations:** 

**Conclusion:** The study contributes to understanding the effectiveness of AI tutors in language learning and offers guidelines for future tool design, particularly regarding privacy.

**Abstract:** This paper focuses on AI tutors in foreign language learning, a field of application of AI tutors with great development, especially during the last years, when great advances in natural language understanding and processing in real time, have been achieved. These tutors attempt to address needs for improving language skills (speaking, or communicative competence, understanding). In this paper, a mixed-methos empirical study on the use of different kinds of state-of-the-art AI tutors for language learning is reported. This study involves a user experience evaluation of typical such tools, with special focus in their conversation functionality and an evaluation of their quality, based on chat transcripts. This study can help establish criteria for assessing the quality of such systems and inform the design of future tools, including concerns about data privacy and secure handling of learner information.

</details>


### [16] [CWEFS: Brain volume conduction effects inspired channel-wise EEG feature selection for multi-dimensional emotion recognition](https://arxiv.org/abs/2508.05228)

*Xueyuan Xu, Wenjia Dong, Fulin Wei, Li Zhuo*

**Main category:** cs.HC

**Keywords:** EEG, emotion recognition, feature selection, latent structure, multi-dimensional

**Relevance Score:** 7

**TL;DR:** The paper presents a novel channel-wise EEG feature selection method to improve emotion recognition from EEG signals by addressing redundancy and relevance issues in high-dimensional data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing EEG feature selection techniques do not consider the influence of latent EEG feature structures on emotional correlations, leading to suboptimal models for emotion recognition.

**Method:** The proposed CWEFS method utilizes a shared latent structure model that integrates EEG emotional feature selection and latent semantic analysis to determine the significance of various EEG channels.

**Key Contributions:**

	1. Introduction of CWEFS for emotion recognition in EEG data
	2. Integration of latent semantic analysis in feature selection
	3. Adaptive learning of channel significance

**Result:** Experimental validation on three EEG datasets showed that CWEFS outperforms nineteen existing feature selection methods, achieving optimal emotion recognition across six metrics.

**Limitations:** 

**Conclusion:** CWEFS enhances emotion recognition by effectively selecting relevant EEG features while considering the latent structures of emotional labels and channel importance.

**Abstract:** Due to the intracranial volume conduction effects, high-dimensional multi-channel electroencephalography (EEG) features often contain substantial redundant and irrelevant information. This issue not only hinders the extraction of discriminative emotional representations but also compromises the real-time performance. Feature selection has been established as an effective approach to address the challenges while enhancing the transparency and interpretability of emotion recognition models. However, existing EEG feature selection research overlooks the influence of latent EEG feature structures on emotional label correlations and assumes uniform importance across various channels, directly limiting the precise construction of EEG feature selection models for multi-dimensional affective computing. To address these limitations, a novel channel-wise EEG feature selection (CWEFS) method is proposed for multi-dimensional emotion recognition. Specifically, inspired by brain volume conduction effects, CWEFS integrates EEG emotional feature selection into a shared latent structure model designed to construct a consensus latent space across diverse EEG channels. To preserve the local geometric structure, this consensus space is further integrated with the latent semantic analysis of multi-dimensional emotional labels. Additionally, CWEFS incorporates adaptive channel-weight learning to automatically determine the significance of different EEG channels in the emotional feature selection task. The effectiveness of CWEFS was validated using three popular EEG datasets with multi-dimensional emotional labels. Comprehensive experimental results, compared against nineteen feature selection methods, demonstrate that the EEG feature subsets chosen by CWEFS achieve optimal emotion recognition performance across six evaluation metrics.

</details>


### [17] [ADSEL: Adaptive dual self-expression learning for EEG feature selection via incomplete multi-dimensional emotional tagging](https://arxiv.org/abs/2508.05229)

*Tianze Yu, Junming Zhang, Wenjia Dong, Xueyuan Xu, Li Zhuo*

**Main category:** cs.HC

**Keywords:** EEG, emotion recognition, feature selection, machine learning, label recovery

**Relevance Score:** 6

**TL;DR:** A novel algorithm for EEG-based emotion recognition that addresses incomplete label data using adaptive dual self-expression learning for improved feature selection.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenges of high dimensionality in EEG features and limited sample sizes that lead to classifier overfitting and complexity in emotion recognition.

**Method:** The proposed method integrates an adaptive dual self-expression learning (ADSEL) with least squares regression to improve feature selection under incomplete multi-dimensional emotion labels.

**Key Contributions:**

	1. Proposes a novel incomplete multi-dimensional feature selection algorithm for EEG emotion recognition.
	2. Integrates ADSEL with least squares regression to improve label recovery and feature selection.
	3. Addresses the neglect of correlations between samples and dimensions in feature selection methods.

**Result:** The ADSEL method enhances label recovery accuracy and effectively identifies the optimal EEG feature subset for emotion recognition tasks.

**Limitations:** 

**Conclusion:** The integration of sample-level and dimension-level self-expression learning improves the exploitation of information necessary for effective emotion label reconstruction.

**Abstract:** EEG based multi-dimension emotion recognition has attracted substantial research interest in human computer interfaces. However, the high dimensionality of EEG features, coupled with limited sample sizes, frequently leads to classifier overfitting and high computational complexity. Feature selection constitutes a critical strategy for mitigating these challenges. Most existing EEG feature selection methods assume complete multi-dimensional emotion labels. In practice, open acquisition environment, and the inherent subjectivity of emotion perception often result in incomplete label data, which can compromise model generalization. Additionally, existing feature selection methods for handling incomplete multi-dimensional labels primarily focus on correlations among various dimensions during label recovery, neglecting the correlation between samples in the label space and their interaction with various dimensions. To address these issues, we propose a novel incomplete multi-dimensional feature selection algorithm for EEG-based emotion recognition. The proposed method integrates an adaptive dual self-expression learning (ADSEL) with least squares regression. ADSEL establishes a bidirectional pathway between sample-level and dimension-level self-expression learning processes within the label space. It could facilitate the cross-sharing of learned information between these processes, enabling the simultaneous exploitation of effective information across both samples and dimensions for label reconstruction. Consequently, ADSEL could enhances label recovery accuracy and effectively identifies the optimal EEG feature subset for multi-dimensional emotion recognition.

</details>


### [18] [FDC-Net: Rethinking the association between EEG artifact removal and multi-dimensional affective computing](https://arxiv.org/abs/2508.05231)

*Wenjia Dong, Xueyuan Xu, Tianze Yu, Junming Zhang, Li Zhuo*

**Main category:** cs.HC

**Keywords:** EEG, emotion recognition, denoising, deep learning, artifact removal

**Relevance Score:** 6

**TL;DR:** A novel framework, FDC-Net, integrates denoising and emotion recognition for robust EEG-based emotion recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current EEG emotion recognition methods fail to integrate denoising and recognition tasks, leading to errors and not leveraging synergies between them.

**Method:** FDC-Net employs bidirectional gradient propagation and a gated attention mechanism with a frequency-adaptive Transformer for joint optimization of denoising and emotion recognition.

**Key Contributions:**

	1. A framework that couples denoising and emotion recognition tasks in EEG data
	2. Dynamic collaborative mechanism using bidirectional gradient propagation
	3. Gated attention mechanism with frequency-adaptive Transformer

**Result:** FDC-Net achieved high performance with a maximum CC of 96.30% on DEAP and 90.31% on DREAMER for denoising, and emotion recognition accuracies of 82.3% and 88.1% respectively.

**Limitations:** 

**Conclusion:** The proposed FDC-Net enhances noise robustness in EEG-based emotion recognition by effectively coupling artifact removal and emotion detection.

**Abstract:** Electroencephalogram (EEG)-based emotion recognition holds significant value in affective computing and brain-computer interfaces. However, in practical applications, EEG recordings are susceptible to the effects of various physiological artifacts. Current approaches typically treat denoising and emotion recognition as independent tasks using cascaded architectures, which not only leads to error accumulation, but also fails to exploit potential synergies between these tasks. Moreover, conventional EEG-based emotion recognition models often rely on the idealized assumption of "perfectly denoised data", lacking a systematic design for noise robustness. To address these challenges, a novel framework that deeply couples denoising and emotion recognition tasks is proposed for end-to-end noise-robust emotion recognition, termed as Feedback-Driven Collaborative Network for Denoising-Classification Nexus (FDC-Net). Our primary innovation lies in establishing a dynamic collaborative mechanism between artifact removal and emotion recognition through: (1) bidirectional gradient propagation with joint optimization strategies; (2) a gated attention mechanism integrated with frequency-adaptive Transformer using learnable band-position encoding. Two most popular EEG-based emotion datasets (DEAP and DREAMER) with multi-dimensional emotional labels were employed to compare the artifact removal and emotion recognition performance between ASLSL and nine state-of-the-art methods. In terms of the denoising task, FDC-Net obtains a maximum correlation coefficient (CC) value of 96.30% on DEAP and a maximum CC value of 90.31% on DREAMER. In terms of the emotion recognition task under physiological artifact interference, FDC-Net achieves emotion recognition accuracies of 82.3+7.1% on DEAP and 88.1+0.8% on DREAMER.

</details>


### [19] [Driver Assistant: Persuading Drivers to Adjust Secondary Tasks Using Large Language Models](https://arxiv.org/abs/2508.05238)

*Wei Xiang, Muchen Li, Jie Yan, Manling Zheng, Hanfei Zhu, Mengyun Jiang, Lingyun Sun*

**Main category:** cs.HC

**Keywords:** Level 3 automation, Large Language Model, driver attention, cognitive load, persuasive advice

**Relevance Score:** 8

**TL;DR:** This study presents a tool utilizing a Large Language Model (LLM) to assist drivers in maintaining attention during Level 3 automated driving by providing humanized persuasive advice.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce cognitive load and improve driver attention in Level 3 automated driving systems, where drivers engage in secondary tasks.

**Method:** The research uses a Large Language Model to provide visual and auditory persuasive advice to drivers, based on real-time road conditions.

**Key Contributions:**

	1. Development of an LLM-based tool for driver assistance in automated driving
	2. Proactive steering of driver behavior through real-time condition triggers
	3. Empirical evidence demonstrating reduced cognitive burden and enhanced attention management

**Result:** The tool was empirically shown to sustain driver attention while reducing cognitive load and improving the coordination of secondary tasks during takeover situations.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs can effectively support drivers in automated environments through tailored, proactive engagement.

**Abstract:** Level 3 automated driving systems allows drivers to engage in secondary tasks while diminishing their perception of risk. In the event of an emergency necessitating driver intervention, the system will alert the driver with a limited window for reaction and imposing a substantial cognitive burden. To address this challenge, this study employs a Large Language Model (LLM) to assist drivers in maintaining an appropriate attention on road conditions through a "humanized" persuasive advice. Our tool leverages the road conditions encountered by Level 3 systems as triggers, proactively steering driver behavior via both visual and auditory routes. Empirical study indicates that our tool is effective in sustaining driver attention with reduced cognitive load and coordinating secondary tasks with takeover behavior. Our work provides insights into the potential of using LLMs to support drivers during multi-task automated driving.

</details>


### [20] [A Methodological Framework and Questionnaire for Investigating Perceived Algorithmic Fairness](https://arxiv.org/abs/2508.05281)

*Ahmed Abdal Shafi Rasel, Ahmed Mustafa Amlan, Tasmim Shajahan Mim, Tanvir Hasan*

**Main category:** cs.HC

**Keywords:** algorithmic fairness, cultural perspectives, AI transparency

**Relevance Score:** 7

**TL;DR:** This study examines users' perceptions of fairness in algorithmic decision-making in Bangladesh, utilizing a mixed-methods approach combining surveys and interviews.

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how cultural, social, and contextual factors shape perceptions of fairness, transparency, and accountability in AI systems among users from Bangladesh.

**Method:** A comprehensive mixed-methods approach integrating quantitative survey data with qualitative insights from interviews.

**Key Contributions:**

	1. Exploration of algorithmic fairness perceptions in a non-Western context.
	2. Integration of quantitative and qualitative research methods.
	3. Recommendations for culturally aware design principles in AI systems.

**Result:** Users exhibit nuanced attitudes towards human oversight, explanation mechanisms, and contestability in AI systems, emphasizing the need for culturally aware design principles.

**Limitations:** 

**Conclusion:** Culturally informed design is crucial for creating equitable and trustworthy algorithmic systems, contributing to the global dialogue on ethical AI.

**Abstract:** This study explores perceptions of fairness in algorithmic decision-making among users in Bangladesh through a comprehensive mixed-methods approach. By integrating quantitative survey data with qualitative interview insights, we examine how cultural, social, and contextual factors influence users' understanding of fairness, transparency, and accountability in AI systems. Our findings reveal nuanced attitudes toward human oversight, explanation mechanisms, and contestability, highlighting the importance of culturally aware design principles for equitable and trustworthy algorithmic systems. These insights contribute to ongoing discussions on algorithmic fairness by foregrounding perspectives from a non-Western context, thus broadening the global dialogue on ethical AI deployment.

</details>


### [21] [Critical Design Strategy: a Method for Heuristically Evaluating Visualisation Designs](https://arxiv.org/abs/2508.05325)

*Jonathan C. Roberts, Hanan Alnjar, Aron E. Owen, Panagiotis D. Ritsos*

**Main category:** cs.HC

**Keywords:** Critical Design Strategy, visualization, heuristic evaluation, design thinking, higher education

**Relevance Score:** 6

**TL;DR:** The paper introduces the Critical Design Strategy (CDS), a method for enhancing visualisation designs through structured reflection and heuristic evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The CDS aims to support designers, particularly those new to visualisation, in critically evaluating and improving their designs.

**Method:** The CDS involves three stages: Stage 1 defines the idea and captures initial impressions; Stage 2 conducts an in-depth critique with heuristic questions; Stage 3 synthesizes insights and reflects on future steps.

**Key Contributions:**

	1. Introduction of a structured method for design evaluation
	2. Implementation in educational settings
	3. Refinement of heuristic evaluation techniques

**Result:** The CDS has been successfully implemented in visualisation modules for both undergraduate and postgraduate courses, leading to significant enhancements in design reflection and critique.

**Limitations:** No specific limitations mentioned; focus on practical application and refinement over time.

**Conclusion:** The sustained application and refinement of the CDS demonstrate its effectiveness and provide a framework for others to follow in their visualisation practices.

**Abstract:** We present the Critical Design Strategy (CDS) - a structured method designed to facilitate the examination of visualisation designs through reflection and critical thought. The CDS helps designers think critically and make informed improvements using heuristic evaluation. When developing a visual tool or pioneering a novel visualisation approach, identifying areas for enhancement can be challenging. Critical thinking is particularly crucial for visualisation designers and tool developers, especially those new to the field, such as studying visualisation in higher education. The CDS consists of three stages across six perspectives: Stage 1 captures the essence of the idea by assigning an indicative title and selecting five adjectives (from twenty options) to form initial impressions of the design. Stage 2 involves an in-depth critique using 30 heuristic questions spanning six key perspectives - user, environment, interface, components, design, and visual marks. Stage 3 focuses on synthesising insights, reflecting on design decisions, and determining the next steps forward. We introduce the CDS and explore its use across three visualisation modules in both undergraduate and postgraduate courses. Our longstanding experience with the CDS has allowed us to refine and develop it over time: from its initial creation through workshops in 2017/18 to improvements in wording and the development of two applications by 2020, followed by the expansion of support notes and refinement of heuristics through 2023; while using it in our teaching each year. This sustained use allows us to reflect on its practical application and offer guidance on how others can incorporate it into their own work.

</details>


### [22] [Implementation and Application of Multi-Format 3D Data Integration in a Cross-Device Commercial Metaverse Platform](https://arxiv.org/abs/2508.05332)

*Masanori Ibara, Yuichi Hiroi, Takushi Kamegai, Takefumi Hiraki*

**Main category:** cs.HC

**Keywords:** 3D design, metaverse, collaborative decision-making, digital twins, industrial applications

**Relevance Score:** 4

**TL;DR:** This paper explores how the integration of 3D design data with the metaverse can enhance collaborative decision-making in industrial and architectural settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address barriers that limit access to specialized 3D design data for general users and to promote more inclusive decision-making processes in industry and architecture.

**Method:** The paper presents a systematic overview and implementation cases on Cluster, a metaverse platform, analyzing major data formats and outlining integration workflows for the industrial metaverse.

**Key Contributions:**

	1. Overview of practical insights for using 3D data in industrial metaverse applications
	2. Case studies demonstrating the integration of metaverse with digital twin technologies
	3. Analysis of data formats and workflows for improved collaboration in decision-making.

**Result:** The study demonstrates that multi-device access and simultaneous participation improve collaborative decision-making, fostering democratic environments in the industrial metaverse.

**Limitations:** 

**Conclusion:** The fusion of metaverse and digital twin technologies in industrial applications can democratize access to 3D design data, involving a wider range of participants in decision-making processes.

**Abstract:** Traditionally, specialized 3D design data, such as BIM and CAD, have been accessible only to a select group of experts, creating significant barriers that prevent general users from participating in decision-making processes. This paper provides a systematic overview of practical insights for utilizing 3D data in industrial and architectural domains by presenting implementation cases of the industrial metaverse on Cluster, a commercial cross-device metaverse platform. This paper analyzes the characteristics and constraints of major data formats in the industrial and architectural fields and organizes integration workflows for the metaverse. Through application cases utilizing 3D data across multiple domains, we present practical examples of collaborative decision-making support enabled by the fusion of metaverse and digital twin technologies. Specifically, we demonstrate that multi-device access and simultaneous multi-user participation capabilities foster democratic environments in the industrial metaverse, which are challenging to achieve with conventional, expert-dependent systems.

</details>


### [23] [Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study](https://arxiv.org/abs/2508.05497)

*Federico Scarì, Olger Siebinga, Arkady Zgonnikov*

**Main category:** cs.HC

**Keywords:** automated vehicles, human-robot interaction, evaluation framework, drivingSimulator, human-centered design

**Relevance Score:** 4

**TL;DR:** This paper proposes a framework for evaluating automated vehicle (AV) interactions with human-driven vehicles (HDVs) that includes human-centered metrics. A case study demonstrates its application in assessing AV behavior during merging scenarios to improve driver experience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the oversight of human-centered dimensions in evaluating automated vehicle (AV) control algorithms by incorporating metrics from human-robot interaction research.

**Method:** The paper introduces a structured evaluation framework that spans four domains: interaction effect, perception, effort, and ability, and applies it in a driving simulator case study of AV and HDV interactions during merging.

**Key Contributions:**

	1. Proposed an evaluation framework incorporating human-centered metrics for AVs.
	2. Demonstrated the framework's application in a simulated merging scenario.
	3. Highlight critical differences in human driver experience with AVs based on the proposed metrics.

**Result:** The study found critical differences in driver experience when interacting with AVs, emphasizing the importance of including human-centered metrics in AV evaluations.

**Limitations:** 

**Conclusion:** A comprehensive evaluation approach is necessary for understanding AV behavior, promoting the development of AVs that are safe and acceptable from a human perspective.

**Abstract:** As automated vehicles (AVs) increasingly integrate into mixed-traffic environments, evaluating their interaction with human-driven vehicles (HDVs) becomes critical. In most research focused on developing new AV control algorithms (controllers), the performance of these algorithms is assessed solely based on performance metrics such as collision avoidance or lane-keeping efficiency, while largely overlooking the human-centred dimensions of interaction with HDVs. This paper proposes a structured evaluation framework that addresses this gap by incorporating metrics grounded in the human-robot interaction literature. The framework spans four key domains: a) interaction effect, b) interaction perception, c) interaction effort, and d) interaction ability. These domains capture both the performance of the AV and its impact on human drivers around it. To demonstrate the utility of the framework, we apply it to a case study evaluating how a state-of-the-art AV controller interacts with human drivers in a merging scenario in a driving simulator. Measuring HDV-HDV interactions as a baseline, this study included one representative metric per domain: a) perceived safety, b) subjective ratings, specifically how participants perceived the other vehicle's driving behaviour (e.g., aggressiveness or predictability) , c) driver workload, and d) merging success. The results showed that incorporating metrics covering all four domains in the evaluation of AV controllers can illuminate critical differences in driver experience when interacting with AVs. This highlights the need for a more comprehensive evaluation approach. Our framework offers researchers, developers, and policymakers a systematic method for assessing AV behaviour beyond technical performance, fostering the development of AVs that are not only functionally capable but also understandable, acceptable, and safe from a human perspective.

</details>


### [24] [Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis](https://arxiv.org/abs/2508.05572)

*Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Ruiyuan Kang, Jiahua Dong, Cheng Jiang, Chenzhong Li*

**Main category:** cs.HC

**Keywords:** medical time series, disease diagnosis, contrastive learning, AE-GAN, healthcare applications

**Relevance Score:** 8

**TL;DR:** The paper introduces the LMCF framework to enhance disease diagnosis in medical time series, addressing annotation costs and representation challenges by integrating contrastive learning and an AE-GAN.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve medical time series disease diagnosis by addressing high annotation costs and limitations of existing contrastive learning approaches.

**Method:** The proposed Learnable Multi-views Contrastive Framework (LMCF) incorporates a multi-head attention mechanism and contrastive learning strategies to adaptively learn representations from diverse data views, leveraging pre-trained AE-GAN for enhancing data quality.

**Key Contributions:**

	1. Introduction of LMCF for disease-specific feature learning in medical time series
	2. Integration of AE-GAN for data reconstruction and enhancing contrastive learning
	3. Empirical validation on multiple datasets showing superior performance

**Result:** Experiments show that LMCF outperforms seven baseline methods across three medical datasets, demonstrating its effectiveness in diagnosing conditions like myocardial infarction, Alzheimer's, and Parkinson's disease.

**Limitations:** 

**Conclusion:** LMCF significantly enhances the process of disease diagnosis in medical time series, addressing key challenges related to data complexity and representation.

**Abstract:** In medical time series disease diagnosis, two key challenges are identified. First, the high annotation cost of medical data leads to overfitting in models trained on label-limited, single-center datasets. To address this, we propose incorporating external data from related tasks and leveraging AE-GAN to extract prior knowledge, providing valuable references for downstream tasks. Second, many existing studies employ contrastive learning to derive more generalized medical sequence representations for diagnostic tasks, usually relying on manually designed diverse positive and negative sample pairs. However, these approaches are complex, lack generalizability, and fail to adaptively capture disease-specific features across different conditions. To overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that integrates a multi-head attention mechanism and adaptively learns representations from different views through inter-view and intra-view contrastive learning strategies. Additionally, the pre-trained AE-GAN is used to reconstruct discrepancies in the target data as disease probabilities, which are then integrated into the contrastive learning process. Experiments on three target datasets demonstrate that our method consistently outperforms other seven baselines, highlighting its significant impact on healthcare applications such as the diagnosis of myocardial infarction, Alzheimer's disease, and Parkinson's disease. We release the source code at xxxxx.

</details>


### [25] ["Mango Mango, How to Let The Lettuce Dry Without A Spinner?": Exploring User Perceptions of Using An LLM-Based Conversational Assistant Toward Cooking Partner](https://arxiv.org/abs/2310.05853)

*Szeyi Chan, Jiachen Li, Bingsheng Yao, Amama Mahmood, Chien-Ming Huang, Holly Jimison, Elizabeth D Mynatt, Dakuo Wang*

**Main category:** cs.HC

**Keywords:** Large Language Models, Conversational Assistants, User Experience, Human-Computer Interaction, Cooking

**Relevance Score:** 9

**TL;DR:** This research explores user experiences with a Large Language Model-based conversational assistant in cooking scenarios, identifying key expectations and preferences for future development.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the real-world interactions of users with Large Language Model-based conversational assistants, focusing on cooking tasks as a complex and everyday scenario.

**Method:** The study involved participants interacting with the LLM-based conversational assistant, Mango Mango, while cooking, allowing for the collection of both successful and unsatisfactory experiences.

**Key Contributions:**

	1. Identification of user preferences in LLM interaction during cooking tasks
	2. Introduction of design considerations for improving LLM-CA responsiveness
	3. Insights into user perception of LLMs as personal assistants rather than just tools

**Result:** Participants appreciated the system's ability to provide customized instructions, extensive information beyond recipes, and support for dynamic task planning, but desired better conversational adaptability and more suggestive responses.

**Limitations:** 

**Conclusion:** Users began to see the LLM-CA as a partner rather than just a tool, leading to the proposal of five design considerations for enhancing conversational assistants.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has created numerous potentials for integration with conversational assistants (CAs) assisting people in their daily tasks, particularly due to their extensive flexibility. However, users' real-world experiences interacting with these assistants remain unexplored. In this research, we chose cooking, a complex daily task, as a scenario to explore people's successful and unsatisfactory experiences while receiving assistance from an LLM-based CA, Mango Mango. We discovered that participants value the system's ability to offer customized instructions based on context, provide extensive information beyond the recipe, and assist them in dynamic task planning. However, users expect the system to be more adaptive to oral conversation and provide more suggestive responses to keep them actively involved. Recognizing that users began treating our LLM-CA as a personal assistant or even a partner rather than just a recipe-reading tool, we propose five design considerations for future development.

</details>


### [26] [The Homework Wars: Exploring Emotions, Behaviours, and Conflicts in Parent-Child Homework Interactions](https://arxiv.org/abs/2502.01325)

*Nan Gao, Yibin Liu, Xin Tang, Yanyan Liu, Chun Yu, Yun Huang, Yuntao Wang, Flora D. Salim, Xuhai Orson Xu, Jun Wei, Yuanchun Shi*

**Main category:** cs.HC

**Keywords:** parental involvement, homework, large language models, family dynamics, conflict patterns

**Relevance Score:** 6

**TL;DR:** The paper presents a framework utilizing naturalistic parent-child interaction data and large language models to analyze homework conversations, revealing emotional dynamics and conflict patterns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand the emotional and conflict dynamics of parental involvement in homework and its effects on family well-being.

**Method:** A four-week in situ study with 78 Chinese families, collecting 475 hours of audio recordings and daily surveys, which were analyzed using a large language model-based pipeline to extract parental behaviors and conflict types from homework sessions.

**Key Contributions:**

	1. Development of a framework for analyzing parent-child interactions using LLMs
	2. Identification of common conflict types in homework situations
	3. Empirical insights into emotional dynamics linked to parental behaviors

**Result:** The study identified significant emotional shifts in parents, 18 recurring behaviors, and seven common conflict types, with a particular focus on the frequent occurrence of Knowledge Conflict.

**Limitations:** 

**Conclusion:** This research enhances understanding of family dynamics in educational contexts and suggests avenues for more effective parenting strategies based on empirical insights.

**Abstract:** Parental involvement in homework is a crucial aspect of family education, but it often triggers emotional strain and conflicts. Despite growing concern over its impact on family well-being, prior research has lacked access to fine-grained, real-time dynamics of these interactions. To bridge this gap, we present a framework that leverages naturalistic parent-child interaction data and large language models (LLMs) to analyse homework conversations at scale. In a four-week in situ study with 78 Chinese families, we collected 475 hours of audio recordings and accompanying daily surveys, capturing 602 homework sessions in everyday home settings. Our LLM-based pipeline reliably extracted and categorised parental behaviours and conflict patterns from transcribed conversations, achieving high agreement with expert annotations. The analysis revealed significant emotional shifts in parents before and after homework, 18 recurring parental behaviours and seven common conflict types, with Knowledge Conflict being the most frequent. Notably, even well-intentioned behaviours were significantly positively correlated with specific conflicts. This work advances ubiquitous computing methods for studying complex family dynamics and offers empirical insights to enrich family education theory and inform more effective parenting strategies and interventions in the future.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [27] [Enhancing Dialogue Annotation with Speaker Characteristics Leveraging a Frozen LLM](https://arxiv.org/abs/2508.04795)

*Thomas Thebaud, Yen-Ju Lu, Matthew Wiesner, Peter Viechnicki, Najim Dehak*

**Main category:** cs.CL

**Keywords:** dialogue transcription, speaker profiling, metadata tagging, Large Language Models, frozen models

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel post-processing step for dialogue transcription that incorporates metadata tagging for speaker characteristics using combined audio and language models without fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to enhance transcription outputs by enriching them with metadata related to speaker characteristics such as age, gender, and emotion, thereby improving the utility of the transcriptions in various applications.

**Method:** The study utilizes frozen audio foundation models (like Whisper or WavLM) in conjunction with a frozen LLAMA language model for the task of speaker profiling, using efficient connectors to integrate audio and language representations without task-specific fine-tuning.

**Key Contributions:**

	1. Introduction of a new metadata tagging method for transcribed dialogues
	2. Combination of audio foundation models with language models without fine-tuning
	3. Achieved competitive performance in speaker profiling tasks with an EER of 8.8%.

**Result:** The approach delivers competitive performance on speaker profiling tasks and demonstrates the capability of the frozen LLAMA model to compare x-vectors with an Equal Error Rate of 8.8% in certain scenarios.

**Limitations:** 

**Conclusion:** The presented method shows promise in enhancing transcription quality and allowing for detailed speaker characterization while maintaining speed and modularity.

**Abstract:** In dialogue transcription pipelines, Large Language Models (LLMs) are frequently employed in post-processing to improve grammar, punctuation, and readability. We explore a complementary post-processing step: enriching transcribed dialogues by adding metadata tags for speaker characteristics such as age, gender, and emotion. Some of the tags are global to the entire dialogue, while some are time-variant. Our approach couples frozen audio foundation models, such as Whisper or WavLM, with a frozen LLAMA language model to infer these speaker attributes, without requiring task-specific fine-tuning of either model. Using lightweight, efficient connectors to bridge audio and language representations, we achieve competitive performance on speaker profiling tasks while preserving modularity and speed. Additionally, we demonstrate that a frozen LLAMA model can compare x-vectors directly, achieving an Equal Error Rate of 8.8% in some scenarios.

</details>


### [28] [Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization](https://arxiv.org/abs/2508.04796)

*Negar Foroutan, Clara Meister, Debjit Paul, Joel Niklaus, Sina Ahmadi, Antoine Bosselut, Rico Sennrich*

**Main category:** cs.CL

**Keywords:** Tokenization, NLP, Byte Pair Encoding, Cross-lingual, Equity

**Relevance Score:** 8

**TL;DR:** Parity-aware BPE addresses the inequities in tokenization across languages in NLP by focusing on better token counts for lower-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate the computational and financial inequalities faced by users from different language backgrounds due to suboptimal tokenization in lower-resource languages.

**Method:** Introducing Parity-aware Byte Pair Encoding (BPE), which prioritizes token compression for the worst-compressed language at each merge step while maintaining overall compression rates.

**Key Contributions:**

	1. Development of Parity-aware BPE for equitable tokenization
	2. Empirical evidence showing improved token counts for lower-resource languages
	3. Balancing compression gains with language-model performance

**Result:** Parity-aware BPE improves token counts for lower-resource languages with a minimal impact on global compression and language-model performance.

**Limitations:** 

**Conclusion:** The method enhances equity in tokenization without significantly compromising performance, thus promoting fairer NLP applications across diverse languages.

**Abstract:** Tokenization is the first -- and often least scrutinized -- step of most NLP pipelines. Standard algorithms for learning tokenizers rely on frequency-based objectives, which favor languages dominant in the training data and consequently leave lower-resource languages with tokenizations that are disproportionately longer, morphologically implausible, or even riddled with <UNK> placeholders. This phenomenon ultimately amplifies computational and financial inequalities between users from different language backgrounds. To remedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of the widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes the compression gain of the currently worst-compressed language, trading a small amount of global compression for cross-lingual parity. We find empirically that Parity-aware BPE leads to more equitable token counts across languages, with negligible impact on global compression rate and no substantial effect on language-model performance in downstream tasks.

</details>


### [29] [Pitch Accent Detection improves Pretrained Automatic Speech Recognition](https://arxiv.org/abs/2508.04814)

*David Sasu, Natalie Schluter*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, pitch accent detection, semi-supervised learning, prosodic cues, LibriSpeech

**Relevance Score:** 6

**TL;DR:** This paper presents a joint model for Automatic Speech Recognition (ASR) and pitch accent detection, showing significant improvements in both tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to improve ASR systems by incorporating pitch accent detection, addressing limitations in current models that overlook prosodic features.

**Method:** A joint ASR and pitch accent detection model is developed, leveraging semi-supervised speech representations alongside a new component for pitch accent detection.

**Key Contributions:**

	1. Development of a joint ASR and pitch accent detection model
	2. Significant improvement in pitch accent detection accuracy
	3. Reduction in ASR WER on the LibriSpeech dataset

**Result:** The joint model achieves a 41% improvement in F1-score for pitch accent detection and reduces the Word Error Rate (WER) by 28.3% on the LibriSpeech dataset during fine-tuning.

**Limitations:** 

**Conclusion:** The findings suggest that integrating prosodic cues like pitch accent in training enhances the performance of ASR systems, emphasizing the value of extending pretrained models.

**Abstract:** We show the performance of Automatic Speech Recognition (ASR) systems that use semi-supervised speech representations can be boosted by a complimentary pitch accent detection module, by introducing a joint ASR and pitch accent detection model. The pitch accent detection component of our model achieves a significant improvement on the state-of-the-art for the task, closing the gap in F1-score by 41%. Additionally, the ASR performance in joint training decreases WER by 28.3% on LibriSpeech, under limited resource fine-tuning. With these results, we show the importance of extending pretrained speech models to retain or re-learn important prosodic cues such as pitch accent.

</details>


### [30] [Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History](https://arxiv.org/abs/2508.04826)

*Tommaso Tosato, Saskia Helbling, Yorguin-Jose Mantilla-Ramos, Mahmood Hegazy, Alberto Tosato, David John Lemay, Irina Rish, Guillaume Dumas*

**Main category:** cs.CL

**Keywords:** Personality Stability, Large Language Models, Evaluation Framework

**Relevance Score:** 9

**TL;DR:** This paper introduces PERSIST, a framework for evaluating the behavioral consistency of large language models, revealing substantial response variability and challenges in personality alignment for safe deployment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the poorly understood personality traits of large language models and their implications for safe and consistent deployment in critical applications.

**Method:** Evaluates 25+ open-source models using traditional and LLM-adapted personality instruments, varying question order, paraphrasing, personas, and reasoning modes across over 500,000 responses.

**Key Contributions:**

	1. Introduction of the PERSIST evaluation framework
	2. Empirical analysis of personality stability across 25+ large language models
	3. Insights on the inadequacy of personality-based alignment strategies for safety-critical applications.

**Result:** Findings indicate that even large models show significant response variability, prompt reordering affects personality measurements substantially, and common stabilization strategies can increase variability.

**Limitations:** Focuses on open-source models; findings may not generalize to proprietary models; does not explore long-term stability in deployments.

**Conclusion:** The instability in responses across model scales and requested stabilization strategies implies current LLMs are not equipped for consistent behavioral performance, questioning the efficacy of personality-alignment strategies for safety-critical applications.

**Abstract:** Large language models require consistent behavioral patterns for safe deployment, yet their personality-like traits remain poorly understood. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25+ open-source models (1B-671B parameters) across 500,000+ responses. Using traditional (BFI-44, SD3) and novel LLM-adapted personality instruments, we systematically vary question order, paraphrasing, personas, and reasoning modes. Our findings challenge fundamental deployment assumptions: (1) Even 400B+ models exhibit substantial response variability (SD > 0.4); (2) Minor prompt reordering alone shifts personality measurements by up to 20%; (3) Interventions expected to stabilize behavior, such as chain-of-thought reasoning, detailed personas instruction, inclusion of conversation history, can paradoxically increase variability; (4) LLM-adapted instruments show equal instability to human-centric versions, confirming architectural rather than translational limitations. This persistent instability across scales and mitigation strategies suggests current LLMs lack the foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that personality-based alignment strategies may be fundamentally inadequate.

</details>


### [31] [RCR-Router: Efficient Role-Aware Context Routing for Multi-Agent LLM Systems with Structured Memory](https://arxiv.org/abs/2508.04903)

*Jun Liu, Zhenglun Kong, Changdi Yang, Fan Yang, Tianqi Li, Peiyan Dong, Joannah Nanjekye, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang*

**Main category:** cs.CL

**Keywords:** multi-agent LLMs, context routing, memory selection, collaboration, QA benchmarks

**Relevance Score:** 9

**TL;DR:** RCR-Router is a dynamic context routing framework for multi-agent LLMs that improves efficiency by intelligently selecting relevant memory subsets based on agent roles and tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing coordination methods for multi-agent LLMs lead to high token consumption and poor adaptability. RCR-Router aims to address these issues.

**Method:** The framework uses a scoring policy to select relevant memory subsets dynamically for each agent based on its role and task stage, integrating agent outputs into a shared memory for context refinement.

**Key Contributions:**

	1. Introduction of a novel role-aware context routing framework (RCR-Router) for multi-agent LLMs
	2. Dynamic memory selection based on agent roles and stages
	3. Development of the Answer Quality Score metric for evaluating LLM outputs

**Result:** Experiments showed RCR-Router reduces token usage by up to 30% while maintaining or improving answer quality on multi-hop QA benchmarks.

**Limitations:** 

**Conclusion:** Structured memory routing and output-aware evaluation are critical for enhancing scalable performance in multi-agent LLM systems.

**Abstract:** Multi-agent large language model (LLM) systems have shown strong potential in complex reasoning and collaborative decision-making tasks. However, most existing coordination schemes rely on static or full-context routing strategies, which lead to excessive token consumption, redundant memory exposure, and limited adaptability across interaction rounds. We introduce RCR-Router, a modular and role-aware context routing framework designed to enable efficient, adaptive collaboration in multi-agent LLMs. To our knowledge, this is the first routing approach that dynamically selects semantically relevant memory subsets for each agent based on its role and task stage, while adhering to a strict token budget. A lightweight scoring policy guides memory selection, and agent outputs are iteratively integrated into a shared memory store to facilitate progressive context refinement. To better evaluate model behavior, we further propose an Answer Quality Score metric that captures LLM-generated explanations beyond standard QA accuracy. Experiments on three multi-hop QA benchmarks -- HotPotQA, MuSiQue, and 2WikiMultihop -- demonstrate that RCR-Router reduces token usage (up to 30%) while improving or maintaining answer quality. These results highlight the importance of structured memory routing and output-aware evaluation in advancing scalable multi-agent LLM systems.

</details>


### [32] [I Think, Therefore I Am Under-Qualified? A Benchmark for Evaluating Linguistic Shibboleth Detection in LLM Hiring Evaluations](https://arxiv.org/abs/2508.04939)

*Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah*

**Main category:** cs.CL

**Keywords:** Large Language Models, linguistic bias, demographic attributes, fairness, automated evaluation systems

**Relevance Score:** 9

**TL;DR:** This paper presents a benchmark for evaluating Large Language Models' responses to subtle linguistic markers that can indicate demographic attributes, revealing biases in automated evaluation systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address and measure demographic bias present in AI systems, particularly in Large Language Models, by using linguistic shibboleths as markers.

**Method:** The paper utilizes a benchmark created through interview simulations with 100 validated question-response pairs, allowing for controlled linguistic variations while maintaining content quality.

**Key Contributions:**

	1. Introduces a benchmark for evaluating linguistic bias in LLMs.
	2. Demonstrates systematic penalties for hedging language in LLM responses.
	3. Validates the effectiveness of the benchmark in identifying model-specific biases.

**Result:** Hedged responses from LLMs received, on average, 25.6% lower ratings, indicating systematic penalties for certain linguistic patterns.

**Limitations:** 

**Conclusion:** The benchmark establishes a foundation for detecting linguistic discrimination in AI systems, highlighting its importance for fairness in automated decision-making.

**Abstract:** This paper introduces a comprehensive benchmark for evaluating how Large Language Models (LLMs) respond to linguistic shibboleths: subtle linguistic markers that can inadvertently reveal demographic attributes such as gender, social class, or regional background. Through carefully constructed interview simulations using 100 validated question-response pairs, we demonstrate how LLMs systematically penalize certain linguistic patterns, particularly hedging language, despite equivalent content quality. Our benchmark generates controlled linguistic variations that isolate specific phenomena while maintaining semantic equivalence, which enables the precise measurement of demographic bias in automated evaluation systems. We validate our approach along multiple linguistic dimensions, showing that hedged responses receive 25.6% lower ratings on average, and demonstrate the benchmark's effectiveness in identifying model-specific biases. This work establishes a foundational framework for detecting and measuring linguistic discrimination in AI systems, with broad applications to fairness in automated decision-making contexts.

</details>


### [33] [Towards Robust Evaluation of Visual Activity Recognition: Resolving Verb Ambiguity with Sense Clustering](https://arxiv.org/abs/2508.04945)

*Louie Hong Yao, Nicholas Jarvis, Tianyu Jiang*

**Main category:** cs.CL

**Keywords:** visual activity recognition, evaluation methods, verb semantics, clustering framework, human judgments

**Relevance Score:** 5

**TL;DR:** Proposes a vision-language clustering framework to improve evaluation of visual activity recognition systems by addressing verb semantic ambiguities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluation methods fail to capture the nuances of verb semantics and image interpretations in activity recognition, leading to incomplete assessments.

**Method:** Developing a clustering framework that groups synonymous verbs and different perspectives, allowing for a more robust evaluation of models.

**Key Contributions:**

	1. Introduces a vision-language clustering framework for verb sense clustering
	2. Demonstrates improved evaluation metrics for activity recognition
	3. Shows that cluster-based evaluations align better with human judgments

**Result:** Analysis of the imSitu dataset reveals each image corresponds to an average of 2.8 sense clusters, indicating multiple valid interpretations per image. The new cluster-based evaluation method aligns better with human judgments compared to standard methods.

**Limitations:** 

**Conclusion:** The proposed method provides a more nuanced and complete assessment of activity recognition model performance, reflecting human interpretation more accurately.

**Abstract:** Evaluating visual activity recognition systems is challenging due to inherent ambiguities in verb semantics and image interpretation. When describing actions in images, synonymous verbs can refer to the same event (e.g., brushing vs. grooming), while different perspectives can lead to equally valid but distinct verb choices (e.g., piloting vs. operating). Standard exact-match evaluation, which relies on a single gold answer, fails to capture these ambiguities, resulting in an incomplete assessment of model performance. To address this, we propose a vision-language clustering framework that constructs verb sense clusters, providing a more robust evaluation. Our analysis of the imSitu dataset shows that each image maps to an average of 2.8 sense clusters, with each cluster representing a distinct perspective of the image. We evaluate multiple activity recognition models and compare our cluster-based evaluation with standard evaluation methods. Additionally, our human alignment analysis suggests that the cluster-based evaluation better aligns with human judgements, offering a more nuanced assessment of model performance.

</details>


### [34] [A Multi-Stage Large Language Model Framework for Extracting Suicide-Related Social Determinants of Health](https://arxiv.org/abs/2508.05003)

*Song Wang, Yishu Wei, Haotian Ma, Max Lovitt, Kelly Deng, Yuan Meng, Zihan Xu, Jingze Zhang, Yunyu Xiao, Ying Ding, Xuhai Xu, Joydeep Ghosh, Yifan Peng*

**Main category:** cs.CL

**Keywords:** social determinants of health, suicide prevention, large language models

**Relevance Score:** 9

**TL;DR:** A multi-stage large language model framework enhances the extraction of social determinants of health related to suicide incidents from unstructured text, improving accuracy and explainability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding social determinants of health factors is crucial for early intervention and prevention of suicide incidents, but data-driven approaches face challenges.

**Method:** A multi-stage large language model framework was developed, compared to state-of-the-art models like BioBERT and GPT-3.5-turbo, and evaluated through automated comparisons and a pilot user study.

**Key Contributions:**

	1. Multi-stage framework for SDoH extraction
	2. Comparison with state-of-the-art language models
	3. Improved explainability through intermediate models

**Result:** The framework showed improved performance in extracting SDoH factors, with fine-tuning a smaller model achieving comparable results at lower costs. It also enhanced explainability by providing intermediate explanations.

**Limitations:** 

**Conclusion:** The approach not only improves accuracy but also transparency in extracting suicide-related SDoH, supporting early risk identification and effective prevention strategies.

**Abstract:** Background: Understanding social determinants of health (SDoH) factors contributing to suicide incidents is crucial for early intervention and prevention. However, data-driven approaches to this goal face challenges such as long-tailed factor distributions, analyzing pivotal stressors preceding suicide incidents, and limited model explainability. Methods: We present a multi-stage large language model framework to enhance SDoH factor extraction from unstructured text. Our approach was compared to other state-of-the-art language models (i.e., pre-trained BioBERT and GPT-3.5-turbo) and reasoning models (i.e., DeepSeek-R1). We also evaluated how the model's explanations help people annotate SDoH factors more quickly and accurately. The analysis included both automated comparisons and a pilot user study. Results: We show that our proposed framework demonstrated performance boosts in the overarching task of extracting SDoH factors and in the finer-grained tasks of retrieving relevant context. Additionally, we show that fine-tuning a smaller, task-specific model achieves comparable or better performance with reduced inference costs. The multi-stage design not only enhances extraction but also provides intermediate explanations, improving model explainability. Conclusions: Our approach improves both the accuracy and transparency of extracting suicide-related SDoH from unstructured texts. These advancements have the potential to support early identification of individuals at risk and inform more effective prevention strategies.

</details>


### [35] [Dialogues Aspect-based Sentiment Quadruple Extraction via Structural Entropy Minimization Partitioning](https://arxiv.org/abs/2508.05023)

*Kun Peng, Cong Cao, Hao Peng, Zhifeng Hao, Lei Jiang, Kongjing Gu, Yanbing Liu, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** aspect-based sentiment analysis, dialogue processing, machine learning, natural language processing, information extraction

**Relevance Score:** 7

**TL;DR:** This paper presents a method for extracting sentiment quadruples from dialogues by partitioning dialogues into semantically independent sub-dialogues, improving extraction accuracy and reducing noise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for aspect-based sentiment extraction in dialogues are ineffective due to the uniform treatment of dialogue structures, which often contain independent sub-dialogues.

**Method:** The proposed method utilizes a structural entropy minimization algorithm to partition dialogues into semantically independent sub-dialogues, followed by a two-step framework for quadruple extraction.

**Key Contributions:**

	1. Introduction of a structural entropy minimization algorithm to dialogue partitioning
	2. A novel two-step framework for quadruple extraction
	3. Achieving state-of-the-art performance in DiaASQ with reduced computational costs

**Result:** The proposed approach achieves state-of-the-art performance in Dialogue Aspect-based Sentiment Quadruple Extraction with significantly lower computational costs compared to existing methods.

**Limitations:** 

**Conclusion:** The research demonstrates that focused partitioning of dialogues significantly enhances the accuracy of sentiment extraction while minimizing computational issues.

**Abstract:** Dialogues Aspect-based Sentiment Quadruple Extraction (DiaASQ) aims to extract all target-aspect-opinion-sentiment quadruples from a given multi-round, multi-participant dialogue. Existing methods typically learn word relations across entire dialogues, assuming a uniform distribution of sentiment elements. However, we find that dialogues often contain multiple semantically independent sub-dialogues without clear dependencies between them. Therefore, learning word relationships across the entire dialogue inevitably introduces additional noise into the extraction process. To address this, our method focuses on partitioning dialogues into semantically independent sub-dialogues. Achieving completeness while minimizing these sub-dialogues presents a significant challenge. Simply partitioning based on reply relationships is ineffective. Instead, we propose utilizing a structural entropy minimization algorithm to partition the dialogues. This approach aims to preserve relevant utterances while distinguishing irrelevant ones as much as possible. Furthermore, we introduce a two-step framework for quadruple extraction: first extracting individual sentiment elements at the utterance level, then matching quadruples at the sub-dialogue level. Extensive experiments demonstrate that our approach achieves state-of-the-art performance in DiaASQ with much lower computational costs.

</details>


### [36] [Evaluation of LLMs in AMR Parsing](https://arxiv.org/abs/2508.05028)

*Shu Han Ho*

**Main category:** cs.CL

**Keywords:** semantic formalism, AMR parsing, Large Language Models, finetuning, machine learning

**Relevance Score:** 7

**TL;DR:** This paper evaluates the performance of four LLMs finetuned for AMR parsing, demonstrating that simpler finetuning methods can achieve competitive results compared to complex models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of finetuning decoder-only LLMs for AMR parsing and compare their performance with that of State of the Art parsers.

**Method:** Four distinct LLM architectures (Phi 3.5, Gemma 2, LLaMA 3.2, DeepSeek R1 LLaMA Distilled) were finetuned and evaluated using the LDC2020T02 Gold AMR3.0 test set.

**Key Contributions:**

	1. Evaluation of four LLM architectures in AMR parsing
	2. Demonstration of comparable performance to SOTA parsers with simpler methods
	3. Identification of LLaMA 3.2 as a leading model in semantic performance

**Result:** Finetuning achieved SMATCH F1: 0.804, comparable to advanced AMR parsers, with LLaMA 3.2 showing the best semantic performance and Phi 3.5 excelling in structural validity.

**Limitations:** 

**Conclusion:** Straightforward finetuning of decoder-only LLMs can rival complex SOTA AMR parsers, indicating potential for easier implementations in practical applications.

**Abstract:** Meaning Representation (AMR) is a semantic formalism that encodes sentence meaning as rooted, directed, acyclic graphs, where nodes represent concepts and edges denote semantic relations. Finetuning decoder only Large Language Models (LLMs) represent a promising novel straightfoward direction for AMR parsing. This paper presents a comprehensive evaluation of finetuning four distinct LLM architectures, Phi 3.5, Gemma 2, LLaMA 3.2, and DeepSeek R1 LLaMA Distilled using the LDC2020T02 Gold AMR3.0 test set. Our results have shown that straightfoward finetuning of decoder only LLMs can achieve comparable performance to complex State of the Art (SOTA) AMR parsers. Notably, LLaMA 3.2 demonstrates competitive performance against SOTA AMR parsers given a straightforward finetuning approach. We achieved SMATCH F1: 0.804 on the full LDC2020T02 test split, on par with APT + Silver (IBM) at 0.804 and approaching Graphene Smatch (MBSE) at 0.854. Across our analysis, we also observed a consistent pattern where LLaMA 3.2 leads in semantic performance while Phi 3.5 excels in structural validity.

</details>


### [37] [Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning](https://arxiv.org/abs/2508.05078)

*Jinda Liu, Bo Cheng, Yi Chang, Yuan Wu*

**Main category:** cs.CL

**Keywords:** Parameter-Efficient Fine-Tuning, Multi-Task Learning, Large Language Models, Align-LoRA, Task Representations

**Relevance Score:** 9

**TL;DR:** The paper proposes Align-LoRA, a simplified method for adapting Large Language Models (LLMs) through multi-task learning (MTL) that outperforms complex models with multiple adapters.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of adapting LLMs for diverse tasks in multi-task learning (MTL) scenarios.

**Method:** The authors investigate a simplified multi-head architecture and propose Align-LoRA, which uses a single-adapter LoRA with an explicit loss for aligning task representations.

**Key Contributions:**

	1. Introduction of Align-LoRA, a simplified multi-task learning method for LLMs.
	2. Demonstration that high inter-head similarity with fewer components can yield better performance.
	3. Challenge to the conventional multi-component paradigm in adapting LLMs.

**Result:** The simplified approach with high inter-head similarity outperforms more complex multi-adapter systems, leading to competitive performance with a single-adapter when rank is increased.

**Limitations:** Focus on specific architectures may limit generalizability across all LLM designs.

**Conclusion:** Learning robust shared representations is crucial for effective multi-task learning; Align-LoRA establishes a simpler yet more effective approach for LLM adaptation.

**Abstract:** Parameter-Efficient Fine-Tuning (PEFT) is essential for adapting Large Language Models (LLMs). In practice, LLMs are often required to handle a diverse set of tasks from multiple domains, a scenario naturally addressed by multi-task learning (MTL). Within this MTL context, a prevailing trend involves LoRA variants with multiple adapters or heads, which advocate for structural diversity to capture task-specific knowledge. Our findings present a direct challenge to this paradigm. We first show that a simplified multi-head architecture with high inter-head similarity substantially outperforms complex multi-adapter and multi-head systems. This leads us to question the multi-component paradigm itself, and we further demonstrate that a standard single-adapter LoRA, with a sufficiently increased rank, also achieves highly competitive performance. These results lead us to a new hypothesis: effective MTL generalization hinges on learning robust shared representations, not isolating task-specific features. To validate this, we propose Align-LoRA, which incorporates an explicit loss to align task representations within the shared adapter space. Experiments confirm that Align-LoRA significantly surpasses all baselines, establishing a simpler yet more effective paradigm for adapting LLMs to multiple tasks. The code is available at https://github.com/jinda-liu/Align-LoRA.

</details>


### [38] [Multimodal Fact Checking with Unified Visual, Textual, and Contextual Representations](https://arxiv.org/abs/2508.05097)

*Aditya Kishore, Gaurav Kumar, Jasabanta Patro*

**Main category:** cs.CL

**Keywords:** multimodal misinformation, fact-checking, machine learning, human-computer interaction, contrastive learning

**Relevance Score:** 7

**TL;DR:** Proposed MultiCheck framework for multimodal fact verification using text and images.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of fact-checking systems that primarily rely on textual evidence amidst increasing multimodal misinformation.

**Method:** MultiCheck integrates dedicated encoders for text and images with a fusion module to capture cross-modal relationships, followed by a classification head that predicts claim veracity.

**Key Contributions:**

	1. Development of MultiCheck framework for fact verification
	2. Integration of text and image encoders for multimodal reasoning
	3. Successful application leading to improved performance on the Factify 2 dataset

**Result:** Achieved a weighted F1 score of 0.84 on the Factify 2 dataset, outperforming baseline methods.

**Limitations:** 

**Conclusion:** The effectiveness of explicit multimodal reasoning in fact-checking is underscored, demonstrating scalability and interpretability in real-world applications.

**Abstract:** The growing rate of multimodal misinformation, where claims are supported by both text and images, poses significant challenges to fact-checking systems that rely primarily on textual evidence. In this work, we have proposed a unified framework for fine-grained multimodal fact verification called "MultiCheck", designed to reason over structured textual and visual signals. Our architecture combines dedicated encoders for text and images with a fusion module that captures cross-modal relationships using element-wise interactions. A classification head then predicts the veracity of a claim, supported by a contrastive learning objective that encourages semantic alignment between claim-evidence pairs in a shared latent space. We evaluate our approach on the Factify 2 dataset, achieving a weighted F1 score of 0.84, substantially outperforming the baseline. These results highlight the effectiveness of explicit multimodal reasoning and demonstrate the potential of our approach for scalable and interpretable fact-checking in complex, real-world scenarios.

</details>


### [39] [BEE-RAG: Balanced Entropy Engineering for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05100)

*Yuhao Wang, Ruiyang Ren, Yucheng Wang, Jing Liu, Wayne Xin Zhao, Hua Wu, Haifeng Wang*

**Main category:** cs.CL

**Keywords:** large language models, retrieval-augmented generation, entropy engineering, attention dynamics, adaptive fine-tuning

**Relevance Score:** 9

**TL;DR:** The BEE-RAG framework enhances retrieval-augmented generation by managing attention entropy, allowing better adaptability to varying context lengths and improving overall RAG performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance issues in RAG systems caused by long context lengths and attention dilution from unconstrained entropy growth.

**Method:** The BEE-RAG framework implements entropy invariance to reformulate attention dynamics, while introducing a zero-shot inference strategy for multi-importance estimation and a parameter-efficient adaptive fine-tuning mechanism.

**Key Contributions:**

	1. Proposes a framework that improves RAG by controlling entropy dynamics
	2. Introduces zero-shot inference for multi-importance estimation
	3. Develops a parameter-efficient adaptive fine-tuning mechanism

**Result:** Extensive experiments show that BEE-RAG significantly improves the performance of retrieval-augmented generation tasks compared to existing methods.

**Limitations:** 

**Conclusion:** BEE-RAG effectively balances entropy sensitivity with context length, leading to improved adaptability and performance in RAG applications.

**Abstract:** With the rapid advancement of large language models (LLMs), retrieval-augmented generation (RAG) has emerged as a critical approach to supplement the inherent knowledge limitations of LLMs. However, due to the typically large volume of retrieved information, RAG tends to operate with long context lengths. From the perspective of entropy engineering, we identify unconstrained entropy growth and attention dilution due to long retrieval context as significant factors affecting RAG performance. In this paper, we propose the balanced entropy-engineered RAG (BEE-RAG) framework, which improves the adaptability of RAG systems to varying context lengths through the principle of entropy invariance. By leveraging balanced context entropy to reformulate attention dynamics, BEE-RAG separates attention sensitivity from context length, ensuring a stable entropy level. Building upon this, we introduce a zero-shot inference strategy for multi-importance estimation and a parameter-efficient adaptive fine-tuning mechanism to obtain the optimal balancing factor for different settings. Extensive experiments across multiple RAG tasks demonstrate the effectiveness of BEE-RAG.

</details>


### [40] [Attention Basin: Why Contextual Position Matters in Large Language Models](https://arxiv.org/abs/2508.05128)

*Zihao Yi, Delong Zeng, Zhenqing Ling, Haohao Luo, Zhe Xu, Wei Liu, Jian Luan, Wanxia Cao, Ying Shen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Attention Bias, Reranking, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper investigates the positional bias in Large Language Models and proposes a framework called Attention-Driven Reranking (AttnRank) to enhance model performance by optimizing the arrangement of input information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the positional bias in LLMs and improve their performance by enhancing how they attend to contextually important information.

**Method:** The authors conduct extensive experiments to observe how LLMs allocate attention and propose AttnRank, which ranks information based on intrinsic positional preferences without requiring additional training.

**Key Contributions:**

	1. Identification of the attention basin phenomenon in LLMs
	2. Introduction of the Attention-Driven Reranking (AttnRank) framework
	3. Demonstration of AttnRank's effectiveness across multiple tasks and model architectures

**Result:** AttnRank led to significant performance improvements across 10 different large language models in tasks like multi-hop QA and few-shot learning, proving effective without modifying model parameters.

**Limitations:** 

**Conclusion:** Optimizing attention allocation based on the identified positional bias can substantially enhance the performance of LLMs without extensive retraining or adaptations.

**Abstract:** The performance of Large Language Models (LLMs) is significantly sensitive to the contextual position of information in the input. To investigate the mechanism behind this positional bias, our extensive experiments reveal a consistent phenomenon we term the attention basin: when presented with a sequence of structured items (e.g., retrieved documents or few-shot examples), models systematically assign higher attention to the items at the beginning and end of the sequence, while neglecting those in the middle. Crucially, our analysis further reveals that allocating higher attention to critical information is key to enhancing model performance. Based on these insights, we introduce Attention-Driven Reranking (AttnRank), a two-stage framework that (i) estimates a model's intrinsic positional attention preferences using a small calibration set, and (ii) reorders retrieved documents or few-shot examples to align the most salient content with these high-attention positions. AttnRank is a model-agnostic, training-free, and plug-and-play method with minimal computational overhead. Experiments on multi-hop QA and few-shot in-context learning tasks demonstrate that AttnRank achieves substantial improvements across 10 large language models of varying architectures and scales, without modifying model parameters or training procedures.

</details>


### [41] [Towards Assessing Medical Ethics from Knowledge to Practice](https://arxiv.org/abs/2508.05132)

*Chang Hong, Minghao Wu, Qingying Xiao, Yuchi Wang, Xiang Wan, Guangjun Yu, Benyou Wang, Yan Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Medical Ethics, Benchmarking, Artificial Intelligence, Healthcare

**Relevance Score:** 9

**TL;DR:** This paper introduces PrinciplismQA, a benchmark for evaluating large language models' adherence to medical ethics, revealing gaps in ethical reasoning, particularly in applying principles like Beneficence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously evaluate the ethical reasoning capabilities of large language models in healthcare, as current benchmarks lack this focus.

**Method:** Developed PrinciplismQA, a benchmark comprising 3,648 questions that assess models' alignment with core medical ethics, using multiple-choice and open-ended questions validated by medical experts.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for evaluating ethical reasoning in LLMs in healthcare.
	2. Demonstration of the gap between models' knowledge and their practical ethical applications.
	3. Showing the impact of medical domain fine-tuning on improving models' ethical competence.

**Result:** Found significant discrepancies between LLMs' ethical knowledge and their practical applications, particularly in handling ethical dilemmas involving Beneficence.

**Limitations:** 

**Conclusion:** PrinciplismQA serves as a scalable tool to identify ethical weaknesses in AI, highlighting the need for better alignment of LLMs with medical ethical standards.

**Abstract:** The integration of large language models into healthcare necessitates a rigorous evaluation of their ethical reasoning, an area current benchmarks often overlook. We introduce PrinciplismQA, a comprehensive benchmark with 3,648 questions designed to systematically assess LLMs' alignment with core medical ethics. Grounded in Principlism, our benchmark features a high-quality dataset. This includes multiple-choice questions curated from authoritative textbooks and open-ended questions sourced from authoritative medical ethics case study literature, all validated by medical experts. Our experiments reveal a significant gap between models' ethical knowledge and their practical application, especially in dynamically applying ethical principles to real-world scenarios. Most LLMs struggle with dilemmas concerning Beneficence, often over-emphasizing other principles. Frontier closed-source models, driven by strong general capabilities, currently lead the benchmark. Notably, medical domain fine-tuning can enhance models' overall ethical competence, but further progress requires better alignment with medical ethical knowledge. PrinciplismQA offers a scalable framework to diagnose these specific ethical weaknesses, paving the way for more balanced and responsible medical AI.

</details>


### [42] [ATLANTIS at SemEval-2025 Task 3: Detecting Hallucinated Text Spans in Question Answering](https://arxiv.org/abs/2508.05179)

*Catherine Kobus, François Lancelot, Marion-Cécile Martin, Nawal Ould Amer*

**Main category:** cs.CL

**Keywords:** hallucination detection, question answering, large language models

**Relevance Score:** 8

**TL;DR:** The paper discusses methods for detecting hallucinated text spans in question answering systems using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs are prone to generating hallucinated content, which can mislead users, necessitating effective detection methods.

**Method:** The study explores methods using few-shot prompting and token-level classification, with models fine-tuned on synthetic data, both with and without external context.

**Key Contributions:**

	1. Developed effective detection methods for hallucinated text spans.
	2. Demonstrated the impact of context integration on model accuracy.
	3. Achieved competitive results across multiple languages.

**Result:** The approaches achieved top rankings in Spanish and competitive placements in English and German for the task of detecting hallucinations.

**Limitations:** 

**Conclusion:** Integrating relevant context is crucial for reducing hallucination instances, and fine-tuning and prompt engineering can enhance model performance.

**Abstract:** This paper presents the contributions of the ATLANTIS team to SemEval-2025 Task 3, focusing on detecting hallucinated text spans in question answering systems. Large Language Models (LLMs) have significantly advanced Natural Language Generation (NLG) but remain susceptible to hallucinations, generating incorrect or misleading content. To address this, we explored methods both with and without external context, utilizing few-shot prompting with a LLM, token-level classification or LLM fine-tuned on synthetic data. Notably, our approaches achieved top rankings in Spanish and competitive placements in English and German. This work highlights the importance of integrating relevant context to mitigate hallucinations and demonstrate the potential of fine-tuned models and prompt engineering.

</details>


### [43] [Resource-Limited Joint Multimodal Sentiment Reasoning and Classification via Chain-of-Thought Enhancement and Distillation](https://arxiv.org/abs/2508.05234)

*Haonan Shangguan, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang, Ge Yu*

**Main category:** cs.CL

**Keywords:** Multimodal Sentiment Analysis, Resource-Limited, Large Language Models

**Relevance Score:** 7

**TL;DR:** This paper introduces the MulCoT-RD model for joint multimodal sentiment reasoning and classification in resource-constrained environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle multimodal sentiment analysis (MSA) challenges in resource-limited settings using lightweight models instead of heavy parameter ones.

**Method:** The MulCoT-RD model adopts a 'Teacher-Assistant-Student' distillation approach, where a high-performance MLLM generates initial reasoning data, an assistant model is trained with multi-task learning, and a lightweight student model performs reasoning and classification.

**Key Contributions:**

	1. Introduction of MulCoT-RD for joint reasoning and classification in MSA
	2. Use of a 'Teacher-Assistant-Student' distillation strategy
	3. Demonstrated strong performance with fewer parameters

**Result:** MulCoT-RD, with only 3B parameters, shows strong performance on the JMSRC task across four datasets, enhancing generalization and interpretability.

**Limitations:** 

**Conclusion:** The proposed model efficiently manages to generate reasoning chains and classify sentiments, making it suitable for deployment in resource-constrained environments.

**Abstract:** The surge in rich multimodal content on social media platforms has greatly advanced Multimodal Sentiment Analysis (MSA), with Large Language Models (LLMs) further accelerating progress in this field. Current approaches primarily leverage the knowledge and reasoning capabilities of parameter-heavy (Multimodal) LLMs for sentiment classification, overlooking autonomous multimodal sentiment reasoning generation in resource-constrained environments. Therefore, we focus on the Resource-Limited Joint Multimodal Sentiment Reasoning and Classification task, JMSRC, which simultaneously performs multimodal sentiment reasoning chain generation and sentiment classification only with a lightweight model. We propose a Multimodal Chain-of-Thought Reasoning Distillation model, MulCoT-RD, designed for JMSRC that employs a "Teacher-Assistant-Student" distillation paradigm to address deployment constraints in resource-limited environments. We first leverage a high-performance Multimodal Large Language Model (MLLM) to generate the initial reasoning dataset and train a medium-sized assistant model with a multi-task learning mechanism. A lightweight student model is jointly trained to perform efficient multimodal sentiment reasoning generation and classification. Extensive experiments on four datasets demonstrate that MulCoT-RD with only 3B parameters achieves strong performance on JMSRC, while exhibiting robust generalization and enhanced interpretability.

</details>


### [44] [Pruning Large Language Models by Identifying and Preserving Functional Networks](https://arxiv.org/abs/2508.05239)

*Yiheng Liu, Junhao Ning, Sichen Xia, Xiaohui Gao, Ning Qiang, Bao Ge, Junwei Han, Xintao Hu*

**Main category:** cs.CL

**Keywords:** structured pruning, large language models, functional networks, neural networks, model compression

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel structured pruning method for large language models that preserves functional networks to improve efficiency during model compression.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance degradation in structured pruning by recognizing the importance of interactions among neurons in large language models.

**Method:** Models are decomposed into functional networks, akin to human brain networks, allowing for targeted preservation of key neurons during pruning.

**Key Contributions:**

	1. Introduction of a novel pruning method based on preserving functional networks in LLMs.
	2. Demonstration of improved pruning performance by maintaining key interactions among neurons.
	3. Code availability for practical application and further research.

**Result:** The proposed pruning approach successfully identifies functional networks and key neurons in LLMs, leading to more efficient model compression with less performance degradation.

**Limitations:** 

**Conclusion:** Preserving functional networks enhances the structured pruning of LLMs, offering a promising direction for maintaining performance while reducing resource consumption.

**Abstract:** Structured pruning is one of the representative techniques for compressing large language models (LLMs) to reduce GPU memory consumption and accelerate inference speed. It offers significant practical value in improving the efficiency of LLMs in real-world applications. Current structured pruning methods typically rely on assessment of the importance of the structure units and pruning the units with less importance. Most of them overlooks the interaction and collaboration among artificial neurons that are crucial for the functionalities of LLMs, leading to a disruption in the macro functional architecture of LLMs and consequently a pruning performance degradation. Inspired by the inherent similarities between artificial neural networks and functional neural networks in the human brain, we alleviate this challenge and propose to prune LLMs by identifying and preserving functional networks within LLMs in this study. To achieve this, we treat an LLM as a digital brain and decompose the LLM into functional networks, analogous to identifying functional brain networks in neuroimaging data. Afterwards, an LLM is pruned by preserving the key neurons within these functional networks. Experimental results demonstrate that the proposed method can successfully identify and locate functional networks and key neurons in LLMs, enabling efficient model pruning. Our code is available at https://github.com/WhatAboutMyStar/LLM_ACTIVATION.

</details>


### [45] [CodeBoost: Boosting Code LLMs by Squeezing Knowledge from Code Snippets with RL](https://arxiv.org/abs/2508.05242)

*Sijie Wang, Quanjiang Guo, Kai Zhao, Yawei Zhang, Xin Li, Xiang Li, Siqi Li, Rui She, Shangshu Yu, Wee Peng Tay*

**Main category:** cs.CL

**Keywords:** Code LLMs, Post-training, Reinforcement learning, Heterogeneous augmentation, Error-aware prediction

**Relevance Score:** 8

**TL;DR:** CodeBoost enhances code LLMs without relying on human-annotated instructions by using code snippets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing models rely on difficult to scale human-annotated instructions, creating a bottleneck in instruction-based post-training.

**Method:** CodeBoost introduces maximum-clique curation, bi-directional prediction, error-aware prediction, heterogeneous augmentation, and heterogeneous rewarding.

**Key Contributions:**

	1. Maximum-clique curation of a diverse training corpus from code snippets.
	2. Bi-directional and error-aware prediction learning objectives.
	3. Heterogeneous rewarding incorporating multiple types of feedback.

**Result:** Extensive experiments show that CodeBoost improves performance across several code LLMs and benchmarks.

**Limitations:** 

**Conclusion:** CodeBoost proves to be a scalable and effective training pipeline for code LLMs.

**Abstract:** Code large language models (LLMs) have become indispensable tools for building efficient and automated coding pipelines. Existing models are typically post-trained using reinforcement learning (RL) from general-purpose LLMs using "human instruction-final answer" pairs, where the instructions are usually from manual annotations. However, collecting high-quality coding instructions is both labor-intensive and difficult to scale. On the other hand, code snippets are abundantly available from various sources. This imbalance presents a major bottleneck in instruction-based post-training. We propose CodeBoost, a post-training framework that enhances code LLMs purely from code snippets, without relying on human-annotated instructions. CodeBoost introduces the following key components: (1) maximum-clique curation, which selects a representative and diverse training corpus from code; (2) bi-directional prediction, which enables the model to learn from both forward and backward prediction objectives; (3) error-aware prediction, which incorporates learning signals from both correct and incorrect outputs; (4) heterogeneous augmentation, which diversifies the training distribution to enrich code semantics; and (5) heterogeneous rewarding, which guides model learning through multiple reward types including format correctness and execution feedback from both successes and failures. Extensive experiments across several code LLMs and benchmarks verify that CodeBoost consistently improves performance, demonstrating its effectiveness as a scalable and effective training pipeline.

</details>


### [46] [ASCoT: An Adaptive Self-Correction Chain-of-Thought Method for Late-Stage Fragility in LLMs](https://arxiv.org/abs/2508.05282)

*Dongxu Zhang, Ning Yang, Jihua Zhu, Jinnan Yang, Miao Xin, Baoliang Tian*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Large Language Models, Adaptive Self-Correction, Machine Learning, Error Analysis

**Relevance Score:** 9

**TL;DR:** This paper challenges the assumption that early errors in Chain-of-Thought prompting are most detrimental, introducing the concept of 'Late-Stage Fragility' where late errors are more harmful. It proposes the ASCoT method, which includes an Adaptive Verification Manager and a Multi-Perspective Self-Correction Engine to improve LLM reasoning accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the reliability challenge of reasoning chains in Large Language Models, which is critical for their effective application, especially in scenarios requiring high accuracy.

**Method:** The paper conducts error-injection experiments to investigate the impact of errors at different stages of Chain-of-Thought reasoning, subsequently proposing the Adaptive Self-Correction Chain-of-Thought (ASCoT) method to enhance error correction.

**Key Contributions:**

	1. Challenges the 'cascading failure' hypothesis in LLM reasoning
	2. Introduces the 'Late-Stage Fragility' concept in reasoning chains
	3. Proposes the ASCoT method for adaptive correction of errors

**Result:** ASCoT significantly outperforms standard Chain-of-Thought prompting methods on benchmarks like GSM8K and MATH, demonstrating improved reasoning accuracy.

**Limitations:** 

**Conclusion:** The findings highlight the importance of identifying specific failure modes in LLM reasoning and favor a shift towards adaptive correction strategies rather than uniform strategies.

**Abstract:** Chain-of-Thought (CoT) prompting has significantly advanced the reasoning capabilities of Large Language Models (LLMs), yet the reliability of these reasoning chains remains a critical challenge. A widely held "cascading failure" hypothesis suggests that errors are most detrimental when they occur early in the reasoning process. This paper challenges that assumption through systematic error-injection experiments, revealing a counter-intuitive phenomenon we term "Late-Stage Fragility": errors introduced in the later stages of a CoT chain are significantly more likely to corrupt the final answer than identical errors made at the beginning. To address this specific vulnerability, we introduce the Adaptive Self-Correction Chain-of-Thought (ASCoT) method. ASCoT employs a modular pipeline in which an Adaptive Verification Manager (AVM) operates first, followed by the Multi-Perspective Self-Correction Engine (MSCE). The AVM leverages a Positional Impact Score function I(k) that assigns different weights based on the position within the reasoning chains, addressing the Late-Stage Fragility issue by identifying and prioritizing high-risk, late-stage steps. Once these critical steps are identified, the MSCE applies robust, dual-path correction specifically to the failure parts. Extensive experiments on benchmarks such as GSM8K and MATH demonstrate that ASCoT achieves outstanding accuracy, outperforming strong baselines, including standard CoT. Our work underscores the importance of diagnosing specific failure modes in LLM reasoning and advocates for a shift from uniform verification strategies to adaptive, vulnerability-aware correction mechanisms.

</details>


### [47] [Decision-Making with Deliberation: Meta-reviewing as a Document-grounded Dialogue](https://arxiv.org/abs/2508.05283)

*Sukannya Purkayastha, Nils Dycke, Anne Lauscher, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** meta-reviewing, dialogue agents, synthetic data, peer review, large language models

**Relevance Score:** 8

**TL;DR:** This paper explores the development of dialogue agents to assist in the meta-reviewing process of academic papers, leveraging synthetic data generated by LLMs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Meta-reviewing plays a critical role in the peer-review process, yet there is a lack of effective tools to assist reviewers in decision-making.

**Method:** The authors generate synthetic data using Large Language Models to address data scarcity in training dialogue agents, and then train these agents specifically for the meta-reviewing task.

**Key Contributions:**

	1. Development of dialogue agents specifically for meta-reviewing using synthetic data.
	2. Improvement in the quality of synthetic data through a self-refinement strategy.
	3. Demonstration of the effectiveness of these agents in practical scenarios.

**Result:** The study finds that the dialogue agents trained with synthetic data outperform typical LLM-based assistants and enhance efficiency in real-world meta-reviewing scenarios.

**Limitations:** The study may be limited by the quality of synthetic data and the generalizability of the results across different domains.

**Conclusion:** The research indicates the efficacy of leveraging dialogue agents and synthetic data in improving the meta-reviewing process, suggesting a new approach to assist decision-making in peer-review.

**Abstract:** Meta-reviewing is a pivotal stage in the peer-review process, serving as the final step in determining whether a paper is recommended for acceptance. Prior research on meta-reviewing has treated this as a summarization problem over review reports. However, complementary to this perspective, meta-reviewing is a decision-making process that requires weighing reviewer arguments and placing them within a broader context. Prior research has demonstrated that decision-makers can be effectively assisted in such scenarios via dialogue agents. In line with this framing, we explore the practical challenges for realizing dialog agents that can effectively assist meta-reviewers. Concretely, we first address the issue of data scarcity for training dialogue agents by generating synthetic data using Large Language Models (LLMs) based on a self-refinement strategy to improve the relevance of these dialogues to expert domains. Our experiments demonstrate that this method produces higher-quality synthetic data and can serve as a valuable resource towards training meta-reviewing assistants. Subsequently, we utilize this data to train dialogue agents tailored for meta-reviewing and find that these agents outperform \emph{off-the-shelf} LLM-based assistants for this task. Finally, we apply our agents in real-world meta-reviewing scenarios and confirm their effectiveness in enhancing the efficiency of meta-reviewing.\footnote{Code and Data: https://github.com/UKPLab/arxiv2025-meta-review-as-dialog

</details>


### [48] [SONAR-LLM: Autoregressive Transformer that Thinks in Sentence Embeddings and Speaks in Tokens](https://arxiv.org/abs/2508.05305)

*Nikita Dragunov, Temurbek Rahmatullaev, Elizaveta Goncharova, Andrey Kuznetsov, Anton Razzhigaev*

**Main category:** cs.CL

**Keywords:** Large Concept Model, SONAR-LLM, text generation, transformer architecture, token-level cross-entropy

**Relevance Score:** 8

**TL;DR:** SONAR-LLM is a decoder-only transformer that generates text using token-level cross-entropy while retaining the semantic abstractness of the Large Concept Model (LCM).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve text generation quality by retaining semantic abstraction while eliminating diffusion sampling from the LCM architecture.

**Method:** SONAR-LLM uses a decoder-only transformer architecture, employing a hybrid objective that combines token-level cross-entropy with a frozen SONAR decoder.

**Key Contributions:**

	1. Introduces the SONAR-LLM, a novel text generation model leveraging token-level cross-entropy.
	2. Maintains semantic abstraction while avoiding the limitations of diffusion sampling in LCM.
	3. Provides open-source resources to aid reproducibility in future research.

**Result:** SONAR-LLM achieves competitive text generation quality across various model sizes (39M to 1.3B parameters).

**Limitations:** 

**Conclusion:** The proposed SONAR-LLM architecture not only improves generation quality but also supports reproducibility with released training code and pretrained checkpoints.

**Abstract:** The recently proposed Large Concept Model (LCM) generates text by predicting a sequence of sentence-level embeddings and training with either mean-squared error or diffusion objectives. We present SONAR-LLM, a decoder-only transformer that "thinks" in the same continuous SONAR embedding space, yet is supervised through token-level cross-entropy propagated via the frozen SONAR decoder. This hybrid objective retains the semantic abstraction of LCM while eliminating its diffusion sampler and restoring a likelihood-based training signal. Across model sizes from 39M to 1.3B parameters, SONAR-LLM attains competitive generation quality. We report scaling trends, ablations, benchmark results, and release the complete training code and all pretrained checkpoints to foster reproducibility and future research.

</details>


### [49] [Efficient Reasoning for Large Reasoning Language Models via Certainty-Guided Reflection Suppression](https://arxiv.org/abs/2508.05337)

*Jiameng Huang, Baijiong Lin, Guhao Feng, Jierun Chen, Di He, Lu Hou*

**Main category:** cs.CL

**Keywords:** Large Reasoning Language Models, Reflection Suppression, Efficient Reasoning, Token Usage, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper introduces Certainty-Guided Reflection Suppression (CGRS), a method to reduce overthinking in Large Reasoning Language Models (LRLMs) while maintaining reasoning accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LRLMs often face the overthinking problem due to redundant reasoning prompted by specific trigger words, leading to increased token usage and reduced practical utility.

**Method:** CGRS suppresses the generation of reflection triggers when the model shows high confidence, preventing unnecessary reflection cycles without degrading output quality. It is model-agnostic and does not require retraining or architectural changes.

**Key Contributions:**

	1. Introduction of Certainty-Guided Reflection Suppression (CGRS)
	2. Demonstrated effectiveness in reducing token usage
	3. Maintained accuracy without architectural changes

**Result:** CGRS reduces token usage by 18.5% to 41.9% while preserving accuracy across multiple reasoning benchmarks.

**Limitations:** 

**Conclusion:** The method effectively balances length reduction and performance, validated across various model architectures and scales, thereby providing a practical solution for efficient reasoning in LRLMs.

**Abstract:** Recent Large Reasoning Language Models (LRLMs) employ long chain-of-thought reasoning with complex reflection behaviors, typically signaled by specific trigger words (e.g., "Wait" and "Alternatively") to enhance performance. However, these reflection behaviors can lead to the overthinking problem where the generation of redundant reasoning steps that unnecessarily increase token usage, raise inference costs, and reduce practical utility. In this paper, we propose Certainty-Guided Reflection Suppression (CGRS), a novel method that mitigates overthinking in LRLMs while maintaining reasoning accuracy. CGRS operates by dynamically suppressing the model's generation of reflection triggers when it exhibits high confidence in its current response, thereby preventing redundant reflection cycles without compromising output quality. Our approach is model-agnostic, requires no retraining or architectural modifications, and can be integrated seamlessly with existing autoregressive generation pipelines. Extensive experiments across four reasoning benchmarks (i.e., AIME24, AMC23, MATH500, and GPQA-D) demonstrate CGRS's effectiveness: it reduces token usage by an average of 18.5% to 41.9% while preserving accuracy. It also achieves the optimal balance between length reduction and performance compared to state-of-the-art baselines. These results hold consistently across model architectures (e.g., DeepSeek-R1-Distill series, QwQ-32B, and Qwen3 family) and scales (4B to 32B parameters), highlighting CGRS's practical value for efficient reasoning.

</details>


### [50] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)

*Fenya Wasserroth, Eleftherios Avramidis, Vera Czehmann, Tanja Kojic, Fabrizio Nunnari, Sebastian Möller*

**Main category:** cs.CL

**Keywords:** sign language avatar, user experience, Hololens 2, adjustable features, comprehensibility

**Relevance Score:** 6

**TL;DR:** Investigation of sign language avatar adjustments on Hololens 2 shows no significant UX improvement and highlights the need for better comprehensibility and animation quality.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the impact of adjustable features on sign language avatars and their effect on user experience and comprehensibility.

**Method:** Detailed analysis of interactions of expert DGS users with adjustable vs non-adjustable avatars in specific use cases.

**Key Contributions:**

	1. Identification of factors affecting SL avatar usability
	2. Empirical findings on user preferences and interaction issues
	3. Recommendations for enhancing SL avatar design

**Result:** Users preferred adjustable settings, but no significant improvements in UX or comprehensibility were observed, with noted implementation issues and missing SL elements.

**Limitations:** Low UX and comprehensibility ratings due to implementation issues and missing features.

**Conclusion:** Personalization is not enough; SL avatars must be inherently comprehensible, and design improvements are necessary.

**Abstract:** This paper presents an investigation into the impact of adding adjustment features to an existing sign language (SL) avatar on a Microsoft Hololens 2 device. Through a detailed analysis of interactions of expert German Sign Language (DGS) users with both adjustable and non-adjustable avatars in a specific use case, this study identifies the key factors influencing the comprehensibility, the user experience (UX), and the acceptability of such a system. Despite user preference for adjustable settings, no significant improvements in UX or comprehensibility were observed, which remained at low levels, amid missing SL elements (mouthings and facial expressions) and implementation issues (indistinct hand shapes, lack of feedback and menu positioning). Hedonic quality was rated higher than pragmatic quality, indicating that users found the system more emotionally or aesthetically pleasing than functionally useful. Stress levels were higher for the adjustable avatar, reflecting lower performance, greater effort and more frustration. Additionally, concerns were raised about whether the Hololens adjustment gestures are intuitive and easy to familiarise oneself with. While acceptability of the concept of adjustability was generally positive, it was strongly dependent on usability and animation quality. This study highlights that personalisation alone is insufficient, and that SL avatars must be comprehensible by default. Key recommendations include enhancing mouthing and facial animation, improving interaction interfaces, and applying participatory design.

</details>


### [51] [Can Language Models Critique Themselves? Investigating Self-Feedback for Retrieval Augmented Generation at BioASQ 2025](https://arxiv.org/abs/2508.05366)

*Samy Ateia, Udo Kruschwitz*

**Main category:** cs.CL

**Keywords:** Agentic Retrieval, Large Language Models, Biomedical Research, Self-feedback Mechanism, Professional Search

**Relevance Score:** 9

**TL;DR:** This paper explores the performance of LLMs in autonomous search processes for biomedical research, focusing on a self-feedback mechanism for iterative output refinement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges of applying RAG systems in domain-specific professional search, particularly in biomedical research, where high user expertise and transparency are essential.

**Method:** The study involved testing reasoning and nonreasoning LLMs (Gemini-Flash 2.0, o3-mini, o4-mini, DeepSeek-R1) using a self-feedback mechanism for query expansion and answer generation across multiple types.

**Key Contributions:**

	1. Investigation of LLM performance in biomedical professional search
	2. Implementation of a self-feedback mechanism in output generation
	3. Analysis of reasoning vs. non-reasoning LLM capabilities.

**Result:** Preliminary results show varied performance of the self-feedback strategy across different models and tasks, with indications that reasoning models may provide better feedback.

**Limitations:** 

**Conclusion:** This research provides insights into LLM self-correction mechanisms and sets the stage for future comparisons between LLM-generated feedback and expert input.

**Abstract:** Agentic Retrieval Augmented Generation (RAG) and 'deep research' systems aim to enable autonomous search processes where Large Language Models (LLMs) iteratively refine outputs. However, applying these systems to domain-specific professional search, such as biomedical research, presents challenges, as automated systems may reduce user involvement and misalign with expert information needs. Professional search tasks often demand high levels of user expertise and transparency. The BioASQ CLEF 2025 challenge, using expert-formulated questions, can serve as a platform to study these issues. We explored the performance of current reasoning and nonreasoning LLMs like Gemini-Flash 2.0, o3-mini, o4-mini and DeepSeek-R1. A key aspect of our methodology was a self-feedback mechanism where LLMs generated, evaluated, and then refined their outputs for query expansion and for multiple answer types (yes/no, factoid, list, ideal). We investigated whether this iterative self-correction improves performance and if reasoning models are more capable of generating useful feedback. Preliminary results indicate varied performance for the self-feedback strategy across models and tasks. This work offers insights into LLM self-correction and informs future work on comparing the effectiveness of LLM-generated feedback with direct human expert input in these search systems.

</details>


### [52] [Evaluation of a Sign Language Avatar on Comprehensibility, User Experience \& Acceptability](https://arxiv.org/abs/2508.05358)

*Fenya Wasserroth, Eleftherios Avramidis, Vera Czehmann, Tanja Kojic, Fabrizio Nunnari, Sebastian Möller*

**Main category:** cs.CL

**Keywords:** Sign Language Avatars, User Experience, Augmented Reality, Adjustability, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** Investigation of adjustable sign language avatars on Microsoft Hololens 2 shows no significant UX improvements despite user preference for adjustability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the impact of adjustable features on sign language avatars for enhancing user experience and comprehensibility.

**Method:** Detailed analysis of interactions of expert German Sign Language users with adjustable and non-adjustable avatars in a specific context.

**Key Contributions:**

	1. Investigation of SL avatar adjustability in AR settings
	2. User experience analysis between adjustable and non-adjustable avatars
	3. Recommendations for improving sign language avatar design

**Result:** Users preferred adjustable avatars but reported low levels of UX and comprehensibility. High stress levels associated with adjustable avatars indicated lower performance and increased frustration.

**Limitations:** Issues with missing SL elements and indistinct hand shapes, along with usability concerns regarding adjustment gestures.

**Conclusion:** Personalization alone is insufficient; comprehensibility of SL avatars must be improved by enhancing mouthing and facial animations and better designing interaction interfaces.

**Abstract:** This paper presents an investigation into the impact of adding adjustment features to an existing sign language (SL) avatar on a Microsoft Hololens 2 device. Through a detailed analysis of interactions of expert German Sign Language (DGS) users with both adjustable and non-adjustable avatars in a specific use case, this study identifies the key factors influencing the comprehensibility, the user experience (UX), and the acceptability of such a system. Despite user preference for adjustable settings, no significant improvements in UX or comprehensibility were observed, which remained at low levels, amid missing SL elements (mouthings and facial expressions) and implementation issues (indistinct hand shapes, lack of feedback and menu positioning). Hedonic quality was rated higher than pragmatic quality, indicating that users found the system more emotionally or aesthetically pleasing than functionally useful. Stress levels were higher for the adjustable avatar, reflecting lower performance, greater effort and more frustration. Additionally, concerns were raised about whether the Hololens adjustment gestures are intuitive and easy to familiarise oneself with. While acceptability of the concept of adjustability was generally positive, it was strongly dependent on usability and animation quality. This study highlights that personalisation alone is insufficient, and that SL avatars must be comprehensible by default. Key recommendations include enhancing mouthing and facial animation, improving interaction interfaces, and applying participatory design.

</details>


### [53] [The TUB Sign Language Corpus Collection](https://arxiv.org/abs/2508.05374)

*Eleftherios Avramidis, Vera Czehmann, Fabian Deckert, Lorenz Hufe, Aljoscha Lipski, Yuni Amaloa Quintero Villalobos, Tae Kwon Rhee, Mengqian Shi, Lennart Stölting, Fabrizio Nunnari, Sebastian Möller*

**Main category:** cs.CL

**Keywords:** sign languages, parallel corpora, video data, language processing, data collection

**Relevance Score:** 4

**TL;DR:** This paper presents a large collection of parallel corpora for 12 sign languages, featuring over 1,300 hours of video and extensive subtitles, significantly advancing resources in this field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the availability and consistency of parallel corpora for sign languages, particularly for research and applications in sign language processing and interpretation.

**Method:** The collection was created through systematic data collection from various online sources, involving video scraping, cropping, and subtitle generation with a multi-stage preparation process.

**Key Contributions:**

	1. First consistent parallel corpora for 8 Latin American sign languages
	2. German Sign Language corpus is significantly larger than previous datasets
	3. Comprehensive statistics and methods for data collection presented

**Result:** The final collection consists of 1,300 hours of video across 4,381 files, paired with 1.3 million subtitles containing 14 million tokens, including significant contributions to Latin American sign languages and a substantial German Sign Language corpus.

**Limitations:** 

**Conclusion:** This resource is a crucial step towards improving sign language data availability for research and practical applications.

**Abstract:** We present a collection of parallel corpora of 12 sign languages in video format, together with subtitles in the dominant spoken languages of the corresponding countries. The entire collection includes more than 1,300 hours in 4,381 video files, accompanied by 1,3~M subtitles containing 14~M tokens. Most notably, it includes the first consistent parallel corpora for 8 Latin American sign languages, whereas the size of the German Sign Language corpora is ten times the size of the previously available corpora. The collection was created by collecting and processing videos of multiple sign languages from various online sources, mainly broadcast material of news shows, governmental bodies and educational channels. The preparation involved several stages, including data collection, informing the content creators and seeking usage approvals, scraping, and cropping. The paper provides statistics on the collection and an overview of the methods used to collect the data.

</details>


### [54] [MyCulture: Exploring Malaysia's Diverse Culture under Low-Resource Language Constraints](https://arxiv.org/abs/2508.05429)

*Zhong Ken Hew, Jia Xin Low, Sze Jue Yang, Chee Seng chan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Bias, Benchmarking, Malaysian Culture, Open-ended Questions

**Relevance Score:** 9

**TL;DR:** Introducing MyCulture, a new benchmark for evaluating LLMs on Malaysian cultural understanding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address cultural biases in LLMs due to the prevalence of training data from high-resource languages.

**Method:** MyCulture uses open-ended multiple-choice questions without predefined options to assess LLMs on six cultural pillars in Bahasa Melayu.

**Key Contributions:**

	1. MyCulture benchmark assesses cultural understanding of LLMs in low-resource settings.
	2. Innovative open-ended question format reduces guessing and bias.
	3. Highlights disparities in LLM cultural comprehension across different languages.

**Result:** Significant disparities in cultural comprehension were found across LLMs when evaluated with MyCulture, exposing the limitations of conventional benchmarks.

**Limitations:** 

**Conclusion:** There is an urgent need for culturally grounded benchmarks to improve fairness and inclusivity in LLM evaluations.

**Abstract:** Large Language Models (LLMs) often exhibit cultural biases due to training data dominated by high-resource languages like English and Chinese. This poses challenges for accurately representing and evaluating diverse cultural contexts, particularly in low-resource language settings. To address this, we introduce MyCulture, a benchmark designed to comprehensively evaluate LLMs on Malaysian culture across six pillars: arts, attire, customs, entertainment, food, and religion presented in Bahasa Melayu. Unlike conventional benchmarks, MyCulture employs a novel open-ended multiple-choice question format without predefined options, thereby reducing guessing and mitigating format bias. We provide a theoretical justification for the effectiveness of this open-ended structure in improving both fairness and discriminative power. Furthermore, we analyze structural bias by comparing model performance on structured versus free-form outputs, and assess language bias through multilingual prompt variations. Our evaluation across a range of regional and international LLMs reveals significant disparities in cultural comprehension, highlighting the urgent need for culturally grounded and linguistically inclusive benchmarks in the development and assessment of LLMs.

</details>


### [55] [LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models](https://arxiv.org/abs/2508.05452)

*Ming Zhang, Yujiong Shen, Jingyi Deng, Yuhui Wang, Yue Zhang, Junzhe Wang, Shichun Liu, Shihan Dou, Huayu Sha, Qiyuan Peng, Changhao Jiang, Jingqi Tong, Yilong Wu, Zhihao Zhang, Mingqi Wu, Zhiheng Xi, Mingxu Chai, Tao Liang, Zhihui Fei, Zhen Wang, Mingyang Wan, Guojun Ma, Tao Gui, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation, dynamic assessment, data contamination, robustness

**Relevance Score:** 8

**TL;DR:** LLMEval-3 introduces a dynamic evaluation framework for Large Language Models (LLMs) that addresses issues like data contamination and leaderboard overfitting, using a proprietary bank of questions and automated methodologies to ensure integrity and robust ranking.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate the vulnerabilities in current static evaluations of LLMs that lead to misrepresented model capabilities and leaderboards.

**Method:** LLMEval-3 utilizes a dynamic sampling method from a proprietary bank of 220k graduate-level questions, employing automated data curation and evaluation processes to maintain integrity.

**Key Contributions:**

	1. Dynamic evaluation framework for LLMs
	2. Robust anti-cheating architecture
	3. Automated contamination-resistant data curation

**Result:** A longitudinal study of nearly 50 leading models identified a performance ceiling in knowledge memorization and highlighted data contamination issues, proving that static benchmarks can miss critical vulnerabilities.

**Limitations:** 

**Conclusion:** LLMEval-3 establishes a more credible methodology for LLM evaluation, advancing the standards for assessing AI model performance beyond traditional leaderboard metrics.

**Abstract:** Existing evaluation of Large Language Models (LLMs) on static benchmarks is vulnerable to data contamination and leaderboard overfitting, critical issues that obscure true model capabilities. To address this, we introduce LLMEval-3, a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary bank of 220k graduate-level questions, from which it dynamically samples unseen test sets for each evaluation run. Its automated pipeline ensures integrity via contamination-resistant data curation, a novel anti-cheating architecture, and a calibrated LLM-as-a-judge process achieving 90% agreement with human experts, complemented by a relative ranking system for fair comparison. An 20-month longitudinal study of nearly 50 leading models reveals a performance ceiling on knowledge memorization and exposes data contamination vulnerabilities undetectable by static benchmarks. The framework demonstrates exceptional robustness in ranking stability and consistency, providing strong empirical validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and credible methodology for assessing the true capabilities of LLMs beyond leaderboard scores, promoting the development of more trustworthy evaluation standards.

</details>


### [56] [TASE: Token Awareness and Structured Evaluation for Multilingual Language Models](https://arxiv.org/abs/2508.05468)

*Chenzhuo Zhao, Xinda Wang, Yue Huang, Junting Lu, Ziqian Liu*

**Main category:** cs.CL

**Keywords:** large language models, token-level understanding, benchmark, structural reasoning, cross-lingual generalization

**Relevance Score:** 8

**TL;DR:** TASE is a benchmark to evaluate large language models' token-level understanding and structural reasoning across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models in token-level understanding and structural reasoning which are crucial for applications requiring precision.

**Method:** Introduction of TASE, covering 10 tasks in token awareness and structural understanding across Chinese, English, and Korean with a scalable synthetic data generation pipeline.

**Key Contributions:**

	1. Introduction of the TASE benchmark for evaluating LLMs' token-level abilities.
	2. In-depth analysis of LLMs' performance on structural reasoning tasks.
	3. Public availability of the benchmark dataset and code.

**Result:** Evaluation of over 30 leading LLMs showed that human performance significantly surpasses that of current models, indicating persistent weaknesses in token-level reasoning.

**Limitations:** Current models still show significant weaknesses compared to human performance in token-level reasoning tasks.

**Conclusion:** TASE highlights weaknesses in LLMs and provides a framework for future improvements in language understanding and cross-lingual generalization.

**Abstract:** While large language models (LLMs) have demonstrated remarkable performance on high-level semantic tasks, they often struggle with fine-grained, token-level understanding and structural reasoning--capabilities that are essential for applications requiring precision and control. We introduce TASE, a comprehensive benchmark designed to evaluate LLMs' ability to perceive and reason about token-level information across languages. TASE covers 10 tasks under two core categories: token awareness and structural understanding, spanning Chinese, English, and Korean, with a 35,927-instance evaluation set and a scalable synthetic data generation pipeline for training. Tasks include character counting, token alignment, syntactic structure parsing, and length constraint satisfaction. We evaluate over 30 leading commercial and open-source LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a custom Qwen2.5-14B model using the GRPO training method. Results show that human performance significantly outpaces current LLMs, revealing persistent weaknesses in token-level reasoning. TASE sheds light on these limitations and provides a new diagnostic lens for future improvements in low-level language understanding and cross-lingual generalization. Our code and dataset are publicly available at https://github.com/cyzcz/Tase .

</details>


### [57] [Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations](https://arxiv.org/abs/2508.05470)

*Li-Chun Lu, Miri Liu, Pin-Chun Lu, Yufei Tian, Shao-Hua Sun, Nanyun Peng*

**Main category:** cs.CL

**Keywords:** creativity measures, creativity index, LLM-as-a-Judge, evaluation frameworks, human creativity

**Relevance Score:** 8

**TL;DR:** This paper analyzes and compares various creativity measures across different domains, revealing their inconsistencies and limitations, and calls for improved evaluation frameworks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effectiveness of different creativity measures in capturing diverse aspects of creativity and to identify their limitations.

**Method:** A systematic examination and comparison of creativity measures: creativity index, perplexity, syntactic templates, and LLM-as-a-Judge across diverse creative domains.

**Key Contributions:**

	1. Comparison of various creativity metrics across domains
	2. Identification of specific limitations of existing metrics
	3. Call for improved evaluation frameworks for creativity

**Result:** The analysis shows that these creativity measures demonstrate limited consistency and capture different dimensions of creativity, highlighting specific weaknesses in each approach.

**Limitations:** Measures exhibit instability, bias, and focus on narrow aspects of creativity, such as lexical diversity.

**Conclusion:** There is a pressing need for the development of more robust and generalizable evaluation frameworks that align better with human judgments of creativity.

**Abstract:** We systematically examine, analyze, and compare representative creativity measures--creativity index, perplexity, syntactic templates, and LLM-as-a-Judge--across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity index's focus on lexical diversity, perplexity's sensitivity to model confidence, and syntactic templates' inability to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.

</details>


### [58] [LAG: Logic-Augmented Generation from a Cartesian Perspective](https://arxiv.org/abs/2508.05509)

*Yilin Xiao, Chuang Zhou, Qinggang Zhang, Su Dong, Shengyuan Chen, Xiao Huang*

**Main category:** cs.CL

**Keywords:** Logic-Augmented Generation, Large language models, Knowledge augmentation, Reasoning robustness, Hallucination reduction

**Relevance Score:** 9

**TL;DR:** The paper introduces Logic-Augmented Generation (LAG), a new approach that improves knowledge augmentation in LLMs by systematically decomposing complex questions and enhancing reasoning capabilities, addressing limitations in retrieval-augmented generation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of large language models in knowledge-intensive tasks and reduce hallucinations during complex reasoning.

**Method:** LAG decomposes complex questions into atomic sub-questions based on logical dependencies, resolves them sequentially while guiding context retrieval, and incorporates a logical termination mechanism to prevent error propagation.

**Key Contributions:**

	1. Introduction of the Logic-Augmented Generation paradigm
	2. Systematic question decomposition and dependency-aware reasoning
	3. Logical termination mechanism to prevent error propagation

**Result:** LAG significantly enhances reasoning robustness and reduces hallucinations, aligning LLM problem-solving with human cognition based on experiments on four benchmark datasets.

**Limitations:** 

**Conclusion:** LAG offers a principled alternative to existing retrieval-augmented generation methods, improving the reliability of responses from LLMs.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet exhibit critical limitations in knowledge-intensive tasks, often generating hallucinations when faced with questions requiring specialized expertise. While retrieval-augmented generation (RAG) mitigates this by integrating external knowledge, it struggles with complex reasoning scenarios due to its reliance on direct semantic retrieval and lack of structured logical organization. Inspired by Cartesian principles from \textit{Discours de la m\'ethode}, this paper introduces Logic-Augmented Generation (LAG), a novel paradigm that reframes knowledge augmentation through systematic question decomposition and dependency-aware reasoning. Specifically, LAG first decomposes complex questions into atomic sub-questions ordered by logical dependencies. It then resolves these sequentially, using prior answers to guide context retrieval for subsequent sub-questions, ensuring stepwise grounding in logical chain. To prevent error propagation, LAG incorporates a logical termination mechanism that halts inference upon encountering unanswerable sub-questions and reduces wasted computation on excessive reasoning. Finally, it synthesizes all sub-resolutions to generate verified responses. Experiments on four benchmark datasets demonstrate that LAG significantly enhances reasoning robustness, reduces hallucination, and aligns LLM problem-solving with human cognition, offering a principled alternative to existing RAG systems.

</details>


### [59] [The World According to LLMs: How Geographic Origin Influences LLMs' Entity Deduction Capabilities](https://arxiv.org/abs/2508.05525)

*Harsh Nishant Lalai, Raj Sanjay Shah, Jiaxin Pei, Sashank Varma, Yi-Chia Wang, Ali Emami*

**Main category:** cs.CL

**Keywords:** Large Language Models, implicit bias, 20 Questions game, Geo20Q+, geographic disparities

**Relevance Score:** 9

**TL;DR:** This paper investigates subtle implicit biases in Large Language Models (LLMs) by analyzing their performance in a multi-turn game called 20 Questions, revealing geographic disparities in entity deduction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the implicit biases in LLMs that are often overlooked in conventional evaluations, particularly focusing on geographic performance in a multi-turn context.

**Method:** The authors propose using the 20 Questions game as a creative evaluation framework, testing various LLMs on a new dataset, Geo20Q+, which includes entities from diverse regions and evaluates performance in multiple languages.

**Key Contributions:**

	1. Introduction of Geo20Q+, a dataset to evaluate LLMs in a multi-turn game context.
	2. Demonstration of geographic disparities in LLM performance that are not evident in standard evaluations.
	3. Release of tools and dataset for further research into LLM biases.

**Result:** The evaluation reveals that LLMs perform significantly better in recognizing entities from the Global North compared to the Global South, with mild correlations observed with Wikipedia pageviews and corpus frequency but insufficient to explain the disparities.

**Limitations:** The paper primarily addresses geographic disparities; other forms of biases may not be explored and the findings may not generalize across all LLMs.

**Conclusion:** This study highlights the importance of nuanced evaluation methods to uncover biases in LLMs, providing insights into their reasoning processes and the geographic and cultural biases that may be present.

**Abstract:** Large Language Models (LLMs) have been extensively tuned to mitigate explicit biases, yet they often exhibit subtle implicit biases rooted in their pre-training data. Rather than directly probing LLMs with human-crafted questions that may trigger guardrails, we propose studying how models behave when they proactively ask questions themselves. The 20 Questions game, a multi-turn deduction task, serves as an ideal testbed for this purpose. We systematically evaluate geographic performance disparities in entity deduction using a new dataset, Geo20Q+, consisting of both notable people and culturally significant objects (e.g., foods, landmarks, animals) from diverse regions. We test popular LLMs across two gameplay configurations (canonical 20-question and unlimited turns) and in seven languages (English, Hindi, Mandarin, Japanese, French, Spanish, and Turkish). Our results reveal geographic disparities: LLMs are substantially more successful at deducing entities from the Global North than the Global South, and the Global West than the Global East. While Wikipedia pageviews and pre-training corpus frequency correlate mildly with performance, they fail to fully explain these disparities. Notably, the language in which the game is played has minimal impact on performance gaps. These findings demonstrate the value of creative, free-form evaluation frameworks for uncovering subtle biases in LLMs that remain hidden in standard prompting setups. By analyzing how models initiate and pursue reasoning goals over multiple turns, we find geographic and cultural disparities embedded in their reasoning processes. We release the dataset (Geo20Q+) and code at https://sites.google.com/view/llmbias20q/home.

</details>


### [60] [CoCoLex: Confidence-guided Copy-based Decoding for Grounded Legal Text Generation](https://arxiv.org/abs/2508.05534)

*Santosh T. Y. S. S, Youssef Tarek Elkhayat, Oana Ichim, Pranav Shetty, Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Xiaomo Liu*

**Main category:** cs.CL

**Keywords:** Legal Text Generation, Confidence-guided Decoding, Retrieval-Augmented Generation

**Relevance Score:** 6

**TL;DR:** CoCoLex is a decoding strategy for legal text generation that improves fidelity to context by combining model confidence with context-aware copying.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs in the Legal domain face challenges such as generating unfaithful outputs, hindering their adoption.

**Method:** CoCoLex uses a confidence-guided mechanism that interpolates model vocabulary distribution with context-derived distributions to enhance fidelity.

**Key Contributions:**

	1. Introduction of Confidence-guided Copy-based Decoding (CoCoLex) for legal text generation
	2. Demonstration of improved performance over existing context-aware methods
	3. Focus on ensuring context fidelity in LLM outputs

**Result:** Experimental results show that CoCoLex outperforms existing decoding methods, especially in long-form generation tasks on five legal benchmarks.

**Limitations:** 

**Conclusion:** The proposed method ensures greater fidelity to the context, making it a viable solution for legal text generation challenges.

**Abstract:** Due to their ability to process long and complex contexts, LLMs can offer key benefits to the Legal domain, but their adoption has been hindered by their tendency to generate unfaithful, ungrounded, or hallucinatory outputs. While Retrieval-Augmented Generation offers a promising solution by grounding generations in external knowledge, it offers no guarantee that the provided context will be effectively integrated. To address this, context-aware decoding strategies have been proposed to amplify the influence of relevant context, but they usually do not explicitly enforce faithfulness to the context. In this work, we introduce Confidence-guided Copy-based Decoding for Legal Text Generation (CoCoLex)-a decoding strategy that dynamically interpolates the model produced vocabulary distribution with a distribution derived based on copying from the context. CoCoLex encourages direct copying based on the model's confidence, ensuring greater fidelity to the source. Experimental results on five legal benchmarks demonstrate that CoCoLex outperforms existing context-aware decoding methods, particularly in long-form generation tasks.

</details>


### [61] [Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees](https://arxiv.org/abs/2508.05544)

*Guang Yang, Xinyang Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Quantification, Multiple-Choice Question Answering, Conformal Prediction, Predictive Entropy

**Relevance Score:** 9

**TL;DR:** This paper presents a frequency-based uncertainty quantification method for large language models in multiple-choice question answering, addressing inherent unreliability and enhancing trustworthiness through a novel framework.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Address the unreliability of large language models in high-risk domains, particularly in multiple-choice question answering where hallucination and overconfidence pose significant challenges.

**Method:** The proposed method uses multiple independent samplings of the model's output distribution to calculate predictive entropy (PE), leveraging conformal prediction to ensure provable coverage guarantees.

**Key Contributions:**

	1. Frequency-based uncertainty quantification method
	2. Model-agnostic framework for reliable uncertainty quantification
	3. Demonstrated effectiveness in black-box settings with empirical miscoverage control

**Result:** Experimental evaluations show that frequency-based predictive entropy outperforms logit-based predictive entropy in distinguishing between correct and incorrect predictions, achieving better AUROC scores.

**Limitations:** 

**Conclusion:** The study establishes a distribution-free, model-agnostic framework for reliable uncertainty quantification in multiple-choice question answering, enhancing the trustworthiness of LLMs in various applications.

**Abstract:** Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the model's output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications.

</details>


### [62] [Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs](https://arxiv.org/abs/2508.05553)

*Franziska Weeber, Tanise Ceron, Sebastian Padó*

**Main category:** cs.CL

**Keywords:** multilingual large language models, political opinions, cross-lingual transfer, alignment, socio-linguistic dynamics

**Relevance Score:** 5

**TL;DR:** This paper investigates whether political opinions in multilingual large language models (MLLMs) differ across languages and if opinions transfer between them.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine cross-lingual differences in political opinions in MLLMs and understand the effects of language alignment.

**Method:** The authors prompt various MLLMs in five Western languages with political statements from voting advice applications to assess their (dis)agreement.

**Key Contributions:**

	1. Analysis of cross-lingual opinion transfer in MLLMs
	2. Demonstrated effects of political alignment on language models
	3. Study of multilingual political sentiment representation

**Result:** Unaligned MLLMs show minimal significant cross-lingual differences; however, political alignment influences opinions uniformly across all languages.

**Limitations:** Focus on Western languages may not generalize to non-Western contexts and diverse socio-political settings.

**Conclusion:** Political opinions in MLLMs tend to transfer across languages, highlighting the difficulties in aligning MLLMs with socio-linguistic, cultural, and political contexts.

**Abstract:** Public opinion surveys show cross-cultural differences in political opinions between socio-cultural contexts. However, there is no clear evidence whether these differences translate to cross-lingual differences in multilingual large language models (MLLMs). We analyze whether opinions transfer between languages or whether there are separate opinions for each language in MLLMs of various sizes across five Western languages. We evaluate MLLMs' opinions by prompting them to report their (dis)agreement with political statements from voting advice applications. To better understand the interaction between languages in the models, we evaluate them both before and after aligning them with more left or right views using direct preference optimization and English alignment data only. Our findings reveal that unaligned models show only very few significant cross-lingual differences in the political opinions they reflect. The political alignment shifts opinions almost uniformly across all five languages. We conclude that in Western language contexts, political opinions transfer between languages, demonstrating the challenges in achieving explicit socio-linguistic, cultural, and political alignment of MLLMs.

</details>


### [63] [MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy](https://arxiv.org/abs/2508.05592)

*Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, Fei Tang*

**Main category:** cs.CL

**Keywords:** large language models, mathematical reasoning, synthetic data, reinforcement learning, problem generation

**Relevance Score:** 8

**TL;DR:** MathSmith is a framework for synthesizing challenging mathematical problems, aimed at improving the reasoning capabilities of large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in quality and diversity of training data for mathematical reasoning in LLMs.

**Method:** MathSmith constructs new mathematical problems from scratch using concept-explanation pairs, applies nine predefined strategies to increase difficulty, and utilizes reinforcement learning to optimize for structural validity and reasoning complexity.

**Key Contributions:**

	1. Development of MathSmith framework for problem synthesis
	2. Implementation of reinforcement learning for optimization
	3. Demonstrated scalability and effectiveness across multiple benchmarks

**Result:** MathSmith consistently outperforms existing baselines in generating mathematical problems across various difficulty benchmarks and shows strong scalability.

**Limitations:** 

**Conclusion:** High-difficulty synthetic data can significantly enhance LLM reasoning capabilities.

**Abstract:** Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy & medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities.

</details>


### [64] [Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2508.05613)

*Haitao Hong, Yuchen Yan, Xingyu Wu, Guiyang Hou, Wenqi Zhang, Weiming Lu, Yongliang Shen, Jun Xiao*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Reward Models, Large Language Models

**Relevance Score:** 7

**TL;DR:** Cooper is a reinforcement learning framework that optimizes both the policy and reward models to enhance reasoning in large language models while addressing limitations of existing reward paradigms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the robustness of reward frameworks in reinforcement learning for large language models and mitigate issues like reward hacking.

**Method:** Cooper optimizes policy and reward models simultaneously, utilizing rule-based rewards for precision and dynamically generates sample pairs for continued training of the reward model.

**Key Contributions:**

	1. Introduction of Cooper, a novel RL framework
	2. Development of VerifyRM, a reward model outperforming others
	3. Hybrid annotation strategy for efficient training data generation

**Result:** Cooper enhances robustness against reward hacking and shows a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct compared to previous models.

**Limitations:** 

**Conclusion:** The approach demonstrates that dynamically updating the reward model is effective in improving reinforcement learning performance and combating reward hacking.

**Abstract:** Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.

</details>


### [65] [Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis](https://arxiv.org/abs/2405.00708)

*Furui Cheng, Vilém Zouhar, Robin Shing Moon Chan, Daniel Fürst, Hendrik Strobelt, Mennatallah El-Assady*

**Main category:** cs.CL

**Keywords:** Large Language Models, Explainable AI, Counterfactual Analysis, User Interaction, Visualization

**Relevance Score:** 9

**TL;DR:** LLM Analyzer is an interactive visualization system for exploring LLM behaviors through counterfactual analysis, addressing limitations of current XAI methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of LLM behaviors and provide more effective means of explanation beyond traditional methods.

**Method:** An interactive visualization system that uses counterfactual analysis, featuring an algorithm for generating fluent counterfactuals and integration with a table-based visualization for dynamic analysis.

**Key Contributions:**

	1. Introduction of LLM Analyzer for interactive exploration of LLMs
	2. Use of counterfactuals for generating feature attribution scores
	3. Demonstrated usability and effectiveness through user studies

**Result:** User studies indicate that LLM Analyzer enhances understanding and usability for LLM practitioners by enabling active participation in the explanation process.

**Limitations:** The scope of the system may be limited by the choice of counterfactual generation operations and user-defined granularity levels.

**Conclusion:** Involving users actively in the explanation process improves the understanding of LLM behaviors and the effectiveness of XAI methods.

**Abstract:** Understanding the behavior of large language models (LLMs) is crucial for ensuring their safe and reliable use. However, existing explainable AI (XAI) methods for LLMs primarily rely on word-level explanations, which are often computationally inefficient and misaligned with human reasoning processes. Moreover, these methods often treat explanation as a one-time output, overlooking its inherently interactive and iterative nature. In this paper, we present LLM Analyzer, an interactive visualization system that addresses these limitations by enabling intuitive and efficient exploration of LLM behaviors through counterfactual analysis. Our system features a novel algorithm that generates fluent and semantically meaningful counterfactuals via targeted removal and replacement operations at user-defined levels of granularity. These counterfactuals are used to compute feature attribution scores, which are then integrated with concrete examples in a table-based visualization, supporting dynamic analysis of model behavior. A user study with LLM practitioners and interviews with experts demonstrate the system's usability and effectiveness, emphasizing the importance of involving humans in the explanation process as active participants rather than passive recipients.

</details>


### [66] [OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks](https://arxiv.org/abs/2508.05614)

*Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang*

**Main category:** cs.CL

**Keywords:** embodied AI, language models, reasoning, evaluation framework, multi-agent coordination

**Relevance Score:** 8

**TL;DR:** OmniEAR framework evaluates language models' reasoning in embodied tasks, revealing performance challenges in tool usage and multi-agent coordination.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how language models reason about physical interactions and coordination in embodied tasks, which is largely unexplored.

**Method:** The framework involves text-based environment representation in 1,500 scenarios, requiring agents to dynamically acquire capabilities and establish coordination strategies based on task demands.

**Key Contributions:**

	1. Introduction of OmniEAR as a comprehensive evaluation framework for embodied AI
	2. Identification of performance discrepancies between explicit instruction and dynamic reasoning
	3. Demonstration of architectural limitations in multi-agent coordination tasks

**Result:** Models perform well (85-96%) with explicit instructions but struggle significantly (56-85% for tool reasoning, 63-85% for implicit collaboration), highlighting architectural limitations in reasoning under constraints.

**Limitations:** Current models cannot filter task-relevant constraints despite having complete environmental information, leading to degraded performance in coordination.

**Conclusion:** OmniEAR establishes a new benchmark for embodied AI, demonstrating that current models have fundamental limitations in addressing embodied reasoning tasks.

**Abstract:** Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.

</details>


### [67] [Learning to Reason for Factuality](https://arxiv.org/abs/2508.05618)

*Xilun Chen, Ilia Kulikov, Vincent-Pierre Berges, Barlas Oğuz, Rulin Shao, Gargi Ghosh, Jason Weston, Wen-tau Yih*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Factuality, Natural Language Processing, Online Learning

**Relevance Score:** 9

**TL;DR:** The paper introduces a novel reward function for online reinforcement learning aimed at improving factual reasoning in large language models, reducing hallucinations and enhancing response detail without sacrificing helpfulness.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Although reasoning large language models have made strides in complex reasoning tasks, they produce more hallucinations compared to non-reasoning models, particularly on long-form factual benchmarks. This paper addresses the challenge of reliable verification methods in the context of online reinforcement learning to improve factual accuracy.

**Method:** The authors propose a new reward function that evaluates factual precision, response detail, and answer relevance while applying online reinforcement learning (RL) techniques.

**Key Contributions:**

	1. Introduction of a new reward function for online RL in long-form factuality settings
	2. Demonstration of significant reductions in hallucination rates
	3. Increase in response detail without degrading helpfulness

**Result:** The new factual reasoning model outperformed previous attempts, achieving an average reduction of 23.1 percentage points in hallucination rates and a 23% increase in answer detail level, with no decline in helpfulness on six long-form factuality benchmarks.

**Limitations:** 

**Conclusion:** The proposed model enhances the quality of factual reasoning in large language models by effectively minimizing hallucinations and improving detail in responses through a well-defined reward function.

**Abstract:** Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet we find that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. We propose a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, our factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness.

</details>


### [68] [How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations](https://arxiv.org/abs/2508.05625)

*Brandon Jaipersaud, David Krueger, Ekdeep Singh Lubana*

**Main category:** cs.CL

**Keywords:** Large Language Models, Persuasion, Linear Probes, Natural Language Processing, Cognitive Science

**Relevance Score:** 9

**TL;DR:** This paper explores the use of linear probes to study persuasion dynamics in large language models during multi-turn conversations, revealing insights into persuasion success, personality, and strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the dynamics of persuasion in conversations mediated by large language models, an area that is currently underexplored in AI research.

**Method:** The authors employ linear probes to analyze model representations focusing on aspects of persuasion including success, personality traits of the persuadee, and the strategies employed during persuasion. The effectiveness of probes is tested against traditional prompting methods.

**Key Contributions:**

	1. Introduction of linear probes for studying persuasion in LLMs
	2. Demonstration of probes identifying persuasion moments in conversations
	3. Comparison showing probes' efficiency and efficacy over traditional methods

**Result:** Probes successfully capture various aspects of persuasion, identifying key moments of persuasion and outperforming traditional prompting in certain scenarios, demonstrating their efficacy in analyzing complex behaviors in conversation.

**Limitations:** 

**Conclusion:** Linear probes present a practical and computationally efficient alternative to prompting-based methods for studying persuasion in natural language processing, with potential applications in understanding other complex behaviors.

**Abstract:** Large Language Models (LLMs) have started to demonstrate the ability to persuade humans, yet our understanding of how this dynamic transpires is limited. Recent work has used linear probes, lightweight tools for analyzing model representations, to study various LLM skills such as the ability to model user sentiment and political perspective. Motivated by this, we apply probes to study persuasion dynamics in natural, multi-turn conversations. We leverage insights from cognitive science to train probes on distinct aspects of persuasion: persuasion success, persuadee personality, and persuasion strategy. Despite their simplicity, we show that they capture various aspects of persuasion at both the sample and dataset levels. For instance, probes can identify the point in a conversation where the persuadee was persuaded or where persuasive success generally occurs across the entire dataset. We also show that in addition to being faster than expensive prompting-based approaches, probes can do just as well and even outperform prompting in some settings, such as when uncovering persuasion strategy. This suggests probes as a plausible avenue for studying other complex behaviours such as deception and manipulation, especially in multi-turn settings and large-scale dataset analysis where prompting-based methods would be computationally inefficient.

</details>


### [69] [H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages](https://arxiv.org/abs/2508.05628)

*Mehrdad Zakershahrak, Samira Ghodratnama*

**Main category:** cs.CL

**Keywords:** hierarchical modeling, dynamic chunking, morphologically-rich languages, byte-level language models, natural language processing

**Relevance Score:** 4

**TL;DR:** H-NET++ is a hierarchical language model that efficiently learns segmentation for morphologically-rich languages, achieving state-of-the-art results.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of processing morphologically-rich languages with byte-level language models that currently face computational issues.

**Method:** The model utilizes a lightweight Transformer context-mixer for cross-chunk attention, coupled with a two-level latent hyper-prior for document consistency and specialized handling of orthographic artifacts, trained through curriculum-based approaches.

**Key Contributions:**

	1. Hierarchical dynamic chunking for effective tokenization
	2. Lightweight Transformer for context-mixing
	3. Curriculum-based training with staged sequence lengths

**Result:** H-NET++ achieved a 0.159 BPB reduction compared to BPE-based GPT-2-fa, a 5.4pp improvement on ParsGLUE, enhanced robustness to ZWNJ corruption, and an F1 score of 73.8 on morphological boundaries.

**Limitations:** 

**Conclusion:** H-NET++ demonstrates that hierarchical dynamic chunking is an effective solution for tokenizer-free processing of morphologically-rich languages, offering enhanced computational efficiency.

**Abstract:** Byte-level language models eliminate fragile tokenizers but face computational challenges in morphologically-rich languages (MRLs), where words span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that learns linguistically-informed segmentation through end-to-end training. Key innovations include: (1) a lightweight Transformer context-mixer (1.9M parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for document-level consistency, (3) specialized handling of orthographic artifacts (e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks align with Persian morphology without explicit supervision, demonstrating that hierarchical dynamic chunking provides an effective tokenizer-free solution for MRLs while maintaining computational efficiency.

</details>


### [70] [A Latent-Variable Model for Intrinsic Probing](https://arxiv.org/abs/2201.08214)

*Karolina Stańczak, Lucas Torroba Hennigen, Adina Williams, Ryan Cotterell, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** intrinsic probing, linguistic representation, pre-trained models, morphosyntax, variational approximation

**Relevance Score:** 8

**TL;DR:** This paper presents a novel approach to intrinsic probing of pre-trained contextualized representations in NLP, revealing insights into their linguistic encoding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze pre-trained contextualized representations for their linguistic information and understanding where this information is encoded.

**Method:** A latent-variable formulation for constructing intrinsic probes is proposed, along with a tractable variational approximation to the log-likelihood.

**Key Contributions:**

	1. Novel latent-variable approach for intrinsic probing
	2. Derivation of variational approximation for log-likelihood
	3. Empirical evidence of cross-lingual morphosyntactic entanglement.

**Result:** The proposed model outperforms previous intrinsic probes in mutual information estimation, showing that pre-trained representations exhibit cross-lingually entangled morphosyntax.

**Limitations:** 

**Conclusion:** The findings indicate that pre-trained models not only encode linguistic attributes but do so in a way that reflects cross-linguistic similarities in morphosyntax.

**Abstract:** The success of pre-trained contextualized representations has prompted researchers to analyze them for the presence of linguistic information. Indeed, it is natural to assume that these pre-trained representations do encode some level of linguistic knowledge as they have brought about large empirical improvements on a wide variety of NLP tasks, which suggests they are learning true linguistic generalization. In this work, we focus on intrinsic probing, an analysis technique where the goal is not only to identify whether a representation encodes a linguistic attribute but also to pinpoint where this attribute is encoded. We propose a novel latent-variable formulation for constructing intrinsic probes and derive a tractable variational approximation to the log-likelihood. Our results show that our model is versatile and yields tighter mutual information estimates than two intrinsic probes previously proposed in the literature. Finally, we find empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax.

</details>


### [71] [Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A](https://arxiv.org/abs/2402.13213)

*Benjamin Plaut, Nguyen X. Khanh, Tu Trinh*

**Main category:** cs.CL

**Keywords:** language models, machine learning, calibration, uncertainty, Q&A

**Relevance Score:** 9

**TL;DR:** Study of LLMs finding miscalibration in maximum softmax probabilities (MSPs) on Q&A tasks, suggesting utility in correctness prediction despite calibration errors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the calibration of maximum softmax probabilities (MSPs) in chat-tuned large language models and how they relate to correctness in multiple-choice Q&A.

**Method:** Statistical testing on LLMs to analyze the relationship between MSPs and Q&A answer correctness, examining the impact of MSPs on performance when models can abstain from answering.

**Key Contributions:**

	1. Identified miscalibration of MSPs in LLMs on multi-choice Q&A tasks.
	2. Established correlation between MSPs and accuracy in predicting correctness.
	3. Proposed abstention strategies for improved performance leveraging MSPs.

**Result:** Demonstrated that lower MSPs correlate with wrong answers and that accuracy in Q&A is directionally related to correct predictions based on MSPs, irrespective of calibration error.

**Limitations:** 

**Conclusion:** As fine-tuning improves LLMs, correctness prediction will enhance even as calibration remains an issue, suggesting strategies to improve model performance by judiciously abstaining based on initial MSPs.

**Abstract:** We study 15 large language models (LLMs) fine-tuned for chat and find that their maximum softmax probabilities (MSPs) are consistently miscalibrated on multiple-choice Q&A. However, those MSPs might still encode useful uncertainty information. Specifically, we hypothesized that wrong answers would be associated with smaller MSPs compared to correct answers. Via rigorous statistical testing, we show that this hypothesis holds for models which perform well on the underlying Q&A task. We also find a strong direction correlation between Q&A accuracy and MSP correctness prediction, while finding no correlation between Q&A accuracy and calibration error. This suggests that within the current fine-tuning paradigm, we can expect correctness prediction but not calibration to improve as LLM capabilities progress. To demonstrate the utility of correctness prediction, we show that when models have the option to abstain, performance can be improved by selectively abstaining based on the MSP of the initial model response, using only a small amount of labeled data to choose the MSP threshold.

</details>


### [72] [Understanding Large Language Model Behaviors through Interactive Counterfactual Generation and Analysis](https://arxiv.org/abs/2405.00708)

*Furui Cheng, Vilém Zouhar, Robin Shing Moon Chan, Daniel Fürst, Hendrik Strobelt, Mennatallah El-Assady*

**Main category:** cs.CL

**Keywords:** large language models, explainable AI, counterfactual analysis, interactive visualization, human-in-the-loop

**Relevance Score:** 9

**TL;DR:** The paper introduces LLM Analyzer, an interactive visualization system for exploring LLM behaviors through counterfactual analysis, addressing limitations of existing XAI methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding and usability of large language models (LLMs) by providing more interactive and intuitive explanations.

**Method:** The system employs a novel algorithm for generating counterfactuals by targeted removal and replacement operations to analyze LLM behavior dynamically.

**Key Contributions:**

	1. Interactive visualization for LLM behavior analysis
	2. Novel counterfactual generation algorithm
	3. User-centered approach in explanation processes

**Result:** User studies indicate that LLM Analyzer enhances usability and effectiveness in understanding model behaviors and feature attributions.

**Limitations:** The study may need more extensive scalability testing for various LLMs and user scenarios.

**Conclusion:** Engaging users as active participants in the explanation process significantly improves their understanding of LLM behaviors.

**Abstract:** Understanding the behavior of large language models (LLMs) is crucial for ensuring their safe and reliable use. However, existing explainable AI (XAI) methods for LLMs primarily rely on word-level explanations, which are often computationally inefficient and misaligned with human reasoning processes. Moreover, these methods often treat explanation as a one-time output, overlooking its inherently interactive and iterative nature. In this paper, we present LLM Analyzer, an interactive visualization system that addresses these limitations by enabling intuitive and efficient exploration of LLM behaviors through counterfactual analysis. Our system features a novel algorithm that generates fluent and semantically meaningful counterfactuals via targeted removal and replacement operations at user-defined levels of granularity. These counterfactuals are used to compute feature attribution scores, which are then integrated with concrete examples in a table-based visualization, supporting dynamic analysis of model behavior. A user study with LLM practitioners and interviews with experts demonstrate the system's usability and effectiveness, emphasizing the importance of involving humans in the explanation process as active participants rather than passive recipients.

</details>


### [73] [CrisisSense-LLM: Instruction Fine-Tuned Large Language Model for Multi-label Social Media Text Classification in Disaster Informatics](https://arxiv.org/abs/2406.15477)

*Kai Yin, Bo Li, Chengkai Liu, Ali Mostafavi, Xia Hu*

**Main category:** cs.CL

**Keywords:** disaster informatics, text classification, Large Language Model, multi-label classification, situational awareness

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach for multi-label classification of disaster-related tweets using a fine-tuned Large Language Model to enhance situational awareness in crisis informatics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve situational awareness in disaster response by developing more effective text classification tools for social media data, particularly in multi-label contexts.

**Method:** The study fine-tunes a pre-trained Large Language Model with an instruction dataset compiled from disaster-related tweets, targeting multi-label classification.

**Key Contributions:**

	1. Introduces a novel multi-label classification approach using fine-tuned LLMs for disaster tweets.
	2. Enhances the ability to capture diverse insights from social media in crisis scenarios.
	3. Demonstrates improved utility of social media data for real-time situational awareness.

**Result:** The fine-tuned model successfully classifies multiple aspects of disaster-related information, improving the categorization of critical insights from social media.

**Limitations:** 

**Conclusion:** This research enhances the deployment of social media data for situational awareness during emergencies, paving the way for advanced disaster management tools leveraging LLM capabilities.

**Abstract:** In the field of crisis/disaster informatics, social media is increasingly being used for improving situational awareness to inform response and relief efforts. Efficient and accurate text classification tools have been a focal area of investigation in crisis informatics. However, current methods mostly rely on single-label text classification models, which fails to capture different insights embedded in dynamic and multifaceted disaster-related social media data. This study introduces a novel approach to disaster text classification by enhancing a pre-trained Large Language Model (LLM) through instruction fine-tuning targeted for multi-label classification of disaster-related tweets. Our methodology involves creating a comprehensive instruction dataset from disaster-related tweets, which is then used to fine-tune an open-source LLM, thereby embedding it with disaster-specific knowledge. This fine-tuned model can classify multiple aspects of disaster-related information simultaneously, such as the type of event, informativeness, and involvement of human aid, significantly improving the utility of social media data for situational awareness in disasters. The results demonstrate that this approach enhances the categorization of critical information from social media posts, thereby facilitating a more effective deployment for situational awareness during emergencies. This research paves the way for more advanced, adaptable, and robust disaster management tools, leveraging the capabilities of LLMs to improve real-time situational awareness and response strategies in disaster scenarios.

</details>


### [74] [When in Doubt, Cascade: Towards Building Efficient and Capable Guardrails](https://arxiv.org/abs/2407.06323)

*Manish Nagireddy, Inkit Padhi, Soumya Ghosh, Prasanna Sattigeri*

**Main category:** cs.CL

**Keywords:** Large Language Models, Synthetic Data, Guardrail Models, Bias Detection, Text Generation

**Relevance Score:** 9

**TL;DR:** This paper presents a synthetic data generation pipeline for developing guardrail models to detect bias in large language models (LLMs), demonstrating competitive performance with reduced compute costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the generation of harmful and biased text by large language models, the paper seeks to enhance the performance of detectors for social bias.

**Method:** The authors developed a synthetic data generation pipeline that uses taxonomy-driven instructions to create targeted and labeled data, generating over 300K unique contrastive samples for systematic evaluation.

**Key Contributions:**

	1. Introduction of a synthetic data generation pipeline for bias detection
	2. Generation of over 300K contrastive samples
	3. Cost-effective method for improving guardrail model performance

**Result:** The method achieves competitive performance comparable to existing models at a fraction of the computational cost.

**Limitations:** The paper contains potentially harmful examples of toxic and biased text.

**Conclusion:** The study provides a framework for developing more efficient and capable guardrail models, highlighting the importance of synthetic data in this process.

**Abstract:** Large language models (LLMs) have convincing performance in a variety of downstream tasks. However, these systems are prone to generating undesirable outputs such as harmful and biased text. In order to remedy such generations, the development of guardrail (or detector) models has gained traction. Motivated by findings from developing a detector for social bias, we adopt the notion of a use-mention distinction - which we identified as the primary source of under-performance in the preliminary versions of our social bias detector. Armed with this information, we describe a fully extensible and reproducible synthetic data generation pipeline which leverages taxonomy-driven instructions to create targeted and labeled data. Using this pipeline, we generate over 300K unique contrastive samples and provide extensive experiments to systematically evaluate performance on a suite of open source datasets. We show that our method achieves competitive performance with a fraction of the cost in compute and offers insight into iteratively developing efficient and capable guardrail models.   Warning: This paper contains examples of text which are toxic, biased, and potentially harmful.

</details>


### [75] [CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation](https://arxiv.org/abs/2409.02098)

*Ingo Ziegler, Abdullatif Köksal, Desmond Elliott, Hinrich Schütze*

**Main category:** cs.CL

**Keywords:** synthetic datasets, task-specific training, large language models

**Relevance Score:** 8

**TL;DR:** CRAFT is a method for generating synthetic datasets from a few user-written examples using document retrieval and instruction-tuned LLMs, demonstrated on multiple tasks.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for high-quality datasets for specialized tasks in fields like biology, medicine, and QA, which are often challenging to create due to resource intensity and domain knowledge requirements.

**Method:** CRAFT employs document retrieval from large web-crawled corpora based on similarity to user-written few-shots, followed by augmentation using LLMs to create task-specific samples.

**Key Contributions:**

	1. Introduces a novel method for synthetic dataset generation.
	2. Outperforms existing methods in various relevant tasks.
	3. Shows robustness in performance despite the quality of input samples.

**Result:** CRAFT demonstrates superior performance on QA tasks and summarization when compared to existing methods, including models trained on curated data.

**Limitations:** 

**Conclusion:** CRAFT efficiently generates large-scale, task-specific datasets and remains effective even with varying initial few-shot quality.

**Abstract:** Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given these examples, CRAFT uses large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology, medicine, and commonsense question-answering (QA), as well as summarization. Our experiments show that CRAFT-based models outperform or match general LLMs on QA tasks, while exceeding models trained on human-curated summarization data by 46 preference points. CRAFT outperforms other synthetic dataset generation methods such as Self- and Evol-Instruct, and remains robust even when the quality of the initial few-shots varies.

</details>


### [76] [Medal Matters: Probing LLMs' Failure Cases Through Olympic Rankings](https://arxiv.org/abs/2409.06518)

*Juhwan Choi, Seunguk Yu, JungMin Yun, YoungBin Kim*

**Main category:** cs.CL

**Keywords:** large language models, knowledge structures, Olympic medal counts, ranking tasks, natural language processing

**Relevance Score:** 7

**TL;DR:** This study investigates the internal knowledge structures of large language models (LLMs) using Olympic medal tallies, revealing strengths in recalling medal counts but weaknesses in ranking teams.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand the internal knowledge structures of LLMs and their limitations in knowledge integration.

**Method:** The study evaluates LLM performance on tasks related to retrieving Olympic medal counts and identifying team rankings based on historical data.

**Key Contributions:**

	1. Reveals weaknesses of LLMs in ranking tasks compared to human reasoning
	2. Provides a dataset and code for further research
	3. Offers insights into the internal knowledge organization of LLMs

**Result:** LLMs perform well at recalling medal counts but struggle with ranking teams, indicating a divergence from human reasoning.

**Limitations:** 

**Conclusion:** The findings highlight the limitations of LLMs in knowledge organization and suggest potential research directions for improvement in LLM capabilities.

**Abstract:** Large language models (LLMs) have achieved remarkable success in natural language processing tasks, yet their internal knowledge structures remain poorly understood. This study examines these structures through the lens of historical Olympic medal tallies, evaluating LLMs on two tasks: (1) retrieving medal counts for specific teams and (2) identifying rankings of each team. While state-of-the-art LLMs excel in recalling medal counts, they struggle with providing rankings, highlighting a key difference between their knowledge organization and human reasoning. These findings shed light on the limitations of LLMs' internal knowledge integration and suggest directions for improvement. To facilitate further research, we release our code, dataset, and model outputs.

</details>


### [77] [WhisperNER: Unified Open Named Entity and Speech Recognition](https://arxiv.org/abs/2409.08107)

*Gil Ayache, Menachem Pirchi, Aviv Navon, Aviv Shamsian, Gill Hetz, Joseph Keshet*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Automatic Speech Recognition, WhisperNER

**Relevance Score:** 8

**TL;DR:** WhisperNER integrates named entity recognition with automatic speech recognition to improve transcription accuracy and informativeness, allowing open-type NER.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accuracy and informativeness of transcriptions by integrating NER with ASR in a single model.

**Method:** We introduce WhisperNER, a model that jointly performs speech transcription and entity recognition by training on a large synthetic dataset augmented with synthetic speech samples.

**Key Contributions:**

	1. Introduction of WhisperNER for joint transcription and entity recognition
	2. Support for open-type named entity recognition
	3. Training on a large synthetic dataset with diverse NER tags

**Result:** WhisperNER shows improved performance compared to natural baselines on various benchmarks for out-of-domain open-type NER and in supervised finetuning tasks.

**Limitations:** 

**Conclusion:** The results indicate that integrating NER and ASR in WhisperNER provides significant advantages in transcription quality.

**Abstract:** Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning.

</details>


### [78] [MedHalu: Hallucinations in Responses to Healthcare Queries by Large Language Models](https://arxiv.org/abs/2409.19492)

*Vibhor Agarwal, Yiqiao Jin, Mohit Chandra, Munmun De Choudhury, Srijan Kumar, Nishanth Sastry*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, healthcare, MedHalu, MedHaluDetect

**Relevance Score:** 9

**TL;DR:** This paper studies hallucinations in large language models (LLMs) within healthcare contexts and introduces MedHalu, a benchmark for evaluating these hallucinations, along with a detection framework, MedHaluDetect.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs in healthcare raises concerns about their accuracy due to hallucinations, making it essential to understand their performance in real-world patient interactions.

**Method:** The study introduces MedHalu, a benchmark capturing various health-related responses generated by LLMs, annotated for types of hallucinations. It also proposes MedHaluDetect for evaluating LLMs' hallucination detection capabilities and assesses vulnerability to hallucinations across medical experts, LLMs, and laypeople.

**Key Contributions:**

	1. Introduction of MedHalu, a medical hallucination benchmark
	2. Development of MedHaluDetect for evaluating LLM hallucination detection
	3. Demonstration of expert-in-the-loop method to improve LLM performance in detecting medical hallucinations

**Result:** LLMs showed significantly poorer performance in detecting medical hallucinations compared to human experts and sometimes even underperformed compared to laypeople. The expert-in-the-loop approach improved detection efficacy, achieving a 6.3% macro-F1 score improvement for GPT-4.

**Limitations:** The study focuses on a specific set of health-related queries and may not generalize across all healthcare scenarios or to all LLMs.

**Conclusion:** Integrating expert reasoning into LLM inputs enhances hallucination detection capabilities, emphasizing the need for robust evaluation benchmarks like MedHalu to assess LLMs in critical fields like healthcare.

**Abstract:** Large language models (LLMs) are starting to complement traditional information seeking mechanisms such as web search. LLM-powered chatbots like ChatGPT are gaining prominence among the general public. AI chatbots are also increasingly producing content on social media platforms. However, LLMs are also prone to hallucinations, generating plausible yet factually incorrect or fabricated information. This becomes a critical problem when laypeople start seeking information about sensitive issues such as healthcare. Existing works in LLM hallucinations in the medical domain mainly focus on testing the medical knowledge of LLMs through standardized medical exam questions which are often well-defined and clear-cut with definitive answers. However, these approaches may not fully capture how these LLMs perform during real-world interactions with patients. This work conducts a pioneering study on hallucinations in LLM-generated responses to real-world healthcare queries from patients.We introduce MedHalu, a novel medical hallucination benchmark featuring diverse health-related topics and hallucinated responses from LLMs, with detailed annotation of the hallucination types and text spans. We also propose MedHaluDetect, a comprehensive framework for evaluating LLMs' abilities to detect hallucinations. Furthermore, we study the vulnerability to medical hallucinations among three groups -- medical experts, LLMs, and laypeople. Notably, LLMs significantly underperform human experts and, in some cases, even laypeople in detecting medical hallucinations. To improve hallucination detection, we propose an expert-in-the-loop approach that integrates expert reasoning into LLM inputs, significantly improving hallucination detection for all LLMs, including a 6.3% macro-F1 improvement for GPT-4.

</details>


### [79] [From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging](https://arxiv.org/abs/2410.01215)

*Yuling Shi, Songsong Wang, Chengcheng Wan, Min Wang, Xiaodong Gu*

**Main category:** cs.CL

**Keywords:** Large language models, Code generation, Debugging systems, Human-Computer Interaction, Machine Learning

**Relevance Score:** 9

**TL;DR:** MGDebugger is a hierarchical code debugger that improves the pass rate of LLM-generated code by identifying and resolving bugs at multiple levels of granularity, demonstrating significant performance improvements over existing debugging systems.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current LLM-based debugging systems that treat generated code as monolithic entities, leading to inefficiencies in debugging complex programs.

**Method:** MGDebugger uses a hierarchical tree structure to decompose code into subfunctions, analyzing and debugging them iteratively from the bottom up, aided by an LLM-simulated Python executor that tracks execution and variable states.

**Key Contributions:**

	1. Introduction of a novel hierarchical debugging approach
	2. Use of LLM-simulated execution for accurate error tracing
	3. Demonstrated substantial accuracy improvements over existing systems

**Result:** MGDebugger achieves an 18.9% improvement in accuracy on HumanEval and a 97.6% repair success rate on HumanEvalFix, surpassing traditional debugging systems.

**Limitations:** 

**Conclusion:** The robustness and effectiveness of MGDebugger in fixing bugs across various categories highlight its potential to improve code generation from LLMs.

**Abstract:** While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.

</details>


### [80] [Recent Advances in Speech Language Models: A Survey](https://arxiv.org/abs/2410.03751)

*Wenqian Cui, Dianzhi Yu, Xiaoqi Jiao, Ziqiao Meng, Guangyan Zhang, Qichao Wang, Yiwen Guo, Irwin King*

**Main category:** cs.CL

**Keywords:** Speech Language Models, Automatic Speech Recognition, Text-to-Speech, Human-Computer Interaction, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper surveys recent methodologies for constructing Speech Language Models (SpeechLMs) that generate speech directly, bypassing text conversion, to address issues in traditional models using ASR + LLM + TTS pipelines.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The shift towards voice-based models is necessary for more natural human-computer interactions, as traditional text-based methods have limitations like information loss and latency.

**Method:** This survey reviews architectures and training methods for SpeechLMs and categorizes their capabilities, evaluation metrics, and research challenges.

**Key Contributions:**

	1. First comprehensive overview of methodologies for SpeechLMs.
	2. Detailed analysis of architecture and training recipes for SpeechLMs.
	3. Categorization of capabilities and evaluation metrics in SpeechLMs.

**Result:** SpeechLMs present a more efficient alternative to traditional speech processing pipelines by eliminating the conversion from speech to text and back.

**Limitations:** The paper focuses primarily on methodologies without extensive experimental results from various models.

**Conclusion:** The field of SpeechLMs is rapidly evolving, with numerous challenges and future research opportunities identified.

**Abstract:** Large Language Models (LLMs) have recently garnered significant attention, primarily for their capabilities in text-based interactions. However, natural human interaction often relies on speech, necessitating a shift towards voice-based models. A straightforward approach to achieve this involves a pipeline of ``Automatic Speech Recognition (ASR) + LLM + Text-to-Speech (TTS)", where input speech is transcribed to text, processed by an LLM, and then converted back to speech. Despite being straightforward, this method suffers from inherent limitations, such as information loss during modality conversion, significant latency due to the complex pipeline, and error accumulation across the three stages. To address these issues, Speech Language Models (SpeechLMs) -- end-to-end models that generate speech without converting from text -- have emerged as a promising alternative. This survey paper provides the first comprehensive overview of recent methodologies for constructing SpeechLMs, detailing the key components of their architecture and the various training recipes integral to their development. Additionally, we systematically survey the various capabilities of SpeechLMs, categorize their evaluation metrics, and discuss the challenges and future research directions in this rapidly evolving field. The GitHub repository is available at https://github.com/dreamtheater123/Awesome-SpeechLM-Survey

</details>


### [81] [BloomWise: Enhancing Problem-Solving capabilities of Large Language Models using Bloom's-Taxonomy-Inspired Prompts](https://arxiv.org/abs/2410.04094)

*Maria-Eleni Zoumpoulidi, Georgios Paraskevopoulos, Alexandros Potamianos*

**Main category:** cs.CL

**Keywords:** large language models, mathematical reasoning, cognitive prompting

**Relevance Score:** 8

**TL;DR:** BloomWise enhances LLMs' mathematical problem-solving through cognitively-inspired prompting techniques, making solutions more explainable.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To improve mathematical reasoning in LLMs by mimicking human thought processes.

**Method:** BloomWise employs a sequence of cognitive operations, allowing LLMs to generate solutions with explanations by progressing through levels of reasoning.

**Key Contributions:**

	1. Introduction of BloomWise for LLMs in mathematical reasoning
	2. Cognitive operation progression mirroring human learning
	3. Demonstration of effectiveness across multiple datasets

**Result:** Extensive experiments show BloomWise's effectiveness on five popular math reasoning datasets; ablation studies highlight strengths in the system's components.

**Limitations:** 

**Conclusion:** BloomWise can significantly enhance LLM performance in mathematical reasoning, making it more interpretable.

**Abstract:** Despite the remarkable capabilities of large language models (LLMs) across a range of tasks, mathematical reasoning remains a challenging frontier. Motivated by the observation that humans learn more effectively when prompted not what to think but how to think, we introduce BloomWise, a cognitively-inspired prompting technique designed to enhance LLMs' performance on mathematical problem solving while making their solutions more explainable. BloomWise encourages LLMs to generate solutions - in the form of explanations - by progressing through a sequence of cognitive operations-from basic (e.g., remembering) to more advanced reasoning skills (e.g., evaluating) - mirroring how humans build understanding. The process iterates through these levels, halting early if a convergence criterion is met: specifically, if two or more consecutive levels yield the same answer, the solution from the earliest such level is output; otherwise, the process continues until all levels are completed. Through extensive experiments across five popular math reasoning datasets, we demonstrate the effectiveness of BloomWise. We also present comprehensive ablation studies to analyze the strengths of each component within our system.

</details>


### [82] [Large Language Models Still Exhibit Bias in Long Text](https://arxiv.org/abs/2410.17519)

*Wonje Jeung, Dongjae Jeon, Ashkan Yousefpour, Jonghyun Choi*

**Main category:** cs.CL

**Keywords:** fairness, large language models, long text generation, bias, finetuning

**Relevance Score:** 9

**TL;DR:** Introduction of the Long Text Fairness Test (LTF-TEST) framework that assesses biases in large language models (LLMs) through essay-style prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing fairness benchmarks for LLMs focus mainly on simple tasks, neglecting biases in complex scenarios like long-text generation.

**Method:** The LTF-TEST framework evaluates biases in LLMs using 11,948 samples across 14 topics and 10 demographic axes, analyzing both model responses and their reasoning.

**Key Contributions:**

	1. Introduction of LTF-TEST for evaluating long-text biases in LLMs
	2. Identification of bias patterns in LLM model responses
	3. Proposal of FT-REGARD as a finetuning method to mitigate biases

**Result:** Evaluation of five recent LLMs revealed biases favoring certain demographic groups and excessive sensitivity towards disadvantaged groups. FT-REGARD finetuning reduced gender bias by 34.6% and improved BBQ benchmark performance by 1.4 percentage points.

**Limitations:** 

**Conclusion:** The LTF-TEST framework provides a comprehensive evaluation of LLM biases, and FT-REGARD presents a viable approach for mitigating these biases in long-text generation tasks.

**Abstract:** Existing fairness benchmarks for large language models (LLMs) primarily focus on simple tasks, such as multiple-choice questions, overlooking biases that may arise in more complex scenarios like long-text generation. To address this gap, we introduce the Long Text Fairness Test (LTF-TEST), a framework that evaluates biases in LLMs through essay-style prompts. LTF-TEST covers 14 topics and 10 demographic axes, including gender and race, resulting in 11,948 samples. By assessing both model responses and the reasoning behind them, LTF-TEST uncovers subtle biases that are difficult to detect in simple responses. In our evaluation of five recent LLMs, including GPT-4o and LLaMa3, we identify two key patterns of bias. First, these models frequently favor certain demographic groups in their responses. Second, they show excessive sensitivity toward traditionally disadvantaged groups, often providing overly protective responses while neglecting others. To mitigate these biases, we propose FT-REGARD, a finetuning approach that pairs biased prompts with neutral responses. FT-REGARD reduces gender bias by 34.6% and improves performance by 1.4 percentage points on the BBQ benchmark, offering a promising approach to addressing biases in long-text generation tasks.

</details>


### [83] [GuARD: Effective Anomaly Detection through a Text-Rich and Graph-Informed Language Model](https://arxiv.org/abs/2412.03930)

*Yunhe Pang, Bo Chen, Fanjin Zhang, Yanghui Rao, Evgeny Kharlamov, Jie Tang*

**Main category:** cs.CL

**Keywords:** anomaly detection, large language models, graph-based methods, text-rich graphs, multi-modal tuning

**Relevance Score:** 7

**TL;DR:** Introduction of GuARD, a model combining aspects of graphs and language models for anomaly detection on text-rich graphs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage LLMs and structural graph features for effective anomaly detection while overcoming high fine-tuning costs and structural biases typically neglected by LLMs.

**Method:** GuARD combines structural features from graph methods with semantic attributes from small language models using a multi-modal multi-turn instruction tuning framework tailored for task-guided instruction tuning.

**Key Contributions:**

	1. Introduction of GuARD model for anomaly detection
	2. Combines graph structural features with semantic analysis from LLMs
	3. Achieves significant speed improvements over traditional methods

**Result:** GuARD outperforms existing methods in anomaly detection on four datasets and provides significant speedups in both training and inference on large-scale datasets.

**Limitations:** 

**Conclusion:** GuARD effectively incorporates both rich-text and structural modalities, achieving state-of-the-art performance for anomaly detection.

**Abstract:** Anomaly detection on text-rich graphs is widely prevalent in real life, such as detecting incorrectly assigned academic papers to authors and detecting bots in social networks. The remarkable capabilities of large language models (LLMs) pave a new revenue by utilizing rich-text information for effective anomaly detection. However, simply introducing rich texts into LLMs can obscure essential detection cues and introduce high fine-tuning costs. Moreover, LLMs often overlook the intrinsic structural bias of graphs which is vital for distinguishing normal from abnormal node patterns. To this end, this paper introduces GuARD, a text-rich and graph-informed language model that combines key structural features from graph-based methods with fine-grained semantic attributes extracted via small language models for effective anomaly detection on text-rich graphs. GuARD is optimized with the progressive multi-modal multi-turn instruction tuning framework in the task-guided instruction tuning regime tailed to incorporate both rich-text and structural modalities. Extensive experiments on four datasets reveal that GuARD outperforms graph-based and LLM-based anomaly detection methods, while offering up to 5$\times$ times speedup in training and 5$\times$ times speedup in inference over vanilla long-context LLMs on the large-scale WhoIsWho dataset.

</details>


### [84] [Efficient Knowledge Injection in LLMs via Self-Distillation](https://arxiv.org/abs/2412.14964)

*Kalle Kujanpää, Pekka Marttinen, Harri Valpola, Alexander Ilin*

**Main category:** cs.CL

**Keywords:** large language models, knowledge acquisition, prompt distillation, fine-tuning, retrieval-augmented generation

**Relevance Score:** 9

**TL;DR:** This paper introduces prompt distillation, a method for efficiently internalizing new knowledge in large language models (LLMs) without needing larger teacher models or structured formats, outperforming fine-tuning and RAG in several cases.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the process of knowledge acquisition in LLMs, which often rely on outdated pre-training data, by exploring a new method that can surpass existing approaches.

**Method:** The proposed method, prompt distillation, employs a self-distillation technique to internalize new knowledge from free-form text, compared against supervised fine-tuning and retrieval-augmented generation (RAG).

**Key Contributions:**

	1. Introduction of prompt distillation for knowledge internalization in LLMs.
	2. Demonstration of improved performance over fine-tuning and RAG methods.
	3. Analysis of scaling factors for prompt distillation's effectiveness.

**Result:** Prompt distillation demonstrates superior performance compared to standard fine-tuning and can exceed RAG outcomes across various LLM sizes and families.

**Limitations:** 

**Conclusion:** The effectiveness of prompt distillation is attributed to its unique approach, allowing for successful knowledge acquisition in LLMs without the constraints of prior methods.

**Abstract:** In many practical applications, large language models (LLMs) need to acquire new knowledge not present in their pre-training data. Efficiently leveraging this knowledge usually relies on supervised fine-tuning or retrieval-augmented generation (RAG). Although RAG has emerged as the industry standard for knowledge injection, fine-tuning has not yet achieved comparable success. This paper proposes utilizing prompt distillation, a self-distillation-based method previously explored primarily for style alignment and instruction tuning, to internalize new factual knowledge from free-form documents. Unlike prior methods, our approach requires neither larger teacher models nor structured knowledge formats. Across multiple LLM sizes and model families, we show that prompt distillation outperforms standard supervised fine-tuning and can even surpass RAG. We analyze the key factors contributing to prompt distillation's effectiveness and examine how it scales.

</details>


### [85] [Multi-Agents Based on Large Language Models for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2412.18351)

*Zhongjian Hu, Peng Yang, Bing Li, Zhenqi Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Visual Question Answering, multi-agent systems, collaboration, tool utilization

**Relevance Score:** 8

**TL;DR:** Proposal of a multi-agent voting framework using LLMs for improved Visual Question Answering (VQA) in teams with external tool utilization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges in current VQA methods related to autonomous use of external tools and collaboration among agents.

**Method:** Developed a multi-agent voting framework with three levels of LLM-based agents that utilize available tools and vote on final answers.

**Key Contributions:**

	1. Introduction of a multi-agent voting framework for VQA
	2. Demonstration of LLM-based agents with varying levels of expertise
	3. Empirical results showing improved performance on benchmark datasets

**Result:** Achieved improvements over baseline methods with increases of 2.2 and 1.0 in performance on OK-VQA and A-OKVQA datasets, respectively.

**Limitations:** None stated due to the paper's withdrawal for review.

**Conclusion:** The proposed method enhances VQA capabilities by simulating collaborative team responses and tool usage.

**Abstract:** Large Language Models (LLMs) have achieved impressive results in knowledge-based Visual Question Answering (VQA). However existing methods still have challenges: the inability to use external tools autonomously, and the inability to work in teams. Humans tend to know whether they need to use external tools when they encounter a new question, e.g., they tend to be able to give a direct answer to a familiar question, whereas they tend to use tools such as search engines when they encounter an unfamiliar question. In addition, humans also tend to collaborate and discuss with others to get better answers. Inspired by this, we propose the multi-agent voting framework. We design three LLM-based agents that simulate different levels of staff in a team, and assign the available tools according to the levels. Each agent provides the corresponding answer, and finally all the answers provided by the agents are voted to get the final answer. Experiments on OK-VQA and A-OKVQA show that our approach outperforms other baselines by 2.2 and 1.0, respectively.

</details>


### [86] [RLTHF: Targeted Human Feedback for LLM Alignment](https://arxiv.org/abs/2502.13417)

*Yifei Xu, Tusher Chakraborty, Emre Kıcıman, Bibek Aryal, Eduardo Rodrigues, Srinagesh Sharma, Roberto Estevao, Maria Angels de Luis Balaguer, Jessica Wolk, Rafael Padilha, Leonardo Nunes, Shobana Balakrishnan, Songwu Lu, Ranveer Chandra*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Large Language Models, Model Alignment, AI Annotation

**Relevance Score:** 9

**TL;DR:** This paper introduces RLTHF, a human-AI hybrid framework designed to fine-tune large language models with minimal human annotation effort by combining LLM initial alignment and selective human corrections.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning large language models with user preferences is hampered by the cost and limitations of human annotations; RLTHF aims to reduce the required human effort while improving model alignment.

**Method:** The RLTHF framework utilizes an LLM-based initial alignment and selectively incorporates human annotations on hard-to-annotate samples identified through a reward model's distribution, iteratively refining alignment using both LLM-labeled and human-corrected samples.

**Key Contributions:**

	1. Introduction of RLTHF framework
	2. Demonstrated effectiveness in reducing human annotation effort
	3. Outperformed traditional human-annotated dataset models in downstream tasks

**Result:** Experiments demonstrate that RLTHF achieves alignment comparable to full human annotation using only 6-7% of the typical human effort, and models trained on RLTHF datasets outperform those trained on fully annotated datasets.

**Limitations:** 

**Conclusion:** RLTHF highlights a more efficient approach to model alignment, achieving significant performance with reduced human resource investment.

**Abstract:** Fine-tuning large language models (LLMs) to align with user preferences is challenging due to the high cost of quality human annotations in Reinforcement Learning from Human Feedback (RLHF) and the generalizability limitations of AI Feedback. To address these challenges, we propose RLTHF, a human-AI hybrid framework that combines LLM-based initial alignment with selective human annotations to achieve full-human annotation alignment with minimal effort. RLTHF identifies hard-to-annotate samples mislabeled by LLMs using a reward model's reward distribution and iteratively enhances alignment by integrating strategic human corrections while leveraging LLM's correctly labeled samples. Evaluations on HH-RLHF and TL;DR datasets show that RLTHF reaches full-human annotation-level alignment with only 6-7% of the human annotation effort. Furthermore, models trained on RLTHF's curated datasets for downstream tasks outperform those trained on fully human-annotated datasets, underscoring the effectiveness of RLTHF.

</details>


### [87] [Which Questions Improve Learning the Most? Utility Estimation of Questions with LM-based Simulations](https://arxiv.org/abs/2502.17383)

*Dong-Ho Lee, Hyundong Cho, Jonathan May, Jay Pujara*

**Main category:** cs.CL

**Keywords:** question generation, learning outcomes, language models, human-computer interaction, educational technology

**Relevance Score:** 7

**TL;DR:** This paper introduces QUEST, a framework for evaluating and generating learner-driven questions based on their direct impact on examination performance, using language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating and generating good questions that enhance learning is challenging, as prior metrics do not measure actual learning outcomes effectively.

**Method:** The QUEST framework simulates learners asking questions while studying and measures the direct contribution of these questions to exam performance, using a curated benchmark called TEXTBOOK-EXAM.

**Key Contributions:**

	1. Introduction of QUEST framework for question evaluation
	2. Curated TEXTBOOK-EXAM benchmark
	3. Demonstrated significant improvement in test scores using QUEST-generated questions

**Result:** Experiments show that questions generated by QUEST improve simulated test scores by over 20% compared to models using indirect evaluation metrics.

**Limitations:** The framework relies on simulations, and its applicability in real-world settings may vary.

**Conclusion:** QUEST provides a new paradigm for question evaluation and generation, focusing on measurable learning improvements rather than indirect metrics.

**Abstract:** Asking good questions is critical for comprehension and learning, yet evaluating and generating such questions remains a challenging problem. Prior work on inquisitive questions focuses on learner-generated, curiosity-driven queries and evaluates them using indirect metrics, such as salience or information gain, that do not directly capture a question's impact on actual learning outcomes. We introduce QUEST (Question Utility Estimation with Simulated Tests), a framework that uses language models to simulate learners and directly quantify the utility of a question - its contribution to exam performance. QUEST simulates a learner who asks questions and receives answers while studying a textbook chapter, then uses them to take an end-of-chapter exam. Through this simulation, the utility of each question is estimated by its direct effect on exam performance, rather than inferred indirectly based on the underlying content. To support this evaluation, we curate TEXTBOOK-EXAM, a benchmark that aligns textbook sections with end-of-section exam questions across five academic disciplines. Using QUEST, we filter for high-utility questions and fine-tune question generators via rejection sampling. Experiments show that questions generated by QUEST-trained models improve simulated test scores by over 20% compared to strong baselines that are fine-tuned using indirect metrics or leverage prompting methods. Furthermore, utility is only weakly correlated with salience and similarity to exam questions, suggesting that it captures unique signal that benefits downstream performance. QUEST offers a new outcome-driven paradigm for question evaluation and generation - one that moves beyond question-answer content toward measurable improvements in learning outcomes.

</details>


### [88] [Language Model Uncertainty Quantification with Attention Chain](https://arxiv.org/abs/2503.19168)

*Yinghao Li, Rushi Qiang, Lama Moukheiber, Chao Zhang*

**Main category:** cs.CL

**Keywords:** predictive uncertainty, large language models, attention chains

**Relevance Score:** 9

**TL;DR:** Proposes UQAC, an efficient method for quantifying predictive uncertainty in large language models (LLMs) that involves intermediate reasoning steps.

**Read time:** 36 min

<details>
  <summary>Details</summary>

**Motivation:** Accurately quantifying predictive uncertainty in LLMs is essential for assessing the reliability of their answers, especially for complex questions requiring intermediate reasoning.

**Method:** UQAC narrows the reasoning space by constructing an 'attention chain' of semantically crucial tokens for the final answer through a backtracking procedure, using attention weights to identify influential predecessors.

**Key Contributions:**

	1. Introduction of UQAC for efficient uncertainty quantification in LLMs
	2. Construction of an attention chain to narrow the reasoning space
	3. Empirical validation demonstrating improved reliability and efficiency in UQ estimation

**Result:** UQAC is validated on multiple reasoning benchmarks with advanced open-source LLMs, consistently delivering reliable uncertainty estimates while maintaining high computational efficiency.

**Limitations:** Limited to the frameworks and benchmarks used in validation; further exploration needed across diverse domains and models.

**Conclusion:** The approach enhances the ability to quantify uncertainty for LLMs in complex reasoning scenarios, addressing challenges posed by traditional methods of marginalization.

**Abstract:** Accurately quantifying a large language model's (LLM) predictive uncertainty is crucial for judging the reliability of its answers. While most existing research focuses on short, directly answerable questions with closed-form outputs (e.g., multiple-choice), involving intermediate reasoning steps in LLM responses is increasingly important. This added complexity complicates uncertainty quantification (UQ) because the probabilities assigned to answer tokens are conditioned on a vast space of preceding reasoning tokens. Direct marginalization is infeasible, and the dependency inflates probability estimates, causing overconfidence in UQ. To address this, we propose UQAC, an efficient method that narrows the reasoning space to a tractable size for marginalization. UQAC iteratively constructs an "attention chain" of tokens deemed "semantically crucial" to the final answer via a backtracking procedure. Starting from the answer tokens, it uses attention weights to identify the most influential predecessors, then iterates this process until reaching the input tokens. The resulting chain is further refined with similarity filtering and probability thresholding, which reduce the reasoning space, facilitating the approximation of the marginal answer token probabilities. We validate UQAC on multiple reasoning benchmarks with advanced open-source LLMs, demonstrating that it consistently delivers reliable UQ estimates with high computational efficiency.

</details>


### [89] [You Cannot Feed Two Birds with One Score: the Accuracy-Naturalness Tradeoff in Translation](https://arxiv.org/abs/2503.24013)

*Gergely Flamich, David Vilar, Jan-Thorsten Peter, Markus Freitag*

**Main category:** cs.CL

**Keywords:** machine translation, accuracy, naturalness, evaluation metrics, information theory

**Relevance Score:** 7

**TL;DR:** This paper critiques the use of single-score evaluations in machine translation, demonstrating the tradeoff between accuracy and naturalness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of single-score evaluations in machine translation and to propose a more nuanced evaluation framework.

**Method:** The authors use information theory to mathematically prove the existence of a tradeoff between accuracy and naturalness in machine translation, supported by empirical data from WMT24 submissions.

**Key Contributions:**

	1. Mathematical proof of the accuracy-naturalness tradeoff in translations.
	2. Empirical demonstration using WMT24 dataset.
	3. Proposal for a dual-axis evaluation framework for machine translation.

**Result:** The study shows that while optimizing for accuracy can initially improve naturalness, excessive optimization leads to a decrease in naturalness.

**Limitations:** 

**Conclusion:** The authors advocate for evaluating translations using a dual-axis approach that considers both accuracy and naturalness rather than a single score.

**Abstract:** The goal of translation, be it by human or by machine, is, given some text in a source language, to produce text in a target language that simultaneously 1) preserves the meaning of the source text and 2) achieves natural expression in the target language. However, researchers in the machine translation community usually assess translations using a single score intended to capture semantic accuracy and the naturalness of the output simultaneously. In this paper, we build on recent advances in information theory to mathematically prove and empirically demonstrate that such single-score summaries do not and cannot give the complete picture of a system's true performance. Concretely, we prove that a tradeoff exists between accuracy and naturalness and demonstrate it by evaluating the submissions to the WMT24 shared task. Our findings help explain well-known empirical phenomena, such as the observation that optimizing translation systems for a specific accuracy metric (like BLEU) initially improves the system's naturalness, while ``overfitting'' the system to the metric can significantly degrade its naturalness. Thus, we advocate for a change in how translations are evaluated: rather than comparing systems using a single number, they should be compared on an accuracy-naturalness plane.

</details>


### [90] [SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers](https://arxiv.org/abs/2504.00255)

*Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Algorithm Comprehension, Code Generation, NLP, Benchmark

**Relevance Score:** 9

**TL;DR:** This research evaluates LLMs in generating code from algorithms in recent NLP papers, introducing a benchmark and a dual-agent framework for improved understanding and implementation of algorithms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capabilities of large language models in generating code from algorithm descriptions in academic literature and tackle the challenge of algorithm comprehension and coding expertise.

**Method:** We introduce the SciReplicate-Bench benchmark with 100 tasks from 36 NLP papers and propose the Sci-Reproducer framework, which consists of a Paper Agent for interpreting algorithm concepts and a Code Agent for retrieving dependencies and implementing solutions.

**Key Contributions:**

	1. Introduction of SciReplicate-Bench, a benchmark for task evaluation.
	2. Development of Sci-Reproducer, a dual-agent framework for code generation.
	3. Introduction of metrics for assessing algorithm understanding and implementation quality.

**Result:** The evaluation reveals the best-performing LLM achieving only 39% execution accuracy, indicating the complexity of the benchmark and the importance of clear algorithm descriptions.

**Limitations:** Challenges in successful reproduction due to missing or inconsistent algorithm descriptions.

**Conclusion:** The study emphasizes the challenges in reproduction due to inconsistent algorithm descriptions and provides a resource for further research.

**Abstract:** This study evaluates large language models (LLMs) in generating code from algorithm descriptions in recent NLP papers. The task requires two key competencies: (1) algorithm comprehension: synthesizing information from papers and academic literature to understand implementation logic, and (2) coding expertise: identifying dependencies and correctly implementing necessary APIs. To facilitate rigorous evaluation, we introduce SciReplicate-Bench, a benchmark of 100 tasks from 36 NLP papers published in 2024, featuring detailed annotations and comprehensive test cases. Building on SciReplicate-Bench, we propose Sci-Reproducer, a dual-agent framework consisting of a Paper Agent that interprets algorithmic concepts from literature and a Code Agent that retrieves dependencies from repositories and implements solutions. To assess algorithm understanding, we introduce reasoning graph accuracy, which quantifies similarity between generated and reference reasoning graphs derived from code comments and structure. For evaluating implementation quality, we employ execution accuracy, CodeBLEU, and repository dependency/API recall metrics. In our experiments, we evaluate various powerful non-reasoning and reasoning LLMs as foundational models. The best-performing LLM using \ModelName~achieves only 39% execution accuracy, highlighting the benchmark's difficulty. Our analysis identifies missing or inconsistent algorithm descriptions as key barriers to successful reproduction. We make available our benchmark and code at https://github.com/xyzCS/SciReplicate-Bench and project homepage at https://xyzcs.github.io/scireplicate.github.io/.

</details>


### [91] [PolyGuard: A Multilingual Safety Moderation Tool for 17 Languages](https://arxiv.org/abs/2504.04377)

*Priyanshu Kumar, Devansh Jain, Akhila Yerukola, Liwei Jiang, Himanshu Beniwal, Thomas Hartvigsen, Maarten Sap*

**Main category:** cs.CL

**Keywords:** multilingual LLMs, safety moderation, POLYGUARD, safety benchmarks, machine learning

**Relevance Score:** 9

**TL;DR:** POLYGUARD is a new multilingual safety model for Large Language Models, addressing gaps in moderation by providing comprehensive safety definitions and a large training corpus.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current multilingual safety moderation for LLMs is limited to a few languages and lacks comprehensive safety definitions, creating significant gaps in moderation functionality.

**Method:** POLYGUARD is trained on POLYGUARDMIX, a multilingual corpus with 1.91M samples in 17 languages, and evaluated with POLYGUARDPROMPTS, a benchmark of 29K samples for safety evaluations.

**Key Contributions:**

	1. Introduction of POLYGUARD and practical multilingual safety model for LLMs
	2. Release of POLYGUARDMIX and POLYGUARDPROMPTS datasets
	3. Demonstrated performance improvement over existing safety classifiers

**Result:** POLYGUARD outperforms existing state-of-the-art safety classifiers by 5.5%, showing improved moderation capabilities across multiple safety and toxicity benchmarks.

**Limitations:** 

**Conclusion:** The advancements provided by POLYGUARD help achieve safer multilingual LLMs, enhancing moderation for global users.

**Abstract:** Truly multilingual safety moderation efforts for Large Language Models (LLMs) have been hindered by a narrow focus on a small set of languages (e.g., English, Chinese) as well as a limited scope of safety definition, resulting in significant gaps in moderation capabilities. To bridge these gaps, we release POLYGUARD, a new state-of-the-art multilingual safety model for safeguarding LLM generations, and the corresponding training and evaluation datasets. POLYGUARD is trained on POLYGUARDMIX, the largest multilingual safety training corpus to date containing 1.91M samples across 17 languages (e.g., Chinese, Czech, English, Hindi). We also introduce POLYGUARDPROMPTS, a high quality multilingual benchmark with 29K samples for the evaluation of safety guardrails. Created by combining naturally occurring multilingual human-LLM interactions and human-verified machine translations of an English-only safety dataset (WildGuardMix; Han et al., 2024), our datasets contain prompt-output pairs with labels of prompt harmfulness, response harmfulness, and response refusal. Through extensive evaluations across multiple safety and toxicity benchmarks, we demonstrate that POLYGUARD outperforms existing state-of-the-art open-weight and commercial safety classifiers by 5.5%. Our contributions advance efforts toward safer multilingual LLMs for all global users.

</details>


### [92] [DEL: Context-Aware Dynamic Exit Layer for Efficient Self-Speculative Decoding](https://arxiv.org/abs/2504.05598)

*Hossein Entezari Zarch, Lei Gao, Chaoyi Jiang, Murali Annavaram*

**Main category:** cs.CL

**Keywords:** Speculative Decoding, Large Language Models, Dynamic Exit Layer

**Relevance Score:** 9

**TL;DR:** Introduction of DEL, a method to improve inference speed in LLMs via dynamic exit layer selection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the inference speed of large language models while maintaining generation quality by addressing static hyperparameter selection in Speculative Decoding.

**Method:** DEL employs dynamic tracking of token acceptance rates during inference to adaptively choose the exit layer and speculation length for improved performance.

**Key Contributions:**

	1. Introduction of DEL for dynamic exit layer selection in LLMs
	2. Demonstrated significant speedup over conventional decoding methods
	3. Provided empirical evidence of performance improvements across multiple tasks

**Result:** DEL achieves speedups of 2.16x to 2.62x over standard auto-regressive decoding and enhances existing Speculative Decoding methods.

**Limitations:** 

**Conclusion:** The adaptive method shows significant improvements in inference efficiency across various models and tasks, offering a more effective approach to Speculative Decoding.

**Abstract:** Speculative Decoding (SD) is a widely used approach to accelerate the inference of large language models (LLMs) without reducing generation quality. It operates by first using a compact model to draft multiple tokens efficiently, followed by parallel verification using the target LLM. This approach leads to faster inference compared to auto-regressive decoding. While there are multiple approaches to create a draft model, one promising approach is to use early-exit methods. These methods draft candidate tokens by using a subset of layers of the primary model and applying the remaining layers for verification, allowing a single model to handle both drafting and verification. While this technique reduces memory usage and computational cost, its performance relies on the choice of the exit layer for drafting and the number of tokens drafted (speculation length) in each SD round. Prior works use hyperparameter exploration to statically select these values. However, our evaluations show that these hyperparameter values are task-specific, and even within a task they are dependent on the current sequence context. We introduce DEL (Dynamic Exit Layer), a plug-and-play method that adaptively selects the exit layer and speculation length during inference. DEL dynamically tracks the token acceptance rate if the tokens are drafted at each layer of an LLM and uses that knowledge to heuristically select the optimal exit layer and speculation length. Our experiments across a broad range of models and downstream tasks show that DEL achieves overall speedups of $2.16\times$$\sim$$2.62\times$ over vanilla auto-regressive decoding and improves upon state-of-the-art SD methods, which peak at $2.43\times$, by up to $0.19\times$. The code is available at https://github.com/hoenza/DEL.

</details>
