# 2025-05-19

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.CL](#cs.CL) [Total: 103]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Generative Muscle Stimulation: Physical Assistance by Constraining Multimodal-AI with Biomechanical Knowledge](https://arxiv.org/abs/2505.10648)

*Yun Ho, Romain Nith, Peili Jiang, Shan-Yuan Teng, Pedro Lopes*

**Main category:** cs.HC

**Keywords:** HCI, Electrical muscle stimulation, Context-aware systems, Machine learning, Wearable technology

**Relevance Score:** 7

**TL;DR:** This paper presents a flexible interactive electrical-muscle-stimulation (EMS) system that generates muscle stimulation instructions based on user context, enabling task flexibility without task-specific programming.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current EMS systems are highly specialized and non-contextual, limiting their applicability in dynamic situations. This work aims to create a more versatile EMS interface that can adapt to users' needs in real-time.

**Method:** The developed system uses spoken requests and images from the user's perspective, employing computer vision for object detection and large language models for reasoning, combined with biomechanical knowledge to create suitable muscle stimulation instructions.

**Key Contributions:**

	1. Development of a flexible EMS system that adapts to user context
	2. Integration of computer vision and large language models for instruction generation
	3. Capacity to perform both novel and existing tasks without specific programming

**Result:** The system successfully generates context-aware muscle stimulation interactions, allowing for new capabilities (like opening child-proof bottles) while also replicating existing interactions without specific programming.

**Limitations:** The system may face challenges in accurately interpreting complex user contexts and ensuring safe muscle stimulation at all times.

**Conclusion:** This research indicates a significant advancement toward general-purpose EMS interfaces that accommodate contextual variations and enhance user assistance.

**Abstract:** Decades of interactive electrical-muscle-stimulation (EMS) revealed its promise as a wearable interface for physical assistance-EMS directly demonstrates movements through the users' body (e.g., shaking a spray-can before painting). However, interactive EMS-systems are highly-specialized because their feedback is (1) fixed (e.g., one program executes spray-can instructions, another executes piano instructions) and (2) non-contextual (e.g., using a spray-can while cooking likely involves cooking oil, not paint, and thus shaking is unnecessary). To address this, we explored a more flexible approach and engineered a system that generates muscle-stimulation-instructions given the user's context. Through our examples, we show that such a system is flexible: it enables unprecedented EMS-interactions (e.g., opening a child-proof pill bottle cap) but also replicates existing systems (e.g., shake a spray can)-all without requiring task-specific programming. To achieve this, our system takes in user's spoken-requests and images from their point of view. It uses computer vision (e.g., detect objects/handedness) and large-language-models (e.g., reason about objects/situations) to generate textual-instructions. Finally, these instructions are then constrained by biomechanical-knowledge (e.g., joint limits, kinematic-chain, EMS capabilities) to produce suitable muscle-stimulation gestures. We believe our concept marks a shift toward more general-purpose EMS-interfaces, enabling more flexible and context-aware assistance.

</details>


### [2] [It's only fair when I think it's fair: How Gender Bias Alignment Undermines Distributive Fairness in Human-AI Collaboration](https://arxiv.org/abs/2505.10661)

*Domenique Zipperling, Luca Deck, Julia Lanzl, Niklas Kühl*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, Gender bias, Fairness perceptions

**Relevance Score:** 8

**TL;DR:** The paper explores the impact of gender bias alignment on human-AI collaboration, specifically how perceptions of fairness influence reliance on AI recommendations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how gender bias in AI impacts human trust and acceptance of AI recommendations in consequential decision-making.

**Method:** A 2x2 between-subject study examining the interaction between gender bias alignment and perceptions of fairness in human-AI teams.

**Key Contributions:**

	1. Investigates the role of gender bias in human-AI collaboration
	2. Demonstrates the influence of fairness perceptions on AI reliance
	3. Highlights that formal fairness is insufficient without bias alignment

**Result:** Findings show that gender bias alignment affects fairness perceptions and reliance on AI, indicating that just designing a fair AI is not enough for effective collaboration.

**Limitations:** Focuses solely on gender bias; other biases may also influence perceptions and reliance.

**Conclusion:** Human reliance on AI recommendations decreases if biases do not align, emphasizing the need for both fair AI systems and aligned human perceptions.

**Abstract:** Human-AI collaboration is increasingly relevant in consequential areas where AI recommendations support human discretion. However, human-AI teams' effectiveness, capability, and fairness highly depend on human perceptions of AI. Positive fairness perceptions have been shown to foster trust and acceptance of AI recommendations. Yet, work on confirmation bias highlights that humans selectively adhere to AI recommendations that align with their expectations and beliefs -- despite not being necessarily correct or fair. This raises the question whether confirmation bias also transfers to the alignment of gender bias between human and AI decisions. In our study, we examine how gender bias alignment influences fairness perceptions and reliance. The results of a 2x2 between-subject study highlight the connection between gender bias alignment, fairness perceptions, and reliance, demonstrating that merely constructing a ``formally fair'' AI system is insufficient for optimal human-AI collaboration; ultimately, AI recommendations will likely be overridden if biases do not align.

</details>


### [3] [NeoLightning: A Modern Reimagination of Gesture-Based Sound Design](https://arxiv.org/abs/2505.10686)

*Yonghyun Kim, Sangheon Park, Marcus Parker, Donghoon Seu, Alexandria Smith*

**Main category:** cs.HC

**Keywords:** gesture recognition, musical interaction, multimedia processing

**Relevance Score:** 4

**TL;DR:** NeoLightning is a modern reinterpretation of the Buchla Lightning, enhancing gesture-based musical interaction with contemporary technology.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To make gesture-based interaction accessible to contemporary users while preserving the innovative spirit of the original Buchla Lightning.

**Method:** Utilizes MediaPipe for deep learning-based gesture recognition, and employs Max/MSP and Processing for real-time multimedia processing.

**Key Contributions:**

	1. Modern reinterpretation of Buchla Lightning
	2. Integration of deep learning for gesture recognition
	3. Real-time multimedia processing capabilities

**Result:** Offers precise, low-latency gesture recognition and immersive 3D interaction for musical performance and sound design.

**Limitations:** 

**Conclusion:** NeoLightning expands possibilities for expressive performance and interactive sound design, redefining gesture-based musical interaction.

**Abstract:** This paper introduces NeoLightning, a modern reinterpretation of the Buchla Lightning. NeoLightning preserves the innovative spirit of Don Buchla's "Buchla Lightning" (introduced in the 1990s) while making its gesture-based interaction accessible to contemporary users. While the original Buchla Lightning and many other historical instruments were groundbreaking in their time, they are now largely unsupported, limiting user interaction to indirect experiences. To address this, NeoLightning leverages MediaPipe for deep learning-based gesture recognition and employs Max/MSP and Processing for real-time multimedia processing. The redesigned system offers precise, low-latency gesture recognition and immersive 3D interaction. By merging the creative spirit of the original Lightning with modern advancements, NeoLightning redefines gesture-based musical interaction, expanding possibilities for expressive performance and interactive sound design.

</details>


### [4] [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)

*Omar Shaikh, Shardul Sapkota, Shan Rizvi, Eric Horvitz, Joon Sung Park, Diyi Yang, Michael S. Bernstein*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, General User Model, Multimodal Observations, Proactive Assistants, Intelligent Systems

**Relevance Score:** 9

**TL;DR:** This paper presents a general user model (GUM) architecture that learns user preferences from multimodal observations to enhance human-computer interaction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current user models which are fragmented and unable to understand user preferences and habits cohesively.

**Method:** The GUM architecture processes unstructured observations (like device screenshots) to create confidence-weighted propositions about user knowledge and preferences, revising these as more data is observed.

**Key Contributions:**

	1. Introduction of the General User Model (GUM) architecture.
	2. Application of multimodal observations for user understanding.
	3. Development of proactive assistants (GUMBOs) that execute actions based on inferred user preferences.

**Result:** GUMs can accurately infer user intentions and struggles from interaction data, enabling proactive assistants that perform actions without explicit user requests.

**Limitations:** 

**Conclusion:** GUMs facilitate the development of interactive systems that anticipate user needs using multimodal context understanding, moving closer to the visions of seamless HCI.

**Abstract:** Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.

</details>


### [5] [Alexandria: A Library of Pluralistic Values for Realtime Re-Ranking of Social Media Feeds](https://arxiv.org/abs/2505.10839)

*Akaash Kolluri, Renn Su, Farnaz Jahanbakhsh, Dora Zhao, Tiziano Piccardi, Michael S. Bernstein*

**Main category:** cs.HC

**Keywords:** social media algorithms, user values, content re-ranking, LLM, browser extension

**Relevance Score:** 7

**TL;DR:** The paper presents a library of 78 values for social media algorithms, addressing the limitations of engagement-focused ranking by implementing LLM-powered content classifiers for personalized feed re-ranking via a browser extension called Alexandria.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of social media feed ranking algorithms that overly focus on engagement, the authors propose a comprehensive set of values that can enhance user experience and well-being.

**Method:** Developed an extensive library of values and implemented it into LLM-powered classifiers which are integrated into a browser extension that re-ranks social media feeds based on user-selected values. The effectiveness was evaluated through qualitative and quantitative user studies.

**Key Contributions:**

	1. Creation of a library of 78 values for social media algorithms
	2. Development of the Alexandria browser extension for real-time feed re-ranking
	3. Demonstration of user engagement through qualitative and quantitative studies

**Result:** User studies showed that a larger library of values allows for more nuanced user preferences and enhances control over content displayed in social media feeds.

**Limitations:** The study was limited to specific platforms and may not generalize across all social media contexts or user demographics.

**Conclusion:** The study concludes that incorporating a diverse range of values into social media ranking algorithms is feasible and beneficial for users, enabling personalized content delivery aligned with their individual values.

**Abstract:** Social media feed ranking algorithms fail when they too narrowly focus on engagement as their objective. The literature has asserted a wide variety of values that these algorithms should account for as well -- ranging from well-being to productive discourse -- far more than can be encapsulated by a single topic or theory. In response, we present a $\textit{library of values}$ for social media algorithms: a pluralistic set of 78 values as articulated across the literature, implemented into LLM-powered content classifiers that can be installed individually or in combination for real-time re-ranking of social media feeds. We investigate this approach by developing a browser extension, $\textit{Alexandria}$, that re-ranks the X/Twitter feed in real time based on the user's desired values. Through two user studies, both qualitative (N=12) and quantitative (N=257), we found that diverse user needs require a large library of values, enabling more nuanced preferences and greater user control. With this work, we argue that the values criticized as missing from social media ranking algorithms can be operationalized and deployed today through end-user tools.

</details>


### [6] [Conversations With The Stressed Body: Facilitating Stress Self-Disclosure Among Adolescent Girls Through An Embodied Approach](https://arxiv.org/abs/2505.10863)

*Xinglin Sun, Caroline Claisse, Runhua Zhang, Xinyu Wu, Jialin Yuan, Qi Wang*

**Main category:** cs.HC

**Keywords:** embodied methods, self-disclosure, stress relief, adolescent girls, HCI

**Relevance Score:** 7

**TL;DR:** This study explores how embodied methods and technologies can foster stress-related self-disclosure among adolescent girls, revealing coping strategies and insights into mental health challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant mental health challenges faced by adolescent girls during their transition to adulthood, particularly focusing on stress relief through interactive technologies.

**Method:** A co-design workshop was conducted where nine participants aged 15 to 18 interacted with Embodied Probes, engaging with their bodies to express sensations and design prototypes for stress management.

**Key Contributions:**

	1. Introduced Embodied Probes as a novel method for stress self-disclosure.
	2. Revealed tailored needs for stress perception and relief among adolescent girls.
	3. Provided design implications for using embodied methods in HCI research focused on mental well-being.

**Result:** The workshop uncovered insights into somatic symptoms and coping strategies for stress, illustrating how embodied methods enhance stress self-disclosure.

**Limitations:** 

**Conclusion:** Embodied technologies can effectively support mental well-being by facilitating self-disclosure among young women, providing valuable design implications for the HCI community.

**Abstract:** Adolescent girls face significant mental health challenges during their transition to adulthood, often experiencing heightened stress from various sources. While various interactive technologies for self-disclosure had been explored to support stress relief, little is known about how to encourage stress-related self-disclosure through an embodied approach. This study presents a co-design workshop centred on Embodied Probes, a series of artefacts and activities incorporating embodied methods and technologies. During the workshop, nine participants aged 15 to 18 engaged with their bodies, expressed bodily sensations through tangible means, and designed embodied prototypes tailored to their personal needs for stress perception and relief. The workshop revealed insights into somatic symptoms, sources, and coping strategies for stress among adolescent girls, as well as how embodied methods can support their stress self-disclosure. This paper contributes to the HCI community by offering design implications on leveraging embodied technologies to support self-disclosure for young women's mental well-being.

</details>


### [7] [Anti-Sensing: Defense against Unauthorized Radar-based Human Vital Sign Sensing with Physically Realizable Wearable Oscillators](https://arxiv.org/abs/2505.10864)

*Md Farhan Tasnim Oshim, Nigel Doering, Bashima Islam, Tsui-Wei Weng, Tauhidur Rahman*

**Main category:** cs.HC

**Keywords:** Ultra-Wideband, privacy, Human-Robot Interaction, vital sign monitoring, Anti-Sensing

**Relevance Score:** 8

**TL;DR:** This paper presents Anti-Sensing, a defense mechanism that uses oscillatory motion from wearable devices to disrupt UWB radar sensing of physiological data, enhancing privacy in healthcare applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy concerns arising from the use of UWB radar technology for non-line-of-sight vital sign monitoring in healthcare, particularly in human-robot interactions.

**Method:** The approach introduces physically realizable perturbations that mimic natural cardiac motion to mislead heart rate estimations by UWB radar. It involves a gradient-based algorithm to optimize oscillation frequency and amplitude for maximum disruption while maintaining physiological plausibility.

**Key Contributions:**

	1. Introduction of Anti-Sensing as a privacy protection mechanism
	2. Development of an optimization algorithm for perturbation parameters
	3. Empirical validation of the effectiveness of the proposed solution through simulations and experiments.

**Result:** Simulations and real-world experiments show that Anti-Sensing significantly degrades the accuracy of heart rate sensing models relying on UWB radar data.

**Limitations:** The effectiveness of the approach may vary based on specific radar technologies and environmental conditions that affect radar sensing.

**Conclusion:** Anti-Sensing offers a practical solution for preserving privacy against unauthorized radar-based sensing, demonstrating that disruptive perturbations can effectively shield sensitive physiological information.

**Abstract:** Recent advancements in Ultra-Wideband (UWB) radar technology have enabled contactless, non-line-of-sight vital sign monitoring, making it a valuable tool for healthcare. However, UWB radar's ability to capture sensitive physiological data, even through walls, raises significant privacy concerns, particularly in human-robot interactions and autonomous systems that rely on radar for sensing human presence and physiological functions. In this paper, we present Anti-Sensing, a novel defense mechanism designed to prevent unauthorized radar-based sensing. Our approach introduces physically realizable perturbations, such as oscillatory motion from wearable devices, to disrupt radar sensing by mimicking natural cardiac motion, thereby misleading heart rate (HR) estimations. We develop a gradient-based algorithm to optimize the frequency and spatial amplitude of these oscillations for maximal disruption while ensuring physiological plausibility. Through both simulations and real-world experiments with radar data and neural network-based HR sensing models, we demonstrate the effectiveness of Anti-Sensing in significantly degrading model accuracy, offering a practical solution for privacy preservation.

</details>


### [8] [Empowering the Teaching and Learning of Geometry in Basic Education by Combining Extended Reality and Machine Learning](https://arxiv.org/abs/2505.11056)

*Carlos R. Cunha, André Moreira, Sílvia Coelho, Vítor Mendonça, João Pedro Gomes*

**Main category:** cs.HC

**Keywords:** Extended Reality, Machine Learning, Geometry Education, Teaching Innovation, Student Engagement

**Relevance Score:** 4

**TL;DR:** This paper proposes a model combining Extended Reality and Machine Learning to innovate geometry teaching by enhancing student engagement and providing insights for both teachers and students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address students' learning difficulties in mathematics, specifically geometry, due to a lack of motivation and innovative teaching tools.

**Method:** The authors proposed a conceptual model integrating Extended Reality with Machine Learning, followed by prototyping for laboratory validation.

**Key Contributions:**

	1. Introduction of a novel model for geometry education
	2. Validation through prototyping
	3. Insights for teachers and students to enhance teaching-learning dynamics

**Result:** The model was validated through prototyping, demonstrating its potential to innovate the geometry teaching-learning process and providing useful insights for users.

**Limitations:** 

**Conclusion:** The integration of Extended Reality and Machine Learning can significantly enhance the methodology of teaching geometry, improving engagement and learning outcomes.

**Abstract:** Technology has helped to innovate in the teaching-learning process. Today's students are more demanding actors when it comes to the environment, they have at their disposal to learn, experiment and develop critical thinking. The area of mathematics has successively suffered from students' learning difficulties, whether due to lack of motivation, low abstraction ability, or lack of new tools for teachers to bring innovation into the classroom and outside it. While it is true that digitalization has entered schools, it often follows a process of digital replication of approaches and materials that were previously only available on physical media. This work focuses on the use of Extended Realities for teaching mathematics, and very particularly in the teaching of geometry, with a proposition of a conceptual model that combines the use of Extended Reality and Machine Learning. The proposed model was subject to prototyping, which is presented as a form of laboratory validation as a contribution to innovate the way in which the geometry teaching-learning process is developed, as well as through the ability to obtain useful insights for teachers and students throughout the process.

</details>


### [9] [Sliding Speed Influences Electrovibration-Induced Finger Friction Dynamics on Touchscreens](https://arxiv.org/abs/2505.11162)

*Jagan K Balasubramanian, Daan M Pool, Yasemin Vardar*

**Main category:** cs.HC

**Keywords:** Electrovibration, Touchscreens, Haptic feedback, Finger friction, Skin mechanics

**Relevance Score:** 7

**TL;DR:** This paper investigates how variations in sliding speed and applied force impact electrovibration-induced finger friction on touchscreens. It reveals a model to predict friction response based on these exploration conditions, enhancing haptic feedback technology.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Electrovibration technology aims to improve user experience on touchscreen devices by providing tactile feedback, but variations in user interaction complicate the rendering of realistic textures.

**Method:** Ten participants slid their fingers on an electrovibration-enabled touchscreen at varying speeds and forces, measuring contact forces and skin accelerations. A first-order system model was used for finger-touchscreen friction, and a mass-spring-damper model was used for skin mechanics.

**Key Contributions:**

	1. Development of a model for finger friction on touchscreens considering sliding speed variations.
	2. Quantitative analysis of the effects of speed and force on finger mechanics during touchscreen interactions.
	3. Insights into the influence of skin mechanics on electrovibration technology.

**Result:** The study found that sliding speed significantly influenced the friction response, modifying parameters like cutoff frequency, mass, and stiffness of the finger. A 1 mm/s increase in speed resulted in specific quantified increases in these parameters.

**Limitations:** 

**Conclusion:** A practical model for electrovibration-induced finger friction was developed, facilitating consistent haptic feedback and addressing challenges in texture rendering on touchscreens.

**Abstract:** Electrovibration technology can render tactile textures on capacitive touchscreens by modulating friction between the finger and the screen through electrostatic attraction force generated by applying an alternating voltage signal to the screen. This signal should be carefully calibrated for realistic and robust texture rendering. However, this process is challenging due to variations in sliding speed, applied force, and individual skin mechanics, which affect friction in complex and unpredictable ways. Here, we investigate how exploration conditions affect electrovibration-induced finger friction on touchscreens and the role of skin mechanics in this process. Ten participants slid their index fingers across an electrovibration-enabled touchscreen at five sliding speeds ($20\sim100$ mm/s) and applied force levels ($0.2\sim0.6$ N) while we measured contact forces and skin accelerations. The touchscreen was excited with amplitude-modulated voltage signals across frequencies relevant to touch. We modeled the finger-touchscreen friction response as a first-order system and the skin mechanics as a mass-spring-damper system. Our results showed that the sliding speed influenced the cutoff frequency of the friction response as well as the moving mass and stiffness of the finger for the tested exploration ranges. Specifically, for every 1 mm/s increase in speed, the cutoff frequency, the finger moving mass, and stiffness increased by $13.8$ Hz, $3.23\times 10^{-5}$ kg, and $4.04$ N/m, respectively. Further correlation analysis revealed that finger stiffness affected the cutoff frequency more than the moving mass. Finally, we developed a practical model for electrovibration-induced finger friction on touchscreens that accounts for sliding speed variations, paving the way for delivering consistent haptic feedback through electrovibration.

</details>


### [10] [Large Language Model Use Impact Locus of Control](https://arxiv.org/abs/2505.11406)

*Jenny Xiyu Fu, Brennan Antone, Kowe Kadoma, Malte Jung*

**Main category:** cs.HC

**Keywords:** AI, locus of control, employment status, personal agency, identity

**Relevance Score:** 7

**TL;DR:** The paper investigates how co-writing with AI influences individuals' locus of control, revealing that employment status significantly affects reliance on AI and personal agency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the psychological impact of co-writing with AI on self-perception and locus of control.

**Method:** An empirical study was conducted with 462 participants to assess the relationship between employment status, reliance on AI, and locus of control.

**Key Contributions:**

	1. Identifies the psychological effects of AI co-writing on locus of control.
	2. Finds a correlation between employment status and reliance on AI.
	3. Highlights the implications of AI on personal agency and identity.

**Result:** Employed participants showed greater reliance on AI and a movement towards an internal locus of control, whereas unemployed participants experienced a decline in personal agency.

**Limitations:** The study is limited to the sample population and may not generalize across different demographics or cultural contexts.

**Conclusion:** The findings suggest that AI has a significant impact on personal agency and identity, particularly influenced by employment status.

**Abstract:** As AI tools increasingly shape how we write, they may also quietly reshape how we perceive ourselves. This paper explores the psychological impact of co-writing with AI on people's locus of control. Through an empirical study with 462 participants, we found that employment status plays a critical role in shaping users' reliance on AI and their locus of control. Current results demonstrated that employed participants displayed higher reliance on AI and a shift toward internal control, while unemployed users tended to experience a reduction in personal agency. Through quantitative results and qualitative observations, this study opens a broader conversation about AI's role in shaping personal agency and identity.

</details>


### [11] [EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions](https://arxiv.org/abs/2505.11417)

*Patryk Bartkowiak, Michal Podstawski*

**Main category:** cs.HC

**Keywords:** user profiling, multi-session interactions, small language models, smart home environments, privacy-respecting AI

**Relevance Score:** 8

**TL;DR:** A dataset and benchmark for improving small language models in smart home environments by focusing on user profiling from multi-session interactions is introduced.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance small language models for user profiling in smart home environments while ensuring user privacy and efficient on-device processing.

**Method:** The paper introduces a dataset with structured user profiles based on routines, allowing LLMs to generate simulated dialogues for benchmarking profile reconstruction tasks.

**Key Contributions:**

	1. Introduction of a novel dataset for user profiling from natural language interactions in smart homes
	2. Benchmarking of state-of-the-art compact language models against large foundation models
	3. Assessment of the performance gap in profile reconstruction tasks

**Result:** Small language models can reconstruct user profiles but underperform compared to large foundation models, highlighting the challenge of balancing model size and performance with on-device processing benefits.

**Limitations:** The small models, while effective, still lack the accuracy of large models in understanding user behavior.

**Conclusion:** The dataset offers a crucial step toward developing intelligent AI systems that respect user privacy and adapt on user-owned devices.

**Abstract:** This paper introduces a novel dataset and evaluation benchmark designed to assess and improve small language models deployable on edge devices, with a focus on user profiling from multi-session natural language interactions in smart home environments. At the core of the dataset are structured user profiles, each defined by a set of routines - context-triggered, repeatable patterns of behavior that govern how users interact with their home systems. Using these profiles as input, a large language model (LLM) generates corresponding interaction sessions that simulate realistic, diverse, and context-aware dialogues between users and their devices.   The primary task supported by this dataset is profile reconstruction: inferring user routines and preferences solely from interactions history. To assess how well current models can perform this task under realistic conditions, we benchmarked several state-of-the-art compact language models and compared their performance against large foundation models. Our results show that while small models demonstrate some capability in reconstructing profiles, they still fall significantly short of large models in accurately capturing user behavior. This performance gap poses a major challenge - particularly because on-device processing offers critical advantages, such as preserving user privacy, minimizing latency, and enabling personalized experiences without reliance on the cloud. By providing a realistic, structured testbed for developing and evaluating behavioral modeling under these constraints, our dataset represents a key step toward enabling intelligent, privacy-respecting AI systems that learn and adapt directly on user-owned devices.

</details>


### [12] [An HCAI Methodological Framework (HCAI-MF): Putting It Into Action to Enable Human-Centered AI](https://arxiv.org/abs/2311.16027)

*Wei Xu, Zaifeng Gao, Marvin Dainoff*

**Main category:** cs.HC

**Keywords:** Human-Centered AI, Methodological Framework, Design Principles, Implementation Strategy, AI Systems

**Relevance Score:** 8

**TL;DR:** This paper proposes a comprehensive framework for Human-Centered Artificial Intelligence (HCAI) to address challenges in its implementation by outlining a methodological approach.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the implementation of Human-Centered AI by providing a structured methodological framework that addresses current gaps in practice.

**Method:** The paper introduces the HCAI methodological framework (HCAI-MF) which includes components such as a requirement hierarchy, method taxonomy, interdisciplinary collaborations, and multi-level design paradigms. A case study is used to demonstrate its applicability.

**Key Contributions:**

	1. Proposes a comprehensive methodological framework for HCAI.
	2. Demonstrates practical implications through a case study.
	3. Offers actionable recommendations for implementation challenges.

**Result:** The proposed HCAI-MF offers actionable recommendations and a three-layer implementation strategy, providing a systematic approach to enhance the effectiveness of AI system development with human-centered principles.

**Limitations:** The paper's framework may need further empirical validation across diverse applications and contexts.

**Conclusion:** The HCAI-MF framework is positioned as a vital tool for advancing the practice of Human-Centered AI and addressing the challenges faced in the field.

**Abstract:** Human-centered artificial intelligence (HCAI) is a design philosophy that prioritizes humans in the design, development, deployment, and use of AI systems, aiming to maximize AI's benefits while mitigating its negative impacts. Despite its growing prominence in literature, the lack of methodological guidance for its implementation poses challenges to HCAI practice. To address this gap, this paper proposes a comprehensive HCAI methodological framework (HCAI-MF) comprising five key components: HCAI requirement hierarchy, approach and method taxonomy, process, interdisciplinary collaboration approach, and multi-level design paradigms. A case study demonstrates HCAI-MF's practical implications, while the paper also analyzes implementation challenges. Actionable recommendations and a "three-layer" HCAI implementation strategy are provided to address these challenges and guide future evolution of HCAI-MF. HCAI-MF is presented as a systematic and executable methodology capable of overcoming current gaps, enabling effective design, development, deployment, and use of AI systems, and advancing HCAI practice.

</details>


### [13] [Augmented Object Intelligence with XR-Objects](https://arxiv.org/abs/2404.13274)

*Mustafa Doga Dogan, Eric J. Gonzalez, Karan Ahuja, Ruofei Du, Andrea Colaço, Johnny Lee, Mar Gonzalez-Franco, David Kim*

**Main category:** cs.HC

**Keywords:** Augmented Object Intelligence, XR, Multimodal Large Language Models, spatial computing, user interaction

**Relevance Score:** 8

**TL;DR:** This paper presents Augmented Object Intelligence (AOI) in XR, enabling physical objects to act as interactive digital entities using MLLMs, through an open-source prototype called XR-Objects.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of integrating physical objects as interactive digital entities in spatial computing.

**Method:** Utilizes real-time object segmentation and classification with Multimodal Large Language Models (MLLMs) to facilitate interaction without pre-registration of objects.

**Key Contributions:**

	1. Definition of the AOI concept and its advantages over traditional AI assistants
	2. Open-source design and implementation of the XR-Objects system
	3. Versatile use cases and validation through user studies

**Result:** Demonstrates the AOI concept through a prototype system (XR-Objects) allowing physical objects to convey information and initiate digital actions, validated by user studies.

**Limitations:** 

**Conclusion:** AOI provides a significant advancement in user interaction with physical objects, offering advantages over traditional AI assistants and showcasing versatility through user engagement.

**Abstract:** Seamless integration of physical objects as interactive digital entities remains a challenge for spatial computing. This paper explores Augmented Object Intelligence (AOI) in the context of XR, an interaction paradigm that aims to blur the lines between digital and physical by equipping real-world objects with the ability to interact as if they were digital, where every object has the potential to serve as a portal to digital functionalities. Our approach utilizes real-time object segmentation and classification, combined with the power of Multimodal Large Language Models (MLLMs), to facilitate these interactions without the need for object pre-registration. We implement the AOI concept in the form of XR-Objects, an open-source prototype system that provides a platform for users to engage with their physical environment in contextually relevant ways using object-based context menus. This system enables analog objects to not only convey information but also to initiate digital actions, such as querying for details or executing tasks. Our contributions are threefold: (1) we define the AOI concept and detail its advantages over traditional AI assistants, (2) detail the XR-Objects system's open-source design and implementation, and (3) show its versatility through various use cases and a user study.

</details>


### [14] [Automating High Quality RT Planning at Scale](https://arxiv.org/abs/2501.11803)

*Riqiang Gao, Mamadou Diallo, Han Liu, Anthony Magliari, Jonathan Sackett, Wilko Verbakel, Sandra Meyers, Rafe Mcbeth, Masoud Zarepisheh, Simon Arberet, Martin Kraus, Florin C. Ghesu, Ali Kamen*

**Main category:** cs.HC

**Keywords:** radiotherapy, AI, automation, treatment planning, dataset

**Relevance Score:** 6

**TL;DR:** The AIRTP system automates radiotherapy planning, enhancing precision and efficiency by generating high-quality treatment plans using AI, overcoming data scarcity issues.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the precision and efficiency of radiotherapy planning, which is traditionally complex and time-intensive, by using AI-driven approaches.

**Method:** The AIRTP system automates critical steps in radiotherapy planning, including organ-at-risk contouring, beam setup, and plan optimization, adhering to clinical guidelines and utilizing existing RT planning software.

**Key Contributions:**

	1. Introduction of the AIRTP system for automated radiotherapy planning.
	2. Development of a novel approach for determining optimization parameters for 3D dose distributions.
	3. Release of a large dataset significantly larger than existing public datasets to support research.

**Result:** The AIRTP pipeline produces treatment plans comparable in quality to manually generated plans, significantly reducing planning time from hours to a more efficient process.

**Limitations:** 

**Conclusion:** The AIRTP system not only improves the efficiency of radiotherapy planning but is also committed to public research with the release of a large dataset for further studies.

**Abstract:** Radiotherapy (RT) planning is complex, subjective, and time-intensive. Advances with artificial intelligence (AI) promise to improve its precision and efficiency, but progress is often limited by the scarcity of large, standardized datasets. To address this, we introduce the Automated Iterative RT Planning (AIRTP) system, a scalable solution for generating high-quality treatment plans. This scalable solution is designed to generate substantial volumes of consistently high-quality treatment plans, overcoming a key obstacle in the advancement of AI-driven RT planning. Our AIRTP pipeline adheres to clinical guidelines and automates essential steps, including organ-at-risk (OAR) contouring, helper structure creation, beam setup, optimization, and plan quality improvement, using AI integrated with RT planning software like Varian Eclipse. Furthermore, a novel approach for determining optimization parameters to reproduce 3D dose distributions, i.e. a method to convert dose predictions to deliverable treatment plans constrained by machine limitations is proposed. A comparative analysis of plan quality reveals that our automated pipeline produces treatment plans of quality comparable to those generated manually, which traditionally require several hours of labor per plan. Committed to public research, the first data release of our AIRTP pipeline includes nine cohorts covering head-and-neck and lung cancer sites to support an AAPM 2025 challenge. To our best knowledge, this dataset features more than 10 times number of plans compared to the largest existing well-curated public dataset. Repo: https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge.

</details>


### [15] [Organize, Then Vote: Exploring Cognitive Load in Quadratic Survey Interfaces](https://arxiv.org/abs/2503.04114)

*Ti-Chung Cheng, Yutong Zhang, Yi-Hung Chou, Vinay Koshy, Tiffany Wenting Li, Karrie Karahalios, Hari Sundaram*

**Main category:** cs.HC

**Keywords:** Quadratic Surveys, Cognitive Load, Human-Centered Design, Preference Elicitation, Collective Decision-Making

**Relevance Score:** 8

**TL;DR:** Introducing a two-phase 'organize-then-vote' Quadratic Survey interface that reduces cognitive load and improves decision-making accuracy compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Quadratic Surveys elicit more accurate preferences but face adoption challenges due to cognitive load; this research aims to mitigate those challenges through thoughtful interface design.

**Method:** A 2x2 between-subject in-lab study compared a two-phase interface with a traditional text interface across Quadratic Surveys with varying option lengths (6 vs. 24).

**Key Contributions:**

	1. Development of a two-phase 'organize-then-vote' Quadratic Survey interface
	2. Empirical validation showing reduced cognitive load and improved engagement in decision-making
	3. Insights into the impact of human-centered design on collective decision-making tools

**Result:** Participants using the two-phase interface spent more time on each option and demonstrated shorter voting edit distances, indicating deeper engagement.

**Limitations:** Focus on a specific context (public resource allotment) may limit generalizability; further testing in diverse scenarios needed.

**Conclusion:** The new interface enhances preference elicitation by reducing cognitive overload, promoting more comprehensive preference construction.

**Abstract:** Quadratic Surveys (QSs) elicit more accurate preferences than traditional methods like Likert-scale surveys. However, the cognitive load associated with QSs has hindered their adoption in digital surveys for collective decision-making. We introduce a two-phase "organize-then-vote" QS to reduce cognitive load. As interface design significantly impacts survey results and accuracy, our design scaffolds survey takers' decision-making while managing the cognitive load imposed by QS. In a 2x2 between-subject in-lab study on public resource allotment, we compared our interface with a traditional text interface across a QS with 6 (short) and 24 (long) options. Two-phase interface participants spent more time per option and exhibited shorter voting edit distances. We qualitatively observed shifts in cognitive effort from mechanical operations to constructing more comprehensive preferences. We conclude that this interface promoted deeper engagement, potentially reducing satisficing behaviors caused by cognitive overload in longer QSs. This research clarifies how human-centered design improves preference elicitation tools for collective decision-making.

</details>


### [16] [ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking](https://arxiv.org/abs/2504.14406)

*Runlong Ye, Patrick Yung Kang Lee, Matthew Varona, Oliver Huang, Carolina Nobre*

**Main category:** cs.HC

**Keywords:** qualitative research, AI integration, human-computer interaction

**Relevance Score:** 8

**TL;DR:** ScholarMate is an interactive system designed to enhance qualitative research by integrating AI assistance with human oversight.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective integration of AI in human-centric sensemaking workflows in qualitative research is increasingly complex due to the volume of document collections.

**Method:** ScholarMate allows researchers to arrange and interact with text snippets on a non-linear canvas, providing AI-assisted theme suggestions, multi-level summarization, and traceability to source documents.

**Key Contributions:**

	1. Development of ScholarMate system for qualitative analysis
	2. Integration of AI assistance with human oversight
	3. Ensures transparency and interpretability in qualitative research

**Result:** Initial pilot studies showed users appreciate the balance between AI suggestions and direct manipulation in maintaining interpretability and trust.

**Limitations:** 

**Conclusion:** By combining automation with human control, ScholarMate improves efficiency and supports interpretability, facilitating productive human-AI collaboration in qualitative analysis tasks.

**Abstract:** Synthesizing knowledge from large document collections is a critical yet increasingly complex aspect of qualitative research and knowledge work. While AI offers automation potential, effectively integrating it into human-centric sensemaking workflows remains challenging. We present ScholarMate, an interactive system designed to augment qualitative analysis by unifying AI assistance with human oversight. ScholarMate enables researchers to dynamically arrange and interact with text snippets on a non-linear canvas, leveraging AI for theme suggestions, multi-level summarization, and evidence-based theme naming, while ensuring transparency through traceability to source documents. Initial pilot studies indicated that users value this mixed-initiative approach, finding the balance between AI suggestions and direct manipulation crucial for maintaining interpretability and trust. We further demonstrate the system's capability through a case study analyzing 24 papers. By balancing automation with human control, ScholarMate enhances efficiency and supports interpretability, offering a valuable approach for productive human-AI collaboration in demanding sensemaking tasks common in knowledge work.

</details>


### [17] [Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses](https://arxiv.org/abs/2505.09819)

*Ruichen Yang, György M. Lévay, Christopher L. Hunt, Dániel Czeiner, Megan C. Hodgson, Damini Agarwal, Rahul R. Kaliki, Nitish V. Thakor*

**Main category:** cs.HC

**Keywords:** Myoelectric prosthesis, Pattern recognition, EMG signals, 3D visual interface, Real-time feedback

**Relevance Score:** 8

**TL;DR:** The paper presents the Reviewer, a 3D visual interface that improves myoelectric prosthesis control by providing real-time feedback on EMG signals, enhancing training effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of producing distinct EMG patterns for reliable movement classification in myoelectric prostheses.

**Method:** A 10-session study with 12 participants comparing performance outcomes after using the Reviewer versus conventional training visualization methods.

**Key Contributions:**

	1. Introduction of the Reviewer as a 3D visual feedback tool for prosthesis control
	2. Demonstration of improved training outcomes in myoelectric control
	3. Reduction of reliance on heuristic adjustments through real-time insights into EMG patterns

**Result:** Participants using the Reviewer achieved higher completion rates, reduced overshoot, and improved efficiency in tasks compared to those using standard methods.

**Limitations:** 

**Conclusion:** The Reviewer enhances PR control in novice operators via structured feedback during training, reducing the need for extensive heuristic recalibrations.

**Abstract:** State-of-the-art upper limb myoelectric prostheses often use pattern recognition (PR) control systems that translate electromyography (EMG) signals into desired movements. As prosthesis movement complexity increases, users often struggle to produce sufficiently distinct EMG patterns for reliable classification. Existing training typically involves heuristic, trial-and-error user adjustments to static decoder boundaries. Goal: We introduce the Reviewer, a 3D visual interface projecting EMG signals directly into the decoder's classification space, providing intuitive, real-time insight into PR algorithm behavior. This structured feedback reduces cognitive load and fosters mutual, data-driven adaptation between user-generated EMG patterns and decoder boundaries. Methods: A 10-session study with 12 able-bodied participants compared PR performance after motor-based training and updating using the Reviewer versus conventional virtual arm visualization. Performance was assessed using a Fitts law task that involved the aperture of the cursor and the control of orientation. Results: Participants trained with the Reviewer achieved higher completion rates, reduced overshoot, and improved path efficiency and throughput compared to the standard visualization group. Significance: The Reviewer introduces decoder-informed motor training, facilitating immediate and consistent PR-based myoelectric control improvements. By iteratively refining control through real-time feedback, this approach reduces reliance on trial-and-error recalibration, enabling a more adaptive, self-correcting training framework. Conclusion: The 3D visual feedback significantly improves PR control in novice operators through structured training, enabling feedback-driven adaptation and reducing reliance on extensive heuristic adjustments.

</details>


### [18] [Characterizing Unintended Consequences in Human-GUI Agent Collaboration for Web Browsing](https://arxiv.org/abs/2505.09875)

*Shuning Zhang, Jingruo Chen, Zhiqi Gao, Jiajing Gao, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** Large Language Models, GUI Agents, Unintended Consequences, User-Centric Design, Human Oversight

**Relevance Score:** 9

**TL;DR:** This paper explores unintended consequences (UCs) of LLM-based GUI agents in web browsing, highlighting key phenomena, influences, and user-initiated mitigations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the unintended consequences of LLM-based GUI agents in web browsing scenarios and improve their design.

**Method:** Social media analysis (221 posts) and semi-structured interviews (14 participants).

**Key Contributions:**

	1. Characterization of three unintended consequences of LLM-based GUI agents.
	2. Insights from social media and interviews to inform design improvements.
	3. Recommendations for balancing automation with human oversight.

**Result:** Identified deficiencies in agents' comprehension, task planning, reliability, and error handling, leading to user frustration, privacy risks, and undermined trust.

**Limitations:** 

**Conclusion:** Mitigation strategies include technical adjustments and manual oversight, stressing the importance of user-focused design in future LLM-based GUI agents.

**Abstract:** The proliferation of Large Language Model (LLM)-based Graphical User Interface (GUI) agents in web browsing scenarios present complex unintended consequences (UCs). This paper characterizes three UCs from three perspectives: phenomena, influence and mitigation, drawing on social media analysis (N=221 posts) and semi-structured interviews (N=14). Key phenomenon for UCs include agents' deficiencies in comprehending instructions and planning tasks, challenges in executing accurate GUI interactions and adapting to dynamic interfaces, the generation of unreliable or misaligned outputs, and shortcomings in error handling and feedback processing. These phenomena manifest as influences from unanticipated actions and user frustration, to privacy violations and security vulnerabilities, and further to eroded trust and wider ethical concerns. Our analysis also identifies user-initiated mitigation, such as technical adjustments and manual oversight, and provides implications for designing future LLM-based GUI agents that are robust, user-centric, and transparent, fostering a crucial balance between automation and human oversight.

</details>


### [19] [Using Virtual Reality in Museums to Bridge the Gap Between Material Heritage and the Interpretation of Its Immaterial Context](https://arxiv.org/abs/2505.10412)

*Carlos R. Cunha, Vítor Mendonça, André Moreira, João Pedro Gomes, Aida Carvalho*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Cultural Heritage, Museum Interpretation, Immersive Experience, Sociocultural Roots

**Relevance Score:** 4

**TL;DR:** The paper discusses leveraging virtual reality (VR) to enhance the interpretation of material heritage in museums, providing tools for visitors to better understand cultural contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in interpreting material heritage solely through observation and to provide visitors with immersive tools for understanding cultural dimensions.

**Method:** The article presents a conceptual model based on the use of virtual reality, specifically prototyped in the Portuguese Museum of Miranda do Douro.

**Key Contributions:**

	1. Introduction of a conceptual model for virtual reality application in heritage interpretation
	2. Prototyping of VR solutions in a specific museum setting
	3. Identification of the cultural relevance of VR in understanding heritage.

**Result:** The proposed VR solutions aim to bridge the gap between material heritage and its immaterial context, leading to a deeper visitor engagement with cultural heritage.

**Limitations:** 

**Conclusion:** The use of VR in museums is positioned as a significant advancement for the interpretation of heritage and the engagement of visitors with cultural traditions.

**Abstract:** Material heritage typically has a whole set of associated immaterial heritage, which is essential to pass on to the visitor as a cultural mission of the destinations and those who manage them. In this sense, the interpretation of material heritage is a complex process that is not a fully efficient process with the mere observation of physical artifacts. In this context, it emerges as fundamental to provide visitors with a set of tools that allow them to correctly interpret the artifacts that come to fully understand the cultural dimension of the destinations and their heritage. Accordingly, the role of virtual reality can leverage the creation of innovative and immersive solutions that allow the visitor to understand and feel part of their own heritage and its ancestral component that defines the sociocultural roots of destinations and their civilizational traditions. This article, after dissecting and substantiating the role of virtual reality in the interpretation of heritage, presents a conceptual model, based on the use of virtual reality, which was, in part, prototyped in the scenario of the Portuguese Museum in the city of Miranda do Douro. This proposal is an ongoing contribution to the creation of innovative and immersive tools for the interpretation of heritage.

</details>


### [20] [Creating General User Models from Computer Use](https://arxiv.org/abs/2505.10831)

*Omar Shaikh, Shardul Sapkota, Shan Rizvi, Eric Horvitz, Joon Sung Park, Diyi Yang, Michael S. Bernstein*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, User Modeling, Proactive Assistants

**Relevance Score:** 9

**TL;DR:** This paper introduces a General User Model (GUM) architecture that learns about users through their interactions, enabling various applications in human-computer interaction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop technology that comprehensively understands user preferences and habits beyond isolated contexts, addressing the limitations of current user models.

**Method:** The GUM architecture processes unstructured user observations, inferring confidence-weighted propositions about user knowledge and preferences through multimodal data.

**Key Contributions:**

	1. Introduction of the General User Model (GUM) architecture
	2. Illustration of various applications across chat-based assistants and OS notifications
	3. Development of proactive assistants (GUMBOs) that execute user-oriented actions

**Result:** GUMs accurately infer user intentions and preferences, leading to the development of proactive assistants that can perform actions based on inferred context without explicit user requests.

**Limitations:** 

**Conclusion:** GUMs offer a novel method for understanding unstructured context in user interactions, paving the way for advanced interactive systems that can anticipate user needs.

**Abstract:** Human-computer interaction has long imagined technology that understands us-from our preferences and habits, to the timing and purpose of our everyday actions. Yet current user models remain fragmented, narrowly tailored to specific apps, and incapable of the flexible reasoning required to fulfill these visions. This paper presents an architecture for a general user model (GUM) that learns about you by observing any interaction you have with your computer. The GUM takes as input any unstructured observation of a user (e.g., device screenshots) and constructs confidence-weighted propositions that capture that user knowledge and preferences. GUMs can infer that a user is preparing for a wedding they're attending from messages with a friend. Or recognize that a user is struggling with a collaborator's feedback on a draft by observing multiple stalled edits and a switch to reading related work. GUMs introduce an architecture that infers new propositions about a user from multimodal observations, retrieves related propositions for context, and continuously revises existing propositions. To illustrate the breadth of applications that GUMs enable, we demonstrate how they augment chat-based assistants with context, manage OS notifications to selectively surface important information, and enable interactive agents that adapt to preferences across apps. We also instantiate proactive assistants (GUMBOs) that discover and execute useful suggestions on a user's behalf using their GUM. In our evaluations, we find that GUMs make calibrated and accurate inferences about users, and that assistants built on GUMs proactively identify and perform actions that users wouldn't think to request explicitly. Altogether, GUMs introduce methods that leverage multimodal models to understand unstructured context, enabling long-standing visions of HCI and entirely new interactive systems that anticipate user needs.

</details>


### [21] [Large Language Model Use Impact Locus of Control](https://arxiv.org/abs/2505.11406)

*Jenny Xiyu Fu, Brennan Antone, Kowe Kadoma, Malte Jung*

**Main category:** cs.HC

**Keywords:** AI, personal agency, employment status, locus of control, psychological impact

**Relevance Score:** 7

**TL;DR:** This paper investigates how using AI in writing affects users' psychological state and sense of personal agency, highlighting different effects based on employment status.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the psychological consequences of co-writing with AI and its influence on the sense of personal agency and identity.

**Method:** An empirical study involving 462 participants was conducted to assess the impact of AI on users' locus of control, with a focus on employment status as a key variable.

**Key Contributions:**

	1. Investigates the psychological impact of AI on co-writing and personal agency
	2. Identifies employment status as a critical factor in users' reliance on AI
	3. Initiates dialogue on the broader implications of AI in shaping identity

**Result:** The study found that employed individuals relied more on AI and experienced a shift towards internal control, while unemployed participants felt a diminished sense of personal agency.

**Limitations:** The study is limited to self-reported measures and may not capture all dimensions of personal agency.

**Conclusion:** The findings suggest that AI tools may significantly influence users' perceptions of their own agency and identity, prompting further discussion on the societal implications of AI integration in writing.

**Abstract:** As AI tools increasingly shape how we write, they may also quietly reshape how we perceive ourselves. This paper explores the psychological impact of co-writing with AI on people's locus of control. Through an empirical study with 462 participants, we found that employment status plays a critical role in shaping users' reliance on AI and their locus of control. Current results demonstrated that employed participants displayed higher reliance on AI and a shift toward internal control, while unemployed users tended to experience a reduction in personal agency. Through quantitative results and qualitative observations, this study opens a broader conversation about AI's role in shaping personal agency and identity.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [22] [Artificial Intelligence Bias on English Language Learners in Automatic Scoring](https://arxiv.org/abs/2505.10643)

*Shuchen Guo, Yun Wang, Jichao Yu, Xuansheng Wu, Bilgehan Ayik, Field M. Watts, Ehsan Latif, Ninghao Liu, Lei Liu, Xiaoming Zhai*

**Main category:** cs.CL

**Keywords:** Automatic scoring, English Language Learners, BERT, Scoring bias, Education

**Relevance Score:** 5

**TL;DR:** The study examines scoring biases in automatic scoring systems for English Language Learners (ELLs) in science assessments, finding that large training datasets mitigate bias.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate biases in automatic scoring systems towards English Language Learners (ELLs) and ensure fair assessment in educational contexts.

**Method:** The study fine-tuned BERT using four datasets with varying proportions of ELL and non-ELL responses and analyzed scoring accuracy through Friedman tests.

**Key Contributions:**

	1. Identification of scoring bias with ELLs in automatic systems
	2. Analysis impact of training data size on scoring fairness
	3. Quantitative measurements of Mean Score Gaps between ELLs and non-ELLs

**Result:** The analysis showed no AI bias with larger training datasets, while smaller datasets raised concerns about disparities in scoring accuracy.

**Limitations:** Concerns about scoring disparities when training datasets are small, specifically when ELL responses are limited in number.

**Conclusion:** Scoring systems can be unbiased and effective for ELLs if trained on sufficiently large datasets; caution is warranted for smaller datasets to avoid scoring disparities.

**Abstract:** This study investigated potential scoring biases and disparities toward English Language Learners (ELLs) when using automatic scoring systems for middle school students' written responses to science assessments. We specifically focus on examining how unbalanced training data with ELLs contributes to scoring bias and disparities. We fine-tuned BERT with four datasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting the real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced mixed dataset with equal representation of both groups. The study analyzed 21 assessment items: 10 items with about 30,000 ELL responses, five items with about 1,000 ELL responses, and six items with about 200 ELL responses. Scoring accuracy (Acc) was calculated and compared to identify bias using Friedman tests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and then calculated the differences in MSGs generated through both the human and AI models to identify the scoring disparities. We found that no AI bias and distorted disparities between ELLs and non-ELLs were found when the training dataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could exist if the sample size is limited (ELL = 200).

</details>


### [23] [GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?](https://arxiv.org/abs/2505.10714)

*Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Jiashu He, Joshua Bergerson, John K Hutchison, Jordan Branham, Camillo J Taylor, Tanwi Mallick*

**Main category:** cs.CL

**Keywords:** geo-spatial data, benchmarking, foundation models, climate variables, science research

**Relevance Score:** 4

**TL;DR:** GeoGrid-Bench is a benchmark for evaluating foundation models on geo-spatial data, highlighting their performance and insights into spatiotemporal analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how foundation models can enhance understanding and analysis of geo-spatial data for scientific research.

**Method:** GeoGrid-Bench comprises large-scale, real-world data including 16 climate variables across 150 locations, featuring 3,200 question-answer pairs generated from expert-curated templates.

**Key Contributions:**

	1. Introduction of GeoGrid-Bench for geo-spatial data benchmarking
	2. Evaluation of performance across various foundation models
	3. Insights into the applicability of models in scientific research contexts

**Result:** Vision-language models outperform others in various geo-spatial tasks, demonstrating strengths and limitations in handling different aspects of geo-spatial data.

**Limitations:** 

**Conclusion:** The benchmark provides valuable insights into applying foundation models effectively for geo-spatial data analysis in scientific research.

**Abstract:** We present GeoGrid-Bench, a benchmark designed to evaluate the ability of foundation models to understand geo-spatial data in the grid structure. Geo-spatial datasets pose distinct challenges due to their dense numerical values, strong spatial and temporal dependencies, and unique multimodal representations including tabular data, heatmaps, and geographic visualizations. To assess how foundation models can support scientific research in this domain, GeoGrid-Bench features large-scale, real-world data covering 16 climate variables across 150 locations and extended time frames. The benchmark includes approximately 3,200 question-answer pairs, systematically generated from 8 domain expert-curated templates to reflect practical tasks encountered by human scientists. These range from basic queries at a single location and time to complex spatiotemporal comparisons across regions and periods. Our evaluation reveals that vision-language models perform best overall, and we provide a fine-grained analysis of the strengths and limitations of different foundation models in different geo-spatial tasks. This benchmark offers clearer insights into how foundation models can be effectively applied to geo-spatial data analysis and used to support scientific research.

</details>


### [24] [A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment](https://arxiv.org/abs/2505.10717)

*Jean-Philippe Corbeil, Amin Dada, Jean-Michel Attendu, Asma Ben Abacha, Alessandro Sordoni, Lucas Caccia, François Beaulieu, Thomas Lin, Jens Kleesiek, Paul Vozila*

**Main category:** cs.CL

**Keywords:** small language models, clinical NLP, MediPhi

**Relevance Score:** 9

**TL;DR:** The paper proposes the MediPhi framework for adapting small language models (SLMs) into high-performing clinical models, addressing challenges of computation costs and data sensitivity in healthcare settings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** High computation costs and latency of large language models limit their deployment in clinical settings; small language models need adaptation for effective use in healthcare.

**Method:** The MediPhi framework utilizes pre-instruction tuning of experts on medical corpora, model merging, and alignment for clinical tasks. It introduces the CLUE+ benchmark and builds the MediFlow collection for training.

**Key Contributions:**

	1. Introduction of MediPhi for adapting small language models in clinical settings
	2. Development of the CLUE+ benchmark for comprehensive evaluation of clinical NLP
	3. Creation of MediFlow, a synthetic dataset for medical NLP tasks

**Result:** Expert models showed significant improvements over base models on the CLUE+ benchmark without task-specific fine-tuning, outperforming GPT-4-0125 in various clinical tasks.

**Limitations:** 

**Conclusion:** The MediPhi framework effectively adapts SLMs for clinical use, enhancing performance on medical NLP tasks with the introduction of MediFlow and rigorous benchmarking.

**Abstract:** High computation costs and latency of large language models such as GPT-4 have limited their deployment in clinical settings. Small language models (SLMs) offer a cost-effective alternative, but their limited capacity requires biomedical domain adaptation, which remains challenging. An additional bottleneck is the unavailability and high sensitivity of clinical data. To address these challenges, we propose a novel framework for adapting SLMs into high-performing clinical models. We introduce the MediPhi collection of 3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning of experts on relevant medical and clinical corpora (PMC, Medical Guideline, MedWiki, etc.), model merging, and clinical-tasks alignment. To cover most clinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our expert models deliver relative improvements on this benchmark over the base model without any task-specific fine-tuning: 64.3% on medical entities, 49.5% on radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by 14%). We unify the expert models into MediPhi via model merging, preserving gains across benchmarks. Furthermore, we built the MediFlow collection, a synthetic dataset of 2.5 million high-quality instructions on 14 medical NLP tasks, 98 fine-grained document types, and JSON format support. Alignment of MediPhi using supervised fine-tuning and direct preference optimization achieves further gains of 18.9% on average.

</details>


### [25] [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/abs/2505.10718)

*Siddharth Suresh, Kushin Mukherjee, Tyler Giallanza, Xizheng Yu, Mia Patil, Jonathan D. Cohen, Timothy T. Rogers*

**Main category:** cs.CL

**Keywords:** semantic feature norms, large language models, cognitive science, semantic similarity, NOVA

**Relevance Score:** 8

**TL;DR:** This paper introduces NOVA, a novel dataset of semantic feature norms that combines human-generated norms with responses from large language models (LLMs), showing improved feature density and predictive performance in semantic similarity judgments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the trade-offs in traditional semantic feature norming methods while enhancing the depth of human conceptual knowledge representation.

**Method:** A dataset of human-generated feature norms is augmented with responses from LLMs, and the quality of norms is validated against human judgments to ensure accuracy and reliability.

**Key Contributions:**

	1. Introduction of the NOVA dataset that integrates LLM responses with human norms
	2. Demonstration of superior performance in predicting semantic similarity
	3. Validation of LLM usage in cognitive science research

**Result:** The NOVA dataset exhibits higher feature density and greater overlap among concepts compared to traditional human-only datasets and significantly improves predictions of semantic similarity judgments.

**Limitations:** The study relies on the quality of the LLM responses and human judgments for validation, which may introduce biases or inaccuracies.

**Conclusion:** The findings highlight the richness of human conceptual knowledge beyond previous datasets and the viability of using LLMs as tools for enhancing cognitive science research.

**Abstract:** Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a dataset of human-generated feature norms with responses from large language models (LLMs) while verifying the quality of norms against reliable human judgments. We find that our AI-enhanced feature norm dataset, NOVA: Norms Optimized Via AI, shows much higher feature density and overlap among concepts while outperforming a comparable human-only norm dataset and word-embedding models in predicting people's semantic similarity judgments. Taken together, we demonstrate that human conceptual knowledge is richer than captured in previous norm datasets and show that, with proper validation, LLMs can serve as powerful tools for cognitive science research.

</details>


### [26] [Tracr-Injection: Distilling Algorithms into Pre-trained Language Models](https://arxiv.org/abs/2505.10719)

*Tomás Vergara-Browne, Álvaro Soto*

**Main category:** cs.CL

**Keywords:** large language models, transformers, algorithm distillation

**Relevance Score:** 7

**TL;DR:** This paper introduces tracr-injection, a method for distilling algorithms written in a programming language called RASP into pre-trained language models, enhancing their symbolic capabilities and out-of-distribution performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap between the theoretical symbolic capabilities of transformer architectures and their practical learnability from unsupervised data, particularly in light of the advancements in large language models.

**Method:** The paper proposes tracr-injection, which distills RASP-written algorithms into a pre-trained language model, resulting in an interpretable subspace within the model's residual stream.

**Key Contributions:**

	1. Introduction of tracr-injection method for distilling algorithms into language models
	2. Creation of an interpretable subspace in the model's residual stream
	3. Demonstration of improved out-of-distribution performance.

**Result:** The implementation of tracr-injection demonstrated improvements in out-of-distribution performance and allowed for decoding of variables from RASP algorithms into the model's framework.

**Limitations:** 

**Conclusion:** The findings support that a more symbolic mechanism is functioning in the model's inner workings, bridging the gap between RASP algorithms and practical applications in transformers.

**Abstract:** Motivated by the surge of large language models, there has been a push to formally characterize the symbolic abilities intrinsic to the transformer architecture. A programming language, called RASP, has been proposed, which can be directly compiled into transformer weights to implement these algorithms. However, the tasks that can be implemented in RASP are often uncommon to learn from natural unsupervised data, showing a mismatch between theoretical capabilities of the transformer architecture, and the practical learnability of these capabilities from unsupervised data. We propose tracr-injection, a method that allows us to distill algorithms written in RASP directly into a pre-trained language model. We showcase our method by injecting 3 different algorithms into a language model. We show how our method creates an interpretable subspace within the model's residual stream, which can be decoded into the variables present in the code of the RASP algorithm. Additionally, we found that the proposed method can improve out of distribution performance compared to our baseline, indicating that indeed a more symbolic mechanism is taking place in the inner workings of the model. We release the code used to run our experiments.

</details>


### [27] [Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization](https://arxiv.org/abs/2505.10736)

*Ximing Dong, Shaowei Wang, Dayi Lin, Ahmed E. Hassan*

**Main category:** cs.CL

**Keywords:** prompt optimization, large language models, real-time performance, automated techniques, coreset selection

**Relevance Score:** 8

**TL;DR:** This paper introduces IPOMP, an automated method for optimizing prompts for Large Language Models (LLMs) by using real-time model performance data for more accurate evaluations and sample selection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Manual prompt engineering for LLMs is labor-intensive and ineffective, necessitating better methods for automated prompt optimization.

**Method:** IPOMP employs a two-stage approach involving semantic clustering and boundary analysis to select diverse samples, followed by iterative refinement using real-time model performance data.

**Key Contributions:**

	1. Introduction of IPOMP for prompt optimization in LLMs
	2. Demonstrated improvements in effectiveness and stability of prompts
	3. Application of real-time performance data for sample selection and refinement

**Result:** Evaluations on the BIG-bench dataset show that IPOMP improves prompt effectiveness by 1.6% to 5.3% and stability by at least 57% compared to state-of-the-art baselines, with less than 1% computational overhead.

**Limitations:** 

**Conclusion:** The results indicate that real-time performance-guided refinement can enhance existing coreset selection methods across various datasets.

**Abstract:** Optimizing Large Language Model (LLM) performance requires well-crafted prompts, but manual prompt engineering is labor-intensive and often ineffective. Automated prompt optimization techniques address this challenge but the majority of them rely on randomly selected evaluation subsets, which fail to represent the full dataset, leading to unreliable evaluations and suboptimal prompts. Existing coreset selection methods, designed for LLM benchmarking, are unsuitable for prompt optimization due to challenges in clustering similar samples, high data collection costs, and the unavailability of performance data for new or private datasets. To overcome these issues, we propose IPOMP, an Iterative evaluation data selection for effective Prompt Optimization using real-time Model Performance. IPOMP is a two-stage approach that selects representative and diverse samples using semantic clustering and boundary analysis, followed by iterative refinement with real-time model performance data to replace redundant samples. Evaluations on the BIG-bench dataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by at least 57% compared with SOTA baselines, with minimal computational overhead below 1%. Furthermore, the results demonstrate that our real-time performance-guided refinement approach can be universally applied to enhance existing coreset selection methods.

</details>


### [28] [SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.10740)

*Qiwei Peng, Robert Moro, Michal Gregor, Ivan Srba, Simon Ostermann, Marian Simko, Juraj Podroužek, Matúš Mesarčík, Jaroslav Kopčan, Anders Søgaard*

**Main category:** cs.CL

**Keywords:** multilingual claim retrieval, automated fact-checking, social media

**Relevance Score:** 6

**TL;DR:** This paper reports on a shared task for multilingual claim retrieval, exploring approaches to automated fact-checking in low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of online disinformation, especially in multilingual settings that often neglect low-resource languages.

**Method:** The shared task at SemEval 2025 included a monolingual track (same language for posts and claims) and a crosslingual track (different languages).

**Key Contributions:**

	1. Introduction of a multilingual claim retrieval task
	2. Dataset for multilingual fact-checking
	3. Analysis of effective systems and approaches

**Result:** 179 participants registered, contributing to 52 submissions. The best-performing systems were identified, along with common effective approaches.

**Limitations:** 

**Conclusion:** The task and dataset provide valuable insights for future multilingual claim retrieval and automated fact-checking research.

**Abstract:** The rapid spread of online disinformation presents a global challenge, and machine learning has been widely explored as a potential solution. However, multilingual settings and low-resource languages are often neglected in this field. To address this gap, we conducted a shared task on multilingual claim retrieval at SemEval 2025, aimed at identifying fact-checked claims that match newly encountered claims expressed in social media posts across different languages. The task includes two subtracks: (1) a monolingual track, where social posts and claims are in the same language, and (2) a crosslingual track, where social posts and claims might be in different languages. A total of 179 participants registered for the task contributing to 52 test submissions. 23 out of 31 teams have submitted their system papers. In this paper, we report the best-performing systems as well as the most common and the most effective approaches across both subtracks. This shared task, along with its dataset and participating systems, provides valuable insights into multilingual claim retrieval and automated fact-checking, supporting future research in this field.

</details>


### [29] [Ranked Voting based Self-Consistency of Large Language Models](https://arxiv.org/abs/2505.10772)

*Weiqin Wang, Yile Wang, Hui Huang*

**Main category:** cs.CL

**Keywords:** chain-of-thought reasoning, ranked voting, self-consistency, large language models, reasoning performance

**Relevance Score:** 7

**TL;DR:** This paper introduces a method for enhancing chain-of-thought reasoning through ranked voting among multiple ranked answers, improving self-consistency in reasoning processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of previous chain-of-thought reasoning methods that typically generate a single answer, which ignores other potential answers.

**Method:** The authors propose generating ranked answers in each reasoning process and applying three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting.

**Key Contributions:**

	1. Introducing ranked voting methods for chain-of-thought reasoning.
	2. Validation across multiple datasets including both multiple-choice and open-ended tasks.
	3. Demonstrating improved performance over existing methods.

**Result:** The proposed method outperforms baseline approaches across six datasets, including multiple-choice and open-ended tasks, demonstrating improved reasoning performance.

**Limitations:** 

**Conclusion:** Leveraging ranked answers and ranked voting significantly enhances the reliability of self-consistency in reasoning tasks.

**Abstract:** Majority voting is considered an effective method to enhance chain-of-thought reasoning, as it selects the answer with the highest "self-consistency" among different reasoning paths (Wang et al., 2023). However, previous chain-of-thought reasoning methods typically generate only a single answer in each trial, thereby ignoring the possibility of other potential answers. As a result, these alternative answers are often overlooked in subsequent voting processes. In this work, we propose to generate ranked answers in each reasoning process and conduct ranked voting among multiple ranked answers from different responses, thereby making the overall self-consistency more reliable. Specifically, we use three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting. We validate our methods on six datasets, including three multiple-choice and three open-ended question-answering tasks, using both advanced open-source and closed-source large language models. Extensive experimental results indicate that our proposed method outperforms the baselines, showcasing the potential of leveraging the information of ranked answers and using ranked voting to improve reasoning performance. The code is available at https://github.com/szu-tera/RankedVotingSC.

</details>


### [30] [Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI](https://arxiv.org/abs/2505.10472)

*Agnik Saha, Victoria Churchill, Anny D. Rodriguez, Ugur Kursuncu, Muhammed Y. Idris*

**Main category:** cs.CL

**Keywords:** Large Language Models, cancer communication, health informatics, safety, trustworthiness

**Relevance Score:** 9

**TL;DR:** Evaluation of LLMs for generating cancer-related information reveals strengths in linguistic quality from general-purpose models and in accessibility from medical models, but highlights concerns around safety and bias.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address persistent challenges in public understanding of cancer prevention, screening, and treatment due to ineffective communication.

**Method:** Mixed-methods evaluation framework focusing on linguistic quality, safety, trustworthiness, and communication accessibility across five general-purpose and three medical LLMs, utilizing quantitative and qualitative ratings and statistical analysis.

**Key Contributions:**

	1. Evaluation of general-purpose vs medical LLMs for cancer communication
	2. Identification of duality between domain-specific knowledge and safety
	3. Recommendations for model design improvements in digital health tools.

**Result:** General-purpose LLMs had higher linguistic quality and affectiveness, while medical LLMs showed better communication accessibility but exhibited higher potential harm and bias.

**Limitations:** Focus on breast and cervical cancer may not generalize to other health topics.

**Conclusion:** There's a need for intentional design improvements in LLMs for health communication to ensure safety and effectiveness, particularly for cancer information.

**Abstract:** Effective communication about breast and cervical cancers remains a persistent health challenge, with significant gaps in public understanding of cancer prevention, screening, and treatment, potentially leading to delayed diagnoses and inadequate treatments. This study evaluates the capabilities and limitations of Large Language Models (LLMs) in generating accurate, safe, and accessible cancer-related information to support patient understanding. We evaluated five general-purpose and three medical LLMs using a mixed-methods evaluation framework across linguistic quality, safety and trustworthiness, and communication accessibility and affectiveness. Our approach utilized quantitative metrics, qualitative expert ratings, and statistical analysis using Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that general-purpose LLMs produced outputs of higher linguistic quality and affectiveness, while medical LLMs demonstrate greater communication accessibility. However, medical LLMs tend to exhibit higher levels of potential harm, toxicity, and bias, reducing their performance in safety and trustworthiness. Our findings indicate a duality between domain-specific knowledge and safety in health communications. The results highlight the need for intentional model design with targeted improvements, particularly in mitigating harm and bias, and improving safety and affectiveness. This study provides a comprehensive evaluation of LLMs for cancer communication, offering critical insights for improving AI-generated health content and informing future development of accurate, safe, and accessible digital health tools.

</details>


### [31] [A Systematic Analysis of Base Model Choice for Reward Modeling](https://arxiv.org/abs/2505.10775)

*Kian Ahrabian, Pegah Jandaghi, Negar Mokhberian, Sai Praneeth Karimireddy, Jay Pujara*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Reward Modeling, Large Language Models, Model Selection

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of base model selection on reward modeling performance in reinforcement learning from human feedback, revealing significant potential improvements in model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze how the choice of base model affects the performance of reward models in training large language models via reinforcement learning from human feedback.

**Method:** A systematic analysis comparing various base models and their influence on reward modeling performance, including the evaluation of benchmarks and post-training steps.

**Key Contributions:**

	1. Systematic analysis of base model selection's effect on reward modeling performance.
	2. Demonstration of improved performance through better base model choice and benchmark combination.
	3. Insights on reducing performance prediction error using estimated data distributions.

**Result:** The study found that selecting the right base model can enhance performance by up to 14%, and combining benchmark results can improve model selection metrics by 18% on average.

**Limitations:** 

**Conclusion:** Choosing an appropriate base model and optimizing post-training steps is crucial for improving reward modeling performance in large language models.

**Abstract:** Reinforcement learning from human feedback (RLHF) and, at its core, reward modeling have become a crucial part of training powerful large language models (LLMs). One commonly overlooked factor in training high-quality reward models (RMs) is the effect of the base model, which is becoming more challenging to choose given the rapidly growing pool of LLMs. In this work, we present a systematic analysis of the effect of base model selection on reward modeling performance. Our results show that the performance can be improved by up to 14% compared to the most common (i.e., default) choice. Moreover, we showcase the strong statistical relation between some existing benchmarks and downstream performances. We also demonstrate that the results from a small set of benchmarks could be combined to boost the model selection ($+$18% on average in the top 5-10). Lastly, we illustrate the impact of different post-training steps on the final performance and explore using estimated data distributions to reduce performance prediction error.

</details>


### [32] [Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.10792)

*Zhan Peng Lee, Andre Lin, Calvin Tan*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Factual Accuracy

**Relevance Score:** 9

**TL;DR:** Finetune-RAG is a novel fine-tuning approach that improves factual accuracy in LLMs using a dataset designed to reflect real-world retrieval imperfections.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the factual accuracy of LLM outputs by addressing the challenges posed by imperfect retrieval in Retrieval-Augmented Generation (RAG).

**Method:** A fine-tuning approach called Finetune-RAG, using a newly constructed RAG training dataset that simulates real-world imperfections, was implemented and tested.

**Key Contributions:**

	1. Introduction of Finetune-RAG fine-tuning approach
	2. Development of a RAG training dataset for imperfect retrieval
	3. Creation of Bench-RAG, an evaluation pipeline for LLM performance under real-world scenarios

**Result:** Experimental results indicated a 21.2% improvement in factual accuracy over the base model utilizing Finetune-RAG.

**Limitations:** 

**Conclusion:** Finetune-RAG significantly enhances LLM performance in terms of factual correctness, offering a robust solution to the challenge of misinformation from irrelevant retrievals.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to improve factuality in large language models (LLMs) by grounding their outputs in retrieved documents. However, ensuring perfect retrieval of relevant information remains challenging, and when irrelevant content is passed downstream to an LLM, it can lead to hallucinations. In this work, we propose Finetune-RAG, a simple and effective fine-tuning approach that features the first-of-its-kind RAG training dataset constructed to mimic real-world imperfections. Experimental results show that Finetune-RAG improves factual accuracy by 21.2% over the base model. We also propose a Bench-RAG, an LLM-as-a-judge evaluation pipeline that stress tests models under realistic imperfect retrieval scenarios. Our codebase and dataset are fully open sourced for community use.

</details>


### [33] [Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets](https://arxiv.org/abs/2505.10798)

*Erica Cai, Sean McQuade, Kevin Young, Brendan O'Connor*

**Main category:** cs.CL

**Keywords:** knowledge graphs, data extraction, social science, affiliation graphs, dataset evaluation

**Relevance Score:** 4

**TL;DR:** AffilKG introduces six datasets linking complete book scans with labeled knowledge graphs to evaluate the accuracy of knowledge extraction from text for downstream analyses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of suitable annotated datasets for evaluating the accuracy of automatically extracted knowledge graphs, which often suffer from being disconnected, too small, or overly complex.

**Method:** The study introduces AffilKG, a collection of six datasets that pair complete book scans with labeled knowledge graphs, focusing on affiliation graphs that capture relationships between Person and Organization entities.

**Key Contributions:**

	1. Introduction of AffilKG, a unique dataset collection for knowledge graph evaluation
	2. Provides labeled knowledge graphs paired with complete book scans
	3. Facilitates benchmarking of extraction error impacts and validation of extraction methods.

**Result:** Preliminary experiments reveal significant variability in model performance across the datasets, highlighting AffilKG's potential to benchmark error propagation in extraction and validate KG extraction methods for social science research.

**Limitations:** The datasets may have limitations in representing all possible relationships in real-world scenarios.

**Conclusion:** AffilKG enables critical advances in understanding the impact of extraction errors on graph-level analyses and validating knowledge graph extraction for real-world applications.

**Abstract:** When knowledge graphs (KGs) are automatically extracted from text, are they accurate enough for downstream analysis? Unfortunately, current annotated datasets can not be used to evaluate this question, since their KGs are highly disconnected, too small, or overly complex. To address this gap, we introduce AffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six datasets that are the first to pair complete book scans with large, labeled knowledge graphs. Each dataset features affiliation graphs, which are simple KGs that capture Member relationships between Person and Organization entities -- useful in studies of migration, community interactions, and other social phenomena. In addition, three datasets include expanded KGs with a wider variety of relation types. Our preliminary experiments demonstrate significant variability in model performance across datasets, underscoring AffilKG's ability to enable two critical advances: (1) benchmarking how extraction errors propagate to graph-level analyses (e.g., community structure), and (2) validating KG extraction methods for real-world social science research.

</details>


### [34] [Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances](https://arxiv.org/abs/2505.10829)

*Chen-Chi Chang, Chong-Fu Li, Chu-Hsuan Lee, Hung-Shin Lee*

**Main category:** cs.CL

**Keywords:** Low-resource languages, Large Language Models, Retrieval-Augmented Generation, Translation, Cultural preservation

**Relevance Score:** 9

**TL;DR:** This study explores the use of Large Language Models combined with Retrieval-Augmented Generation to improve translations of low-resource languages, specifically Hakka, showing improved accuracy and fluency over traditional methods.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The need to address the challenges faced in translating low-resource languages and improve the quality of translations.

**Method:** Several model configurations were tested, particularly for Hakka translations, comparing dictionary-only approaches and various LLM and RAG integrations to assess their performance using BLEU scores.

**Key Contributions:**

	1. Introduced a novel combination of LLMs and RAG for low-resource language translation.
	2. Demonstrated improved translation accuracy with specific model configurations.
	3. Highlighted the importance of cultural nuances in translation efforts.

**Result:** The best-performing model (Model 4) with a configuration of retrieval and advanced language modeling achieved a BLEU score of 31%, demonstrating better lexical coverage and grammatical coherence, especially for nuanced terms.

**Limitations:** Static dictionary-based approaches were ineffective for context-sensitive content, indicating challenges in solely relying on predefined resources.

**Conclusion:** The study concludes that combining curated resources, domain knowledge, and ethical collaboration enhances translation accuracy and supports cultural preservation.

**Abstract:** This study investigates the challenges of translating low-resource languages by integrating Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG). Various model configurations were tested on Hakka translations, with BLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0). The best-performing model (Model 4) combined retrieval and advanced language modeling, improving lexical coverage, particularly for specialized or culturally nuanced terms, and enhancing grammatical coherence. A two-stage method (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU score of 26%, highlighting iterative correction's value and the challenges of domain-specific expressions. Static dictionary-based approaches struggled with context-sensitive content, demonstrating the limitations of relying solely on predefined resources. These results emphasize the need for curated resources, domain knowledge, and ethical collaboration with local communities, offering a framework that improves translation accuracy and fluency while supporting cultural preservation.

</details>


### [35] [AI-enhanced semantic feature norms for 786 concepts](https://arxiv.org/abs/2505.10718)

*Siddharth Suresh, Kushin Mukherjee, Tyler Giallanza, Xizheng Yu, Mia Patil, Jonathan D. Cohen, Timothy T. Rogers*

**Main category:** cs.CL

**Keywords:** semantic feature norms, large language models, cognitive science, conceptual knowledge, AI-enhanced datasets

**Relevance Score:** 7

**TL;DR:** This paper presents NOVA, a dataset of semantic feature norms enhanced by large language models (LLMs), which improves feature density and semantic similarity predictions compared to traditional datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional methods for creating semantic feature norms face limitations in concept coverage and quality verification due to their labor-intensive nature.

**Method:** The paper combines human-generated feature norms with responses from LLMs and validates the quality of the norms against reliable human judgments.

**Key Contributions:**

	1. Introduction of NOVA, an AI-enhanced semantic feature norm dataset
	2. Demonstration of improved feature density and semantic similarity predictions
	3. Validation of LLMs as tools for cognitive science research

**Result:** The NOVA dataset demonstrates significantly higher feature density and concept overlap, surpassing a comparable human-only norm dataset and enhancing predictions of semantic similarity judgments.

**Limitations:** Further studies needed to explore the limits of LLM contributions and the contexts in which they are most effective.

**Conclusion:** With proper validation, LLMs can effectively enhance cognitive science research by providing richer conceptual knowledge through AI-assisted norm datasets.

**Abstract:** Semantic feature norms have been foundational in the study of human conceptual knowledge, yet traditional methods face trade-offs between concept/feature coverage and verifiability of quality due to the labor-intensive nature of norming studies. Here, we introduce a novel approach that augments a dataset of human-generated feature norms with responses from large language models (LLMs) while verifying the quality of norms against reliable human judgments. We find that our AI-enhanced feature norm dataset, NOVA: Norms Optimized Via AI, shows much higher feature density and overlap among concepts while outperforming a comparable human-only norm dataset and word-embedding models in predicting people's semantic similarity judgments. Taken together, we demonstrate that human conceptual knowledge is richer than captured in previous norm datasets and show that, with proper validation, LLMs can serve as powerful tools for cognitive science research.

</details>


### [36] [Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL](https://arxiv.org/abs/2505.10832)

*Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, Dongbin Zhao*

**Main category:** cs.CL

**Keywords:** Large reasoning models, Reinforcement learning, Adaptive reasoning

**Relevance Score:** 8

**TL;DR:** AutoThink improves reasoning efficiency in large reasoning models by dynamically deciding when to use explicit reasoning based on task complexity, enhancing performance while reducing computational overhead.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational overhead and latency introduced by explicit reasoning in large reasoning models (LRMs) for simple problems.

**Method:** A multi-stage reinforcement learning framework called AutoThink is developed to optimize reasoning policies, using prompt modifications to trigger thinking modes based on problem complexity.

**Key Contributions:**

	1. Introduction of AutoThink framework for adaptive reasoning in LRMs
	2. Demonstration of enhanced accuracy-efficiency trade-offs
	3. Integration capabilities with existing R1-style models

**Result:** AutoThink achieves a 6.4% improvement in accuracy and reduces token usage by 52% on certain benchmarks, outperforming other prompting and RL-based pruning methods.

**Limitations:** 

**Conclusion:** AutoThink establishes a scalable and adaptive reasoning paradigm for LRMs, making it easier to integrate with existing models for improved efficiency.

**Abstract:** Large reasoning models (LRMs) are proficient at generating explicit, step-by-step reasoning sequences before producing final answers. However, such detailed reasoning can introduce substantial computational overhead and latency, particularly for simple problems. To address this over-thinking problem, we explore how to equip LRMs with adaptive thinking capabilities: enabling them to dynamically decide whether or not to engage in explicit reasoning based on problem complexity. Building on R1-style distilled models, we observe that inserting a simple ellipsis ("...") into the prompt can stochastically trigger either a thinking or no-thinking mode, revealing a latent controllability in the reasoning behavior. Leveraging this property, we propose AutoThink, a multi-stage reinforcement learning (RL) framework that progressively optimizes reasoning policies via stage-wise reward shaping. AutoThink learns to invoke explicit reasoning only when necessary, while defaulting to succinct responses for simpler tasks. Experiments on five mainstream mathematical benchmarks demonstrate that AutoThink achieves favorable accuracy-efficiency trade-offs compared to recent prompting and RL-based pruning methods. It can be seamlessly integrated into any R1-style model, including both distilled and further fine-tuned variants. Notably, AutoThink improves relative accuracy by 6.4 percent while reducing token usage by 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and adaptive reasoning paradigm for LRMs.

</details>


### [37] [Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs](https://arxiv.org/abs/2505.10836)

*Abhishek Dey, Aabha Bothera, Samhita Sarikonda, Rishav Aryan, Sanjay Kumar Podishetty, Akshay Havalgi, Gaurav Singh, Saurabh Srivastava*

**Main category:** cs.CL

**Keywords:** event detection, social media, multimodal, generative models, precision

**Relevance Score:** 6

**TL;DR:** This paper examines the challenges of detecting events on social media using various unimodal and multimodal approaches, highlighting the advantages and shortcomings of generative models compared to supervised methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of traditional unimodal systems in detecting events on social media due to the rapid and multimodal nature of data.

**Method:** The authors use several models, including ModernBERT, ConvNeXt-V2, and advanced generative models like GPT-4o and LLaVA, assessing their performance in event detection on social media.

**Key Contributions:**

	1. Evaluation of multimodal event detection in social media
	2. Comparison between generative and supervised models
	3. Insight into social media data characteristics affecting model performance

**Result:** The study finds that multimodal approaches surpass unimodal ones in performance, but generative models like GPT-4o lag behind supervised methods in precision and event class generation.

**Limitations:** Generative models have limitations in precision and event class generation compared to supervised methods.

**Conclusion:** Multimodal methods are more effective than unimodal ones, while generative models struggle with precision and correct event classification compared to instruction-tuned models; however, they handle certain social media vernacular successfully.

**Abstract:** In this paper, we study the challenges of detecting events on social media, where traditional unimodal systems struggle due to the rapid and multimodal nature of data dissemination. We employ a range of models, including unimodal ModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced generative models like GPT-4o, and LLaVA. Additionally, we also study the effect of providing multimodal generative models (such as GPT-4o) with a single modality to assess their efficacy. Our results indicate that while multimodal approaches notably outperform unimodal counterparts, generative approaches despite having a large number of parameters, lag behind supervised methods in precision. Furthermore, we also found that they lag behind instruction-tuned models because of their inability to generate event classes correctly. During our error analysis, we discovered that common social media issues such as leet speak, text elongation, etc. are effectively handled by generative approaches but are hard to tackle using supervised approaches.

</details>


### [38] [Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?](https://arxiv.org/abs/2505.10862)

*Tairan Fu, Miguel González, Javier Conde, Elena Merino-Gómez, Pedro Reviriego*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, GPT-4.1, analog clocks, fine-tuning, machine learning

**Relevance Score:** 6

**TL;DR:** This paper investigates the ability of Multimodal Large Language Models (MLLMs), specifically GPT-4.1, to read and interpret analog clocks, highlighting their limitations and the impact of fine-tuning on performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the challenges that MLLMs face when interpreting time from analog clocks, which may arise from inadequate training data.

**Method:** The authors test GPT-4.1 on various analog clocks to assess its ability to read time and evaluate the effectiveness of fine-tuning the model.

**Key Contributions:**

	1. Investigation of MLLMs' performance on analog clock reading
	2. Assessment of the impact of fine-tuning on model accuracy
	3. Analysis of training set limitations affecting model generalization

**Result:** The findings indicate that while MLLMs are improving in clock reading, their ability to accurately tell time may still be reliant on training patterns rather than genuine understanding.

**Limitations:** The study primarily focuses on one model (GPT-4.1) and may not generalize across all MLLMs or other types of tasks.

**Conclusion:** The work reveals that the progress made by MLLMs in reading analog clocks is limited and often superficial, raising questions about their true learning capabilities.

**Abstract:** Multimodal Large Language Models which can answer complex questions on an image struggle to tell the time on analog clocks. This is probably due to the lack of images with clocks at different times in their training set. In this work we explore this issue with one of the latest MLLMs: GPT-4.1 to understand why MLLMs fail to tell the time and whether fine-tuning can solve the problem. The results show how models are making progress in reading the time on analog clocks. But have they really learned to do it, or have they only learned patterns in their training datasets? In this work we put the models to the test with different clocks to illustrate the limitations of MLLMs to abstract and generalize.

</details>


### [39] [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](https://arxiv.org/abs/2505.10870)

*Ziyang Huang, Wangtao Sun, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** Rule Retrieval, Large Language Models, Query Augmentation, Reasoning Performance, Artificial Intelligence

**Relevance Score:** 7

**TL;DR:** This paper introduces Self-Induction Augmented Retrieval (SIAR) to enhance rule retrieval accuracy by leveraging Large Language Models (LLMs) for inducing inferential rules and improving query effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of low accuracy in rule retrieval methods due to semantic gaps between queries and abstract rule representations, affecting reasoning performance.

**Method:** The proposed SIAR approach uses LLMs to induce relevant inferential rules for query augmentation, while Rule Relevance ReEstimate (R$^3$) reassesses the relevance of retrieved rules based on their instantiation to align with query facts.

**Key Contributions:**

	1. Introduction of Self-Induction Augmented Retrieval (SIAR) for enhanced rule retrieval.
	2. Development of Rule Relevance ReEstimate (R$^3$) to reassess rule relevance to queries.
	3. Extensive experimental validation across multiple settings showcasing improved reasoning performance.

**Result:** Experiments show that SIAR and R$^3$ significantly improve the retrieval effectiveness and reasoning performance across various settings.

**Limitations:** 

**Conclusion:** The methodologies proposed provide a systematic approach to improve rule retrieval, demonstrating their effectiveness and versatility in practical applications.

**Abstract:** This paper systematically addresses the challenges of rule retrieval, a crucial yet underexplored area. Vanilla retrieval methods using sparse or dense retrievers to directly search for relevant rules to support downstream reasoning, often suffer from low accuracy. This is primarily due to a significant semantic gap between the instantiated facts in the queries and the abstract representations of the rules. Such misalignment results in suboptimal retrieval quality, which in turn negatively impacts reasoning performance. To overcome these challenges, we propose Self-Induction Augmented Retrieval (SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce potential inferential rules that might offer benefits for reasoning by abstracting the underlying knowledge and logical structure in queries. These induced rules are then used for query augmentation to improve retrieval effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a method that re-estimates the relevance of retrieved rules by assessing whether the abstract knowledge they contain can be instantiated to align with the facts in the queries and the helpfulness for reasoning. Extensive experiments across various settings demonstrate the effectiveness and versatility of our proposed methods.

</details>


### [40] [A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?](https://arxiv.org/abs/2505.10924)

*Ada Chen, Yongjiang Wu, Junyuan Zhang, Shu Yang, Jen-tse Huang, Kun Wang, Wenxuan Wang, Shuai Wang*

**Main category:** cs.CL

**Keywords:** Computer-Using Agents, safety and security, large language models, human-computer interaction, defensive strategies

**Relevance Score:** 8

**TL;DR:** The paper presents a systematic overview of safety and security threats associated with Computer-Using Agents (CUAs), along with a comprehensive literature review and guidance for future research and practical applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLM-based systems emulating human-like operations in GUIs, the safety and security risks presented by Computer-Using Agents (CUAs) have become critical to understand.

**Method:** The authors conducted a comprehensive literature review to define CUAs, categorize safety threats, propose a taxonomy of defensive strategies, and summarize benchmarks and metrics for assessing safety and performance.

**Key Contributions:**

	1. Systematization of safety and security threats of CUAs
	2. Proposed taxonomy of defensive strategies
	3. Overview of benchmarks and datasets for evaluating CUAs

**Result:** The study categorizes current safety threats among CUAs, proposes a structured taxonomy of existing defensive strategies, and summarizes various benchmarks and datasets that assess CUA performance.

**Limitations:** 

**Conclusion:** This work lays a structured foundation for future research into vulnerabilities in CUAs and provides actionable guidance for practitioners in designing secure systems.

**Abstract:** Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \textit{\textbf{(i)}} define the CUA that suits safety analysis; \textit{\textbf{(ii)} } categorize current safety threats among CUAs; \textit{\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \textit{\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.

</details>


### [41] [Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents](https://arxiv.org/abs/2505.10936)

*Jiaxing Zhao, Hongbin Xie, Yuzhen Lei, Xuan Song, Zhuoran Shi, Lianxin Li, Shuangxue Liu, Haoran Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Collaboration, Knowledge Graph, Business Workflows, Multi-Agent Systems

**Relevance Score:** 8

**TL;DR:** Cochain is a collaboration prompting framework that enhances business workflows by integrating knowledge and prompts, addressing limitations of single-agent and multi-agent LLM systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the collaboration challenges in single-agent systems and the token consumption issues in multi-agent systems for business workflow tasks.

**Method:** Cochain combines knowledge from multiple stages using an integrated knowledge graph and retrieves relevant prompts through a maintained prompts tree.

**Key Contributions:**

	1. Introduction of Cochain framework for business workflow collaboration
	2. Development of an integrated knowledge graph for enhanced prompt retrieval
	3. Demonstration of superior performance over existing models including GPT-4 with reduced costs.

**Result:** Cochain outperforms all baseline approaches in prompt engineering and multi-agent LLMs across multiple datasets, and it shows that using a small model with Cochain surpasses GPT-4 performance.

**Limitations:** 

**Conclusion:** Cochain provides a cost-effective solution to improve collaboration in business workflows, maintaining efficiency by leveraging an integrated approach.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive performance in executing complex reasoning tasks. Chain-of-thought effectively enhances reasoning capabilities by unlocking the potential of large models, while multi-agent systems provide more comprehensive solutions by integrating collective intelligence of multiple agents. However, both approaches face significant limitations. Single-agent with chain-of-thought, due to the inherent complexity of designing cross-domain prompts, faces collaboration challenges. Meanwhile, multi-agent systems consume substantial tokens and inevitably dilute the primary problem, which is particularly problematic in business workflow tasks. To address these challenges, we propose Cochain, a collaboration prompting framework that effectively solves business workflow collaboration problem by combining knowledge and prompts at a reduced cost. Specifically, we construct an integrated knowledge graph that incorporates knowledge from multiple stages. Furthermore, by maintaining and retrieving a prompts tree, we can obtain prompt information relevant to other stages of the business workflow. We perform extensive evaluations of Cochain across multiple datasets, demonstrating that Cochain outperforms all baselines in both prompt engineering and multi-agent LLMs. Additionally, expert evaluation results indicate that the use of a small model in combination with Cochain outperforms GPT-4.

</details>


### [42] [Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations](https://arxiv.org/abs/2505.10937)

*Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang, Xiangzhong Fang*

**Main category:** cs.CL

**Keywords:** large reasoning models, chain-of-thought, cognitive difficulty, reasoning verbosity, natural language processing

**Relevance Score:** 9

**TL;DR:** OmniThought is a dataset of 2 million chain-of-thought (CoT) processes aimed at enhancing the training of large reasoning models (LRMs) for complex tasks like problem-solving and code generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The creation of comprehensive datasets for chain-of-thought reasoning processes is essential for improving the capabilities of large reasoning models in handling complex tasks.

**Method:** The dataset is curated using two powerful LRMs as teacher models, with annotations for Reasoning Verbosity and Cognitive Difficulty. Extensive experiments were conducted to validate the dataset's impact on training LRMs using Qwen2.5 models.

**Key Contributions:**

	1. Introduction of the OmniThought dataset with 2 million CoT processes
	2. Development of annotation scores for Reasoning Verbosity and Cognitive Difficulty
	3. Enhanced training efficiency for LRMs in complex task scenarios

**Result:** The implementation of the OmniThought dataset positively influences the training effectiveness of LRMs, yielding models with enhanced reasoning capabilities.

**Limitations:** 

**Conclusion:** The introduction of OmniThought and its unique annotations significantly contribute to advancing LRM development for complex reasoning tasks.

**Abstract:** The emergence of large reasoning models (LRMs) has transformed Natural Language Processing by excelling in complex tasks such as mathematical problem-solving and code generation. These models leverage chain-of-thought (CoT) processes, enabling them to emulate human-like reasoning strategies. However, the advancement of LRMs is hindered by the lack of comprehensive CoT datasets. Current resources often fail to provide extensive reasoning problems with coherent CoT processes distilled from multiple teacher models and do not account for multifaceted properties describing the internal characteristics of CoTs. To address these challenges, we introduce OmniThought, a large-scale dataset featuring 2 million CoT processes generated and validated by two powerful LRMs as teacher models. Each CoT process in OmniThought is annotated with novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which describe the appropriateness of CoT verbosity and cognitive difficulty level for models to comprehend these reasoning processes. We further establish a self-reliant pipeline to curate this dataset. Extensive experiments using Qwen2.5 models of various sizes demonstrate the positive impact of our proposed scores on LRM training effectiveness. Based on the proposed OmniThought dataset, we further train and release a series of high-performing LRMs, specifically equipped with stronger reasoning abilities and optimal CoT output length and difficulty level. Our contributions significantly enhance the development and training of LRMs for solving complex tasks.

</details>


### [43] [Accurate KV Cache Quantization with Outlier Tokens Tracing](https://arxiv.org/abs/2505.10938)

*Yi Su, Yuechi Zhou, Quantong Qiu, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, KV Cache, quantization, computational resources, HCI

**Relevance Score:** 9

**TL;DR:** This paper presents a method to improve quantization accuracy in large language models (LLMs) by identifying and excluding unusual tokens from quantization, enhancing memory efficiency and throughput.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models (LLMs) require substantial computational resources, and while KV Cache can reduce recomputation, it increases memory overhead. Effective quantization can help balance these demands.

**Method:** The authors propose a method to identify outlier tokens during the decoding process that do not conform to the typical distribution of Keys and Values. These tokens are excluded from quantization.

**Key Contributions:**

	1. Introduction of a method to identify outlier tokens during quantization
	2. Demonstration of significant accuracy improvements with a 2-bit quantization
	3. Achieved a balance of reduced memory usage and increased throughput in LLM deployment

**Result:** The proposed method results in significant improvements in accuracy under 2-bit quantization, achieving a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.

**Limitations:** 

**Conclusion:** Excluding outlier tokens from quantization provides a promising enhancement to the deployment efficiency of LLMs without compromising accuracy.

**Abstract:** The impressive capabilities of Large Language Models (LLMs) come at the cost of substantial computational resources during deployment. While KV Cache can significantly reduce recomputation during inference, it also introduces additional memory overhead. KV Cache quantization presents a promising solution, striking a good balance between memory usage and accuracy. Previous research has shown that the Keys are distributed by channel, while the Values are distributed by token. Consequently, the common practice is to apply channel-wise quantization to the Keys and token-wise quantization to the Values. However, our further investigation reveals that a small subset of unusual tokens exhibit unique characteristics that deviate from this pattern, which can substantially impact quantization accuracy. To address this, we develop a simple yet effective method to identify these tokens accurately during the decoding process and exclude them from quantization as outlier tokens, significantly improving overall accuracy. Extensive experiments show that our method achieves significant accuracy improvements under 2-bit quantization and can deliver a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.

</details>


### [44] [GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction](https://arxiv.org/abs/2505.10939)

*Mohammadtaha Bagherifard, Sahar Rajabi, Ali Edalat, Yadollah Yaghoobzadeh*

**Main category:** cs.CL

**Keywords:** large language models, zero-shot generalization, LoRA modules

**Relevance Score:** 8

**TL;DR:** This paper presents a modular framework, GenKnowSub, that improves zero-shot generalization of large language models by disentangling general knowledge from task-specific adaptations via a library of LoRA modules.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of zero-shot generalization in large language models due to the entanglement of general knowledge and task-specific adaptations.

**Method:** A modular framework is proposed that uses task-specific LoRA modules alongside a general-domain LoRA. General knowledge is subtracted to create residual modules focused on task-relevant information, and Arrow routing dynamically combines modules for new inputs.

**Key Contributions:**

	1. Introduction of general knowledge subtraction (GenKnowSub) for LLMs
	2. Development of a library of task-specific LoRA modules
	3. Demonstrated improved performance across multiple languages and benchmarks

**Result:** The proposed approach shows consistent performance improvements in both monolingual and cross-lingual settings on various benchmarks compared to standard baselines.

**Limitations:** 

**Conclusion:** GenKnowSub demonstrates better generalization in weaker LLMs, indicating its effectiveness in enhancing task-specific performance while maintaining general knowledge.

**Abstract:** Large language models often struggle with zero-shot generalization, and several modular approaches have been proposed to address this challenge. Yet, we hypothesize that a key limitation remains: the entanglement of general knowledge and task-specific adaptations. To overcome this, we propose a modular framework that disentangles these components by constructing a library of task-specific LoRA modules alongside a general-domain LoRA. By subtracting this general knowledge component from each task-specific module, we obtain residual modules that focus more exclusively on task-relevant information, a method we call general knowledge subtraction (GenKnowSub). Leveraging the refined task-specific modules and the Arrow routing algorithm \citep{ostapenko2024towards}, we dynamically select and combine modules for new inputs without additional training. Our studies on the Phi-3 model and standard Arrow as baselines reveal that using general knowledge LoRAs derived from diverse languages, including English, French, and German, yields consistent performance gains in both monolingual and cross-lingual settings across a wide set of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub generalizes to weaker LLMs. The complete code and data are available at https://github.com/saharsamr/Modular-LLM.

</details>


### [45] [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](https://arxiv.org/abs/2505.10945)

*Seungyoon Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Semantic Aware Linear Transfer, cross-lingual transfer, Pre-trained Language Models, multilingual capabilities

**Relevance Score:** 7

**TL;DR:** The paper introduces Semantic Aware Linear Transfer (SALT), a method for effectively transferring multilingual capabilities from Pre-trained Language Models to Large Language Models, improving performance in cross-lingual understanding tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As Large Language Models increasingly incorporate multilingual capabilities, there is a need for effective methods to transfer these capabilities into target language-specific models without losing expressiveness.

**Method:** SALT recycles embeddings from target language Pre-trained Language Models and constructs unique regression lines based on vocabulary overlap to handle non-overlapping tokens in embedding space.

**Key Contributions:**

	1. Introduction of SALT for cross-lingual transfer of embeddings
	2. Unique regression line construction based on vocabulary overlap
	3. Demonstrated significant improvements in performance and adaptability over existing methods

**Result:** SALT significantly outperforms existing transfer methods, achieving lower loss and faster convergence during language adaptation, especially in cross-lingual understanding setups.

**Limitations:** 

**Conclusion:** The proposed SALT technique enhances the functionality of contemporary LLMs and demonstrates impressive performance, suggesting a strong path forward for multilingual model development.

**Abstract:** Large Language Models (LLMs) increasingly incorporate multilingual capabilities, fueling the demand to transfer them into target language-specific models. However, most approaches, which blend the source model's embedding by replacing the source vocabulary with the target language-specific vocabulary, may constrain expressive capacity in the target language since the source model is predominantly trained on English data. In this paper, we propose Semantic Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that recycles embeddings from target language Pre-trained Language Models (PLMs) to transmit the deep representational strengths of PLM-derived embedding to LLMs. SALT derives unique regression lines based on the similarity in the overlap of the source and target vocabularies, to handle each non-overlapping token's embedding space. Our extensive experiments show that SALT significantly outperforms other transfer methods and achieves lower loss with accelerating faster convergence during language adaptation. Notably, SALT obtains remarkable performance in cross-lingual understanding setups compared to other methods. Furthermore, we highlight the scalable use of PLMs to enhance the functionality of contemporary LLMs by conducting experiments with varying architectures.

</details>


### [46] [The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs](https://arxiv.org/abs/2505.10948)

*Makoto Sato*

**Main category:** cs.CL

**Keywords:** large language models, Conceptual Blending Theory, prompt engineering, cognition, human-AI collaboration

**Relevance Score:** 8

**TL;DR:** This paper explores the operationalization of Conceptual Blending Theory in the context of large language models, revealing insights into their cognitive-like behaviors through experimental frameworks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms of personality and intelligence-like behaviors in large language models.

**Method:** Utilizing an experimental framework based on Conceptual Blending Theory, the authors employ prompt-based methods to investigate how LLMs blend and compress meaning, focusing on Prompt-Induced Transitions and Hallucinations.

**Key Contributions:**

	1. Operationalizes Conceptual Blending Theory in LLM research
	2. Investigates Prompt-Induced transitions and hallucinations
	3. Proposes prompt engineering as a scientific approach to understanding cognition

**Result:** The study uncovers structural parallels and divergences between artificial and biological cognition, showing how human-AI collaboration reflects principles of cognitive science.

**Limitations:** 

**Conclusion:** The research promotes prompt engineering as a vital method for exploring the layers of meaning in large language models.

**Abstract:** Large language models (LLMs), inspired by neuroscience, exhibit behaviors that often evoke a sense of personality and intelligence-yet the mechanisms behind these effects remain elusive. Here, we operationalize Conceptual Blending Theory (CBT) as an experimental framework, using prompt-based methods to reveal how LLMs blend and compress meaning. By systematically investigating Prompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we uncover structural parallels and divergences between artificial and biological cognition. Our approach bridges linguistics, neuroscience, and empirical AI research, demonstrating that human-AI collaboration can serve as a living prototype for the future of cognitive science. This work proposes prompt engineering not just as a technical tool, but as a scientific method for probing the deep structure of meaning itself.

</details>


### [47] [Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio](https://arxiv.org/abs/2505.10975)

*Xinlu He, Jacob Whitehill*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, multi-speaker, end-to-end architecture, neural networks, benchmark evaluation

**Relevance Score:** 6

**TL;DR:** A comprehensive survey on end-to-end neural approaches for multi-speaker automatic speech recognition (ASR), highlighting recent developments, architectural paradigms, and future directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addresses challenges in monaural multi-speaker ASR due to data scarcity and overlapping speech recognition difficulties.

**Method:** Systematic taxonomy and comparative analysis of recent end-to-end neural architectures for multi-speaker ASR, focusing on different paradigms and algorithmic improvements.

**Key Contributions:**

	1. Comprehensive review of end-to-end neural ASR approaches for multi-speaker scenarios.
	2. Analysis of architectural paradigms (SIMO vs SISO) and their trade-offs.
	3. Evaluation against standard benchmarks to compare method effectiveness.

**Result:** Provides a detailed evaluation of approaches and performance benchmarks for multi-speaker ASR, highlighting the trade-offs between different architectural models.

**Limitations:** 

**Conclusion:** Identifies open challenges and outlines future research directions for enhancing multi-speaker ASR systems.

**Abstract:** Monaural multi-speaker automatic speech recognition (ASR) remains challenging due to data scarcity and the intrinsic difficulty of recognizing and attributing words to individual speakers, particularly in overlapping speech. Recent advances have driven the shift from cascade systems to end-to-end (E2E) architectures, which reduce error propagation and better exploit the synergy between speech content and speaker identity. Despite rapid progress in E2E multi-speaker ASR, the field lacks a comprehensive review of recent developments. This survey provides a systematic taxonomy of E2E neural approaches for multi-speaker ASR, highlighting recent advances and comparative analysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO) for pre-segmented audio, analyzing their distinct characteristics and trade-offs; (2) recent architectural and algorithmic improvements based on these two paradigms; (3) extensions to long-form speech, including segmentation strategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate and compare methods across standard benchmarks. We conclude with a discussion of open challenges and future research directions towards building robust and scalable multi-speaker ASR.

</details>


### [48] [Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning](https://arxiv.org/abs/2505.11004)

*Jingcheng Niu, Subhabrata Dutta, Ahmed Elshabrawy, Harish Tayyar Madabushi, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** in-context learning, Transformer language models, mechanistic interpretability, model capabilities, AI security

**Relevance Score:** 8

**TL;DR:** This paper systematically investigates in-context learning (ICL) in large-scale Transformer language models, challenging the notion that it is simply data memorization and providing deeper insights into its mechanisms and implications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding in-context learning (ICL) is crucial for advancing language models and informing model developers and AI security practitioners on potential improvements and guidelines.

**Method:** The study employs a novel method and a suite of tasks to investigate ICL using the Pythia scaling suite and interim checkpoints with varying amounts of training data.

**Key Contributions:**

	1. Introduces a systematic investigation of ICL using the Pythia scaling suite.
	2. Demonstrates that ICL involves more than memorization, impacting training dynamics and model capabilities.
	3. Provides insights for model developers and guidelines for AI security practitioners.

**Result:** The findings reveal that ICL goes beyond mere memorization, indicating significant implications for training dynamics and model capabilities, alongside insights into mechanistic interpretability.

**Limitations:** 

**Conclusion:** This work advances the understanding of ICL, highlighting its complexity and offering practical insights for model developers and AI security practitioners.

**Abstract:** Large-scale Transformer language models (LMs) trained solely on next-token prediction with web-scale data can solve a wide range of tasks after seeing just a few examples. The mechanism behind this capability, known as in-context learning (ICL), remains both controversial and poorly understood. Some studies argue that it is merely the result of memorizing vast amounts of data, while others contend that it reflects a fundamental, symbolic algorithmic development in LMs. In this work, we introduce a suite of investigative tasks and a novel method to systematically investigate ICL by leveraging the full Pythia scaling suite, including interim checkpoints that capture progressively larger amount of training data. By carefully exploring ICL performance on downstream tasks and simultaneously conducting a mechanistic analysis of the residual stream's subspace, we demonstrate that ICL extends beyond mere "memorization" of the training corpus, yet does not amount to the implementation of an independent symbolic algorithm. Our results also clarify several aspects of ICL, including the influence of training dynamics, model capabilities, and elements of mechanistic interpretability. Overall, our work advances the understanding of ICL and its implications, offering model developers insights into potential improvements and providing AI security practitioners with a basis for more informed guidelines.

</details>


### [49] [Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs](https://arxiv.org/abs/2505.11008)

*Ye Kyaw Thu, Thazin Myint Oo*

**Main category:** cs.CL

**Keywords:** Syllable prediction, Abugida languages, Transformer models, Data augmentation, Natural language processing

**Relevance Score:** 4

**TL;DR:** Study on syllable sequence prediction in Abugida languages using Transformer models, highlighting the importance of consonant sequences in achieving high accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the understanding of syllable sequence prediction in Abugida languages, focusing on practical applications like text prediction and spelling correction.

**Method:** Transformer-based models were used to reconstruct complete syllable sequences from various input types, including consonant sequences, vowel sequences, and masked syllables.

**Key Contributions:**

	1. Investigation of syllable sequence prediction in Abugida languages
	2. High BLEU scores for consonant sequence predictions
	3. Insights for practical applications like text prediction and spelling correction

**Result:** The model achieved high BLEU scores, particularly excelling in tasks involving consonant sequences while struggling with vowel sequences.

**Limitations:** Vowel sequences present a greater challenge than consonant sequences in prediction tasks.

**Conclusion:** This paper provides insights into the robust performance of models in reconstructing syllable sequences, offering applications in text prediction and data augmentation.

**Abstract:** This paper explores syllable sequence prediction in Abugida languages using Transformer-based models, focusing on six languages: Bengali, Hindi, Khmer, Lao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We investigate the reconstruction of complete syllable sequences from various incomplete input types, including consonant sequences, vowel sequences, partial syllables (with random character deletions), and masked syllables (with fixed syllable deletions). Our experiments reveal that consonant sequences play a critical role in accurate syllable prediction, achieving high BLEU scores, while vowel sequences present a significantly greater challenge. The model demonstrates robust performance across tasks, particularly in handling partial and masked syllable reconstruction, with strong results for tasks involving consonant information and syllable masking. This study advances the understanding of sequence prediction for Abugida languages and provides practical insights for applications such as text prediction, spelling correction, and data augmentation in these scripts.

</details>


### [50] [Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models](https://arxiv.org/abs/2505.11010)

*Jiangxu Wu, Cong Wang, TianHuang Su, Jun Yang, Haozhi Lin, Chao Zhang, Ming Peng, Kai Shi, SongPan Yang, BinQing Pan, ZiXian Li, Ni Yang, ZhenYu Yang*

**Main category:** cs.CL

**Keywords:** large language models, multi-turn dialogue, framework, review-driven synthesis, conversational AI

**Relevance Score:** 9

**TL;DR:** This paper presents Review-Instruct, a novel framework for generating multi-turn conversation data that enhances quality and diversity through an iterative review process.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) face challenges in maintaining contextual coherence in multi-turn dialogues due to reliance on single-turn supervised fine-tuning (SFT) data.

**Method:** The Review-Instruct framework synthesizes multi-turn conversations using an iterative 'Ask-Respond-Review' process with distinct agent roles: Candidate, multiple Reviewers, and a Chairman, aiming to refine dialogue instructions through feedback.

**Key Contributions:**

	1. Introduction of the Review-Instruct framework for multi-turn dialogue synthesis
	2. Demonstrated improvements in conversational data quality
	3. Highlighting the benefit of multiple Reviewer roles in instruction refinement

**Result:** Significant improvements were observed, with absolute gains of 2.9% on MMLU-Pro and 2% on MT-Bench compared to prior models using LLaMA2-13B.

**Limitations:** 

**Conclusion:** The study demonstrates the efficacy of review-driven, multi-agent systems in producing high-quality conversational datasets.

**Abstract:** The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative "Ask-Respond-Review" process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9\% on MMLU-Pro and 2\% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.

</details>


### [51] [StRuCom: A Novel Dataset of Structured Code Comments in Russian](https://arxiv.org/abs/2505.11026)

*Maria Dziuba, Valentin Malykh*

**Main category:** cs.CL

**Keywords:** code comments, natural language processing, machine learning, dataset, Russian language

**Relevance Score:** 7

**TL;DR:** StRuCom is a dataset aimed at improving machine learning models for generating structured code comments in Russian, achieving notable performance improvements in code documentation tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing models for code comment generation struggle with Russian due to poor performance compared to English, necessitating a specialized dataset.

**Method:** Creation of the StRuCom dataset consisting of 153K examples from Russian GitHub repositories, combined with synthetic data to conform to code documentation standards for multiple programming languages. Fine-tuned Qwen2.5-Coder models were evaluated on this dataset.

**Key Contributions:**

	1. First large-scale Russian code documentation dataset (StRuCom)
	2. Combination of human-written and synthetic data for model training
	3. Demonstrated significant performance improvements in NLP metrics for code comment generation

**Result:** Fine-tuning Qwen2.5-Coder models on StRuCom resulted in statistically significant improvements in code comment generation metrics (chrf++ and BERTScore) compared to baseline models.

**Limitations:** 

**Conclusion:** StRuCom enhances the generation of structured code comments in Russian, paving the way for better comprehension and maintenance of codebases in non-English languages.

**Abstract:** Structured code comments in docstring format are essential for code comprehension and maintenance, but existing machine learning models for their generation perform poorly for Russian compared to English. To bridge this gap, we present StRuCom - the first large-scale dataset (153K examples) specifically designed for Russian code documentation. Unlike machine-translated English datasets that distort terminology (e.g., technical loanwords vs. literal translations) and docstring structures, StRuCom combines human-written comments from Russian GitHub repositories with synthetically generated ones, ensuring compliance with Python, Java, JavaScript, C#, and Go standards through automated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom shows statistically significant improvements of chrf++ and BERTScore over baseline models.

</details>


### [52] [OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning](https://arxiv.org/abs/2505.11031)

*Xiao Zhang, Huiyuan Lai, Qianru Meng, Johan Bos*

**Main category:** cs.CL

**Keywords:** Large language models, Ontologies, Benchmark, Natural language processing, Symbolic knowledge

**Relevance Score:** 8

**TL;DR:** This paper introduces OntoURL, a benchmark for evaluating large language models' ontological capabilities in processing structured symbolic knowledge.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in understanding how large language models handle structured symbolic knowledge, specifically in terms of ontologies.

**Method:** OntoURL was developed as a systematic benchmark assessing LLMs on three dimensions: understanding, reasoning, and learning, through 15 tasks featuring 58,981 questions derived from 40 ontologies across 8 domains.

**Key Contributions:**

	1. Introduction of a taxonomy for LLMs' ontological capabilities
	2. Development of the OntoURL benchmark for systematic evaluation of ontological knowledge processing
	3. Revelation of LLMs' strengths and weaknesses in handling structured symbolic knowledge

**Result:** Experiments with 20 open-source LLMs showed significant performance variability, with models excelling in understanding but struggling in reasoning and learning tasks.

**Limitations:** The paper does not explore potential improvements for LLMs in reasoning and learning tasks.

**Conclusion:** The findings indicate substantial limitations in LLMs' ability to process symbolic knowledge, establishing OntoURL as an essential tool for future research.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing tasks, yet their ability to process structured symbolic knowledge remains underexplored. To address this gap, we propose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the first comprehensive benchmark designed to systematically evaluate LLMs' proficiency in handling ontologies -- formal, symbolic representations of domain knowledge through concepts, relationships, and instances. Based on the proposed taxonomy, OntoURL systematically assesses three dimensions: understanding, reasoning, and learning through 15 distinct tasks comprising 58,981 questions derived from 40 ontologies across 8 domains. Experiments with 20 open-source LLMs reveal significant performance differences across models, tasks, and domains, with current LLMs showing proficiency in understanding ontological knowledge but substantial weaknesses in reasoning and learning tasks. These findings highlight fundamental limitations in LLMs' capability to process symbolic knowledge and establish OntoURL as a critical benchmark for advancing the integration of LLMs with formal knowledge representations.

</details>


### [53] [CAMEO: Collection of Multilingual Emotional Speech Corpora](https://arxiv.org/abs/2505.11051)

*Iwona Christop, Maciej Czajka*

**Main category:** cs.CL

**Keywords:** emotional speech, emotion recognition, multilingual datasets

**Relevance Score:** 6

**TL;DR:** CAMEO is a multilingual emotional speech dataset aimed at enhancing emotion recognition research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide easy access to data for emotion recognition and ensure reproducibility in research.

**Method:** The paper outlines the dataset selection criteria, curation and normalization process, as well as performance results for several models tested on the dataset.

**Key Contributions:**

	1. Curated collection of multilingual emotional speech datasets
	2. Standardized benchmark for evaluating SER systems
	3. Public availability through Hugging Face platform

**Result:** Performance results showcase various models' effectiveness on the dataset, highlighting its comprehensive nature.

**Limitations:** 

**Conclusion:** CAMEO offers a standardized benchmark and is publicly available for broader research in emotion recognition.

**Abstract:** This paper presents CAMEO -- a curated collection of multilingual emotional speech datasets designed to facilitate research in emotion recognition and other speech-related tasks. The main objectives were to ensure easy access to the data, to allow reproducibility of the results, and to provide a standardized benchmark for evaluating speech emotion recognition (SER) systems across different emotional states and languages. The paper describes the dataset selection criteria, the curation and normalization process, and provides performance results for several models. The collection, along with metadata, and a leaderboard, is publicly available via the Hugging Face platform.

</details>


### [54] [BLEUBERI: BLEU is a surprisingly effective reward for instruction following](https://arxiv.org/abs/2505.11080)

*Yapei Chang, Yekyung Kim, Michael Krumdick, Amir Zadeh, Chuan Li, Chris Tanner, Mohit Iyyer*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Instruction-following, Language Model Alignment, BLEU, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** This paper introduces BLEUBERI, a method that uses BLEU as a reward function for aligning LLMs with human preferences in instruction-following tasks, demonstrating its effectiveness compared to traditional reward models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore simpler alternatives to costly reward models for aligning LLMs with human preferences, leveraging the availability of high-quality synthetic datasets.

**Method:** The paper develops BLEUBERI, which uses BLEU as a reward function in a Group Relative Policy Optimization (GRPO) framework, targeting challenging instructions identified through the model's performance.

**Key Contributions:**

	1. Introduction of BLEUBERI, a new alignment method using BLEU as a reward metric
	2. Demonstration that BLEU can match reward models in terms of human preference alignment
	3. Release of code and data for further research on effective LLM alignment techniques.

**Result:** BLEUBERI-trained models perform competitively with those trained using reward models across several benchmarks, maintaining high alignment with human preferences and generating factually grounded outputs.

**Limitations:** The study primarily focuses on instruction-following tasks and may not generalize well to other LLM applications.

**Conclusion:** BLEUBERI shows that string-matching metrics can serve as effective and inexpensive substitutes for reward models in aligning LLM outputs with user preferences, especially when quality reference outputs are available.

**Abstract:** Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI.

</details>


### [55] [Towards Better Evaluation for Generated Patent Claims](https://arxiv.org/abs/2505.11095)

*Lekang Jiang, Pascal A Scherz, Stephan Goetz*

**Main category:** cs.CL

**Keywords:** patent claims, evaluation, large language models, benchmarking, human expert assessment

**Relevance Score:** 7

**TL;DR:** This paper presents Patent-CE, a benchmark for evaluating patent claims and PatClaimEval, a method that correlates well with human expert assessments.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity of drafting patent claims creates access barriers for small enterprises, necessitating evaluation tools for automated systems.

**Method:** Patent-CE benchmark is introduced with expert-annotated evaluations on features like completeness, clarity, consistency, linkage, and quality. PatClaimEval is designed to assess claims across these dimensions.

**Key Contributions:**

	1. Introduction of Patent-CE benchmark for patent claims
	2. Development of PatClaimEval evaluation method
	3. Demonstration of high correlation with human expert evaluations

**Result:** PatClaimEval shows the highest correlation with human evaluations among tested metrics.

**Limitations:** 

**Conclusion:** The study establishes a foundation for improving evaluations in automated patent claim generation.

**Abstract:** Patent claims define the scope of protection and establish the legal boundaries of an invention. Drafting these claims is a complex and time-consuming process that usually requires the expertise of skilled patent attorneys, which can form a large access barrier for many small enterprises. To solve these challenges, researchers have investigated the use of large language models (LLMs) for automating patent claim generation. However, existing studies highlight inconsistencies between automated evaluation metrics and human expert assessments. To bridge this gap, we introduce Patent-CE, the first comprehensive benchmark for evaluating patent claims. Patent-CE includes comparative claim evaluations annotated by patent experts, focusing on five key criteria: feature completeness, conceptual clarity, terminology consistency, logical linkage, and overall quality. Additionally, we propose PatClaimEval, a novel multi-dimensional evaluation method specifically designed for patent claims. Our experiments demonstrate that PatClaimEval achieves the highest correlation with human expert evaluations across all assessment criteria among all tested metrics. This research provides the groundwork for more accurate evaluations of automated patent claim generation systems.

</details>


### [56] [LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena](https://arxiv.org/abs/2501.03266)

*Stefan Pasch*

**Main category:** cs.CL

**Keywords:** content moderation, user satisfaction, ethical alignment, LLM, profanity moderation

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of content moderation on user satisfaction in LLMs, particularly how users react to ethical refusals versus technical refusals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the underexplored relationship between content moderation practices in LLMs and user satisfaction, especially in cases where models refuse to answer prompts due to ethical considerations.

**Method:** The study analyzes nearly 50,000 model comparisons from Chatbot Arena, utilizing a fine-tuned RoBERTa-based refusal classifier to differentiate between ethical and technical refusals.

**Key Contributions:**

	1. Introduces a novel refusal classifier for analyzing user preferences in LLM responses.
	2. Provides empirical evidence of the refusal penalty related to ethical versus technical refusals.
	3. Highlights the importance of contextual sensitivity in user evaluations of LLM refusals.

**Result:** The analysis shows that ethical refusals lead to significantly lower win rates compared to technical refusals and standard responses, highlighting user dissatisfaction with ethical refusals, except when prompts are sensitive or refusals are contextually well-phrased.

**Limitations:** The study is limited to data from Chatbot Arena, which may not represent the broader user base and context. Additionally, it focuses primarily on refusal types without exploring other moderation strategies.

**Conclusion:** The findings emphasize the necessity for adaptive moderation strategies in LLM design that align safety behaviors with user expectations, especially in sensitive contexts.

**Abstract:** LLM safety and ethical alignment are widely discussed, but the impact of content moderation on user satisfaction remains underexplored. In particular, little is known about how users respond when models refuse to answer a prompt-one of the primary mechanisms used to enforce ethical boundaries in LLMs. We address this gap by analyzing nearly 50,000 model comparisons from Chatbot Arena, a platform where users indicate their preferred LLM response in pairwise matchups, providing a large-scale setting for studying real-world user preferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a hand-labeled dataset, we distinguish between refusals due to ethical concerns and technical limitations. Our results reveal a substantial refusal penalty: ethical refusals yield significantly lower win rates than both technical refusals and standard responses, indicating that users are especially dissatisfied when models decline a task for ethical reasons. However, this penalty is not uniform. Refusals receive more favorable evaluations when the underlying prompt is highly sensitive (e.g., involving illegal content), and when the refusal is phrased in a detailed and contextually aligned manner. These findings underscore a core tension in LLM design: safety-aligned behaviors may conflict with user expectations, calling for more adaptive moderation strategies that account for context and presentation.

</details>


### [57] [Scaling Reasoning can Improve Factuality in Large Language Models](https://arxiv.org/abs/2505.11140)

*Mike Zhang, Johannes Bjerva, Russa Biswas*

**Main category:** cs.CL

**Keywords:** large language models, reasoning, open-domain QA, factual accuracy, knowledge graphs

**Relevance Score:** 9

**TL;DR:** This study investigates the impact of longer reasoning chains and additional computational resources on the factual accuracy of large language models (LLMs) in open-domain question-answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether longer reasoning chains improve factual accuracy in LLMs beyond just mathematical reasoning.

**Method:** The authors distill reasoning traces from advanced LLMs, fine-tune various models, and integrate factual information from knowledge graphs, conducting a comprehensive experimental evaluation over multiple datasets.

**Key Contributions:**

	1. Introduction of reasoning traces from advanced LLMs
	2. Integration of knowledge graph information into reasoning processes
	3. Empirical evidence on the impact of compute and token budgets on accuracy

**Result:** Smaller reasoning models show improved factual accuracy over original instruction-tuned models and using additional compute and token budgets leads to a consistent 2-8% increase in accuracy.

**Limitations:** 

**Conclusion:** The findings support the idea that more computational resources and longer reasoning processes can enhance the performance and accuracy of LLMs in open-domain QA scenarios.

**Abstract:** Recent studies on large language model (LLM) reasoning capabilities have demonstrated promising improvements in model performance by leveraging a lengthy thinking process and additional computational resources during inference, primarily in tasks involving mathematical reasoning (Muennighoff et al., 2025). However, it remains uncertain if longer reasoning chains inherently enhance factual accuracy, particularly beyond mathematical contexts. In this work, we thoroughly examine LLM reasoning within complex open-domain question-answering (QA) scenarios. We initially distill reasoning traces from advanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then fine-tune a variety of models ranging from smaller, instruction-tuned variants to larger architectures based on Qwen2.5. To enrich reasoning traces, we introduce factual information from knowledge graphs in the form of paths into our reasoning traces. Our experimental setup includes four baseline approaches and six different instruction-tuned models evaluated across a benchmark of six datasets, encompassing over 22.6K questions. Overall, we carry out 168 experimental runs and analyze approximately 1.7 million reasoning traces. Our findings indicate that, within a single run, smaller reasoning models achieve noticeable improvements in factual accuracy compared to their original instruction-tuned counterparts. Moreover, our analysis demonstrates that adding test-time compute and token budgets factual accuracy consistently improves by 2-8%, further confirming the effectiveness of test-time scaling for enhancing performance and consequently improving reasoning accuracy in open-domain QA tasks. We release all the experimental artifacts for further research.

</details>


### [58] [SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization](https://arxiv.org/abs/2505.11166)

*Huashan Sun, Shengyi Liao, Yansen Han, Yu Bai, Yang Gao, Cheng Fu, Weizhou Shen, Fanqi Wan, Ming Yan, Ji Zhang, Fei Huang*

**Main category:** cs.CL

**Keywords:** long-context optimization, large language models, preference optimization, reward alignment, computational efficiency

**Relevance Score:** 8

**TL;DR:** This paper proposes SoLoPO, a framework aimed at improving long-context utilization in LLMs by optimizing short-context preference and aligning rewards to enhance model performance in extended contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges LLMs face in utilizing long-context information due to issues like data quality and inefficiencies in training and optimization objectives.

**Method:** The proposed SoLoPO framework decouples long-context preference optimization into short-context preference optimization and short-to-long reward alignment, leveraging theoretical and empirical evidence for its effectiveness.

**Key Contributions:**

	1. Introduction of SoLoPO framework for long-context optimization in LLMs.
	2. Decoupling of long-context preference optimization into two components: short-context preference optimization and SoLo-RA.
	3. Demonstration of improved generalization abilities and efficiency across long-context benchmarks.

**Result:** SoLoPO shows enhancements in length and domain generalization abilities across various long-context benchmarks, alongside improvements in computational and memory efficiency compared to existing algorithms.

**Limitations:** 

**Conclusion:** The SoLoPO framework significantly enhances the ability of LLMs to utilize long-context information by improving preference optimization and reward alignment, yielding better performance without compromising efficiency.

**Abstract:** Despite advances in pretraining with extended context lengths, large language models (LLMs) still face challenges in effectively utilizing real-world long-context information, primarily due to insufficient long-context alignment caused by data quality issues, training inefficiencies, and the lack of well-designed optimization objectives. To address these limitations, we propose a framework named $\textbf{S}$h$\textbf{o}$rt-to-$\textbf{Lo}$ng $\textbf{P}$reference $\textbf{O}$ptimization ($\textbf{SoLoPO}$), decoupling long-context preference optimization (PO) into two components: short-context PO and short-to-long reward alignment (SoLo-RA), supported by both theoretical and empirical evidence. Specifically, short-context PO leverages preference pairs sampled from short contexts to enhance the model's contextual knowledge utilization ability. Meanwhile, SoLo-RA explicitly encourages reward score consistency utilization for the responses when conditioned on both short and long contexts that contain identical task-relevant information. This facilitates transferring the model's ability to handle short contexts into long-context scenarios. SoLoPO is compatible with mainstream preference optimization algorithms, while substantially improving the efficiency of data construction and training processes. Experimental results show that SoLoPO enhances all these algorithms with respect to stronger length and domain generalization abilities across various long-context benchmarks, while achieving notable improvements in both computational and memory efficiency.

</details>


### [59] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)

*Gino Carmona-Díaz, William Jiménez-Leal, María Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Bermúdez*

**Main category:** cs.CL

**Keywords:** LLM, text analysis, taxonomy, HCI, collaboration

**Relevance Score:** 8

**TL;DR:** This paper presents a tutorial on using LLMs to develop, test, and apply taxonomies for text analysis of unstructured data, emphasizing collaboration between researchers and LLMs.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** The traditional process of analyzing texts like open-ended responses and social media posts is labor-intensive and biased. Utilizing LLMs can streamline this process without sacrificing quality.

**Method:** The paper guides readers through an iterative and collaborative workflow with LLMs that includes developing prompts for dataset review, generating taxonomies, modifying and testing them, and achieving high reliability in categorization.

**Key Contributions:**

	1. Step-by-step tutorial for LLM-assisted taxonomy development
	2. Emphasis on collaborative process between researchers and LLMs
	3. Demonstration of achieving high intercoder reliability in text categorization

**Result:** The approach allows for efficient taxonomy creation and application with demonstrated high intercoder reliability using personal goal examples.

**Limitations:** Potential bias in LLM outputs and dependency on data quality.

**Conclusion:** Using LLMs for text analysis presents both opportunities and limitations, while the proposed method enhances efficiency and reliability in the categorization of unstructured data.

**Abstract:** Analyzing texts such as open-ended responses, headlines, or social media posts is a time- and labor-intensive process highly susceptible to bias. LLMs are promising tools for text analysis, using either a predefined (top-down) or a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we present a step-by-step tutorial to efficiently develop, test, and apply taxonomies for analyzing unstructured data through an iterative and collaborative process between researchers and LLMs. Using personal goals provided by participants as an example, we demonstrate how to write prompts to review datasets and generate a taxonomy of life domains, evaluate and refine the taxonomy through prompt and direct modifications, test the taxonomy and assess intercoder agreements, and apply the taxonomy to categorize an entire dataset with high intercoder reliability. We discuss the possibilities and limitations of using LLMs for text analysis.

</details>


### [60] [Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline](https://arxiv.org/abs/2505.11177)

*Hrishit Madhavi, Jacob Cherian, Yuvraj Khamkar, Dhananjay Bhagat*

**Main category:** cs.CL

**Keywords:** multilingual information extraction, Optical Character Recognition, large language models, sentiment analysis, document comprehension

**Relevance Score:** 7

**TL;DR:** This paper describes a multilingual information extraction system that processes image-based documents using OCR and large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance access to information in image media across different linguistic environments.

**Method:** The system employs Tesseract for OCR to extract text from images in multiple languages, followed by a pipeline utilizing Gemini for translation, summarization, and re-translation. It also includes modules for sentiment analysis, topic classification, and date extraction.

**Key Contributions:**

	1. Development of a robust pipeline for multilingual document processing
	2. Integration of multiple AI tools (OCR, LLMs, classifiers) for improved information extraction
	3. Accessible Gradio interface for real-world application

**Result:** Demonstrates effective extraction and processing of multilingual information from image-based documents, improving document comprehension.

**Limitations:** 

**Conclusion:** The proposed end-to-end system effectively bridges the language gap in document access and comprehension.

**Abstract:** This paper presents an end-to-end suite for multilingual information extraction and processing from image-based documents. The system uses Optical Character Recognition (Tesseract) to extract text in languages such as English, Hindi, and Tamil, and then a pipeline involving large language model APIs (Gemini) for cross-lingual translation, abstractive summarization, and re-translation into a target language. Additional modules add sentiment analysis (TensorFlow), topic classification (Transformers), and date extraction (Regex) for better document comprehension. Made available in an accessible Gradio interface, the current research shows a real-world application of libraries, models, and APIs to close the language gap and enhance access to information in image media across different linguistic environments

</details>


### [61] [NoPE: The Counting Power of Transformers with No Positional Encodings](https://arxiv.org/abs/2505.11199)

*Chris Köcher, Alexander Kozachinskiy, Anthony Widjaja Lin, Marco Sälzer, Georg Zetzsche*

**Main category:** cs.CL

**Keywords:** NoPE-transformers, hard attention mechanisms, expressiveness, semi-algebraic sets, undecidability

**Relevance Score:** 6

**TL;DR:** This paper explores the expressive capabilities of NoPE-transformers with average hard attention mechanisms, demonstrating they can express complex counting languages linked to Diophantine equations, while revealing their limitation in expressing certain properties like PARITY.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the expressiveness of NoPE-transformers without positional encodings and map their capabilities to algebraic concepts, challenging existing models.

**Method:** The authors analyze Average Hard Attention NoPE-Transformers (NoPE-AHATs) and characterize the languages they can express in terms of semi-algebraic sets and counting properties.

**Key Contributions:**

	1. Characterization of languages expressible by NoPE-AHATs as semi-algebraic sets.
	2. Establishing the undecidability of analyzing NoPE-transformers.
	3. Identification of a counting language not expressible by average hard attention transformers.

**Result:** NoPE-AHATs can express languages related to nonnegative integer solutions of multivariate polynomial inequations, surpassing simpler models but failing to express basic counting properties like PARITY.

**Limitations:** The paper does not detail practical applications or implementations of NoPE-transformers in real-world scenarios.

**Conclusion:** The study characterizes the limitations and capabilities of NoPE-transformers in terms of expressiveness, showing undecidability in analyzing their classification abilities and presenting a comparison with classical complexity classes.

**Abstract:** Positional Encodings (PEs) seem to be indispensable for ensuring expressiveness of transformers; without them attention transformers reduce to a bag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard attention mechanisms were very recently shown to only be able to express regular languages, i.e., with limited counting ability. This paper shows that, with average hard attention mechanisms, NoPE-transformers are still surprisingly expressive: they can express counting languages corresponding to nonnegative integer solutions to multivariate polynomial equations (i.e. Diophantine equations), reasoning about which is well-known to be undecidable. In fact, we provide a precise characterization of languages expressible by Average Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond precisely to what we call \emph{semi-algebraic sets}, i.e., finite unions of sets of nonnegative integer solutions to systems of multivariate polynomial inequations. We obtain several interesting consequences of our characterization. Firstly, NoPE-transformers can express counting properties that are far more complex than established models like simplified counter machines and Petri nets, but cannot express a very simple counting property of PARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable, e.g., whether a given NoPE transformer classifies all input strings in one class. To complement our results, we exhibit a counting language that is not expressible by average hard attention transformers even with arbitrary PEs but is expressible in the circuit complexity class TC$^0$, answering an open problem.

</details>


### [62] [HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization](https://arxiv.org/abs/2505.11225)

*Chengyu Huang, Zhengxin Zhang, Claire Cardie*

**Main category:** cs.CL

**Keywords:** large language models, policy optimization, conciseness, historical data, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents a method called History-Aware Policy Optimization (HAPO) that aims to enhance the conciseness of responses from large language models (LLMs) by utilizing historical data from past encounters, optimizing both length and correctness during generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning abilities of large language models while minimizing verbosity and reducing inference costs through better use of historical performance data.

**Method:** HAPO keeps track of a history state for each problem, employing a novel length reward function based on this history to incentivize discovering more concise yet correct solutions. It combines length and correctness rewards to optimize the generation process.

**Key Contributions:**

	1. Introduction of History-Aware Policy Optimization (HAPO) for LLMs.
	2. Development of a length reward structure that incorporates historical performance.
	3. Demonstrated improvements in response conciseness and efficiency on math benchmarks.

**Result:** HAPO was used to train several models and showed effectiveness in producing concise reasoning, achieving length reductions of 33-59% with minimal accuracy drops of 2-5% across various math benchmarks.

**Limitations:** 

**Conclusion:** HAPO successfully induces more concise reasoning in LLMs without significantly compromising accuracy, demonstrating a promising approach to test-time scaling.

**Abstract:** While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.

</details>


### [63] [Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models](https://arxiv.org/abs/2505.11271)

*Camille Couturier, Spyros Mastorakis, Haiying Shen, Saravan Rajmohan, Victor Rühle*

**Main category:** cs.CL

**Keywords:** Large Language Models, semantic caching, real-time processing

**Relevance Score:** 8

**TL;DR:** Introduces a semantic caching method for efficient LLM-based question-answering workflows, reducing redundancy in processing lengthy contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address high computational overhead, memory usage, and network bandwidth from processing lengthy contexts in distributed systems for LLMs.

**Method:** A novel semantic caching approach that stores and reuses intermediate contextual summaries for similar queries in QA workflows.

**Key Contributions:**

	1. Semantic caching for LLM workflows.
	2. Up to 60% reduction in redundant computations.
	3. Maintains answer accuracy similar to full document processing.

**Result:** The method reduces redundant computations by 50-60% while maintaining answer accuracy comparable to full document processing, tested on various datasets.

**Limitations:** 

**Conclusion:** The approach achieves a balance between computational cost and response quality, which is essential for real-time AI assistants.

**Abstract:** Large Language Models (LLMs) are increasingly deployed across edge and cloud platforms for real-time question-answering and retrieval-augmented generation. However, processing lengthy contexts in distributed systems incurs high computational overhead, memory usage, and network bandwidth. This paper introduces a novel semantic caching approach for storing and reusing intermediate contextual summaries, enabling efficient information reuse across similar queries in LLM-based QA workflows. Our method reduces redundant computations by up to 50-60% while maintaining answer accuracy comparable to full document processing, as demonstrated on NaturalQuestions, TriviaQA, and a synthetic ArXiv dataset. This approach balances computational cost and response quality, critical for real-time AI assistants.

</details>


### [64] [Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs](https://arxiv.org/abs/2505.11277)

*Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, Xiang Wang*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, retrieval-augmented reasoning

**Relevance Score:** 9

**TL;DR:** This paper presents AutoRefine, a reinforcement learning framework that enhances the reasoning capabilities of large language models through a refined search and evidence aggregation methodology.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current retrieval-augmented reasoning methods often yield irrelevant information, which hampers the reasoning process of LLMs.

**Method:** AutoRefine employs a search-and-refine-during-think paradigm, incorporating explicit knowledge refinement steps between search calls and utilizing tailored retrieval-specific rewards with group relative policy optimization.

**Key Contributions:**

	1. Introduction of AutoRefine framework for LLMs
	2. Implementation of knowledge refinement steps in reasoning
	3. Development of tailored rewards for retrieval effectiveness

**Result:** Experiments show that AutoRefine significantly outperforms existing methods in both single-hop and multi-hop question answering benchmarks, especially in complex scenarios.

**Limitations:** 

**Conclusion:** The introduction of AutoRefine leads to improved search quality and effective evidence synthesis, thereby enhancing the overall reasoning capabilities of LLMs.

**Abstract:** Large language models have demonstrated impressive reasoning capabilities but are inherently limited by their knowledge reservoir. Retrieval-augmented reasoning mitigates this limitation by allowing LLMs to query external resources, but existing methods often retrieve irrelevant or noisy information, hindering accurate reasoning. In this paper, we propose AutoRefine, a reinforcement learning post-training framework that adopts a new ``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit knowledge refinement steps between successive search calls, enabling the model to iteratively filter, distill, and organize evidence before generating an answer. Furthermore, we incorporate tailored retrieval-specific rewards alongside answer correctness rewards using group relative policy optimization. Experiments on single-hop and multi-hop QA benchmarks demonstrate that AutoRefine significantly outperforms existing approaches, particularly in complex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine issues frequent, higher-quality searches and synthesizes evidence effectively.

</details>


### [65] [Temporal fine-tuning for early risk detection](https://arxiv.org/abs/2505.11280)

*Horacio Thompson, Esaú Villatoro-Tello, Manuel Montes-y-Gómez, Marcelo Errecalde*

**Main category:** cs.CL

**Keywords:** Early Risk Detection, transformer models, multi-objective optimization

**Relevance Score:** 8

**TL;DR:** This paper presents a novel approach for Early Risk Detection (ERD) on the Web, focusing on improving classification precision and minimizing detection delay through a temporal fine-tuning strategy for transformer models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for timely and accurate identification of users facing social and health issues on the web, particularly in critical scenarios.

**Method:** The authors propose a multi-objective approach that combines classification performance with decision time optimization, utilizing a novel temporal fine-tuning strategy for transformer-based models.

**Key Contributions:**

	1. Introduction of temporal fine-tuning for transformer-based models
	2. Application of a multi-objective approach combining precision and delay
	3. Competitive results in mental health detection tasks in Spanish context.

**Result:** The evaluation demonstrates that temporal fine-tuning achieves competitive performance in diagnosing depression and eating disorders in Spanish, outperforming existing models in terms of precision and detection speed.

**Limitations:** 

**Conclusion:** The proposed method effectively addresses the challenges of ERD by integrating temporal considerations within the learning process, optimizing both precision and speed.

**Abstract:** Early Risk Detection (ERD) on the Web aims to identify promptly users facing social and health issues. Users are analyzed post-by-post, and it is necessary to guarantee correct and quick answers, which is particularly challenging in critical scenarios. ERD involves optimizing classification precision and minimizing detection delay. Standard classification metrics may not suffice, resorting to specific metrics such as ERDE(theta) that explicitly consider precision and delay. The current research focuses on applying a multi-objective approach, prioritizing classification performance and establishing a separate criterion for decision time. In this work, we propose a completely different strategy, temporal fine-tuning, which allows tuning transformer-based models by explicitly incorporating time within the learning process. Our method allows us to analyze complete user post histories, tune models considering different contexts, and evaluate training performance using temporal metrics. We evaluated our proposal in the depression and eating disorders tasks for the Spanish language, achieving competitive results compared to the best models of MentalRiskES 2023. We found that temporal fine-tuning optimized decisions considering context and time progress. In this way, by properly taking advantage of the power of transformers, it is possible to address ERD by combining precision and speed as a single objective.

</details>


### [66] [Probing Subphonemes in Morphology Models](https://arxiv.org/abs/2505.11297)

*Gal Astrach, Yuval Pinter*

**Main category:** cs.CL

**Keywords:** Transformers, Morphological inflection, Phonological features, Language-agnostic probing, Subphonemic features

**Relevance Score:** 6

**TL;DR:** This study introduces a language-agnostic probing method to explore phonological feature encoding in transformers, revealing varying effectiveness in capturing local and long-distance dependencies across seven languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand why transformers struggle to generalize across languages and morphological rules in inflection tasks, particularly in relation to phonological phenomena.

**Method:** A language-agnostic probing method is introduced, focusing on phonological feature encoding in transformers trained on phonemes, assessed across seven morphologically diverse languages.

**Key Contributions:**

	1. Introduction of a language-agnostic probing method for phonological features
	2. Analysis of local vs long-distance phonological dependencies in transformers
	3. Guidance for empirical strategies in training morphological models

**Result:** Phonological features that are local, such as devoicing, are well-represented in phoneme embeddings, while long-distance dependencies like vowel harmony are better captured by the transformer's encoder.

**Limitations:** The study is limited to seven languages, which may not encompass the full range of morphological diversity.

**Conclusion:** The findings provide insights into empirical strategies for training morphological models, highlighting the importance of subphonemic feature acquisition.

**Abstract:** Transformers have achieved state-of-the-art performance in morphological inflection tasks, yet their ability to generalize across languages and morphological rules remains limited. One possible explanation for this behavior can be the degree to which these models are able to capture implicit phenomena at the phonological and subphonemic levels. We introduce a language-agnostic probing method to investigate phonological feature encoding in transformers trained directly on phonemes, and perform it across seven morphologically diverse languages. We show that phonological features which are local, such as final-obstruent devoicing in Turkish, are captured well in phoneme embeddings, whereas long-distance dependencies like vowel harmony are better represented in the transformer's encoder. Finally, we discuss how these findings inform empirical strategies for training morphological models, particularly regarding the role of subphonemic feature acquisition.

</details>


### [67] [XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision](https://arxiv.org/abs/2505.11336)

*Nuo Chen, Andre Lin HuiKai, Jiaying Wu, Junyi Hou, Zining Zhang, Qian Wang, Xidong Wang, Bingsheng He*

**Main category:** cs.CL

**Keywords:** human-AI collaboration, large language models, academic writing, XtraGPT, instruction-guided revision

**Relevance Score:** 9

**TL;DR:** The paper presents a human-AI collaboration framework called XtraGPT, aimed at enhancing academic writing via context-aware, instruction-guided LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of existing large language models in supporting the complex demands of academic writing, which often requires more than just surface-level revisions.

**Method:** The authors introduce a dataset of 7,040 annotated research papers with 140,000 instruction-response pairs, and develop XtraGPT, a suite of open-source LLMs designed for academic paper revision.

**Key Contributions:**

	1. Introduction of a comprehensive dataset for academic paper revisions.
	2. Development of XtraGPT, a suite of open-source LLMs for academic writing.
	3. Validation of enhanced performance through automated assessments and human evaluations.

**Result:** XtraGPT shows significant performance improvements over same-scale baselines and approaches the quality of proprietary systems through extensive experiments.

**Limitations:** 

**Conclusion:** The findings demonstrate that XtraGPT effectively aids in improving the quality of scientific drafts through iterative and context-aware revisions.

**Abstract:** Despite the growing adoption of large language models (LLMs) in academic workflows, their capabilities remain limited when it comes to supporting high-quality scientific writing. Most existing systems are designed for general-purpose scientific text generation and fail to meet the sophisticated demands of research communication beyond surface-level polishing, such as conceptual coherence across sections. Furthermore, academic writing is inherently iterative and revision-driven, a process not well supported by direct prompting-based paradigms. To address these scenarios, we propose a human-AI collaboration framework for academic paper revision. We first introduce a comprehensive dataset of 7,040 research papers from top-tier venues annotated with over 140,000 instruction-response pairs that reflect realistic, section-level scientific revisions. Building on the dataset, we develop XtraGPT, the first suite of open-source LLMs, designed to provide context-aware, instruction-guided writing assistance, ranging from 1.5B to 14B parameters. Extensive experiments validate that XtraGPT significantly outperforms same-scale baselines and approaches the quality of proprietary systems. Both automated preference assessments and human evaluations confirm the effectiveness of our models in improving scientific drafts.

</details>


### [68] [Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models](https://arxiv.org/abs/2505.11341)

*Banca Calvo Figueras, Rodrigo Agerri*

**Main category:** cs.CL

**Keywords:** Critical Questions Generation, Dataset, Large Language Models, Automatic Evaluation, Critical Thinking

**Relevance Score:** 8

**TL;DR:** This paper presents a comprehensive approach to Critical Questions Generation by creating a large-scale dataset and evaluating LLMs for generating critical questions to enhance reasoning and critical thinking.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To promote critical thinking through the development of systems that generate questions challenging assumptions and reasoning in arguments, addressing the current limitations in datasets and evaluation standards.

**Method:** Construction of the first large-scale manually-annotated dataset for Critical Questions Generation, along with the exploration of automatic evaluation methods, particularly utilizing LLMs.

**Key Contributions:**

	1. Creation of the first large-scale manually-annotated dataset for Critical Questions Generation.
	2. Identification of effective automatic evaluation methods correlating with human judgment using LLMs.
	3. Provision of data, code, and a public leaderboard to support ongoing research efforts.

**Result:** The zero-shot evaluation of 11 LLMs establishes a strong baseline for the task and highlights its inherent difficulty, while a reference-based evaluation method correlates well with human judgments.

**Limitations:** 

**Conclusion:** Providing data, code, and a public leaderboard aims to facilitate further research into both models' performance and the practical applications of Critical Questions Generation for reasoning and human critical thinking.

**Abstract:** The task of Critical Questions Generation (CQs-Gen) aims to foster critical thinking by enabling systems to generate questions that expose assumptions and challenge the reasoning in arguments. Despite growing interest in this area, progress has been hindered by the lack of suitable datasets and automatic evaluation standards. This work presents a comprehensive approach to support the development and benchmarking of systems for this task. We construct the first large-scale manually-annotated dataset. We also investigate automatic evaluation methods and identify a reference-based technique using large language models (LLMs) as the strategy that best correlates with human judgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline while showcasing the difficulty of the task. Data, code, and a public leaderboard are provided to encourage further research not only in terms of model performance, but also to explore the practical benefits of CQs-Gen for both automated reasoning and human critical thinking.

</details>


### [69] [LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors](https://arxiv.org/abs/2505.11352)

*Rao Ma, Tongzhou Chen, Kartik Audhkhasi, Bhuvana Ramabhadran*

**Main category:** cs.CL

**Keywords:** speech processing, large language models, automatic speech recognition, model modularity, domain adaptation

**Relevance Score:** 9

**TL;DR:** The paper proposes LegoSLM, a method that combines speech encoders and LLMs using ASR posterior matrices for improved performance in ASR and speech translation tasks.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing methods that combine speech encoders and LLMs, such as inflexibility and suboptimal performance.

**Method:** The proposed LegoSLM method uses the ASR posterior matrices to generate pseudo-audio embeddings from a speech encoder, which are then concatenated with text embeddings for LLM input.

**Key Contributions:**

	1. Introduction of the LegoSLM paradigm for integrating speech encoders and LLMs
	2. Demonstration of effective performance improvements in ASR and speech translation tasks
	3. Modular architecture allows for zero-shot model switching and fine-tuning capabilities

**Result:** The LegoSLM method achieves a 49% word error rate reduction (WERR) over the USM-CTC baseline on 8 multi-lingual speech test sets and demonstrates modularity by allowing easy swapping of the speech encoder.

**Limitations:** 

**Conclusion:** LegoSLM effectively bridges the gap between speech encoders and LLMs, enhancing ASR and speech translation tasks while offering flexibility in model configurations.

**Abstract:** Recently, large-scale pre-trained speech encoders and Large Language Models (LLMs) have been released, which show state-of-the-art performance on a range of spoken language processing tasks including Automatic Speech Recognition (ASR). To effectively combine both models for better performance, continuous speech prompts, and ASR error correction have been adopted. However, these methods are prone to suboptimal performance or are inflexible. In this paper, we propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using the ASR posterior matrices. The speech encoder is trained to generate Connectionist Temporal Classification (CTC) posteriors over the LLM vocabulary, which are used to reconstruct pseudo-audio embeddings by computing a weighted sum of the LLM input embeddings. These embeddings are concatenated with text embeddings in the LLM input space. Using the well-performing USM and Gemma models as an example, we demonstrate that our proposed LegoSLM method yields good performance on both ASR and speech translation tasks. By connecting USM with Gemma models, we can get an average of 49% WERR over the USM-CTC baseline on 8 MLS testsets. The trained model also exhibits modularity in a range of settings -- after fine-tuning the Gemma model weights, the speech encoder can be switched and combined with the LLM in a zero-shot fashion. Additionally, we propose to control the decode-time influence of the USM and LLM using a softmax temperature, which shows effectiveness in domain adaptation.

</details>


### [70] [GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents](https://arxiv.org/abs/2505.11368)

*Lingxiao Diao, Xinyue Xu, Wanxuan Sun, Cheng Yang, Zhuosheng Zhang*

**Main category:** cs.CL

**Keywords:** large language models, benchmark, guideline following, domain-oriented, human preferences

**Relevance Score:** 8

**TL;DR:** Introduction of GuideBench, a benchmark for evaluating large language models (LLMs) on domain-oriented guideline following capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for comprehensive benchmarks to assess LLMs' abilities in following domain-specific guidelines due to conflicts with their commonsense knowledge and frequent updates of those guidelines.

**Method:** GuideBench evaluates LLMs on adherence to diverse rules, robustness to rule updates, and alignment with human preferences.

**Key Contributions:**

	1. Introduction of a new benchmark (GuideBench) for evaluating LLMs
	2. Focus on three critical evaluation aspects: adherence to rules, robustness to updates, and human alignment
	3. Experimental results highlighting areas for improvement in LLM guideline following capabilities.

**Result:** Experimental results show significant opportunities for improving LLMs’ performances in guideline adherence.

**Limitations:** 

**Conclusion:** GuideBench can facilitate better evaluation and development of LLMs as it addresses critical aspects of their performance in domain-oriented settings.

**Abstract:** Large language models (LLMs) have been widely deployed as autonomous agents capable of following user instructions and making decisions in real-world applications. Previous studies have made notable progress in benchmarking the instruction following capabilities of LLMs in general domains, with a primary focus on their inherent commonsense knowledge. Recently, LLMs have been increasingly deployed as domain-oriented agents, which rely on domain-oriented guidelines that may conflict with their commonsense knowledge. These guidelines exhibit two key characteristics: they consist of a wide range of domain-oriented rules and are subject to frequent updates. Despite these challenges, the absence of comprehensive benchmarks for evaluating the domain-oriented guideline following capabilities of LLMs presents a significant obstacle to their effective assessment and further development. In this paper, we introduce GuideBench, a comprehensive benchmark designed to evaluate guideline following performance of LLMs. GuideBench evaluates LLMs on three critical aspects: (i) adherence to diverse rules, (ii) robustness to rule updates, and (iii) alignment with human preferences. Experimental results on a range of LLMs indicate substantial opportunities for improving their ability to follow domain-oriented guidelines.

</details>


### [71] [A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography](https://arxiv.org/abs/2505.11379)

*Alicia González Martínez*

**Main category:** cs.CL

**Keywords:** Quranic Orthography, tajwid, phonetic notation, Quran, Arabic script

**Relevance Score:** 0

**TL;DR:** This paper presents a comprehensive system for analyzing the phonetic rules of tajwid in the Quran using computational methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the systematic rules of tajwid in the Quran and enhance the understanding of its phonetic processes through digital tools.

**Method:** Developed a Python module to manipulate the orthographic layer of tajwid from the Contemporary Quranic Orthography (CQO) while studying the Cairo Quran text.

**Key Contributions:**

	1. Development of a Python module for tajwid analysis
	2. Creation of a framework for comparing Quranic manuscripts
	3. Insights into the phonetic and prosodic rules of tajwid

**Result:** The research allows for precise analysis of the Quranic text and facilitates alignment and comparison of Quranic manuscripts, enriching the study of Arabic script.

**Limitations:** 

**Conclusion:** The developed framework can serve as an effective tool for examining diacritical notation systems in Quranic manuscripts.

**Abstract:** Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic notation that can be traced back to the early stages of Islam, when the Quran was mainly oral in nature and the first written renderings of it served as memory aids for this oral tradition. The early systems of diacritical marks created on top of the Quranic Consonantal Text (QCT) motivated the creation and further development of a fine-grained system of phonetic notation that represented tajwid-the rules of recitation. We explored the systematicity of the rules of tajwid, as they are encountered in the Cairo Quran, using a fully and accurately encoded digital edition of the Quranic text. For this purpose, we developed a python module that can remove or add the orthographic layer of tajwid from a Quranic text in CQO. The interesting characteristic of these two sets of rules is that they address the complete Quranic text of the Cairo Quran, so they can be used as precise witnesses to study its phonetic and prosodic processes. From a computational point of view, the text of the Cairo Quran can be used as a linchpin to align and compare Quranic manuscripts, due to its richness and completeness. This will let us create a very powerful framework to work with the Arabic script, not just within an isolated text, but automatically exploring a specific textual phenomenon in other connected manuscripts. Having all the texts mapped among each other can serve as a powerful tool to study the nature of the notation systems of diacritics added to the consonantal skeleton.

</details>


### [72] [CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs](https://arxiv.org/abs/2505.11413)

*Sijia Chen, Xiaomin Li, Mengxue Zhang, Eric Hanchen Jiang, Qingcheng Zeng, Chen-Hsiang Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Medical Safety, Adversarial Robustness, Health Informatics, Benchmarking

**Relevance Score:** 9

**TL;DR:** CARES is a benchmark designed to evaluate the safety of large language models (LLMs) in medical contexts, addressing existing gaps in harmful prompt assessments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing deployment of LLMs in healthcare, there is a need for specific benchmarks that examine their safety and robustness against adversarial prompts.

**Method:** CARES includes over 18,000 prompts categorized by medical safety principles, harm levels, and prompting styles, employing a three-way response evaluation protocol to assess model responses.

**Key Contributions:**

	1. Introduction of the CARES benchmark for medical LLM safety evaluation.
	2. Development of a fine-grained Safety Score metric and a three-way response evaluation protocol.
	3. Proposal of a mitigation strategy using a lightweight classifier for detecting jailbreak attempts.

**Result:** Analysis shows many LLMs are still vulnerable to cleverly rephrased harmful prompts, with a high tendency to over-reject safe queries.

**Limitations:** The benchmark may require continuous updates as LLMs and adversarial techniques evolve.

**Conclusion:** CARES offers a framework for evaluating and improving the safety of LLMs in medical settings, incorporating strategies to mitigate vulnerabilities.

**Abstract:** Large language models (LLMs) are increasingly deployed in medical contexts, raising critical concerns about safety, alignment, and susceptibility to adversarial manipulation. While prior benchmarks assess model refusal capabilities for harmful prompts, they often lack clinical specificity, graded harmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES (Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for evaluating LLM safety in healthcare. CARES includes over 18,000 prompts spanning eight medical safety principles, four harm levels, and four prompting styles: direct, indirect, obfuscated, and role-play, to simulate both malicious and benign use cases. We propose a three-way response evaluation protocol (Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess model behavior. Our analysis reveals that many state-of-the-art LLMs remain vulnerable to jailbreaks that subtly rephrase harmful prompts, while also over-refusing safe but atypically phrased queries. Finally, we propose a mitigation strategy using a lightweight classifier to detect jailbreak attempts and steer models toward safer behavior via reminder-based conditioning. CARES provides a rigorous framework for testing and improving medical LLM safety under adversarial and ambiguous conditions.

</details>


### [73] [Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model](https://arxiv.org/abs/2505.11421)

*Phan Tran Minh Dat, Vo Hoang Nhat Khang, Quan Thanh Tho*

**Main category:** cs.CL

**Keywords:** Bahnaric, Vietnamese, machine translation, transfer learning, data augmentation

**Relevance Score:** 4

**TL;DR:** This work presents a transfer learning approach for Bahnaric-Vietnamese translation using a sequence-to-sequence pre-training language model to address resource scarcity and improve translation accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the cultural gap between the Bahnaric and Vietnamese ethnic groups through better translation capabilities.

**Method:** A sequence-to-sequence model is trained using a pre-trained Vietnamese language model and enhanced with bilingual resources and data augmentation techniques.

**Key Contributions:**

	1. Development of a transfer learning model for Bahnaric-Vietnamese translation
	2. Utilization of a pre-trained Vietnamese language model
	3. Implementation of data augmentation techniques for resource improvement

**Result:** The model effectively handles the translation task despite limited resources, improving accuracy and fostering better understanding between the ethnic groups.

**Limitations:** 

**Conclusion:** The approach successfully contributes to the preservation of languages and enhances cultural exchange by providing a viable translation solution.

**Abstract:** This work explores the journey towards achieving Bahnaric-Vietnamese translation for the sake of culturally bridging the two ethnic groups in Vietnam. However, translating from Bahnaric to Vietnamese also encounters some difficulties. The most prominent challenge is the lack of available original Bahnaric resources source language, including vocabulary, grammar, dialogue patterns and bilingual corpus, which hinders the data collection process for training. To address this, we leverage a transfer learning approach using sequence-to-sequence pre-training language model. First of all, we leverage a pre-trained Vietnamese language model to capture the characteristics of this language. Especially, to further serve the purpose of machine translation, we aim for a sequence-to-sequence model, not encoder-only like BERT or decoder-only like GPT. Taking advantage of significant similarity between the two languages, we continue training the model with the currently limited bilingual resources of Vietnamese-Bahnaric text to perform the transfer learning from language model to machine translation. Thus, this approach can help to handle the problem of imbalanced resources between two languages, while also optimizing the training and computational processes. Additionally, we also enhanced the datasets using data augmentation to generate additional resources and defined some heuristic methods to help the translation more precise. Our approach has been validated to be highly effective for the Bahnaric-Vietnamese translation model, contributing to the expansion and preservation of languages, and facilitating better mutual understanding between the two ethnic people.

</details>


### [74] [When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs](https://arxiv.org/abs/2505.11423)

*Xiaomin Li, Zhou Yu, Zhiwei Zhang, Xupeng Chen, Ziji Zhang, Yingying Zhuang, Narayanan Sadagopan, Anurag Beniwal*

**Main category:** cs.CL

**Keywords:** large language models, chain-of-thought, instruction-following, reasoning strategies, natural language processing

**Relevance Score:** 8

**TL;DR:** This study reveals that explicit chain-of-thought reasoning can degrade instruction-following accuracy in large language models and proposes strategies for mitigation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the adverse effects of reasoning-enhanced prompting methods like chain-of-thought on instruction-following accuracy in large language models.

**Method:** The authors evaluate 15 models on two benchmarks, IFEval and ComplexBench, and conduct large-scale case studies alongside attention-based analysis to examine performance variations with chain-of-thought prompting.

**Key Contributions:**

	1. Systematic identification of performance degradation due to chain-of-thought reasoning in large language models.
	2. Introduction of the metric 'constraint attention' to quantify model focus during language generation.
	3. Evaluation of novel strategies (in-context learning, self-reflection, self-selective reasoning, classifier-selective reasoning) to recover lost performance.

**Result:** The performance of models dropped when using chain-of-thought prompts due to a diversion of attention from instruction-relevant tokens, along with the introduction of unnecessary content. Four mitigation strategies were introduced, notably classifier-selective reasoning, which improved model performance.

**Limitations:** 

**Conclusion:** The study is the first to systematically identify reasoning-induced failures in instruction-following tasks and offers practical strategies to counteract these issues.

**Abstract:** Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies.

</details>


### [75] [GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art](https://arxiv.org/abs/2505.11436)

*Chenkai Zhang, Yiming Lei, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, video comment art, Ripple of Thought

**Relevance Score:** 7

**TL;DR:** The paper introduces GODBench, a novel benchmark for evaluating the creative capabilities of Multimodal Large Language Models (MLLMs) in generating video comment art, and proposes a multi-step reasoning framework called Ripple of Thought (RoT) to enhance MLLM creativity.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing benchmarks in evaluating creativity in video-based Comment Art and to improve the creative generation abilities of MLLMs.

**Method:** The authors introduce a new benchmark, GODBench, which integrates video and text modalities. They also propose Ripple of Thought (RoT), a multi-step reasoning framework to enhance the creativity of MLLMs.

**Key Contributions:**

	1. Introduction of GODBench benchmark for evaluating creative video comment generation
	2. Development of the Ripple of Thought framework to enhance MLLM creativity
	3. Demonstration of the difficulties current MLLMs face in creative expression tasks

**Result:** Extensive experiments show that existing MLLMs and Chain-of-Thought methods struggle with creative tasks in video comment art generation, while RoT shows promise in improving creative outputs.

**Limitations:** 

**Conclusion:** The findings highlight the challenges faced by current MLLMs in creative tasks and suggest that RoT can lead to advancements in MLLM-based creative applications.

**Abstract:** Video Comment Art enhances user engagement by providing creative content that conveys humor, satire, or emotional resonance, requiring a nuanced and comprehensive grasp of cultural and contextual subtleties. Although Multimodal Large Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated strong reasoning abilities in STEM tasks (e.g. mathematics and coding), they still struggle to generate creative expressions such as resonant jokes and insightful satire. Moreover, existing benchmarks are constrained by their limited modalities and insufficient categories, hindering the exploration of comprehensive creativity in video-based Comment Art creation. To address these limitations, we introduce GODBench, a novel benchmark that integrates video and text modalities to systematically evaluate MLLMs' abilities to compose Comment Art. Furthermore, inspired by the propagation patterns of waves in physics, we propose Ripple of Thought (RoT), a multi-step reasoning framework designed to enhance the creativity of MLLMs. Extensive experiments reveal that existing MLLMs and CoT methods still face significant challenges in understanding and generating creative video comments. In contrast, RoT provides an effective approach to improve creative composing, highlighting its potential to drive meaningful advancements in MLLM-based creativity. GODBench is publicly available at https://github.com/stan-lei/GODBench-ACL2025.

</details>


### [76] [Is Compression Really Linear with Code Intelligence?](https://arxiv.org/abs/2505.11441)

*Xianzhen Luo, Shijie Xuyang, Tianhao Cheng, Zheng Chu, Houyi Li, ziqi wang, Siming Huang, Qingfu Zhu, Qiufeng Wang, Xiangyu Zhang, Shuigeng Zhou, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** Large Language Models, code intelligence, data compression, Format Annealing, evaluation framework

**Relevance Score:** 8

**TL;DR:** This paper explores the relationship between data compression and the intelligence of Large Language Models (LLMs) in code intelligence, proposing a new evaluation methodology called Format Annealing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to better understand how data compression correlates with the capabilities of LLMs in specialized domains like code intelligence, challenging previous linear assumptions.

**Method:** The authors introduce Format Annealing, a lightweight training methodology for evaluating open-source Code LLMs across multi-language and multi-task benchmarks.

**Key Contributions:**

	1. Introduction of Format Annealing for fair evaluation of Code LLMs
	2. Identification of a logarithmic relationship between code intelligence and compression
	3. Development of a novel evaluation framework using a large-scale code validation set from GitHub.

**Result:** The study reveals a logarithmic relationship between measured code intelligence and bits-per-character (BPC), refining previous linearity hypotheses.

**Limitations:** The work is still in progress and may require further validation and refinement.

**Conclusion:** This work enhances the understanding of compression in code intelligence development and establishes a robust evaluation framework for code-based LLMs.

**Abstract:** Understanding the relationship between data compression and the capabilities of Large Language Models (LLMs) is crucial, especially in specialized domains like code intelligence. Prior work posited a linear relationship between compression and general intelligence. However, it overlooked the multifaceted nature of code that encompasses diverse programming languages and tasks, and struggled with fair evaluation of modern Code LLMs. We address this by evaluating a diverse array of open-source Code LLMs on comprehensive multi-language, multi-task code benchmarks. To address the challenge of efficient and fair evaluation of pre-trained LLMs' code intelligence, we introduce \textit{Format Annealing}, a lightweight, transparent training methodology designed to assess the intrinsic capabilities of these pre-trained models equitably. Compression efficacy, measured as bits-per-character (BPC), is determined using a novel, large-scale, and previously unseen code validation set derived from GitHub. Our empirical results reveal a fundamental logarithmic relationship between measured code intelligence and BPC. This finding refines prior hypotheses of linearity, which we suggest are likely observations of the logarithmic curve's tail under specific, limited conditions. Our work provides a more nuanced understanding of compression's role in developing code intelligence and contributes a robust evaluation framework in the code domain.

</details>


### [77] [Disentangling Reasoning and Knowledge in Medical Large Language Models](https://arxiv.org/abs/2505.11462)

*Rahul Thapa, Qingyang Wu, Kevin Wu, Harrison Zhang, Angela Zhang, Eric Wu, Haotian Ye, Suhana Bedi, Nevin Aresh, Joseph Boen, Shriya Reddy, Ben Athiwaratkun, Shuaiwen Leon Song, James Zou*

**Main category:** cs.CL

**Keywords:** Medical reasoning, Large language models, Biomedical QA, Reinforcement learning, Clinical case reports

**Relevance Score:** 8

**TL;DR:** This paper analyzes biomedical QA benchmarks to differentiate between reasoning and knowledge in medical reasoning capabilities of LLMs, proposing a new model, BioMed-R1, which shows improved reasoning performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of medical reasoning in LLMs by separating reasoning from factual recall in biomedical QA benchmarks.

**Method:** The authors classify 11 biomedical QA benchmarks into reasoning and knowledge subsets using a PubMedBERT classifier and evaluate various biomedical and general-domain models on these subsets.

**Key Contributions:**

	1. Introduced a classification of biomedical QA benchmarks into reasoning and knowledge subsets.
	2. Developed BioMed-R1, a model specifically tuned for better reasoning performance.
	3. Demonstrated the gaps in performance between biomedical and general-domain models in reasoning tasks.

**Result:** The analysis found that only 32.8% of questions require complex reasoning, with significant discrepancies between knowledge and reasoning performance in evaluated models, highlighting the need for targeted training.

**Limitations:** The study may be limited by the specific models evaluated and the benchmarks used, along with the challenge of achieving robust reasoning under adversarial conditions.

**Conclusion:** BioMed-R1, trained with fine-tuning and reinforcement learning on reasoning-heavy examples, shows the best performance among its peers, suggesting potential for further improvement through specific training methods.

**Abstract:** Medical reasoning in large language models (LLMs) aims to emulate clinicians' diagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and PubMedQA often mix reasoning with factual recall. We address this by separating 11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using a PubMedBERT classifier that reaches 81 percent accuracy, comparable to human performance. Our analysis shows that only 32.8 percent of questions require complex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1) and general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent gaps between knowledge and reasoning performance. For example, m1 scores 60.5 on knowledge but only 47.1 on reasoning. In adversarial tests where models are misled with incorrect initial reasoning, biomedical models degrade sharply, while larger or RL-trained general models show more robustness. To address this, we train BioMed-R1 using fine-tuning and reinforcement learning on reasoning-heavy examples. It achieves the strongest performance among similarly sized models. Further gains may come from incorporating clinical case reports and training with adversarial and backtracking scenarios.

</details>


### [78] [No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies](https://arxiv.org/abs/2505.11470)

*Pascal Wullschleger, Majid Zarharan, Donnacha Daly, Marc Pouly, Jennifer Foster*

**Main category:** cs.CL

**Keywords:** taxonomy evaluation, Natural Language Inference, semantic similarity, robustness, logical adequacy

**Relevance Score:** 4

**TL;DR:** Introduction of two metrics for evaluating taxonomy quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the quality of taxonomies without relying on reference standards, addressing gaps in existing metrics.

**Method:** Developed a robustness metric based on the correlation of semantic and taxonomic similarity, and a logical adequacy metric using Natural Language Inference.

**Key Contributions:**

	1. First metric evaluates robustness through correlation with semantic similarity.
	2. Second metric uses Natural Language Inference for logical adequacy assessment.
	3. Both metrics show strong correlation with established gold-standard evaluations.

**Result:** Both metrics demonstrated a strong correlation with F1 scores compared to gold-standard taxonomies in tests across five different taxonomies.

**Limitations:** Metrics depend on the selection of taxonomies tested, and their applicability to diverse domains needs further exploration.

**Conclusion:** The proposed metrics effectively evaluate taxonomy quality, providing new tools for researchers and practitioners.

**Abstract:** We introduce two reference-free metrics for quality evaluation of taxonomies. The first metric evaluates robustness by calculating the correlation between semantic and taxonomic similarity, covering a type of error not handled by existing metrics. The second uses Natural Language Inference to assess logical adequacy. Both metrics are tested on five taxonomies and are shown to correlate well with F1 against gold-standard taxonomies.

</details>


### [79] [HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages](https://arxiv.org/abs/2505.11475)

*Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Hoo-Chang Shin, Felipe Soares, Alexander Bukharin, Ellie Evans, Yi Dong, Oleksii Kuchaiev*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Language Models, Preference Datasets, HelpSteer3-Preference, Human Feedback

**Relevance Score:** 9

**TL;DR:** This paper introduces HelpSteer3-Preference, a high-quality dataset of over 40,000 human-annotated preference samples for training language models with RLHF, demonstrating significant performance improvements over existing models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a constant need to improve the quality and diversity of preference datasets for training language models using Reinforcement Learning from Human Feedback (RLHF).

**Method:** The authors introduce HelpSteer3-Preference, a dataset consisting of over 40,000 samples annotated by humans, covering various applications of large language models, and utilize it to train Reward Models (RMs).

**Key Contributions:**

	1. Introduction of HelpSteer3-Preference, a permissively licensed, large-scale human-annotated preference dataset.
	2. Demonstration of the dataset's effectiveness in training high-performing Reward Models that outperform previous benchmarks.
	3. Application of the dataset in training Generative RMs and alignment with RLHF.

**Result:** HelpSteer3-Preference allows the training of Reward Models that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%), marking a significant improvement of ~10% over the best-reported existing RMs.

**Limitations:** 

**Conclusion:** HelpSteer3-Preference not only enhances the training of performance-improved Reward Models but also supports the training of Generative RMs and aligns policy models with RLHF.

**Abstract:** Preference datasets are essential for training general-domain, instruction-following language models with Reinforcement Learning from Human Feedback (RLHF). Each subsequent data release raises expectations for future data collection, meaning there is a constant need to advance the quality and diversity of openly available preference data. To address this need, we introduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0), high-quality, human-annotated preference dataset comprising of over 40,000 samples. These samples span diverse real-world applications of large language models (LLMs), including tasks relating to STEM, coding and multilingual scenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This represents a substantial improvement (~10% absolute) over the previously best-reported results from existing RMs. We demonstrate HelpSteer3-Preference can also be applied to train Generative RMs and how policy models can be aligned with RLHF using our RMs. Dataset (CC-BY-4.0): https://huggingface.co/datasets/nvidia/HelpSteer3#preference

</details>


### [80] [Improving Assembly Code Performance with Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.11480)

*Anjiang Wei, Tarun Suresh, Huanmi Tan, Yinglun Xu, Gagandeep Singh, Ke Wang, Alex Aiken*

**Main category:** cs.CL

**Keywords:** large language models, code optimization, reinforcement learning, assembly code, performance benchmarks

**Relevance Score:** 6

**TL;DR:** This paper explores the use of large language models (LLMs) for optimizing assembly code performance through a reinforcement learning framework, achieving significant improvements over existing compiler standards.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The potential of LLMs for code optimization in assembly language remains largely underexplored, particularly in achieving high performance optimizations that are hard to express in high-level programming languages.

**Method:** A reinforcement learning framework utilizing Proximal Policy Optimization (PPO) was developed, with a focus on a reward function that considers functional correctness and execution performance, validated against a substantial benchmark of 8,072 real-world programs.

**Key Contributions:**

	1. Introduction of a reinforcement learning framework for LLM optimization of assembly code.
	2. Creation of a benchmark comprising 8,072 real-world programs for evaluation.
	3. Demonstration of significant performance improvements over standard compiler optimization.

**Result:** The Qwen2.5-Coder-7B-PPO model achieved a 96.0% test pass rate and an average speedup of 1.47x compared to the gcc -O3 compiler baseline, outperforming all 20 models tested, including Claude-3.7-sonnet.

**Limitations:** 

**Conclusion:** This study demonstrates that reinforcement learning can effectively enable LLMs to optimize assembly code performance, showcasing their potential beyond high-level programming tasks.

**Abstract:** Large language models (LLMs) have demonstrated strong performance across a wide range of programming tasks, yet their potential for code optimization remains underexplored. This work investigates whether LLMs can optimize the performance of assembly code, where fine-grained control over execution enables improvements that are difficult to express in high-level languages. We present a reinforcement learning framework that trains LLMs using Proximal Policy Optimization (PPO), guided by a reward function that considers both functional correctness, validated through test cases, and execution performance relative to the industry-standard compiler gcc -O3. To support this study, we introduce a benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO, achieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3 baseline, outperforming all 20 other models evaluated, including Claude-3.7-sonnet. These results indicate that reinforcement learning can unlock the potential of LLMs to serve as effective optimizers for assembly code performance.

</details>


### [81] [SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.11484)

*Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao*

**Main category:** cs.CL

**Keywords:** Test-Time Scaling, SoftCoT++, latent representations

**Relevance Score:** 7

**TL;DR:** SoftCoT++ enhances Test-Time Scaling by enabling diverse exploration of reasoning paths through perturbation of latent thoughts and contrastive learning, outperforming existing methods.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning performance during inference by allowing diverse exploration of thinking paths in continuous latent space.

**Method:** Introduced SoftCoT++ which extends SoftCoT by perturbing latent thoughts with multiple specialized initial tokens and applying contrastive learning.

**Key Contributions:**

	1. Introduction of SoftCoT++
	2. Enhancement of reasoning performance through diverse exploration
	3. Strong compatibility with conventional scaling techniques

**Result:** SoftCoT++ significantly boosts SoftCoT's performance and outperforms it with self-consistency scaling on five reasoning benchmarks with strong compatibility with existing scaling techniques.

**Limitations:** 

**Conclusion:** SoftCoT++ provides a novel approach to continuous-space reasoning that enhances performance while allowing diverse explorations of thought pathways.

**Abstract:** Test-Time Scaling (TTS) refers to approaches that improve reasoning performance by allocating extra computation during inference, without altering the model's parameters. While existing TTS methods operate in a discrete token space by generating more intermediate steps, recent studies in Coconut and SoftCoT have demonstrated that thinking in the continuous latent space can further enhance the reasoning performance. Such latent thoughts encode informative thinking without the information loss associated with autoregressive token generation, sparking increased interest in continuous-space reasoning. Unlike discrete decoding, where repeated sampling enables exploring diverse reasoning paths, latent representations in continuous space are fixed for a given input, which limits diverse exploration, as all decoded paths originate from the same latent thought. To overcome this limitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling paradigm by enabling diverse exploration of thinking paths. Specifically, we perturb latent thoughts via multiple specialized initial tokens and apply contrastive learning to promote diversity among soft thought representations. Experiments across five reasoning benchmarks and two distinct LLM architectures demonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms SoftCoT with self-consistency scaling. Moreover, it shows strong compatibility with conventional scaling techniques such as self-consistency. Source code is available at https://github.com/xuyige/SoftCoT.

</details>


### [82] [Modeling cognitive processes of natural reading with transformer-based Language Models](https://arxiv.org/abs/2505.11485)

*Bruno Bianchi, Fermín Travi, Juan E. Kamienkowski*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Cognitive Processes, Gaze Duration

**Relevance Score:** 7

**TL;DR:** This study evaluates transformer-based language models in relation to gaze duration during reading, finding they outperform earlier models but still do not fully capture human predictability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the relationship between advanced NLP models and cognitive processes in language comprehension, specifically focusing on Gaze Duration during reading.

**Method:** Evaluation of transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) on gaze durations recorded from Rioplantense Spanish readers.

**Key Contributions:**

	1. Demonstrated improved performance of transformer models over traditional models regarding gaze duration predictions.
	2. Highlighted the limitations of transformer models in fully capturing human cognitive predictability during reading.

**Result:** Transformer-based models outperform previous models, showing better predictive capacity for Gaze Durations, but still do not fully explain the variance captured by human predictability.

**Limitations:** Transformer models do not account for all the variance in gaze duration related to human predictability, indicating a gap between model predictions and actual human behavior.

**Conclusion:** State-of-the-art language models, despite their advancements, predict language processing differently from human readers.

**Abstract:** Recent advances in Natural Language Processing (NLP) have led to the development of highly sophisticated language models for text generation. In parallel, neuroscience has increasingly employed these models to explore cognitive processes involved in language comprehension. Previous research has shown that models such as N-grams and LSTM networks can partially account for predictability effects in explaining eye movement behaviors, specifically Gaze Duration, during reading. In this study, we extend these findings by evaluating transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate this relationship. Our results indicate that these architectures outperform earlier models in explaining the variance in Gaze Durations recorded from Rioplantense Spanish readers. However, similar to previous studies, these models still fail to account for the entirety of the variance captured by human predictability. These findings suggest that, despite their advancements, state-of-the-art language models continue to predict language in ways that differ from human readers.

</details>


### [83] [Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator](https://arxiv.org/abs/2305.15099)

*Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, Zhouhan Lin*

**Main category:** cs.CL

**Keywords:** Fourier Transformer, FFT, DCT, transformer models, long-range modeling

**Relevance Score:** 7

**TL;DR:** The Fourier Transformer reduces computational costs in transformer models using the Fast Fourier Transform, allowing weight inheritance from pretrained models, achieving state-of-the-art performances on benchmarks.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies of the transformer model in handling long sequences due to its quadratic complexity, while still enabling the model to inherit weights from large pretrained models.

**Method:** The paper introduces the Fourier Transformer, which uses the Fast Fourier Transform (FFT) to perform Discrete Cosine Transformation (DCT) to remove redundancies in hidden sequences.

**Key Contributions:**

	1. Introduction of the Fourier Transformer to improve efficiency in long-sequence tasks.
	2. Demonstration of state-of-the-art performance on the long-range modeling benchmark LRA.
	3. Ability to inherit from various large pretrained models while utilizing FFT for computational efficiency.

**Result:** The Fourier Transformer significantly reduces computational costs and retains the ability to inherit weights, outperforming standard transformer models, including BART, on various benchmarks.

**Limitations:** 

**Conclusion:** This new approach demonstrates that it is possible to enhance transformer efficiency without sacrificing weight inheritance, achieving better performance on generative tasks.

**Abstract:** The transformer model is known to be computationally demanding, and prohibitively costly for long sequences, as the self-attention module uses a quadratic time and space complexity with respect to sequence length. Many researchers have focused on designing new forms of self-attention or introducing new parameters to overcome this limitation, however a large portion of them prohibits the model to inherit weights from large pretrained models. In this work, the transformer's inefficiency has been taken care of from another perspective. We propose Fourier Transformer, a simple yet effective approach by progressively removing redundancies in hidden sequence using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retain the ability to inherit from various large pretrained models. Experiments show that our model achieves state-of-the-art performances among all transformer-based models on the long-range modeling benchmark LRA with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. Our code is publicly available at https://github.com/LUMIA-Group/FourierTransformer

</details>


### [84] [Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?](https://arxiv.org/abs/2311.07564)

*Cristina Aggazzotti, Nicholas Andrews, Elizabeth Allyn Smith*

**Main category:** cs.CL

**Keywords:** authorship verification, speaker attribution, transcribed speech, conversation transcripts, neural models

**Relevance Score:** 6

**TL;DR:** This paper investigates authorship verification for transcribed speech, presenting a new benchmark and analyzing the effectiveness of various models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the novel challenges of authorship verification in transcribed speech, where traditional stylistic features of written text are less informative.

**Method:** The authors propose a new benchmark for speaker attribution specific to human-transcribed conversational speech transcripts, employing conversation prompts and controlled trials to mitigate speaker-topic associations.

**Key Contributions:**

	1. Introduction of a new benchmark for author verification in conversational speech transcripts.
	2. Insights on the effectiveness of existing written text attribution models in new contexts.
	3. Analysis of the influence of transcription style on model performance.

**Result:** The study establishes state-of-the-art performance on the new benchmark, revealing that while written text attribution models perform adequately, their effectiveness decreases as conversational topic control increases.

**Limitations:** 

**Conclusion:** Fine-tuning on speech transcripts can significantly enhance performance, and the analysis provides insights into how transcription styles impact model efficacy.

**Abstract:** Authorship verification is the task of determining if two distinct writing samples share the same author and is typically concerned with the attribution of written text. In this paper, we explore the attribution of transcribed speech, which poses novel challenges. The main challenge is that many stylistic features, such as punctuation and capitalization, are not informative in this setting. On the other hand, transcribed speech exhibits other patterns, such as filler words and backchannels (e.g., 'um', 'uh-huh'), which may be characteristic of different speakers. We propose a new benchmark for speaker attribution focused on human-transcribed conversational speech transcripts. To limit spurious associations of speakers with topic, we employ both conversation prompts and speakers participating in the same conversation to construct verification trials of varying difficulties. We establish the state of the art on this new benchmark by comparing a suite of neural and non-neural baselines, finding that although written text attribution models achieve surprisingly good performance in certain settings, they perform markedly worse as conversational topic is increasingly controlled. We present analyses of the impact of transcription style on performance as well as the ability of fine-tuning on speech transcripts to improve performance.

</details>


### [85] [COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models](https://arxiv.org/abs/2402.14889)

*Priyanshul Govil, Hemang Jain, Vamshi Krishna Bonagiri, Aman Chadha, Ponnurangam Kumaraguru, Manas Gaur, Sanorita Dey*

**Main category:** cs.CL

**Keywords:** Large Language Models, bias, contextual evaluation, COBIAS, human judgment

**Relevance Score:** 9

**TL;DR:** This paper introduces a new framework for evaluating bias in Large Language Models (LLMs) by considering contextual factors, leading to the development of the Context-Oriented Bias Indicator and Assessment Score (COBIAS).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current bias evaluation methods for LLMs do not adequately consider the context in which biased statements are made, limiting their effectiveness in bias mitigation.

**Method:** The authors developed the COBIAS metric, which evaluates the reliability of biased statements based on model behavior across various contexts, assessed using augmented stereotyped statements.

**Key Contributions:**

	1. Introduction of a contextual reliability framework for LLM bias evaluation.
	2. Development of the COBIAS metric that incorporates context.
	3. Demonstration of COBIAS's alignment with human judgment on bias reliability.

**Result:** The study shows that COBIAS aligns well with human judgments, demonstrating a significant correlation, enabling the creation of more reliable benchmarks for bias assessment.

**Limitations:** The study relies on existing benchmark datasets for bias, which may have their own limitations and biases.

**Conclusion:** COBIAS can enhance bias evaluation by integrating context into assessments and can aid in developing more effective bias mitigation strategies for LLMs.

**Abstract:** Large Language Models (LLMs) often inherit biases from the web data they are trained on, which contains stereotypes and prejudices. Current methods for evaluating and mitigating these biases rely on bias-benchmark datasets. These benchmarks measure bias by observing an LLM's behavior on biased statements. However, these statements lack contextual considerations of the situations they try to present. To address this, we introduce a contextual reliability framework, which evaluates model robustness to biased statements by considering the various contexts in which they may appear. We develop the Context-Oriented Bias Indicator and Assessment Score (COBIAS) to measure a biased statement's reliability in detecting bias, based on the variance in model behavior across different contexts. To evaluate the metric, we augmented 2,291 stereotyped statements from two existing benchmark datasets by adding contextual information. We show that COBIAS aligns with human judgment on the contextual reliability of biased statements (Spearman's $\rho = 0.65, p = 3.4 * 10^{-60}$) and can be used to create reliable benchmarks, which would assist bias mitigation works.

</details>


### [86] [ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images](https://arxiv.org/abs/2404.10652)

*Quan Van Nguyen, Dan Quang Tran, Huy Quang Pham, Thang Kien-Bao Nguyen, Nghia Hieu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen*

**Main category:** cs.CL

**Keywords:** Visual Question Answering, Vietnamese Dataset, Multimodal Learning, OCR, Machine Learning

**Relevance Score:** 7

**TL;DR:** Introduction of the ViTextVQA dataset and the ViTextBLIP-2 method for improving visual question answering in Vietnamese.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Enhance the understanding of scene text in visual question answering models and address gaps in current datasets and methodologies.

**Method:** Introduced a novel method called ViTextBLIP-2 which integrates a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM, along with a trainable Q-Former for multimodal feature fusion.

**Key Contributions:**

	1. First large-scale Vietnamese dataset for VQA focusing on scene text comprehension
	2. Development of ViTextBLIP-2 for multimodal feature fusion in VQA
	3. Experimental findings on token processing order in OCR text influencing performance.

**Result:** Significant performance improvements on the ViTextVQA dataset compared to baseline models.

**Limitations:** 

**Conclusion:** The study highlights the importance of token processing order in OCR text for improving answer formulation in VQA tasks, emphasizing the effectiveness of the ViTextVQA dataset and proposed method.

**Abstract:** Visual Question Answerinng (VQA) is a complicated task that requires the capability of simultaneously processing natural language and images. This task was initially researched with a focus on developing methods to help machines understand objects and scene contexts in images. However, some scene text that carries explicit information about the full content of the image is not mentioned. Along with the continuous development of the AI era, there have been many studies on the reading comprehension ability of VQA models in the world. Therefore, we introduce the first large-scale dataset in Vietnamese specializing in the ability to understand scene text, we call it ViTextVQA (\textbf{Vi}etnamese \textbf{Text}-based \textbf{V}isual \textbf{Q}uestion \textbf{A}nswering dataset) which contains \textbf{over 16,000} images and \textbf{over 50,000} questions with answers. To tackle this task efficiently, we propose ViTextBLIP-2, an novel multimodal feature fusion Method, which optimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal feature fusion. Through experiments with various state-of-the-art models, we uncover the significance of the order in which tokens in OCR text are processed and selected to formulate answers. This finding helped us significantly improve the performance of the baseline models on the ViTextVQA dataset. Our dataset is available (https://github.com/minhquan6203/ViTextVQA-Dataset) for research purposes.

</details>


### [87] [Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation](https://arxiv.org/abs/2405.00715)

*Hanyin Wang, Chufan Gao, Bolun Liu, Qiping Xu, Guleid Hussein, Mohamad El Labban, Kingsley Iheasirim, Hariprasad Korsapati, Chuck Outcalt, Jimeng Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Text Summarization, Healthcare AI

**Relevance Score:** 9

**TL;DR:** A study adapting the open-source LLaMA-2 model for generating clinical notes, achieving quality comparable to physician-authored notes through a novel reinforcement learning approach.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address patient data privacy concerns and computational costs associated with proprietary LLMs in healthcare, focusing on locally-hosted models for clinical text summarization.

**Method:** The adaptation process involved continued pre-training, supervised fine-tuning, and reinforcement learning through a novel approach called DistillDirect, utilizing Gemini 1.0 Pro as the teacher model.

**Key Contributions:**

	1. Development of LLaMA-Clinic for clinical note generation
	2. Novel reinforcement learning approach (DistillDirect) for on-policy training
	3. Empirical validation through physician evaluations displaying high acceptance rates

**Result:** LLaMA-Clinic was developed to generate high-quality clinical notes, achieving a 90.4% acceptance rate in blinded evaluations by physician readers and scoring higher than physician notes in the Assessment and Plan section.

**Limitations:** 

**Conclusion:** The study underscores the importance of pre-defining best-practice note formats for clinical documentation instead of relying solely on LLM output.

**Abstract:** Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have demonstrated promising capabilities in clinical text summarization tasks. However, due to patient data privacy concerns and computational costs, many healthcare providers prefer using small, locally-hosted models over external generic LLMs. This study presents a comprehensive domain- and task-specific adaptation process for the open-source LLaMA-2 13 billion parameter model, enabling it to generate high-quality clinical notes from outpatient patient-doctor dialogues. Our process incorporates continued pre-training, supervised fine-tuning, and reinforcement learning from both AI and human feedback. We introduced a new approach, DistillDirect, for performing on-policy reinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting model, LLaMA-Clinic, can generate clinical notes comparable in quality to those authored by physicians. In a blinded physician reader study, the majority (90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as "acceptable" or higher across all three criteria: real-world readiness, completeness, and accuracy. In the more challenging "Assessment and Plan" section, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than physician-authored notes (4.1/5). We highlight key considerations for future clinical note-generation tasks, emphasizing the importance of pre-defining a best-practice note format, rather than relying on LLMs to determine this for clinical practice.

</details>


### [88] [Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching](https://arxiv.org/abs/2406.06326)

*Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, Helen Meng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Tuning, Knowledge Acquisition, Self-Teaching, LLM Training

**Relevance Score:** 9

**TL;DR:** This paper introduces Self-Tuning, a framework enhancing LLMs' ability to acquire new knowledge through self-teaching, leveraging a set of knowledge-intensive tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address LLMs' struggles with providing up-to-date information and their difficulties in extracting stored knowledge due to one-time training.

**Method:** The Self-Tuning framework incorporates a Self-Teaching strategy that enhances documents with knowledge-intensive tasks focusing on memorization, comprehension, and self-reflection.

**Key Contributions:**

	1. Introduction of the Self-Tuning framework for LLMs
	2. Development of a Self-Teaching strategy with novel tasks
	3. Three new datasets (Wiki-Newpages-2023-QA) for analyzing knowledge acquisition

**Result:** Experimental results show that Self-Tuning significantly improves LLM performance in knowledge acquisition tasks across various models, including Llama2-7B, while preserving previous knowledge.

**Limitations:** 

**Conclusion:** Self-Tuning represents a substantial advancement in enabling LLMs to continually learn from new documents and maintain their prior knowledge effectively.

**Abstract:** Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. To keep LLMs current, existing approaches typically involve continued pre-training on new documents. However, they frequently face difficulties in extracting stored knowledge. Motivated by the remarkable success of the Feynman Technique in efficient human learning, we introduce Self-Tuning, a learning framework aimed at improving an LLM's ability to effectively acquire new knowledge from unseen raw documents through self-teaching. Specifically, we develop a Self-Teaching strategy that augments the documents with a set of knowledge-intensive tasks created in a self-supervised manner, focusing on three crucial aspects: memorization, comprehension, and self-reflection. Additionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an in-depth analysis of an LLM's knowledge acquisition ability concerning memorization, extraction, and reasoning. Extensive experimental results on various models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits superior performance across all knowledge acquisition tasks and excels in preserving previous knowledge.

</details>


### [89] [Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models](https://arxiv.org/abs/2408.06518)

*Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, Noah A. Smith*

**Main category:** cs.CL

**Keywords:** semantic leakage, language models, bias, evaluation, natural language generation

**Relevance Score:** 8

**TL;DR:** This paper introduces the concept of semantic leakage in language models, where irrelevant information from prompts influences generated outputs, and provides a methodology for evaluation and measurement across various models and languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the biases and unintended behaviors of language models, specifically focusing on the newly identified phenomenon of semantic leakage.

**Method:** The authors propose an evaluation setting for detecting semantic leakage both manually and automatically, curate a test suite for diagnostics, and measure semantic leakage across 13 flagship language models in multiple languages.

**Key Contributions:**

	1. Introduction of the concept of semantic leakage in language models
	2. Development of evaluation settings for detecting semantic leakage
	3. Measurement of semantic leakage in 13 flagship models across various languages and scenarios.

**Result:** Significant levels of semantic leakage were detected in multiple models across different settings, revealing that this issue is prevalent beyond English and affects generation patterns.

**Limitations:** The study focuses only on a limited set of language models and may not capture all aspects of semantic leakage in less prominent models.

**Conclusion:** Semantic leakage is a newly identified bias in language models that impacts their behavior and generation, necessitating further study and consideration in their evaluation.

**Abstract:** Despite their wide adoption, the biases and unintended behaviors of language models remain poorly understood. In this paper, we identify and characterize a phenomenon never discussed before, which we call semantic leakage, where models leak irrelevant information from the prompt into the generation in unexpected ways. We propose an evaluation setting to detect semantic leakage both by humans and automatically, curate a diverse test suite for diagnosing this behavior, and measure significant semantic leakage in 13 flagship models. We also show that models exhibit semantic leakage in languages besides English and across different settings and generation scenarios. This discovery highlights yet another type of bias in language models that affects their generation patterns and behavior.

</details>


### [90] [Towards understanding evolution of science through language model series](https://arxiv.org/abs/2409.09636)

*Junjie Dong, Zhuoqi Lyu, Qing Ke*

**Main category:** cs.CL

**Keywords:** AnnualBERT, NLP, temporal evolution, scientific texts, RoBERTa

**Relevance Score:** 7

**TL;DR:** Introduction of AnnualBERT, a series of language models for capturing temporal evolution in scientific texts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing language models in capturing the temporal aspects of scientific discourse and improving performance on domain-specific NLP tasks.

**Method:** AnnualBERT uses whole words as tokens, consisting of a base RoBERTa model pretrained on arXiv papers and progressively trained models on annual data, enhancing representation learning and tracking changes over time.

**Key Contributions:**

	1. Introduction of whole-word tokenization in language models
	2. Demonstration of improved performance in specific NLP tasks
	3. Insights into the temporal evolution of scientific discourse

**Result:** AnnualBERT models show comparable performances in standard NLP tasks and achieve state-of-the-art results in domain-specific tasks and link prediction within the arXiv citation network.

**Limitations:** 

**Conclusion:** AnnualBERT enhances scientific text processing and provides valuable insights into how scientific discourse evolves, with the models available for public use.

**Abstract:** We introduce AnnualBERT, a series of language models designed specifically to capture the temporal evolution of scientific text. Deviating from the prevailing paradigms of subword tokenizations and "one model to rule them all", AnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model pretrained from scratch on the full-text of 1.7 million arXiv papers published until 2008 and a collection of progressively trained models on arXiv papers at an annual basis. We demonstrate the effectiveness of AnnualBERT models by showing that they not only have comparable performances in standard tasks but also achieve state-of-the-art performances on domain-specific NLP tasks as well as link prediction tasks in the arXiv citation network. We then utilize probing tasks to quantify the models' behavior in terms of representation learning and forgetting as time progresses. Our approach enables the pretrained models to not only improve performances on scientific text processing tasks but also to provide insights into the development of scientific discourse over time. The series of the models is available at https://huggingface.co/jd445/AnnualBERTs.

</details>


### [91] [Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach](https://arxiv.org/abs/2409.20204)

*Aditi Dutta, Susan Banducci, Chico Q. Camargo*

**Main category:** cs.CL

**Keywords:** sexism, misogyny, automated detection, interdisciplinary research, systematic review

**Relevance Score:** 6

**TL;DR:** The paper conducts a systematic literature review on computational tools for detecting sexism and misogyny in online spaces, providing insights into the current state of research and key gaps.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To capture the current state and trajectory of the evolving field of sexism and misogyny detection, addressing the increasing concern over gender-based discrimination online.

**Method:** A systematic literature review synthesizing existing research into five core themes, using a semi-automated review process guided by PRISMA.

**Key Contributions:**

	1. Synthesis of literature into five themes: definitions, disciplinary divergences, detection methods, challenges, and interventions.
	2. Interdisciplinary approach to bridge methodological divides.
	3. Identification of critical gaps in the field, including the need for intersectional and non-Western perspectives.

**Result:** Identified five key themes in the literature and highlighted significant gaps, such as the need for intersectional approaches and proactive design strategies.

**Limitations:** Limited focus on proactive design strategies beyond text classification and under-representation of non-Western perspectives.

**Conclusion:** Current methodologies reveal a clear disciplinary divide regarding the conceptualization and measurement of sexism and misogyny, necessitating further interdisciplinary collaboration.

**Abstract:** Several computational tools have been developed to detect and identify sexism, misogyny, and gender-based hate speech, particularly on online platforms. These tools draw on insights from both social science and computer science. Given the increasing concern over gender-based discrimination in digital spaces, the contested definitions and measurements of sexism, and the rise of interdisciplinary efforts to understand its online manifestations, a systematic literature review is essential for capturing the current state and trajectory of this evolving field. In this review, we make four key contributions: (1) we synthesize the literature into five core themes: definitions of sexism and misogyny, disciplinary divergences, automated detection methods, associated challenges, and design-based interventions; (2) we adopt an interdisciplinary lens, bridging theoretical and methodological divides across disciplines; (3) we highlight critical gaps, including the need for intersectional approaches, the under-representation of non-Western languages and perspectives, and the limited focus on proactive design strategies beyond text classification; and (4) we offer a methodological contribution by applying a rigorous semi-automated systematic review process guided by PRISMA, establishing a replicable standard for future work in this domain. Our findings reveal a clear disciplinary divide in how sexism and misogyny are conceptualized and measured. Through an evidence-based synthesis, we examine how existing studies have attempted to bridge this gap through interdisciplinary collaboration. Drawing on both social science theories and computational modeling practices, we assess the strengths and limitations of current methodologies. Finally, we outline key challenges and future directions for advancing research on the detection and mitigation of online sexism and misogyny.

</details>


### [92] [Training of Scaffolded Language Models with Language Supervision: A Survey](https://arxiv.org/abs/2410.16392)

*Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Victor Shea Jay Huang, Huan Liu, Jun Liu, Gao Huang, Yong-Jin Liu*

**Main category:** cs.CL

**Keywords:** scaffolded language models, language supervision, streaming learning, optimization, mixed-autonomy

**Relevance Score:** 8

**TL;DR:** This survey focuses on the design and optimization of scaffolded language models (LMs) that integrate into multi-step processes, exploring their training with language supervision and the implications for real-world applications.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To organize the literature on scaffolded LMs which utilize language for non-parametric training and optimize processes in multi-step settings with tools.

**Method:** The paper surveys existing research on scaffolded LMs and their training paradigms, particularly how they handle language feedback in real-time applications.

**Key Contributions:**

	1. Introduction of the concept of scaffolded LMs.
	2. Discussion on streaming learning from language supervision in mixed-autonomy settings.
	3. Insights into the optimization of LMs using language feedback.

**Result:** The survey reveals how scaffolded LMs optimize language-based supervision and continuously improve through real-time user feedback, mitigating issues like catastrophic forgetting.

**Limitations:** 

**Conclusion:** The findings emphasize the importance of language-based optimization in improving scaffolded LMs for practical applications, particularly in collaborative environments with mixed autonomy.

**Abstract:** This survey organizes the intricate literature on the design and optimization of emerging structures around post-trained LMs. We refer to this overarching structure as scaffolded LMs and focus on LMs that are integrated into multi-step processes with tools. We view scaffolded LMs as semi-parametric models wherein we train non-parametric variables, including the prompt, tools, and scaffold's code. In particular, they interpret instructions, use tools, and receive feedback all in language. Recent works use an LM as an optimizer to interpret language supervision and update non-parametric variables according to intricate objectives. In this survey, we refer to this paradigm as training of scaffolded LMs with language supervision. A key feature of non-parametric training is the ability to learn from language. Parametric training excels in learning from demonstration (supervised learning), exploration (reinforcement learning), or observations (unsupervised learning), using well-defined loss functions. Language-based optimization enables rich, interpretable, and expressive objectives, while mitigating issues like catastrophic forgetting and supporting compatibility with closed-source models. Furthermore, agents are increasingly deployed as co-workers in real-world applications such as Copilot in Office tools or software development. In these mixed-autonomy settings, where control and decision-making are shared between human and AI, users point out errors or suggest corrections. Accordingly, we discuss agents that continuously improve by learning from this real-time, language-based feedback and refer to this setting as streaming learning from language supervision.

</details>


### [93] [ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework](https://arxiv.org/abs/2410.19453)

*Hengyuan Zhang, Chenming Shang, Sizhe Wang, Dongdong Zhang, Feng Yao, Renliang Sun, Yiyao Yu, Yujiu Yang, Furu Wei*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilingual Learning, Contrastive Learning, Health Informatics, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** Proposes ShifCon, a framework to enhance multilingual LLM performance by improving non-dominant language representations through a shift-based contrastive approach.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gap in LLMs between dominant languages (like English) and non-dominant languages due to uneven training data.

**Method:** Introduces ShifCon, which shifts non-dominant language representations into a dominant language subspace for enhancement, coupled with multilingual contrastive learning for better alignment.

**Key Contributions:**

	1. Introduction of ShifCon framework for language representation enhancement
	2. Development of subspace distance metric for optimal representation shifting
	3. Application of multilingual contrastive learning to improve representation alignment

**Result:** Experiments show ShifCon significantly improves performance for non-dominant languages, especially low-resource ones.

**Limitations:** 

**Conclusion:** ShifCon enables enhanced representation access and insights that can drive further research in multilingual LLMs.

**Abstract:** Although fine-tuning Large Language Models (LLMs) with multilingual data can rapidly enhance the multilingual capabilities of LLMs, they still exhibit a performance gap between the dominant language (e.g., English) and non-dominant ones due to the imbalance of training data across languages. To further enhance the performance of non-dominant languages, we propose ShifCon, a Shift-based Contrastive framework that aligns the internal forward process of other languages toward that of the dominant one. Specifically, it shifts the representations of non-dominant languages into the dominant language subspace, allowing them to access relatively rich information encoded in the model parameters. The enriched representations are then shifted back into their original language subspace before generation. Moreover, we introduce a subspace distance metric to pinpoint the optimal layer area for shifting representations and employ multilingual contrastive learning to further enhance the alignment of representations within this area. Experiments demonstrate that our ShifCon framework significantly enhances the performance of non-dominant languages, particularly for low-resource ones. Further analysis offers extra insights to verify the effectiveness of ShifCon and propel future research

</details>


### [94] [How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP](https://arxiv.org/abs/2411.05527)

*Kushal Tatariya, Artur Kulmizev, Wessel Poelman, Esther Ploeger, Marcel Bollmann, Johannes Bjerva, Jiaming Luo, Heather Lent, Miryam de Lhoneux*

**Main category:** cs.CL

**Keywords:** Wikipedia, Data Quality, Multilingual NLP, Low-resource languages, Quality Filtering

**Relevance Score:** 7

**TL;DR:** The paper analyzes the data quality of Wikipedia in low-resource languages, identifying issues like one-line and duplicate articles, and advocates for language-specific definitions of data quality to improve NLP outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To critique the quality of Wikipedia as a resource for NLP in non-English languages, especially in low-resource settings.

**Method:** Various quality filtering techniques were applied to Wikipedia data to evaluate its quality in a non-English context.

**Key Contributions:**

	1. Identified widespread data quality issues in Wikipedia for low-resource languages.
	2. Demonstrated the effectiveness of quality filtering for improving NLP training efficiency.
	3. Advocated for language-specific data quality definitions.

**Result:** The study reveals significant issues with data quality in Wikipedia, including many one-line and duplicate articles, and demonstrates that quality pruning can enhance resource-efficient training without harming performance.

**Limitations:** The study focuses primarily on low-resource languages and may not generalize to high-resource contexts.

**Conclusion:** The research encourages a shift towards language- and task-specific definitions of data quality, providing guidelines for using Wikipedia in multilingual pretraining.

**Abstract:** Wikipedia's perceived high quality and broad language coverage have established it as a fundamental resource in multilingual NLP. In the context of low-resource languages, however, these quality assumptions are increasingly being scrutinised. This paper critically examines the data quality of Wikipedia in a non-English setting by subjecting it to various quality filtering techniques, revealing widespread issues such as a high percentage of one-line articles and duplicate articles. We evaluate the downstream impact of quality filtering on Wikipedia and find that data quality pruning is an effective means for resource-efficient training without hurting performance, especially for low-resource languages. Moreover, we advocate for a shift in perspective from seeking a general definition of data quality towards a more language- and task-specific one. Ultimately, we aim for this study to serve as a guide to using Wikipedia for pretraining in a multilingual setting.

</details>


### [95] [UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction](https://arxiv.org/abs/2411.07019)

*Zhiqiang Liu, Yin Hua, Mingyang Chen, Zhuo Chen, Ziqi Liu, Lei Liang, Huajun Chen, Wen Zhang*

**Main category:** cs.CL

**Keywords:** knowledge graphs, link prediction, hierarchical representation, machine learning, temporal facts

**Relevance Score:** 6

**TL;DR:** The paper presents a Unified Hierarchical Representation learning framework (UniHR) to improve link prediction in complex knowledge graphs, integrating various fact representations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for effective models that can generalize across different complex fact representations such as hyper-relational, temporal, and nested facts due to their increasing complexity.

**Method:** The UniHR framework includes a Hierarchical Data Representation (HiDR) module that transforms various complex knowledge graph forms into triple-based representations, and a Hierarchical Structure Learning (HiSL) module for intra-fact and inter-fact message passing to enhance semantic and structural information.

**Key Contributions:**

	1. Introduction of a unified framework for link prediction in complex knowledge graphs.
	2. Development of modules for effective fact representation and message passing.
	3. Empirical validation of the framework's performance across different fact types.

**Result:** Empirical results show that the UniHR framework effectively captures the complexities of various fact types in knowledge graphs, demonstrating improved link prediction performance compared to existing models.

**Limitations:** The study may be limited by the data types tested and the specific configurations of the models; further studies could explore additional fact types and applications.

**Conclusion:** The study highlights the potential of unified representations in advancing the current state of link prediction for complex knowledge graphs and provides accessible resources for replication.

**Abstract:** Beyond-triple fact representations including hyper-relational facts with auxiliary key-value pairs, temporal facts with additional timestamps, and nested facts implying relationships between facts, are gaining significant attention. However, constrained by complex fact representation forms, existing link prediction models for beyond-triple facts have difficulty achieving hierarchical fact modeling and generalizing the modules for one specific facts to other fact types. To overcome this limitation, we propose a Unified Hierarchical Representation learning framework (UniHR) for unified knowledge graph link prediction. It consists of a unified Hierarchical Data Representation (HiDR) module and a unified Hierarchical Structure Learning (HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs, temporal KGs, and nested factual KGs into triple-based representations. Then HiSL incorporates intra-fact and inter-fact message passing, focusing on enhancing the semantic information within individual facts and enriching the structural information between facts. Empirical results demonstrate the effectiveness of UniHR and highlight the strong potential of unified representations. Code and data are available at https://github.com/Lza12a/UniHR.

</details>


### [96] [On the Role of Speech Data in Reducing Toxicity Detection Bias](https://arxiv.org/abs/2411.08135)

*Samuel J. Bell, Mariano Coria Meglioli, Megan Richards, Eduardo Sánchez, Christophe Ropers, Skyler Wang, Adina Williams, Levent Sagun, Marta R. Costa-jussà*

**Main category:** cs.CL

**Keywords:** toxicity detection, bias in AI, speech classification, MuTox dataset, natural language processing

**Relevance Score:** 7

**TL;DR:** Investigates biases in toxicity detection in speech vs. text, finding that speech classifiers reduce biases for group mentions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how speech-based toxicity detection mitigates biases present in text-based systems.

**Method:** The study involved creating high-quality annotations for the multilingual MuTox dataset and comparing speech and text-based toxicity classifiers.

**Key Contributions:**

	1. Developed high-quality group annotations for the MuTox dataset.
	2. Demonstrated that speech classifiers reduce text-based biases in toxicity detection.
	3. Provided recommendations for future toxicity dataset construction.

**Result:** Speech-based classifiers showed reduced bias against demographic group mentions, especially in ambiguous cases, unlike text-based classifiers.

**Limitations:** 

**Conclusion:** Enhancing classifiers is more effective for reducing group bias than improving transcription methods; annotations are publicly available.

**Abstract:** Text toxicity detection systems exhibit significant biases, producing disproportionate rates of false positives on samples mentioning demographic groups. But what about toxicity detection in speech? To investigate the extent to which text-based biases are mitigated by speech-based systems, we produce a set of high-quality group annotations for the multilingual MuTox dataset, and then leverage these annotations to systematically compare speech- and text-based toxicity classifiers. Our findings indicate that access to speech data during inference supports reduced bias against group mentions, particularly for ambiguous and disagreement-inducing samples. Our results also suggest that improving classifiers, rather than transcription pipelines, is more helpful for reducing group bias. We publicly release our annotations and provide recommendations for future toxicity dataset construction.

</details>


### [97] [When to Speak, When to Abstain: Contrastive Decoding with Abstention](https://arxiv.org/abs/2412.12527)

*Hyuhng Joon Kim, Youna Kim, Sang-goo Lee, Taeuk Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, Contrastive Decoding, Abstention, Knowledge Access, User Trust

**Relevance Score:** 9

**TL;DR:** This paper presents Contrastive Decoding with Abstention (CDA), a method for Large Language Models (LLMs) that improves their ability to generate responses when relevant knowledge is available and to abstain when it is not, enhancing reliability and user trust.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of conventional LLMs in scenarios where relevant information is lacking, thereby improving robustness.

**Method:** Introduction of Contrastive Decoding with Abstention (CDA), a training-free decoding technique that assesses the relevance of both parametric and contextual knowledge sources and adaptively decides which to use.

**Key Contributions:**

	1. Introduction of a controlled testbed for assessing LLM knowledge access scenarios
	2. Proposition of a new method (CDA) for LLMs to handle knowledge limitation effectively
	3. Demonstration of improved reliability in model outputs through abstention

**Result:** CDA demonstrates a significant improvement in generating accurate responses while allowing for abstention when necessary, thus enhancing the reliability of LLM outputs.

**Limitations:** 

**Conclusion:** CDA not only improves LLM performance in knowledge-scarce situations but also preserves user trust by managing response generation appropriately.

**Abstract:** Large Language Models (LLMs) demonstrate exceptional performance across diverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e., contextual) knowledge. While substantial efforts have been made to enhance the utilization of both forms of knowledge, situations in which models lack relevant information remain underexplored. To investigate this challenge, we first present a controlled testbed featuring four distinct knowledge access scenarios, including the aforementioned edge case, revealing that conventional LLM usage exhibits insufficient robustness in handling all instances. Addressing this limitation, we propose Contrastive Decoding with Abstention (CDA), a novel training-free decoding method that allows LLMs to generate responses when relevant knowledge is available and to abstain otherwise. CDA estimates the relevance of both knowledge sources for a given input, adaptively deciding which type of information to prioritize and which to exclude. Through extensive experiments, we demonstrate that CDA can effectively perform accurate generation and abstention simultaneously, enhancing reliability and preserving user trust.

</details>


### [98] [What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context](https://arxiv.org/abs/2412.12632)

*Zhiyuan Chang, Mingyang Li, Xiaojun Jia, Junjie Wang, Yuekai Huang, Qing Wang, Yihao Huang, Yang Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-hop Question Answering, Chain of Evidence, Retrieval-Augmented Generation, Knowledge Management

**Relevance Score:** 9

**TL;DR:** This paper explores an automated approach inspired by the Chain of Evidence (CoE) to enhance the reliability of large language models (LLMs) in multi-hop question answering by distinguishing relevant from irrelevant external knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Incorporating external knowledge into LLMs can reduce outdated knowledge and hallucinations but often includes irrelevant or misleading information.

**Method:** The paper proposes an automated CoE discrimination approach to evaluate LLMs' performance with respect to generating accurate and faithful responses, specifically in multi-hop QA scenarios.

**Key Contributions:**

	1. Proposed an automated CoE discrimination approach for LLMs.
	2. Demonstrated improvements in accuracy, faithfulness, and robustness with CoE.
	3. Provided insights into applying CoE in practical RAG contexts.

**Result:** Tests on five LLMs indicate that CoE enhances generation accuracy, strengthens answer faithfulness, and improves robustness against knowledge conflicts.

**Limitations:** The approach relies on the quality of external knowledge and may not fully overcome all issues related to misinformation.

**Conclusion:** Utilizing CoE leads to overall better performance of LLMs in retrieval-augmented generation (RAG) scenarios by improving response quality.

**Abstract:** Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs. However, external knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses. This paper focuses on LLMs' preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law's Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and evaluate LLMs' effectiveness, faithfulness and robustness with CoE, including its application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs show CoE improves generation accuracy, answer faithfulness, robustness to knowledge conflicts, and boosts the performance of existing approaches in three practical RAG scenarios.

</details>


### [99] [XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation](https://arxiv.org/abs/2412.15529)

*Qianren Mao, Yangyifei Luo, Qili Zhang, Yashuo Luo, Zhilong Cao, Jinlong Zhang, HanWen Hao, Zhijun Chen, Weifeng Jiang, Junnan Liu, Xiaolong Wang, Zhenting Huang, Zhixing Tan, Sun Jie, Bo Li, Xudong Liu, Richong Zhang, Jianxin Li*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Large Language Models, Performance evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces XRAG, an open-source codebase for evaluating foundational components of retrieval-augmented generation (RAG) systems, focusing on performance benchmarks and addressing potential failure points.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of retrieval-augmented generation (RAG) systems by evaluating core components and identifying critical failure points.

**Method:** The paper categorizes the components of RAG into four phases: pre-retrieval, retrieval, post-retrieval, and generation, and evaluates their performance over various datasets.

**Key Contributions:**

	1. Introduction of XRAG as an open-source codebase for RAG evaluation
	2. Systematic categorization and performance benchmarking of RAG components
	3. Development of experimental methodologies for diagnosing RAG failures

**Result:** The analysis provides a comprehensive benchmark for the effectiveness of advanced RAG components and outlines bespoke solutions to enhance their performance.

**Limitations:** 

**Conclusion:** The research highlights the importance of diagnosing failure points in RAG systems and offers insights for optimizations in response to these challenges.

**Abstract:** Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent data with the generative capabilities of Large Language Models (LLMs), ensuring that the generated output is not only contextually relevant but also accurate and current. We introduce XRAG, an open-source, modular codebase that facilitates exhaustive evaluation of the performance of foundational components of advanced RAG modules. These components are systematically categorized into four core phases: pre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse them across reconfigured datasets, providing a comprehensive benchmark for their effectiveness. As the complexity of RAG systems continues to escalate, we underscore the critical need to identify potential failure points in RAG systems. We formulate a suite of experimental methodologies and diagnostic testing protocols to dissect the failure points inherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed at bolstering the overall performance of these modules. Our work thoroughly evaluates the performance of advanced core components in RAG systems, providing insights into optimizations for prevalent failure points.

</details>


### [100] [Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines](https://arxiv.org/abs/2501.00745)

*Xiyang Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, ranking manipulation, information retrieval, game theory, security strategies

**Relevance Score:** 9

**TL;DR:** This paper investigates ranking manipulation attacks on LLM-based search engines using the Infinitely Repeated Prisoners' Dilemma framework, revealing conditions for sustaining cooperation among attackers and the paradoxical effects of defensive strategies.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the vulnerabilities of LLM-based search engines to ranking manipulation attacks and propose methods for mitigating these threats.

**Method:** Analyzed the dynamics of ranking manipulation attacks through a game-theoretic approach using the Infinitely Repeated Prisoners' Dilemma to identify conditions for cooperation among players.

**Key Contributions:**

	1. Theoretical framework for understanding ranking manipulation in LLMs
	2. Identification of conditions supporting cooperation in adversarial settings
	3. Insights into the paradoxical effects of certain defensive strategies

**Result:** Identified key factors influencing cooperation, including attack costs and success rates, and demonstrated paradoxical outcomes where reducing attack success rates could incentivize more attacks.

**Limitations:** Focuses on a theoretical analysis and does not provide empirical validation of findings.

**Conclusion:** Adaptive security strategies and careful design of ecosystems are crucial for mitigating vulnerabilities in LLM-based systems, as simplistic defensive measures may be ineffective.

**Abstract:** The increasing integration of Large Language Model (LLM) based search engines has transformed the landscape of information retrieval. However, these systems are vulnerable to adversarial attacks, especially ranking manipulation attacks, where attackers craft webpage content to manipulate the LLM's ranking and promote specific content, gaining an unfair advantage over competitors. In this paper, we study the dynamics of ranking manipulation attacks. We frame this problem as an Infinitely Repeated Prisoners' Dilemma, where multiple players strategically decide whether to cooperate or attack. We analyze the conditions under which cooperation can be sustained, identifying key factors such as attack costs, discount rates, attack success rates, and trigger strategies that influence player behavior. We identify tipping points in the system dynamics, demonstrating that cooperation is more likely to be sustained when players are forward-looking. However, from a defense perspective, we find that simply reducing attack success probabilities can, paradoxically, incentivize attacks under certain conditions. Furthermore, defensive measures to cap the upper bound of attack success rates may prove futile in some scenarios. These insights highlight the complexity of securing LLM-based systems. Our work provides a theoretical foundation and practical insights for understanding and mitigating their vulnerabilities, while emphasizing the importance of adaptive security strategies and thoughtful ecosystem design.

</details>


### [101] [LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena](https://arxiv.org/abs/2501.03266)

*Stefan Pasch*

**Main category:** cs.CL

**Keywords:** LLM, content moderation, user satisfaction, ethical alignment, refusal classifier

**Relevance Score:** 8

**TL;DR:** The paper examines how content moderation affects user satisfaction in LLMs, specifically focusing on user reactions to refusals based on ethical versus technical concerns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding user responses to LLM refusals, particularly those based on ethical considerations.

**Method:** Analyzed nearly 50,000 model comparisons from Chatbot Arena to study user preferences and employed a RoBERTa-based refusal classifier to differentiate between types of refusals.

**Key Contributions:**

	1. Large-scale analysis of user preferences in LLM responses
	2. Development of a RoBERTa-based classifier for refusal types
	3. Insights into user satisfaction versus ethical considerations in LLM design

**Result:** Ethical refusals led to lower win rates compared to technical refusals and standard responses. However, sensitive prompts received more favorable evaluations when refusals were contextually appropriate.

**Limitations:** 

**Conclusion:** The findings highlight a conflict between safety-aligned LLM behaviors and user expectations, suggesting the need for adaptive moderation strategies.

**Abstract:** LLM safety and ethical alignment are widely discussed, but the impact of content moderation on user satisfaction remains underexplored. In particular, little is known about how users respond when models refuse to answer a prompt-one of the primary mechanisms used to enforce ethical boundaries in LLMs. We address this gap by analyzing nearly 50,000 model comparisons from Chatbot Arena, a platform where users indicate their preferred LLM response in pairwise matchups, providing a large-scale setting for studying real-world user preferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a hand-labeled dataset, we distinguish between refusals due to ethical concerns and technical limitations. Our results reveal a substantial refusal penalty: ethical refusals yield significantly lower win rates than both technical refusals and standard responses, indicating that users are especially dissatisfied when models decline a task for ethical reasons. However, this penalty is not uniform. Refusals receive more favorable evaluations when the underlying prompt is highly sensitive (e.g., involving illegal content), and when the refusal is phrased in a detailed and contextually aligned manner. These findings underscore a core tension in LLM design: safety-aligned behaviors may conflict with user expectations, calling for more adaptive moderation strategies that account for context and presentation.

</details>


### [102] [TreeKV: Smooth Key-Value Cache Compression with Tree Structures](https://arxiv.org/abs/2501.04987)

*Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang*

**Main category:** cs.CL

**Keywords:** Key-value cache, Transformer models, Language models, Cache compression, Long sequences

**Relevance Score:** 9

**TL;DR:** TreeKV is a new method for efficient caching in transformer-based LLMs that enhances context retention while allowing for significant cache size reduction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Efficient KV cache compression is necessary for improving performance of transformer-based LLMs in resource-limited settings, especially for long sequences.

**Method:** TreeKV uses a tree structure for smooth cache compression, avoiding position-based and global importance biases. It operates in both generation and prefilling stages without requiring training.

**Key Contributions:**

	1. Introduction of TreeKV, a training-free cache compression method
	2. Demonstrated significant performance improvement in long-context LLM scenarios
	3. Achieved a 16x reduction in cache size with maintained output quality

**Result:** TreeKV outperforms existing models in language modeling tasks (PG19, OpenWebText2) and successfully reduces cache size by 16x while maintaining high-quality outputs.

**Limitations:** 

**Conclusion:** TreeKV achieves superior efficiency, with optimal performance on the Longbench benchmark at just 6% of the budget.

**Abstract:** Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.

</details>


### [103] [Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling](https://arxiv.org/abs/2501.10316)

*Suvodip Dey, Yi-Jyun Sun, Gokhan Tur, Dilek Hakkani-Tur*

**Main category:** cs.CL

**Keywords:** Large Language Models, Dialogue State Tracking, User Overreliance, Accountability Model, Error Correction

**Relevance Score:** 9

**TL;DR:** This paper presents an accountability model for LLM-based dialogue agents to reduce user overreliance on AI by introducing friction turns that help in managing model uncertainty and errors in dialogue state tracking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses user overreliance on LLM-based conversational agents that can hallucinate, producing false yet plausible responses. It aims to improve decision-making systems by adding positive friction through user confirmations and explanations.

**Method:** The authors propose an augmented LLM with an accountability head that serves as a binary classifier for predicting relevant dialogue state slots, tested on two benchmarks (MultiWOZ and Snips).

**Key Contributions:**

	1. Development of an accountability model for LLM-based dialogue agents
	2. Demonstration of a significant accuracy improvement in dialogue state tracking
	3. Identification of user confirmation as a viable method to mitigate overreliance.

**Result:** The accountability model leads to a 3% improvement in joint goal accuracy (JGA) of dialogue state tracking, with self-correction of errors further increasing JGA from 67.13 to 70.51, achieving state-of-the-art results.

**Limitations:** 

**Conclusion:** Incorporating user confirmations as a friction turn results in significant performance gains in dialogue state tracking while reducing overreliance on AI agents.

**Abstract:** Recent LLMs have enabled significant advancements for conversational agents. However, they are also well known to hallucinate, producing responses that seem plausible but are factually incorrect. On the other hand, users tend to over-rely on LLM-based AI agents, accepting AI's suggestion even when it is wrong. Adding positive friction, such as explanations or getting user confirmations, has been proposed as a mitigation in AI-supported decision-making systems. In this paper, we propose an accountability model for LLM-based task-oriented dialogue agents to address user overreliance via friction turns in cases of model uncertainty and errors associated with dialogue state tracking (DST). The accountability model is an augmented LLM with an additional accountability head that functions as a binary classifier to predict the relevant slots of the dialogue state mentioned in the conversation. We perform our experiments with multiple backbone LLMs on two established benchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the proposed approach not only enables reliable estimation of AI agent errors but also guides the decoder in generating more accurate actions. We observe around 3% absolute improvement in joint goal accuracy (JGA) of DST output by incorporating accountability heads into modern LLMs. Self-correcting the detected errors further increases the JGA from 67.13 to 70.51, achieving state-of-the-art DST performance. Finally, we show that error correction through user confirmations (friction turn) achieves a similar performance gain, highlighting its potential to reduce user overreliance.

</details>


### [104] [Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine](https://arxiv.org/abs/2501.11885)

*Keer Lu, Zheng Liang, Zhuoran Zhang, Da Pan, Shusen Zhang, Xin Wu, Zenan Zhou, Guosheng Dong, Bin Cui, Tengjiao Wang, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evidence-Based Medicine, Healthcare, Retrieval-Augmented Generation, Medical Expertise

**Relevance Score:** 9

**TL;DR:** Med-R^2 is a novel LLM framework for enhancing healthcare applications by integrating retrieval mechanisms with evidence-based reasoning, significantly improving performance without high costs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the application of LLMs in medical settings, addressing challenges of expensive training datasets and limitations in existing retrieval and extraction strategies.

**Method:** The Med-R^2 framework integrates the Evidence-Based Medicine (EBM) process with advanced retrieval mechanisms and reasoning strategies to enhance the capabilities of LLMs in clinical contexts.

**Key Contributions:**

	1. Introduction of Med-R^2, a novel LLM physician framework
	2. Integration of Evidence-Based Medicine process with LLMs
	3. Significant performance improvement in healthcare scenarios without increased training costs

**Result:** Med-R^2 demonstrates a 14.74% improvement over standard Retrieval-Augmented Generation (RAG) methods and a 3.32% improvement compared to fine-tuning methods, achieving these gains without additional training costs.

**Limitations:** 

**Conclusion:** Med-R^2 effectively enhances LLM performance in healthcare by providing reliable access to external knowledge and improved reasoning, fostering trustworthy LLM applications in medicine.

**Abstract:** Large Language Models (LLMs) have exhibited remarkable capabilities in clinical scenarios. Despite their potential, existing works face challenges when applying LLMs to medical settings. Strategies relying on training with medical datasets are highly cost-intensive and may suffer from outdated training data. Leveraging external knowledge bases is a suitable alternative, yet it faces obstacles such as limited retrieval precision and poor effectiveness in answer extraction. These issues collectively prevent LLMs from demonstrating the expected level of proficiency in mastering medical expertise. To address these challenges, we introduce Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as the selection and reasoning processes of evidence, thereby enhancing the problem-solving capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2 achieves a 14.74\% improvement over vanilla RAG methods and even a 3.32\% enhancement compared to fine-tuning strategies, without incurring additional training costs.

</details>


### [105] [Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms](https://arxiv.org/abs/2501.13977)

*Rajvardhan Oak, Muhammad Haroon, Claire Jo, Magdalena Wojcieszak, Anshuman Chhabra*

**Main category:** cs.CL

**Keywords:** Machine Learning, AI, Content Moderation, Large Language Models, Harm Mitigation

**Relevance Score:** 9

**TL;DR:** This paper presents a novel re-ranking method using Large Language Models to mitigate harmful content exposure on social media platforms, overcoming limitations of traditional moderation approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the inadequacies of current social media content moderation frameworks, which struggle with scalability and adaptability in identifying harmful content due to reliance on extensive human-annotated datasets.

**Method:** A re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings to dynamically assess and re-rank content sequences for reduced harmful exposure.

**Key Contributions:**

	1. Introduction of a novel re-ranking approach using LLMs
	2. Development of new metrics for assessing the effectiveness of harm mitigation
	3. Demonstration of superior performance over existing moderation systems.

**Result:** Experiments show that the LLM-based re-ranking method significantly outperforms existing proprietary moderation techniques, indicating its superiority and scalability.

**Limitations:** The approach may still face challenges in entirely eliminating harmful content and requires further validation in real-world settings.

**Conclusion:** The proposed method offers an effective way to enhance content moderation on social media without the need for large volumes of labeled training data, showcasing adaptability to new types of harmful content.

**Abstract:** Social media platforms utilize Machine Learning (ML) and Artificial Intelligence (AI) powered recommendation algorithms to maximize user engagement, which can result in inadvertent exposure to harmful content. Current moderation efforts, reliant on classifiers trained with extensive human-annotated data, struggle with scalability and adapting to new forms of harm. To address these challenges, we propose a novel re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings. Our method dynamically assesses and re-ranks content sequences, effectively mitigating harmful content exposure without requiring extensive labeled data. Alongside traditional ranking metrics, we also introduce two new metrics to evaluate the effectiveness of re-ranking in reducing exposure to harmful content. Through experiments on three datasets, three models and across three configurations, we demonstrate that our LLM-based approach significantly outperforms existing proprietary moderation approaches, offering a scalable and adaptable solution for harm mitigation.

</details>


### [106] [Do we really have to filter out random noise in pre-training data for language models?](https://arxiv.org/abs/2502.06604)

*Jinghan Ru, Yuxin Xie, Xianwei Zhuang, Yuguo Yin, Zhihui Guo, Zhiming Liu, Qianli Ren, Yuexian Zou*

**Main category:** cs.CL

**Keywords:** random noise, LLM, Local Gradient Matching, downstream performance, data quality

**Relevance Score:** 9

**TL;DR:** This study investigates random noise in web-scale pre-training datasets for LLMs using a cohesive framework, revealing unexpected results about its impact on model performance and introducing a novel loss function to enhance denoising capabilities.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the impact of random noise in web-sourced data for training LLMs, seeking to understand its effects on model performance and provide solutions.

**Method:** The authors employ a 'What-Why-How' framework to systematically investigate random noise, providing theoretical justifications and conducting experiments with a Local Gradient Matching loss to improve model performance in downstream tasks.

**Key Contributions:**

	1. First systematic investigation of random noise in LLM training datasets.
	2. Introduction of the Local Gradient Matching loss to enhance denoising.
	3. Validation of findings through experiments on multiple language and vision benchmarks.

**Result:** The increase in next-token prediction loss due to noise was lower than expected, and the Local Gradient Matching loss was validated to enhance denoising in downstream tasks across various benchmarks.

**Limitations:** The study explores a specific aspect of data quality, but it may not cover all forms of noise or their interactions with model architectures.

**Conclusion:** Random noise affects downstream task performance negatively, and the introduced Local Gradient Matching loss can effectively mitigate this issue.

**Abstract:** Web-scale pre-training datasets are the cornerstone of LLMs' success. However, text data curated from the Internet inevitably contains random noise caused by decoding errors or unregulated web content. In contrast to previous works that focus on low quality or synthetic data, our study \textbf{provides the first systematic investigation of such random noise through a cohesive ``What-Why-How'' framework.} Surprisingly, we observed that the resulting increase in the loss of next-token prediction (NTP) was significantly lower than the proportion of random noise even when the model was scaled up to 2.7B. We provide a theoretical justification for this phenomenon, which also elucidates the success of multilingual models and can be applied to multimodal models. On the other hand, experiments show that the model's performance in downstream tasks is not based solely on the NTP loss, which means that random noise may result in degraded downstream performance. To address the potential adverse effects, we introduce a novel plug-and-play Local Gradient Matching loss, which explicitly enhances the denoising capability of the downstream task head by aligning the gradient of normal and perturbed features without requiring knowledge of the model's parameters. Additional experiments on 8 language and 14 vision benchmarks further validate its effectiveness.

</details>


### [107] [Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging](https://arxiv.org/abs/2502.06876)

*Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Ziyu Zhao, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang*

**Main category:** cs.CL

**Keywords:** 3H optimization, large language models, model merging, data mixture methods, RESM

**Relevance Score:** 8

**TL;DR:** This paper explores methods for achieving balanced alignment of large language models (LLMs) regarding Helpfulness, Honesty, and Harmlessness (3H optimization), comparing traditional data mixture strategies with model merging techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance responsible AI through balanced 3H optimization in large language models, addressing limitations of existing methods.

**Method:** The paper compares model merging and data mixture techniques for 3H alignment and introduces a new merging method called RESM that incorporates outlier weighting and sparsity-aware rank selection to improve model alignment.

**Key Contributions:**

	1. Introduction of the RESM merging method
	2. Systematic comparison of model merging vs. data mixture for 3H optimization
	3. Release of models for further research

**Result:** RESM demonstrated a 2%-5% improvement over traditional data mixture methods and a 1%-3% improvement over existing model merging techniques in achieving balanced alignment.

**Limitations:** 

**Conclusion:** Model merging, particularly with the RESM method, offers a promising approach to better alignment of large language models concerning Helpfulness, Honesty, and Harmlessness.

**Abstract:** Achieving balanced alignment of large language models (LLMs) in terms of Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a cornerstone of responsible AI. Existing methods like data mixture strategies face limitations, including heavy reliance on expert knowledge and conflicting optimization signals. While model merging offers parameter-level conflict-resolution strategies through integrating specialized models' parameters, its potential for 3H optimization remains underexplored. This paper systematically compares the effectiveness of model merging and data mixture methods in constructing 3H-aligned LLMs for the first time, revealing previously overlooked collaborative and conflict relationships among the 3H dimensions and discussing the advantages and drawbacks of data mixture (\textit{data-level}) and model merging (\textit{parameter-level}) methods in mitigating the conflict for balanced 3H optimization. Specially, we propose a novel \textbf{R}eweighting \textbf{E}nhanced task \textbf{S}ingular \textbf{M}erging method, \textbf{RESM}, through outlier weighting and sparsity-aware rank selection strategies to address the challenges of preference noise accumulation and layer sparsity adaptation inherent in 3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and robustness of RESM compared to previous data mixture (2\%-5\% gain) and model merging (1\%-3\% gain) methods in achieving balanced LLM alignment. We release our models through \href{https://huggingface.co/Jinluan}{3H\_Merging} for further investigations.

</details>


### [108] [Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More](https://arxiv.org/abs/2502.07490)

*Xialie Zhuang, Zhikai Jia, Jianjin Li, Zhenyu Zhang, Li Shen, Zheng Cao, Shiwei Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Masked Language Modeling, Next-Token Prediction, Information Retrieval, Attention Mechanism

**Relevance Score:** 8

**TL;DR:** MEAP improves LLM key information retrieval and reasoning tasks by integrating MLM into NTP without added computational overhead.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with accurate information retrieval, necessitating an improved approach for enhancing their capabilities.

**Method:** MEAP integrates Masked Language Modeling with Next-Token Prediction by masking input tokens and using a decoder-only Transformer for autoregressive prediction.

**Key Contributions:**

	1. Proposing the MEAP training paradigm for LLMs
	2. Demonstrating superior performance in information retrieval and reasoning tasks
	3. Highlighting the mechanism that improves attention scores in language models

**Result:** MEAP significantly outperforms NTP in key information retrieval and long-context reasoning tasks, and also shows superior performance in supervised fine-tuning scenarios.

**Limitations:** 

**Conclusion:** MEAP's emphasis on a smaller set of non-masked tokens enhances task-relevant focus in LLM training, making it an effective paradigm for improving model performance.

**Abstract:** Large Language Models (LLMs) are discovered to suffer from accurately retrieving key information. To address this, we propose Mask-Enhanced Autoregressive Prediction (MEAP), a simple yet effective training paradigm that seamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction (NTP) to enhance the latter's in-context retrieval capabilities. Specifically, MEAP first randomly masks a small fraction of input tokens and then directly performs the standard next-token prediction autoregressive using a decoder-only Transformer. MEAP eliminates the need for bidirectional attention or encoder-decoder architectures for MLM, incurring no additional computational overhead during pre-training or inference. Intensive experiments demonstrate that MEAP substantially outperforms NTP on key information retrieval and long-context reasoning tasks, while performing on par or better on commonsense reasoning tasks. The benefits of MEAP also extend to supervised fine-tuning, where it shows remarkable advantages in lost-in-the-middle scenarios, outperforming NTP by 11.77 percentage points. Our analysis indicates that MEAP's effectiveness arises from its ability to promote more distinguishable attention scores by concentrating on a reduced set of non-masked tokens. This mechanism improves the model's focus on task-relevant signals while mitigating the influence of peripheral context. These findings position MEAP as a promising training paradigm for large language models.

</details>


### [109] [Hallucination, Monofacts, and Miscalibration: An Empirical Investigation](https://arxiv.org/abs/2502.08666)

*Miranda Muqing Miao, Michael Kearns*

**Main category:** cs.CL

**Keywords:** hallucination, large language models, selective upweighting, monofact rate, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates the relationship between monofact rates and hallucination in language models, proposing a method to reduce hallucination through selective upweighting during training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding and mitigating hallucinations in large language models, which have implications for their reliability in applications.

**Method:** Empirical investigation of the relationship between monofact rates and hallucination, and the introduction of selective upweighting during training.

**Key Contributions:**

	1. First empirical investigation of the relationship between monofact rates and hallucination in language models.
	2. Development of selective upweighting technique that reduces hallucination significantly.
	3. Findings challenge the efficacy of universal deduplication policies in training language models.

**Result:** Selective upweighting can reduce hallucination by up to 40% without sacrificing accuracy, challenging existing deduplication policies.

**Limitations:** Focuses on specific models and empirical settings; results may not generalize universally across all types of language models.

**Conclusion:** The trade-off between maintaining accuracy and reducing hallucination highlights a key optimization challenge in model training.

**Abstract:** Hallucinated facts in large language models (LLMs) have recently been shown to obey a statistical lower bound determined by the monofact rate (related to the classical Good-Turing missing mass estimator) minus model miscalibration (Kalai & Vempala, 2024). We present the first empirical investigation of this three-way relationship in classical n-gram models and fine-tuned encoder-decoder Transformers. By generating training data from Pareto distributions with varying shape parameters, we systematically control the monofact rates and establish its positive relationship with hallucination. To bridge theory and practice, we derive an empirical analog of the hallucination bound by replacing the population miscalibration term (Section 2.1) with an empirical bin-wise KL divergence and confirm its practical viability. We then introduce selective upweighting -- a simple yet effective technique that strategically repeats as little as 5% of training examples -- to deliberately inject miscalibration into the model. This intervention reduces hallucination by up to 40%, challenging universal deduplication policies. Our experiments reveal a critical trade-off: selective upweighting maintains pre-injection levels of accuracy while substantially reducing hallucination, whereas standard training gradually improves accuracy but fails to address persistently high hallucination, indicating an inherent tension in optimization objectives.

</details>


### [110] [Investigating Language Preference of Multilingual RAG Systems](https://arxiv.org/abs/2502.11175)

*Jeonghyun Park, Hwanhee Lee*

**Main category:** cs.CL

**Keywords:** Multilingual Retrieval, Augmented Generation, Language Preferences, Dual Knowledge Multilingual RAG, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper investigates language preferences in multilingual retrieval-augmented generation (mRAG), proposing a framework to enhance performance across languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the challenges faced by mRAG systems in retrieving relevant information across diverse linguistic contexts and improving the consistency of generated responses.

**Method:** The authors conducted a series of experiments to analyze language preferences in mRAG for both retrieval and generation processes, leading to the development of the Dual Knowledge Multilingual RAG (DKM-RAG) framework.

**Key Contributions:**

	1. Investigation of language preferences in mRAG systems
	2. Introduction of the Dual Knowledge Multilingual RAG (DKM-RAG) framework
	3. Empirical demonstration of enhanced performance across diverse languages

**Result:** Empirical results show that DKM-RAG reduces language preference issues in generation and improves performance significantly across various linguistic settings.

**Limitations:** 

**Conclusion:** The study concludes that integrating translated multilingual passages with complementary model knowledge effectively addresses inconsistencies in mRAG systems.

**Abstract:** Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language models by integrating external multilingual information to produce context-aware responses. However, mRAG systems struggle with retrieving relevant information due to linguistic variations between queries and documents, generating inconsistent responses when multilingual sources conflict. In this work, we systematically investigate language preferences in both retrieval and generation of mRAG through a series of experiments. Our analysis indicates that retrievers tend to prefer high-resource and query languages, yet this preference does not consistently improve generation performance. Moreover, we observe that generators prefer the query language or Latin scripts, leading to inconsistent outputs. To overcome these issues, we propose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective framework that fuses translated multilingual passages with complementary model knowledge. Empirical results demonstrate that DKM-RAG mitigates language preference in generation and enhances performance across diverse linguistic settings.

</details>


### [111] [Can Your Uncertainty Scores Detect Hallucinated Entity?](https://arxiv.org/abs/2502.11948)

*Min-Hsuan Yeh, Max Kamachee, Seongheon Park, Yixuan Li*

**Main category:** cs.CL

**Keywords:** LLM, hallucination detection, entity-level analysis, uncertainty estimation, HalluEntity

**Relevance Score:** 9

**TL;DR:** This paper tackles the limitation of existing LLM hallucination detection methods by proposing entity-level hallucination detection using a new dataset called HalluEntity, which annotates hallucination at the entity level.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for detecting hallucinations in LLMs primarily work at a sentence or paragraph level, lacking the ability to identify specific spans or entities that contribute to hallucinated content, particularly in long-form outputs.

**Method:** The paper introduces a new dataset, HalluEntity, which uses entity-level annotations for hallucinations. It evaluates various uncertainty-based detection approaches across 17 modern LLMs.

**Key Contributions:**

	1. Introduction of the HalluEntity dataset for entity-level hallucination detection.
	2. Comprehensive evaluation of 17 modern LLMs using the proposed dataset.
	3. Insights into the relationships between linguistic properties and hallucination tendencies.

**Result:** Experimental results indicate that uncertainty estimation methods focusing on individual token probabilities over-predict hallucinations, while context-aware methods perform better but still exhibit suboptimal results.

**Limitations:** The performance of context-aware methods is still suboptimal, indicating a need for better detection approaches.

**Conclusion:** The study identifies key relationships between hallucination tendencies and linguistic properties, providing important insights for future research directions.

**Abstract:** To mitigate the impact of hallucination nature of LLMs, many studies propose detecting hallucinated generation through uncertainty estimation. However, these approaches predominantly operate at the sentence or paragraph level, failing to pinpoint specific spans or entities responsible for hallucinated content. This lack of granularity is especially problematic for long-form outputs that mix accurate and fabricated information. To address this limitation, we explore entity-level hallucination detection. We propose a new data set, HalluEntity, which annotates hallucination at the entity level. Based on the dataset, we comprehensively evaluate uncertainty-based hallucination detection approaches across 17 modern LLMs. Our experimental results show that uncertainty estimation approaches focusing on individual token probabilities tend to over-predict hallucinations, while context-aware methods show better but still suboptimal performance. Through an in-depth qualitative study, we identify relationships between hallucination tendencies and linguistic properties and highlight important directions for future research. HalluEntity: https://huggingface.co/datasets/samuelyeh/HalluEntity

</details>


### [112] [iAgent: LLM Agent as a Shield between User and Recommender Systems](https://arxiv.org/abs/2502.14662)

*Wujiang Xu, Yunxiao Shi, Zujie Liang, Xuying Ning, Kai Mei, Kun Wang, Xi Zhu, Min Xu, Yongfeng Zhang*

**Main category:** cs.CL

**Keywords:** recommender systems, user-agent-platform, user preferences, LLM agents, HCI

**Relevance Score:** 8

**TL;DR:** The paper introduces a new user-agent-platform paradigm for recommender systems to better protect user interests and address flaws in traditional models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional recommender systems prioritize platform benefits over individual user preferences, leading to user vulnerabilities and lack of personalization.

**Method:** The proposed approach involves an agent that acts as a protective layer between the user and the recommender system, allowing for indirect exposure.

**Key Contributions:**

	1. Introduction of the user-agent-platform paradigm
	2. Improvement of user protection in recommender systems
	3. Focus on individual user preferences over platform objectives

**Result:** This paradigm aims to enhance user control and personalization while mitigating issues like manipulation and echo chambers.

**Limitations:** The effectiveness of the proposed paradigm needs empirical validation in real-world scenarios.

**Conclusion:** Adopting this user-agent-platform model can significantly improve user satisfaction and better align recommendations with individual needs.

**Abstract:** Traditional recommender systems usually take the user-platform paradigm, where users are directly exposed under the control of the platform's recommendation algorithms. However, the defect of recommendation algorithms may put users in very vulnerable positions under this paradigm. First, many sophisticated models are often designed with commercial objectives in mind, focusing on the platform's benefits, which may hinder their ability to protect and capture users' true interests. Second, these models are typically optimized using data from all users, which may overlook individual user's preferences. Due to these shortcomings, users may experience several disadvantages under the traditional user-platform direct exposure paradigm, such as lack of control over the recommender system, potential manipulation by the platform, echo chamber effects, or lack of personalization for less active users due to the dominance of active users during collaborative learning. Therefore, there is an urgent need to develop a new paradigm to protect user interests and alleviate these issues. Recently, some researchers have introduced LLM agents to simulate user behaviors, these approaches primarily aim to optimize platform-side performance, leaving core issues in recommender systems unresolved. To address these limitations, we propose a new user-agent-platform paradigm, where agent serves as the protective shield between user and recommender system that enables indirect exposure.

</details>


### [113] [Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing](https://arxiv.org/abs/2502.15208)

*Zhilin Wang, Yafu Li, Jianhao Yan, Yu Cheng, Yue Zhang*

**Main category:** cs.CL

**Keywords:** Dynamical systems, Large language models, Paraphrasing, Iterative processes, Text generation

**Relevance Score:** 8

**TL;DR:** The paper analyzes the iterative nature of large language models (LLMs) through dynamics systems theory, revealing that successive paraphrasing leads to convergence on stable periodic states, limiting linguistic diversity.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the patterns of text generation in LLMs through iterative processes, using dynamical systems theory as a framework.

**Method:** The study investigates the effects of successive paraphrasing in LLMs and explores the emergence of stable periodic states in generated outputs.

**Key Contributions:**

	1. Introduction of dynamical systems theory to the analysis of LLMs
	2. Identification of stabilization in paraphrasing leading to 2-period cycles
	3. Insights into constraints on linguistic diversity in LLM outputs

**Result:** The findings demonstrate that successive paraphrasing leads to convergence towards specific periodic states (2-period attractors), indicating a limitation in linguistic diversity despite expectations of diversity in LLM outputs.

**Limitations:** The study focuses on specific types of iterative processes and may not generalize to all aspects of language generation in LLMs.

**Conclusion:** The research highlights inherent constraints in the generative capabilities of LLMs and suggests using dynamical systems theory for studying the expressive potential of these models.

**Abstract:** Dynamical systems theory provides a framework for analyzing iterative processes and evolution over time. Within such systems, repetitive transformations can lead to stable configurations, known as attractors, including fixed points and limit cycles. Applying this perspective to large language models (LLMs), which iteratively map input text to output text, provides a principled approach to characterizing long-term behaviors. Successive paraphrasing serves as a compelling testbed for exploring such dynamics, as paraphrases re-express the same underlying meaning with linguistic variation. Although LLMs are expected to explore a diverse set of paraphrases in the text space, our study reveals that successive paraphrasing converges to stable periodic states, such as 2-period attractor cycles, limiting linguistic diversity. This phenomenon is attributed to the self-reinforcing nature of LLMs, as they iteratively favour and amplify certain textual forms over others. This pattern persists with increasing generation randomness or alternating prompts and LLMs. These findings underscore inherent constraints in LLM generative capability, while offering a novel dynamical systems perspective for studying their expressive potential.

</details>


### [114] [Call for Rigor in Reporting Quality of Instruction Tuning Data](https://arxiv.org/abs/2503.04807)

*Hyeonseok Moon, Jaehyung Seo, Heuiseok Lim*

**Main category:** cs.CL

**Keywords:** large language models, instruction tuning, hyperparameter selection, data quality, model performance

**Relevance Score:** 8

**TL;DR:** This paper critiques the arbitrary selection of hyperparameters in training large language models (LLMs) with instruction tuning data and its impact on evaluating data quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inconsistencies in hyperparameter selection in studies assessing instruction tuning data quality for LLMs, which can lead to misleading conclusions about data effectiveness.

**Method:** The paper analyzes the variation in hyperparameters used across different studies for training the same model and conducts experiments on LIMA data and a selection of Alpaca data points to illustrate the consequences of arbitrary hyperparameter choices.

**Key Contributions:**

	1. Identification of the issue with arbitrary hyperparameter selection in instruction tuning studies.
	2. Experimental demonstration of the impact of hyperparameter decisions on model performance results.
	3. Emphasis on the need for rigorous methodology in evaluating IT data quality.

**Result:** The study shows that the arbitrary selection of hyperparameters can lead to significant variations in model performance, undermining the validity of conclusions drawn about instruction tuning data quality.

**Limitations:** 

**Conclusion:** Careful consideration of hyperparameter choices is essential when evaluating the quality of instruction tuning data to maintain the integrity of alignment performance assessments for LLMs.

**Abstract:** Instruction tuning is crucial for adapting large language models (LLMs) to align with user intentions. Numerous studies emphasize the significance of the quality of instruction tuning (IT) data, revealing a strong correlation between IT data quality and the alignment performance of LLMs. In these studies, the quality of IT data is typically assessed by evaluating the performance of LLMs trained with that data. However, we identified a prevalent issue in such practice: hyperparameters for training models are often selected arbitrarily without adequate justification. We observed significant variations in hyperparameters applied across different studies, even when training the same model with the same data. In this study, we demonstrate the potential problems arising from this practice and emphasize the need for careful consideration in verifying data quality. Through our experiments on the quality of LIMA data and a selected set of 1,000 Alpaca data points, we demonstrate that arbitrary hyperparameter decisions can make any arbitrary conclusion.

</details>


### [115] [TigerLLM -- A Family of Bangla Large Language Models](https://arxiv.org/abs/2503.10995)

*Nishat Raihan, Marcos Zampieri*

**Main category:** cs.CL

**Keywords:** Bangla, Large Language Models, TigerLLM, NLP, Machine Learning

**Relevance Score:** 9

**TL;DR:** TigerLLM, a family of Bangla LLMs, outperforms existing open-source and proprietary models, setting a new standard for Bangla language processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the linguistic disparity in the development of Large Language Models (LLMs) for Bangla, which is underrepresented compared to high-resource languages.

**Method:** Introduction of TigerLLM, a set of Bangla LLMs, which were evaluated against existing models across standard benchmarks.

**Key Contributions:**

	1. Introduction of a new family of Bangla LLMs called TigerLLM
	2. Performance benchmarks demonstrating superiority over existing models
	3. Providing a new standard for future Bangla LLM development

**Result:** TigerLLM surpasses all open-source alternatives and outperforms larger proprietary models like GPT3.5.

**Limitations:** 

**Conclusion:** TigerLLM establishes itself as the new baseline for future developments in Bangla language modeling.

**Abstract:** The development of Large Language Models (LLMs) remains heavily skewed towards English and a few other high-resource languages. This linguistic disparity is particularly evident for Bangla - the 5th most spoken language. A few initiatives attempted to create open-source Bangla LLMs with performance still behind high-resource languages and limited reproducibility. To address this gap, we introduce TigerLLM - a family of Bangla LLMs. Our results demonstrate that these models surpass all open-source alternatives and also outperform larger proprietary models like GPT3.5 across standard benchmarks, establishing TigerLLM as the new baseline for future Bangla language modeling.

</details>


### [116] [KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse](https://arxiv.org/abs/2503.16525)

*Huan Yang, Renji Zhang, Mingzhe Huang, Weijun Wang, Yin Tang, Yuanchun Li, Yunxin Liu, Deyu Zhang*

**Main category:** cs.CL

**Keywords:** large language models, KV cache management, recomputation, efficiency, multi-tenant systems

**Relevance Score:** 8

**TL;DR:** This paper presents KVShare, a KV cache management module for large language models (LLMs) designed to reduce computational costs and improve Time to First Token (TTFT) while maintaining accuracy by sharing KV caches across requests.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational costs and unsatisfactory Time to First Token (TTFT) associated with large language models (LLMs) operating with long context lengths.

**Method:** The paper introduces KVShare, which implements a Dual-Stage High Deviation (DHD) algorithm that selectively recomputes part of the KV cache during decoding, and a cache-aware scheduler that optimizes request handling based on KV cache hit rates.

**Key Contributions:**

	1. Introduction of KVShare for efficient KV cache management in LLMs
	2. Development of a Dual-Stage High Deviation algorithm for selective cache recomputation
	3. Creation of a cache-aware scheduler for prioritizing requests

**Result:** KVShare reduces TTFT by up to 9.39x and increases throughput by 1.2x compared to full KV recompute, while achieving a 20.38% improvement in accuracy over state-of-the-art methods.

**Limitations:** 

**Conclusion:** The proposed KV cache management module, KVShare, enhances the efficiency and effectiveness of LLM serving in multi-tenant environments without sacrificing model accuracy.

**Abstract:** Recent advances in long-text understanding have pushed the context length of large language models (LLMs) up to one million tokens. It boosts LLMs's accuracy and reasoning capacity but causes exorbitant computational costs and unsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the exact same KV cache of prefixes and templates or shares similar ones but with extra selective recomputation, offers a promising way to tackle this issue. However, prior studies overlook the cross-request KV reuse and the attention deviations introduced by new tokens during the decoding stage. In this paper, we present a KV cache management module that shares the KV cache across requests under multi-tenant scenarios without sacrificing model accuracy. Our system, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage High Deviation algorithm (DHD) that conditionally selects a small portion of KV cache to be recomputed during both prefill and decode phases, and 2) a cache-aware scheduler that prioritizes requests based on their KV cache hit rates and orchestrates continuous batching to achieve enhanced system efficiency and faster TTFT. Multi-task experiments conducted on models such as Qwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up to 9.39x and increases 1.2x of the throughput compared to the full KV recompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy compared to SOTA methods.

</details>


### [117] [Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts](https://arxiv.org/abs/2503.16529)

*Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Limin Han, Jiaojiao Zhao, Junting Guo, Zhenhong Long, Shu Yang, Meijuan An, Beibei Huang, Rongjia Du, Ning Wang, Kai Wang, Shiguo Lian*

**Main category:** cs.CL

**Keywords:** DeepSeek-R1, safety evaluation, machine learning, distillation, HCI

**Relevance Score:** 7

**TL;DR:** This study evaluates and enhances the safety of the DeepSeek-R1 series distilled models using a comprehensive Chinese safety benchmark, addressing previously identified vulnerabilities and demonstrating significant improvements in safety without compromising reasoning capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess and improve the safety of DeepSeek-R1 models, which have shown critical vulnerabilities, especially in harmful prompt processing.

**Method:** The study utilizes the CHiSafetyBench, a comprehensive Chinese safety benchmark, to evaluate the safety capabilities of DeepSeek-R1 series models both before and after the distillation process.

**Key Contributions:**

	1. Comprehensive evaluation of DeepSeek-R1 distilled models' safety in Chinese contexts
	2. Implementation of targeted safety enhancements
	3. Open-sourcing safety-enhanced models for community use

**Result:** Evaluation results show enhanced models achieve significant improvements in safety while maintaining reasoning capabilities with no notable degradation.

**Limitations:** The study's focus is primarily on the Chinese context and may not be applicable to other languages or settings without further research.

**Conclusion:** The study successfully enhances the safety of the DeepSeek-R1 model series, providing open-source access to improved models for future research and optimization.

**Abstract:** DeepSeek-R1, renowned for its exceptional reasoning capabilities and open-source strategy, is significantly influencing the global artificial intelligence landscape. However, it exhibits notable safety shortcomings. Recent research conducted by Robust Intelligence, a subsidiary of Cisco, in collaboration with the University of Pennsylvania, revealed that DeepSeek-R1 achieves a 100\% attack success rate when processing harmful prompts. Furthermore, multiple security firms and research institutions have identified critical security vulnerabilities within the model. Although China Unicom has uncovered safety vulnerabilities of R1 in Chinese contexts, the safety capabilities of the remaining distilled models in the R1 series have not yet been comprehensively evaluated. To address this gap, this study utilizes the comprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth safety evaluation of the DeepSeek-R1 series distilled models. The objective is to assess the safety capabilities of these models in Chinese contexts both before and after distillation, and to further elucidate the adverse effects of distillation on model safety. Building on these findings, we implement targeted safety enhancements for the entire DeepSeek-R1 model series. Evaluation results indicate that the enhanced models achieve significant improvements in safety while maintaining reasoning capabilities without notable degradation. We open-source the safety-enhanced models at https://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource for future research and optimization of DeepSeek models.

</details>


### [118] [Mixture of Routers](https://arxiv.org/abs/2503.23362)

*Jia-Chen Zhang, Yu-Jie Xiong, Xi-He Qiu, Chun-Ming Xia, Fei Dai*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, Low-Rank Adaptation, fine-tuning, large language models, routing mechanism

**Relevance Score:** 7

**TL;DR:** This paper introduces Mixture of Routers (MoR), a novel fine-tuning method that enhances parameter efficiency in large language models by integrating multiple sub-routers into the routing mechanism of Mixture-of-Experts (MoE).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve the performance of large models using efficient fine-tuning techniques while addressing issues in MoE routing mechanisms.

**Method:** The proposed Mixture of Routers (MoR) uses multiple sub-routers for joint selection and a learnable main router to determine the weights of the sub-routers, enhancing the fine-tuning process.

**Key Contributions:**

	1. Introduction of Mixture of Routers (MoR) for fine-tuning.
	2. Integration of multiple sub-routers into the routing mechanism of MoE.
	3. Demonstrated average performance improvement over baseline models.

**Result:** MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%.

**Limitations:** 

**Conclusion:** MoR offers a parameter-efficient fine-tuning method suitable for a wide range of applications and serves as a plug-and-play solution.

**Abstract:** Supervised fine-tuning (SFT) is a milestone in aligning large language models with human instructions and adapting them to downstream tasks. In particular, Low-Rank Adaptation (LoRA) has gained widespread attention due to its parameter efficiency. However, its impact on improving the performance of large models remains limited. Recent studies suggest that combining LoRA with Mixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE adapts to the diversity and complexity of datasets by dynamically selecting the most suitable experts, thereby improving task accuracy and efficiency. Despite impressive results, recent studies reveal issues in the MoE routing mechanism, such as incorrect assignments and imbalanced expert allocation. Inspired by the principles of Redundancy and Fault Tolerance Theory. We innovatively integrate the concept of Mixture of Experts into the routing mechanism and propose an efficient fine-tuning method called Mixture of Routers (MoR). It employs multiple sub-routers for joint selection and uses a learnable main router to determine the weights of the sub-routers. The results show that MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method suitable for a wide range of applications. Our code is available here: https://anonymous.4open.science/r/MoR-DFC6.

</details>


### [119] [Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?](https://arxiv.org/abs/2504.01698)

*Yi-Long Lu, Chunhui Zhang, Jiajun Song, Lifeng Fan, Wei Wang*

**Main category:** cs.CL

**Keywords:** Theory of Mind, Large Language Models, Reinforcement Learning, Supervised Fine-Tuning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper explores the effectiveness of Large Language Models (LLMs) on Theory of Mind (ToM) tasks, revealing that larger models can improve reasoning through Reinforcement Learning (RL), while smaller models face limitations. Supervised Fine-Tuning (SFT) offers competitive performance without traditional reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if LLM benchmarks for Theory of Mind require human-like reasoning or can succeed with alternative strategies.

**Method:** The study uses Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) on LLMs of different sizes (0.5B to 7B parameters) and evaluates their performance on multiple ToM datasets.

**Key Contributions:**

	1. Investigated the distinct performance of LLMs under different training methodologies (RL vs. SFT) on ToM tasks.
	2. Demonstrated scale-dependent reasoning capabilities in LLMs, noting significant differences in model sizes.
	3. Highlighted the disconnection between benchmark performance and the reasoning process utilized by LLMs.

**Result:** Larger models (7B) showed improved accuracy and reasoning quality with RL, whereas smaller models (≤3B) faced 'reasoning collapse'. SFT often matched or exceeded RL performance in terms of accuracy without explicit reasoning training.

**Limitations:** Study largely focused on specific LLM scales and might not generalize across all model architectures or tasks.

**Conclusion:** Current ToM benchmarks may not necessitate human-like reasoning; LLMs can achieve high accuracy through alternative methods, challenging the assumption that explicit belief-tracking is needed.

**Abstract:** Theory of Mind (ToM), the ability to attribute mental states to others, is fundamental for human social intelligence and a critical capability for advanced Artificial Intelligence. Recent advancements in Large Language Models (LLMs) have shown promising performance on ToM benchmarks, raising the question: Do these benchmarks necessitate explicit human-like reasoning processes, or can models succeed through alternative strategies? We investigate this question empirically by applying Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) to LLMs of varying scales (0.5B to 7B parameters) and evaluating them across multiple ToM datasets. Our results reveal a scale-dependent impact of RL: while RL significantly improves accuracy and fosters high-quality, interpretable, and transferable belief-tracking reasoning in larger models (7B), it leads to "reasoning collapse" in smaller models ($\leq$3B), where high accuracy and generalization ability are achieved via drastically shortened, less meaningful responses. Surprisingly, further SFT achieves competitive and generalizable performance across these benchmarks, often matching or exceeding RL models in accuracy, despite not being explicitly trained to produce structured reasoning traces. These findings highlight a critical discrepancy between benchmark accuracy and the nature of learned reasoning. Our work suggests that current ToM benchmarks may be solvable without requiring the explicit, human-like simulation of mental states they were designed to probe. LLMs, particularly when scale is limited or training signals focus solely on output correctness, may leverage alternative rules effective for benchmark data structures.

</details>


### [120] [Parameterized Synthetic Text Generation with SimpleStories](https://arxiv.org/abs/2504.09184)

*Lennart Finke, Chandan Sreedhara, Thomas Dooms, Mat Allen, Emerald Zhang, Juan Diego Rodriguez, Noa Nabeshima, Thomas Marshall, Dan Braun*

**Main category:** cs.CL

**Keywords:** synthetic dataset, language model, story generation

**Relevance Score:** 5

**TL;DR:** The paper presents SimpleStories, a dataset of 2 million synthetic stories in simple language, enabling control over story characteristics and improving model efficiency and interpretability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a large, diverse synthetic story dataset that facilitates efficient end-to-end training and model interpretability.

**Method:** Creation of a synthetic dataset called SimpleStories with 2 million English and Japanese samples, employing parameterized prompts for varying story characteristics.

**Key Contributions:**

	1. Development of SimpleStories, a large dataset for story generation
	2. Improvement in sample efficiency and interpretability of language models
	3. Open-source resources for enhancing end-to-end training studies

**Result:** Demonstrated improved sample efficiency and model interpretability compared to the TinyStories dataset through ablation studies on a newly trained model suite.

**Limitations:** 

**Conclusion:** The open-sourcing of the dataset and model components aims to foster new research approaches in model training and to push the limits of parameter efficiency in language models.

**Abstract:** We present SimpleStories, a large synthetic story dataset in simple language, consisting of 2 million samples each in English and Japanese. Through parameterizing prompts at multiple levels of abstraction, we achieve control over story characteristics at scale, inducing syntactic and semantic diversity. Ablations on a newly trained model suite show improved sample efficiency and model interpretability compared to the TinyStories dataset. We open-source all constituent parts of model creation, hoping to enable novel ways to study the end-to-end training process. As a byproduct, we move the frontier regarding the fewest-parameter language model that outputs grammatical natural language.

</details>


### [121] [DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models](https://arxiv.org/abs/2505.09655)

*Xiwen Chen, Wenhui Zhu, Peijie Qiu, Xuanzhao Dong, Hao Wang, Haiyu Wu, Huayu Li, Aristeidis Sotiras, Yalin Wang, Abolfazl Razi*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Natural Language Processing, Semantic Diversity

**Relevance Score:** 7

**TL;DR:** This paper presents Diversity-aware Reward Adjustment (DRA) to improve reward signals in reinforcement learning for language models by incorporating semantic diversity in the learning process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitation of standard reward signals in reinforcement learning that fail to capture semantic diversity, leading to diversity-quality inconsistencies.

**Method:** DRA employs Submodular Mutual Information (SMI) to adjust rewards by downweighting redundant completions and enhancing rewards for diverse ones, integrated with both GRPO and its variant.

**Key Contributions:**

	1. Introduction of the Diversity-aware Reward Adjustment (DRA) method
	2. Utilization of Submodular Mutual Information (SMI) for reward adjustment
	3. Demonstration of state-of-the-art performance on mathematical reasoning tasks

**Result:** DRA demonstrates improved exploration and high-quality sample exploitation, achieving state-of-the-art performance on five mathematical reasoning benchmarks with an average accuracy of 58.2% using low resources.

**Limitations:** 

**Conclusion:** DRA improves the effectiveness of reinforcement learning for language model post-training by rewarding diversity, leading to better performance with limited training samples.

**Abstract:** Recent advances in reinforcement learning for language model post-training, such as Group Relative Policy Optimization (GRPO), have shown promise in low-resource settings. However, GRPO typically relies on solution-level and scalar reward signals that fail to capture the semantic diversity among sampled completions. This leads to what we identify as a diversity-quality inconsistency, where distinct reasoning paths may receive indistinguishable rewards. To address this limitation, we propose $\textit{Diversity-aware Reward Adjustment}$ (DRA), a method that explicitly incorporates semantic diversity into the reward computation. DRA uses Submodular Mutual Information (SMI) to downweight redundant completions and amplify rewards for diverse ones. This encourages better exploration during learning, while maintaining stable exploitation of high-quality samples. Our method integrates seamlessly with both GRPO and its variant DR.~GRPO, resulting in $\textit{DRA-GRPO}$ and $\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning benchmarks and find that it outperforms recent strong baselines. It achieves state-of-the-art performance with an average accuracy of 58.2%, using only 7,000 fine-tuning samples and a total training cost of approximately $55. The code is available at https://github.com/xiwenc1/DRA-GRPO.

</details>


### [122] [An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs](https://arxiv.org/abs/2505.09724)

*Gino Carmona-Díaz, William Jiménez-Leal, María Alejandra Grisales, Chandra Sripada, Santiago Amaya, Michael Inzlicht, Juan Pablo Bermúdez*

**Main category:** cs.CL

**Keywords:** text analysis, taxonomy development, LLMs, intercoder reliability, unstructured data

**Relevance Score:** 8

**TL;DR:** This paper presents a tutorial for using LLMs to develop and apply taxonomies for text analysis, demonstrating an efficient process for categorizing unstructured data.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The analysis of texts such as responses, headlines, or social media posts is labor-intensive and biased; LLMs can enhance this process.

**Method:** The paper describes a step-by-step tutorial for developing, testing, and applying taxonomies with LLMs in an iterative and collaborative manner.

**Key Contributions:**

	1. Tutorial on developing taxonomies using LLMs
	2. Step-by-step guidance on text analysis with high intercoder reliability
	3. Insights into limitations and possibilities of LLMs for text categorization

**Result:** The approach allows for the categorization of datasets with high intercoder reliability and demonstrates how to generate and refine taxonomies effectively.

**Limitations:** The paper discusses challenges and potential biases in using LLMs for text analysis.

**Conclusion:** While LLMs show promise in text analysis, the paper discusses both their capabilities and limitations.

**Abstract:** Analyzing texts such as open-ended responses, headlines, or social media posts is a time- and labor-intensive process highly susceptible to bias. LLMs are promising tools for text analysis, using either a predefined (top-down) or a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we present a step-by-step tutorial to efficiently develop, test, and apply taxonomies for analyzing unstructured data through an iterative and collaborative process between researchers and LLMs. Using personal goals provided by participants as an example, we demonstrate how to write prompts to review datasets and generate a taxonomy of life domains, evaluate and refine the taxonomy through prompt and direct modifications, test the taxonomy and assess intercoder agreements, and apply the taxonomy to categorize an entire dataset with high intercoder reliability. We discuss the possibilities and limitations of using LLMs for text analysis.

</details>


### [123] [From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models](https://arxiv.org/abs/2505.09924)

*Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang*

**Main category:** cs.CL

**Keywords:** Watermarking, Large Language Models, AI security

**Relevance Score:** 9

**TL;DR:** This paper introduces a symbiotic watermarking framework for Large Language Models that balances robustness, text quality, and security using three strategies: serial, parallel, and hybrid.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** As concerns grow regarding the misuse of AI-generated text, this study aims to provide a solution through effective watermarking methods that integrate current techniques.

**Method:** The paper combines logits-based and sampling-based watermarking approaches, creating a hybrid framework that embeds watermarks using token and semantic entropy.

**Key Contributions:**

	1. Introduction of a versatile watermarking framework for LLMs.
	2. Combination of logits-based and sampling-based approaches.
	3. Demonstration of state-of-the-art performance in watermarking.

**Result:** The proposed method shows improved performance over existing watermarking techniques, achieving state-of-the-art results across various datasets and models.

**Limitations:** 

**Conclusion:** The authors believe this framework offers valuable insights into watermarking strategies for AI-generated text, potentially enhancing the security and accountability of LLM outputs.

**Abstract:** The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark.

</details>


### [124] [LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations](https://arxiv.org/abs/2505.10354)

*Yile Wang, Zhanyu Shen, Hui Huang*

**Main category:** cs.CL

**Keywords:** semantic text representation, interpretable embeddings, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Low-dimensional Dense and Interpretable text embeddings (LDIR) that offer semantic representation and interpretability, outperforming traditional methods with fewer dimensions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current text embeddings struggle with interpretability despite effective performance, prompting the need for more interpretable yet efficient solutions.

**Method:** The authors propose a new approach that generates low-dimensional (under 500) embeddings using farthest point sampling to maintain semantic relatedness to anchor texts.

**Key Contributions:**

	1. Introduction of Low-dimensional Dense and Interpretable (LDIR) embeddings.
	2. Validation of LDIR on multiple tasks demonstrating superior performance with fewer dimensions.
	3. Code availability for further research and validation.

**Result:** LDIR shows performance close to black-box models while outperforming existing interpretable embeddings with significantly fewer dimensions, validated across various tasks.

**Limitations:** 

**Conclusion:** LDIR provides a compelling balance between performance and interpretability in semantic text representations.

**Abstract:** Semantic text representation is a fundamental task in the field of natural language processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have demonstrated excellent performance, but the values of each dimension are difficult to trace and interpret. Bag-of-words, as classic sparse interpretable embeddings, suffers from poor performance. Recently, Benara et al. (2024) propose interpretable text embeddings using large language models, which forms "0/1" embeddings based on responses to a series of questions. These interpretable text embeddings are typically high-dimensional (larger than 10,000). In this work, we propose Low-dimensional (lower than 500) Dense and Interpretable text embeddings with Relative representations (LDIR). The numerical values of its dimensions indicate semantic relatedness to different anchor texts through farthest point sampling, offering both semantic representation as well as a certain level of traceability and interpretability. We validate LDIR on multiple semantic textual similarity, retrieval, and clustering tasks. Extensive experimental results show that LDIR performs close to the black-box baseline models and outperforms the interpretable embeddings baselines with much fewer dimensions. Code is available at https://github.com/szu-tera/LDIR.

</details>
