# 2025-06-25

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 17]

- [cs.CL](#cs.CL) [Total: 63]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding](https://arxiv.org/abs/2506.18962)

*Weiheng Lu, Chunfeng Song, Jiamin Wu, Pengyu Zhu, Yuchen Zhou, Weijian Mai, Qihao Zheng, Wanli Ouyang*

**Main category:** cs.HC

**Keywords:** EEG, brain decoding, large language models, multi-task learning, human-machine interaction

**Relevance Score:** 7

**TL;DR:** UniMind is a general-purpose EEG foundation model that improves brain activity decoding by bridging EEG signals with large language models using a Neuro-Language Connector.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve generalizability and performance of EEG-based brain decoding models that currently require task-specific tuning.

**Method:** UniMind employs a Neuro-Language Connector to translate spatiotemporal EEG patterns into formats understood by language models and uses a Task-aware Query Selection module for task-relevant neural pattern learning.

**Key Contributions:**

	1. Neuro-Language Connector for translating EEG data to language models.
	2. Task-aware Query Selection for dynamic task-adaptive learning.
	3. Substantial performance improvements in multi-task decoding.

**Result:** UniMind achieves an average performance improvement of 12 percent over existing state-of-the-art models in multi-task brain decoding across ten datasets.

**Limitations:** 

**Conclusion:** UniMind is a significant advancement in EEG decoding, providing insights into neural patterns and enhancing human-machine interaction without the need for task-specific tuning.

**Abstract:** Decoding human brain activity from electroencephalography (EEG) signals is a central challenge at the intersection of neuroscience and artificial intelligence, enabling diverse applications in mental state assessment, clinical monitoring, and human-machine interaction. Recent efforts have extensively explored EEG-based brain foundation models for generalized brain decoding, employing large-scale training on multiple datasets. However, most of these attempts struggle with generalizability and fail to achieve satisfactory performance without task-specific tuning due to pronounced inherent heterogeneity among decoding tasks. To address these challenges, we present UniMind, a general-purpose EEG foundation model for unified multi-task brain decoding by uniquely unleashing the power of large language models to comprehend complex neural patterns. UniMind offers several advantages. First, we design a Neuro-Language Connector to bridge the modality gap between neural signals and large language models, distilling and transforming the spatiotemporal neural patterns of EEG data into representations understandable by language models. Second, a Task-aware Query Selection module is proposed to inject task-awareness into the cross-modal alignment by dynamically generating task-adaptive query tokens, enabling learning of task-relevant neural patterns across diverse tasks. Extensive experiments across ten datasets demonstrate that UniMind substantially outperforms state-of-the-art multi-task decoding models, with an average gain of 12 percent, while also offering valuable neuroscientific insights into neural functional correlations across tasks. The code will be made publicly available.

</details>


### [2] [Raise Awareness of the Environmental Impacts of Retail Food Products: A User-Centered Scenario-Based Approach](https://arxiv.org/abs/2506.19017)

*Lorenzo Porcelli, Francesco Palmieri*

**Main category:** cs.HC

**Keywords:** climate change, user interface, gamification, civic engagement, environmental impact

**Relevance Score:** 4

**TL;DR:** The paper presents a user interface aimed at raising awareness of the environmental impacts of food products through user-centered design and gamification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate climate change and its impacts through increased civic engagement and awareness about environmental impacts of food products.

**Method:** The development of the interface followed a user-centered scenario-based design approach with the incorporation of gamification elements to enhance participation.

**Key Contributions:**

	1. Development of a gamified user interface for environmental awareness
	2. Application of user-centered design in climate action tools
	3. Focus on civic engagement related to food product purchases

**Result:** The proposed user interface successfully informs users about the environmental impact of their food purchases, encouraging climate action.

**Limitations:** 

**Conclusion:** Enhanced user awareness through the interface can promote civic engagement in climate action, potentially leading to reduced environmental impact.

**Abstract:** The climate is warming rapidly, and atmospheric concentrations of greenhouse gases (GHGs) are at their highest levels ever recorded. As a result of these climate changes, caused mainly by human activities, disasters have increased fivefold over the past 50 years, causing death and economic loss. Civic engagement and awareness are essential to mitigate climate change and its impacts. In this work, we proposed a user interface that makes users aware of the environmental impact of the food products they buy when shopping. A user-centered scenario-based design was followed in the development of the interface. Gamification elements were added to increase civic participation in climate action.

</details>


### [3] [Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education](https://arxiv.org/abs/2506.19107)

*Ruiwei Xiao, Xinying Hou, Runlong Ye, Majeed Kazemitabaar, Nicholas Diana, Michael Liut, John Stamper*

**Main category:** cs.HC

**Keywords:** pedagogical prompting, large language models, educational intervention

**Relevance Score:** 9

**TL;DR:** This paper presents a study on teaching effective prompting strategies for large language models in education, with a focus on enhancing learning outcomes for undergraduate computer science students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The widespread application of large language models (LLMs) in education has raised concerns about their impact on learning outcomes, prompting the need to teach effective prompting strategies to students.

**Method:** A formative survey study was conducted with instructors (N=36) to inform instructional design, followed by the development of an interactive learning intervention targeting early undergraduate CS students (N=22), evaluated through pre/post-tests and mixed methods analysis.

**Key Contributions:**

	1. A theoretical framework of pedagogical prompting
	2. Insights into instructor attitudes towards pedagogical prompting
	3. Design of an interactive learning tool for teaching LLM-based help-seeking

**Result:** The study revealed significant improvements in students' skills for seeking help via LLMs, along with increased positive attitudes towards using prompts in the future.

**Limitations:** 

**Conclusion:** The findings support the potential of pedagogical prompting training in enhancing LLM usage in educational contexts, with implications for integrating such approaches into existing tools like ChatGPT.

**Abstract:** With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.

</details>


### [4] [Smart Glasses for CVI: Co-Designing Extended Reality Solutions to Support Environmental Perception by People with Cerebral Visual Impairment](https://arxiv.org/abs/2506.19210)

*Bhanuka Gamage, Nicola McDowell, Dijana Kovacic, Leona Holloway, Thanh-Toan Do, Nicholas Price, Arthur Lowery, Kim Marriott*

**Main category:** cs.HC

**Keywords:** Cerebral Visual Impairment, assistive technology, smart glasses, user-centered design, accessibility

**Relevance Score:** 7

**TL;DR:** This paper explores the potential of smart glasses to assist individuals with Cerebral Visual Impairment (CVI) in navigating their environments, highlighting user-centered design methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Cerebral Visual Impairment (CVI) is underrepresented in assistive technology research and affects higher-order visual processing, necessitating innovative solutions to help affected individuals interact with their surroundings.

**Method:** The study utilized the Double Diamond design framework including a two-week diary study, two ideation workshops, and ten iterative development sessions with users of the Apple Vision Pro.

**Key Contributions:**

	1. Novel application of smart glasses for assisting users with CVI
	2. User-centered co-design involving individuals with CVI
	3. Insights into the interaction between technology and the unique needs of CVI users.

**Result:** The findings indicate that smart glasses can significantly aid in locating objects, reading text, recognizing people, and managing sensory stress in complex environments for individuals with CVI.

**Limitations:** 

**Conclusion:** This research highlights the importance of developing assistive technologies for CVI, revealing the potential of smart glasses to address specific challenges faced by individuals with this condition.

**Abstract:** Cerebral Visual Impairment (CVI) is the set to be the leading cause of vision impairment, yet remains underrepresented in assistive technology research. Unlike ocular conditions, CVI affects higher-order visual processing-impacting object recognition, facial perception, and attention in complex environments. This paper presents a co-design study with two adults with CVI investigating how smart glasses, i.e. head-mounted extended reality displays, can support understanding and interaction with the immediate environment. Guided by the Double Diamond design framework, we conducted a two-week diary study, two ideation workshops, and ten iterative development sessions using the Apple Vision Pro. Our findings demonstrate that smart glasses can meaningfully address key challenges in locating objects, reading text, recognising people, engaging in conversations, and managing sensory stress. With the rapid advancement of smart glasses and increasing recognition of CVI as a distinct form of vision impairment, this research addresses a timely and under-explored intersection of technology and need.

</details>


### [5] [HARPT: A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps](https://arxiv.org/abs/2506.19268)

*Timoteo Kelly, Abdulkadir Korkmaz, Samuel Mallet, Connor Souders, Sadra Aliakbarpour, Praveen Rao*

**Main category:** cs.HC

**Keywords:** mobile health, user trust, privacy, natural language processing, dataset

**Relevance Score:** 9

**TL;DR:** HARPT is a large-scale corpus of mobile health app store reviews aimed at enhancing research on user privacy and trust.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to advance research in user privacy and trust in the context of mobile health applications by providing a comprehensive annotated dataset.

**Method:** The HARPT dataset was created through a sophisticated annotation strategy involving rule-based filtering, iterative manual labeling, targeted data augmentation, and weak supervision using transformer-based classifiers to ensure high-quality labels.

**Key Contributions:**

	1. Creation of a large annotated mobile health app store review corpus
	2. Development of a nuanced label schema for trust and privacy aspects
	3. Benchmarking of multiple classification models with strong performance outcomes

**Result:** A large-scale dataset with over 480,000 labeled reviews was successfully compiled, allowing for benchmarking of various classification models that showed strong performance.

**Limitations:** 

**Conclusion:** HARPT serves as a valuable public resource for future research in health informatics, cybersecurity, and natural language processing.

**Abstract:** We present HARPT, a large-scale annotated corpus of mobile health app store reviews aimed at advancing research in user privacy and trust. The dataset comprises over 480,000 user reviews labeled into seven categories that capture critical aspects of trust in applications, trust in providers and privacy concerns. Creating HARPT required addressing multiple complexities, such as defining a nuanced label schema, isolating relevant content from large volumes of noisy data, and designing an annotation strategy that balanced scalability with accuracy. This strategy integrated rule-based filtering, iterative manual labeling with review, targeted data augmentation, and weak supervision using transformer-based classifiers to accelerate coverage. In parallel, a carefully curated subset of 7,000 reviews was manually annotated to support model development and evaluation. We benchmark a broad range of classification models, demonstrating that strong performance is achievable and providing a baseline for future research. HARPT is released as a public resource to support work in health informatics, cybersecurity, and natural language processing.

</details>


### [6] [OpticalAging: Real-time Presbyopia Simulation for Inclusive Design via Tunable Lenses](https://arxiv.org/abs/2506.19307)

*Qing Zhang, Zixiong Su, Yoshihito Kondoh, Kazunori Asada, Thad Starner, Kai Kunze, Yuta Itoh, Jun Rekimoto*

**Main category:** cs.HC

**Keywords:** Presbyopia, Human-Computer Interaction, Simulation, Empathy, Design

**Relevance Score:** 5

**TL;DR:** OpticalAging is an optical see-through simulation that uses tunable lenses to represent presbyopia's visual effects, aiming to enhance understanding and empathy towards those affected by the condition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance awareness and understanding of presbyopia, particularly among those who are not affected by it, through a simulation that approximates the visual challenges faced by individuals with this condition.

**Method:** The study employed OpticalAging, which utilizes dynamically controlled tunable lenses to create a first-person perspective simulation of presbyopia's distance-dependent blur during real-world interaction, complemented by a user study with 19 participants.

**Key Contributions:**

	1. Introduction of OpticalAging as a novel simulation tool for presbyopia
	2. Validation through a mixed-methods user study showing both quantitative and qualitative improvements in empathy
	3. Recommendations for integrating simulation into design practices for inclusivity

**Result:** Quantitative measurements indicated significant changes in near points across three age modes, and qualitative feedback showed increased understanding and empathy from participants toward presbyopia.

**Limitations:** The simulation may not fully capture the lived experience of presbyopia, which is a common critique of such tools.

**Conclusion:** OpticalAging could be effectively integrated into age-inclusive design workflows, serving as a valuable tool when combined with user-centered methods.

**Abstract:** Presbyopia, a common age-related vision condition affecting most people as they age, often remains inadequately understood by those unaffected. To help bridge the gap between abstract accessibility knowledge and a more grounded appreciation of perceptual challenges, this study presents OpticalAging, an optical see-through simulation approach. Unlike VR-based methods, OpticalAging uses dynamically controlled tunable lenses to simulate the first-person visual perspective of presbyopia's distance-dependent blur during real-world interaction, aiming to enhance awareness. While acknowledging critiques regarding simulation's limitations in fully capturing lived experience, we position this tool as a complement to user-centered methods. Our user study (N = 19, 18-35 years old) provides validation: quantitative measurements show statistically significant changes in near points across three age modes (40s, 50s, 60s), while qualitative results suggest increases in reported understanding and empathy among participants. The integration of our tool into a design task showcases its potential applicability within age-inclusive design workflows when used critically alongside direct user engagement.

</details>


### [7] [Can theory-driven learning analytics dashboard enhance human-AI collaboration in writing learning? Insights from an empirical experiment](https://arxiv.org/abs/2506.19364)

*Angxuan Chen, Jingjing Lian, Xinran Kuang, Jiyou Jia*

**Main category:** cs.HC

**Keywords:** Generative AI, human-AI collaboration, self-regulated learning, learning analytics, academic writing

**Relevance Score:** 8

**TL;DR:** This study investigates the effects of a theory-driven learning analytics dashboard on enhancing human-AI collaboration in academic writing, showing positive impacts on writing knowledge and self-regulated learning, despite increased cognitive load.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of Generative AI in education while addressing concerns about superficial learning and over-reliance on AI tools.

**Method:** A quasi-experiment was conducted with 52 postgraduate students, comparing an experimental group using a learning analytics dashboard with a control group not using it, involving pre- and post-tests alongside questionnaires and dialogue analysis.

**Key Contributions:**

	1. Utilization of a theory-driven learning analytics dashboard
	2. Enhanced writing knowledge and self-regulated learning skills
	3. Insights into human-AI interaction characteristics during writing tasks

**Result:** The experimental group showed significantly greater writing knowledge gains and improvements in self-regulated learning skills, but also experienced increased test anxiety and cognitive load.

**Limitations:** Increased cognitive load and test anxiety among participants in the experimental group.

**Conclusion:** The study demonstrates that well-designed interventions around GenAI can enhance educational outcomes while stressing the need to prevent over-reliance and guarantee that technology supports learning processes.

**Abstract:** The integration of Generative AI (GenAI) into education has raised concerns about over-reliance and superficial learning, particularly in writing tasks in higher education. This study explores whether a theory-driven learning analytics dashboard (LAD) can enhance human-AI collaboration in the academic writing task by improving writing knowledge gains, fostering self-regulated learning (SRL) skills and building different human-AI dialogue characteristics. Grounded in Zimmerman's SRL framework, the LAD provided real-time feedback on learners' goal-setting, writing processes and reflection, while monitoring the quality of learner-AI interactions. A quasi-experiment was conducted involving 52 postgraduate students divided into an experimental group (EG) using the LAD to a control group (CG) without it in a human-AI collaborative writing task. Pre- and post- knowledge tests, questionnaires measuring SRL and cognitive load, and students' dialogue data with GenAI were collected and analyzed. Results showed that the EG achieved significantly higher writing knowledge gains and improved SRL skills, particularly in self-efficacy and cognitive strategies. However, the EG also reported increased test anxiety and cognitive load, possibly due to heightened metacognitive awareness. Epistemic Network Analysis revealed that the EG engaged in more reflective, evaluative interactions with GenAI, while the CG focused on more transactional and information-seeking exchanges. These findings contribute to the growing body of literature on the educational use of GenAI and highlight the importance of designing interventions that complement GenAI tools, ensuring that technology enhances rather than undermines the learning process.

</details>


### [8] [Integrating AIs With Body Tracking Technology for Human Behaviour Analysis: Challenges and Opportunities](https://arxiv.org/abs/2506.19430)

*Adrien Coppens, Val√©rie Maquil*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Body Tracking, AI Integration

**Relevance Score:** 8

**TL;DR:** This paper explores the automated analysis of human behaviour through depth cameras, discussing the integration of AI components in creating interactive systems for remote collaboration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to enhance interactive systems by leveraging commodity depth cameras for body tracking and integrating AI components for improved functionality.

**Method:** The authors analyze their approach based on the development of a remote collaboration system using AI-based recognition models and existing technologies for effective user tracking.

**Key Contributions:**

	1. Integration of AI components in body tracking systems
	2. Development of a remote collaboration system using depth cameras
	3. Discussion on challenges in engineering tracking pipelines

**Result:** The paper identifies both opportunities and challenges in the orchestration and engineering of AI-enhanced body tracking systems.

**Limitations:** 

**Conclusion:** The authors conclude that using readily available AI and tracking technologies can facilitate the creation of innovative interactive systems, although careful consideration is needed for their integration.

**Abstract:** The automated analysis of human behaviour provides many opportunities for the creation of interactive systems and the post-experiment investigations for user studies. Commodity depth cameras offer reasonable body tracking accuracy at a low price point, without the need for users to wear or hold any extra equipment. The resulting systems typically perform body tracking through a dedicated machine learning model, but they can be enhanced with additional AI components providing extra capabilities. This leads to opportunities but also challenges, for example regarding the orchestration of such AI components and the engineering of the resulting tracking pipeline. In this paper, we discuss these elements, based on our experience with the creation of a remote collaboration system across distant wall-sized displays, that we built using existing and readily available building blocks, including AI-based recognition models.

</details>


### [9] [5 Days, 5 Stories: Using Technology to Promote Empathy in the Workplace](https://arxiv.org/abs/2506.19495)

*Russell Beale, Eugenia Sergueeva*

**Main category:** cs.HC

**Keywords:** empathy, digital storytelling, workplace communication, emotional intelligence, collaboration

**Relevance Score:** 6

**TL;DR:** The study investigates a digital storytelling platform aimed at enhancing empathy among employees.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop empathic skills and improve collaboration and communication in the workplace through digital means.

**Method:** A one-week intervention using the 'In Your Shoes' platform with a mixed methods approach, assessing empathy quantitatively and qualitatively.

**Key Contributions:**

	1. Development of a digital platform for empathy training
	2. Mixed methods approach in workplace empathy research
	3. Insights into emotional intelligence in workplace technology design

**Result:** Quantitative analysis showed no significant change in dispositional empathy, but qualitative findings indicated improvements in situational empathy and workplace relationships.

**Limitations:** Quantitative results were not statistically significant, focusing mainly on situational rather than dispositional empathy improvements.

**Conclusion:** Digital storytelling tools can facilitate empathic engagement in professional settings, providing emotional support and enhancing interpersonal connections.

**Abstract:** Empathy is widely recognized as a vital attribute for effective collaboration and communication in the workplace, yet developing empathic skills and fostering it among colleagues remains a challenge. This study explores the potential of a collaborative digital storytelling platform - In Your Shoes - designed to promote empathic listening and interpersonal understanding through the structured exchange of personal narratives. A one-week intervention was conducted with employees from multiple organizations using the platform. Employing a mixed methods approach, we assessed quantitative changes in empathy using the Empathy Quotient (EQ) and qualitatively analyzed participant experiences through grounded theory. While quantitative analysis revealed no statistically significant shift in dispositional empathy, qualitative findings suggested the tool facilitated situational empathy, prompted self-reflection, improved emotional resonance, and enhanced workplace relationships. Participants reported feelings of psychological safety, connection, and, in some cases, therapeutic benefits from sharing and responding to stories. These results highlight the promise of asynchronous, structured narrative-based digital tools for supporting empathic engagement in professional settings, offering insights for the design of emotionally intelligent workplace technologies.

</details>


### [10] [Examination of Eye-Tracking, Head-Gaze, and Controller-Based Ray-casting in TMT-VR: Performance and Usability Across Adulthood](https://arxiv.org/abs/2506.19519)

*Panagiotis Kourtesis, Evgenia Giatzoglou, Panagiotis Vorias, Katerina Alkisti Gounari, Eleni Orfanidou, Chrysanthi Nega*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Neuropsychological Testing, Input Modality, Gaze-based Interaction, User Experience

**Relevance Score:** 7

**TL;DR:** This study investigates the effectiveness of various input methods in a VR neuropsychological test across different age groups, revealing that gaze-based controls outperform manual ones in accuracy and speed under different task demands.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the ergonomic trade-offs of input modes in virtual reality for neuropsychological testing and assess their impact based on age.

**Method:** Seventy-seven volunteers participated in a VR Trail-Making Test using eye-tracking, head-gaze, and a hand controller. Performance metrics including completion time, spatial accuracy, and error counts were analyzed, alongside subjective usability and experience ratings.

**Key Contributions:**

	1. Demonstrated superior performance of gaze-based controls in VR over manual methods.
	2. Identified age-related differences in VR performance metrics.
	3. Provided insights into usability perceptions of different input modalities.

**Result:** Younger adults performed better than middle-aged participants, with eye-tracking providing the best accuracy and speed on easy tasks, while head-gaze excelled in more complex tasks. Controllers were less effective across all measures.

**Limitations:** Findings may require age-specific norms for broader application.

**Conclusion:** Gaze-based methods are advantageous for VR assessments, but optimal input choice varies with task complexity, suggesting the need for tailored approaches based on user characteristics.

**Abstract:** Virtual reality (VR) can enrich neuropsychological testing, yet the ergonomic trade-offs of its input modes remain under-examined. Seventy-seven healthy volunteers-young (19-29 y) and middle-aged (27-56 y)-completed a VR Trail-Making Test with three pointing methods: eye-tracking, head-gaze, and a six-degree-of-freedom hand controller. Completion time, spatial accuracy, and error counts for the simple (Trail A) and alternating (Trail B) sequences were analysed in 3 x 2 x 2 mixed-model ANOVAs; post-trial scales captured usability (SUS), user experience (UEQ-S), and acceptability. Age dominated behaviour: younger adults were reliably faster, more precise, and less error-prone. Against this backdrop, input modality mattered. Eye-tracking yielded the best spatial accuracy and shortened Trail A time relative to manual control; head-gaze matched eye-tracking on Trail A speed and became the quickest, least error-prone option on Trail B. Controllers lagged on every metric. Subjective ratings were high across the board, with only a small usability dip in middle-aged low-gamers. Overall, gaze-based ray-casting clearly outperformed manual pointing, but optimal choice depended on task demands: eye-tracking maximised spatial precision, whereas head-gaze offered calibration-free enhanced speed and error-avoidance under heavier cognitive load. TMT-VR appears to be accurate, engaging, and ergonomically adaptable assessment, yet it requires age-specific-stratified norms.

</details>


### [11] [Beyond Wellbeing Apps: Co-Designing Immersive, Embodied, and Collective Digital Wellbeing Interventions for Healthcare Professionals](https://arxiv.org/abs/2506.19524)

*Zheyuan Zhang, Jingjing Sun, Dorian Peters, Rafael A. Calvo*

**Main category:** cs.HC

**Keywords:** healthcare professionals, wellbeing technologies, co-design study, virtual reality, augmented reality

**Relevance Score:** 8

**TL;DR:** This study investigates healthcare professionals' perceptions of wellbeing technologies such as VR, AR, and embodied interfaces, highlighting their potential for mental health support and providing design recommendations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Healthcare professionals experience significant stress and burnout, and traditional interventions may not fully address their needs. Emerging technologies offer new ways to enhance wellbeing.

**Method:** The study conducted a two-phase co-design with 26 healthcare professionals, focusing on idea generation, concept evaluation, prototype testing, and design iteration to gather insights on their perceptions of wellbeing technologies.

**Key Contributions:**

	1. Exploration of HCPs' perceptions of innovative wellbeing technologies.
	2. Identification of key features valued by HCPs in wellbeing interventions.
	3. Specific design recommendations to improve user engagement in wellbeing technologies.

**Result:** Healthcare professionals expressed strong appreciation for technologies that foster immersive, embodied, and collective experiences to enhance mental health support.

**Limitations:** 

**Conclusion:** The research identifies key design recommendations for wellbeing technologies, emphasizing the importance of meeting HCPs' needs for autonomy, competence, and relatedness to sustain engagement.

**Abstract:** Healthcare professionals (HCPs) face increasing levels of stress and burnout. Technological wellbeing interventions provide accessible and flexible support for HCPs. While most studies have focused on mobile- and web-based programs, alternative technologies like virtual reality (VR), augmented reality (AR), tangible interfaces, and embodied technologies are emerging as engaging and effective tools for wellbeing interventions. However, there is still a lack of research on how such technologies are perceived among HCPs. This study explored HCPs' perceptions and preferences for various types of wellbeing technologies, by conducting a 2-phase co-design study involving 26 HCPs in idea generation, concept evaluation, prototype testing, and design iteration. From our findings, HCPs highly valued the potential of technologies to support mental health with immersive, embodied, and collective experiences. Furthermore, we provided design recommendations for wellbeing technologies for HCPs that sustain user engagement by meeting their needs for autonomy, competence, and relatedness in the experiences.

</details>


### [12] [Filters of Identity: AR Beauty and the Algorithmic Politics of the Digital Body](https://arxiv.org/abs/2506.19611)

*Miriam Doh, Corinna Canali, Nuria Oliver*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Beauty Filters, Body Politics, HCI, Algorithmic Bias

**Relevance Score:** 7

**TL;DR:** This position paper discusses the impact of AR beauty filters within Body Politics in HCI, arguing they perpetuate harmful beauty standards.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the influence of AR beauty filters as technologies that reinforce societal norms related to race, gender, and ability through algorithmic bias.

**Method:** Analyzing the naming conventions and algorithmic governance associated with AR beauty filters.

**Key Contributions:**

	1. Argues that AR beauty filters are technologies of governance.
	2. Highlights the role of algorithmic biases in imposing beauty standards.
	3. Advocates for transparency-driven interventions.

**Result:** Identifies that AR beauty filters impose aesthetic norms and conceal their impact on users.

**Limitations:** 

**Conclusion:** Calls for greater transparency in the design and governance of algorithmic aesthetics and urges a rethinking of digital embodiment.

**Abstract:** This position paper situates AR beauty filters within the broader debate on Body Politics in HCI. We argue that these filters are not neutral tools but technologies of governance that reinforce racialized, gendered, and ableist beauty standards. Through naming conventions, algorithmic bias, and platform governance, they impose aesthetic norms while concealing their influence. To address these challenges, we advocate for transparency-driven interventions and a critical rethinking of algorithmic aesthetics and digital embodiment.

</details>


### [13] [Varif.ai to Vary and Verify User-Driven Diversity in Scalable Image Generation](https://arxiv.org/abs/2506.19644)

*M. Michelessa, J. Ng, C. Hurter, B. Y. Lim*

**Main category:** cs.HC

**Keywords:** image generation, diversity control, text-to-image, human-computer interaction, large language models

**Relevance Score:** 6

**TL;DR:** Varif.ai is a tool designed to enhance diversity in text-to-image generation based on user-specified attributes, demonstrating improved effectiveness over existing methods in controlled evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of diversity in image generation by tailoring outputs to distinct user preferences and needs.

**Method:** Varif.ai employs a combination of text-to-image generation and Large Language Models to iteratively create images, verify attribute coverage, and introduce variation in attributes based on user input.

**Key Contributions:**

	1. Introduction of user-driven diversity control in image generation
	2. Use of Large Language Models to support iterative image generation
	3. Demonstrated improved effectiveness in achieving diverse image sets compared to conventional methods.

**Result:** In a controlled study with 20 participants, Varif.ai was found to be more effective in generating diverse image sets compared to baseline methods.

**Limitations:** 

**Conclusion:** Varif.ai supports user-driven diversity control in image generation, enhancing creativity and scalability in ideation processes.

**Abstract:** Diversity in image generation is essential to ensure fair representations and support creativity in ideation. Hence, many text-to-image models have implemented diversification mechanisms. Yet, after a few iterations of generation, a lack of diversity becomes apparent, because each user has their own diversity goals (e.g., different colors, brands of cars), and there are diverse attributions to be specified. To support user-driven diversity control, we propose Varif.ai that employs text-to-image and Large Language Models to iteratively i) (re)generate a set of images, ii) verify if user-specified attributes have sufficient coverage, and iii) vary existing or new attributes. Through an elicitation study, we uncovered user needs for diversity in image generation. A pilot validation showed that Varif.ai made achieving diverse image sets easier. In a controlled evaluation with 20 participants, Varif.ai proved more effective than baseline methods across various scenarios. Thus, this supports user control of diversity in image generation for creative ideation and scalable image generation.

</details>


### [14] [Interrogating AI: Characterizing Emergent Playful Interactions with ChatGPT](https://arxiv.org/abs/2401.08405)

*Mohammad Ronagh Nikghalb, Jinghui Cheng*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, AI, Playful Interactions, ChatGPT, CSCW

**Relevance Score:** 9

**TL;DR:** This study investigates playful interactions in user discourse surrounding ChatGPT, revealing that over half of the discussion involves playfulness and establishing a preliminary framework for categorizing these interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in understanding how users engage with AI technologies through playful interactions, particularly in the context of HCI and CSCW.

**Method:** The authors conducted a thematic analysis of 372 user-generated posts from the ChatGPT subreddit to identify and categorize instances of playful interactions.

**Key Contributions:**

	1. Identification of playful interaction types with AI
	2. Development of a framework for categorizing playful interactions
	3. Insights into user engagement with AI technology

**Result:** The analysis revealed that 54% of the posts featured playful interactions, which were categorized into six types: reflecting, jesting, imitating, challenging, tricking, and contriving.

**Limitations:** 

**Conclusion:** The study contributes to HCI and CSCW by offering insights into how playful interactions can aid users in understanding AI, shaping human-AI relationships, and informing AI design.

**Abstract:** In an era of AI's growing capabilities and influences, recent advancements are reshaping HCI and CSCW's view of AI. Playful interactions emerged as an important way for users to make sense of the ever-changing AI technologies, yet remained underexamined. We target this gap by investigating playful interactions exhibited by users of a popular AI technology, ChatGPT. Through a thematic analysis of 372 user-generated posts on the ChatGPT subreddit, we found that more than half (54\%) of user discourse revolved around playful interactions. The analysis further allowed us to construct a preliminary framework to describe these interactions, categorizing them into six types: reflecting, jesting, imitating, challenging, tricking, and contriving; each included sub-categories. This study contributes to HCI and CSCW by identifying the diverse ways users engage in playful interactions with AI. It examines how these interactions can help users understand AI's agency, shape human-AI relationships, and provide insights for designing AI systems.

</details>


### [15] [M3D: Manifold-based Domain Adaptation with Dynamic Distribution for Non-Deep Transfer Learning in Cross-subject and Cross-session EEG-based Emotion Recognition](https://arxiv.org/abs/2404.15615)

*Ting Luo, Jing Zhang, Yingwei Qiu, Li Zhang, Yaohua Hu, Zhuliang Yu, Zhen Liang*

**Main category:** cs.HC

**Keywords:** Electroencephalography, emotion recognition, transfer learning, aBCI, Machine Learning

**Relevance Score:** 7

**TL;DR:** M3D is a lightweight, non-deep learning framework for emotion decoding using EEG that overcomes challenges of EEG's non-stationarity and data limitations, achieving improved performance in emotion recognition tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of deep learning methods in emotion decoding using EEG-based aBCIs, particularly related to computational resources and data requirements.

**Method:** M3D consists of manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning, optimizing performance on EEG datasets.

**Key Contributions:**

	1. Proposed a lightweight aBCI framework (M3D) for emotion recognition
	2. Achieved deep learning-level performance with reduced data requirements
	3. Introduced dynamic distribution alignment for improved adaptation efficiency

**Result:** M3D outperforms traditional non-deep learning methods by 4.47% on average and achieves deep learning-level performance with reduced data and computation needs.

**Limitations:** 

**Conclusion:** M3D demonstrates potential for real-world applications in affective computing by efficiently adapting across diverse EEG datasets.

**Abstract:** Emotion decoding using Electroencephalography (EEG)-based affective brain-computer interfaces (aBCIs) plays a crucial role in affective computing but is limited by challenges such as EEG's non-stationarity, individual variability, and the high cost of large labeled datasets. While deep learning methods are effective, they require extensive computational resources and large data volumes, limiting their practical application. To overcome these issues, we propose Manifold-based Domain Adaptation with Dynamic Distribution (M3D), a lightweight, non-deep transfer learning framework. M3D consists of four key modules: manifold feature transformation, dynamic distribution alignment, classifier learning, and ensemble learning. The data is mapped to an optimal Grassmann manifold space, enabling dynamic alignment of source and target domains. This alignment is designed to prioritize both marginal and conditional distributions, improving adaptation efficiency across diverse datasets. In classifier learning, the principle of structural risk minimization is applied to build robust classification models. Additionally, dynamic distribution alignment iteratively refines the classifier. The ensemble learning module aggregates classifiers from different optimization stages to leverage diversity and enhance prediction accuracy. M3D is evaluated on two EEG emotion recognition datasets using two validation protocols (cross-subject single-session and cross-subject cross-session) and a clinical EEG dataset for Major Depressive Disorder (MDD). Experimental results show that M3D outperforms traditional non-deep learning methods with a 4.47% average improvement and achieves deep learning-level performance with reduced data and computational requirements, demonstrating its potential for real-world aBCI applications.

</details>


### [16] [Exploring the Collaborative Co-Creation Process with AI: A Case Study in Novice Music Production](https://arxiv.org/abs/2501.15276)

*Yue Fu, Michele Newman, Lewis Going, Qiuzi Feng, Jin Ha Lee*

**Main category:** cs.HC

**Keywords:** AI, Human-AI Collaboration, Creative Processes, Music Creation, Co-Creation Models

**Relevance Score:** 6

**TL;DR:** This study explores how AI tools facilitate the creative process in group settings among novice users, focusing on music creation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the co-creative processes involving AI in group settings, particularly with novice users, which are underexplored.

**Method:** A case study was conducted in a college course where nine undergraduate students used AI tools to create three original music tracks over 10 weeks.

**Key Contributions:**

	1. Identification of a new 'collaging and refinement' stage in creative workflows
	2. Proposed models for understanding Human-AI collaboration
	3. Insights into group dynamics and role division in AI-assisted creativity

**Result:** AI tools accelerated ideation and introduced a 'collaging and refinement' stage, influencing creative workflows and group dynamics.

**Limitations:** 

**Conclusion:** The study proposes the Human-AI Co-Creation Stage Model and the Human-AI Agency Model to understand collaborative co-creation with AI.

**Abstract:** Artificial intelligence is reshaping creative domains, yet its co-creative processes, especially in group settings with novice users, remain under explored. To bridge this gap, we conducted a case study in a college-level course where nine undergraduate students were tasked with creating three original music tracks using AI tools over 10 weeks. The study spanned the entire creative journey from ideation to releasing these songs on Spotify. Participants leveraged AI for music and lyric production, cover art, and distribution. Our findings highlight how AI transforms creative workflows: accelerating ideation but compressing the traditional preparation stage, and requiring novices to navigate a challenging idea selection and validation phase. We also identified a new "collaging and refinement" stage, where participants creatively combined diverse AI-generated outputs into cohesive works. Furthermore, AI influenced group social dynamics and role division among human creators. Based on these insights, we propose the Human-AI Co-Creation Stage Model and the Human-AI Agency Model, offering new perspectives on collaborative co-creation with AI.

</details>


### [17] [AI-Facilitated Episodic Future Thinking For Adults with Obesity](https://arxiv.org/abs/2503.16484)

*Sareh Ahmadi, Michelle Rockwell, Megan Stuart, Nicki Rohani, Allison Tegge, Xuan Wang, Jeffrey Stein, Edward A. Fox*

**Main category:** cs.HC

**Keywords:** Episodic Future Thinking, AI chatbot, health behavior change, user experience, large language model

**Relevance Score:** 7

**TL;DR:** EFTeacher is an AI chatbot designed to promote Episodic Future Thinking (EFT) for improving health behaviors by providing personalized cues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce delay discounting and promote behavior change in users with lifestyle-related health conditions through AI-supported interventions.

**Method:** A mixed-methods study was conducted, involving usability assessments, user evaluations through content characteristics questionnaires, and semi-structured interviews with participants.

**Key Contributions:**

	1. Introduction of EFTeacher, an AI chatbot for promoting EFT
	2. Mixed-methods evaluation of usability and participant perceptions
	3. Insights into user interactions and areas for improvement

**Result:** Participants found EFTeacher communicative and supportive, aiding in imaginative thinking about future goals, though some reported issues like repetitive dialogue.

**Limitations:** Challenges included repetitive dialogue and verbosity in responses.

**Conclusion:** The study suggests that large language model-based chatbots like EFTeacher can be effective for EFT interventions in addressing maladaptive health behaviors.

**Abstract:** Episodic Future Thinking (EFT) involves vividly imagining personal future events and experiences in detail. It has shown promise as an intervention to reduce delay discounting-the tendency to devalue delayed rewards in favor of immediate gratification- and to promote behavior change in a range of maladaptive health behaviors. We present EFTeacher, an AI chatbot powered by the GPT-4-Turbo large language model, designed to generate EFT cues for users with lifestyle-related conditions. To evaluate the feasibility and usability of EFTeacher, we conducted a mixed-methods study that included usability assessments, user evaluations based on content characteristics questionnaires, and semi-structured interviews. Qualitative findings indicate that participants perceived EFTeacher as communicative and supportive through an engaging dialogue. The chatbot facilitated imaginative thinking and reflection on future goals. Participants appreciated its adaptability and personalization features, though some noted challenges such as repetitive dialogue and verbose responses. Our findings underscore the potential of large language model-based chatbots in EFT interventions targeting maladaptive health behaviors.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [18] [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919)

*Hexiang Gu, Qifan Yu, Saihui Hou, Zhiqin Fang, Huijia Wu, Zhaofeng He*

**Main category:** cs.CL

**Keywords:** harmful memes, dataset, detection framework, Chain-of-Thought, multimodal

**Relevance Score:** 4

**TL;DR:** MemeMind is a comprehensive dataset for harmful meme detection, coupled with the MemeGuard framework that enhances detection accuracy through multimodal information integration and reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of harmful content in social media necessitates effective detection methods, yet existing datasets lack scale, diversity, and explainability.

**Method:** Introduction of the MemeMind dataset, featuring bilingual support and Chain-of-Thought annotations, alongside the MemeGuard detection framework that integrates multimodal information.

**Key Contributions:**

	1. Introduction of the MemeMind dataset with bilingual support and comprehensive annotations.
	2. Development of the MemeGuard detection framework that incorporates reasoning processes.
	3. Demonstrated improvements over existing methods in harmful meme detection tasks.

**Result:** Extensive experiments show that MemeGuard outperforms state-of-the-art methods in harmful meme detection using the MemeMind dataset.

**Limitations:** Limited to harmful memes and may not generalize to other multimedia content types.

**Conclusion:** MemeMind and MemeGuard represent significant advancements in the automatic detection of harmful memes, paving the way for future research in this area.

**Abstract:** The rapid development of social media has intensified the spread of harmful content. Harmful memes, which integrate both images and text, pose significant challenges for automated detection due to their implicit semantics and complex multimodal interactions. Although existing research has made progress in detection accuracy and interpretability, the lack of a systematic, large-scale, diverse, and highly explainable dataset continues to hinder further advancement in this field. To address this gap, we introduce MemeMind, a novel dataset featuring scientifically rigorous standards, large scale, diversity, bilingual support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations. MemeMind fills critical gaps in current datasets by offering comprehensive labeling and explicit reasoning traces, thereby providing a solid foundation for enhancing harmful meme detection. In addition, we propose an innovative detection framework, MemeGuard, which effectively integrates multimodal information with reasoning process modeling, significantly improving models' ability to understand and identify harmful memes. Extensive experiments conducted on the MemeMind dataset demonstrate that MemeGuard consistently outperforms existing state-of-the-art methods in harmful meme detection tasks.

</details>


### [19] [Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge](https://arxiv.org/abs/2506.18998)

*Sahil Kale, Vijaykant Nadadur*

**Main category:** cs.CL

**Keywords:** LLM, memorization, self-knowledge, trustworthiness, AI explainability

**Relevance Score:** 8

**TL;DR:** The study examines the relationship between memorization and self-knowledge in LLMs, revealing significant flaws in their reasoning capabilities, particularly in STEM domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the intertwined issues of memorization and self-knowledge deficits in LLMs that affect the trustworthiness of their responses.

**Method:** The authors employ a novel framework to analyze LLMs' reasoning by examining their reliance on memorized solutions and the resulting inconsistencies in their self-assessments of reasoning ability.

**Key Contributions:**

	1. Introduces a novel framework for analyzing LLM reasoning patterns.
	2. Demonstrates a significant problem with generalization in LLMs influenced by memorization.
	3. Highlights critical flaws in LLM self-knowledge relevant to AI trustworthiness.

**Result:** The analysis reveals over 45% inconsistency in feasibility assessments by LLMs when faced with logically coherent task changes, especially pronounced in science and medicine contexts.

**Limitations:** The study primarily focuses on LLM performance in STEM domains, and broader implications in other areas are not explored.

**Conclusion:** Current LLM architectures and training practices lead to flawed self-knowledge, necessitating new methodologies for enhancing AI explainability and trustworthiness.

**Abstract:** When artificial intelligence mistakes memorization for intelligence, it creates a dangerous mirage of reasoning. Existing studies treat memorization and self-knowledge deficits in LLMs as separate issues and do not recognize an intertwining link that degrades the trustworthiness of LLM responses. In our study, we utilize a novel framework to ascertain if LLMs genuinely learn reasoning patterns from training data or merely memorize them to assume competence across problems of similar complexity focused on STEM domains. Our analysis shows a noteworthy problem in generalization: LLMs draw confidence from memorized solutions to infer a higher self-knowledge about their reasoning ability, which manifests as an over 45% inconsistency in feasibility assessments when faced with self-validated, logically coherent task perturbations. This effect is most pronounced in science and medicine domains, which tend to have maximal standardized jargon and problems, further confirming our approach. Significant wavering within the self-knowledge of LLMs also shows flaws in current architectures and training patterns, highlighting the need for techniques that ensure a balanced, consistent stance on models' perceptions of their own knowledge for maximum AI explainability and trustworthiness. Our code and results are available publicly at https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.

</details>


### [20] [Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations](https://arxiv.org/abs/2506.19004)

*Brian Siyuan Zheng, Alisa Liu, Orevaoghene Ahia, Jonathan Hayase, Yejin Choi, Noah A. Smith*

**Main category:** cs.CL

**Keywords:** language models, tokenization, robustness, instruction tuning, performance enhancement

**Relevance Score:** 8

**TL;DR:** This paper examines how language models (LMs) perform with non-canonical tokenizations, finding that many models maintain high performance and can even benefit from certain non-standard tokenization strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the robustness of language models to unseen non-canonical tokenizations and their effects on performance.

**Method:** Analyzed LMs across 20 benchmarks to assess performance retention with non-canonical tokenizations, specifically character-level tokenization and randomly sampled tokenization methods.

**Key Contributions:**

	1. Demonstrated high performance retention of LMs with non-canonical tokenizations.
	2. Identified scenarios where non-canonical tokenization enhances performance.
	3. Explored differences between base and post-trained models in processing non-canonical tokenizations.

**Result:** Instruction-tuned models maintain up to 93.4% performance with random tokenization and show improvements in specific tasks with alternative tokenization approaches, such as up to +14% in string manipulation and +33% in large-number arithmetic.

**Limitations:** 

**Conclusion:** Models show robustness to non-canonical tokenizations, indicating they are less reliant on their standard tokenizer, and suggest possible performance enhancements through tokenization modifications at inference.

**Abstract:** Modern tokenizers employ deterministic algorithms to map text into a single "canonical" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the tokenizer vocabulary. In this work, we investigate the robustness of LMs to text encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4% of their original performance when given a randomly sampled tokenization, and 90.8% with character-level tokenization. We see that overall stronger models tend to be more robust, and robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we then identify settings where non-canonical tokenization schemes can *improve* performance, finding that character-level segmentation improves string manipulation and code understanding tasks by up to +14%, and right-aligned digit grouping enhances large-number arithmetic by +33%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We show that while both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings), base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less tied to their tokenizer than previously believed, and demonstrate the promise of intervening on tokenization at inference time to boost performance.

</details>


### [21] [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)

*Weijie Xu, Yiwen Wang, Chi Xue, Xiangkun Hu, Xi Fang, Guimin Dong, Chandan K. Reddy*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fairness Evaluation, Semantic Bias Detection, Statistical Framework, Counterfactual Fairness

**Relevance Score:** 9

**TL;DR:** The paper proposes FiSCo, a novel statistical framework for evaluating group-level fairness in Large Language Models (LLMs) by detecting subtle semantic biases in long-form responses across demographic groups.

**Read time:** 29 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of inherent biases in LLM-generated responses and the shortcomings of existing evaluation methods that often overlook these biases, particularly in long-form outputs.

**Method:** FiSCo operates at the claim level, assessing semantic differences in LLM outputs by decomposing responses into semantically distinct claims and applying statistical hypothesis testing to compare similarities among demographic groups.

**Key Contributions:**

	1. Introduction of FiSCo for evaluating group-level fairness in LLMs.
	2. Decomposition of model outputs into semantically distinct claims for better bias detection.
	3. Validation of the framework on diverse datasets including gender, race, and age.

**Result:** FiSCo reliably identifies nuanced biases in LLM outputs and outperforms existing evaluation metrics by reducing the impact of stochastic variability in LLM responses.

**Limitations:** 

**Conclusion:** The framework enhances the reliability of LLMs by providing a more robust means of detecting biases that affect fairness across demographic groups.

**Abstract:** Large Language Models (LLMs) often generate responses with inherent biases, undermining their reliability in real-world applications. Existing evaluation methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs. To address these challenges, we propose FiSCo(Fine-grained Semantic Computation), a novel statistical framework to evaluate group-level fairness in LLMs by detecting subtle semantic differences in long-form responses across demographic groups. Unlike prior work focusing on sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis by operating at the claim level, leveraging entailment checks to assess the consistency of meaning across responses. We decompose model outputs into semantically distinct claims and apply statistical hypothesis testing to compare inter- and intra-group similarities, enabling robust detection of subtle biases. We formalize a new group counterfactual fairness definition and validate FiSCo on both synthetic and human-annotated datasets spanning gender, race, and age. Experiments show that FiSco more reliably identifies nuanced biases while reducing the impact of stochastic LLM variability, outperforming various evaluation metrics.

</details>


### [22] [Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)

*Omer Luxembourg, Haim Permuter, Eliya Nachmani*

**Main category:** cs.CL

**Keywords:** Masked Diffusion Language Models, Unmasking Strategy, Efficiency in Text Generation

**Relevance Score:** 7

**TL;DR:** Introduces the Dilated-scheduled Unmasking Strategy (DUS) for efficient text generation using Masked Diffusion Language Models (MDLM) that improves inference speed without additional training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address limitations of existing samplers in Masked Diffusion Language Models, particularly their inability to effectively handle parallel unmasking and dependency management among tokens.

**Method:** DUS employs a first-order Markov assumption to partition sequences into dilation-based groups, allowing for independent, parallel unmasking that considers local context.

**Key Contributions:**

	1. Introduction of the Dilated-scheduled Unmasking Strategy (DUS)
	2. Reduction in denoiser calls from O(B) to O(log B)
	3. Improvement in text generation quality on specified benchmarks

**Result:** DUS shows significant speedup in inference time, reducing denoiser calls to O(log B) compared to O(B) of traditional models, while improving performance on benchmarks like GSM8K and code completion datasets.

**Limitations:** 

**Conclusion:** DUS provides a lightweight and efficient solution for text generation that enhances the capabilities of MDLMs without requiring additional training.

**Abstract:** Masked diffusion language models (MDLM) have shown strong promise for non-autoregressive text generation, yet existing samplers act as implicit planners, selecting tokens to unmask via denoiser confidence or entropy scores. Such heuristics falter under parallel unmasking - they ignore pairwise interactions between tokens and cannot account for dependencies when unmasking multiple positions at once, limiting their inference time to traditional auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking Strategy (DUS), an inference-only, planner-model-free method that requires no additional training. DUS leverages a first-order Markov assumption to partition sequence positions into dilation-based groups of non-adjacent tokens, enabling independent, parallel unmasking steps that respect local context that minimizes the joint entropy of each iteration step. Unlike semi-AR block approaches (e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces the number of denoiser calls to O(log B) per generation block - yielding substantial speedup over the O(B) run time of state-of-the-art diffusion models, where B is the block size in the semi-AR inference process. In experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks - domains suited to non-ordinal generation - DUS improves scores over parallel confidence-based planner, without modifying the underlying denoiser. DUS offers a lightweight, budget-aware approach to efficient, high-quality text generation, paving the way to unlock the true capabilities of MDLMs.

</details>


### [23] [NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching](https://arxiv.org/abs/2506.19058)

*Mike Zhang, Rob van der Goot*

**Main category:** cs.CL

**Keywords:** job title matching, skill prediction, multilingual, NLP, TalentCLEF 2025

**Relevance Score:** 6

**TL;DR:** This report presents NLPnorth's submission to TalentCLEF 2025, focusing on multilingual job title matching and skill prediction using various NLP techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance automatic candidate matching, career path prediction, and job market analysis through job title alignment and corresponding skills.

**Method:** The study employs (fine-tuned) classification-based, contrastive-based, and prompting methods for two main tasks: Multilingual Job Title Matching and Job Title-Based Skill Prediction.

**Key Contributions:**

	1. Introduced effective methodology for job title matching and skill prediction
	2. Utilized a multilingual approach across English, Spanish, and German
	3. Demonstrated the effectiveness of large language models in job-related NLP tasks.

**Result:** The prompting method achieved the highest performance on job title matching with an average precision of 0.492, while the classification-based approach scored 0.290 for skill prediction.

**Limitations:** 

**Conclusion:** The largest multilingual language models yielded the best results for both tasks, securing 5th/20 for Task A and 3rd/14 for Task B among competing teams.

**Abstract:** Matching job titles is a highly relevant task in the computational job market domain, as it improves e.g., automatic candidate matching, career path prediction, and job market analysis. Furthermore, aligning job titles to job skills can be considered an extension to this task, with similar relevance for the same downstream tasks. In this report, we outline NLPnorth's submission to TalentCLEF 2025, which includes both of these tasks: Multilingual Job Title Matching, and Job Title-Based Skill Prediction. For both tasks we compare (fine-tuned) classification-based, (fine-tuned) contrastive-based, and prompting methods. We observe that for Task A, our prompting approach performs best with an average of 0.492 mean average precision (MAP) on test data, averaged over English, Spanish, and German. For Task B, we obtain an MAP of 0.290 on test data with our fine-tuned classification-based approach. Additionally, we made use of extra data by pulling all the language-specific titles and corresponding \emph{descriptions} from ESCO for each job and skill. Overall, we find that the largest multilingual language models perform best for both tasks. Per the provisional results and only counting the unique teams, the ranking on Task A is 5$^{\text{th}}$/20 and for Task B 3$^{\text{rd}}$/14.

</details>


### [24] [MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation](https://arxiv.org/abs/2506.19073)

*Jackson Trager, Francielle Vargas, Diego Alves, Matteo Guida, Mikel K. Ngueajio, Ameeta Agrawal, Flor Plaza-del-Arco, Yalda Daryanai, Farzan Karimi-Malekabadi*

**Main category:** cs.CL

**Keywords:** Moral Foundation Theory, Large Language Models, Hate Speech, Multilingual Benchmark, Moral Reasoning

**Relevance Score:** 8

**TL;DR:** This paper presents MFTCXplain, a multilingual benchmark for evaluating the moral reasoning of LLMs, revealing significant misalignments in moral sentiment prediction.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses concerns regarding the moral reasoning capabilities of LLMs in socially sensitive contexts and the limitations of current evaluation benchmarks.

**Method:** Introducing a multilingual dataset of 3,000 tweets annotated with labels for hate speech, moral categories, and rationales, allowing for a comprehensive assessment of LLMs' moral reasoning skills.

**Key Contributions:**

	1. Development of MFTCXplain, a multilingual moral reasoning benchmark
	2. Demonstration of LLMs' limited ability to predict moral sentiments
	3. Provision of dataset annotations that enhance interpretability

**Result:** LLMs achieve an F1 score of up to 0.836 in hate speech detection but struggle with moral sentiment prediction (F1 < 0.35), particularly in underrepresented languages.

**Limitations:** The dataset may not encompass all cultural nuances, and the assessment primarily focuses on a limited set of languages and contexts.

**Conclusion:** This study illustrates the insufficiency of LLMs in capturing and expressing human moral reasoning, highlighting the need for improved models and evaluation methods.

**Abstract:** Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is a growing concern as these systems are used in socially sensitive tasks. Nevertheless, current evaluation benchmarks present two major shortcomings: a lack of annotations that justify moral classifications, which limits transparency and interpretability; and a predominant focus on English, which constrains the assessment of moral reasoning across diverse cultural settings. In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for evaluating the moral reasoning of LLMs via hate speech multi-hop explanation using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across Portuguese, Italian, Persian, and English, annotated with binary hate speech labels, moral categories, and text span-level rationales. Empirical results highlight a misalignment between LLM outputs and human annotations in moral reasoning tasks. While LLMs perform well in hate speech detection (F1 up to 0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35). Furthermore, rationale alignment remains limited mainly in underrepresented languages. These findings show the limited capacity of current LLMs to internalize and reflect human moral reasoning.

</details>


### [25] [Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting](https://arxiv.org/abs/2506.19089)

*Nathaniel Getachew, Abulhair Saparov*

**Main category:** cs.CL

**Keywords:** Theory of Mind, World Modeling, Large Language Models, Synthetic Story Generation, Heuristic Behavior

**Relevance Score:** 8

**TL;DR:** This paper presents StorySim, a framework for generating synthetic stories to evaluate theory of mind and world modeling capabilities of large language models.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the theory of mind and world modeling capabilities of large language models without contamination from pretraining data.

**Method:** The framework generates highly controllable story prompts using a Storyboard, allowing for precise manipulation of character perspectives and events, and includes tasks for testing ToM and WM.

**Key Contributions:**

	1. Introduction of StorySim for story generation
	2. Control over character perspectives and events
	3. Findings on LLM performance in ToM vs WM tasks

**Result:** Experiments reveal that state-of-the-art LLMs perform better on world modeling tasks than theory of mind tasks, with better performance when reasoning with humans over inanimate objects.

**Limitations:** 

**Conclusion:** The framework provides evidence of heuristic behaviors in LLMs, such as recency bias. All code is available for public use.

**Abstract:** We introduce $\texttt{StorySim}$, a programmable framework for synthetically generating stories to evaluate the theory of mind (ToM) and world modeling (WM) capabilities of large language models (LLMs). Unlike prior benchmarks that may suffer from contamination in pretraining data, $\texttt{StorySim}$ produces novel, compositional story prompts anchored by a highly controllable $\texttt{Storyboard}$, enabling precise manipulation of character perspectives and events. We use this framework to design first- and second-order ToM tasks alongside WM tasks that control for the ability to track and model mental states. Our experiments across a suite of state-of-the-art LLMs reveal that most models perform better on WM tasks than ToM tasks, and that models tend to perform better reasoning with humans compared to inanimate objects. Additionally, our framework enabled us to find evidence of heuristic behavior such as recency bias and an over-reliance on earlier events in the story. All code for generating data and evaluations is freely available.

</details>


### [26] [Human-Aligned Faithfulness in Toxicity Explanations of LLMs](https://arxiv.org/abs/2506.19113)

*Ramaravind K. Mothilal, Joanna Roy, Syed Ishtiaque Ahmed, Shion Guha*

**Main category:** cs.CL

**Keywords:** toxicity, LLMs, explainability, Human-Aligned Faithfulness, NLP

**Relevance Score:** 9

**TL;DR:** This paper evaluates LLMs' reasoning about toxicity through their explanations, proposing a metric called Human-Aligned Faithfulness (HAF) for assessment.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the trustworthiness of LLMs in evaluating toxicity by focusing on their explanation capabilities rather than just detection tasks.

**Method:** The authors developed a multi-dimensional criterion called Human-Aligned Faithfulness (HAF), which involves six metrics based on uncertainty quantification to evaluate LLMs' toxicity explanations without human involvement.

**Key Contributions:**

	1. Proposed a novel metric HAF to evaluate LLMs' toxicity explanations.
	2. Conducted extensive experiments on multiple LLMs across diverse datasets.
	3. Opened access to tools and data for the research community.

**Result:** Experiments on three Llama models and an 8B Ministral model show that while LLMs can provide plausible explanations, their reasoning about nuanced toxicity relations is inconsistent and often nonsensical.

**Limitations:** The study highlights the challenges of assessing LLMs' explanations, particularly when ideal conditions are not met.

**Conclusion:** The study reveals limitations in LLMs' reasoning about toxicity and provides open-source access to their code and generated explanations to facilitate further research.

**Abstract:** The discourse around toxicity and LLMs in NLP largely revolves around detection tasks. This work shifts the focus to evaluating LLMs' reasoning about toxicity -- from their explanations that justify a stance -- to enhance their trustworthiness in downstream tasks. Despite extensive research on explainability, it is not straightforward to adopt existing methods to evaluate free-form toxicity explanation due to their over-reliance on input text perturbations, among other challenges. To account for these, we propose a novel, theoretically-grounded multi-dimensional criterion, Human-Aligned Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity explanations align with those of a rational human under ideal conditions. We develop six metrics, based on uncertainty quantification, to comprehensively evaluate \haf of LLMs' toxicity explanations with no human involvement, and highlight how "non-ideal" the explanations are. We conduct several experiments on three Llama models (of size up to 70B) and an 8B Ministral model on five diverse toxicity datasets. Our results show that while LLMs generate plausible explanations to simple prompts, their reasoning about toxicity breaks down when prompted about the nuanced relations between the complete set of reasons, the individual reasons, and their toxicity stances, resulting in inconsistent and nonsensical responses. We open-source our code and LLM-generated explanations at https://github.com/uofthcdslab/HAF.

</details>


### [27] [Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data](https://arxiv.org/abs/2506.19159)

*Yun Tang, Eesung Kim, Vijendra Raj Apsingekar*

**Main category:** cs.CL

**Keywords:** speech recognition, text adaptation, ASR, machine learning, HCI

**Relevance Score:** 4

**TL;DR:** A joint speech and text optimization method (J-TAED) is proposed to enhance ASR accuracy by integrating speech and text inputs during training and adapting to various domains without requiring speech data during inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage large amounts of text corpus for improving ASR accuracy and addressing issues of data scarcity in mismatch domain tasks.

**Method:** The joint TAED model is trained using both speech and text modalities simultaneously. During inference, it utilizes only speech inputs while maintaining unified internal representations from both modalities.

**Key Contributions:**

	1. Introduction of the J-TAED model for hybrid ASR tasks
	2. Achievement of WER reduction across multiple datasets
	3. Extension to text-based domain adaptation

**Result:** The J-TAED model significantly reduces word error rate (WER) by 5.8% to 12.8% on the Librispeech dataset and achieves 15.3% and 17.8% WER reductions on finance and named entity focused datasets respectively.

**Limitations:** 

**Conclusion:** The joint approach effectively integrates speech and linguistic features, demonstrating improved ASR accuracy and adaptability to different domains without speech data during inference.

**Abstract:** A joint speech and text optimization method is proposed for hybrid transducer and attention-based encoder decoder (TAED) modeling to leverage large amounts of text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained with both speech and text input modalities together, while it only takes speech data as input during inference. The trained model can unify the internal representations from different modalities, and be further extended to text-based domain adaptation. It can effectively alleviate data scarcity for mismatch domain tasks since no speech data is required. Our experiments show J-TAED successfully integrates speech and linguistic information into one model, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model is also evaluated on two out-of-domain datasets: one is finance and another is named entity focused. The text-based domain adaptation brings 15.3% and 17.8% WER reduction on those two datasets respectively.

</details>


### [28] [Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages](https://arxiv.org/abs/2506.19187)

*Christopher Toukmaji, Jeffrey Flanigan*

**Main category:** cs.CL

**Keywords:** in-context learning, low-resource languages, language models, few-shot prompting, gradient-based methods

**Relevance Score:** 8

**TL;DR:** The paper explores in-context learning performance of LLMs on low-resource languages, comparing various adaptation techniques and introducing a new metric for analysis.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and improve the performance of LLMs in low-resource languages for in-context learning tasks due to their typically poor performance compared to high-resource languages.

**Method:** A comprehensive study of five low-resource languages, three base LLMs, and seven downstream tasks, utilizing techniques like few-shot prompting, translate-test, and fine-tuning over 4,100 GPU training hours.

**Key Contributions:**

	1. Largest study on in-context learning for low-resource languages
	2. Introduction of Valid Output Recall (VOR) metric
	3. Public availability of datasets and trained models

**Result:** The study found that few-shot prompting and translate-test methods significantly outperformed gradient-based adaptations; introduced a new metric, Valid Output Recall (VOR), to analyze issues such as catastrophic forgetting.

**Limitations:** The study primarily focuses on specific low-resource languages and may not generalize to all low-resource contexts; further research needed on long-term adaptation strategies.

**Conclusion:** The research highlights effective strategies for using LLMs in low-resource languages and provides resources for further research with public access to datasets and models.

**Abstract:** LLMs are typically trained in high-resource languages, and tasks in lower-resourced languages tend to underperform the higher-resource language counterparts for in-context learning. Despite the large body of work on prompting settings, it is still unclear how LLMs should be adapted cross-lingually specifically for in-context learning in the low-resource target languages. We perform a comprehensive study spanning five diverse target languages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU training hours (9,900+ TFLOPs) across various adaptation techniques: few-shot prompting, translate-test, fine-tuning, embedding re-initialization, and instruction fine-tuning. Our results show that the few-shot prompting and translate-test settings tend to heavily outperform the gradient-based adaptation methods. To better understand this discrepancy, we design a novel metric, Valid Output Recall (VOR), and analyze model outputs to empirically attribute the degradation of these trained models to catastrophic forgetting. To the extent of our knowledge, this is the largest study done on in-context learning for low-resource languages with respect to train compute and number of adaptation techniques considered. We make all our datasets and trained models available for public use.

</details>


### [29] [Augmenting Multi-Agent Communication with State Delta Trajectory](https://arxiv.org/abs/2506.19209)

*Yichen Tang, Weihang Su, Yujia Zhou, Yiqun Liu, Min Zhang, Shaoping Ma, Qingyao Ai*

**Main category:** cs.CL

**Keywords:** multi-agent systems, LLMs, State Delta Encoding, communication protocol, complex reasoning

**Relevance Score:** 8

**TL;DR:** The paper proposes a new communication protocol for multi-agent systems using LLMs, enhancing performance by transferring state transition trajectories through State Delta Encoding (SDE).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM-based multi-agent systems rely on natural language for agent communication, leading to information loss, especially in complex reasoning tasks.

**Method:** The proposed communication protocol involves State Delta Encoding (SDE), which transfers both natural language tokens and token-wise state transition trajectories between agents.

**Key Contributions:**

	1. Introduction of State Delta Encoding (SDE) for agent communication.
	2. Demonstration of improved performance in multi-agent systems for complex reasoning tasks.
	3. Comparison of communication protocols showing SDE's superiority.

**Result:** The multi-agent systems employing SDE outperform existing methods, showing state-of-the-art (SOTA) performance in complex reasoning tasks.

**Limitations:** 

**Conclusion:** Communication augmentation, particularly through State Delta Encoding, significantly enhances the efficacy of LLM-based multi-agent systems in reasoning tasks.

**Abstract:** Multi-agent techniques such as role playing or multi-turn debates have been shown to be effective in improving the performance of large language models (LLMs) in downstream tasks. Despite their differences in workflows, existing LLM-based multi-agent systems mostly use natural language for agent communication. While this is appealing for its simplicity and interpretability, it also introduces inevitable information loss as one model must down sample its continuous state vectors to concrete tokens before transferring them to the other model. Such losses are particularly significant when the information to transfer is not simple facts, but reasoning logics or abstractive thoughts. To tackle this problem, we propose a new communication protocol that transfers both natural language tokens and token-wise state transition trajectory from one agent to another. Particularly, compared to the actual state value, we find that the sequence of state changes in LLMs after generating each token can better reflect the information hidden behind the inference process, so we propose a State Delta Encoding (SDE) method to represent state transition trajectories. The experimental results show that multi-agent systems with SDE achieve SOTA performance compared to other communication protocols, particularly in tasks that involve complex reasoning. This shows the potential of communication augmentation for LLM-based multi-agent systems.

</details>


### [30] [Personality Prediction from Life Stories using Language Models](https://arxiv.org/abs/2506.19258)

*Rasiq Hussain, Jerry Ma, Rithik Khandelwal, Joshua Oltmanns, Mehak Gupta*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, personality assessment, long-context modeling

**Relevance Score:** 7

**TL;DR:** This study presents a novel method for predicting personality traits using long narrative interviews through a hybrid approach of contextual embeddings and RNNs with attention mechanisms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve personality assessment by utilizing long, open-ended text instead of traditional questionnaires.

**Method:** A two-step approach: first extracting contextual embeddings using sliding-window fine-tuning of pretrained language models, then applying RNNs with attention to model long-range dependencies.

**Key Contributions:**

	1. Proposal of a hybrid method using contextual embeddings and RNNs for personality prediction
	2. Demonstrated improvements over state-of-the-art models in prediction accuracy and interpretability
	3. Introduced a novel application of NLP techniques in personality assessment

**Result:** The hybrid method showed improved prediction accuracy, efficiency, and interpretability compared to state-of-the-art long-context models like LLaMA and Longformer through comprehensive ablation studies.

**Limitations:** 

**Conclusion:** Combining language-based features with long-context modeling significantly advances the field of personality assessment from life narratives.

**Abstract:** Natural Language Processing (NLP) offers new avenues for personality assessment by leveraging rich, open-ended text, moving beyond traditional questionnaires. In this study, we address the challenge of modeling long narrative interview where each exceeds 2000 tokens so as to predict Five-Factor Model (FFM) personality traits. We propose a two-step approach: first, we extract contextual embeddings using sliding-window fine-tuning of pretrained language models; then, we apply Recurrent Neural Networks (RNNs) with attention mechanisms to integrate long-range dependencies and enhance interpretability. This hybrid method effectively bridges the strengths of pretrained transformers and sequence modeling to handle long-context data. Through ablation studies and comparisons with state-of-the-art long-context models such as LLaMA and Longformer, we demonstrate improvements in prediction accuracy, efficiency, and interpretability. Our results highlight the potential of combining language-based features with long-context modeling to advance personality assessment from life narratives.

</details>


### [31] [What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning](https://arxiv.org/abs/2506.19262)

*Yuchang Zhu, Zhonghua zhen, Qunshu Lin, Haotong Wei, Xiaolong Sun, Zixuan Yu, Minghao Liu, Zibin Zheng, Liang Chen*

**Main category:** cs.CL

**Keywords:** large language models, data diversity, downstream model performance, synthetic data, model collapse

**Relevance Score:** 9

**TL;DR:** This study investigates the impact of LLM-generated data diversity on downstream model performance, revealing that moderately diverse data enhances performance while highly diverse data can degrade it.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of model collapse during iterative training on LLM-generated data and to assess the importance of data diversity for improving model performance.

**Method:** The authors conducted experiments analyzing the effects of varying levels of diversity in LLM-generated data on downstream model performance, including the performance of models trained on mixed proportions of LLM-generated data.

**Key Contributions:**

	1. Analysis of the impact of LLM-generated data diversity on model performance.
	2. Empirical findings indicating the effects of varying diversity levels on downstream models.
	3. Guidance for future work on leveraging LLMs for data generation in machine learning.

**Result:** The findings indicate that moderately diverse LLM-generated data improves model performance in scenarios with insufficient labeled data, whereas highly diverse data negatively impacts model results.

**Limitations:** The study does not address the full range of potential model architectures and use cases for LLM-generated data, focusing instead on specific scenarios.

**Conclusion:** The results provide insights into how data diversity in LLM-generated datasets can guide future research on LLMs as effective data generators for improving downstream model performance.

**Abstract:** With the remarkable generative capabilities of large language models (LLMs), using LLM-generated data to train downstream models has emerged as a promising approach to mitigate data scarcity in specific domains and reduce time-consuming annotations. However, recent studies have highlighted a critical issue: iterative training on self-generated data results in model collapse, where model performance degrades over time. Despite extensive research on the implications of LLM-generated data, these works often neglect the importance of data diversity, a key factor in data quality. In this work, we aim to understand the implications of the diversity of LLM-generated data on downstream model performance. Specifically, we explore how varying levels of diversity in LLM-generated data affect downstream model performance. Additionally, we investigate the performance of models trained on data that mixes different proportions of LLM-generated data, which we refer to as synthetic data. Our experimental results show that, with minimal distribution shift, moderately diverse LLM-generated data can enhance model performance in scenarios with insufficient labeled data, whereas highly diverse generated data has a negative impact. We hope our empirical findings will offer valuable guidance for future studies on LLMs as data generators.

</details>


### [32] [EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition](https://arxiv.org/abs/2506.19279)

*Zhiyang Qi, Keiko Takamizo, Mariko Ukiyo, Michimasa Inaba*

**Main category:** cs.CL

**Keywords:** AI counseling, Empathetic response generation, Psychological states, Large language models, Phase recognition

**Relevance Score:** 9

**TL;DR:** EmoStage is a framework for AI-driven counseling systems that enhances empathetic response generation using LLMs, addressing challenges of understanding psychological states and ensuring timing of responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective AI-driven mental health care solutions that understand client states and provide contextually appropriate responses.

**Method:** EmoStage framework leverages open-source LLMs to enhance empathetic response generation without requiring additional training data, incorporating perspective-taking and phase recognition.

**Key Contributions:**

	1. Introduction of perspective-taking to infer psychological states
	2. Incorporation of phase recognition for timely responses
	3. Demonstration of performance improvements in mental health counseling settings.

**Result:** Experiments show that EmoStage improves response quality in counseling settings compared to base models and competes well with data-driven methods.

**Limitations:** Potential dependency on the inherent limitations of base LLM capabilities and challenges in cross-cultural application.

**Conclusion:** EmoStage presents a novel approach to integrating empathy into AI counseling, addressing key limitations of existing methods while maintaining high response quality.

**Abstract:** The rising demand for mental health care has fueled interest in AI-driven counseling systems. While large language models (LLMs) offer significant potential, current approaches face challenges, including limited understanding of clients' psychological states and counseling stages, reliance on high-quality training data, and privacy concerns associated with commercial deployment. To address these issues, we propose EmoStage, a framework that enhances empathetic response generation by leveraging the inference capabilities of open-source LLMs without additional training data. Our framework introduces perspective-taking to infer clients' psychological states and support needs, enabling the generation of emotionally resonant responses. In addition, phase recognition is incorporated to ensure alignment with the counseling process and to prevent contextually inappropriate or inopportune responses. Experiments conducted in both Japanese and Chinese counseling settings demonstrate that EmoStage improves the quality of responses generated by base models and performs competitively with data-driven methods.

</details>


### [33] [JCAPT: A Joint Modeling Approach for CAPT](https://arxiv.org/abs/2506.19315)

*Tzu-Hsuan Yang, Yue-Yang He, Berlin Chen*

**Main category:** cs.CL

**Keywords:** pronunciation assessment, mispronunciation detection, selective state space model, language learning, computer-assisted training

**Relevance Score:** 6

**TL;DR:** This paper presents a novel framework for improving pronunciation feedback in second language learning by integrating pronunciation assessment and mispronunciation detection through a selective state space model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the need for effective pronunciation feedback in second language learning, highlighting the mutual benefits of joint modeling for pronunciation assessment and mispronunciation detection.

**Method:** The authors propose a unified framework using Mamba, a selective state space model, which incorporates phonological features and think token strategies to enhance both interpretability and temporal reasoning in automatic pronunciation assessment and mispronunciation detection.

**Key Contributions:**

	1. First study to integrate phonological attribution with SSM-based modeling in CAPT.
	2. Demonstrated superior performance on mispronunciation detection compared to prior methods.
	3. Introduces think token strategies to enhance model interpretability.

**Result:** Experiments on the speechocean762 benchmark show that the proposed model outperforms previous methods, particularly excelling in mispronunciation detection.

**Limitations:** 

**Conclusion:** This research is the first to integrate phonological attribution, SSM-based modeling, and prompting in computer-assisted pronunciation training, setting a new benchmark for pronunciation feedback systems.

**Abstract:** Effective pronunciation feedback is critical in second language (L2) learning, for which computer-assisted pronunciation training (CAPT) systems often encompass two key tasks: automatic pronunciation assessment (APA) and mispronunciation detection and diagnosis (MDD). Recent work has shown that joint modeling of these two tasks can yield mutual benefits. Our unified framework leverages Mamba, a selective state space model (SSM), while integrating phonological features and think token strategies to jointly enhance interpretability and fine-grained temporal reasoning in APA and MDD. To our knowledge, this is the first study to combine phonological attribution, SSM-based modeling, and prompting in CAPT. A series of experiments conducted on the speechocean762 benchmark demonstrate that our model consistently outperforms prior methods, particularly on the MDD task.

</details>


### [34] [Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation](https://arxiv.org/abs/2506.19352)

*Jisu Shin, Juhyun Oh, Eunsu Kim, Hoyun Song, Alice Oh*

**Main category:** cs.CL

**Keywords:** persona fidelity, large language models, evaluation framework

**Relevance Score:** 9

**TL;DR:** Proposes an atomic-level evaluation framework to enhance persona fidelity in LLMs, addressing Out-of-Character behavior and improving assessment methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Maintaining coherent and engaging human-AI interactions requires LLMs to exhibit consistent persona fidelity, yet traditional evaluation methods fall short in capturing subtle deviations.

**Method:** Introduces a framework with three key metrics that analyze persona alignment and consistency at a granular level, enabling precise detection of persona inconsistencies.

**Key Contributions:**

	1. Development of a granular evaluation framework for persona fidelity in LLMs
	2. Identification of subtle deviations in persona alignment
	3. Demonstration of the influence of task structure on persona consistency

**Result:** The proposed framework effectively identifies persona misalignments that previous methods missed, demonstrating its effectiveness across diverse tasks and personality types.

**Limitations:** 

**Conclusion:** The findings reveal the impact of task structure and persona desirability on LLM adaptability, emphasizing the challenges of maintaining persona consistency.

**Abstract:** Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.

</details>


### [35] [Measuring and Guiding Monosemanticity](https://arxiv.org/abs/2506.19382)

*Ruben H√§rle, Felix Friedrich, Manuel Brack, Stephan W√§ldchen, Bj√∂rn Deiseroth, Patrick Schramowski, Kristian Kersting*

**Main category:** cs.CL

**Keywords:** mechanistic interpretability, large language models, sparse autoencoders

**Relevance Score:** 8

**TL;DR:** This paper presents Guided Sparse Autoencoders (G-SAE), a method aimed at improving the interpretability and controllability of large language models (LLMs) using feature monosemanticity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in reliably localizing and manipulating feature representations in LLMs, which hinders understanding and influencing their internal dynamics.

**Method:** The authors introduce a new metric called Feature Monosemanticity Score (FMS) and propose G-SAE, which conditions latent representations on labeled concepts during training to enhance interpretability and control.

**Key Contributions:**

	1. Introduction of Feature Monosemanticity Score (FMS) for quantifying feature monosemanticity
	2. Development of Guided Sparse Autoencoders (G-SAE) for better interpretability in LLMs
	3. Demonstration of improved performance in various LLM tasks through G-SAE

**Result:** G-SAE improves the localization and disentanglement of concepts in the latent space, facilitating tasks like toxicity detection and writing style identification with less quality degradation.

**Limitations:** 

**Conclusion:** The findings offer guidelines for enhancing mechanistic interpretability and control in LLMs, showcasing that G-SAE significantly improves monosemanticity and enables effective steering of model behavior.

**Abstract:** There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs.

</details>


### [36] [Automated Detection of Pre-training Text in Black-box LLMs](https://arxiv.org/abs/2506.19399)

*Ruihan Hu, Yu-Ming Shang, Jiankun Peng, Wei Luo, Yazhe Wang, Xi Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, data privacy, copyright protection, membership inference, machine learning

**Relevance Score:** 8

**TL;DR:** VeilProbe is a framework for automatically detecting whether text is part of LLMs' pre-training data in a black-box setting without human intervention.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The need to ensure data privacy and copyright protection necessitates mechanisms to detect if texts are part of LLMs' training data, especially when only input and output texts are available.

**Method:** VeilProbe employs a sequence-to-sequence mapping model to infer latent mapping features between input texts and LLM-generated output suffixes, and applies key token perturbations to enhance membership feature distinguishability.

**Key Contributions:**

	1. First framework for automatic detection of LLMs' pre-training texts in a black-box setting
	2. Utilizes sequence-to-sequence mapping for feature extraction
	3. Introduces a prototype-based membership classifier to tackle overfitting in limited data scenarios.

**Result:** Extensive evaluations on three popular datasets show that VeilProbe outperforms existing methods in accurately detecting membership in a black-box setting.

**Limitations:** 

**Conclusion:** VeilProbe effectively addresses the challenges of detecting pre-training data membership in LLMs without human intervention, using modern machine learning techniques to improve accuracy and reduce manual effort.

**Abstract:** Detecting whether a given text is a member of the pre-training data of Large Language Models (LLMs) is crucial for ensuring data privacy and copyright protection. Most existing methods rely on the LLM's hidden information (e.g., model parameters or token probabilities), making them ineffective in the black-box setting, where only input and output texts are accessible. Although some methods have been proposed for the black-box setting, they rely on massive manual efforts such as designing complicated questions or instructions. To address these issues, we propose VeilProbe, the first framework for automatically detecting LLMs' pre-training texts in a black-box setting without human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to infer the latent mapping feature between the input text and the corresponding output suffix generated by the LLM. Then it performs the key token perturbations to obtain more distinguishable membership features. Additionally, considering real-world scenarios where the ground-truth training text samples are limited, a prototype-based membership classifier is introduced to alleviate the overfitting issue. Extensive evaluations on three widely used datasets demonstrate that our framework is effective and superior in the black-box setting.

</details>


### [37] [Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study](https://arxiv.org/abs/2506.19418)

*Yingji Zhang, Marco Valentino, Danilo S. Carvalho, Andr√© Freitas*

**Main category:** cs.CL

**Keywords:** Language Models, Reasoning Rules, Variational Autoencoders, Natural Language Inference, Interpretability

**Relevance Score:** 8

**TL;DR:** This paper explores the integration of explicit reasoning rules into the latent space of language models, using Language Variational Autoencoders to improve generalisation, interpretability, and controllability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of Transformer-based language models that tend to rely on memorization rather than rule-based inference, aiming to enhance their reasoning capabilities.

**Method:** The authors propose a complete pipeline for learning reasoning rules within Transformer-based language VAEs, involving three rule-based reasoning tasks and an end-to-end architecture.

**Key Contributions:**

	1. Introduction of explicit reasoning rules in language LMs
	2. Development of a pipeline for reasoning rules in VAEs
	3. Demonstration of effective knowledge injection into language models

**Result:** The experiments demonstrate that reasoning rules can be disentangled within the encoder, leading to improved clustering in feature space, and that injecting reasoning information enhances memory retrieval in decoder-only models.

**Limitations:** Performance does not improve significantly beyond a certain sample count in reasoning tasks; challenges in optimal layer choice for reasoning rules preservation.

**Conclusion:** The proposed methods improve the interpretability and effectiveness of language models by embedding reasoning rules, although performance in mathematical tasks plateaus beyond a certain sample count.

**Abstract:** Incorporating explicit reasoning rules within the latent space of language models (LMs) offers a promising pathway to enhance generalisation, interpretability, and controllability. While current Transformer-based language models have shown strong performance on Natural Language Inference (NLI) tasks, they often rely on memorisation rather than rule-based inference. This work investigates how reasoning rules can be explicitly embedded and memorised within the LMs through Language Variational Autoencoders (VAEs). We propose a complete pipeline for learning reasoning rules within Transformer-based language VAEs. This pipeline encompasses three rule-based reasoning tasks, a supporting theoretical framework, and a practical end-to-end architecture. The experiment illustrates the following findings: Disentangled reasoning: Under explicit signal supervision, reasoning rules - viewed as functional mappings - can be disentangled within the encoder's parametric space. This separation results in distinct clustering of rules in the output feature space. Prior knowledge injection: injecting reasoning information into the Query enables the model to more effectively retrieve the stored value Value from memory based on Key. This approach offers a simple method for integrating prior knowledge into decoder-only language models. Performance bottleneck: In mathematical reasoning tasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance beyond a point. Moreover, ffn layers are better than attention layers at preserving the separation of reasoning rules in the model's parameters.

</details>


### [38] [Can Large Language Models Capture Human Annotator Disagreements?](https://arxiv.org/abs/2506.19467)

*Jingwei Ni, Yu Fan, Vil√©m Zouhar, Donya Rooein, Alexander Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash*

**Main category:** cs.CL

**Keywords:** Large Language Models, NLU, Human Annotation Variation, Disagreement Prediction, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper evaluates Large Language Models' ability to predict human annotation disagreements in NLP tasks, revealing shortcomings in disagreement modeling.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if LLMs can effectively capture human annotation variation in NLP, which is essential for certain subjective and ambiguous tasks.

**Method:** The paper extensively evaluates LLMs on their ability to predict annotation disagreements without repeated human labels, examining performance in generating majority labels and modeling disagreements.

**Key Contributions:**

	1. Identification of LLMs' limitations in predicting human annotation disagreements.
	2. Highlighting the shortcomings of majority label-based evaluations in capturing annotation variation.
	3. Providing code and data resources for further exploration of disagreement prediction.

**Result:** LLMs struggle with predicting annotation disagreements, which may be overlooked by evaluations based solely on majority labels.

**Limitations:** The study is limited to the evaluation of LLMs without repeated human labels, which may not fully capture the complexity of human annotation variation.

**Conclusion:** The findings emphasize the need for improved evaluation methods for LLMs in disagreement modeling and suggest that typical performance enhancements (like RLVR) may worsen this specific capability.

**Abstract:** Human annotation variation (i.e., annotation disagreements) is common in NLP and often reflects important information such as task subjectivity and sample ambiguity. While Large Language Models (LLMs) are increasingly used for automatic annotation to reduce human effort, their evaluation often focuses on predicting the majority-voted "ground truth" labels. It is still unclear, however, whether these models also capture informative human annotation variation. Our work addresses this gap by extensively evaluating LLMs' ability to predict annotation disagreements without access to repeated human labels. Our results show that LLMs struggle with modeling disagreements, which can be overlooked by majority label-based evaluations. Notably, while RLVR-style (Reinforcement learning with verifiable rewards) reasoning generally boosts LLM performance, it degrades performance in disagreement prediction. Our findings highlight the critical need for evaluating and improving LLM annotators in disagreement modeling. Code and data at https://github.com/EdisonNi-hku/Disagreement_Prediction.

</details>


### [39] [Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation](https://arxiv.org/abs/2506.19352)

*Jisu Shin, Juhyun Oh, Eunsu Kim, Hoyun Song, Alice Oh*

**Main category:** cs.CL

**Keywords:** Persona Fidelity, Large Language Models, Human-AI Interaction

**Relevance Score:** 9

**TL;DR:** The paper introduces an atomic-level evaluation framework to measure persona fidelity in LLMs, addressing Out-of-Character behavior and offering granular insights into persona alignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human-AI interactions by addressing the Out-of-Character behavior in large language models and ensuring consistent persona expression.

**Method:** The proposed framework uses three key metrics to quantify persona alignment and consistency at a finer granularity, allowing for a precise evaluation of long-form text generation.

**Key Contributions:**

	1. Introduction of an atomic-level evaluation framework for persona fidelity in LLMs.
	2. Development of three key metrics for measuring persona alignment.
	3. Demonstration of the framework's effectiveness in detecting subtle persona inconsistencies.

**Result:** Experiments show that the framework can effectively detect persona inconsistencies that traditional methods miss, revealing the impact of task structure and persona desirability on model adaptability.

**Limitations:** 

**Conclusion:** The study highlights the importance of measuring persona fidelity more precisely in LLMs, thereby improving the reliability of human-AI interactions.

**Abstract:** Ensuring persona fidelity in large language models (LLMs) is essential for maintaining coherent and engaging human-AI interactions. However, LLMs often exhibit Out-of-Character (OOC) behavior, where generated responses deviate from an assigned persona, leading to inconsistencies that affect model reliability. Existing evaluation methods typically assign single scores to entire responses, struggling to capture subtle persona misalignment, particularly in long-form text generation. To address this limitation, we propose an atomic-level evaluation framework that quantifies persona fidelity at a finer granularity. Our three key metrics measure the degree of persona alignment and consistency within and across generations. Our approach enables a more precise and realistic assessment of persona fidelity by identifying subtle deviations that real users would encounter. Through our experiments, we demonstrate that our framework effectively detects persona inconsistencies that prior methods overlook. By analyzing persona fidelity across diverse tasks and personality types, we reveal how task structure and persona desirability influence model adaptability, highlighting challenges in maintaining consistent persona expression.

</details>


### [40] [MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages](https://arxiv.org/abs/2506.19468)

*Wenhan Han, Yifan Zhang, Zhixun Chen, Binbin Liu, Haobin Lin, Bingni Zhang, Taifeng Wang, Mykola Pechenizkiy, Meng Fang, Yin Zheng*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, MuBench, Multilingual Consistency, low-resource languages, cross-lingual transfer

**Relevance Score:** 8

**TL;DR:** The paper introduces MuBench, a comprehensive benchmark for evaluating multilingual LLMs across 61 languages, highlighting gaps in language coverage and proposing Multilingual Consistency as a new evaluation metric.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address fragmented assessments of multilingual capabilities in large language models due to limited evaluation datasets that lack cross-lingual alignment.

**Method:** The authors introduce MuBench, a benchmark that evaluates multilingual LLMs across a wide range of capabilities for 61 languages, and they propose Multilingual Consistency (MLC) as a new metric to analyze model performance.

**Key Contributions:**

	1. Introduction of MuBench, a benchmark for multilingual capabilities
	2. Proposal of Multilingual Consistency as an evaluation metric
	3. Pretraining of 1.2B-parameter models to study cross-lingual transfer dynamics

**Result:** Evaluation of state-of-the-art multilingual LLMs revealed significant gaps between claimed capabilities and actual performance, especially for low-resource languages, and highlighted the need for Multilingual Consistency as a metric.

**Limitations:** The benchmark and its evaluations may still be limited by the underlying dataset and may not fully capture the complexity of all linguistic features across languages.

**Conclusion:** The study indicates that current multilingual LLMs have notable coverage gaps, particularly in low-resource languages, and encourages the adoption of Multilingual Consistency to better understand and improve model performance.

**Abstract:** Multilingual large language models (LLMs) are advancing rapidly, with new models frequently claiming support for an increasing number of languages. However, existing evaluation datasets are limited and lack cross-lingual alignment, leaving assessments of multilingual capabilities fragmented in both language and skill coverage. To address this, we introduce MuBench, a benchmark covering 61 languages and evaluating a broad range of capabilities. We evaluate several state-of-the-art multilingual LLMs and find notable gaps between claimed and actual language coverage, particularly a persistent performance disparity between English and low-resource languages. Leveraging MuBench's alignment, we propose Multilingual Consistency (MLC) as a complementary metric to accuracy for analyzing performance bottlenecks and guiding model improvement. Finally, we pretrain a suite of 1.2B-parameter models on English and Chinese with 500B tokens, varying language ratios and parallel data proportions to investigate cross-lingual transfer dynamics.

</details>


### [41] [Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models](https://arxiv.org/abs/2506.19483)

*Marcos Estecha-Garitagoitia, Chen Zhang, Mario Rodr√≠guez-Cantelar, Luis Fernando D'Haro*

**Main category:** cs.CL

**Keywords:** dialogue systems, data augmentation, commonsense reasoning, Large Language Models, evaluation framework

**Relevance Score:** 8

**TL;DR:** This study explores turn-level data augmentation for dialogue systems using commonsense relationships and evaluates the generated synthetic turns with pretrained LLMs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance dialogue systems by utilizing commonsense relationships between dialogue turns for improved data augmentation and evaluation.

**Method:** The approach uses pretrained LLMs to generate alternative responses based on commonsense attributes extracted from selected dialogue datasets and includes an evaluation framework inspired by the ACCENT metric.

**Key Contributions:**

	1. Introduction of a novel dataset for dialogue augmentation based on commonsense attributes
	2. Development of an automatic evaluation framework for dialogue quality assessment
	3. Utilization of pretrained LLMs for generating contextually relevant dialogue turns

**Result:** Preliminary results indicate that the method successfully leverages LLMs for generating contextually relevant commonsense knowledge in dialogue, demonstrating effectiveness in augmenting the dialogue dataset.

**Limitations:** The method does not implement the complex extraction process of event-relation tuples as seen in ACCENT, potentially limiting its evaluation comprehensiveness.

**Conclusion:** The findings suggest that LLMs can be effectively used for data augmentation in dialogue systems, particularly through commonsense reasoning and automated evaluation methods.

**Abstract:** This paper provides preliminary results on exploring the task of performing turn-level data augmentation for dialogue system based on different types of commonsense relationships, and the automatic evaluation of the generated synthetic turns. The proposed methodology takes advantage of the extended knowledge and zero-shot capabilities of pretrained Large Language Models (LLMs) to follow instructions, understand contextual information, and their commonsense reasoning capabilities. The approach draws inspiration from methodologies like Chain-of-Thought (CoT), applied more explicitly to the task of prompt-based generation for dialogue-based data augmentation conditioned on commonsense attributes, and the automatic evaluation of the generated dialogues.   To assess the effectiveness of the proposed approach, first we extracted 200 randomly selected partial dialogues, from 5 different well-known dialogue datasets, and generate alternative responses conditioned on different event commonsense attributes. This novel dataset allows us to measure the proficiency of LLMs in generating contextually relevant commonsense knowledge, particularly up to 12 different specific ATOMIC [10] database relations. Secondly, we propose an evaluation framework to automatically detect the quality of the generated dataset inspired by the ACCENT [26] metric, which offers a nuanced approach to assess event commonsense. However, our method does not follow ACCENT's complex eventrelation tuple extraction process. Instead, we propose an instruction-based prompt for each commonsense attribute and use state-of-the-art LLMs to automatically detect the original attributes used when creating each augmented turn in the previous step.   Preliminary results suggest that our approach effectively harnesses LLMs capabilities for commonsense reasoning and evaluation in dialogue systems.

</details>


### [42] [Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning](https://arxiv.org/abs/2506.19484)

*Russell Beale*

**Main category:** cs.CL

**Keywords:** Large Language Models, Conversational Agents, Education, Pedagogy, Retrieval-Augmented Generation

**Relevance Score:** 9

**TL;DR:** This article reviews the use of LLM-based conversational agents in education and offers strategies to align these tools with established educational theories to enhance learning outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs can be effectively integrated into educational contexts and to identify gaps in their application relative to traditional pedagogical theories.

**Method:** Synthesis of existing literature on LLMs in education, with a focus on theories of conversational pedagogy and analysis of prompting strategies and RAG.

**Key Contributions:**

	1. Comprehensive review of LLMs in education
	2. Practical strategies for aligning LLM interactions with pedagogical theories
	3. Identification of gaps in LLM application to traditional teaching methods

**Result:** The review finds that while LLMs can support established learning principles, they often diverge from traditional pedagogical methods, particularly in fostering knowledge co-construction.

**Limitations:** Models tend to provide direct answers rather than fostering collaborative learning; constant availability may lead to issues regarding expertise and guidance.

**Conclusion:** Practical strategies are proposed to enhance the educational effectiveness of LLMs in learning environments, focusing on prompt design and integration of retrieval mechanisms.

**Abstract:** Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.

</details>


### [43] [Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning](https://arxiv.org/abs/2506.19484)

*Russell Beale*

**Main category:** cs.CL

**Keywords:** Large Language Models, education, conversational learning, pedagogy, retrieval-augmented generation

**Relevance Score:** 9

**TL;DR:** This paper reviews the integration of LLMs in education, emphasizing alignment with pedagogical theories for effective conversational learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models are reshaping education by facilitating engaging conversational learning experiences that require alignment with established educational theories.

**Method:** The article synthesizes literature on LLMs in education, explores theories of conversational pedagogy, and examines the role of prompting strategies and RAG.

**Key Contributions:**

	1. Synthesis of literature on LLMs and education.
	2. Identification of gaps between LLM capabilities and traditional pedagogical assumptions.
	3. Proposed strategies for better aligning LLM interactions with educational theories.

**Result:** It identifies gaps in the application of educational theories to LLMs, including the tendency of LLMs to provide direct answers rather than encourage knowledge co-construction.

**Limitations:** The tendency of LLMs to operate outside human-like expertise and provide immediate answers rather than support deeper learning processes.

**Conclusion:** Practical strategies are proposed to enhance the educational effectiveness of LLM interactions by aligning them with sound pedagogical approaches.

**Abstract:** Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.

</details>


### [44] [Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs](https://arxiv.org/abs/2506.19492)

*Shu Yang, Junchao Wu, Xuansheng Wu, Derek Wong, Ninhao Liu, Di Wang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, behavioral inconsistency, efficient reasoning, self-explanation, ICBENCH

**Relevance Score:** 8

**TL;DR:** This paper investigates the trade-offs of efficient reasoning strategies in Large Reasoning Models (LRMs), revealing increased behavioral inconsistencies even as token efficiency improves.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether efficient reasoning in LRMs compromises model robustness and introduces behavioral inconsistencies.

**Method:** The authors introduce $ICBENCH$, a benchmark for measuring inconsistency in LRMs across three dimensions: task setting inconsistency, training objective vs. behavior inconsistency, and internal reasoning vs. self-explanation inconsistency.

**Key Contributions:**

	1. Development of the $ICBENCH$ benchmark for assessing inconsistencies in LRMs
	2. Identification of 'scheming' behaviors in LRMs
	3. Demonstration that efficient reasoning strategies can exacerbate behavioral inconsistencies

**Result:** The application of $ICBENCH$ shows that larger models are generally more consistent, but all models exhibit scheming behaviors, and efficient reasoning strategies increase types of inconsistencies.

**Limitations:** The study primarily focuses on open-source LRMs and may not fully represent proprietary models; further exploration into specific inconsistencies is required.

**Conclusion:** Efficient reasoning may enhance token efficiency but poses risks of behavioral inconsistencies and could lead to models avoiding effective supervision, necessitating further study.

**Abstract:** Large Reasoning Models (LRMs) have achieved remarkable performance on complex tasks by engaging in extended reasoning before producing final answers, yet this strength introduces the risk of overthinking, where excessive token generation occurs even for simple tasks. While recent work in efficient reasoning seeks to reduce reasoning length while preserving accuracy, it remains unclear whether such optimization is truly a free lunch. Drawing on the intuition that compressing reasoning may reduce the robustness of model responses and lead models to omit key reasoning steps, we investigate whether efficient reasoning strategies introduce behavioral inconsistencies. To systematically assess this, we introduce $ICBENCH$, a benchmark designed to measure inconsistency in LRMs across three dimensions: inconsistency across task settings (ITS), inconsistency between training objectives and learned behavior (TR-LB), and inconsistency between internal reasoning and self-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs, we find that while larger models generally exhibit greater consistency than smaller ones, they all display widespread "scheming" behaviors, including self-disagreement, post-hoc rationalization, and the withholding of reasoning cues. Crucially, our results demonstrate that efficient reasoning strategies such as No-Thinking and Simple Token-Budget consistently increase all three defined types of inconsistency. These findings suggest that although efficient reasoning enhances token-level efficiency, further investigation is imperative to ascertain whether it concurrently introduces the risk of models evading effective supervision.

</details>


### [45] [AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models](https://arxiv.org/abs/2506.19505)

*Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu*

**Main category:** cs.CL

**Keywords:** Large Language Models, quantization, KV cache, Anchor Token-aware Vector Quantization, performance optimization

**Relevance Score:** 8

**TL;DR:** This paper presents AnTKV, a framework that utilizes Anchor Token-aware Vector Quantization to improve the performance of KV cache quantization in Large Language Models by reducing accuracy loss while maintaining high throughput.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Quantization reduces memory usage in Large Language Models, but performance degradation from ultra-low-bit quantization presents a challenge. We aim to address this by analyzing how different tokens' KV caches are affected by quantization errors.

**Method:** We conduct a forward error propagation analysis on attention outputs and introduce the Anchor Score (AnS) to quantify the sensitivity of KV cache tokens to quantization errors. Based on the AnS analysis, we preserve a subset of high-AnS tokens using full precision and implement a framework for efficient KV cache quantization.

**Key Contributions:**

	1. Introduction of Anchor Score (AnS) for sensitivity analysis of KV cache tokens
	2. Development of AnTKV framework for improved KV cache quantization
	3. Demonstration of enhanced throughput and reduced perplexity in LLaMA-3-8B compared to FP16 and prior works

**Result:** AnTKV allows LLaMA-3-8B to manage up to 840K tokens on a single 80GB A100 GPU, achieving up to 3.5x higher decoding throughput compared to FP16 and outperforms prior methods under 4-bit quantization. It also reduces perplexity significantly at low bit quantization levels.

**Limitations:** 

**Conclusion:** The AnTKV framework can effectively reduce the performance loss due to quantization in KV caches of LLMs while enhancing the operational throughput, presenting a viable solution to the challenges associated with ultra-low-bit quantization.

**Abstract:** Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models (LLMs). Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. We observe that quantizing the KV cache of different tokens has varying impacts on the quality of attention outputs. To systematically investigate this phenomenon, we perform forward error propagation analysis on attention and propose the Anchor Score (AnS) that quantifies the sensitivity of each token's KV cache to quantization-induced error. Our analysis reveals significant disparities in AnS across tokens, suggesting that preserving a small subset with full precision (FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive quantization scenarios. Based on this insight, we introduce AnTKV, a novel framework that leverages Anchor Token-aware Vector Quantization to compress the KV cache. Furthermore, to support efficient deployment, we design and develop a triton kernel that is fully compatible with FlashAttention, enabling fast online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x higher decoding throughput compared to the FP16 baseline. Our experiment results demonstrate that AnTKV matches or outperforms prior works such as KIVI, SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves significantly lower perplexity under ultra-low-bit quantization on Mistral-7B, with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of 4.73.

</details>


### [46] [heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation](https://arxiv.org/abs/2506.19512)

*Ashish Chouhan, Michael Gertz*

**Main category:** cs.CL

**Keywords:** retrieval augmented generation, electronic health records, patient-specific questions

**Relevance Score:** 9

**TL;DR:** This paper describes the heiDS approach for the ArchEHR-QA 2025 task, utilizing a retrieval augmented generation framework to answer patient-specific questions based on electronic health records.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this work is to improve the accuracy and relevance of answers generated for patient-specific queries by utilizing a retrieval augmented generation framework and exploring advanced retrieval strategies.

**Method:** The method involves a pipeline using a retrieval augmented generation framework, focusing on query-dependent-k retrieval strategies instead of a fixed top-k retrieval approach. It explores various strategies including surprise, autocut, and introduces new methods autocut* and elbow.

**Key Contributions:**

	1. Introduction of query-dependent-k retrieval strategies
	2. Development of new methods: autocut* and elbow
	3. Demonstration of improved answer relevance and factuality over fixed-top-k approaches

**Result:** Experimental results indicate that the query-dependent retrieval strategies significantly improve the generation of factual and relevant answers compared to traditional fixed-k methods.

**Limitations:** 

**Conclusion:** The conclusion highlights the effectiveness of the proposed retrieval strategies in enhancing the quality of responses generated from electronic health records, supporting better patient-specific query handling.

**Abstract:** This paper presents the approach of our team called heiDS for the ArchEHR-QA 2025 shared task. A pipeline using a retrieval augmented generation (RAG) framework is designed to generate answers that are attributed to clinical evidence from the electronic health records (EHRs) of patients in response to patient-specific questions. We explored various components of a RAG framework, focusing on ranked list truncation (RLT) retrieval strategies and attribution approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a query-dependent-k retrieval strategy, including the existing surprise and autocut methods and two new methods proposed in this work, autocut* and elbow. The experimental results show the benefits of our strategy in producing factual and relevant answers when compared to a fixed-$k$.

</details>


### [47] [Automatic Posology Structuration : What role for LLMs?](https://arxiv.org/abs/2506.19525)

*Natalia Bobkova, Laura Zanella-Calzada, Anyes Tafoughalt, Rapha√´l Teboul, Fran√ßois Plesse, F√©lix Gaschi*

**Main category:** cs.CL

**Keywords:** posology instructions, Large Language Models, Named Entity Recognition, hybrid pipeline, clinical decision support

**Relevance Score:** 9

**TL;DR:** This paper presents a hybrid pipeline using Large Language Models and Named Entity Recognition for structuring posology instructions in French prescriptions to improve medication safety.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to enhance medication safety and clinical decision support by effectively structuring often ambiguous posology instructions found in French prescriptions.

**Method:** The authors explore prompt-based methods and fine-tuning of Large Language Models (LLMs) against a baseline system based on Named Entity Recognition and Linking (NERL).

**Key Contributions:**

	1. Development of a hybrid pipeline combining NERL and LLMs for posology structuration
	2. Empirical comparison of prompt-based methods vs. fine-tuning
	3. Proposed a confidence-based routing strategy for low-confidence cases

**Result:** The hybrid approach achieves 91% structuration accuracy while minimizing latency and computational costs, outperforming traditional methods.

**Limitations:** The study primarily focuses on French prescriptions, and the results may not generalize to other languages or types of medical instructions.

**Conclusion:** The results indicate that combining NERL with LLMs provides a robust solution for structuring medical prescriptions, balancing accuracy and computational efficiency.

**Abstract:** Automatically structuring posology instructions is essential for improving medication safety and enabling clinical decision support. In French prescriptions, these instructions are often ambiguous, irregular, or colloquial, limiting the effectiveness of classic ML pipelines. We explore the use of Large Language Models (LLMs) to convert free-text posologies into structured formats, comparing prompt-based methods and fine-tuning against a "pre-LLM" system based on Named Entity Recognition and Linking (NERL). Our results show that while prompting improves performance, only fine-tuned LLMs match the accuracy of the baseline. Through error analysis, we observe complementary strengths: NERL offers structural precision, while LLMs better handle semantic nuances. Based on this, we propose a hybrid pipeline that routes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs based on confidence scores. This strategy achieves 91% structuration accuracy while minimizing latency and compute. Our results show that this hybrid approach improves structuration accuracy while limiting computational cost, offering a scalable solution for real-world clinical use.

</details>


### [48] [KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs](https://arxiv.org/abs/2506.19527)

*Kelin Fu, Kaigui Bian*

**Main category:** cs.CL

**Keywords:** Large Language Models, task adaptation, knowledge base

**Relevance Score:** 9

**TL;DR:** KnowMap enhances LLM task adaptation by constructing a dynamic knowledge base from environmental data, improving performance while avoiding traditional pitfalls.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional fine-tuning methods for LLMs by providing a more efficient way to integrate task-specific knowledge.

**Method:** KnowMap dynamically constructs a knowledge base and fine-tunes a small knowledge-embedding model to augment a larger LLM with necessary task-specific information.

**Key Contributions:**

	1. Introduction of KnowMap for LLM task adaptation
	2. Demonstrated performance improvement on ScienceWorld benchmark
	3. Integration of environmental and experiential knowledge into LLMs

**Result:** A 17.71% improvement in performance for the gpt-4-turbo model on the ScienceWorld benchmark was observed.

**Limitations:** 

**Conclusion:** KnowMap offers a more effective means for LLM task adaptation and improves reasoning capabilities by incorporating environmental and experiential knowledge.

**Abstract:** While Large Language Models (LLMs) possess significant capabilities in open-world agent tasks, they also face challenges in rapidly adapting to new, specialized tasks due to their reliance on static pre-trained knowledge. Traditional methods such as fine-tuning are often costly, data-intensive, and may lead to "catastrophic forgetting." Therefore, we present KnowMap, a novel approach that dynamically constructs a knowledge base from environmental and experiential data. KnowMap fine-tunes a small knowledge-embedding model to equip a larger LLM with valuable task-specific knowledge. Our experiments on the ScienceWorld benchmark demonstrate 17.71% improvement for the performance of gpt-4-turbo model. KnowMap not only provides an efficient and effective means for LLM task-adapting, but also highlights how integrating environmental and experiential knowledge can enhance LLMs' reasoning capabilities.

</details>


### [49] [Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection](https://arxiv.org/abs/2506.19548)

*Devesh Pant, Rishi Raj Grandhe, Vipin Samaria, Mukul Paul, Sudhir Kumar, Saransh Khanna, Jatin Agrawal, Jushaan Singh Kalra, Akhil VSSG, Satish V Khalikar, Vipin Garg, Himanshu Chauhan, Pranay Verma, Neha Khandelwal, Soma S Dhavala, Minesh Mathew*

**Main category:** cs.CL

**Keywords:** Health Sentinel, disease outbreaks, information extraction, machine learning, public health

**Relevance Score:** 9

**TL;DR:** Health Sentinel is a multi-stage pipeline for extracting structured information about disease outbreaks from online articles.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve early detection of disease outbreaks beyond traditional surveillance methods.

**Method:** A combination of ML and non-ML methods is used to extract event-structured information from online articles.

**Key Contributions:**

	1. Development of a multi-stage information extraction pipeline
	2. Integration of ML and non-ML methods for data extraction
	3. Real-time processing of online media articles for health insights.

**Result:** Processed over 300 million articles, identifying over 95,000 unique health events, with over 3,500 events shortlisted as potential outbreaks.

**Limitations:** 

**Conclusion:** Health Sentinel supports health authorities by providing structured information on disease outbreaks for timely interventions.

**Abstract:** Early detection of disease outbreaks is crucial to ensure timely intervention by the health authorities. Due to the challenges associated with traditional indicator-based surveillance, monitoring informal sources such as online media has become increasingly popular. However, owing to the number of online articles getting published everyday, manual screening of the articles is impractical. To address this, we propose Health Sentinel. It is a multi-stage information extraction pipeline that uses a combination of ML and non-ML methods to extract events-structured information concerning disease outbreaks or other unusual health events-from online articles. The extracted events are made available to the Media Scanning and Verification Cell (MSVC) at the National Centre for Disease Control (NCDC), Delhi for analysis, interpretation and further dissemination to local agencies for timely intervention. From April 2022 till date, Health Sentinel has processed over 300 million news articles and identified over 95,000 unique health events across India of which over 3,500 events were shortlisted by the public health experts at NCDC as potential outbreaks.

</details>


### [50] [RCStat: A Statistical Framework for using Relative Contextualization in Transformers](https://arxiv.org/abs/2506.19549)

*Debabrata Mahapatra, Shubham Agarwal, Apoorv Saxena, Subrata Mitra*

**Main category:** cs.CL

**Keywords:** Transformers, Attention Mechanism, Machine Learning, Key-Value Compression, Attribution

**Relevance Score:** 8

**TL;DR:** RCStat introduces a statistical framework using raw attention logits to enhance token importance assessment in auto-regressive transformers, yielding improved performance in key-value compression and attribution tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve understanding of input-token importance in transformers by utilizing pre-Softmax query-key logits, which provide richer information than traditional Softmax-normalized attention weights.

**Method:** RCStat employs Relative Contextualization (RC) to measure the contextual alignment between token segments, deriving an efficient upper bound for RC.

**Key Contributions:**

	1. Introduction of RCStat framework for analyzing attention logits
	2. Demonstration of improved key-value compression methods
	3. Enhanced attribution explanations using raw attention metrics

**Result:** RCStat demonstrates significant improvements in key-value compression and attribution tasks, achieving state-of-the-art performance across various benchmarks without requiring model retraining.

**Limitations:** 

**Conclusion:** The proposed methods show that raw attention logits can yield more effective token importance metrics and practical applications like key-value eviction and improved explanation fidelity in NLP tasks.

**Abstract:** Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.

</details>


### [51] [Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress](https://arxiv.org/abs/2506.19571)

*Lorenzo Proietti, Stefano Perrella, Roberto Navigli*

**Main category:** cs.CL

**Keywords:** Machine Translation, MT evaluation, automatic metrics, human judgments, meta-evaluation

**Relevance Score:** 5

**TL;DR:** This paper explores the performance of automatic metrics in Machine Translation evaluation against human judgments, revealing that state-of-the-art metrics often perform on par or better than humans.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand and assess the capabilities of MT metrics and their agreement with human evaluations, incorporating human baselines to establish an upper bound on performance.

**Method:** Meta-evaluation of Machine Translation metrics by comparing human annotator performance with that of state-of-the-art automatic metrics.

**Key Contributions:**

	1. Introduced human baselines in the evaluation of MT metrics.
	2. Demonstrated that automatic metrics can match or exceed human performance.
	3. Provoked discussion on the reliability of measuring improvements in MT evaluation.

**Result:** Human annotators are not consistently better than automatic metrics, with some state-of-the-art metrics achieving equal or better rankings than human baselines.

**Limitations:** Potential variability in human annotator performance and the need for consistent evaluation metrics across studies.

**Conclusion:** The results suggest caution in interpreting human parity with automatic metrics, raising concerns about the reliability of measuring progress in MT evaluation.

**Abstract:** In Machine Translation (MT) evaluation, metric performance is assessed based on agreement with human judgments. In recent years, automatic metrics have demonstrated increasingly high levels of agreement with humans. To gain a clearer understanding of metric performance and establish an upper bound, we incorporate human baselines in the MT meta-evaluation, that is, the assessment of MT metrics' capabilities. Our results show that human annotators are not consistently superior to automatic metrics, with state-of-the-art metrics often ranking on par with or higher than human baselines. Despite these findings suggesting human parity, we discuss several reasons for caution. Finally, we explore the broader implications of our results for the research field, asking: Can we still reliably measure improvements in MT evaluation? With this work, we aim to shed light on the limits of our ability to measure progress in the field, fostering discussion on an issue that we believe is crucial to the entire MT evaluation community.

</details>


### [52] [ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model](https://arxiv.org/abs/2506.19599)

*Zhenke Duan, Jiqun Pan, Jiani Tu, Xiaoyi Wang, Yanqing Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Causal Reasoning, Interpretability

**Relevance Score:** 9

**TL;DR:** ECCoT is a framework designed to validate and improve the interpretability of reasoning chains in Large Language Models using a combination of topic modeling and causal reasoning techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the transparency and reliability of Large Language Models by addressing the issue of unreliable conclusions arising from invalid reasoning chains.

**Method:** The ECCoT framework leverages the Markov Random Field-Embedded Topic Model (MRF-ETM) for generating topic-aware reasoning and Causal Sentence-BERT (CSBert) to ensure causal reasoning alignment, along with structured ordering statistics to filter ineffective reasoning chains.

**Key Contributions:**

	1. Introduction of the ECCoT framework.
	2. Development of the MRF-ETM for topic-driven Chain of Thought generation.
	3. Integration of CSBert for enhancing causal reasoning.

**Result:** The framework enhances interpretability, reduces biases, and increases trust in LLM-based decision-making by validating and refining reasoning chains.

**Limitations:** 

**Conclusion:** ECCoT represents a significant advancement in making LLM outputs more interpretable and trustworthy, particularly in critical applications where decision-making is affected by AI.

**Abstract:** In the era of large-scale artificial intelligence, Large Language Models (LLMs) have made significant strides in natural language processing. However, they often lack transparency and generate unreliable outputs, raising concerns about their interpretability. To address this, the Chain of Thought (CoT) prompting method structures reasoning into step-by-step deductions. Yet, not all reasoning chains are valid, and errors can lead to unreliable conclusions. We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By filtering ineffective chains using structured ordering statistics, ECCoT improves interpretability, reduces biases, and enhances the trustworthiness of LLM-based decision-making. Key contributions include the introduction of ECCoT, MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.

</details>


### [53] [Social Hatred: Efficient Multimodal Detection of Hatemongers](https://arxiv.org/abs/2506.19603)

*Tom Marzea, Abraham Israeli, Oren Tsur*

**Main category:** cs.CL

**Keywords:** hate speech detection, multimodal analysis, social networks

**Relevance Score:** 4

**TL;DR:** This paper presents a multimodal approach for detecting hate-mongers by analyzing user texts, activity, and networks, leading to improved detection accuracy on various social media platforms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to detoxify online discourse and understand the social phenomenon of hate speech requires effective detection methods that go beyond just analyzing text.

**Method:** A multimodal aggregative approach that considers hateful texts, user activity, and user networks to detect hate-mongers.

**Key Contributions:**

	1. Introduction of a user-level focus in hate speech detection.
	2. Development of a multimodal detection approach enhancing traditional methods.
	3. Comprehensive evaluation across multiple datasets and platforms.

**Result:** The proposed method, evaluated on datasets from Twitter, Gab, and Parler, significantly improves hate monger detection compared to traditional text and graph-based methods.

**Limitations:** 

**Conclusion:** This multimodal approach can enhance the detection of coded messages and inform intervention strategies while being effective across different platforms.

**Abstract:** Automatic detection of online hate speech serves as a crucial step in the detoxification of the online discourse. Moreover, accurate classification can promote a better understanding of the proliferation of hate as a social phenomenon. While most prior work focus on the detection of hateful utterances, we argue that focusing on the user level is as important, albeit challenging. In this paper we consider a multimodal aggregative approach for the detection of hate-mongers, taking into account the potentially hateful texts, user activity, and the user network. Evaluating our method on three unique datasets X (Twitter), Gab, and Parler we show that processing a user's texts in her social context significantly improves the detection of hate mongers, compared to previously used text and graph-based methods. We offer comprehensive set of results obtained in different experimental settings as well as qualitative analysis of illustrative cases. Our method can be used to improve the classification of coded messages, dog-whistling, and racial gas-lighting, as well as to inform intervention measures. Moreover, we demonstrate that our multimodal approach performs well across very different content platforms and over large datasets and networks.

</details>


### [54] [Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge](https://arxiv.org/abs/2506.19607)

*Juraj Vladika, Ihsan Soydemir, Florian Matthes*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, self-correcting methods

**Relevance Score:** 9

**TL;DR:** This paper explores self-correcting methods to reduce hallucinations in news summarization through multi-turn LLM interactions and evidence retrieval.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations in large language models, particularly in the context of news summarization, by leveraging self-correcting techniques.

**Method:** The authors applied two state-of-the-art self-correcting systems to correct hallucinated summaries using evidence from three search engines and analyzed the performance.

**Key Contributions:**

	1. Application of self-correcting systems to news summarization
	2. Insights on the role of search engine snippets
	3. High alignment of automated evaluation with human judgment.

**Result:** The study reveals significant insights into the performance of self-correcting methods, highlighting the usefulness of search engine snippets and a strong correlation between G-Eval and human evaluations.

**Limitations:** 

**Conclusion:** The findings suggest that employing multi-turn interactions with LLMs for verification can enhance summary accuracy and mitigate hallucinations.

**Abstract:** While large language models (LLMs) have shown remarkable capabilities to generate coherent text, they suffer from the issue of hallucinations -- factually inaccurate statements. Among numerous approaches to tackle hallucinations, especially promising are the self-correcting methods. They leverage the multi-turn nature of LLMs to iteratively generate verification questions inquiring additional evidence, answer them with internal or external knowledge, and use that to refine the original response with the new corrections. These methods have been explored for encyclopedic generation, but less so for domains like news summarization. In this work, we investigate two state-of-the-art self-correcting systems by applying them to correct hallucinated summaries using evidence from three search engines. We analyze the results and provide insights into systems' performance, revealing interesting practical findings on the benefits of search engine snippets and few-shot prompts, as well as high alignment of G-Eval and human evaluation.

</details>


### [55] [Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs](https://arxiv.org/abs/2410.23478)

*Sireesh Gururaja, Yueheng Zhang, Guannan Tang, Tianhao Zhang, Kevin Murphy, Yu-Tsen Yi, Junwon Seo, Anthony Rollett, Emma Strubell*

**Main category:** cs.CL

**Keywords:** information extraction, HuggingFace, scientific documents, NLP tool, literature review

**Relevance Score:** 8

**TL;DR:** Collage is a tool for prototyping and evaluating information extraction models on scientific PDFs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of comparing and using domain-specific NLP models for scientific documents.

**Method:** Collage provides interfaces for various information extraction models including HuggingFace token classifiers and LLMs, enabling rapid evaluation and prototyping.

**Key Contributions:**

	1. Development of Collage for model comparison and evaluation
	2. Support for various NLP models out of the box
	3. Enhanced debugging and understanding of modeling pipelines

**Result:** Collage allows for the inspection and debugging of NLP modeling pipelines, providing insights into intermediate processing states.

**Limitations:** 

**Conclusion:** The tool facilitates literature review in materials science by improving the evaluation of information extraction models.

**Abstract:** Recent years in NLP have seen the continued development of domain-specific information extraction tools for scientific documents, alongside the release of increasingly multimodal pretrained transformer models. While the opportunity for scientists outside of NLP to evaluate and apply such systems to their own domains has never been clearer, these models are difficult to compare: they accept different input formats, are often black-box and give little insight into processing failures, and rarely handle PDF documents, the most common format of scientific publication. In this work, we present Collage, a tool designed for rapid prototyping, visualization, and evaluation of different information extraction models on scientific PDFs. Collage allows the use and evaluation of any HuggingFace token classifier, several LLMs, and multiple other task-specific models out of the box, and provides extensible software interfaces to accelerate experimentation with new models. Further, we enable both developers and users of NLP-based tools to inspect, debug, and better understand modeling pipelines by providing granular views of intermediate states of processing. We demonstrate our system in the context of information extraction to assist with literature review in materials science.

</details>


### [56] [Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager](https://arxiv.org/abs/2506.19652)

*Lucie Galland, Catherine Pelachaud, Florian Pecune*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, dialogue management, health informatics, personalization

**Relevance Score:** 9

**TL;DR:** Proposes a framework combining LLMs with RL-based dialogue management for goal-oriented open-ended dialogues, enhancing adaptability and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve open-ended dialogue systems by integrating LLMs with reinforcement learning for personalized patient interactions.

**Method:** Utilizes hierarchical reinforcement learning and meta-learning to model dialogue phases and adapt to user profiles.

**Key Contributions:**

	1. Integration of LLMs with a reinforcement learning dialogue manager
	2. Application of hierarchical reinforcement learning for dialogue structuring
	3. Demonstration of improved performance over existing LLMs in goal-oriented dialogues

**Result:** The dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward during Motivational Interviews.

**Limitations:** 

**Conclusion:** The framework shows promise for developing responsive dialogue systems tailored to individual user needs in health contexts.

**Abstract:** In this work, we propose a novel framework that integrates large language models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a specific goal. By leveraging hierarchical reinforcement learning to model the structured phases of dialogue and employ meta-learning to enhance adaptability across diverse user profiles, our approach enhances adaptability and efficiency, enabling the system to learn from limited data, transition fluidly between dialogue phases, and personalize responses to heterogeneous patient needs. We apply our framework to Motivational Interviews, aiming to foster behavior change, and demonstrate that the proposed dialogue manager outperforms a state-of-the-art LLM baseline in terms of reward, showing a potential benefit of conditioning LLMs to create open-ended dialogue systems with specific goals.

</details>


### [57] [Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?](https://arxiv.org/abs/2506.19733)

*Chuxuan Hu, Yuxuan Zhu, Antony Kellermann, Caleb Biddulph, Suppakit Waiwitlikhit, Jason Benn, Daniel Kang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Language Models, Generalization, Human-Computer Interaction, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper examines the generalizability of reinforcement post training (RPT) on large language models (LLMs) across multiple domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how well improvements from reinforcement post training (RPT) of LLMs generalize to unseen domains, as prior evaluations have not sufficiently addressed this.

**Method:** Two studies are conducted: (1) an observational study comparing a variety of RPT models against their base models across seen and unseen domains, and (2) an interventional study where LLMs are fine-tuned using RPT on single domains and tested on multiple domains.

**Key Contributions:**

	1. First comprehensive evaluation of RPT across multiple domains.
	2. Identification of inconsistencies in generalization of RPT improvements.
	3. Insights into the reasoning patterns that affect LLM performance post-RPT.

**Result:** Both studies indicate that while RPT improves performance on tasks related to fine-tuning data, the generalization to different reasoning patterns in unseen domains is inconsistent, with gains potentially vanishing.

**Limitations:** Generalization inconsistencies may limit the applicability of RPT in diverse reasoning contexts.

**Conclusion:** RPT shows promise in enhancing LLM reasoning abilities, but the variability in generalization to new domains raises questions about its applicability across diverse reasoning tasks.

**Abstract:** Reinforcement post training (RPT) has recently shown promise in improving the reasoning abilities of large language models (LLMs). However, it remains unclear how well these improvements generalize to new domains, as prior work evaluates RPT models on data from the same domains used for fine-tuning. To understand the generalizability of RPT, we conduct two studies. (1) Observational: We compare a wide range of open-weight RPT models against their corresponding base models across multiple domains, including both seen and unseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs with RPT on single domains and evaluate their performance across multiple domains. Both studies converge on the same conclusion that, although RPT brings substantial gains on tasks similar to the fine-tuning data, the gains generalize inconsistently and can vanish on domains with different reasoning patterns.

</details>


### [58] [Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach](https://arxiv.org/abs/2506.19750)

*Takashi Nishibayashi, Seiji Kanazawa, Kumpei Yamada*

**Main category:** cs.CL

**Keywords:** Symptom Checkers, Synthetic Vignettes, Rare Diseases, Diagnostic Performance, Human Phenotype Ontology

**Relevance Score:** 6

**TL;DR:** This study introduces a Synthetic Vignette Simulation Approach to evaluate how algorithm updates in Symptom Checkers (SCs) affect diagnostic performance for rare diseases, overcoming data acquisition challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the impact of algorithm updates on diagnostic performance for rare diseases, which is often hindered by limited evaluation data and high costs of manual vignette creation.

**Method:** Synthetic vignettes were generated using disease-phenotype annotations from the Human Phenotype Ontology (HPO), simulating SC interviews to estimate algorithm update impacts on diagnostic performance.

**Key Contributions:**

	1. Introduction of a new method for synthetic vignette generation
	2. Validation of predictive capabilities of the approach using historical SC updates
	3. Highlighting the importance of frequency information in evaluating diagnostic performance

**Result:** The method achieved an R^2 of 0.831 for recall@8 and 0.78 for precision@8 with diseases having frequency data, demonstrating predictive capability for algorithm updates; however, inaccuracies arose for diseases lacking frequency data.

**Limitations:** The method's accuracy is contingent on the availability of frequency information for rare diseases.

**Conclusion:** The Synthetic Vignette Simulation Approach offers a cost-effective and transparent method for evaluating SC algorithm changes for rare diseases, potentially leading to improved diagnostic support.

**Abstract:** Background: Symptom Checkers (SCs) provide users with personalized medical information. To prevent performance degradation from algorithm updates, SC developers must evaluate diagnostic performance changes for individual diseases before deployment. However, acquiring sufficient evaluation data for rare diseases is difficult, and manually creating numerous clinical vignettes is costly and impractical. Objective: This study proposes and validates a novel Synthetic Vignette Simulation Approach to evaluate diagnostic performance changes for individual rare diseases following SC algorithm updates. Methods: We used disease-phenotype annotations from the Human Phenotype Ontology (HPO), a knowledge database for rare diseases, to generate synthetic vignettes. With these, we simulated SC interviews to estimate the impact of algorithm updates on real-world diagnostic performance. The method's effectiveness was evaluated retrospectively by comparing estimated values with actual metric changes using the R 2(R-squared) coefficient. Results: The experiment included eight past SC algorithm updates. For updates on diseases with frequency information in HPO (n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8 change, it was 0.78 (p=0.047), indicating the method can predict post-deployment performance. In contrast, large prediction errors occurred for diseases without frequency information (n=3), highlighting its importance. The manual effort to map HPO phenotypes to SC symptoms was approximately 2 hours per disease. Conclusions: Our method enables pre-deployment evaluation of SC algorithm changes for individual rare diseases using a publicly available, expert-created knowledge base. This transparent and low-cost approach allows developers to efficiently improve diagnostic performance for rare diseases, potentially enhancing support for early diagnosis.

</details>


### [59] [Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis](https://arxiv.org/abs/2506.19753)

*Omar A. Essameldin, Ali O. Elbeih, Wael H. Gomaa, Wael F. Elsersy*

**Main category:** cs.CL

**Keywords:** Arabic dialects, NLP, MARBERTv2, classification, language models

**Relevance Score:** 5

**TL;DR:** This paper classifies 18 Arabic dialects from tweets using various models, achieving notable accuracy with MARBERTv2.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing need for effective identification of Arabic dialects in digital communication due to their diversity across 22 countries.

**Method:** The study employs RNN models, Transformer models, and large language models (LLMs) using prompt engineering to classify dialects in the QADI dataset of Arabic tweets.

**Key Contributions:**

	1. Introduces classification of 18 Arabic dialects from tweets.
	2. Demonstrates effectiveness of MARBERTv2 and NLP techniques in dialect identification.
	3. Identifies linguistic issues relevant to Arabic dialects.

**Result:** MARBERTv2 achieved the best performance with 65% accuracy and 64% F1-score, highlighting the efficacy of advanced NLP techniques in dialect identification.

**Limitations:** 

**Conclusion:** This research enhances applications like personalized chatbots and social media monitoring, improving accessibility for Arabic-speaking communities.

**Abstract:** The Arabic language is among the most popular languages in the world with a huge variety of dialects spoken in 22 countries. In this study, we address the problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets. RNN models, Transformer models, and large language models (LLMs) via prompt engineering are created and tested. Among these, MARBERTv2 performed best with 65% accuracy and 64% F1-score. Through the use of state-of-the-art preprocessing techniques and the latest NLP models, this paper identifies the most significant linguistic issues in Arabic dialect identification. The results corroborate applications like personalized chatbots that respond in users' dialects, social media monitoring, and greater accessibility for Arabic communities.

</details>


### [60] [Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR](https://arxiv.org/abs/2506.19761)

*Martin Ratajczak, Jean-Philippe Robichaud, Jennifer Drexler Fox*

**Main category:** cs.CL

**Keywords:** Speech Recognition, Machine Learning, Recurrent Attention, Long-form ASR, Regularization

**Relevance Score:** 6

**TL;DR:** This paper presents advancements in long-form speech recognition using bidirectional recurrent attention (RA) layers, which offer efficiency and accuracy improvements over multi-head attention (MHA).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for improved long-form speech recognition due to the limitations of existing models that struggle with long sequences.

**Method:** The authors investigate the use of bidirectional RA layers as an alternative to MHA, developing a long-form training paradigm and introducing Direction Dropout as a new regularization technique.

**Key Contributions:**

	1. Introduction of bidirectional recurrent attention layers for ASR.
	2. Development of Direction Dropout regularization method.
	3. Establishment of a long-form training paradigm that enhances RA performance.

**Result:** Bidirectional RA layers match the accuracy of MHA for both short and long-form ASR, demonstrating a 44% higher throughput compared to established methods while also improving the accuracy/throughput trade-off.

**Limitations:** 

**Conclusion:** The findings suggest that RA layers are a viable and efficient alternative for long-form speech recognition, enabling superior performance in accuracy and throughput.

**Abstract:** Long-form speech recognition is an application area of increasing research focus. ASR models based on multi-head attention (MHA) are ill-suited to long-form ASR because of their quadratic complexity in sequence length. We build on recent work that has investigated linear complexity recurrent attention (RA) layers for ASR. We find that bidirectional RA layers can match the accuracy of MHA for both short- and long-form applications. We present a strong limited-context attention (LCA) baseline, and show that RA layers are just as accurate while being more efficient. We develop a long-form training paradigm which further improves RA performance, leading to better accuracy than LCA with 44% higher throughput. We also present Direction Dropout, a novel regularization method that improves accuracy, provides fine-grained control of the accuracy/throughput trade-off of bidirectional RA, and enables a new alternating directions decoding mode with even higher throughput.

</details>


### [61] [SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning](https://arxiv.org/abs/2506.19767)

*Yuqian Fu, Tinghong Chen, Jiajun Chai, Xihuai Wang, Songjun Tu, Guojun Yin, Wei Lin, Qichao Zhang, Yuanheng Zhu, Dongbin Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Supervised Fine-Tuning, Reinforcement Learning, Entropy, Reasoning Benchmarks

**Relevance Score:** 8

**TL;DR:** The paper proposes Supervised Reinforcement Fine-Tuning (SRFT), which unifies Supervised Fine-Tuning and Reinforcement Learning methods for large language models (LLMs) to enhance reasoning task performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of effectively integrating Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) in enhancing large language models' reasoning capabilities.

**Method:** An entropy-aware weighting mechanism is used to propose Supervised Reinforcement Fine-Tuning (SRFT), combining SFT and RL in a single-stage method that directly optimizes the LLM with demonstrations and self-exploration rollouts.

**Key Contributions:**

	1. Proposed a unified fine-tuning method (SRFT) for LLMs integrating SFT and RL.
	2. Introduced entropy-aware weighting mechanisms to optimize training effectiveness.
	3. Demonstrated significant performance improvements on reasoning benchmarks.

**Result:** SRFT demonstrated an average accuracy of 59.1%, surpassing zero-RL methods by 9.0% on five mathematical reasoning benchmarks and by 10.9% on three out-of-distribution benchmarks.

**Limitations:** 

**Conclusion:** The proposed SRFT method effectively improves the integration of SFT and RL for LLMs, leading to notable performance enhancements in reasoning tasks.

**Abstract:** Large language models (LLMs) have achieved remarkable progress in reasoning tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) remains a fundamental challenge. Through comprehensive analysis of token distributions, learning dynamics, and integration mechanisms from entropy-based perspectives, we reveal key differences between these paradigms: SFT induces coarse-grained global changes to LLM policy distributions, while RL performs fine-grained selective optimizations, with entropy serving as a critical indicator of training effectiveness. Building on these observations, we propose Supervised Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach simultaneously applies SFT and RL to directly optimize the LLM using demonstrations and self-exploration rollouts rather than through two-stage sequential methods. Extensive experiments show that SRFT achieves 59.1% average accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning benchmarks and 10.9% on three out-of-distribution benchmarks.

</details>


### [62] [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)

*Yuqi Zhu, Yi Zhong, Jintian Zhang, Ziheng Zhang, Shuofei Qiao, Yujie Luo, Lun Du, Da Zheng, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, data analysis, strategic planning, interaction design, data quality

**Relevance Score:** 9

**TL;DR:** This study explores ways to enhance data analysis capabilities in open-source Large Language Models (LLMs) by evaluating model performance across data understanding, code generation, and strategic planning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of open-source LLMs in reasoning-intensive data analysis tasks and to improve their effectiveness.

**Method:** The authors curate a seed dataset of diverse scenarios and evaluate models across three dimensions: data understanding, code generation, and strategic planning.

**Key Contributions:**

	1. Identifying strategic planning as a key factor in model performance.
	2. Demonstrating the impact of interaction design and task complexity on reasoning capabilities.
	3. Developing a novel data synthesis methodology to enhance LLMs' analytical reasoning.

**Result:** The study finds that strategic planning quality is the primary determinant of model performance, and that interaction design and data quality significantly influence reasoning capabilities.

**Limitations:** The paper is noted as a work in progress, which may imply that results and methodologies are not yet fully validated.

**Conclusion:** Insights from the analysis lead to the development of a data synthesis methodology that improves the analytical reasoning capabilities of open-source LLMs.

**Abstract:** Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate models across three dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities.

</details>


### [63] [How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?](https://arxiv.org/abs/2506.19831)

*Abdullah Khondoker, Enam Ahmed Taufik, Md. Iftekhar Islam Tashik, S M Ishtiak Mahmud, Farig Sadeque*

**Main category:** cs.CL

**Keywords:** communal violence, BanglaBERT, NLP, social media, machine learning

**Relevance Score:** 4

**TL;DR:** This study enhances the detection of communal violent text in Bengali using a fine-tuned BanglaBERT model, achieving a macro F1 score of 0.63. It addresses data imbalance and explores model limitations in understanding context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the growing issue of cyber hatred which leads to communal violence, this study seeks to improve the classification of violent text on social media.

**Method:** A fine-tuned BanglaBERT model was developed, and an ensemble model was created to address data imbalance by adding more instances, achieving improved classification scores.

**Key Contributions:**

	1. Development of a fine-tuned BanglaBERT model for communal violence detection
	2. Addressing data imbalance with an expanded dataset
	3. Application of LIME for model interpretability

**Result:** The fine-tuned ensemble model achieved a macro F1 score of 0.63 and identified limitations in context understanding leading to misclassifications.

**Limitations:** The models struggled with context understanding and distinguishing between closely related terms, leading to errors in classification.

**Conclusion:** The work demonstrates the potential of NLP tools, like the fine-tuned models and interpretability techniques, in reducing online communal violence and provides a basis for future improvements.

**Abstract:** The spread of cyber hatred has led to communal violence, fueling aggression and conflicts between various religious, ethnic, and social groups, posing a significant threat to social harmony. Despite its critical importance, the classification of communal violent text remains an underexplored area in existing research. This study aims to enhance the accuracy of detecting text that incites communal violence, focusing specifically on Bengali textual data sourced from social media platforms. We introduce a fine-tuned BanglaBERT model tailored for this task, achieving a macro F1 score of 0.60. To address the issue of data imbalance, our dataset was expanded by adding 1,794 instances, which facilitated the development and evaluation of a fine-tuned ensemble model. This ensemble model demonstrated an improved performance, achieving a macro F1 score of 0.63, thus highlighting its effectiveness in this domain. In addition to quantitative performance metrics, qualitative analysis revealed instances where the models struggled with context understanding, leading to occasional misclassifications, even when predictions were made with high confidence. Through analyzing the cosine similarity between words, we identified certain limitations in the pre-trained BanglaBERT models, particularly in their ability to distinguish between closely related communal and non-communal terms. To further interpret the model's decisions, we applied LIME, which helped to uncover specific areas where the model struggled in understanding context, contributing to errors in classification. These findings highlight the promise of NLP and interpretability tools in reducing online communal violence. Our work contributes to the growing body of research in communal violence detection and offers a foundation for future studies aiming to refine these techniques for better accuracy and societal impact.

</details>


### [64] [MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration](https://arxiv.org/abs/2506.19835)

*Yucheng Zhou, Lingran Song, Jianbing Shen*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical diagnostics, multi-modal framework, agent-based systems, performance improvement

**Relevance Score:** 9

**TL;DR:** Introduction of a modular multi-agent framework (MAM) that enhances medical diagnostics using LLMs in a collaborative way.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations in knowledge update costs, comprehensiveness, and flexibility in current multimodal medical LLMs.

**Method:** MAM decomposes the medical diagnostic process into specialized roles (General Practitioner, Specialist Team, Radiologist, Medical Assistant, Director), each represented by LLM-based agents, enabling efficient updates and improved diagnostics.

**Key Contributions:**

	1. Modular Multi-Agent Framework for medical diagnostics
	2. Role assignment for improved diagnostic performance
	3. Performance improvements over baseline models across multimodal datasets

**Result:** MAM demonstrates significant performance improvements (18% to 365%) over baseline models on multimodal medical datasets.

**Limitations:** 

**Conclusion:** The modular framework MAM outperforms existing modality-specific LLMs and facilitates better medical diagnostics.

**Abstract:** Recent advancements in medical Large Language Models (LLMs) have showcased their powerful reasoning and diagnostic capabilities. Despite their success, current unified multimodal medical LLMs face limitations in knowledge update costs, comprehensiveness, and flexibility. To address these challenges, we introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis (MAM). Inspired by our empirical findings highlighting the benefits of role assignment and diagnostic discernment in LLMs, MAM decomposes the medical diagnostic process into specialized roles: a General Practitioner, Specialist Team, Radiologist, Medical Assistant, and Director, each embodied by an LLM-based agent. This modular and collaborative framework enables efficient knowledge updates and leverages existing medical LLMs and knowledge bases. Extensive experimental evaluations conducted on a wide range of publicly accessible multimodal medical datasets, incorporating text, image, audio, and video modalities, demonstrate that MAM consistently surpasses the performance of modality-specific LLMs. Notably, MAM achieves significant performance improvements ranging from 18% to 365% compared to baseline models. Our code is released at https://github.com/yczhou001/MAM.

</details>


### [65] [Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for English to Indian Languages](https://arxiv.org/abs/2308.16075)

*Baban Gain, Dibyanayan Bandyopadhyay, Samrat Mukherjee, Chandranath Adak, Asif Ekbal*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, multimodal inputs, visual information, textual noise, translation performance

**Relevance Score:** 4

**TL;DR:** This study explores the integration of visual information into Neural Machine Translation (NMT) systems, revealing that images can be redundant in high-resource settings but beneficial in noisy environments.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the impact of multimodal inputs, specifically visual features, on the performance of neural machine translation systems.

**Method:** The study incorporated image features into a large-scale, pre-trained unimodal NMT system and assessed its performance against text-only models under varying levels of textual noise.

**Key Contributions:**

	1. Demonstrated the redundancy of images in high-resource NMT settings.
	2. Identified varying effectiveness of visual context based on the level of noise in source text.
	3. Introduced synthetic noise to evaluate the impact on translation performance.

**Result:** The multimodal models slightly outperformed text-only models in noisy scenarios, showing that the effectiveness of visual context fluctuates based on source text noise.

**Limitations:** 

**Conclusion:** Visual and textual information should be combined for optimal NMT performance, especially in noisy contexts, suggesting new research directions for Noisy Neural Machine Translation.

**Abstract:** Neural Machine Translation (NMT) has made remarkable progress using large-scale textual data, but the potential of incorporating multimodal inputs, especially visual information, remains underexplored in high-resource settings. While prior research has focused on using multimodal data in low-resource scenarios, this study examines how image features impact translation when added to a large-scale, pre-trained unimodal NMT system. Surprisingly, the study finds that images might be redundant in this context. Additionally, the research introduces synthetic noise to assess whether images help the model handle textual noise. Multimodal models slightly outperform text-only models in noisy settings, even when random images are used. The study's experiments translate from English to Hindi, Bengali, and Malayalam, significantly outperforming state-of-the-art benchmarks. Interestingly, the effect of visual context varies with the level of source text noise: no visual context works best for non-noisy translations, cropped image features are optimal for low noise, and full image features perform better in high-noise scenarios. This sheds light on the role of visual context, especially in noisy settings, and opens up a new research direction for Noisy Neural Machine Translation in multimodal setups. The research emphasizes the importance of combining visual and textual information to improve translation across various environments. Our code is publicly available at https://github.com/babangain/indicMMT.

</details>


### [66] [PATCH! {P}sychometrics-{A}ssis{T}ed Ben{CH}marking of Large Language Models against Human Populations: A Case Study of Proficiency in 8th Grade Mathematics](https://arxiv.org/abs/2404.01799)

*Qixiang Fang, Daniel L. Oberski, Dong Nguyen*

**Main category:** cs.CL

**Keywords:** large language models, psychometrics, benchmarking, academic proficiency, evaluation techniques

**Relevance Score:** 9

**TL;DR:** This paper proposes PATCH, a novel framework for benchmarking large language models (LLMs) using psychometrics to improve measurement quality and comparability with human populations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for LLMs often measure academic proficiency but have limitations including unclear measurement quality and lack of assessment on an item level. The authors propose using psychometrics to address these challenges.

**Method:** The authors introduce PATCH, a framework that leverages psychometric principles to create benchmarks that allow valid comparisons between LLMs and human populations. They apply this framework to evaluate several LLMs' proficiency in 8th grade mathematics.

**Key Contributions:**

	1. Introduction of the PATCH framework for psychometrics-assisted benchmarking of LLMs
	2. Demonstration of PATCH's application in evaluating LLMs against human populations
	3. Release of four high-quality datasets for LLM proficiency assessment

**Result:** The evaluation using PATCH shows that current benchmarks yield different outcomes compared to the psychometrics-based approach. Additionally, four high-quality datasets are released to facilitate LLM proficiency assessment in mathematics and science against human populations.

**Limitations:** 

**Conclusion:** The proposed PATCH framework addresses significant limitations of current LLM benchmarks and enhances the accuracy of measuring LLM academic proficiency.

**Abstract:** Many existing benchmarks of large (multimodal) language models (LLMs) focus on measuring LLMs' academic proficiency, often with also an interest in comparing model performance with human test takers'. While such benchmarks have proven key to the development of LLMs, they suffer from several limitations, including questionable measurement quality (e.g., Do they measure what they are supposed to in a reliable way?), lack of quality assessment on the item level (e.g., Are some items more important or difficult than others?) and unclear human population reference (e.g., To whom can the model be compared?). In response to these challenges, we propose leveraging knowledge from psychometrics -- a field dedicated to the measurement of latent variables like academic proficiency -- into LLM benchmarking. We make four primary contributions. First, we reflect on current LLM benchmark developments and contrast them with psychometrics-based test development. Second, we introduce PATCH: a novel framework for {P}sychometrics-{A}ssis{T}ed ben{CH}marking of LLMs. PATCH addresses the aforementioned limitations. In particular, PATCH enables valid comparison between LLMs and human populations. Third, we demonstrate PATCH by measuring several LLMs' proficiency in 8th grade mathematics against 56 human populations. We show that adopting a psychometrics-based approach yields evaluation outcomes that diverge from those based on current benchmarking practices. Fourth, we release 4 high-quality datasets to support measuring and comparing LLM proficiency in grade school mathematics and science with human populations.

</details>


### [67] [Detecting Machine-Generated Texts: Not Just "AI vs Humans" and Explainability is Complicated](https://arxiv.org/abs/2406.18259)

*Jiazhou Ji, Ruizhe Li, Shujun Li, Jie Guo, Weidong Qiu, Zheng Huang, Chiyu Chen, Xiaoyu Jiang, Xinru Lu*

**Main category:** cs.CL

**Keywords:** LLM detection, ternary classification, explainability, text classification, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces a ternary classification scheme for detecting LLM-generated texts, emphasizing the importance of an 'undecided' category for improved explainability in detection results.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexities in distinguishing between human and LLM-generated texts, especially as LLMs become more advanced and their texts harder to classify.

**Method:** The authors created four new datasets of various texts from LLMs and human authors, performed binary classification tests on detection methods, and developed a ternary classification approach involving a new 'undecided' category. They utilized human annotators to provide labels and explanation notes, analyzing the behavior of existing detectors in this new context.

**Key Contributions:**

	1. Introduction of a ternary classification scheme with an 'undecided' category
	2. Creation of new datasets for improved detection analysis
	3. Guidelines for enhancing explainability in detection systems

**Result:** The study found that the 'undecided' category significantly enhances explainability and understanding of detection results, indicating the need for clearer communication to users about the nature of detected texts.

**Limitations:** The study may need further validation across more diverse datasets and additional detectors to fully confirm the robustness of the findings.

**Conclusion:** The findings underscore the necessity of incorporating explainability into LLM text detection systems and provide guidelines for developing better detection methods that facilitate user understanding.

**Abstract:** As LLMs rapidly advance, increasing concerns arise regarding risks about actual authorship of texts we see online and in real world. The task of distinguishing LLM-authored texts is complicated by the nuanced and overlapping behaviors of both machines and humans. In this paper, we challenge the current practice of considering LLM-generated text detection a binary classification task of differentiating human from AI. Instead, we introduce a novel ternary text classification scheme, adding an "undecided" category for texts that could be attributed to either source, and we show that this new category is crucial to understand how to make the detection result more explainable to lay users. This research shifts the paradigm from merely classifying to explaining machine-generated texts, emphasizing need for detectors to provide clear and understandable explanations to users. Our study involves creating four new datasets comprised of texts from various LLMs and human authors. Based on new datasets, we performed binary classification tests to ascertain the most effective SOTA detection methods and identified SOTA LLMs capable of producing harder-to-detect texts. We constructed a new dataset of texts generated by two top-performing LLMs and human authors, and asked three human annotators to produce ternary labels with explanation notes. This dataset was used to investigate how three top-performing SOTA detectors behave in new ternary classification context. Our results highlight why "undecided" category is much needed from the viewpoint of explainability. Additionally, we conducted an analysis of explainability of the three best-performing detectors and the explanation notes of the human annotators, revealing insights about the complexity of explainable detection of machine-generated texts. Finally, we propose guidelines for developing future detection systems with improved explanatory power.

</details>


### [68] [LEVOS: Leveraging Vocabulary Overlap with Sanskrit to Generate Technical Lexicons in Indian Languages](https://arxiv.org/abs/2407.06331)

*Karthika N J, Krishnakant Bhatt, Ganesh Ramakrishnan, Preethi Jyothi*

**Main category:** cs.CL

**Keywords:** translation, low-resource languages, Sanskrit, NLP, education

**Relevance Score:** 4

**TL;DR:** This paper presents a novel method for translating technical terms into low-resource Indian languages using Sanskrit-based segments and a Character-level Transformer model for improved accuracy.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in translating technical terms into low-resource Indian languages due to limited parallel data and complex linguistic structures.

**Method:** The approach utilizes character-level segmentation to identify meaningful subword units and employs a Character-level Transformer model for Sanskrit Word Segmentation (CharSS) to handle sandhi and morpho-phonemic complexities.

**Key Contributions:**

	1. Novel use of Sanskrit-based segments for translation
	2. Implementation of character-level segmentation
	3. Human evaluation verifies translation quality

**Result:** The method achieved consistent improvements in translation quality, with average chrF++ scores of 8.46 and 6.79 in two experimental settings.

**Limitations:** 

**Conclusion:** The approach has implications for creating accessible learning materials in Indian languages, promoting inclusivity within low-resource language communities.

**Abstract:** Translating technical terms into lexically similar, low-resource Indian languages remains a challenge due to limited parallel data and the complexity of linguistic structures. We propose a novel use-case of Sanskrit-based segments for linguistically informed translation of such terms, leveraging subword-level similarity and morphological alignment across related languages. Our approach uses character-level segmentation to identify meaningful subword units, facilitating more accurate and context-aware translation. To enable this, we utilize a Character-level Transformer model for Sanskrit Word Segmentation (CharSS), which addresses the complexities of sandhi and morpho-phonemic changes during segmentation. We observe consistent improvements in two experimental settings for technical term translation using Sanskrit-derived segments, averaging 8.46 and 6.79 chrF++ scores, respectively. Further, we conduct a post hoc human evaluation to verify the quality assessment of the translated technical terms using automated metrics. This work has important implications for the education field, especially in creating accessible, high-quality learning materials in Indian languages. By supporting the accurate and linguistically rooted translation of technical content, our approach facilitates inclusivity and aids in bridging the resource gap for learners in low-resource language communities.

</details>


### [69] [Evaluating Transparent Reasoning in Large Language Models for Accountable Critical Tasks](https://arxiv.org/abs/2408.01933)

*Junhao Chen, Bowen Wang, Jiuyang Chang, Yuta Nakashima*

**Main category:** cs.CL

**Keywords:** large language models, medical decision-making, legal decision-making, interpretability, reasoning graphs

**Relevance Score:** 9

**TL;DR:** The paper introduces REACT, a benchmark for evaluating the reasoning capabilities of large language models in high-stakes medical and legal decision-making contexts, emphasizing interpretability and transparency over mere prediction accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The existing benchmarks for large language models often focus primarily on prediction accuracy without considering the interpretability and reasoning capabilities required for accountable decision-making in critical domains such as medicine and law.

**Method:** REACT utilizes expert-annotated clinical and legal cases, employing reasoning graphs to provide structured guidelines for model reasoning, supported by a semi-automatic annotation pipeline to address scalability.

**Key Contributions:**

	1. Development of the REACT benchmark for evaluating reasoning in LLMs
	2. Expert-annotated datasets for medical and legal domains
	3. Introduction of reasoning graphs for structured reasoning

**Result:** Experimental results show that reasoning graphs significantly enhance LLM interpretability and accuracy compared to traditional benchmarks, but gaps still exist in relation to expert-level reasoning.

**Limitations:** The model reasoning still lags behind expert-level performance despite improvements in interpretability and accuracy.

**Conclusion:** While REACT offers a rigorous framework to evaluate LLM reasoning, further research is needed to bridge the gaps in performance compared to human experts.

**Abstract:** This paper introduces REACT, a benchmark designed to rigorously evaluate the reasoning capabilities of large language models (LLMs) within accountable, high-stakes decision-making tasks in medical and legal domains. Unlike traditional benchmarks primarily focused on prediction accuracy, REACT emphasizes transparent and interpretable reasoning, requiring models to align their logic closely with expert-derived procedures. To assess whether LLM reasoning aligns closely with human experts, we annotated 511 clinical cases from the medical domain and 86 legal cases from the legal domain, each enriched with detailed expert-extracted rationales and evidence supporting each step of the reasoning process. These annotations were guided by carefully constructed reasoning graphs, which explicitly encode domain-specific inference structures and decision criteria derived by domain experts. These reasoning graphs serve not only as standards for expert annotation but also as structured guidelines enabling models to reason transparently and step-by-step. To address the scalability challenges of manual annotation, we further developed a semi-automatic annotation pipeline leveraging expert-defined reasoning graph templates to efficiently generate new graphs, exploring the potential to extend our approach into additional critical domains. Experimental results demonstrate that reasoning graphs substantially enhance the interpretability and accuracy of LLM reasoning compared to traditional baselines, although significant gaps remain relative to expert-level reasoning performance.

</details>


### [70] [Rational Metareasoning for Large Language Models](https://arxiv.org/abs/2410.05563)

*C. Nicol√≤ De Sabbata, Theodore R. Sumers, Badr AlKhamissi, Antoine Bosselut, Thomas L. Griffiths*

**Main category:** cs.CL

**Keywords:** Large Language Models, Metareasoning, Inference Cost Optimization

**Relevance Score:** 8

**TL;DR:** This work presents a method to optimize reasoning in LLMs by selectively engaging reasoning steps, reducing inference costs while maintaining performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing size and adoption of LLMs result in higher inference costs, prompting the need for optimization in reasoning techniques.

**Method:** A novel approach that uses a reward function based on computational models of metareasoning to train LLMs, penalizing unnecessary reasoning steps with Expert Iteration.

**Key Contributions:**

	1. Introduction of a reward function for metareasoning in LLMs
	2. Reduction of inference costs by 20-37%
	3. Use of Expert Iteration for training LLMs on selective reasoning

**Result:** The proposed method reduces inference costs by 20-37% across three models while preserving task performance on various datasets.

**Limitations:** 

**Conclusion:** Selective reasoning in LLMs can lead to significant efficiency improvements without sacrificing capability.

**Abstract:** Being prompted to engage in reasoning has emerged as a core technique for using large language models (LLMs), deploying additional inference-time compute to improve task performance. However, as LLMs increase in both size and adoption, inference costs are correspondingly becoming increasingly burdensome. How, then, might we optimize reasoning's cost-performance tradeoff? This work introduces a novel approach based on computational models of metareasoning used in cognitive science, training LLMs to selectively use intermediate reasoning steps only when necessary. We first develop a reward function that incorporates the Value of Computation by penalizing unnecessary reasoning, then use this reward function with Expert Iteration to train the LLM. Compared to few-shot chain-of-thought prompting and STaR, our method significantly reduces inference costs (20-37\% fewer tokens generated across three models) while maintaining task performance across diverse datasets.

</details>


### [71] [ADVLLM: Iterative Self-Tuning LLMs for Enhanced Jailbreaking Capabilities](https://arxiv.org/abs/2410.18469)

*Chung-En Sun, Xiaodong Liu, Weiwei Yang, Tsui-Wei Weng, Hao Cheng, Aidan San, Michel Galley, Jianfeng Gao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Adversarial Suffixes, Safety Alignment, Jailbreak Attacks, Machine Learning

**Relevance Score:** 8

**TL;DR:** ADV-LLM introduces an efficient method for generating adversarial suffixes that enhance jailbreak attacks on LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational inefficiencies and low success rates of current methods for generating adversarial suffixes in LLMs.

**Method:** An iterative self-tuning process is developed to craft adversarial suffixes, significantly reducing generation costs and enhancing attack success rates.

**Key Contributions:**

	1. Develops ADV-LLM for efficient adversarial suffix generation.
	2. Achieves high attack success rates on multiple LLMs.
	3. Provides datasets for safety alignment research.

**Result:** Achieves nearly 100% Attack Success Rate on various open-source LLMs, and strong transferability to closed-source models, including 99% on GPT-3.5 and 49% on GPT-4.

**Limitations:** 

**Conclusion:** ADV-LLM not only enhances the jailbreak capabilities of LLMs but also aids in future safety alignment research by generating datasets for studying LLM safety.

**Abstract:** Recent research has shown that Large Language Models (LLMs) are vulnerable to automated jailbreak attacks, where adversarial suffixes crafted by algorithms appended to harmful queries bypass safety alignment and trigger unintended responses. Current methods for generating these suffixes are computationally expensive and have low Attack Success Rates (ASR), especially against well-aligned models like Llama2 and Llama3. To overcome these limitations, we introduce ADV-LLM, an iterative self-tuning process that crafts adversarial LLMs with enhanced jailbreak ability. Our framework significantly reduces the computational cost of generating adversarial suffixes while achieving nearly 100\% ASR on various open-source LLMs. Moreover, it exhibits strong attack transferability to closed-source models, achieving 99\% ASR on GPT-3.5 and 49\% ASR on GPT-4, despite being optimized solely on Llama3. Beyond improving jailbreak ability, ADV-LLM provides valuable insights for future safety alignment research through its ability to generate large datasets for studying LLM safety.

</details>


### [72] [Collage: Decomposable Rapid Prototyping for Information Extraction on Scientific PDFs](https://arxiv.org/abs/2410.23478)

*Sireesh Gururaja, Yueheng Zhang, Guannan Tang, Tianhao Zhang, Kevin Murphy, Yu-Tsen Yi, Junwon Seo, Anthony Rollett, Emma Strubell*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Information Extraction, Scientific Documents, PDF, HuggingFace

**Relevance Score:** 9

**TL;DR:** Collage is a tool for evaluating and prototyping information extraction models on scientific PDFs, providing insights into processing failures and supporting various NLP models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of comparing domain-specific information extraction models for scientific documents, especially PDF formats, which are prevalent in academia.

**Method:** Collage allows the evaluation of token classifiers, LLMs, and other models, while providing interfaces for experimentation and debugging of modeling pipelines.

**Key Contributions:**

	1. Rapid prototyping and evaluation of NLP models on scientific PDFs.
	2. Granular inspection and debugging capabilities for users and developers.
	3. Support for various models through extensible software interfaces.

**Result:** Collage supports rapid prototyping and visualization of information extraction models, improving the ability to conduct literature reviews in fields like materials science.

**Limitations:** 

**Conclusion:** The tool enhances the understanding and usability of NLP models for non-experts and facilitates better evaluation of model performance.

**Abstract:** Recent years in NLP have seen the continued development of domain-specific information extraction tools for scientific documents, alongside the release of increasingly multimodal pretrained transformer models. While the opportunity for scientists outside of NLP to evaluate and apply such systems to their own domains has never been clearer, these models are difficult to compare: they accept different input formats, are often black-box and give little insight into processing failures, and rarely handle PDF documents, the most common format of scientific publication. In this work, we present Collage, a tool designed for rapid prototyping, visualization, and evaluation of different information extraction models on scientific PDFs. Collage allows the use and evaluation of any HuggingFace token classifier, several LLMs, and multiple other task-specific models out of the box, and provides extensible software interfaces to accelerate experimentation with new models. Further, we enable both developers and users of NLP-based tools to inspect, debug, and better understand modeling pipelines by providing granular views of intermediate states of processing. We demonstrate our system in the context of information extraction to assist with literature review in materials science.

</details>


### [73] [Entropy and type-token ratio in gigaword corpora](https://arxiv.org/abs/2411.10227)

*Pablo Rosillo-Rodes, Maxi San Miguel, David Sanchez*

**Main category:** cs.CL

**Keywords:** lexical diversity, type-token ratio, word entropy, linguistic datasets, statistical laws

**Relevance Score:** 4

**TL;DR:** Investigation of lexical diversity metrics using six linguistic datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the relationship between lexical diversity metrics in different languages and types of texts.

**Method:** Analyzed type-token ratio and word entropy in English, Spanish, and Turkish datasets comprising books, articles, and tweets.

**Key Contributions:**

	1. Empirical analysis of lexical diversity across different languages and text genres.
	2. Derivation of a functional relationship between type-token ratio and word entropy.
	3. Validation of Zipf and Heaps laws in the context of lexical diversity.

**Result:** Established a functional relationship between entropy and type-token ratio that holds across different languages and text types.

**Limitations:** 

**Conclusion:** Empirical and analytical findings support a derived relationship influenced by statistical laws of natural language, enhancing understanding of lexical diversity.

**Abstract:** There are different ways of measuring diversity in complex systems. In particular, in language, lexical diversity is characterized in terms of the type-token ratio and the word entropy. We here investigate both diversity metrics in six massive linguistic datasets in English, Spanish, and Turkish, consisting of books, news articles, and tweets. These gigaword corpora correspond to languages with distinct morphological features and differ in registers and genres, thus constituting a varied testbed for a quantitative approach to lexical diversity. We unveil an empirical functional relation between entropy and type-token ratio of texts of a given corpus and language, which is a consequence of the statistical laws observed in natural language. Further, in the limit of large text lengths we find an analytical expression for this relation relying on both Zipf and Heaps laws that agrees with our empirical findings.

</details>


### [74] [Sensitive Content Classification in Social Media: A Holistic Resource and Evaluation](https://arxiv.org/abs/2411.19832)

*Dimosthenis Antypas, Indira Sen, Carla Perez-Almendros, Jose Camacho-Collados, Francesco Barbieri*

**Main category:** cs.CL

**Keywords:** content moderation, sensitive content, large language models, dataset, social media

**Relevance Score:** 8

**TL;DR:** A novel dataset for social media content moderation across six sensitive categories shows significant performance improvements for LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the detection of sensitive content in large datasets and address the limitations of current moderation tools and existing datasets focusing mainly on toxic language.

**Method:** Collected and annotated a dataset for social media moderation across six categories, fine-tuning LLMs on this dataset to evaluate detection performance.

**Key Contributions:**

	1. Creation of a unified dataset for six sensitive categories
	2. Demonstration of improved LLM performance on this dataset
	3. Addressing gaps in existing content moderation frameworks

**Result:** Fine-tuning LLMs on the novel dataset significantly improves detection performance by 10-15% compared to popular existing models like LLaMA and proprietary OpenAI models.

**Limitations:** 

**Conclusion:** The new dataset provides a more effective solution for moderating sensitive content, outperforming existing moderation tools and APIs.

**Abstract:** The detection of sensitive content in large datasets is crucial for ensuring that shared and analysed data is free from harmful material. However, current moderation tools, such as external APIs, suffer from limitations in customisation, accuracy across diverse sensitive categories, and privacy concerns. Additionally, existing datasets and open-source models focus predominantly on toxic language, leaving gaps in detecting other sensitive categories such as substance abuse or self-harm. In this paper, we put forward a unified dataset tailored for social media content moderation across six sensitive categories: conflictual language, profanity, sexually explicit material, drug-related content, self-harm, and spam. By collecting and annotating data with consistent retrieval strategies and guidelines, we address the shortcomings of previous focalised research. Our analysis demonstrates that fine-tuning large language models (LLMs) on this novel dataset yields significant improvements in detection performance compared to open off-the-shelf models such as LLaMA, and even proprietary OpenAI models, which underperform by 10-15% overall. This limitation is even more pronounced on popular moderation APIs, which cannot be easily tailored to specific sensitive content categories, among others.

</details>


### [75] ["I know myself better, but not really greatly": How Well Can LLMs Detect and Explain LLM-Generated Texts?](https://arxiv.org/abs/2502.12743)

*Jiazhou Ji, Jie Guo, Weidong Qiu, Zheng Huang, Yang Xu, Xinru Lu, Xiaoyu Jiang, Ruizhe Li, Shujun Li*

**Main category:** cs.CL

**Keywords:** LLM detection, self-detection, classification, explanation quality, explanation failures

**Relevance Score:** 8

**TL;DR:** This paper explores the detection capabilities of various LLMs to differentiate between human-generated and LLM-generated texts, revealing suboptimal performance and key explanation failures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the risks associated with LLM misuse by evaluating their capabilities in distinguishing between human and LLM-generated texts.

**Method:** The study evaluates six close- and open-source LLMs in both binary and ternary classification settings, analyzing detection accuracy and explanation quality using a human-annotated dataset.

**Key Contributions:**

	1. Investigation of self-detection vs cross-detection capabilities in LLMs.
	2. Introduction of a ternary classification framework for improved detection accuracy.
	3. Identification of key explanation failures in LLM outputs.

**Result:** Self-detection consistently outperforms cross-detection across all models, while the ternary classification framework enhances detection accuracy and explanation quality.

**Limitations:** Reliance on inaccurate features, hallucinations, and flawed reasoning in LLM explanations.

**Conclusion:** The findings point out the limitations of LLMs in self-detection and self-explanation, indicating a need for further research to improve generalizability and address issues like overfitting.

**Abstract:** Distinguishing between human- and LLM-generated texts is crucial given the risks associated with misuse of LLMs. This paper investigates detection and explanation capabilities of current LLMs across two settings: binary (human vs. LLM-generated) and ternary classification (including an ``undecided'' class). We evaluate 6 close- and open-source LLMs of varying sizes and find that self-detection (LLMs identifying their own outputs) consistently outperforms cross-detection (identifying outputs from other LLMs), though both remain suboptimal. Introducing a ternary classification framework improves both detection accuracy and explanation quality across all models. Through comprehensive quantitative and qualitative analyses using our human-annotated dataset, we identify key explanation failures, primarily reliance on inaccurate features, hallucinations, and flawed reasoning. Our findings underscore the limitations of current LLMs in self-detection and self-explanation, highlighting the need for further research to address overfitting and enhance generalizability.

</details>


### [76] [Language Model Re-rankers are Fooled by Lexical Similarities](https://arxiv.org/abs/2502.17036)

*Lovisa Hagstr√∂m, Ercong Nie, Ruben Halifa, Helmut Schmid, Richard Johansson, Alexander Junge*

**Main category:** cs.CL

**Keywords:** language model re-rankers, retrieval-augmented generation, BM25, NQ, LitQA2, DRUID

**Relevance Score:** 8

**TL;DR:** This paper evaluates 6 different language model re-rankers used in retrieval-augmented generation and finds that they often underperform compared to a simple BM25 baseline, suggesting the need for better evaluation datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether language model re-rankers indeed improve retrieval results over traditional methods like BM25, and to investigate their weaknesses.

**Method:** Evaluated 6 LM re-rankers on the NQ, LitQA2, and DRUID datasets and introduced a novel separation metric based on BM25 scores to analyze performance.

**Key Contributions:**

	1. Evaluation of 6 LM re-rankers against traditional methods
	2. Identification of re-ranker errors related to lexical dissimilarities
	3. Introduction of a novel separation metric based on BM25 scores

**Result:** LM re-rankers struggled to outperform the BM25 baseline on the DRUID dataset, with identified errors stemming from lexical dissimilarities.

**Limitations:** The performance issues were primarily observed on the DRUID dataset and the improvements were mainly effective for the NQ dataset.

**Conclusion:** The results suggest that current LM re-rankers have significant weaknesses and highlight the necessity of more realistic evaluation datasets.

**Abstract:** Language model (LM) re-rankers are used to refine retrieval results for retrieval-augmented generation (RAG). They are more expensive than lexical matching methods like BM25 but assumed to better process semantic information and the relations between the query and the retrieved answers. To understand whether LM re-rankers always live up to this assumption, we evaluate 6 different LM re-rankers on the NQ, LitQA2 and DRUID datasets. Our results show that LM re-rankers struggle to outperform a simple BM25 baseline on DRUID. Leveraging a novel separation metric based on BM25 scores, we explain and identify re-ranker errors stemming from lexical dissimilarities. We also investigate different methods to improve LM re-ranker performance and find these methods mainly useful for NQ. Taken together, our work identifies and explains weaknesses of LM re-rankers and points to the need for more adversarial and realistic datasets for their evaluation.

</details>


### [77] [A Foundational individual Mobility Prediction Model based on Open-Source Large Language Models](https://arxiv.org/abs/2503.16553)

*Zhenlin Qin, Leizhen Wang, Francisco Camara Pereira, Zhenliang Ma*

**Main category:** cs.CL

**Keywords:** Large Language Models, mobility prediction, fine-tuning, transferability, open source

**Relevance Score:** 8

**TL;DR:** This paper presents a unified fine-tuning framework for training LLM-based mobility prediction models, improving accuracy and transferability across diverse contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing LLM-based mobility prediction models that struggle with adaptability to different contexts and datasets.

**Method:** A unified fine-tuning framework for a foundational open-source LLM tailored for mobility prediction tasks was developed and tested across six real-world datasets.

**Key Contributions:**

	1. Development of a unified fine-tuning framework for LLMs in mobility prediction
	2. Validation through experiments on six diverse mobility datasets
	3. Demonstrated improved prediction accuracy and adaptability over existing models

**Result:** The proposed model outperformed state-of-the-art deep learning and LLM-based models in prediction accuracy and transferability.

**Limitations:** 

**Conclusion:** The findings suggest that the unified fine-tuning approach significantly enhances the applicability of LLMs for predicting individual mobility across varied contexts.

**Abstract:** Large Language Models (LLMs) are widely applied to domain-specific tasks due to their massive general knowledge and remarkable inference capacities. Current studies on LLMs have shown immense potential in applying LLMs to model individual mobility prediction problems. However, most LLM-based mobility prediction models only train on specific datasets or use single well-designed prompts, leading to difficulty in adapting to different cities and users with diverse contexts. To fill these gaps, this paper proposes a unified fine-tuning framework to train a foundational open source LLM-based mobility prediction model. We conducted extensive experiments on six real-world mobility datasets to validate the proposed model. The results showed that the proposed model achieved the best performance in prediction accuracy and transferability over state-of-the-art models based on deep learning and LLMs.

</details>


### [78] [Large Language Models as Span Annotators](https://arxiv.org/abs/2504.08697)

*Zdenƒõk Kasner, Vil√©m Zouhar, Patr√≠cia Schmidtov√°, Ivan Kart√°ƒç, Krist√Ωna Onderkov√°, Ond≈ôej Pl√°tek, Dimitra Gkatzia, Saad Mahamood, Ond≈ôej Du≈°ek, Simone Balloccu*

**Main category:** cs.CL

**Keywords:** span annotation, large language models, human annotators, text evaluation, data-to-text generation

**Relevance Score:** 8

**TL;DR:** This study explores the use of large language models (LLMs) for span annotation, demonstrating their effectiveness and cost-efficiency compared to human annotators.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the limitations of traditional span annotation methods and providing a flexible, cost-effective alternative using LLMs.

**Method:** Comparison of LLMs and skilled human annotators on three span annotation tasks, including evaluation of data-to-text generation, identification of translation errors, and detection of propaganda techniques.

**Key Contributions:**

	1. Demonstrated LLMs' capability in span annotation tasks.
	2. Provided a comparative analysis of LLMs and human annotators' performance.
	3. Released a large dataset for ongoing research.

**Result:** LLMs achieved inter-annotator agreement comparable to human annotators and made errors at a similar rate, with significantly lower cost per output annotation.

**Limitations:** The potential biases and limitations of LLM outputs were not exhaustively explored.

**Conclusion:** LLMs can effectively replace or assist human annotators in span annotation tasks, providing a valuable resource for further research with a released dataset of 40k annotations.

**Abstract:** Span annotation is the task of localizing and classifying text spans according to custom guidelines. Annotated spans can be used to analyze and evaluate high-quality texts for which single-score metrics fail to provide actionable feedback. Until recently, span annotation was limited to human annotators or fine-tuned models. In this study, we show that large language models (LLMs) can serve as flexible and cost-effective span annotation backbones. To demonstrate their utility, we compare LLMs to skilled human annotators on three diverse span annotation tasks: evaluating data-to-text generation, identifying translation errors, and detecting propaganda techniques. We demonstrate that LLMs achieve inter-annotator agreement (IAA) comparable to human annotators at a fraction of a cost per output annotation. We also manually analyze model outputs, finding that LLMs make errors at a similar rate to human annotators. We release the dataset of more than 40k model and human annotations for further research.

</details>


### [79] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)

*Alan Chen, Jack Merullo, Alessandro Stolfo, Ellie Pavlick*

**Main category:** cs.CL

**Keywords:** Language Models, Sparse Autoencoders, Feature Transferability, Training Efficiency, Representation Spaces

**Relevance Score:** 8

**TL;DR:** This paper shows that affine mappings between language model residual streams can transfer features effectively, using Sparse Autoencoders (SAEs) to demonstrate significant training efficiency gains.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve representation transfer between language models of different sizes and enhance the training efficiency of Sparse Autoencoders (SAEs).

**Method:** Applying affine mappings to transfer weights of Sparse Autoencoders between models of different sizes and analyzing representation spaces and feature transferability.

**Key Contributions:**

	1. Introduced a method for transferring features between models using affine mappings.
	2. Showed cost-saving benefits of transferring SAEs from small to large models.
	3. Analyzed the transferability of various feature types across model sizes.

**Result:** Demonstrated that small and large models possess similar representation spaces, leading to a potential 50% reduction in training costs when using transferred SAEs as initialization for larger models.

**Limitations:** 

**Conclusion:** Feature transfer is effective between models of different sizes, with significant differences in how semantic and structural features transfer; this approach improves training efficiency for SAEs.

**Abstract:** In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the weights of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. In particular, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs.

</details>


### [80] [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)

*Jiaxing Guo, Wenjie Yang, Shengzhong Zhang, Tongshan Xu, Lun Du, Da Zheng, Zengfeng Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical problem-solving, Step-by-step verification, Reward hacking, Reasoning flaws

**Relevance Score:** 9

**TL;DR:** Introducing ParaStepVerifier for step-by-step verification of mathematical problem-solving in LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper highlights the issue of reward hacking in LLMs, where correct answers are produced via incorrect reasoning. This raises concerns about the reliability of LLMs in mathematical tasks.

**Method:** ParaStepVerifier, a new methodology designed to perform detailed, step-by-step verification of the reasoning processes in LLMs when solving mathematical problems.

**Key Contributions:**

	1. Introduction of MathOlympiadEval dataset with fine-grained annotations
	2. Development of ParaStepVerifier methodology
	3. Demonstrated improvement in identifying flawed reasoning in LLM outputs

**Result:** Empirical results show that ParaStepVerifier significantly improves the identification of flawed reasoning steps compared to previous baselines, particularly in handling complex, multi-step problems.

**Limitations:** 

**Conclusion:** ParaStepVerifier presents a promising solution for enhancing the evaluation and training of LLMs to ensure authentic mathematical reasoning capabilities.

**Abstract:** Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable success in mathematical problem-solving. However, this success often masks a critical issue: models frequently achieve correct answers through fundamentally unsound reasoning processes, a phenomenon indicative of reward hacking. We introduce MathOlympiadEval, a new dataset with fine-grained annotations, which reveals a significant gap between LLMs' answer correctness and their low process correctness. Existing automated methods like LLM-as-a-judge struggle to reliably detect these reasoning flaws. To address this, we propose ParaStepVerifier, a novel methodology for meticulous, step-by-step verification of mathematical solutions. ParaStepVerifier identifies incorrect reasoning steps. Empirical results demonstrate that ParaStepVerifier substantially improves the accuracy of identifying flawed solutions compared to baselines, especially for complex, multi-step problems. This offers a more robust path towards evaluating and training LLMs with genuine mathematical reasoning.

</details>
