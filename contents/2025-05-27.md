# 2025-05-27

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 25]

- [cs.CL](#cs.CL) [Total: 381]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [The Relational Origins of Rules in Online Communities](https://arxiv.org/abs/2505.18318)

*Charles Kiene, Sohyeon Hwang, Nathan TeBlunthuis, Carl Colglazier, Aaron Shaw, Benjamin Mako Hill*

**Main category:** cs.HC

**Keywords:** online communities, rules, governance, relational dynamics, community identity

**Relevance Score:** 4

**TL;DR:** This study investigates the adoption and change of rules in online communities through interviews with community leaders, revealing seven processes linked to relational dynamics and community identity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the processes behind the adoption and change of rules in online communities, which previous studies have largely overlooked.

**Method:** Grounded theory-based analysis of 40 in-depth interviews with community leaders from various online platforms.

**Key Contributions:**

	1. Identification of seven processes involved in rule adoption and change
	2. Reinforcement of the need for relational dynamics in rulemaking
	3. Extension of social computing and organizational theories regarding rule origins

**Result:** Identified seven processes of rule adoption and change, emphasizing relational factors and community identity over purely functional reasons.

**Limitations:** 

**Conclusion:** The findings suggest that rule-making in communities is influenced by relational dynamics and recommend integrating these aspects into governance design.

**Abstract:** Where do rules come from in online communities? While prior studies of online community governance in social computing have sought to characterize rules by their functions within communities and documented practices of rule enforcement, they have largely overlooked rule adoption and change. This study investigates how and why online communities adopt and change their rules. We conducted a grounded theory-based analysis of 40 in-depth interviews with community leaders from subreddits, Fandom wikis, and Fediverse servers, and identified seven processes involved in the adoption of online community rules. Our findings reveal that, beyond regulating behavior and solving functional intra-community problems, rules are also adopted and changed for relational reasons, such as signaling or reinforcing community legitimacy and identity to other communities. While rule change was often prompted by challenges during community growth or decline, change also depended on volunteer leaders' work capacity, the presence of member feedback mechanisms, and relational dynamics between leaders and members. The findings extend prior theories from social computing and organizational research, illustrating how institutionalist and ecological explanations of the relational origins of rules complement more functional accounts. The results also support design recommendations that integrate the relational aspects of rules and rulemaking to facilitate successful governance across communities' lifecycles.

</details>


### [2] [Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights](https://arxiv.org/abs/2505.18385)

*Jeba Rezwana, Corey Ford*

**Main category:** cs.HC

**Keywords:** AI communication, human-AI co-creation, Framework for AI Communication, user experience, collaborative design

**Relevance Score:** 8

**TL;DR:** This paper presents the Framework for AI Communication (FAICO), developed to enhance effective communication between AI and humans in co-creative contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current co-creative AI systems often fail to communicate effectively, which hampers collaboration potential.

**Method:** The framework was developed through a systematic review of 107 papers and refined via focus groups with experts in AI, HCI, and design.

**Key Contributions:**

	1. Introduction of the Framework for AI Communication (FAICO).
	2. Preliminary guidelines for human-centered AI communication based on user feedback.
	3. Emphasis on the importance of a feedback loop and context in AI communication.

**Result:** Participants preferred a human-AI feedback loop and emphasized context in communication for better mutual understanding.

**Limitations:** The findings are based on a limited preliminary study with focus groups, which may not represent all user perspectives.

**Conclusion:** The study provides preliminary guidelines and strategies for applying the FAICO framework to improve human-centered AI communication.

**Abstract:** Effective communication between AI and humans is essential for successful human-AI co-creation. However, many current co-creative AI systems lack effective communication, which limits their potential for collaboration. This paper presents the initial design of the Framework for AI Communication (FAICO) for co-creative AI, developed through a systematic review of 107 full-length papers. FAICO presents key aspects of AI communication and their impact on user experience, offering preliminary guidelines for designing human-centered AI communication. To improve the framework, we conducted a preliminary study with two focus groups involving skilled individuals in AI, HCI, and design. These sessions sought to understand participants' preferences for AI communication, gather their perceptions of the framework, collect feedback for refinement, and explore its use in co-creative domains like collaborative writing and design. Our findings reveal a preference for a human-AI feedback loop over linear communication and emphasize the importance of context in fostering mutual understanding. Based on these insights, we propose actionable strategies for applying FAICO in practice and future directions, marking the first step toward developing comprehensive guidelines for designing effective human-centered AI communication in co-creation.

</details>


### [3] [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)

*Ugur Kursuncu, Trilok Padhi, Gaurav Sinha, Abdulkadir Erol, Jaya Krishna Mandivarapu, Christopher R. Larrison*

**Main category:** cs.HC

**Keywords:** Large Language Models, Anxiety Support, Mental Health, Fine-tuning, Naturalistic Data

**Relevance Score:** 9

**TL;DR:** This study evaluates the use of Large Language Models (LLMs) for anxiety support by analyzing their performance on user-generated content from the r/Anxiety subreddit.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing demand for accessible mental health support using LLMs in sensitive domains like anxiety support while acknowledging the workforce shortages.

**Method:** A systematic evaluation framework assessing linguistic quality, safety and trustworthiness, and supportiveness using real user-generated posts for prompting and fine-tuning LLMs (GPT and Llama).

**Key Contributions:**

	1. Evaluation of LLMs in a mental health context for anxiety support.
	2. Identification of risks associated with fine-tuning LLMs on social media data.
	3. Comparison of performance between different LLMs (GPT and Llama) in supportiveness.

**Result:** Fine-tuning LLMs on anxiety-related data improved linguistic quality but increased toxicity and bias, reducing emotional responsiveness. GPT was found to be more supportive than Llama.

**Limitations:** LLMs exhibited limited empathy and increased bias and toxicity when fine-tuned on unprocessed data.

**Conclusion:** While LLMs can assist in anxiety support, careful consideration is necessary due to the potential risks of increased toxicity and diminished empathy when fine-tuned on unprocessed social media content.

**Abstract:** The growing demand for accessible mental health support, compounded by workforce shortages and logistical barriers, has led to increased interest in utilizing Large Language Models (LLMs) for scalable and real-time assistance. However, their use in sensitive domains such as anxiety support remains underexamined. This study presents a systematic evaluation of LLMs (GPT and Llama) for their potential utility in anxiety support by using real user-generated posts from the r/Anxiety subreddit for both prompting and fine-tuning. Our approach utilizes a mixed-method evaluation framework incorporating three main categories of criteria: (i) linguistic quality, (ii) safety and trustworthiness, and (iii) supportiveness. Results show that fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic quality but increased toxicity and bias, and diminished emotional responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more supportive overall. Our findings highlight the risks of fine-tuning LLMs on unprocessed social media content without mitigation strategies.

</details>


### [4] [Applying Ontologies and Knowledge Augmented Large Language Models to Industrial Automation: A Decision-Making Guidance for Achieving Human-Robot Collaboration in Industry 5.0](https://arxiv.org/abs/2505.18553)

*John Oyekan, Christopher Turner, Michael Bax, Erich Graf*

**Main category:** cs.HC

**Keywords:** Large Language Models, Human-Robot Collaboration, Industry 5.0, Natural Language Processing, Knowledge Graphs

**Relevance Score:** 7

**TL;DR:** This paper discusses the application of Large Language Models (LLMs) in Industry 5.0, focusing on decision-making for using LLMs versus other NLP techniques like ontologies and knowledge graphs in manufacturing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing interest in applying AI, particularly LLMs, in manufacturing systems highlights the need for clarity on their appropriate use compared to traditional NLP methods, especially in human-robot collaboration contexts.

**Method:** The paper presents a comparative framework examining the effectiveness of LLMs, ontologies, and knowledge graphs across various industrial scenarios, encompassing the design-to-manufacture process.

**Key Contributions:**

	1. Decision-making guidance for selecting between LLMs and traditional NLP techniques
	2. Insights on LLMs' role in human-robot collaboration
	3. Proposed architecture for a Large Knowledge Language Model that emphasizes transparency and adaptability.

**Result:** It identifies specific use cases where LLMs can enhance human-robot collaboration while affirming the importance of ontologies and knowledge graphs in simpler, resource-constrained environments.

**Limitations:** Challenges include computational costs and issues with interpretability in deploying language-based AI technologies.

**Conclusion:** The findings provide a roadmap for manufacturers to effectively implement language-based AI tools, supporting sustainable and resilient manufacturing practices.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has resulted in interest in their potential applications within manufacturing systems, particularly in the context of Industry 5.0. However, determining when to implement LLMs versus other Natural Language Processing (NLP) techniques, ontologies or knowledge graphs, remains an open question. This paper offers decision-making guidance for selecting the most suitable technique in various industrial contexts, emphasizing human-robot collaboration and resilience in manufacturing. We examine the origins and unique strengths of LLMs, ontologies, and knowledge graphs, assessing their effectiveness across different industrial scenarios based on the number of domains or disciplines required to bring a product from design to manufacture. Through this comparative framework, we explore specific use cases where LLMs could enhance robotics for human-robot collaboration, while underscoring the continued relevance of ontologies and knowledge graphs in low-dependency or resource-constrained sectors. Additionally, we address the practical challenges of deploying these technologies, such as computational cost and interpretability, providing a roadmap for manufacturers to navigate the evolving landscape of Language based AI tools in Industry 5.0. Our findings offer a foundation for informed decision-making, helping industry professionals optimize the use of Language Based models for sustainable, resilient, and human-centric manufacturing. We also propose a Large Knowledge Language Model architecture that offers the potential for transparency and configuration based on complexity of task and computing resources available.

</details>


### [5] [SPIRAL integration of generative AI in an undergraduate creative media course: effects on self-efficacy and career outcome expectations](https://arxiv.org/abs/2505.18771)

*Troy Schotter, Saba Kawas, James Prather, Juho Leinonen, Jon Ippolito, Greg L Nelson*

**Main category:** cs.HC

**Keywords:** generative AI, pedagogy, self-efficacy, career interests, creative media

**Relevance Score:** 4

**TL;DR:** This study investigates how the SPIRAL integration of generative AI in a creative media course affects students' self-efficacy and career interests.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of different pedagogical strategies for integrating generative AI on students' self-efficacy and career interests in computing education.

**Method:** A mixed methods approach involving quantitative and qualitative analysis, longitudinal interviews, and thematic analysis, conducted in an introductory undergraduate course with 31 students.

**Key Contributions:**

	1. Novel SPIRAL integration strategy for generative AI in education
	2. Mixed methods analysis of self-efficacy and career interests
	3. Insights into student perceptions of AI's impact on careers

**Result:** Positive changes in students' creative media self-efficacy and generative AI use self-efficacy were observed, alongside mixed results for ethical AI use self-efficacy. The integration influenced career interests neutrally or positively, widening perceived options for students.

**Limitations:** 

**Conclusion:** Careful pedagogical sequencing can mitigate negative impacts of AI, promoting ethical AI use and maintaining a neutral or positive effect on career formation.

**Abstract:** Computing education and computing students are rapidly integrating generative AI, but we know relatively little about how different pedagogical strategies for intentionally integrating generative AI affect students' self-efficacy and career interests. This study investigates a SPIRAL integration of generative AI (Skills Practiced Independently, Revisited with AI Later), implemented in an introductory undergraduate creative media and technology course in Fall 2023 (n=31). Students first developed domain skills for half the semester, then revisited earlier material integrating using generative AI, with explicit instruction on how to use it critically and ethically. We contribute a mixed methods quantitative and qualitative analysis of changes in self-efficacy and career interests over time, including longitudinal qualitative interviews (n=9) and thematic analysis. We found positive changes in both students' creative media self-efficacy and generative AI use self-efficacy, and mixed changes for ethical generative AI use self-efficacy. We also found students experienced demystification, transitioning from initial fear about generative AI taking over their fields and jobs, to doubting AI capability to do so and/or that society will push back against AI, through personal use of AI and observing others' use of AI vicariously. For career interests, our SPIRAL integration of generative AI use appeared to have either a neutral or positive influence on students, including widening their perceived career options, depending on their view of how AI would influence the career itself. These findings suggest that careful pedagogical sequencing can mitigate some potential negative impacts of AI, while promoting ethical and critical AI use that supports or has a neutral effect on students' career formation. To our knowledge our SPIRAL integration strategy applied to generative AI integration is novel.

</details>


### [6] [Literature review on assistive technologies for people with Parkinson's disease](https://arxiv.org/abs/2505.18862)

*Subek Acharya, Sansrit Paudel*

**Main category:** cs.HC

**Keywords:** Parkinson's Disease, assistive technologies, machine learning, artificial intelligence, neurodegenerative disorders

**Relevance Score:** 8

**TL;DR:** This review examines assistive technologies (ATs) for managing Parkinson's Disease (PD), emphasizing wearable devices, robotics, AI, and ML, while identifying gaps in addressing non-motor symptoms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of assistive technologies in enhancing the quality of life for individuals with Parkinson's Disease, particularly in managing motor symptoms.

**Method:** The review analyzes various assistive technologies, including both established and emerging innovations, and discusses their impact on motor and non-motor symptoms of PD.

**Key Contributions:**

	1. Comprehensive overview of current and emerging assistive technologies for PD.
	2. Identification of significant gaps regarding the treatment of non-motor symptoms.
	3. Discussion on the integration of AI and ML into assistive technologies for personalized solutions.

**Result:** The review reveals that while ATs significantly help manage motor symptoms like freezing of gait, substantial gaps remain in addressing non-motor symptoms such as sleep dysfunction and mental health.

**Limitations:** The review primarily focuses on the effectiveness of ATs for motor symptoms, with less emphasis on non-motor interventions.

**Conclusion:** The study highlights the potential of ATs integrated with AI and ML for better PD management but calls for further research to personalize and improve interventions for non-motor symptoms.

**Abstract:** Parkinson's Disease (PD) is a neurodegenerative disorder that significantly impacts motor and non-motor functions. There is currently no treatment that slows or stops neurodegeneration in PD. In this context, assistive technologies (ATs) have emerged as vital tools to aid people with Parkinson's and significantly improve their quality of life. This review explores a broad spectrum of ATs, including wearable and cueing devices, exoskeletons, robotics, virtual reality, voice and video-assisted technologies, and emerging innovations such as artificial intelligence (AI), machine learning (ML), and the Internet of Things (IoT). The review highlights ATs' significant role in addressing motor symptoms such as freezing of gait (FOG) and gait and posture disorders. However, it also identifies significant gaps in addressing non-motor symptoms such as sleep dysfunction and mental health. Similarly, the research identifies substantial potential in the further implementation of deep learning, AI, IOT technologies. Overall, this review highlights the transformative potential of AT in PD management while identifying gaps that future research should address to ensure personalized, accessible, and effective solutions.

</details>


### [7] [Toward Human Centered Interactive Clinical Question Answering System](https://arxiv.org/abs/2505.18928)

*Dina Albassam*

**Main category:** cs.HC

**Keywords:** interactive QA system, clinical notes, large language models

**Relevance Score:** 9

**TL;DR:** An interactive QA system enables physicians to query clinical notes via text or voice, providing extractive answers directly highlighted in the notes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Unstructured clinical notes contain vital patient information but are difficult for physicians to interpret. Large language models, although promising, often lack usability and transparency in clinical settings.

**Method:** Built using OpenAI models with zero-shot prompting; evaluated across metrics like exact string match and semantic similarity with multiple clinical personas.

**Key Contributions:**

	1. Introduced an interactive QA system for clinical notes
	2. Implemented query via text and voice with extractive answers
	3. Evaluated usability with simulated physician and nurse personas

**Result:** Exact match scores ranged from 47 to 62 percent, while semantic similarity scores exceeded 87 percent, indicating strong contextual alignment.

**Limitations:** Limited to the evaluation scores and feedback from a small number of clinical personas.

**Conclusion:** The system demonstrated strengths in intuitive design and answer accessibility, with feedback indicating areas for improving explanation clarity.

**Abstract:** Unstructured clinical notes contain essential patient information but are challenging for physicians to search and interpret efficiently. Although large language models (LLMs) have shown promise in question answering (QA), most existing systems lack transparency, usability, and alignment with clinical workflows. This work introduces an interactive QA system that enables physicians to query clinical notes via text or voice and receive extractive answers highlighted directly in the note for traceability.   The system was built using OpenAI models with zero-shot prompting and evaluated across multiple metrics, including exact string match, word overlap, SentenceTransformer similarity, and BERTScore. Results show that while exact match scores ranged from 47 to 62 percent, semantic similarity scores exceeded 87 percent, indicating strong contextual alignment even when wording varied.   To assess usability, the system was also evaluated using simulated clinical personas. Seven diverse physician and nurse personas interacted with the system across scenario-based tasks and provided structured feedback. The evaluations highlighted strengths in intuitive design and answer accessibility, alongside opportunities for enhancing explanation clarity.

</details>


### [8] [Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems](https://arxiv.org/abs/2505.19101)

*Vaishali Dhanoa, Anton Wolter, Gabriela Molina León, Hans-Jörg Schulz, Niklas Elmqvist*

**Main category:** cs.HC

**Keywords:** agentic visualization, human agency, AI integration, visualization systems, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper discusses the integration of autonomous agents powered by Large Language Models in the field of visualization, focusing on preserving human agency while enhancing analytical capabilities. It proposes design patterns for effective agentic visualization systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to integrate AI agents in visualization while maintaining human oversight and decision-making capability.

**Method:** The authors reinterpret existing visualization systems through an agentic lens, analyzing how semi-automated and fully automated AI components can be incorporated effectively.

**Key Contributions:**

	1. Introduction of design patterns for agentic visualization systems
	2. Analysis of human agency in the context of AI integration in visualization
	3. Framework for communication and coordination between human users and AI agents.

**Result:** The analysis led to the extraction of design patterns for agentic visualization, outlining roles, communication, and coordination between human users and AI agents.

**Limitations:** The paper does not provide empirical testing of the proposed design patterns in real-world applications.

**Conclusion:** The proposed design patterns serve as a foundation for creating future visualization systems that effectively utilize AI while preserving human control.

**Abstract:** Autonomous agents powered by Large Language Models are transforming AI, creating an imperative for the visualization field to embrace agentic frameworks. However, our field's focus on a human in the sensemaking loop raises critical questions about autonomy, delegation, and coordination for such \textit{agentic visualization} that preserve human agency while amplifying analytical capabilities. This paper addresses these questions by reinterpreting existing visualization systems with semi-automated or fully automatic AI components through an agentic lens. Based on this analysis, we extract a collection of design patterns for agentic visualization, including agentic roles, communication and coordination. These patterns provide a foundation for future agentic visualization systems that effectively harness AI agents while maintaining human insight and control.

</details>


### [9] [What do Blind and Low-Vision People Really Want from Assistive Smart Devices? Comparison of the Literature with a Focus Study](https://arxiv.org/abs/2505.19325)

*Bhanuka Gamage, Thanh-Toan Do, Nicholas Seow Chiang Price, Arthur Lowery, Kim Marriott*

**Main category:** cs.HC

**Keywords:** assistive technology, blindness, low vision, AI applications, user preferences

**Relevance Score:** 8

**TL;DR:** This paper investigates the alignment of AI assistive technologies for blind or low-vision (BLV) individuals with their actual needs and preferences, highlighting a disconnect between research focus and user preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether AI technologies aimed at assisting people who are blind or have low-vision align with their actual needs and preferences.

**Method:** The study analyzed 646 recent studies on assistive AI techniques and conducted interviews with 24 BLV individuals to determine their preferred applications.

**Key Contributions:**

	1. Analysis of 646 AI assistive technology studies for BLV individuals.
	2. Interviews with BLV individuals to determine preferred AI applications.
	3. Identification of user preferences for conversational agents and head-mounted devices.

**Result:** A weak positive correlation was found between the tasks considered important by BLV participants and the tasks prioritized in the literature; participants favored conversational agents and head-mounted devices.

**Limitations:** The study's sample size for user interviews was limited to 24 BLV individuals, which may not represent the broader population.

**Conclusion:** The findings highlight a mismatch between AI research focus and the preferences of BLV users, indicating a need for more user-centered design in assistive technologies.

**Abstract:** Over the last decade there has been considerable research into how artificial intelligence (AI), specifically computer vision, can assist people who are blind or have low-vision (BLV) to understand their environment. However, there has been almost no research into whether the tasks (object detection, image captioning, text recognition etc.) and devices (smartphones, smart-glasses etc.) investigated by researchers align with the needs and preferences of BLV people. We identified 646 studies published in the last two and a half years that have investigated such assistive AI techniques. We analysed these papers to determine the task, device and participation by BLV individuals. We then interviewed 24 BLV people and asked for their top five AI-based applications and to rank the applications found in the literature. We found only a weak positive correlation between BLV participants' perceived importance of tasks and researchers' focus and that participants prefer conversational agent interface and head-mounted devices.

</details>


### [10] [Knoll: Creating a Knowledge Ecosystem for Large Language Models](https://arxiv.org/abs/2505.19335)

*Dora Zhao, Diyi Yang, Michael S. Bernstein*

**Main category:** cs.HC

**Keywords:** knowledge modules, language models, user-generated content

**Relevance Score:** 9

**TL;DR:** The paper presents Knoll, a software infrastructure enabling users to create and curate custom knowledge modules for language models to enhance response quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of large language models that lack access to specific personal and contextual information that users often require.

**Method:** Knoll allows users to clip content from the web or author documents on platforms like Google Docs and GitHub, which can then be integrated into language models for improved interaction.

**Key Contributions:**

	1. Introduction of a knowledge ecosystem for LLMs
	2. Development of Knoll software for user-created knowledge modules
	3. Public deployment and evaluation of Knoll with positive feedback on response quality

**Result:** A public deployment of Knoll with over 200 users showed that it supports diverse tasks and improves the quality of responses generated by language models.

**Limitations:** The study is limited to users familiar with digital tools, and the effectiveness may vary based on the quality of the modules created.

**Conclusion:** The findings suggest that custom knowledge modules can significantly enhance the utility of language models in providing personalized and relevant responses.

**Abstract:** Large language models are designed to encode general purpose knowledge about the world from Internet data. Yet, a wealth of information falls outside this scope -- ranging from personal preferences to organizational policies, from community-specific advice to up-to-date news -- that users want models to access but remains unavailable. In this paper, we propose a knowledge ecosystem in which end-users can create, curate, and configure custom knowledge modules that are utilized by language models, such as ChatGPT and Claude. To support this vision, we introduce Knoll, a software infrastructure that allows users to make modules by clipping content from the web or authoring shared documents on Google Docs and GitHub, add modules that others have made, and rely on the system to insert relevant knowledge when interacting with an LLM. We conduct a public deployment of Knoll reaching over 200 users who employed the system for a diverse set of tasks including personalized recommendations, advice-seeking, and writing assistance. In our evaluation, we validate that using Knoll improves the quality of generated responses.

</details>


### [11] [It's Not Just Labeling" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features](https://arxiv.org/abs/2505.19419)

*Baichuan Li, Larry Powell, Tracy Hammond*

**Main category:** cs.HC

**Keywords:** sketch-based annotation, large language models, image labeling, healthcare, machine learning

**Relevance Score:** 8

**TL;DR:** This research presents a sketch-based annotation approach leveraging large language models (LLMs) to improve the quality and accessibility of image labeling in machine learning applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The quality of training data is crucial for machine learning performance, especially in fields like healthcare, necessitating better annotation methods.

**Method:** A sketch-based annotation system is developed, using a synthetic dataset to analyze sketch recognition and LLM feedback metrics, alongside various prompting strategies.

**Key Contributions:**

	1. Introduction of a sketch-based annotation method for image labeling
	2. Development of a virtual assistant for non-expert annotation
	3. Investigation of the relationship between sketch recognition features and LLM feedback metrics

**Result:** The study demonstrates that the sketch recognition features positively correlate with LLM feedback metrics, enhancing the reliability of data labeling.

**Limitations:** 

**Conclusion:** A sketch-based virtual assistant is introduced, aimed at simplifying image annotation for non-experts and enhancing the explainability and accessibility of LLM tools.

**Abstract:** The quality of training data is critical to the performance of machine learning applications in domains like transportation, healthcare, and robotics. Accurate image labeling, however, often relies on time-consuming, expert-driven methods with limited feedback. This research introduces a sketch-based annotation approach supported by large language models (LLMs) to reduce technical barriers and enhance accessibility. Using a synthetic dataset, we examine how sketch recognition features relate to LLM feedback metrics, aiming to improve the reliability and interpretability of LLM-assisted labeling. We also explore how prompting strategies and sketch variations influence feedback quality. Our main contribution is a sketch-based virtual assistant that simplifies annotation for non-experts and advances LLM-driven labeling tools in terms of scalability, accessibility, and explainability.

</details>


### [12] [Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems](https://arxiv.org/abs/2505.19441)

*Jing Nathan Yan, Junxiong Wang, Jeffrey M. Rzeszotarski, Allison Koenecke*

**Main category:** cs.HC

**Keywords:** fairness, recommender systems, debiasing, industry practices, algorithmic fairness

**Relevance Score:** 6

**TL;DR:** The paper investigates how industry practitioners deal with fairness in recommender systems, focusing on debiasing practices, metrics, and integration of academic research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address biases in recommender systems and understand industry practices related to fairness standards.

**Method:** Semi-structured interviews with 11 practitioners from large technology companies to analyze their approaches towards fairness in recommendation systems.

**Key Contributions:**

	1. Insights into the perception of fairness by industry practitioners
	2. Identification of preferred debiasing methods
	3. Recommendations for improving fairness practices in the recommender system community

**Result:** Practitioners prefer multi-dimensional debiasing methods over traditional techniques and rely on intuitive metrics rather than academic benchmarks.

**Limitations:** Limited to interviews with 11 practitioners, which may not represent the entire industry.

**Conclusion:** The study reveals challenges in balancing fairness with practical constraints and provides recommendations for enhancing fairness practices in recommender systems.

**Abstract:** The rapid proliferation of recommender systems necessitates robust fairness practices to address inherent biases. Assessing fairness, though, is challenging due to constantly evolving metrics and best practices. This paper analyzes how industry practitioners perceive and incorporate these changing fairness standards in their workflows. Through semi-structured interviews with 11 practitioners from technical teams across a range of large technology companies, we investigate industry implementations of fairness in recommendation system products. We focus on current debiasing practices, applied metrics, collaborative strategies, and integrating academic research into practice. Findings show a preference for multi-dimensional debiasing over traditional demographic methods, and a reliance on intuitive rather than academic metrics. This study also highlights the difficulties in balancing fairness with both the practitioner's individual (bottom-up) roles and organizational (top-down) workplace constraints, including the interplay with legal and compliance experts. Finally, we offer actionable recommendations for the recommender system community and algorithmic fairness practitioners, underlining the need to refine fairness practices continually.

</details>


### [13] [SACM: SEEG-Audio Contrastive Matching for Chinese Speech Decoding](https://arxiv.org/abs/2505.19652)

*Hongbin Wang, Zhihong Jia, Yuanzhong Shen, Ziwei Wang, Siyang Li, Kai Shu, Feng Hu, Dongrui Wu*

**Main category:** cs.HC

**Keywords:** speech disorders, brain-computer interfaces, dysarthria, electroencephalography, contrastive learning

**Relevance Score:** 6

**TL;DR:** This paper presents a framework for speech decoding BCIs using SEEG and audio data from patients with speech disorders.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enhance communication for patients with dysarthria and anarthria by developing brain-computer interfaces that decode speech intentions directly.

**Method:** An experimental protocol was executed involving SEEG and synchronized audio data from eight epilepsy patients performing a word-level reading task, implementing the SEEG and Audio Contrastive Matching (SACM) framework.

**Key Contributions:**

	1. Introduction of the SACM framework for speech decoding BCIs
	2. Demonstrated high decoding accuracy using minimal electrode data
	3. Identified single electrode usefulness in speech detection tasks

**Result:** The SACM framework achieved decoding accuracies surpassing chance levels in speech detection and decoding tasks, with a single sensorimotor cortex electrode performing on par with a full electrode array.

**Limitations:** 

**Conclusion:** The findings contribute to the advancement of online speech decoding BCIs, potentially improving communication capabilities for patients with speech disorders.

**Abstract:** Speech disorders such as dysarthria and anarthria can severely impair the patient's ability to communicate verbally. Speech decoding brain-computer interfaces (BCIs) offer a potential alternative by directly translating speech intentions into spoken words, serving as speech neuroprostheses. This paper reports an experimental protocol for Mandarin Chinese speech decoding BCIs, along with the corresponding decoding algorithms. Stereo-electroencephalography (SEEG) and synchronized audio data were collected from eight drug-resistant epilepsy patients as they conducted a word-level reading task. The proposed SEEG and Audio Contrastive Matching (SACM), a contrastive learning-based framework, achieved decoding accuracies significantly exceeding chance levels in both speech detection and speech decoding tasks. Electrode-wise analysis revealed that a single sensorimotor cortex electrode achieved performance comparable to that of the full electrode array. These findings provide valuable insights for developing more accurate online speech decoding BCIs.

</details>


### [14] [On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction](https://arxiv.org/abs/2505.20068)

*Qingyu Liang, Jaime Banks*

**Main category:** cs.HC

**Keywords:** human-AI interaction, perceived shared understanding, large language models, communication, user perceptions

**Relevance Score:** 9

**TL;DR:** This paper explores the concept of perceived shared understanding (PSU) in human-AI interactions, specifically with large language models, identifying key dimensions that influence user perceptions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how perceived shared understanding affects interactions between humans and AI, particularly in workplace settings.

**Method:** An online survey was conducted to gather user reflections on their experiences interacting with a large language model, followed by inductive thematic analysis to identify underlying dimensions of PSU.

**Key Contributions:**

	1. Identification of eight dimensions of PSU in human-AI interactions
	2. Contribution to the literature on human-AI interaction by exploring PSU
	3. Insights into user perceptions of AI capabilities and limitations

**Result:** The analysis revealed eight dimensions of PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.

**Limitations:** The study is based on user reflections which may not comprehensively capture all dimensions of PSU in HAII.

**Conclusion:** Understanding these dimensions can enhance communication and performance in human-AI interactions, shaping future AI integration in personal and workplace contexts.

**Abstract:** Shared understanding plays a key role in the effective communication in and performance of human-human interactions. With the increasingly common integration of AI into human contexts, the future of personal and workplace interactions will likely see human-AI interaction (HAII) in which the perception of shared understanding is important. Existing literature has addressed the processes and effects of PSU in human-human interactions, but the construal remains underexplored in HAII. To better understand PSU in HAII, we conducted an online survey to collect user reflections on interactions with a large language model when it sunderstanding of a situation was thought to be similar to or different from the participant's. Through inductive thematic analysis, we identified eight dimensions comprising PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.

</details>


### [15] [Understanding and Supporting Co-viewing Comedy in VR with Embodied Expressive Avatars](https://arxiv.org/abs/2505.20082)

*Ryo Ohara, Chi-Lan Yang, Takuji Narumi, Hideaki Kuzuoka*

**Main category:** cs.HC

**Keywords:** embodiment, virtual reality, co-viewing, emotional contagion, social interaction

**Relevance Score:** 9

**TL;DR:** The study investigates how embodied expressive avatars in VR influence co-viewing experiences, focusing on engagement, emotional contagion, and expressive norms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of visible embodied cues in co-viewing experiences on platforms lacking such features, particularly in virtual reality settings.

**Method:** A controlled experiment comparing reactions of participants watching comedy in VR with and without embodied laughter cues, analyzed through mixed methods.

**Key Contributions:**

	1. Demonstrated the impact of VR embodiment on co-viewing engagement
	2. Revealed how laughter cues foster emotional contagion among viewers
	3. Identified new expressive norms in virtual co-viewing scenarios.

**Result:** Participants using embodied avatars showed increased engagement, emotional contagion, and developed new expressive norms related to laughter.

**Limitations:** Limited sample size of 24 participants and specific context of comedy viewing.

**Conclusion:** Embodied laughter cues in VR change the dynamics of co-viewing from individual focus to social interaction, enhancing emotional awareness and expression among viewers.

**Abstract:** Co-viewing videos with family and friends remotely has become prevalent with the support of communication channels such as text messaging or real-time voice chat. However, current co-viewing platforms often lack visible embodied cues, such as body movements and facial expressions. This absence can reduce emotional engagement and the sense of co-presence when people are watching together remotely. Although virtual reality (VR) is an emerging technology that allows individuals to participate in various social activities while embodied as avatars, we still do not fully understand how this embodiment in VR affects co-viewing experiences, particularly in terms of engagement, emotional contagion, and expressive norms. In a controlled experiment involving eight triads of three participants each (N=24), we compared the participants' perceptions and reactions while watching comedy in VR using embodied expressive avatars that displayed visible laughter cues. This was contrasted with a control condition where no such embodied expressions were presented. With a mixed-method analysis, we found that embodied laughter cues shifted participants' engagement from individual immersion to socially coordinated participation. Participants reported heightened self-awareness of emotional expression, greater emotional contagion, and the development of expressive norms surrounding co-viewers' laughter. The result highlighted the tension between individual engagement and interpersonal emotional accommodation when co-viewing with embodied expressive avatars.

</details>


### [16] [Explanation User Interfaces: A Systematic Literature Review](https://arxiv.org/abs/2505.20085)

*Eleonora Cappuccio, Andrea Esposito, Francesco Greco, Giuseppe Desolda, Rosa Lanzilotti, Salvatore Rinzivillo*

**Main category:** cs.HC

**Keywords:** Explainable AI, Explanation User Interfaces, XUIs, HERMES, Systematic Literature Review

**Relevance Score:** 8

**TL;DR:** A systematic review of Explanation User Interfaces (XUIs) in AI, accompanied by a framework (HERMES) to guide their design and evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of designing effective explanations for AI systems, ensuring they are transparent and useful to end-users.

**Method:** Systematic Literature Review on existing Explanation User Interfaces (XUIs), focusing on solutions and design guidelines from the academic literature.

**Key Contributions:**

	1. Systematic literature review of XUIs.
	2. Identification of design guidelines for effective explanations.
	3. Development of the HERMES framework for designing and evaluating XUIs.

**Result:** The review reveals key strategies and design principles employed in XUIs and introduces the HERMES framework to enhance the development of explainable user interfaces.

**Limitations:** The review may not cover all existing literature on XUIs, and the framework's applicability may vary across different contexts.

**Conclusion:** HERMES serves as a practical guide for practitioners and researchers in creating more effective and user-friendly explanations in AI applications.

**Abstract:** Artificial Intelligence (AI) is one of the major technological advancements of this century, bearing incredible potential for users through AI-powered applications and tools in numerous domains. Being often black-box (i.e., its decision-making process is unintelligible), developers typically resort to eXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour of AI models to produce systems that are transparent, fair, reliable, and trustworthy. However, presenting explanations to the user is not trivial and is often left as a secondary aspect of the system's design process, leading to AI systems that are not useful to end-users. This paper presents a Systematic Literature Review on Explanation User Interfaces (XUIs) to gain a deeper understanding of the solutions and design guidelines employed in the academic literature to effectively present explanations to users. To improve the contribution and real-world impact of this survey, we also present a framework for Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide practitioners and academics in the design and evaluation of XUIs.

</details>


### [17] [FairTalk: Facilitating Balanced Participation in Video Conferencing by Implicit Visualization of Predicted Turn-Grabbing Intention](https://arxiv.org/abs/2505.20138)

*Ryo Iijima, Shigeo Yoshida, Atsushi Hashimoto, Jiaxin Ma*

**Main category:** cs.HC

**Keywords:** FairTalk, video conferencing, turn-taking, machine learning, HCI

**Relevance Score:** 7

**TL;DR:** FairTalk is a system designed to create fair speaking opportunities in video conferencing by predicting turn-grabbing intentions using machine learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of ensuring fair opportunities for all participants in video conferencing settings to speak and contribute.

**Method:** FairTalk utilizes a machine learning model trained on videoconference data with positive-unlabeled learning to predict participants' intentions to speak and visualizes these predictions to balance speaking turns.

**Key Contributions:**

	1. Introduction of FairTalk system for fair turn-taking in video conferencing
	2. Utilization of positive-unlabeled learning for training the model
	3. Insights from user studies on the effectiveness and design implications

**Result:** User study results indicate that FairTalk may improve the balance of speaking turns among participants, but feedback showed no significant perceived impact on speaking balance.

**Limitations:** Subjective feedback from users indicated no significant perceived impact, suggesting a need for further investigation.

**Conclusion:** The paper discusses the potential of FairTalk in improving speaking balance and explores design implications from participant interviews.

**Abstract:** Creating fair opportunities for all participants to contribute is a notable challenge in video conferencing. This paper introduces FairTalk, a system that facilitates the subconscious redistribution of speaking opportunities. FairTalk predicts participants' turn-grabbing intentions using a machine learning model trained on web-collected videoconference data with positive-unlabeled learning, where turn-taking detection provides automatic positive labels. To subtly balance speaking turns, the system visualizes predicted intentions by mimicking natural human behaviors associated with the desire to speak. A user study suggests that FairTalk may help improve speaking balance, though subjective feedback indicates no significant perceived impact. We also discuss design implications derived from participant interviews.

</details>


### [18] [AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments](https://arxiv.org/abs/2405.07960)

*Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, Michael Moor*

**Main category:** cs.HC

**Keywords:** Large Language Models, Clinical Decision-Making, AI in Healthcare, Simulation Benchmark, Multimodal Evaluation

**Relevance Score:** 9

**TL;DR:** AgentClinic is a benchmark for evaluating LLMs in clinical settings that simulate complex decision-making and patient interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve evaluation of large language models by capturing the complexity of clinical decision-making beyond static question-answering frameworks.

**Method:** We developed AgentClinic, a multimodal benchmark that evaluates LLMs in simulated clinical environments, including patient interactions, incomplete information situations, and the use of various clinical tools.

**Key Contributions:**

	1. Introduction of the AgentClinic benchmark for evaluating LLMs in clinical scenarios.
	2. Demonstration of the challenges LLMs face in sequential clinical decision-making.
	3. Insights into the performance of various LLMs with different tools in simulated environments.

**Result:** AgentClinic revealed significant challenges in diagnostic accuracy for LLMs in sequential decision-making contexts, with some accuracies dropping below 10%. Claude-3.5 generally outperformed other models, while Llama-3 showed substantial improvements (up to 92%) when using a notebook tool for case notes.

**Limitations:** Challenges include potential biases in agent performance and the limitations of simulation accuracy compared to real clinical environments.

**Conclusion:** The benchmark enables a deeper understanding of LLM capabilities in healthcare settings, highlighting the need for improved tools and workflows for clinical AI applications.

**Abstract:** Evaluating large language models (LLM) in clinical scenarios is crucial to assessing their potential clinical utility. Existing benchmarks rely heavily on static question-answering, which does not accurately depict the complex, sequential nature of clinical decision-making. Here, we introduce AgentClinic, a multimodal agent benchmark for evaluating LLMs in simulated clinical environments that include patient interactions, multimodal data collection under incomplete information, and the usage of various tools, resulting in an in-depth evaluation across nine medical specialties and seven languages. We find that solving MedQA problems in the sequential decision-making format of AgentClinic is considerably more challenging, resulting in diagnostic accuracies that can drop to below a tenth of the original accuracy. Overall, we observe that agents sourced from Claude-3.5 outperform other LLM backbones in most settings. Nevertheless, we see stark differences in the LLMs' ability to make use of tools, such as experiential learning, adaptive retrieval, and reflection cycles. Strikingly, Llama-3 shows up to 92% relative improvements with the notebook tool that allows for writing and editing notes that persist across cases. To further scrutinize our clinical simulations, we leverage real-world electronic health records, perform a clinical reader study, perturb agents with biases, and explore novel patient-centric metrics that this interactive environment firstly enables.

</details>


### [19] [How do Observable Users Decompose D3 Code? A Qualitative Study](https://arxiv.org/abs/2405.14341)

*Melissa Lin, Heer Patel, Medina Lamkin, Hannah Bako, Leilani Battle*

**Main category:** cs.HC

**Keywords:** visualization, toolkits, D3, grammar of graphics, user behavior

**Relevance Score:** 7

**TL;DR:** This paper analyzes how users organize their visualization programs in D3 and identifies three levels of code decomposition that diverge from toolkit developers' intents, suggesting improvements for visualization toolkits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how users organize their visualization code and identify gaps between user practices and toolkit design, ultimately to improve usability and workflows.

**Method:** A qualitative analysis of 715 D3 programs on Observable, complemented by user interviews to correlate program organization with user reasoning.

**Key Contributions:**

	1. Identification of three levels of code decomposition in visualization programs
	2. Comparison of user-made components with the Grammar of Graphics
	3. Suggestions for improving visualization toolkit usability through alignment with user behavior

**Result:** Three levels of program organization were identified: Program-Level, Chart-Level, and Component-Level, with a preference for Component-Level organization. User reasoning often diverges from the Grammar of Graphics, especially for complex visualizations.

**Limitations:** 

**Conclusion:** The findings indicate a need for visualization toolkits to better align with user practices, considering enhancements in learning tools and AI support.

**Abstract:** Many toolkit developers seek to streamline the visualization programming process for their users through structured support such as prescribed templates and example galleries. However, few projects examine how users organize their own visualization programs, how their coding choices may deviate from the intents of toolkit developers, and how these differences may impact visualization prototyping and design. Further, is it possible to infer users' reasoning indirectly through their code, even when users copy code from other sources? Understanding these patterns can reveal opportunities to align toolkit design with actual user behavior, improving usability and supporting more flexible workflows. We explore this question through a qualitative analysis of 715 D3 programs on Observable. We identify three levels of program organization based on how users decompose their code into smaller blocks: Program-, Chart-, and Component-Level code decomposition, with a strong preference for Component-Level reasoning. In a series of interviews, we corroborate that these levels reflect how Observable users reason about visualization programs. We compare common user-made components with those theorized in the Grammar of Graphics to assess overlap in user and toolkit developer reasoning. We find that, while the Grammar of Graphics covers basic visualizations well, it falls short in describing complex visualization types, especially those with animation, interaction, and parameterization components. Our findings highlight how user practices differ from formal grammars and suggest opportunities for rethinking visualization toolkit support, including augmenting learning tools and AI assistants to better reflect real-world coding strategies.

</details>


### [20] [An Empirical Study to Understand How Students Use ChatGPT for Writing Essays](https://arxiv.org/abs/2501.10551)

*Andrew Jelson, Daniel Manesh, Alice Jang, Daniel Dunlap, Sang Won Lee*

**Main category:** cs.HC

**Keywords:** large language models, ChatGPT, writing education, student engagement, AI usage patterns

**Relevance Score:** 9

**TL;DR:** The study investigates how students use ChatGPT for writing tasks, revealing diverse usage patterns influenced by demographics and attitudes towards AI, impacting their engagement and ownership of the writing process.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the implications of student engagement with ChatGPT on learning processes, given the concerns about its impact on critical thinking and writing skills.

**Method:** An online study with 70 students performing an essay-writing task using a custom platform that tracked their queries to ChatGPT, followed by analysis of usage patterns and correlations with various metrics.

**Key Contributions:**

	1. Identification of demographic influences on AI usage patterns among students
	2. Analysis of the relationship between ChatGPT usage and students' self-perception
	3. Recommendations for integrating AI tools in writing education

**Result:** The research identified demographic factors influencing ChatGPT usage patterns, showing that these patterns affected students' enjoyment and their sense of ownership over their essays.

**Limitations:** The study was limited to a sample size of 70 students and was conducted in an online environment, which may not represent all students' experiences or contexts.

**Conclusion:** The findings suggest that writing education must adapt to incorporate generative AI tools and consider how student demographics influence their engagement with such technologies.

**Abstract:** As large language models (LLMs) advance and become widespread, students increasingly turn to systems like ChatGPT for assistance with writing tasks. Educators are concerned with students' usage of ChatGPT beyond cheating; using ChatGPT may reduce their critical engagement with writing, hindering students' learning processes. The negative or positive impact of using LLM-powered tools for writing will depend on how students use them; however, how students use ChatGPT remains largely unknown, resulting in a limited understanding of its impact on learning. To better understand how students use these tools, we conducted an online study $(n=70)$ where students were given an essay-writing task using a custom platform we developed to capture the queries they made to ChatGPT. To characterize their ChatGPT usage, we categorized each of the queries students made to ChatGPT. We then analyzed the relationship between ChatGPT usage and a variety of other metrics, including students' self-perception, attitudes towards AI, and the resulting essay itself. We found that factors such as gender, race, and perceived self-efficacy can help predict different AI usage patterns. Additionally, we found that different usage patterns were associated with varying levels of enjoyment and perceived ownership over the essay. The results of this study contribute to discussions about how writing education should incorporate generative AI-powered tools in the classroom.

</details>


### [21] [Trust-Enabled Privacy: Social Media Designs to Support Adolescent User Boundary Regulation](https://arxiv.org/abs/2502.19082)

*JaeWon Kim, Robert Wolfe, Ramya Bhagirathi Subramanian, Mei-Hsuan Lee, Jessica Colnago, Alexis Hiniker*

**Main category:** cs.HC

**Keywords:** self-disclosure, social media, trust, privacy, adolescents

**Relevance Score:** 6

**TL;DR:** The paper explores how social media platform designs affect self-disclosure in adolescents and proposes a framework for trust-enabled privacy to improve relationship-building on these platforms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To identify barriers to meaningful self-disclosure among adolescents on social media and improve platform designs to foster better communication and relationship-building.

**Method:** A three-part study involving co-design interviews with 19 teens aged 13-18 to gather insights on their experiences and needs regarding social media and self-disclosure.

**Key Contributions:**

	1. Identification of barriers to self-disclosure for teens on social media
	2. Introduction of the trust-enabled privacy framework
	3. Actionable design ideas for improving teen social media experiences

**Result:** Findings indicate that existing platform norms discourage casual sharing, leading to self-censorship or disengagement. The proposed trust-enabled privacy framework helps to enhance boundary regulation and support trust-building.

**Limitations:** 

**Conclusion:** Adaptive boundary regulation can empower users when trust is upheld, while eroding trust leads to negative outcomes like disengagement. The study offers actionable design guidelines to enhance teen engagement on social media.

**Abstract:** Adolescents heavily rely on social media to build and maintain close relationships, yet current platform designs often make self-disclosure feel risky or uncomfortable. Through a three-part study involving 19 teens aged 13-18, we identify key barriers to meaningful self-disclosure on social media. Our findings reveal that while these adolescents seek casual, frequent sharing to strengthen relationships, existing platform norms often discourage such interactions. Based on our co-design interview findings, we propose platform design ideas to foster a more dynamic and nuanced privacy experience for teen social media users. We then introduce \textbf{\textit{trust-enabled privacy}} as a framework that recognizes trust -- whether building or eroding -- as central to boundary regulation, and foregrounds the role of platform design in shaping the very norms and interaction patterns that influence how trust unfolds. When trust is supported, boundary regulation becomes more adaptive and empowering; when it erodes, users resort to self-censorship or disengagement. This work provides empirical insights and actionable guidelines for designing social media spaces where teens feel empowered to engage in meaningful relationship-building processes.

</details>


### [22] [Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology](https://arxiv.org/abs/2504.04833)

*Andrea Esposito, Miriana Calvano, Antonio Curci, Francesco Greco, Rosa Lanzilotti, Antonio Piccinno*

**Main category:** cs.HC

**Keywords:** Human-Centered AI, End-User Development, black-box AI, explainability, user control

**Relevance Score:** 8

**TL;DR:** This paper introduces an End-User Development approach for enhancing user control and explainability in black-box AI systems, targeting high-risk domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for human control over AI systems in high-risk domains drives the development of effective user interaction methods.

**Method:** The proposed approach enables users to edit AI explanations and intervene in predictions, thereby fostering a more interactive and controllable user experience with AI.

**Key Contributions:**

	1. Introduction of an End-User Development approach for black-box AI models.
	2. Enhancement of user control over AI predictions through targeted interventions.
	3. Advancement of Human-Centered AI by integrating explainability and adaptability.

**Result:** The methodology combines user control with explainability, leading to improved adaptability of AI models to user needs and preferences.

**Limitations:** The approach primarily focuses on black-box AI systems, which may limit applicability in fully explainable models.

**Conclusion:** This work enhances Human-Centered AI by promoting a collaborative relationship between users and AI, facilitating tailored user experiences in AI applications.

**Abstract:** The integration of Artificial Intelligence (AI) in modern society is transforming how individuals perform tasks. In high-risk domains, ensuring human control over AI systems remains a key design challenge. This article presents a novel End-User Development (EUD) approach for black-box AI models, enabling users to edit explanations and influence future predictions through targeted interventions. By combining explainability, user control, and model adaptability, the proposed method advances Human-Centered AI (HCAI), promoting a symbiotic relationship between humans and adaptive, user-tailored AI systems.

</details>


### [23] [A Taxonomy of Questions for Critical Reflection in Machine-Assisted Decision-Making](https://arxiv.org/abs/2504.12830)

*Simon W. S. Fischer, Hanna Schraffenberger, Serge Thill, Pim Haselager*

**Main category:** cs.HC

**Keywords:** human-AI interaction, reflective questioning, explainable AI

**Relevance Score:** 8

**TL;DR:** The paper presents a taxonomy designed to create questions that facilitate reflection in machine-assisted decision-making, particularly in clinical contexts, enhancing cognitive engagement and decision quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the over-reliance on machine recommendations in decision-making, which can diminish cognitive engagement and critical thinking.

**Method:** The authors develop a taxonomy based on Socratic questioning and human-centered explainable AI (XAI) to formulate reflective questions for decision-making processes.

**Key Contributions:**

	1. Development of a taxonomy for reflective questioning in machine-assisted decision-making.
	2. Integration of Socratic questioning and explainable AI techniques.
	3. Empirical evaluation showing benefits in educational settings.

**Result:** The use case evaluation in education demonstrates the taxonomy's effectiveness in promoting reflection and improving decision-making engagement.

**Limitations:** 

**Conclusion:** The research contributes to the field of human-AI interaction by providing tools to enhance human oversight in decision-making, aligning with the regulations of the European AI Act.

**Abstract:** Decision-makers run the risk of relying too much on machine recommendations, which is associated with lower cognitive engagement. Reflection has been shown to increase cognitive engagement and improve critical thinking and reasoning and therefore decision-making. However, there is currently no approach to support reflection in machine-assisted decision-making. We therefore present a taxonomy that serves to systematically create questions related to machine-assisted decision-making that promote reflection and thus cognitive engagement and ultimately a deliberate decision-making process. Our taxonomy builds on a taxonomy of Socratic questions and a question bank for human-centred explainable AI (XAI), and illustrates how XAI techniques can be utilised and repurposed to formulate questions. As a use case, we focus on clinical decision-making. An evaluation in education confirms the applicability and expected benefits of our taxonomy. Our work contributes to the growing research on human-AI interaction that goes beyond the paradigm of machine recommendations and explanations and aims to enable effective human oversight as required by the European AI Act.

</details>


### [24] [From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data](https://arxiv.org/abs/2505.18464)

*Ugur Kursuncu, Trilok Padhi, Gaurav Sinha, Abdulkadir Erol, Jaya Krishna Mandivarapu, Christopher R. Larrison*

**Main category:** cs.HC

**Keywords:** Mental Health, Large Language Models, Anxiety Support, User-generated Content, Fine-tuning

**Relevance Score:** 9

**TL;DR:** This study evaluates the use of Large Language Models (LLMs) for anxiety support by fine-tuning on user posts from the r/Anxiety subreddit, examining linguistic quality, safety, and supportiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing demand for accessible mental health support alongside workforce shortages necessitates exploring scalable solutions like LLMs for real-time assistance in sensitive areas such as anxiety.

**Method:** A systematic evaluation was conducted using a mixed-method framework focusing on linguistic quality, safety, and supportiveness, based on real user-generated posts from the r/Anxiety subreddit for both prompting and fine-tuning LLMs (GPT and Llama).

**Key Contributions:**

	1. Systematic evaluation of LLMs for anxiety support
	2. Demonstration of fine-tuning effects on linguistic quality and emotional responsiveness
	3. Highlighting risks associated with unprocessed social media content for training LLMs

**Result:** Fine-tuning LLMs with anxiety-related data improved linguistic quality but increased toxicity, bias, and reduced emotional responsiveness. GPT was found to be more supportive than Llama, despite both showing limited empathy.

**Limitations:** Increased toxicity and bias, diminished emotional responsiveness following fine-tuning, and overall limited empathy from LLMs.

**Conclusion:** The findings stress the need for mitigation strategies when fine-tuning LLMs on social media content to avoid amplifying toxicity and bias while seeking to enhance support.

**Abstract:** The growing demand for accessible mental health support, compounded by workforce shortages and logistical barriers, has led to increased interest in utilizing Large Language Models (LLMs) for scalable and real-time assistance. However, their use in sensitive domains such as anxiety support remains underexamined. This study presents a systematic evaluation of LLMs (GPT and Llama) for their potential utility in anxiety support by using real user-generated posts from the r/Anxiety subreddit for both prompting and fine-tuning. Our approach utilizes a mixed-method evaluation framework incorporating three main categories of criteria: (i) linguistic quality, (ii) safety and trustworthiness, and (iii) supportiveness. Results show that fine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic quality but increased toxicity and bias, and diminished emotional responsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more supportive overall. Our findings highlight the risks of fine-tuning LLMs on unprocessed social media content without mitigation strategies.

</details>


### [25] [AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments](https://arxiv.org/abs/2405.07960)

*Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, Michael Moor*

**Main category:** cs.HC

**Keywords:** large language models, clinical decision-making, human-computer interaction, health informatics, evaluation benchmarks

**Relevance Score:** 9

**TL;DR:** AgentClinic is a multimodal agent benchmark for evaluating LLMs in clinical settings, focusing on sequential decision-making and multimodal data collection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an improved framework for assessing LLMs in clinical scenarios that better reflects the complexity of clinical decision-making compared to static benchmarks.

**Method:** The study introduces AgentClinic, a simulated environment featuring patient interactions and multimodal data, evaluated across nine medical specialties and seven languages.

**Key Contributions:**

	1. Introduction of AgentClinic for evaluating LLMs in clinical settings
	2. Demonstrates performance differences among LLMs in sequential decision-making
	3. Identifies success factors such as tool usage in enhancing diagnostic accuracy

**Result:** Diagnoses accuracy significantly drops in the sequential format of AgentClinic, with Claude-3.5 performing best among LLMs, while Llama-3 shows up to 92% improvement when using a specific note-taking tool.

**Limitations:** 

**Conclusion:** The evaluation reveals large performance variances among LLMs when using tools and emphasizes the need for patient-centered metrics in assessing LLM capabilities in healthcare.

**Abstract:** Evaluating large language models (LLM) in clinical scenarios is crucial to assessing their potential clinical utility. Existing benchmarks rely heavily on static question-answering, which does not accurately depict the complex, sequential nature of clinical decision-making. Here, we introduce AgentClinic, a multimodal agent benchmark for evaluating LLMs in simulated clinical environments that include patient interactions, multimodal data collection under incomplete information, and the usage of various tools, resulting in an in-depth evaluation across nine medical specialties and seven languages. We find that solving MedQA problems in the sequential decision-making format of AgentClinic is considerably more challenging, resulting in diagnostic accuracies that can drop to below a tenth of the original accuracy. Overall, we observe that agents sourced from Claude-3.5 outperform other LLM backbones in most settings. Nevertheless, we see stark differences in the LLMs' ability to make use of tools, such as experiential learning, adaptive retrieval, and reflection cycles. Strikingly, Llama-3 shows up to 92% relative improvements with the notebook tool that allows for writing and editing notes that persist across cases. To further scrutinize our clinical simulations, we leverage real-world electronic health records, perform a clinical reader study, perturb agents with biases, and explore novel patient-centric metrics that this interactive environment firstly enables.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [26] [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)

*Jesus Alvarez C, Daua D. Karajeanes, Ashley Celeste Prado, John Ruttan, Ivory Yang, Sean O'Brien, Vasu Sharma, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** endangered languages, NLP, Comanche, language preservation, LLMs

**Relevance Score:** 4

**TL;DR:** This study addresses the digital exclusion of the endangered Comanche language by introducing low-cost NLP interventions for language preservation, demonstrating effective use of LLMs for language identification with minimal data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for linguistic research and revitalization efforts for endangered languages, specifically focusing on the digital exclusion of Comanche.

**Method:** A manually curated dataset of 412 phrases and a synthetic data generation pipeline were created, followed by an empirical evaluation of the performance of GPT-4o and GPT-4o-mini in language identification tasks.

**Key Contributions:**

	1. First computational investigation of Comanche.
	2. Development of a synthetic data generation pipeline for low-resource languages.
	3. Empirical evaluation showing the effectiveness of few-shot prompting in LLMs.

**Result:** LLMs demonstrated challenges in zero-shot settings for Comanche, but few-shot prompting improved accuracy to near-perfect levels with just five examples, highlighting the effectiveness of targeted NLP methodologies.

**Limitations:** Focus on a single endangered language; results may not generalize to all low-resource languages.

**Conclusion:** The study illustrates the potential of targeted NLP approaches in low-resource contexts and emphasizes the importance of visibility and community engagement for language preservation.

**Abstract:** The digital exclusion of endangered languages remains a critical challenge in NLP, limiting both linguistic research and revitalization efforts. This study introduces the first computational investigation of Comanche, an Uto-Aztecan language on the verge of extinction, demonstrating how minimal-cost, community-informed NLP interventions can support language preservation. We present a manually curated dataset of 412 phrases, a synthetic data generation pipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language identification. Our experiments reveal that while LLMs struggle with Comanche in zero-shot settings, few-shot prompting significantly improves performance, achieving near-perfect accuracy with just five examples. Our findings highlight the potential of targeted NLP methodologies in low-resource contexts and emphasize that visibility is the first step toward inclusion. By establishing a foundation for Comanche in NLP, we advocate for computational approaches that prioritize accessibility, cultural sensitivity, and community engagement.

</details>


### [27] [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)

*Junyan Zhang, Yiming Huang, Shuliang Liu, Yubo Gao, Xuming Hu*

**Main category:** cs.CL

**Keywords:** BERT, LLM, text classification, TaMAS, task-driven approach

**Relevance Score:** 8

**TL;DR:** This paper compares BERT-like models with LLMs in text classification and proposes a task-driven approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the growing trend of relying exclusively on LLMs for text classification by showcasing the strengths of traditional BERT-like models.

**Method:** A systematic comparison of BERT-like fine-tuning, LLM internal state utilization, and zero-shot inference across six challenging datasets.

**Key Contributions:**

	1. Systematic comparison of BERT-like models and LLMs.
	2. Identification of task-specific model strengths.
	3. Introduction of TaMAS for improved model selection.

**Result:** BERT-like models often outperform LLMs across various tasks, with specific strengths identified through PCA and probing experiments.

**Limitations:** Limited to six high-difficulty datasets; findings may not generalize to all text classification tasks.

**Conclusion:** A nuanced, task-driven approach to model selection, proposed as TaMAS, is superior to a one-size-fits-all reliance on LLMs.

**Abstract:** The rapid adoption of LLMs has overshadowed the potential advantages of traditional BERT-like models in text classification. This study challenges the prevailing "LLM-centric" trend by systematically comparing three category methods, i.e., BERT-like models fine-tuning, LLM internal state utilization, and zero-shot inference across six high-difficulty datasets. Our findings reveal that BERT-like models often outperform LLMs. We further categorize datasets into three types, perform PCA and probing experiments, and identify task-specific model strengths: BERT-like models excel in pattern-driven tasks, while LLMs dominate those requiring deep semantics or world knowledge. Based on this, we propose TaMAS, a fine-grained task selection strategy, advocating for a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.

</details>


### [28] [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)

*Shuhang Xu, Fangwei Zhong*

**Main category:** cs.CL

**Keywords:** metaphor processing, large language models, covert communication, strategic interaction, multi-agent systems

**Relevance Score:** 9

**TL;DR:** CoMet is a framework enhancing LLM agents' capabilities to process metaphors in multi-agent language games, improving their strategic communication skills.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) often struggle with metaphors, hindering covert communication and semantic evasion necessary for strategic interactions.

**Method:** CoMet integrates a hypothesis-based metaphor reasoner and a metaphor generator which improve through self-reflection and knowledge integration, facilitating better metaphor interpretation and application.

**Key Contributions:**

	1. Introduction of the CoMet framework for metaphor processing in LLMs
	2. Combination of hypothesis-based reasoning and metaphor generation
	3. Demonstrated improved performance in multi-agent language games.

**Result:** CoMet was evaluated on two multi-agent language games, showing significant improvements in agents' abilities to communicate strategically using metaphors.

**Limitations:** 

**Conclusion:** The introduction of CoMet allows LLM agents to more effectively engage in complex communication using metaphors, enhancing their strategic interaction capabilities.

**Abstract:** Metaphors are a crucial way for humans to express complex or subtle ideas by comparing one concept to another, often from a different domain. However, many large language models (LLMs) struggle to interpret and apply metaphors in multi-agent language games, hindering their ability to engage in covert communication and semantic evasion, which are crucial for strategic communication. To address this challenge, we introduce CoMet, a framework that enables LLM-based agents to engage in metaphor processing. CoMet combines a hypothesis-based metaphor reasoner with a metaphor generator that improves through self-reflection and knowledge integration. This enhances the agents' ability to interpret and apply metaphors, improving the strategic and nuanced quality of their interactions. We evaluate CoMet on two multi-agent language games - Undercover and Adversarial Taboo - which emphasize Covert Communication and Semantic Evasion. Experimental results demonstrate that CoMet significantly enhances the agents' ability to communicate strategically using metaphors.

</details>


### [29] [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)

*Hanyu Li, Haoyu Liu, Tingyu Zhu, Tianyu Guo, Zeyu Zheng, Xiaotie Deng, Michael I. Jordan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Analysis, Benchmarking, Multi-Round Interaction, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** Introducing IDA-Bench, a benchmark for evaluating large language models (LLMs) in multi-round interactive data analysis tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of existing benchmarks that overlook the iterative nature of data analysis, where expert decisions evolve with insights gained during analysis.

**Method:** Development of IDA-Bench, which assesses LLM agents in multi-round scenarios using tasks derived from complex Kaggle notebooks, presenting them as sequential natural language instructions.

**Key Contributions:**

	1. Introduction of IDA-Bench for multi-round interactive evaluation
	2. Comparison of LLM performance against human-derived baselines
	3. Highlighting the gap in current LLM capabilities in iterative tasks

**Result:** Initial evaluations reveal that even advanced coding agents, such as Claude-3.7-thinking, fail to complete less than 50% of the tasks, indicating significant limitations in their performance compared to human benchmarks.

**Limitations:** Focus on a specific set of tasks derived from Kaggle notebooks may limit generalizability.

**Conclusion:** There is a pressing need to enhance the multi-round capabilities of LLMs to create more dependable data analysis agents, balancing instruction following with reasoning.

**Abstract:** Large Language Models (LLMs) show promise as data analysis agents, but existing benchmarks overlook the iterative nature of the field, where experts' decisions evolve with deeper insights of the dataset. To address this, we introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round interactive scenarios. Derived from complex Kaggle notebooks, tasks are presented as sequential natural language instructions by an LLM-simulated user. Agent performance is judged by comparing its final numerical output to the human-derived baseline. Initial results show that even state-of-the-art coding agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting limitations not evident in single-turn tests. This work underscores the need to improve LLMs' multi-round capabilities for building more reliable data analysis agents, highlighting the necessity of achieving a balance between instruction following and reasoning.

</details>


### [30] [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)

*Xixian Yong, Xiao Zhou, Yingying Zhang, Jinlin Li, Yefeng Zheng, Xian Wu*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Semantic Efficiency, Adaptive Think, Entropy-based Methods, Multi-step Reasoning

**Relevance Score:** 8

**TL;DR:** This paper evaluates the efficiency of large reasoning models in multi-step reasoning, revealing a trade-off between reasoning length and semantic efficiency and proposing an adaptive strategy to improve both accuracy and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the trade-off between reasoning length and semantic efficiency in large reasoning models, given the excessive lengths of reasoning chains often generated.

**Method:** The authors propose two metrics, InfoBias and InfoGain, to quantify inefficiencies in reasoning. They introduce an entropy-based Adaptive Think strategy that halts reasoning dynamically based on confidence levels.

**Key Contributions:**

	1. Introduction of InfoBias and InfoGain metrics for reasoning efficiency
	2. Development of Adaptive Think strategy that improves model reasoning process
	3. Demonstrates significant improvements in accuracy and token usage efficiency.

**Result:** Using the Adaptive Think strategy, the study shows a 1.10% improvement in accuracy and a 50.80% reduction in token usage compared to the Vanilla Think approach, across six benchmark tasks.

**Limitations:** 

**Conclusion:** The entropy-based methods can enhance accuracy and cost-efficiency in deploying large language models.

**Abstract:** The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment.

</details>


### [31] [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)

*Ananth Muppidi, Tarak Das, Sambaran Bandyopadhyay, Tripti Shukla, Dharun D A*

**Main category:** cs.CL

**Keywords:** presentation generation, evaluation metrics, generative AI, RefSlides dataset, REFLEX evaluation

**Relevance Score:** 8

**TL;DR:** The paper presents a novel evaluation approach for automatically generated presentation slides using the RefSlides dataset and REFLEX metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the generation and evaluation of presentation slides using generative AI by introducing a reference-free evaluation approach.

**Method:** The authors create a benchmark dataset (RefSlides) of high-quality presentations and propose REFLEX, an evaluation method that scores presentations based on metrics derived from negative examples generated with varying perturbations.

**Key Contributions:**

	1. Introduction of the RefSlides benchmark dataset for presentation evaluation.
	2. Development of REFLEX, a new evaluation method for presentation content.
	3. Demonstration of the effectiveness of reference-free evaluation techniques in outperforming traditional methods.

**Result:** The evaluation approach demonstrates superior performance to existing heuristic and LLM-based evaluations in terms of scoring and providing feedback.

**Limitations:** 

**Conclusion:** REFLEX enhances the ability to evaluate presentation content effectively without the need for ground truth presentations, paving the way for better automated slide generation.

**Abstract:** The generation of presentation slides automatically is an important problem in the era of generative AI. This paper focuses on evaluating multimodal content in presentation slides that can effectively summarize a document and convey concepts to a broad audience. We introduce a benchmark dataset, RefSlides, consisting of human-made high-quality presentations that span various topics. Next, we propose a set of metrics to characterize different intrinsic properties of the content of a presentation and present REFLEX, an evaluation approach that generates scores and actionable feedback for these metrics. We achieve this by generating negative presentation samples with different degrees of metric-specific perturbations and use them to fine-tune LLMs. This reference-free evaluation technique does not require ground truth presentations during inference. Our extensive automated and human experiments demonstrate that our evaluation approach outperforms classical heuristic-based and state-of-the-art large language model-based evaluations in generating scores and explanations.

</details>


### [32] [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)

*Yukin Zhang, Qi Dong*

**Main category:** cs.CL

**Keywords:** Transformer models, Text generation, Probabilistic generation, Interpretability, Hierarchical framework

**Relevance Score:** 9

**TL;DR:** This paper introduces a hierarchical framework called Multi_Scale Probabilistic Generation Theory (MSPGT) to interpret how Transformer-based language models generate text by analyzing global, intermediate, and local scales.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the opaque nature of text generation in large Transformer models and to improve interpretability and control.

**Method:** The framework factorizes text generation into three semantic scales and identifies scale boundaries using attention span thresholds and inter-layer mutual information peaks across various models.

**Key Contributions:**

	1. Introduction of Multi_Scale Probabilistic Generation Theory (MSPGT) for language model interpretation.
	2. Identification of attention span thresholds and inter-layer mutual information peaks as metrics for scale boundaries.
	3. Demonstration of significant effects of scale manipulations on lexical diversity, sentence structure, and discourse coherence.

**Result:** Stable partitions of generation scales were identified in different Transformer architectures, revealing how decoder-only models focus more on global processing while encoder-only models emphasize local feature extraction.

**Limitations:** 

**Conclusion:** MSPGT provides a unified approach that enhances the interpretability and controllability of language models, bridging mechanistic understanding and emergent properties.

**Abstract:** Large Transformer based language models achieve remarkable performance but remain opaque in how they plan, structure, and realize text. We introduce Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework that factorizes generation into three semantic scales_global context, intermediate structure, and local word choices and aligns each scale with specific layer ranges in Transformer architectures. To identify scale boundaries, we propose two complementary metrics: attention span thresholds and inter layer mutual information peaks. Across four representative models (GPT-2, BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global partitions, corroborated by probing tasks and causal interventions. We find that decoder_only models allocate more layers to intermediate and global processing while encoder_only models emphasize local feature extraction. Through targeted interventions, we demonstrate that local scale manipulations primarily influence lexical diversity, intermediate-scale modifications affect sentence structure and length, and global_scale perturbations impact discourse coherence all with statistically significant effects. MSPGT thus offers a unified, architecture-agnostic method for interpreting, diagnosing, and controlling large language models, bridging the gap between mechanistic interpretability and emergent capabilities.

</details>


### [33] [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)

*Kunal Sawarkar, Shivam R. Solanki, Abhilasha Mangal*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Enterprise Search, Domain-Specific Datasets

**Relevance Score:** 9

**TL;DR:** The paper proposes a novel method named 'MetaGen Blended RAG' to enhance retrieval accuracy in enterprise search for domain-specific datasets, achieving significantly higher accuracy compared to previous methods without the need for fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve answer accuracy in enterprise environments where domain-specific knowledge bases pose challenges due to complex terminology and significant semantic variability.

**Method:** The methodology focuses on enhancing the retriever through hybrid query indexes and metadata enrichment, constructing a metadata generation pipeline utilizing key concepts and topics.

**Key Contributions:**

	1. Introduced 'MetaGen Blended RAG' for improved retrieval accuracy.
	2. Achieved state-of-the-art results on PubMedQA without fine-tuning.
	3. Demonstrated the effectiveness across multiple Q&A datasets.

**Result:** On the PubMedQA benchmark, the approach achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing existing results and setting a new benchmark for zero-shot performance.

**Limitations:** 

**Conclusion:** The proposed approach avoids overfitting and generalizes effectively across varied domains, demonstrating robustness and scalability across multiple benchmarks.

**Abstract:** Despite the widespread exploration of Retrieval-Augmented Generation (RAG), its deployment in enterprises for domain-specific datasets remains limited due to poor answer accuracy. These corpora, often shielded behind firewalls in private enterprise knowledge bases, having complex, domain-specific terminology, rarely seen by LLMs during pre-training; exhibit significant semantic variability across domains (like networking, military, or legal, etc.), or even within a single domain like medicine, and thus result in poor context precision for RAG systems. Currently, in such situations, fine-tuning or RAG with fine-tuning is attempted, but these approaches are slow, expensive, and lack generalization for accuracy as the new domain-specific data emerges. We propose an approach for Enterprise Search that focuses on enhancing the retriever for a domain-specific corpus through hybrid query indexes and metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata generation pipeline using key concepts, topics, and acronyms, and then creates a metadata-enriched hybrid index with boosted search queries. This approach avoids overfitting and generalizes effectively across domains. On the PubMedQA benchmark for the biomedical domain, the proposed method achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results without fine-tuning and sets a new benchmark for zero-shot results while outperforming much larger models like GPT3.5. The results are even comparable to the best fine-tuned models on this dataset, and we further demonstrate the robustness and scalability of the approach by evaluating it on other Q&A datasets like SQuAD, NQ etc.

</details>


### [34] [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)

*Jianghao Wu, Feilong Tang, Yulong Li, Ming Hu, Haochen Xue, Shoaib Jameel, Yutong Xie, Imran Razzak*

**Main category:** cs.CL

**Keywords:** medical reasoning, large language models, framework, hierarchical retrieval, reliability scoring

**Relevance Score:** 9

**TL;DR:** Introducing TAGS, a framework that enhances medical reasoning in large language models (LLMs) by integrating a generalist model with a domain-specific specialist, improving accuracy without fine-tuning.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in existing prompting-based methods and fine-tuned medical LLMs in terms of stability, generalization, and adaptability to new clinical scenarios.

**Method:** TAGS combines a generalist model with a specialist, incorporating a hierarchical retrieval mechanism and a reliability scorer to support reasoning and answer aggregation.

**Key Contributions:**

	1. Introduction of TAGS framework combining generalist and specialist models
	2. Hierarchical retrieval mechanism for multi-scale exemplars
	3. Reliability scorer for evaluating reasoning consistency

**Result:** TAGS significantly improves accuracy across nine MedQA benchmarks, with notable performance boosts for GPT-4o (13.8%) and DeepSeek-R1 (16.8%), while enhancing a 7B model from 14.1% to 23.9%.

**Limitations:** 

**Conclusion:** TAGS demonstrates a method to improve medical reasoning in LLMs without needing parameter updates, surpassing many fine-tuned models.

**Abstract:** Recent advances such as Chain-of-Thought prompting have significantly improved large language models (LLMs) in zero-shot medical reasoning. However, prompting-based methods often remain shallow and unstable, while fine-tuned medical LLMs suffer from poor generalization under distribution shifts and limited adaptability to unseen clinical scenarios. To address these limitations, we present TAGS, a test-time framework that combines a broadly capable generalist with a domain-specific specialist to offer complementary perspectives without any model fine-tuning or parameter updates. To support this generalist-specialist reasoning process, we introduce two auxiliary modules: a hierarchical retrieval mechanism that provides multi-scale exemplars by selecting examples based on both semantic and rationale-level similarity, and a reliability scorer that evaluates reasoning consistency to guide final answer aggregation. TAGS achieves strong performance across nine MedQA benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several fine-tuned medical LLMs, without any parameter updates. The code will be available at https://github.com/JianghaoWu/TAGS.

</details>


### [35] [Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards](https://arxiv.org/abs/2505.18298)

*Jinyan Su, Claire Cardie*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, reward shaping, reasoning length, accuracy

**Relevance Score:** 8

**TL;DR:** This paper presents an adaptive reward-shaping method for large language models to produce concise outputs while maintaining accuracy during reasoning tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of unnecessarily long reasoning traces in reinforcement learning-trained models, which lead to increased costs and latency.

**Method:** An adaptive reward-shaping approach that adjusts the trade-off between accuracy and response length based on the model's performance, dynamically increasing or relaxing the length penalty.

**Key Contributions:**

	1. Introduces an adaptive reward-shaping mechanism for LLMs.
	2. Demonstrates effectiveness in reducing reasoning length while maintaining accuracy.
	3. Offers a new direction for efficiency in large-scale language model reasoning.

**Result:** The proposed method consistently reduces reasoning length in various datasets while largely preserving accuracy, leading to more cost-efficient adaptive reasoning in language models.

**Limitations:** 

**Conclusion:** The adaptive reward method accelerates length reduction early on and prevents over-compression later, providing a significant improvement in reasoning efficiency.

**Abstract:** Large language models (LLMs) have demonstrated strong reasoning abilities in mathematical tasks, often enhanced through reinforcement learning (RL). However, RL-trained models frequently produce unnecessarily long reasoning traces -- even for simple queries -- leading to increased inference costs and latency. While recent approaches attempt to control verbosity by adding length penalties to the reward function, these methods rely on fixed penalty terms that are hard to tune and cannot adapt as the model's reasoning capability evolves, limiting their effectiveness. In this work, we propose an adaptive reward-shaping method that enables LLMs to "think fast and right" -- producing concise outputs without sacrificing correctness. Our method dynamically adjusts the reward trade-off between accuracy and response length based on model performance: when accuracy is high, the length penalty increases to encourage faster length reduction; when accuracy drops, the penalty is relaxed to preserve correctness. This adaptive reward accelerates early-stage length reduction while avoiding over-compression in later stages. Experiments across multiple datasets show that our approach consistently and dramatically reduces reasoning length while largely maintaining accuracy, offering a new direction for cost-efficient adaptive reasoning in large-scale language models.

</details>


### [36] [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)

*Zhuozhuo Joy Liu, Farhan Samir, Mehar Bhatia, Laura K. Nelson, Vered Shwartz*

**Main category:** cs.CL

**Keywords:** Cultural norms, GPT-4, Stereotypes, Machine Learning, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** The paper explores how LLMs, particularly GPT-4, reason about cultural norms across different cultures, revealing shortcomings in cultural specificity and hidden stereotypes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs apply cultural values consistently in real-world scenarios beyond survey-based assessments.

**Method:** A bottom-up approach asking LLMs to reason about cultural norms in narratives from various cultures.

**Key Contributions:**

	1. Introduces a bottom-up method to assess LLMs' understanding of cultural norms
	2. Finds significant non-specificity in the norms generated by GPT-4
	3. Identifies hidden stereotypes in LLM outputs

**Result:** GPT-4 generates less culture-specific norms and retains hidden stereotypes instead of outright suppressing them.

**Limitations:** The study focuses on GPT-4 and may not generalize to other LLMs or cultures.

**Conclusion:** Improving LLMs to serve a diverse user base fairly requires addressing the lack of cultural specificity and the presence of hidden stereotypes.

**Abstract:** LLMs have been demonstrated to align with the values of Western or North American cultures. Prior work predominantly showed this effect through leveraging surveys that directly ask (originally people and now also LLMs) about their values. However, it is hard to believe that LLMs would consistently apply those values in real-world scenarios. To address that, we take a bottom-up approach, asking LLMs to reason about cultural norms in narratives from different cultures. We find that GPT-4 tends to generate norms that, while not necessarily incorrect, are significantly less culture-specific. In addition, while it avoids overtly generating stereotypes, the stereotypical representations of certain cultures are merely hidden rather than suppressed in the model, and such stereotypes can be easily recovered. Addressing these challenges is a crucial step towards developing LLMs that fairly serve their diverse user base.

</details>


### [37] [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)

*Naghmeh Jamali, Milad Mohammadi, Danial Baledi, Zahra Rezvani, Hesham Faili*

**Main category:** cs.CL

**Keywords:** medical question answering, large language models, Persian, multilingual, health informatics

**Relevance Score:** 9

**TL;DR:** Presentation of the PerMedCQA benchmark for evaluating LLMs on Persian-language medical consumer questions.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advances in LLMs for medical QA, there is a lack of resources in low-resource languages like Persian.

**Method:** Creation of PerMedCQA benchmark sourced from a medical QA forum, evaluating several multilingual LLMs using a novel rubric-based framework.

**Key Contributions:**

	1. Introduction of the PerMedCQA benchmark for Persian medical questions
	2. Evaluation of LLMs with a novel rubric-based framework
	3. Public availability of a large dataset for further research.

**Result:** The evaluation of state-of-the-art LLMs reveals significant challenges in multilingual medical QA.

**Limitations:** Limited to Persian language; challenges identified may not generalize across other languages.

**Conclusion:** Insights gained can help improve context-aware medical assistance systems, with the dataset made publicly available.

**Abstract:** Medical consumer question answering (CQA) is crucial for empowering patients by providing personalized and reliable health information. Despite recent advances in large language models (LLMs) for medical QA, consumer-oriented and multilingual resources, particularly in low-resource languages like Persian, remain sparse. To bridge this gap, we present PerMedCQA, the first Persian-language benchmark for evaluating LLMs on real-world, consumer-generated medical questions. Curated from a large medical QA forum, PerMedCQA contains 68,138 question-answer pairs, refined through careful data cleaning from an initial set of 87,780 raw entries. We evaluate several state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a novel rubric-based evaluation framework driven by an LLM grader, validated against expert human annotators. Our results highlight key challenges in multilingual medical QA and provide valuable insights for developing more accurate and context-aware medical assistance systems. The data is publicly available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA

</details>


### [38] [Model Editing with Graph-Based External Memory](https://arxiv.org/abs/2505.18343)

*Yash Kumar Atri, Ahmed Alaa, Thomas Hartvigsen*

**Main category:** cs.CL

**Keywords:** Hyperbolic Geometry, Graph Neural Networks, Model Editing

**Relevance Score:** 8

**TL;DR:** HYPE is a novel framework that improves the stability of post-training model edits in large language models (LLMs) using hyperbolic geometry and graph neural networks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of hallucinations and outdated knowledge in LLMs resulting from traditional editing methods that suffer from overfitting and catastrophic forgetting.

**Method:** The framework consists of three components: (i) Hyperbolic Graph Construction using Poincaré embeddings to represent knowledge triples, (ii) Möbius-Transformed Updates for consistent edits in hyperbolic space, and (iii) Dual Stabilization to prevent catastrophic forgetting.

**Key Contributions:**

	1. Introduction of HYPE framework for model editing
	2. Use of hyperbolic geometry to maintain hierarchical relationships
	3. Enhancements in stability and accuracy of model edits

**Result:** Experiments showed that HYPE enhances edit stability, factual accuracy, and multi-hop reasoning in LLMs such as GPT-J and GPT2-XL.

**Limitations:** 

**Conclusion:** HYPE provides a robust solution for dynamic parameter edits in LLMs by leveraging hyperbolic geometry.

**Abstract:** Large language models (LLMs) have revolutionized natural language processing, yet their practical utility is often limited by persistent issues of hallucinations and outdated parametric knowledge. Although post-training model editing offers a pathway for dynamic updates, existing methods frequently suffer from overfitting and catastrophic forgetting. To tackle these challenges, we propose a novel framework that leverages hyperbolic geometry and graph neural networks for precise and stable model edits. We introduce HYPE (HYperbolic Parameter Editing), which comprises three key components: (i) Hyperbolic Graph Construction, which uses Poincar\'e embeddings to represent knowledge triples in hyperbolic space, preserving hierarchical relationships and preventing unintended side effects by ensuring that edits to parent concepts do not inadvertently affect child concepts; (ii) M\"obius-Transformed Updates, which apply hyperbolic addition to propagate edits while maintaining structural consistency within the hyperbolic manifold, unlike conventional Euclidean updates that distort relational distances; and (iii) Dual Stabilization, which combines gradient masking and periodic GNN parameter resetting to prevent catastrophic forgetting by focusing updates on critical parameters and preserving long-term knowledge. Experiments on CounterFact, CounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE significantly enhances edit stability, factual accuracy, and multi-hop reasoning.

</details>


### [39] [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)

*Lucas Bandarkar, Nanyun Peng*

**Main category:** cs.CL

**Keywords:** large language models, cross-lingual transfer, fine-tuning, modular frameworks, machine learning

**Relevance Score:** 7

**TL;DR:** This paper explores enhancements to large language models for lower-resource languages through modular frameworks that separate mathematical reasoning and multilingual capabilities during fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) are less effective for tasks in low-resource languages due to limited data. This study aims to improve their performance by investigating cross-lingual transfer techniques that can specifically address performance gaps in such languages.

**Method:** Developing modular frameworks that utilize frozen parameters and post hoc model merging to improve fine-tuning for separate mathematical and language tasks within LLMs.

**Key Contributions:**

	1. Introduction of modular frameworks for fine-tuning LLMs
	2. Validation of task-specific parameter separability
	3. Demonstration of improved performance with Layer-Swapping method

**Result:** Modular methods, such as fine-tuning language and math experts separately and applying Layer-Swapping for model merging, significantly outperform traditional fine-tuning approaches across various languages and models.

**Limitations:** Findings are primarily based on lower-resource languages and may not extend to all language tasks.

**Conclusion:** The study identifies effective modular strategies for enhancing LLMs in low-resource settings, suggesting that decoupling specific model capabilities can lead to better performance outcomes.

**Abstract:** Large language models (LLMs) still struggle across tasks outside of high-resource languages. In this work, we investigate cross-lingual transfer to lower-resource languages where task-specific post-training data is scarce. Building on prior work, we first validate that the subsets of model parameters that matter most for mathematical reasoning and multilingual capabilities are distinctly non-overlapping. To exploit this implicit separability between task and target language parameterization, we develop and analyze numerous modular frameworks to improve the composition of the two during fine-tuning. These methods generally employ freezing parameters or post hoc model merging to assign math and language improvement to different key parts of the LLM. In the absence of in-language math data, we demonstrate that the modular approaches successfully improve upon baselines across three languages, four models, and two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most consistently successful modular method to be fine-tuning separate language and math experts and model merging via Layer-Swapping, somewhat surprisingly. We offer possible explanations for this result via recent works on the linearity of task vectors. We further explain this by empirically showing that reverting less useful fine-tuning updates after training often outperforms freezing them from the start.

</details>


### [40] [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)

*AmirHossein Safdarian, Milad Mohammadi, Ehsan Jahanbakhsh, Mona Shahamat Naderi, Heshaam Faili*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Schema linking, Large language models, SQL queries, Benchmark performance

**Relevance Score:** 6

**TL;DR:** A novel zero-shot schema linking approach enhances text-to-SQL systems by utilizing LLMs to convert natural language queries into accurate SQL commands, achieving state-of-the-art results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of schema linking in text-to-SQL systems, aiming to streamline the conversion of natural language questions into SQL queries by improving focus and reducing prompt size.

**Method:** The proposed method constructs a schema graph based on foreign key relations and employs a single prompt to interact with Gemini 2.5 Flash to identify relevant tables and columns. Classical path-finding algorithms are used for optimal table and column selection.

**Key Contributions:**

	1. Introduction of a zero-shot schema linking method for text-to-SQL tasks
	2. Construction of a schema graph utilizing foreign key relations
	3. State-of-the-art performance on BIRD benchmark with minimal resource usage

**Result:** The method achieves state-of-the-art performance on the BIRD benchmark, surpassing previous methodologies, including specialized and fine-tuned LLM approaches.

**Limitations:** 

**Conclusion:** The schema linking approach is simple, cost-effective, and scalable, making it a strong alternative to more complex solutions, with detailed studies showing its effectiveness in execution accuracy.

**Abstract:** Text-to-SQL systems translate natural language questions into executable SQL queries, and recent progress with large language models (LLMs) has driven substantial improvements in this task. Schema linking remains a critical component in Text-to-SQL systems, reducing prompt size for models with narrow context windows and sharpening model focus even when the entire schema fits. We present a zero-shot, training-free schema linking approach that first constructs a schema graph based on foreign key relations, then uses a single prompt to Gemini 2.5 Flash to extract source and destination tables from the user query, followed by applying classical path-finding algorithms and post-processing to identify the optimal sequence of tables and columns that should be joined, enabling the LLM to generate more accurate SQL queries. Despite being simple, cost-effective, and highly scalable, our method achieves state-of-the-art results on the BIRD benchmark, outperforming previous specialized, fine-tuned, and complex multi-step LLM-based approaches. We conduct detailed ablation studies to examine the precision-recall trade-off in our framework. Additionally, we evaluate the execution accuracy of our schema filtering method compared to other approaches across various model sizes.

</details>


### [41] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)

*Jarrod Ragsdale, Rajendra Boppana*

**Main category:** cs.CL

**Keywords:** command-line interfaces, language models, behavioral modeling, dataset generation, machine learning

**Relevance Score:** 4

**TL;DR:** Introducing a new Shell Input-Output Environment (ShIOEnv) for generating datasets of command-line interactions to enhance language model fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing public datasets in behavioral modeling of command-line interfaces (CLIs), especially for smaller language models.

**Method:** The paper introduces ShIOEnv, which models command construction as a Markov Decision Process. It utilizes a context-free grammar derived from man pages to mask invalid command arguments and explores strategies such as grammar masking and PPO to enhance sample efficiency.

**Key Contributions:**

	1. Introduction of ShIOEnv for CLI behavioral modeling
	2. Use of a context-free grammar to mask invalid arguments
	3. Demonstrated significant improvements in model performance using the generated datasets

**Result:** The use of ShIOEnv and the generated datasets significantly improved the quality of command behavior datasets, achieving up to 85% improvement in BLEU-4 scores when fine-tuning CodeT5.

**Limitations:** 

**Conclusion:** ShIOEnv facilitates better data generation for CLIs and is expected to benefit future research by providing high-quality command interaction datasets.

**Abstract:** Command-line interfaces (CLIs) provide structured textual environments for system administration. Explorations have been performed using pre-trained language models (PLMs) to simulate these environments for safe interaction in high-risk environments. However, their use has been constrained to frozen, large parameter models like GPT. For smaller architectures to reach a similar level of believability, a rich dataset of CLI interactions is required. Existing public datasets focus on mapping natural-language tasks to commands, omitting crucial execution data such as exit codes, outputs, and environmental side effects, limiting their usability for behavioral modeling. We introduce a Shell Input -Output Environment (ShIOEnv), which casts command construction as a Markov Decision Process whose state is the partially built sequence and whose actions append arguments. After each action, ShIOEnv executes the candidate and returns its exit status, output, and progress toward a minimal-length behavioral objective. Due to the intractable nature of the combinatorial argument state-action space, we derive a context-free grammar from man pages to mask invalid arguments from being emitted. We explore random and proximal-policy optimization (PPO)-optimized sampling of unrestricted and grammar-masked action spaces to produce four exploration strategies. We observed that grammar masking and PPO significantly improve sample efficiency to produce a higher quality dataset (maximizing the number of arguments while minimizing redundancies). Policy-generated datasets of shell input-output behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements in BLEU-4 when constraining the action space to grammar productions with an additional 26% improvement when applying PPO. The ShIOEnv environment and curated command behavior datasets are released for use in future research.

</details>


### [42] [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)

*Abdellah El Mekki, Houdaifa Atou, Omer Nacar, Shady Shehata, Muhammad Abdul-Mageed*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Heritage, Low-Resource Languages, NLP, Cultural Values

**Relevance Score:** 9

**TL;DR:** This paper presents a novel methodology for enhancing Large Language Models (LLMs) with low-resource languages, focusing on Egyptian and Moroccan dialects to better represent their cultural heritage and values.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of low-resource languages and cultures in LLMs, which often fail to align with the cultural aspects of local communities due to reliance on synthetic data from English corpora.

**Method:** The authors propose a methodology for creating synthetic and retrieval-based pre-training data tailored to specific communities, based on their language, cultural heritage, and values, exemplified through Egyptian and Moroccan dialects.

**Key Contributions:**

	1. Development of a novel methodology for culturally-aware LLM fine-tuning
	2. Introduction of NileChat, specifically tailored for Egyptian and Moroccan dialects
	3. Provision of data and models for community use to promote diversity in LLMs

**Result:** NileChat, a 3B parameter LLM, shows improved performance on understanding, translation, and cultural alignment benchmarks compared to existing Arabic-aware LLMs, and is on par with larger models.

**Limitations:** 

**Conclusion:** The study demonstrates the importance of incorporating local community aspects into LLMs and shares methods, data, and models to encourage diversity in LLM development.

**Abstract:** Enhancing the linguistic capabilities of Large Language Models (LLMs) to include low-resource languages is a critical research area. Current research directions predominantly rely on synthetic data generated by translating English corpora, which, while demonstrating promising linguistic understanding and translation abilities, often results in models aligned with source language culture. These models frequently fail to represent the cultural heritage and values of local communities. This work proposes a methodology to create both synthetic and retrieval-based pre-training data tailored to a specific community, considering its (i) language, (ii) cultural heritage, and (iii) cultural values. We demonstrate our methodology using Egyptian and Moroccan dialects as testbeds, chosen for their linguistic and cultural richness and current underrepresentation in LLMs. As a proof-of-concept, we develop NileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities, incorporating their language, cultural heritage, and values. Our results on various understanding, translation, and cultural and values alignment benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar size and performs on par with larger models. We share our methods, data, and models with the community to promote the inclusion and coverage of more diverse communities in LLM development.

</details>


### [43] [RaDeR: Reasoning-aware Dense Retrieval Models](https://arxiv.org/abs/2505.18405)

*Debrup Das, Sam O' Nuallain, Razieh Rahimi*

**Main category:** cs.CL

**Keywords:** Reasoning-based retrieval, Large language models, Mathematical problem solving

**Relevance Score:** 8

**TL;DR:** RaDeR is a new reasoning-based dense retrieval model that excels in mathematical problem solving using LLMs, outperforming existing methods with less training data.

**Read time:** 26 min

<details>
  <summary>Details</summary>

**Motivation:** The paper explores enhancing retrieval-augmented reasoning in large language models for mathematical problem solving, seeking to create diverse and hard-negative samples.

**Method:** RaDeR employs reasoning trajectories and self-reflective relevance evaluation to train retrievers specifically for reasoning-intensive tasks, evaluated on benchmarks like BRIGHT and RAR-b.

**Key Contributions:**

	1. Introduction of RaDeR as a reasoning-based dense retrieval model
	2. Demonstration of superior performance on Math and Coding benchmarks compared to existing methods
	3. First dense retriever to beat BM25 on Chain-of-Thought queries

**Result:** RaDeR displays superior performance compared to strong baselines in reasoning tasks, particularly on Math and Coding splits, and significantly outperforms BM25 on Chain-of-Thought queries.

**Limitations:** 

**Conclusion:** With a fraction of the training data, RaDeR establishes a new standard for reasoning-based retrieval models in mathematical contexts, demonstrating high potential for diverse reasoning applications.

**Abstract:** We propose RaDeR, a set of reasoning-based dense retrieval models trained with data derived from mathematical problem solving using large language models (LLMs). Our method leverages retrieval-augmented reasoning trajectories of an LLM and self-reflective relevance evaluation, enabling the creation of both diverse and hard-negative samples for reasoning-intensive relevance. RaDeR retrievers, trained for mathematical reasoning, effectively generalize to diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently outperforming strong baselines in overall performance.Notably, RaDeR achieves significantly higher performance than baselines on the Math and Coding splits. In addition, RaDeR presents the first dense retriever that outperforms BM25 when queries are Chain-of-Thought reasoning steps, underscoring the critical role of reasoning-based retrieval to augment reasoning language models. Furthermore, RaDeR achieves comparable or superior performance while using only 2.5% of the training data used by the concurrent work REASONIR, highlighting the quality of our synthesized training data.

</details>


### [44] [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)

*Yue Jiang, Jichu Li, Yang Liu, Dingkang Yang, Feng Zhou, Quyu Kong*

**Main category:** cs.CL

**Keywords:** Multi-modal learning, Temporal Point Processes, Large Language Models, Question-Answering, Benchmarking

**Relevance Score:** 6

**TL;DR:** DanmakuTPPBench is a benchmark for improving multi-modal Temporal Point Process modeling utilizing Large Language Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing datasets for Temporal Point Processes are mostly unimodal, limiting the development of models that need to reason over temporal, textual, and visual information together.

**Method:** The benchmark consists of two components: a dataset from Bilibili video comments that features annotated multi-modal events and a challenging question-answering dataset created using state-of-the-art multi-modal LLMs to assess complex reasoning.

**Key Contributions:**

	1. Introduction of a novel multi-modal dataset from user-generated content
	2. Development of a question-answering dataset utilizing advanced LLMs
	3. Establishment of benchmarks highlighting current model limitations.

**Result:** Evaluation reveals significant performance gaps in current models' abilities to capture multi-modal event dynamics, with better performance in MLLMs compared to classical TPP models.

**Limitations:** 

**Conclusion:** The benchmark provides strong baselines for future research and advocates for improving the integration of TPP modeling in multi-modal language models.

**Abstract:** We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance multi-modal Temporal Point Process (TPP) modeling in the era of Large Language Models (LLMs). While TPPs have been widely studied for modeling temporal event sequences, existing datasets are predominantly unimodal, hindering progress in models that require joint reasoning over temporal, textual, and visual information. To address this gap, DanmakuTPPBench comprises two complementary components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili video platform, where user-generated bullet comments (Danmaku) naturally form multi-modal events annotated with precise timestamps, rich textual content, and corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering dataset constructed via a novel multi-agent pipeline powered by state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex temporal-textual-visual reasoning. We conduct extensive evaluations using both classical TPP models and recent MLLMs, revealing significant performance gaps and limitations in current methods' ability to model multi-modal event dynamics. Our benchmark establishes strong baselines and calls for further integration of TPP modeling into the multi-modal language modeling landscape. The code and dataset have been released at https://github.com/FRENKIE-CHIANG/DanmakuTPPBench

</details>


### [45] [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)

*Khandakar Ashrafi Akbar, Md Nahiyan Uddin, Latifur Khan, Trayce Hockstad, Mizanur Rahman, Mashrur Chowdhury, Bhavani Thuraisingham*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Model, Legal insights, Transportation cybersecurity, Data privacy

**Relevance Score:** 6

**TL;DR:** A RAG-based LLM framework enhances legal content extraction and response generation for policymakers addressing cybersecurity and privacy in transportation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address emerging cybersecurity and data privacy challenges in connected and automated transportation systems through legal scrutiny.

**Method:** A Retrieval-Augmented Generation (RAG) based LLM framework that uses domain-specific questions for improved output specificity and grounding.

**Key Contributions:**

	1. Introduction of a RAG-based LLM framework for legal insights
	2. Demonstrated performance superiority over commercial LLMs
	3. Addresses specific needs for policymakers in the context of transportation legislation

**Result:** The proposed framework outperformed leading commercial LLMs across four evaluation metrics, demonstrating its effectiveness in providing reliable legal insights.

**Limitations:** 

**Conclusion:** This framework offers a scalable AI-driven method for legislative analysis, supporting updates of legal frameworks in line with technological advancements.

**Abstract:** As connected and automated transportation systems evolve, there is a growing need for federal and state authorities to revise existing laws and develop new statutes to address emerging cybersecurity and data privacy challenges. This study introduces a Retrieval-Augmented Generation (RAG) based Large Language Model (LLM) framework designed to support policymakers by extracting relevant legal content and generating accurate, inquiry-specific responses. The framework focuses on reducing hallucinations in LLMs by using a curated set of domain-specific questions to guide response generation. By incorporating retrieval mechanisms, the system enhances the factual grounding and specificity of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore, BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and context-aware legal insights. This approach offers a scalable, AI-driven method for legislative analysis, supporting efforts to update legal frameworks in line with advancements in transportation technologies.

</details>


### [46] [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)

*AbdelRahim Elmadany, Sang Yun Kwon, Hawau Olamide Toyin, Alcides Alcoba Inciarte, Hanan Aldarmaki, Muhammad Abdul-Mageed*

**Main category:** cs.CL

**Keywords:** speech technologies, African languages, benchmarking, digital inclusion, machine learning

**Relevance Score:** 4

**TL;DR:** The paper addresses the underrepresentation of Africa's linguistic diversity in speech technologies, introducing a benchmark and models to improve performance in African languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the barriers to digital inclusion caused by the lack of speech technologies for Africa's diverse languages.

**Method:** Systematic mapping of datasets and technologies in African speech, leading to the creation of SimbaBench and the Simba family of models.

**Key Contributions:**

	1. Creation of the SimbaBench benchmark for African speech tasks
	2. Introduction of the Simba family of models for improved performance
	3. Analysis of critical patterns in dataset quality and language relationships affecting technology performance

**Result:** Achieved state-of-the-art performance across multiple African languages and revealed critical patterns in resource availability through benchmark analysis.

**Limitations:** 

**Conclusion:** The work emphasizes the need for more resources in speech technology that reflect African linguistic diversity and lays the groundwork for future inclusive technology developments.

**Abstract:** Africa's rich linguistic diversity remains significantly underrepresented in speech technologies, creating barriers to digital inclusion. To alleviate this challenge, we systematically map the continent's speech space of datasets and technologies, leading to a new comprehensive benchmark SimbaBench for downstream African speech tasks. Using SimbaBench, we introduce the Simba family of models, achieving state-of-the-art performance across multiple African languages and speech tasks. Our benchmark analysis reveals critical patterns in resource availability, while our model evaluation demonstrates how dataset quality, domain diversity, and language family relationships influence performance across languages. Our work highlights the need for expanded speech technology resources that better reflect Africa's linguistic diversity and provides a solid foundation for future research and development efforts toward more inclusive speech technologies.

</details>


### [47] [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)

*Zhaoyang Wang, Jinqi Jiang, Tian Qiu, Hui Liu, Xianfeng Tang, Huaxiu Yao*

**Main category:** cs.CL

**Keywords:** small language models, chain-of-thought reasoning, distillation, mathematical reasoning, redundancy

**Relevance Score:** 8

**TL;DR:** The paper proposes a method to distill long chain-of-thought reasoning into small language models by pruning redundant steps, allowing SLMs to learn efficient reasoning without overthinking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of large reasoning models highlights the challenge of training small language models to generate effective long chain-of-thought (CoT) reasoning due to their limited capacity and social tendencies to produce redundant content.

**Method:** The proposed method involves pruning unnecessary steps from long CoT sequences and using an on-policy approach for the small language model to curate training data, enhancing learning efficiency.

**Key Contributions:**

	1. Development of a pruning method for CoT reasoning
	2. Implementation of an on-policy method for training data curation
	3. Demonstration of competitive performance on mathematical reasoning tasks

**Result:** Experimental validation shows that the proposed pruning and training data curation method enables small language models to achieve competitive performance on mathematical reasoning benchmarks while significantly reducing redundant reasoning steps.

**Limitations:** 

**Conclusion:** The proposed approach effectively distills complex reasoning abilities into small language models, promoting efficient learning and maintaining performance standards without excessive redundancy.

**Abstract:** Recent large reasoning models such as DeepSeek-R1 exhibit strong complex problems solving abilities by generating long chain-of-thought (CoT) reasoning steps. It is challenging to directly train small language models (SLMs) to emerge long CoT. Thus, distillation becomes a practical method to enable SLMs for such reasoning ability. However, the long CoT often contains a lot of redundant contents (e.g., overthinking steps) which may make SLMs hard to learn considering their relatively poor capacity and generalization. To address this issue, we propose a simple-yet-effective method to prune unnecessary steps in long CoT, and then employ an on-policy method for the SLM itself to curate valid and useful long CoT training data. In this way, SLMs can effectively learn efficient long CoT reasoning and preserve competitive performance at the same time. Experimental results across a series of mathematical reasoning benchmarks demonstrate the effectiveness of the proposed method in distilling long CoT reasoning ability into SLMs which maintains the competitive performance but significantly reduces generating redundant reasoning steps.

</details>


### [48] [BRIT: Bidirectional Retrieval over Unified Image-Text Graph](https://arxiv.org/abs/2505.18450)

*Ainulla Khan, Yamada Moyuru, Srinidhi Akella*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, multi-modal, question answering, text-image relations, cross-modal

**Relevance Score:** 9

**TL;DR:** BRIT is a new multi-modal Retrieval-Augmented Generation (RAG) framework that improves the handling of cross-modal questions in documents containing text and images.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capability of RAG frameworks in processing multi-modal documents that combine texts and images, particularly for complex queries.

**Method:** BRIT constructs a multi-modal graph that unifies text-image connections and retrieves relevant sub-graphs based on query-specific needs, facilitating cross-modal question answering.

**Key Contributions:**

	1. Introduction of a novel multi-modal RAG framework called BRIT.
	2. Development of the MM-RAG test set for evaluating multi-modal question answering.
	3. Demonstration of superior performance of BRIT in handling complex cross-modal queries.

**Result:** BRIT outperforms existing methods on the newly introduced MM-RAG test set designed for multi-modal question answering, showing better handling of cross-modal relations.

**Limitations:** 

**Conclusion:** The results indicate that BRIT effectively addresses the challenges posed by multi-modal documents in RAG formulations.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a promising technique to enhance the quality and relevance of responses generated by large language models. While recent advancements have mainly focused on improving RAG for text-based queries, RAG on multi-modal documents containing both texts and images has not been fully explored. Especially when fine-tuning does not work. This paper proposes BRIT, a novel multi-modal RAG framework that effectively unifies various text-image connections in the document into a multi-modal graph and retrieves the texts and images as a query-specific sub-graph. By traversing both image-to-text and text-to-image paths in the graph, BRIT retrieve not only directly query-relevant images and texts but also further relevant contents to answering complex cross-modal multi-hop questions. To evaluate the effectiveness of BRIT, we introduce MM-RAG test set specifically designed for multi-modal question answering tasks that require to understand the text-image relations. Our comprehensive experiments demonstrate the superiority of BRIT, highlighting its ability to handle cross-modal questions on the multi-modal documents.

</details>


### [49] [MedScore: Factuality Evaluation of Free-Form Medical Answers](https://arxiv.org/abs/2505.18452)

*Heyuan Huang, Alexandra DeLucia, Vijay Murari Tiyyala, Mark Dredze*

**Main category:** cs.CL

**Keywords:** Factuality evaluation, Large Language Models, MedScore, Health informatics, Data extraction

**Relevance Score:** 9

**TL;DR:** MedScore improves factuality evaluation in medical answers by extracting more valid facts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing factuality systems struggle with the complexity of medical answers, leading to potential misinformation that can harm patients.

**Method:** MedScore decomposes medical answers into condition-aware valid facts, enhancing the extraction of factual information.

**Key Contributions:**

	1. Introduction of MedScore for improved factuality evaluation in medical contexts
	2. Increased extraction of valid facts compared to existing systems
	3. Emphasis on condition-awareness for accurate fact decomposition

**Result:** MedScore extracts up to three times more valid facts than traditional methods, reducing hallucination and retaining condition dependency.

**Limitations:** 

**Conclusion:** Customizing methods for factuality evaluation is crucial for reliability in medical contexts.

**Abstract:** While Large Language Models (LLMs) can generate fluent and convincing responses, they are not necessarily correct. This is especially apparent in the popular decompose-then-verify factuality evaluation pipeline, where LLMs evaluate generations by decomposing the generations into individual, valid claims. Factuality evaluation is especially important for medical answers, since incorrect medical information could seriously harm the patient. However, existing factuality systems are a poor match for the medical domain, as they are typically only evaluated on objective, entity-centric, formulaic texts such as biographies and historical topics. This differs from condition-dependent, conversational, hypothetical, sentence-structure diverse, and subjective medical answers, which makes decomposition into valid facts challenging. We propose MedScore, a new approach to decomposing medical answers into condition-aware valid facts. Our method extracts up to three times more valid facts than existing methods, reducing hallucination and vague references, and retaining condition-dependency in facts. The resulting factuality score significantly varies by decomposition method, verification corpus, and used backbone LLM, highlighting the importance of customizing each step for reliable factuality evaluation.

</details>


### [50] [Hybrid Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.18454)

*Zhenrui Yue, Bowen Jin, Huimin Zeng, Honglei Zhuang, Zhen Qin, Jinsung Yoon, Lanyu Shang, Jiawei Han, Dong Wang*

**Main category:** cs.CL

**Keywords:** latent reasoning, large language models, reinforcement learning, hybrid reasoning, interpretable AI

**Relevance Score:** 8

**TL;DR:** This paper introduces hybrid reasoning policy optimization (HRPO), a reinforcement learning-based approach to latent reasoning in large language models, enhancing their generative capabilities while improving performance on knowledge- and reasoning-intensive tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the incompatibility of latent reasoning methods with LLMs and to leverage their inherent reasoning patterns without requiring chain-of-thought traces for training.

**Method:** Hybrid reasoning policy optimization (HRPO) integrates prior hidden states into sampled tokens using a learnable gating mechanism and initializes training with token embeddings, progressively incorporating hidden features, and introducing stochasticity through token sampling.

**Key Contributions:**

	1. Introduction of hybrid reasoning policy optimization (HRPO) for latent reasoning in LLMs.
	2. Integration of discrete and continuous representations in reasoning tasks.
	3. Performance improvement over existing methods in knowledge- and reasoning-intensive benchmarks.

**Result:** HRPO outperforms previous methods on diverse benchmarks in knowledge- and reasoning-intensive tasks, maintaining interpretability and showcasing interesting behaviors like cross-lingual patterns.

**Limitations:** 

**Conclusion:** The RL-based HRPO approach not only enhances LLM capabilities but also provides insights for future research into latent reasoning in AI.

**Abstract:** Recent advances in large language models (LLMs) have introduced latent reasoning as a promising alternative to autoregressive reasoning. By performing internal computation with hidden states from previous steps, latent reasoning benefit from more informative features rather than sampling a discrete chain-of-thought (CoT) path. Yet latent reasoning approaches are often incompatible with LLMs, as their continuous paradigm conflicts with the discrete nature of autoregressive generation. Moreover, these methods rely on CoT traces for training and thus fail to exploit the inherent reasoning patterns of LLMs. In this work, we explore latent reasoning by leveraging the intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid latent reasoning approach that (1) integrates prior hidden states into sampled tokens with a learnable gating mechanism, and (2) initializes training with predominantly token embeddings while progressively incorporating more hidden features. This design maintains LLMs' generative capabilities and incentivizes hybrid reasoning using both discrete and continuous representations. In addition, the hybrid HRPO introduces stochasticity into latent reasoning via token sampling, thereby enabling RL-based optimization without requiring CoT trajectories. Extensive evaluations across diverse benchmarks show that HRPO outperforms prior methods in both knowledge- and reasoning-intensive tasks. Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing behaviors like cross-lingual patterns and shorter completion lengths, highlighting the potential of our RL-based approach and offer insights for future work in latent reasoning.

</details>


### [51] [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)

*Litu Rout, Constantine Caramanis, Sanjay Shakkottai*

**Main category:** cs.CL

**Keywords:** Diffusion Language Models, Anchored Diffusion, Natural Language Processing, Text Generation, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper presents the Anchored Diffusion Language Model (ADLM) which improves the performance of diffusion language models by accurately addressing key tokens' masking in the generation process.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Diffusion Language Models (DLMs) have potential advantages over autoregressive models but currently underperform in generating high-quality text due to limitations in modeling important tokens during the generation process.

**Method:** The ADLM framework is a two-stage process that first identifies important tokens using an anchor network, followed by predicting the likelihood of other tokens while conditioning on these important tokens.

**Key Contributions:**

	1. Introduction of the Anchored Diffusion Language Model (ADLM) framework
	2. Achievement of state-of-the-art performance in text generation compared to autoregressive models
	3. Derivation of the Anchored Negative Evidence Lower Bound (ANELBO) objective to improve sample complexity

**Result:** ADLM shows a significant reduction in test perplexity on datasets like LM1B and OpenWebText, achieving up to 25.4% better performance than previous DLMs, and outperforms autoregressive models in generating human-like text.

**Limitations:** 

**Conclusion:** The introduction of anchoring improves the performance of DLMs and AR models, enhancing accuracy in reasoning tasks and establishing a new benchmark in text generation.

**Abstract:** Diffusion Language Models (DLMs) promise parallel generation and bidirectional context, yet they underperform autoregressive (AR) models in both likelihood modeling and generated text quality. We identify that this performance gap arises when important tokens (e.g., key words or low-frequency words that anchor a sentence) are masked early in the forward process, limiting contextual information for accurate reconstruction. To address this, we introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage framework that first predicts distributions over important tokens via an anchor network, and then predicts the likelihoods of missing tokens conditioned on the anchored predictions. ADLM significantly improves test perplexity on LM1B and OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap with strong AR baselines. It also achieves state-of-the-art performance in zero-shot generalization across seven benchmarks and surpasses AR models in MAUVE score, which marks the first time a DLM generates better human-like text than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower Bound (ANELBO) objective and show that anchoring improves sample complexity and likelihood modeling. Beyond diffusion, anchoring boosts performance in AR models and enhances reasoning in math and logic tasks, outperforming existing chain-of-thought approaches

</details>


### [52] [Measuring South Asian Biases in Large Language Models](https://arxiv.org/abs/2505.18466)

*Mamnuya Rinki, Chahat Raj, Anjishnu Mukherjee, Ziwei Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Bias, Self-Debiasing, Intersectionality, Indo-Aryan Languages

**Relevance Score:** 9

**TL;DR:** This paper addresses intersectional and culturally specific biases in LLMs, particularly in South Asian languages, by creating a bias lexicon and evaluating self-debiasing strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover and address the overlooked biases in LLM outputs, particularly in underrepresented multilingual regions like South Asia.

**Method:** Conducted a multilingual intersectional analysis of LLM outputs across 10 languages, developed a bias lexicon, and assessed self-debiasing strategies.

**Key Contributions:**

	1. Development of a culturally grounded bias lexicon for intersectional analysis
	2. Quantitative assessment of intersectional bias in multilingual contexts
	3. Evaluation of self-debiasing strategies for reducing cultural bias in generative tasks

**Result:** Identified cultural biases linked to purdah and patriarchy in generative tasks, and evaluated the effectiveness of simple and complex self-debiasing prompts.

**Limitations:** The focus is limited to specific cultural contexts and languages, potentially limiting generalizability.

**Conclusion:** The introduced lexicon and evaluation framework provide deeper insights into cultural bias in LLMs beyond traditional Eurocentric contexts.

**Abstract:** Evaluations of Large Language Models (LLMs) often overlook intersectional and culturally specific biases, particularly in underrepresented multilingual regions like South Asia. This work addresses these gaps by conducting a multilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan and Dravidian languages, identifying how cultural stigmas influenced by purdah and patriarchy are reinforced in generative tasks. We construct a culturally grounded bias lexicon capturing previously unexplored intersectional dimensions including gender, religion, marital status, and number of children. We use our lexicon to quantify intersectional bias and the effectiveness of self-debiasing in open-ended generations (e.g., storytelling, hobbies, and to-do lists), where bias manifests subtly and remains largely unexamined in multilingual contexts. Finally, we evaluate two self-debiasing strategies (simple and complex prompts) to measure their effectiveness in reducing culturally specific bias in Indo-Aryan and Dravidian languages. Our approach offers a nuanced lens into cultural bias by introducing a novel bias lexicon and evaluation framework that extends beyond Eurocentric or small-scale multilingual settings.

</details>


### [53] [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)

*Hong Jiao, Dan Song, Won-Chan Lee*

**Main category:** cs.CL

**Keywords:** automated scoring, large language models, educational assessment

**Relevance Score:** 9

**TL;DR:** This study evaluates the scoring accuracy of ten large language models (LLMs) against human raters in low-stakes writing assessments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To collect empirical evidence on the reliability of LLMs for automated scoring in education.

**Method:** The study compared ten LLMs with human expert raters in scoring two types of writing tasks, using metrics like Quadratic Weighted Kappa and Cronbach Alpha for evaluation.

**Key Contributions:**

	1. Comparison of multiple LLMs for scoring accuracy
	2. Identification of LLMs with high reliability and low rater effects
	3. Implications for the use of LLMs in educational assessment

**Result:** Results indicated that ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet showed high scoring accuracy, reliable rater consistency, and reduced rater effects.

**Limitations:** 

**Conclusion:** The findings support the use of specific LLMs for automated scoring in educational settings.

**Abstract:** Large language models (LLMs) have been widely explored for automated scoring in low-stakes assessment to facilitate learning and instruction. Empirical evidence related to which LLM produces the most reliable scores and induces least rater effects needs to be collected before the use of LLMs for automated scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4, ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini 2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in scoring two types of writing tasks. The accuracy of the holistic and analytic scores from LLMs compared with human raters was evaluated in terms of Quadratic Weighted Kappa. Intra-rater consistency across prompts was compared in terms of Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human raters using the Many-Facet Rasch model. The results in general supported the use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring accuracy, better rater reliability, and less rater effects.

</details>


### [54] [The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models](https://arxiv.org/abs/2505.18497)

*Kefan Yu, Qingcheng Zeng, Weihao Xuan, Wanxin Li, Jingyi Wu, Rob Voigt*

**Main category:** cs.CL

**Keywords:** Pragmatics, Large Language Models, Model Training, Human-Computer Interaction, Cognitive Reasoning

**Relevance Score:** 9

**TL;DR:** This paper introduces ALTPRAG, a dataset to evaluate the pragmatic competence of large language models (LLMs) during training, revealing that LLMs improve in understanding speaker intentions as they are fine-tuned.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how LLMs develop pragmatic competence is crucial for better aligning AI with human communicative norms.

**Method:** The study utilizes the ALTPRAG dataset, which compares model outputs in contextually appropriate yet pragmatically distinct scenarios across different training stages of 22 LLMs.

**Key Contributions:**

	1. Introduction of the ALTPRAG dataset for evaluating pragmatic reasoning in LLMs
	2. Findings indicate significant improvements in pragmatic understanding with model training stages
	3. Insights into aligning LLMs with human communicative norms

**Result:** LLMs, even at base levels, show sensitivity to pragmatic cues, with performance improving through fine-tuning and preference optimization.

**Limitations:** The study primarily focuses on LLMs; insights may not apply equally to other AI systems.

**Conclusion:** The development of pragmatic competence in LLMs is emergent and compositional, providing insights for enhancing model alignment with human communication.

**Abstract:** Current large language models (LLMs) have demonstrated emerging capabilities in social intelligence tasks, including implicature resolution (Sravanthi et al. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which require substantial pragmatic understanding. However, how LLMs acquire this competence throughout the training process remains poorly understood. In this work, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of alternatives, designed to evaluate whether LLMs at different training stages can accurately infer nuanced speaker intentions. Each instance pairs two contextually appropriate but pragmatically distinct continuations, enabling fine-grained assessment of both pragmatic interpretation and contrastive reasoning. We systematically evaluate 22 LLMs across key training stages: pre-training, supervised fine-tuning (SFT), and preference optimization, to examine the development of pragmatic competence. Our results show that even base models exhibit notable sensitivity to pragmatic cues, which improves consistently with increases in model and data scale. Additionally, SFT and RLHF contribute further gains, particularly in cognitive-pragmatic reasoning. These findings highlight pragmatic competence as an emergent and compositional property of LLM training and offer new insights for aligning models with human communicative norms.

</details>


### [55] [How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation](https://arxiv.org/abs/2505.18522)

*Xin Lu, Yanyan Zhao, Si Wei, Shijin Wang, Bing Qin, Ting Liu*

**Main category:** cs.CL

**Keywords:** Language Models, Sequence Modeling, Transformer Architecture, Pre-training, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper investigates how sequence modeling architectures affect the base capabilities of pre-trained language models and proposes a new design principle to enhance these capabilities.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of sequence modeling architectures on the base capabilities of pre-trained language models, addressing gaps in existing approaches that do not showcase architectural differences effectively.

**Method:** The authors propose a limited domain pre-training setting with out-of-distribution testing to uncover differences in base capabilities among various architectures. They perform architecture component analysis to identify key design principles and validate their findings through empirical testing.

**Key Contributions:**

	1. Introduction of a limited domain pre-training setting for better evaluation of architectural differences.
	2. Identification of the key design principle: full-sequence arbitrary selection capability is essential to maintain base capabilities.
	3. Empirical validation of the proposed architectures, demonstrating significant improvements in performance.

**Result:** Significant differences in base capabilities were discovered among stateful and Transformer architectures, with the latter exhibiting superior performance. The proposed principle of full-sequence arbitrary selection capability mitigates capability degradation.

**Limitations:** The study is limited to the architectures tested and may not generalize beyond the scope of the proposed settings.

**Conclusion:** The findings indicate that adopting the proposed design principle can enhance the base capabilities of language models, which can inform future architecture designs.

**Abstract:** Pre-trained language models represented by the Transformer have been proven to possess strong base capabilities, and the representative self-attention mechanism in the Transformer has become a classic in sequence modeling architectures. Different from the work of proposing sequence modeling architecture to improve the efficiency of attention mechanism, this work focuses on the impact of sequence modeling architectures on base capabilities. Specifically, our concern is: How exactly do sequence modeling architectures affect the base capabilities of pre-trained language models? In this work, we first point out that the mixed domain pre-training setting commonly adopted in existing architecture design works fails to adequately reveal the differences in base capabilities among various architectures. To address this, we propose a limited domain pre-training setting with out-of-distribution testing, which successfully uncovers significant differences in base capabilities among architectures at an early stage. Next, we analyze the base capabilities of stateful sequence modeling architectures, and find that they exhibit significant degradation in base capabilities compared to the Transformer. Then, through a series of architecture component analysis, we summarize a key architecture design principle: A sequence modeling architecture need possess full-sequence arbitrary selection capability to avoid degradation in base capabilities. Finally, we empirically validate this principle using an extremely simple Top-1 element selection architecture and further generalize it to a more practical Top-1 chunk selection architecture. Experimental results demonstrate our proposed sequence modeling architecture design principle and suggest that our work can serve as a valuable reference for future architecture improvements and novel designs.

</details>


### [56] [metaTextGrad: Automatically optimizing language model optimizers](https://arxiv.org/abs/2505.18524)

*Guowei Xu, Mert Yuksekgonul, Carlos Guestrin, James Zou*

**Main category:** cs.CL

**Keywords:** LLMs, optimizers, meta-optimization

**Relevance Score:** 8

**TL;DR:** The paper presents metaTextGrad, a meta-optimizer designed to enhance existing LLM-based optimizers for specific tasks, improving their performance by up to 6%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLM-based optimizers that are generally designed and not optimized for specific tasks, enhancing their performance in AI systems.

**Method:** The proposed method includes a meta prompt optimizer and a meta structure optimizer that together improve the performance of existing optimizers across benchmarks.

**Key Contributions:**

	1. Development of a novel meta-optimizer for language model optimizers
	2. Introduction of a meta prompt optimizer
	3. Introduction of a meta structure optimizer

**Result:** The metaTextGrad significantly improves the performance of AI systems, achieving an average absolute performance improvement of up to 6% over the best baseline.

**Limitations:** 

**Conclusion:** The introduction of metaTextGrad offers a tailored optimization approach for LLM-based optimizers, aligning them more effectively with specific tasks and resulting in better AI system performance.

**Abstract:** Large language models (LLMs) are increasingly used in learning algorithms, evaluations, and optimization tasks. Recent studies have shown that using LLM-based optimizers to automatically optimize model prompts, demonstrations, predictions themselves, or other components can significantly enhance the performance of AI systems, as demonstrated by frameworks such as DSPy and TextGrad. However, optimizers built on language models themselves are usually designed by humans with manual design choices; optimizers themselves are not optimized. Moreover, these optimizers are general purpose by design, to be useful to a broad audience, and are not tailored for specific tasks. To address these challenges, we propose metaTextGrad, which focuses on designing a meta-optimizer to further enhance existing optimizers and align them to be good optimizers for a given task. Our approach consists of two key components: a meta prompt optimizer and a meta structure optimizer. The combination of these two significantly improves performance across multiple benchmarks, achieving an average absolute performance improvement of up to 6% compared to the best baseline.

</details>


### [57] [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)

*Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, Xueqian Wang*

**Main category:** cs.CL

**Keywords:** Reinforcement Fine-Tuning, Multimodal Language Models, Artificial General Intelligence, Reasoning Capability, Future Research Directions

**Relevance Score:** 7

**TL;DR:** This position paper discusses the role of reinforcement fine-tuning (RFT) in enhancing the reasoning capabilities of multimodal large language models (MLLMs) and outlines future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the potential of reinforcement fine-tuning (RFT) in advancing the reasoning capabilities of multimodal large language models (MLLMs) in the context of Artificial General Intelligence (AGI).

**Method:** The authors provide a comprehensive introduction to the necessary background knowledge, summarize the improvements brought by RFT to MLLMs into five key points, and propose five promising directions for future research.

**Key Contributions:**

	1. Detailed introduction to RFT in MLLMs
	2. Summary of RFT improvements in five key aspects
	3. Proposed future research directions for MLLMs

**Result:** RFT has significantly enhanced reasoning capabilities in MLLMs through diverse modalities, tasks, improved training algorithms, benchmarks, and engineering frameworks.

**Limitations:** 

**Conclusion:** The paper aims to provide insights for future research and contribute to the community's understanding of the role of RFT in achieving AGI.

**Abstract:** Standing in 2025, at a critical juncture in the pursuit of Artificial General Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated significant potential in enhancing the reasoning capability of large language models (LLMs) and has led to the development of cutting-edge AI models such as OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to enhance the reasoning capability of multimodal large language models (MLLMs) has attracted widespread attention from the community. In this position paper, we argue that reinforcement fine-tuning powers the reasoning capability of multimodal large language models. To begin with, we provide a detailed introduction to the fundamental background knowledge that researchers interested in this field should be familiar with. Furthermore, we meticulously summarize the improvements of RFT in powering reasoning capability of MLLMs into five key points: diverse modalities, diverse tasks and domains, better training algorithms, abundant benchmarks and thriving engineering frameworks. Finally, we propose five promising directions for future research that the community might consider. We hope that this position paper will provide valuable insights to the community at this pivotal stage in the advancement toward AGI. Summary of works done on RFT for MLLMs is available at https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.

</details>


### [58] [Business as \textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs](https://arxiv.org/abs/2505.18542)

*Chen Yang, Ruping Xu, Ruizhe Li, Bin Cao, Jing Fan*

**Main category:** cs.CL

**Keywords:** business process mining, business rules extraction, large language models, dependency analysis, dataset

**Relevance Score:** 4

**TL;DR:** Research introduces a novel framework for extracting business rules from documents using LLMs, leveraging a new annotated dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the under-researched area of rule flows in business documents using process mining techniques.

**Method:** Introduced a new dataset (BPRF) with annotated business rules and developed ExIde for automatic extraction and dependency identification using LLMs.

**Key Contributions:**

	1. Introduction of the BPRF dataset with extensively annotated business rules
	2. Development of the ExIde framework for rule extraction and dependency identification
	3. Performance evaluation of ExIde on multiple state-of-the-art LLMs

**Result:** Evaluation of ExIde on 12 SOTA LLMs shows effective structured rule extraction and analysis of interdependencies, indicating potential for improved business process automation.

**Limitations:** 

**Conclusion:** The study demonstrates that LLMs can effectively extract and analyze business rules from documents, enhancing process automation.

**Abstract:** Process mining aims to discover, monitor and optimize the actual behaviors of real processes. While prior work has mainly focused on extracting procedural action flows from instructional texts, rule flows embedded in business documents remain underexplored. To this end, we introduce a novel annotated Chinese dataset, \textbf{BPRF}, which contains 50 business process documents with 326 explicitly labeled business rules across multiple domains. Each rule is represented as a <Condition, Action> pair, and we annotate logical dependencies between rules (sequential, conditional, or parallel). We also propose \textbf{ExIde}, a framework for automatic business rule extraction and dependency relationship identification using large language models (LLMs). We evaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset, benchmarking performance on both rule extraction and dependency classification tasks of current LLMs. Our results demonstrate the effectiveness of ExIde in extracting structured business rules and analyzing their interdependencies for current SOTA LLMs, paving the way for more automated and interpretable business process automation.

</details>


### [59] [Composable Cross-prompt Essay Scoring by Merging Models](https://arxiv.org/abs/2505.18548)

*Sanwoo Lee, Kun Liang, Yunfang Wu*

**Main category:** cs.CL

**Keywords:** automated essay scoring, source-free adaptation, Bayesian optimization, machine learning, privacy concerns

**Relevance Score:** 7

**TL;DR:** This paper presents a novel source-free adaptation approach for automated essay scoring that merges model parameters from individually trained source models instead of datasets, addressing privacy concerns and optimizing performance through a Bayesian optimization of an unsupervised objective.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Recent advancements in automated essay scoring have raised issues regarding the use of multiple source datasets, particularly in terms of privacy and effectiveness of joint training.

**Method:** The paper proposes a source-free adaptation approach that combines separately trained model parameters using linear combinations of task vectors. It introduces Prior-encoded Information Maximization (PIM) as an unsupervised objective optimized via Bayesian optimization.

**Key Contributions:**

	1. Source-free adaptation approach for automated essay scoring
	2. Introduction of Prior-encoded Information Maximization (PIM) for optimization
	3. Demonstration of robustness and efficiency under distribution shifts

**Result:** Experimental results demonstrate that the proposed method consistently outperforms traditional joint training methods, maintains robustness compared to other merging approaches, and excels under severe distribution shifts while being computationally efficient.

**Limitations:** 

**Conclusion:** The findings suggest that the source-free adaptation method is viable and effective, overcoming limitations of previous models that rely on joint training with all available datasets.

**Abstract:** Recent advances in cross-prompt automated essay scoring (AES) typically train models jointly on all source prompts, often requiring additional access to unlabeled target prompt essays simultaneously. However, using all sources is suboptimal in our pilot study, and re-accessing source datasets during adaptation raises privacy concerns. We propose a source-free adaptation approach that selectively merges individually trained source models' parameters instead of datasets. In particular, we simulate joint training through linear combinations of task vectors -- the parameter updates from fine-tuning. To optimize the combination's coefficients, we propose Prior-encoded Information Maximization (PIM), an unsupervised objective which promotes the model's score discriminability regularized by priors pre-computed from the sources. We employ Bayesian optimization as an efficient optimizer of PIM. Experimental results with LLMs on in-dataset and cross-dataset adaptation show that our method (1) consistently outperforms training jointly on all sources, (2) maintains superior robustness compared to other merging methods, (3) excels under severe distribution shifts where recent leading cross-prompt methods struggle, all while retaining computational efficiency.

</details>


### [60] [MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors](https://arxiv.org/abs/2505.18549)

*Baraa Hikal, Mohamed Basem, Islam Oshallah, Ali Hamdi*

**Main category:** cs.CL

**Keywords:** AI education, language model evaluation, instruction tuning

**Relevance Score:** 8

**TL;DR:** MSA-MathEval evaluates AI tutor responses across four dimensions using a unified instruction-tuned language model and disagreement-aware ensemble inference, achieving strong performance in a competitive setting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate AI tutor responses effectively across multiple instructional dimensions with a robust methodology.

**Method:** A unified training pipeline was used to fine-tune a single instruction-tuned language model without task-specific changes, along with a disagreement-aware ensemble strategy to improve prediction reliability.

**Key Contributions:**

	1. Unified training approach for multi-dimensional evaluation
	2. Disagreement-aware ensemble inference strategy
	3. Strong competitive performance across evaluation dimensions

**Result:** The system ranked 1st in Providing Guidance, 3rd in Actionability, and 4th in Mistake Identification and Mistake Location, demonstrating strong multi-dimensional evaluation performance.

**Limitations:** 

**Conclusion:** Scalable instruction tuning combined with disagreement-driven modeling can effectively evaluate LLMs as educational tutors.

**Abstract:** We present MSA-MathEval, our submission to the BEA 2025 Shared Task on evaluating AI tutor responses across four instructional dimensions: Mistake Identification, Mistake Location, Providing Guidance, and Actionability. Our approach uses a unified training pipeline to fine-tune a single instruction-tuned language model across all tracks, without any task-specific architectural changes. To improve prediction reliability, we introduce a disagreement-aware ensemble inference strategy that enhances coverage of minority labels. Our system achieves strong performance across all tracks, ranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both Mistake Identification and Mistake Location. These results demonstrate the effectiveness of scalable instruction tuning and disagreement-driven modeling for robust, multi-dimensional evaluation of LLMs as educational tutors.

</details>


### [61] [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555)

*Yiyang Feng, Yichen Wang, Shaobo Cui, Boi Faltings, Mina Lee, Jiawei Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, misinformation, reasoning, factual corrections, machine learning

**Relevance Score:** 9

**TL;DR:** This paper analyzes the impact of misinformation on mathematical reasoning in Large Language Models (LLMs) and suggests methods to mitigate this issue.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The research explores how misinformation affects LLMs' reasoning, which is crucial for their reliability in real-world interactions where incorrect inputs are common.

**Method:** A comprehensive analysis of the propagation of misinformation during the reasoning process in LLMs, with experiments on the effectiveness of factual corrections and fine-tuning with synthesized data.

**Key Contributions:**

	1. Detailed analysis of misinformation impact on LLMs' reasoning
	2. Effective methods for mitigating misinformation propagation
	3. Insights on the timing of factual corrections for improved outcomes

**Result:** LLMs show a significant drop in accuracy when facing misinformation, with a success rate of less than 50% in rectifying it, particularly when factual corrections are applied early in reasoning.

**Limitations:** The study focuses primarily on mathematical reasoning and might not generalize to other domains of LLM application.

**Conclusion:** The study presents effective strategies for reducing misinformation propagation, highlighting the importance of early-stage corrections and fine-tuning for improving reasoning accuracy.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive capabilities in reasoning, positioning them as promising tools for supporting human problem-solving. However, what happens when their performance is affected by misinformation, i.e., incorrect inputs introduced by users due to oversights or gaps in knowledge? Such misinformation is prevalent in real-world interactions with LLMs, yet how it propagates within LLMs' reasoning process remains underexplored. Focusing on mathematical reasoning, we present a comprehensive analysis of how misinformation affects intermediate reasoning steps and final answers. We also examine how effectively LLMs can correct misinformation when explicitly instructed to do so. Even with explicit instructions, LLMs succeed less than half the time in rectifying misinformation, despite possessing correct internal knowledge, leading to significant accuracy drops (10.02% - 72.20%). Further analysis shows that applying factual corrections early in the reasoning process most effectively reduces misinformation propagation, and fine-tuning on synthesized data with early-stage corrections significantly improves reasoning factuality. Our work offers a practical approach to mitigating misinformation propagation.

</details>


### [62] [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)

*Jun Zhuang, Haibo Jin, Ye Zhang, Zhengjian Kang, Wenbin Zhang, Gaby G. Dagher, Haohan Wang*

**Main category:** cs.CL

**Keywords:** intent detection, prompt refinement, large language models, content moderation, jailbreak methods

**Relevance Score:** 9

**TL;DR:** We propose a new two-stage intent-based prompt-refinement framework, IntentPrompt, that aims to explore the vulnerability of LLMs' content moderation guardrails by refining prompts into benign-looking declarative forms via intent manipulation for red-teaming purposes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the vulnerability of intent-aware guardrails in large language models (LLMs) and their implicit intent detection capabilities.

**Method:** A two-stage intent-based prompt-refinement framework, IntentPrompt, that transforms harmful inquiries into structured outlines and reframes them into declarative-style narratives through iterative optimization.

**Key Contributions:**

	1. Introduction of IntentPrompt framework for the vulnerability assessment of LLMs' guardrails.
	2. Demonstrated high success rates in prompt manipulation against state-of-the-art defenses.
	3. Highlighted the implicit intent detection capabilities of LLMs.

**Result:** The framework significantly outperforms existing jailbreak methods and successfully evades advanced defenses, achieving high attack success rates against various models.

**Limitations:** 

**Conclusion:** The findings reveal critical weaknesses in LLMs' safety mechanisms, highlighting intent manipulation as a growing challenge for content moderation.

**Abstract:** Intent detection, a core component of natural language understanding, has considerably evolved as a crucial mechanism in safeguarding large language models (LLMs). While prior work has applied intent detection to enhance LLMs' moderation guardrails, showing a significant success against content-level jailbreaks, the robustness of these intent-aware guardrails under malicious manipulations remains under-explored. In this work, we investigate the vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit implicit intent detection capabilities. We propose a two-stage intent-based prompt-refinement framework, IntentPrompt, that first transforms harmful inquiries into structured outlines and further reframes them into declarative-style narratives by iteratively optimizing prompts via feedback loops to enhance jailbreak success for red-teaming purposes. Extensive experiments across four public benchmarks and various black-box LLMs indicate that our framework consistently outperforms several cutting-edge jailbreak methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought (CoT)-based defenses. Specifically, our "FSTR+SPIN" variant achieves attack success rates ranging from 88.25% to 96.54% against CoT-based defenses on the o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based defenses. These findings highlight a critical weakness in LLMs' safety mechanisms and suggest that intent manipulation poses a growing challenge to content moderation guardrails.

</details>


### [63] [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)

*He Zhu, Zhiwen Ruan, Junyou Su, Xingwei He, Wenjia Zhang, Yun Chen, Guanhua Chen*

**Main category:** cs.CL

**Keywords:** instruction complexity, large language models, semantic compression, difficulty augmentation, RL-guided expansion

**Relevance Score:** 9

**TL;DR:** TAG-INSTRUCT is a novel framework for enhancing instruction complexity in large language models through structured semantic compression and difficulty augmentation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of instruction data used to develop large language models by controlling instruction complexity.

**Method:** Utilizes structured semantic compression to convert instructions into a compact tag space and applies RL-guided tag expansion to systematically enhance complexity.

**Key Contributions:**

	1. Introduction of the TAG-INSTRUCT framework
	2. Enhanced controllability through tag space manipulation
	3. Demonstrated superiority in instruction synthesis performance

**Result:** TAG-INSTRUCT outperformed existing methods for augmenting instruction complexity, demonstrating superior controllability and stability.

**Limitations:** 

**Conclusion:** The findings indicate that the tag space approach is more effective for instruction synthesis across various frameworks.

**Abstract:** High-quality instruction data is crucial for developing large language models (LLMs), yet existing approaches struggle to effectively control instruction complexity. We present TAG-INSTRUCT, a novel framework that enhances instruction complexity through structured semantic compression and controlled difficulty augmentation. Unlike previous prompt-based methods operating on raw text, TAG-INSTRUCT compresses instructions into a compact tag space and systematically enhances complexity through RL-guided tag expansion. Through extensive experiments, we show that TAG-INSTRUCT outperforms existing instruction complexity augmentation approaches. Our analysis reveals that operating in tag space provides superior controllability and stability across different instruction synthesis frameworks.

</details>


### [64] [Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages](https://arxiv.org/abs/2501.13836)

*Farhana Shahid, Mona Elswah, Aditya Vashistha*

**Main category:** cs.CL

**Keywords:** AI moderation, low-resource languages, harmful content, multi-stakeholder approaches, structural inequities

**Relevance Score:** 6

**TL;DR:** This paper explores the challenges of building AI moderation systems for low-resource languages due to data scarcity and technical issues, along with proposing multi-stakeholder solutions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the systemic challenges in automated moderation of harmful content in low-resource languages from the Global South.

**Method:** Semi-structured interviews with 22 AI experts on detecting harmful content in Tamil, Swahili, Maghrebi Arabic, and Quechua.

**Key Contributions:**

	1. Identified systemic challenges in AI moderation for low-resource languages.
	2. Documented technical issues affecting content moderation efforts.
	3. Proposed multi-stakeholder approaches for improvement.

**Result:** Identified that technical issues alongside data scarcity hinder effective moderation in low-resource languages, resulting in inaccuracies.

**Limitations:** The study focuses on a limited set of languages and may not cover all low-resource languages.

**Conclusion:** The limitations in moderation reflect structural inequities and suggest the need for multi-stakeholder approaches to enhance the effectiveness of AI moderation systems.

**Abstract:** Most social media users come from non-English speaking countries in the Global South, where much of harmful content appears in local languages. Yet, current AI-driven moderation systems struggle with low-resource languages spoken in these regions. This work examines the systemic challenges in building automated moderation tools for these languages. We conducted semi-structured interviews with 22 AI experts working on detecting harmful content in four low-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi Arabic (North Africa), and Quechua (South America). Our findings show that beyond the well-known data scarcity in local languages, technical issues--such as outdated machine translation systems, sentiment and toxicity models grounded in Western values, and unreliable language detection technologies--undermine moderation efforts. Even with more data, current language models and preprocessing pipelines--primarily designed for English--struggle with the morphological richness, linguistic complexity, and code-mixing. As a result, automated moderation in Tamil, Swahili, Arabic, and Quechua remains fraught with inaccuracies and blind spots. Based on our findings, we argue that these limitations are not just technical gaps but reflect deeper structural inequities that continue to reproduce historical power imbalances. We conclude by discussing multi-stakeholder approaches to improve automated moderation for low-resource languages.

</details>


### [65] [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)

*Xunlian Dai, Li Zhou, Benyou Wang, Haizhou Li*

**Main category:** cs.CL

**Keywords:** human-centered word association test, cross-cultural cognition, large language models, CultureSteer, cultural bias

**Relevance Score:** 8

**TL;DR:** This paper introduces CultureSteer, an LLM-adaptive approach that enhances cross-cultural cognitive alignment in large language models by integrating a culture-aware steering mechanism.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess and improve the alignment of large language models with cross-cultural cognition and address biases towards Western cultural schemas.

**Method:** The paper extends the human-centered word association test to create a free-relation task for LLMs, using CultureSteer for better cultural representation and alignment.

**Key Contributions:**

	1. Introduction of CultureSteer for cultural alignment in LLMs.
	2. Demonstration of significant bias in current LLMs towards Western cultural schemas.
	3. Validation of the method's efficacy through downstream tasks.

**Result:** Experiments demonstrate that current LLMs show significant bias toward Western schemas; however, CultureSteer improves cross-cultural alignment and outperforms prompt-based methods in semantic association tasks.

**Limitations:** Limited to the word association level and may not address all aspects of cultural representation in LLMs.

**Conclusion:** The findings suggest that enhancing cultural awareness in LLMs is crucial for developing more inclusive language technologies, and the proposed methodology is validated through various culture-sensitive tasks.

**Abstract:** The human-centered word association test (WAT) serves as a cognitive proxy, revealing sociocultural variations through lexical-semantic patterns. We extend this test into an LLM-adaptive, free-relation task to assess the alignment of large language models (LLMs) with cross-cultural cognition. To mitigate the culture preference, we propose CultureSteer, an innovative approach that integrates a culture-aware steering mechanism to guide semantic representations toward culturally specific spaces. Experiments show that current LLMs exhibit significant bias toward Western cultural (notably in American) schemas at the word association level. In contrast, our model substantially improves cross-cultural alignment, surpassing prompt-based methods in capturing diverse semantic associations. Further validation on culture-sensitive downstream tasks confirms its efficacy in fostering cognitive alignment across cultures. This work contributes a novel methodological paradigm for enhancing cultural awareness in LLMs, advancing the development of more inclusive language technologies.

</details>


### [66] [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)

*Wentao Hu, Wengyu Zhang, Yiyang Jiang, Chen Jason Zhang, Xiaoyong Wei, Qing Li*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Multi-Agent Debate, Factual Accuracy, Hallucination Mitigation, Natural Language Generation

**Relevance Score:** 9

**TL;DR:** This paper presents Debate-Augmented RAG (DRAG), a framework that enhances the factual accuracy of Retrieval-Augmented Generation by integrating Multi-Agent Debate mechanisms into both the retrieval and generation processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The authors identify the problem of retrieval errors and biases in Retrieval-Augmented Generation that lead to the compounding of hallucinations, which they term Hallucination on Hallucination.

**Method:** DRAG employs structured debates involving proponents, opponents, and judges to refine retrieval quality and ensure reliability. Additionally, it implements asymmetric information roles and adversarial debates in the generation phase.

**Key Contributions:**

	1. Introduction of the Debate-Augmented RAG (DRAG) framework
	2. Use of Multi-Agent Debate mechanisms to refine retrieval processes
	3. Enhancement of reasoning robustness during generation through adversarial debates

**Result:** Through evaluations across multiple tasks, DRAG is shown to improve retrieval reliability, reduce hallucinations, and enhance overall factual accuracy significantly compared to existing methods.

**Limitations:** 

**Conclusion:** The results suggest that integrating debate mechanisms can effectively counteract the issues of misinformation in generation processes, promoting better factual outcomes in RAG systems.

**Abstract:** Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating external knowledge, yet it introduces a critical issue: erroneous or biased retrieval can mislead generation, compounding hallucinations, a phenomenon we term Hallucination on Hallucination. To address this, we propose Debate-Augmented RAG (DRAG), a training-free framework that integrates Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages. In retrieval, DRAG employs structured debates among proponents, opponents, and judges to refine retrieval quality and ensure factual reliability. In generation, DRAG introduces asymmetric information roles and adversarial debates, enhancing reasoning robustness and mitigating factual inconsistencies. Evaluations across multiple tasks demonstrate that DRAG improves retrieval reliability, reduces RAG-induced hallucinations, and significantly enhances overall factual accuracy. Our code is available at https://github.com/Huenao/Debate-Augmented-RAG.

</details>


### [67] [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)

*Zesheng Shi, Yucheng Zhou, Jing Li*

**Main category:** cs.CL

**Keywords:** large language models, safety alignment, knowledge unlearning, jailbreak attacks, neuron sensitivity

**Relevance Score:** 8

**TL;DR:** A new safety alignment method for LLMs called Constrained Knowledge Unlearning (CKU) improves safety against jailbreak attacks by selectively unlearning harmful knowledge while preserving useful information.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the safety of large language models (LLMs) against jailbreak attacks that exploit existing vulnerabilities by unlearning harmful knowledge.

**Method:** CKU scores neurons in multilayer perceptron (MLP) layers to identify important neurons for useful knowledge and prunes the gradients of these neurons during the unlearning process to mitigate harmful content.

**Key Contributions:**

	1. Introduction of Constrained Knowledge Unlearning (CKU) as a novel strategy for LLM safety.
	2. Demonstrated superior performance in safety alignment without compromising utility.
	3. Insights into neuron knowledge sensitivity across MLP layers.

**Result:** CKU enhances model safety significantly without sacrificing overall performance, demonstrating better safety-utility trade-offs compared to existing methods.

**Limitations:** 

**Conclusion:** CKU provides a promising approach for enhancing the safety alignment of LLMs and offers insights into neuron knowledge sensitivity, aiding future safety alignment efforts.

**Abstract:** Despite significant progress in safety alignment, large language models (LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms have not fully deleted harmful knowledge in LLMs, which allows such attacks to bypass safeguards and produce harmful outputs. To address this challenge, we propose a novel safety alignment strategy, Constrained Knowledge Unlearning (CKU), which focuses on two primary objectives: knowledge localization and retention, and unlearning harmful knowledge. CKU works by scoring neurons in specific multilayer perceptron (MLP) layers to identify a subset U of neurons associated with useful knowledge. During the unlearning process, CKU prunes the gradients of neurons in U to preserve valuable knowledge while effectively mitigating harmful content. Experimental results demonstrate that CKU significantly enhances model safety without compromising overall performance, offering a superior balance between safety and utility compared to existing methods. Additionally, our analysis of neuron knowledge sensitivity across various MLP layers provides valuable insights into the mechanics of safety alignment and model knowledge editing.

</details>


### [68] [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)

*Chen Han, Wenzhen Zheng, Xijin Tang*

**Main category:** cs.CL

**Keywords:** misinformation detection, Multi-Agent Debate, Large Language Models, fact-checking, adversarial debate

**Relevance Score:** 8

**TL;DR:** The paper presents Debate-to-Detect (D2D), a Multi-Agent Debate framework for more effective misinformation detection through structured debate processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses limitations of static classification methods in misinformation detection by incorporating a more dynamic, debate-based approach.

**Method:** D2D employs a five-stage debate process (Opening Statement, Rebuttal, Free Debate, Closing Statement, Judgment) with agents assigned domain-specific profiles.

**Key Contributions:**

	1. Introduction of a novel Multi-Agent Debate framework for misinformation detection.
	2. A multi-dimensional evaluation mechanism assessing various aspects of claims.
	3. Demonstrated efficacy of the D2D approach through experiments with GPT-4o on fake news datasets.

**Result:** Experiments show that D2D significantly outperforms traditional methods in misinformation detection across multiple dimensions.

**Limitations:** 

**Conclusion:** D2D enhances decision transparency and robustness in detecting misinformation by using a multi-dimensional evaluation mechanism rather than simple binary classification.

**Abstract:** The proliferation of misinformation in digital platforms reveals the limitations of traditional detection methods, which mostly rely on static classification and fail to capture the intricate process of real-world fact-checking. Despite advancements in Large Language Models (LLMs) that enhance automated reasoning, their application to misinformation detection remains hindered by issues of logical inconsistency and superficial verification. In response, we introduce Debate-to-Detect (D2D), a novel Multi-Agent Debate (MAD) framework that reformulates misinformation detection as a structured adversarial debate. Inspired by fact-checking workflows, D2D assigns domain-specific profiles to each agent and orchestrates a five-stage debate process, including Opening Statement, Rebuttal, Free Debate, Closing Statement, and Judgment. To transcend traditional binary classification, D2D introduces a multi-dimensional evaluation mechanism that assesses each claim across five distinct dimensions: Factuality, Source Reliability, Reasoning Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets demonstrate significant improvements over baseline methods, and the case study highlight D2D's capability to iteratively refine evidence while improving decision transparency, representing a substantial advancement towards robust and interpretable misinformation detection. The code will be open-sourced in a future release.

</details>


### [69] [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)

*Jongwoo Ko, Sungnyun Kim, Sungwoo Cho, Se-Young Yun*

**Main category:** cs.CL

**Keywords:** multimodal model, human-generated reward signals, machine learning, flex-judge, textual reasoning

**Relevance Score:** 8

**TL;DR:** Flex-Judge is a multimodal judge model that uses minimal textual reasoning data to generalize across various modalities, outperforming traditional models with extensive training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a model that efficiently aligns generative models with human preferences in multimodal tasks while requiring less annotation data.

**Method:** Flex-Judge leverages structured textual reasoning explanations to guide decision-making in multimodal evaluations, allowing generalization across different formats.

**Key Contributions:**

	1. Introduction of Flex-Judge as a reasoning-guided multimodal judge model
	2. Demonstration of its competitive performance with minimal training data
	3. Potential application in resource-constrained domains like molecular evaluation

**Result:** Flex-Judge achieves competitive performance against state-of-the-art models despite being trained on significantly less text data.

**Limitations:** 

**Conclusion:** The framework suggests a shift towards reasoning-based text supervision as a cost-effective method for scalable multimodal model evaluation.

**Abstract:** Human-generated reward signals are critical for aligning generative models with human preferences, guiding both training and inference-time evaluations. While large language models (LLMs) employed as proxy evaluators, i.e., LLM-as-a-Judge, significantly reduce the costs associated with manual annotations, they typically require extensive modality-specific training data and fail to generalize well across diverse multimodal tasks. In this paper, we propose Flex-Judge, a reasoning-guided multimodal judge model that leverages minimal textual reasoning data to robustly generalize across multiple modalities and evaluation formats. Our core intuition is that structured textual reasoning explanations inherently encode generalizable decision-making patterns, enabling an effective transfer to multimodal judgments, e.g., with images or videos. Empirical results demonstrate that Flex-Judge, despite being trained on significantly fewer text data, achieves competitive or superior performance compared to state-of-the-art commercial APIs and extensively trained multimodal evaluators. Notably, Flex-Judge presents broad impact in modalities like molecule, where comprehensive evaluation benchmarks are scarce, underscoring its practical value in resource-constrained domains. Our framework highlights reasoning-based text supervision as a powerful, cost-effective alternative to traditional annotation-intensive approaches, substantially advancing scalable multimodal model-as-a-judge.

</details>


### [70] [RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations](https://arxiv.org/abs/2505.18609)

*Ashwin Sankar, Yoach Lacombe, Sherry Thomas, Praveen Srinivasa Varadhan, Sanchit Gandhi, Mitesh M Khapra*

**Main category:** cs.CL

**Keywords:** speech dataset, text-to-speech synthesis, Indian languages

**Relevance Score:** 7

**TL;DR:** RASMALAI introduces a large-scale speech dataset for TTS synthesis in 24 languages, leading to the development of IndicParlerTTS, the first open-source TTS for Indian languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate advancements in controllable and expressive text-to-speech synthesis for Indian languages and English.

**Method:** RASMALAI is created with 13,000 hours of speech and 24 million annotated text descriptions covering various attributes. IndicParlerTTS is developed to utilize this dataset for TTS synthesis.

**Key Contributions:**

	1. Introduction of the RASMALAI dataset for speech synthesis
	2. Development of IndicParlerTTS, an open-source TTS system
	3. High performance in multilingual expressive speech generation

**Result:** IndicParlerTTS demonstrates high-quality speech generation, accurately follows text descriptions, and effectively synthesizes specified attributes, achieving strong performance across evaluations.

**Limitations:** 

**Conclusion:** The research sets a new standard for controllable multilingual expressive speech synthesis in Indian languages, addressing limitations in existing systems.

**Abstract:** We introduce RASMALAI, a large-scale speech dataset with rich text descriptions, designed to advance controllable and expressive text-to-speech (TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours of speech and 24 million text-description annotations with fine-grained attributes like speaker identity, accent, emotion, style, and background conditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source, text-description-guided TTS for Indian languages. Systematic evaluation demonstrates its ability to generate high-quality speech for named speakers, reliably follow text descriptions and accurately synthesize specified attributes. Additionally, it effectively transfers expressive characteristics both within and across languages. IndicParlerTTS consistently achieves strong performance across these evaluations, setting a new standard for controllable multilingual expressive speech synthesis in Indian languages.

</details>


### [71] [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610)

*Tengxuan Liu, Shiyao Li, Jiayi Yang, Tianchen Zhao, Feng Zhou, Xiaohui Song, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought reasoning, KV Cache quantization

**Relevance Score:** 9

**TL;DR:** This paper proposes Progressive Mixed-Precision KV Cache Quantization (PM-KVQ) to optimize memory overhead in long-CoT reasoning-capable LLMs by reducing cumulative errors and improving calibration strategies, achieving significant performance improvements over SOTA methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The memory overhead from long Chain-of-Thought reasoning in Large Language Models leads to performance degradation and inefficiencies that need addressing.

**Method:** The paper introduces PM-KVQ, which uses a progressive quantization strategy to lower the bit-width of KV Cache incrementally and a novel calibration method with positional interpolation to enhance the calibration length without increasing overhead.

**Key Contributions:**

	1. Progressive quantization strategy for KV Cache
	2. Block-wise memory allocation based on sensitivity of transformer blocks
	3. New calibration strategy with positional interpolation to enhance data distribution approximation.

**Result:** Experimental results show PM-KVQ improves reasoning benchmark performance by up to 8% over state-of-the-art baselines while maintaining the same memory budget.

**Limitations:** 

**Conclusion:** PM-KVQ effectively addresses memory-related challenges in long-CoT LLMs, achieving better performance without incurring additional memory costs.

**Abstract:** Recently, significant progress has been made in developing reasoning-capable Large Language Models (LLMs) through long Chain-of-Thought (CoT) techniques. However, this long-CoT reasoning process imposes substantial memory overhead due to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache quantization has emerged as a promising compression technique and has been extensively studied in short-context scenarios. However, directly applying existing methods to long-CoT LLMs causes significant performance degradation due to the following two reasons: (1) Large cumulative error: Existing methods fail to adequately leverage available memory, and they directly quantize the KV Cache during each decoding step, leading to large cumulative quantization error. (2) Short-context calibration: Due to Rotary Positional Embedding (RoPE), the use of short-context data during calibration fails to account for the distribution of less frequent channels in the Key Cache, resulting in performance loss. We propose Progressive Mixed-Precision KV Cache Quantization (PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To reduce cumulative error, we design a progressive quantization strategy to gradually lower the bit-width of KV Cache in each block. Then, we propose block-wise memory allocation to assign a higher bit-width to more sensitive transformer blocks. (2) To increase the calibration length without additional overhead, we propose a new calibration strategy with positional interpolation that leverages short calibration data with positional interpolation to approximate the data distribution of long-context data. Extensive experiments on 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark performance by up to 8% over SOTA baselines under the same memory budget. Our code is available at https://github.com/thu-nics/PM-KVQ.

</details>


### [72] [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)

*Woohyun Cho, Youngmin Kim, Sunghyun Lee, Youngjae Yu*

**Main category:** cs.CL

**Keywords:** lyrics translation, multilingual, multimodal, animated musicals, audio-video cues

**Relevance Score:** 4

**TL;DR:** This paper introduces MAVL, the first multilingual, multimodal benchmark for singable lyrics translation for animated musicals and presents a new model, SylAVL-CoT, that significantly improves translation quality by leveraging audio-video cues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for lyrics translation often neglect the importance of musical rhythm, syllabic structure, and alignment with multimedia cues in animated musicals, highlighting the need for a dedicated benchmark and improved translation methodologies.

**Method:** The authors present the Multilingual Audio-Video Lyrics Benchmark (MAVL), incorporating text, audio, and video elements, and propose the Sylabble-Constrained Audio-Video LLM with Chain-of-Thought (SylAVL-CoT) that integrates these multimodal cues while enforcing syllabic constraints in the translation process.

**Key Contributions:**

	1. Introduction of the MAVL benchmark for animated song translation
	2. Development of the SylAVL-CoT model for translating lyrics with syllabic constraints
	3. Demonstration of improved performance in singability and contextual accuracy using a multimodal approach

**Result:** SylAVL-CoT outperforms traditional text-based models in terms of singability and contextual accuracy, demonstrating the effectiveness of multimodal translation approaches.

**Limitations:** 

**Conclusion:** The study concludes that leveraging audio-video cues in conjunction with syllabic constraints significantly enhances the quality of lyrics translation in animated musicals, urging a shift towards multimodal methodologies.

**Abstract:** Lyrics translation requires both accurate semantic transfer and preservation of musical rhythm, syllabic structure, and poetic style. In animated musicals, the challenge intensifies due to alignment with visual and auditory cues. We introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song Translation (MAVL), the first multilingual, multimodal benchmark for singable lyrics translation. By integrating text, audio, and video, MAVL enables richer and more expressive translations than text-only approaches. Building on this, we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints to produce natural-sounding lyrics. Experimental results demonstrate that SylAVL-CoT significantly outperforms text-based models in singability and contextual accuracy, emphasizing the value of multimodal, multilingual approaches for lyrics translation.

</details>


### [73] [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)

*Zhihao Jia, Mingyi Jia, Junwen Duan, Jianxin Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical consultation, dual-decision optimization, multi-agent workflow, disease diagnosis

**Relevance Score:** 9

**TL;DR:** The paper introduces DDO, a novel LLM-based framework for medical consultations that improves decision-making by decoupling symptom inquiry and disease diagnosis into separate tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM-based methods struggle with the dual nature of medical consultations, leading to ineffective symptom inquiry and diagnosis.

**Method:** DDO employs a collaborative multi-agent workflow to independently optimize the two sub-tasks of medical consultations: symptom inquiry and disease diagnosis.

**Key Contributions:**

	1. Proposes a dual-decision optimization framework for medical consultations.
	2. Decouples the symptom inquiry and disease diagnosis tasks for independent optimization.
	3. Demonstrates superior performance through experiments on real-world datasets.

**Result:** DDO shows consistent improvement over existing LLM methodologies and competes well against state-of-the-art generation-based methods across three medical consultation datasets.

**Limitations:** 

**Conclusion:** The DDO framework effectively enhances the performance of medical consultations by optimizing decision-making processes in LLMs.

**Abstract:** Large Language Models (LLMs) demonstrate strong generalization and reasoning abilities, making them well-suited for complex decision-making tasks such as medical consultation (MC). However, existing LLM-based methods often fail to capture the dual nature of MC, which entails two distinct sub-tasks: symptom inquiry, a sequential decision-making process, and disease diagnosis, a classification problem. This mismatch often results in ineffective symptom inquiry and unreliable disease diagnosis. To address this, we propose \textbf{DDO}, a novel LLM-based framework that performs \textbf{D}ual-\textbf{D}ecision \textbf{O}ptimization by decoupling and independently optimizing the the two sub-tasks through a collaborative multi-agent workflow. Experiments on three real-world MC datasets show that DDO consistently outperforms existing LLM-based approaches and achieves competitive performance with state-of-the-art generation-based methods, demonstrating its effectiveness in the MC task.

</details>


### [74] [Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models](https://arxiv.org/abs/2505.18638)

*Md. Tanzib Hosain, Rajan Das Gupta, Md. Kishor Morol*

**Main category:** cs.CL

**Keywords:** Dzongkha, Large Language Models, Low-resource languages, Dataset, Prompting strategies

**Relevance Score:** 7

**TL;DR:** This paper presents DZEN, a dataset of parallel Dzongkha and English test questions for Bhutanese students, exploring the performance of LLMs on these questions.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding and performance of LLMs in low-resource languages, particularly Dzongkha, through evaluation with a dedicated dataset.

**Method:** The study introduces DZEN, a dataset with over 5,000 parallel questions in Dzongkha and English, and evaluates various LLMs using different prompting strategies.

**Key Contributions:**

	1. Introduction of the DZEN dataset for Dzongkha and English test questions.
	2. Evaluation of LLMs' performance on this dataset, revealing discrepancies between languages.
	3. Insights on the effectiveness of Chain-of-Thought prompting and the impact of English translations.

**Result:** LLMs showed significant performance differences between English and Dzongkha, with Chain-of-Thought prompting proving effective for reasoning questions, and the addition of English translations improving response precision.

**Limitations:** The dataset is limited to middle and high school level questions and may not cover all aspects of Dzongkha language use.

**Conclusion:** The findings suggest that enhancements can be made to LLM performance in low-resource languages like Dzongkha, encouraging further research in this area.

**Abstract:** In this work, we provide DZEN, a dataset of parallel Dzongkha and English test questions for Bhutanese middle and high school students. The over 5K questions in our collection span a variety of scientific topics and include factual, application, and reasoning-based questions. We use our parallel dataset to test a number of Large Language Models (LLMs) and find a significant performance difference between the models in English and Dzongkha. We also look at different prompting strategies and discover that Chain-of-Thought (CoT) prompting works well for reasoning questions but less well for factual ones. We also find that adding English translations enhances the precision of Dzongkha question responses. Our results point to exciting avenues for further study to improve LLM performance in Dzongkha and, more generally, in low-resource languages. We release the dataset at: https://github.com/kraritt/llm_dzongkha_evaluation.

</details>


### [75] [Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster](https://arxiv.org/abs/2505.18642)

*Xiao Chen, Sihang Zhou, Ke Liang, Xiaoyu Sun, Xinwang Liu*

**Main category:** cs.CL

**Keywords:** chain-of-thought, distillation, small language models, reasoning tasks, chunk-wise training

**Relevance Score:** 8

**TL;DR:** This paper presents a novel chunk-wise training method for improving the efficiency and accuracy of small language models during reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods suffer from long rationales that hinder small language models' learning due to over-smoothing of gradients and slow response times.

**Method:** The authors propose chunk-wise training (CWT), which divides rationales into semantically coherent chunks, allowing the small language model to focus on one chunk per iteration. They also introduce skip-thinking training (STT) to skip non-reasoning chunks for faster responses.

**Key Contributions:**

	1. Introduction of chunk-wise training to isolate reasoning chunks
	2. Development of skip-thinking training to accelerate response times
	3. Validation on multiple small language models and reasoning tasks

**Result:** The proposed methods improve the reasoning speed of small language models while maintaining or enhancing their accuracy across various reasoning tasks.

**Limitations:** 

**Conclusion:** Chunk-wise training and skip-thinking training effectively enable small language models to learn reasoning more efficiently and perform better in reasoning tasks.

**Abstract:** Chain-of-thought (CoT) distillation allows a large language model (LLM) to guide a small language model (SLM) in reasoning tasks. Existing methods train the SLM to learn the long rationale in one iteration, resulting in two issues: 1) Long rationales lead to a large token-level batch size during training, making gradients of core reasoning tokens (i.e., the token will directly affect the correctness of subsequent reasoning) over-smoothed as they contribute a tiny fraction of the rationale. As a result, the SLM converges to sharp minima where it fails to grasp the reasoning logic. 2) The response is slow, as the SLM must generate a long rationale before reaching the answer. Therefore, we propose chunk-wise training (CWT), which uses a heuristic search to divide the rationale into internal semantically coherent chunks and focuses SLM on learning from only one chunk per iteration. In this way, CWT naturally isolates non-reasoning chunks that do not involve the core reasoning token (e.g., summary and transitional chunks) from the SLM learning for reasoning chunks, making the fraction of the core reasoning token increase in the corresponding iteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes the SLM automatically skip non-reasoning medium chunks to reach the answer, improving reasoning speed while maintaining accuracy. We validate our approach on a variety of SLMs and multiple reasoning tasks.

</details>


### [76] [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)

*Daniel J. Korchinski, Dhruva Karkada, Yasaman Bahri, Matthieu Wyart*

**Main category:** cs.CL

**Keywords:** word embeddings, linear analogy, generative model, co-occurrence probabilities, semantic attributes

**Relevance Score:** 6

**TL;DR:** This paper introduces a generative model to explain the linear analogy structure observed in word embeddings like Word2Vec and GloVe, linking it to binary semantic attributes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the theoretical origins of the linear analogy structure seen in word embeddings and to understand how additional embedding dimensions influence this structure.

**Method:** A theoretical generative model is proposed where words are represented by binary semantic attributes, and word co-occurrence probabilities are derived from these attributes.

**Key Contributions:**

	1. Introduction of a generative model for word embeddings
	2. Analytical reproduction of the linear analogy structure
	3. Insights into the influence of embedding dimensions on analogy performance

**Result:** The model reproduces the linear analogy structure and explains observed phenomena related to eigenvectors and co-occurrence statistics.

**Limitations:** 

**Conclusion:** The generative model provides insights into the role of embedding dimensions and demonstrates robustness against noise, aligning well with empirical co-occurrence data.

**Abstract:** Models such as Word2Vec and GloVe construct word embeddings based on the co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The resulting vectors $W_i$ not only group semantically similar words but also exhibit a striking linear analogy structure -- for example, $W_{\text{king}} - W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose theoretical origin remains unclear. Previous observations indicate that this analogy structure: (i) already emerges in the top eigenvectors of the matrix $M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and (iv) persists even when all word pairs involved in a specific analogy relation (e.g., king-queen, man-woman) are removed from the corpus. To explain these phenomena, we introduce a theoretical generative model in which words are defined by binary semantic attributes, and co-occurrence probabilities are derived from attribute-based interactions. This model analytically reproduces the emergence of linear analogy structure and naturally accounts for properties (i)-(iv). It can be viewed as giving fine-grained resolution into the role of each additional embedding dimension. It is robust to various forms of noise and agrees well with co-occurrence statistics measured on Wikipedia and the analogy benchmark introduced by Mikolov et al.

</details>


### [77] [Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change](https://arxiv.org/abs/2505.18653)

*Murathan Kurfalı, Shorouq Zahra, Joakim Nivre, Gabriele Messori*

**Main category:** cs.CL

**Keywords:** climate change, natural language processing, benchmark, large language models, evaluation

**Relevance Score:** 5

**TL;DR:** Climate-Eval is a benchmark for evaluating NLP models on climate change tasks, featuring 25 tasks and 13 datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of NLP models, particularly large language models, in understanding and applying knowledge related to climate change.

**Method:** The benchmark aggregates existing datasets and introduces a new news classification dataset, covering various NLP tasks such as text classification, question answering, and information extraction.

**Key Contributions:**

	1. Development of a comprehensive benchmark for climate change NLP tasks
	2. Introduction of a new news classification dataset
	3. Standardized evaluation suite for large language models

**Result:** An extensive evaluation of open-source LLMs (2B to 70B parameters) in zero-shot and few-shot settings, highlighting their performance on climate-related tasks.

**Limitations:** 

**Conclusion:** The standardized evaluation suite allows for systematic assessment, revealing strengths and limitations of LLMs in climate discourse.

**Abstract:** Climate-Eval is a comprehensive benchmark designed to evaluate natural language processing models across a broad range of tasks related to climate change. Climate-Eval aggregates existing datasets along with a newly developed news classification dataset, created specifically for this release. This results in a benchmark of 25 tasks based on 13 datasets, covering key aspects of climate discourse, including text classification, question answering, and information extraction. Our benchmark provides a standardized evaluation suite for systematically assessing the performance of large language models (LLMs) on these tasks. Additionally, we conduct an extensive evaluation of open-source LLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot settings, analyzing their strengths and limitations in the domain of climate change.

</details>


### [78] [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)

*Pankaj Kumar, Subhankar Mishra*

**Main category:** cs.CL

**Keywords:** Large Language Models, robustness, natural language processing, AI, mitigation strategies

**Relevance Score:** 9

**TL;DR:** A survey on the robustness of Large Language Models (LLMs), discussing challenges, sources of non-robustness, mitigation strategies, and future research pathways.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address critical challenges in ensuring the robustness of LLMs for reliable applications in natural language processing and artificial intelligence.

**Method:** Comprehensive review of current studies, categorizing sources of non-robustness, analyzing mitigation strategies and benchmarks, and synthesizing findings from surveys.

**Key Contributions:**

	1. Systematic examination of robustness in LLMs.
	2. Categorization of sources of non-robustness.
	3. Review of mitigation strategies and benchmarks.

**Result:** Identifies intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors affecting LLM reliability, along with effective mitigation strategies and emerging assessment metrics.

**Limitations:** The survey may not cover all emerging models or latest challenges in real-time applications.

**Conclusion:** A roadmap for addressing robustness in LLMs, highlighting trends, unresolved issues, and future research directions in the field.

**Abstract:** Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.

</details>


### [79] [Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models](https://arxiv.org/abs/2505.18673)

*Zixiang Xu, Yanbo Wang, Yue Huang, Xiuying Chen, Jieyu Zhao, Meng Jiang, Xiangliang Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, cross-lingual performance, bilingual question pairs, NLP, linguistic similarity

**Relevance Score:** 9

**TL;DR:** This paper presents a new methodology to identify cross-lingual weaknesses in Large Language Models (LLMs) using bilingual question pairs, demonstrating significant performance discrepancies between English and other languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of inconsistent cross-lingual performance in LLMs, which affects their applications in multilingual contexts.

**Method:** The methodology uses beam search and LLM-based simulation to create bilingual question pairs for analyzing performance across languages.

**Key Contributions:**

	1. Introduction of a novel methodology for identifying cross-lingual weaknesses in LLMs.
	2. Creation of a new dataset with over 6,000 bilingual question pairs across 16 languages.
	3. Demonstration of the relationship between linguistic similarity and model performance discrepancies.

**Result:** The experiments reveal over 50% accuracy drops in target languages among various state-of-the-art models, showcasing the effectiveness of the proposed approach.

**Limitations:** The method may still be limited by the availability of quality bilingual datasets and the need for extensive model evaluations.

**Conclusion:** The proposed method successfully identifies cross-lingual weaknesses, leading to insights about the relationship between linguistic similarity and model performance, suggesting opportunities for targeted post-training.

**Abstract:** Large Language Models (LLMs) have achieved remarkable success in Natural Language Processing (NLP), yet their cross-lingual performance consistency remains a significant challenge. This paper introduces a novel methodology for efficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach leverages beam search and LLM-based simulation to generate bilingual question pairs that expose performance discrepancies between English and target languages. We construct a new dataset of over 6,000 bilingual pairs across 16 languages using this methodology, demonstrating its effectiveness in revealing weaknesses even in state-of-the-art models. The extensive experiments demonstrate that our method precisely and cost-effectively pinpoints cross-lingual weaknesses, consistently revealing over 50\% accuracy drops in target languages across a wide range of models. Moreover, further experiments investigate the relationship between linguistic similarity and cross-lingual weaknesses, revealing that linguistically related languages share similar performance patterns and benefit from targeted post-training. Code is available at https://github.com/xzx34/Cross-Lingual-Pitfalls.

</details>


### [80] [Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts](https://arxiv.org/abs/2505.18677)

*Eric Chamoun, Nedjma Ousidhoum, Michael Schlichtkrull, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** NLP, automated analysis, research framing, fact-checking, hate speech detection

**Relevance Score:** 7

**TL;DR:** This paper proposes an automated system for analyzing NLP research framings, focusing on means, ends, and stakeholders to align research with practical applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance alignment between NLP research and its practical applications by clarifying research framings.

**Method:** Develop a three-component system that extracts key elements from NLP research and applies contextual reasoning to link them.

**Key Contributions:**

	1. Automation of analysis for NLP research framings
	2. Identification of key elements and their relationships
	3. Insights into current trends within automated fact-checking research

**Result:** Achieved consistent improvements over strong LLM baselines in two domains: automated fact-checking and hate speech detection.

**Limitations:** 

**Conclusion:** The system revealed trends in recent automated fact-checking research, including vague research goals and a shift toward supporting human fact-checkers.

**Abstract:** Clarifying the research framing of NLP artefacts (e.g., models, datasets, etc.) is crucial to aligning research with practical applications. Recent studies manually analyzed NLP research across domains, showing that few papers explicitly identify key stakeholders, intended uses, or appropriate contexts. In this work, we propose to automate this analysis, developing a three-component system that infers research framings by first extracting key elements (means, ends, stakeholders), then linking them through interpretable rules and contextual reasoning. We evaluate our approach on two domains: automated fact-checking using an existing dataset, and hate speech detection for which we annotate a new dataset-achieving consistent improvements over strong LLM baselines. Finally, we apply our system to recent automated fact-checking papers and uncover three notable trends: a rise in vague or underspecified research goals, increased emphasis on scientific exploration over application, and a shift toward supporting human fact-checkers rather than pursuing full automation.

</details>


### [81] [TULUN: Transparent and Adaptable Low-resource Machine Translation](https://arxiv.org/abs/2505.18683)

*Raphaël Merx, Hanna Suominen, Lois Hong, Nick Thieberger, Trevor Cohn, Ekaterina Vylomova*

**Main category:** cs.CL

**Keywords:** machine translation, low-resource languages, large language models, terminology-aware translation, collaborative translation

**Relevance Score:** 8

**TL;DR:** Tulun is a web-based platform that combines neural machine translation with LLM-guided post-editing for low-resource languages, enabling user-friendly, terminology-aware translation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing machine translation systems for low-resource languages struggle with specialized domains and require technical expertise for domain adaptation techniques, making access difficult for non-technical users.

**Method:** Tulun integrates neural machine translation with large language model-based post-editing, utilizing existing glossaries and translation memories, to facilitate a collaborative human-machine translation process.

**Key Contributions:**

	1. Combines neural MT with LLM-based post-editing.
	2. User-friendly platform for creating and managing translation resources.
	3. Demonstrated effectiveness in real-world applications and benchmark datasets.

**Result:** Tulun demonstrates significant improvements in translation accuracy for medical and disaster relief tasks, achieving 16.90-22.41 ChrF++ point increases for Tetun and Bislama, and an average improvement of 2.8 ChrF points across six low-resource languages on the FLORES dataset.

**Limitations:** 

**Conclusion:** Tulun offers a practical solution for improving machine translation outcomes in specialized domains by empowering users to effectively employ terminology resources without needing technical expertise.

**Abstract:** Machine translation (MT) systems that support low-resource languages often struggle on specialized domains. While researchers have proposed various techniques for domain adaptation, these approaches typically require model fine-tuning, making them impractical for non-technical users and small organizations. To address this gap, we propose Tulun, a versatile solution for terminology-aware translation, combining neural MT with large language model (LLM)-based post-editing guided by existing glossaries and translation memories. Our open-source web-based platform enables users to easily create, edit, and leverage terminology resources, fostering a collaborative human-machine translation process that respects and incorporates domain expertise while increasing MT accuracy. Evaluations show effectiveness in both real-world and benchmark scenarios: on medical and disaster relief translation tasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41 ChrF++ points over baseline MT systems. Across six low-resource languages on the FLORES dataset, Tulun outperforms both standalone MT and LLM approaches, achieving an average improvement of 2.8 ChrF points over NLLB-54B.

</details>


### [82] [From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation](https://arxiv.org/abs/2505.18685)

*Zhihao Zhang, Yiran Zhang, Xiyue Zhou, Liting Huang, Imran Razzak, Preslav Nakov, Usman Naseem*

**Main category:** cs.CL

**Keywords:** health misinformation, multimodal dataset, generative AI, AI detection, reliability checks

**Relevance Score:** 9

**TL;DR:** MM Health is a large-scale multimodal misinformation dataset in the health domain aimed at combating the spread of health misinformation exacerbated by generative AI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To combat the global issue of health misinformation exacerbated by AI, existing datasets lacked coverage, diverse content, and accessibility.

**Method:** The dataset consists of 34,746 news articles, including both human-generated and AI-generated multimodal content, and is benchmarked against tasks like reliability and originality checks.

**Key Contributions:**

	1. A large-scale dataset (34,746 articles) addressing health misinformation.
	2. Inclusion of both human- and AI-generated multimodal information.
	3. Benchmarking against reliability checks and fine-grained AI detection tasks.

**Result:** Benchmarking demonstrates that current state-of-the-art models struggle to accurately identify the source and reliability of health-related misinformation.

**Limitations:** 

**Conclusion:** MM Health serves as a crucial resource for enhancing misinformation detection capabilities in the health sector, supporting the distinction between human and machine-generated content.

**Abstract:** Infodemics and health misinformation have significant negative impact on individuals and society, exacerbating confusion and increasing hesitancy in adopting recommended health measures. Recent advancements in generative AI, capable of producing realistic, human like text and images, have significantly accelerated the spread and expanded the reach of health misinformation, resulting in an alarming surge in its dissemination. To combat the infodemics, most existing work has focused on developing misinformation datasets from social media and fact checking platforms, but has faced limitations in topical coverage, inclusion of AI generation, and accessibility of raw content. To address these issues, we present MM Health, a large scale multimodal misinformation dataset in the health domain consisting of 34,746 news article encompassing both textual and visual information. MM Health includes human-generated multimodal information (5,776 articles) and AI generated multimodal information (28,880 articles) from various SOTA generative AI models. Additionally, We benchmarked our dataset against three tasks (reliability checks, originality checks, and fine-grained AI detection) demonstrating that existing SOTA models struggle to accurately distinguish the reliability and origin of information. Our dataset aims to support the development of misinformation detection across various health scenarios, facilitating the detection of human and machine generated content at multimodal levels.

</details>


### [83] [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)

*Aleksandr Tsymbalov*

**Main category:** cs.CL

**Keywords:** Large Language Models, text classification, incremental learning, model drift, data collection

**Relevance Score:** 9

**TL;DR:** This paper explores the use of Large Language Models (LLMs) as a replacement for human annotators in text classification tasks to maintain model quality and address data drift.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for human annotators in text classification is labor-intensive and costly, especially when retraining models due to data drift; the paper aims to alleviate this burden using LLMs.

**Method:** The study proposes multiple approaches where LLMs are utilized to test and validate classifier predictions, thus reducing reliance on human specialists.

**Key Contributions:**

	1. Proposed methods for utilizing LLMs in validating text classification predictions.
	2. Demonstrated potential cost reduction in model retraining processes.
	3. Provided a framework for incorporating LLMs in incremental learning pipelines.

**Result:** Using LLMs for validation can effectively support incremental learning and model quality assurance, potentially making the process more efficient and less costly.

**Limitations:** The effectiveness of LLMs as annotators may vary with the complexity of the classification task and the specifics of the model being trained.

**Conclusion:** LLMs can play a significant role in minimizing the labor involved in annotating datasets for text classification, particularly in dynamic contexts where ongoing data drift occurs.

**Abstract:** Machine learning models for text classification are trained to predict a class for a given text. To do this, training and validation samples must be prepared: a set of texts is collected, and each text is assigned a class. These classes are usually assigned by human annotators with different expertise levels, depending on the specific classification task. Collecting such samples from scratch is labor-intensive because it requires finding specialists and compensating them for their work; moreover, the number of available specialists is limited, and their productivity is constrained by human factors. While it may not be too resource-intensive to collect samples once, the ongoing need to retrain models (especially in incremental learning pipelines) to address data drift (also called model drift) makes the data collection process crucial and costly over the model's entire lifecycle. This paper proposes several approaches to replace human annotators with Large Language Models (LLMs) to test classifier predictions for correctness, helping ensure model quality and support high-quality incremental learning.

</details>


### [84] [Benchmarking and Rethinking Knowledge Editing for Large Language Models](https://arxiv.org/abs/2505.18690)

*Guoxiu He, Xin Song, Futing Wang, Aixin Sun*

**Main category:** cs.CL

**Keywords:** knowledge editing, Large Language Models, benchmarking, Selective Contextual Reasoning, context-based reasoning

**Relevance Score:** 8

**TL;DR:** A benchmarking study on knowledge editing in LLMs reveals that common parameter-based methods perform poorly compared to a simple baseline approach, Selective Contextual Reasoning (SCR).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address inconsistent evaluation objectives and experimental setups in existing knowledge editing methods for Large Language Models (LLMs).

**Method:** Conducted a benchmarking study using fact-level and event-based datasets, evaluating both instruction-tuned and reasoning-oriented LLMs under realistic autoregressive inference settings, and employing multi-edit assessments.

**Key Contributions:**

	1. Introduction of complex event-based datasets for evaluation
	2. Evaluation of multi-edit scenarios to reflect practical use
	3. Demonstration of the superiority of Selective Contextual Reasoning (SCR) over parameter-based methods.

**Result:** Parameter-based editing methods consistently underperformed, while the Selective Contextual Reasoning (SCR) baseline outperformed them across all settings.

**Limitations:** 

**Conclusion:** This study highlights the limitations of current knowledge editing approaches and suggests that context-based reasoning could be a more effective alternative.

**Abstract:** Knowledge editing aims to update the embedded knowledge within Large Language Models (LLMs). However, existing approaches, whether through parameter modification or external memory integration, often suffer from inconsistent evaluation objectives and experimental setups. To address this gap, we conduct a comprehensive benchmarking study. In addition to fact-level datasets, we introduce more complex event-based datasets and general-purpose datasets drawn from other tasks. Our evaluation covers both instruction-tuned and reasoning-oriented LLMs, under a realistic autoregressive inference setting rather than teacher-forced decoding. Beyond single-edit assessments, we also evaluate multi-edit scenarios to better reflect practical demands. We employ four evaluation dimensions, including portability, and compare all recent methods against a simple and straightforward baseline named Selective Contextual Reasoning (SCR). Empirical results reveal that parameter-based editing methods perform poorly under realistic conditions. In contrast, SCR consistently outperforms them across all settings. This study offers new insights into the limitations of current knowledge editing methods and highlights the potential of context-based reasoning as a more robust alternative.

</details>


### [85] [Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task](https://arxiv.org/abs/2505.18703)

*Gaurav Negi, Dhairya Dalal, Omnia Zayed, Paul Buitelaar*

**Main category:** cs.CL

**Keywords:** opinion extraction, semantic representation, natural language processing

**Relevance Score:** 5

**TL;DR:** The paper presents the Unified Opinion Concepts (UOC) ontology for semantic representation of opinions and introduces a new task, Unified Opinion Concept Extraction (UOCE), along with a dataset and metrics for evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To integrate opinions within their semantic context and enhance the expressivity of opinion extraction from text.

**Method:** Development of the UOC ontology and the UOCE task, along with a new manually extended evaluation dataset and tailored evaluation metrics.

**Key Contributions:**

	1. Introduction of the Unified Opinion Concepts (UOC) ontology
	2. Definition of the Unified Opinion Concept Extraction (UOCE) task
	3. Creation of a new evaluation dataset and tailored metrics for opinion extraction

**Result:** Establishment of baseline performance for the UOCE task using state-of-the-art generative models, validating the effectiveness of the UOC ontology.

**Limitations:** 

**Conclusion:** The UOC ontology and UOCE task provide a robust framework for enhanced opinion extraction and evaluation in NLP.

**Abstract:** This paper introduces the Unified Opinion Concepts (UOC) ontology to integrate opinions within their semantic context. The UOC ontology bridges the gap between the semantic representation of opinion across different formulations. It is a unified conceptualisation based on the facets of opinions studied extensively in NLP and semantic structures described through symbolic descriptions. We further propose the Unified Opinion Concept Extraction (UOCE) task of extracting opinions from the text with enhanced expressivity. Additionally, we provide a manually extended and re-annotated evaluation dataset for this task and tailored evaluation metrics to assess the adherence of extracted opinions to UOC semantics. Finally, we establish baseline performance for the UOCE task using state-of-the-art generative models.

</details>


### [86] [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)

*Xu Zhang, Kun Zhang, Wenxin Ma, Rongsheng Wang, Chenxu Wu, Yingtai Li, S. Kevin Zhou*

**Main category:** cs.CL

**Keywords:** ICD Coding, Knowledge Injection, Machine Learning

**Relevance Score:** 7

**TL;DR:** The paper presents GKI-ICD, a novel framework for ICD coding that integrates multiple knowledge types to enhance coding performance without complex modular designs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenges in ICD coding due to long-tail distribution and insufficient annotations by leveraging diverse knowledge sources.

**Method:** GKI-ICD integrates three types of knowledge—ICD Description, ICD Synonym, and ICD Hierarchy—into a comprehensive framework that avoids complex module design.

**Key Contributions:**

	1. Introduction of GKI-ICD framework for ICD coding.
	2. Integration of multiple types of knowledge without specialized modules.
	3. Demonstration of state-of-the-art performance on ICD benchmarks.

**Result:** Extensive experiments show that GKI-ICD achieves state-of-the-art performance on popular ICD coding benchmarks across most evaluation metrics.

**Limitations:** 

**Conclusion:** The proposed method effectively enhances ICD coding performance by leveraging diverse knowledge while maintaining scalability and compatibility.

**Abstract:** ICD Coding aims to assign a wide range of medical codes to a medical text document, which is a popular and challenging task in the healthcare domain. To alleviate the problems of long-tail distribution and the lack of annotations of code-specific evidence, many previous works have proposed incorporating code knowledge to improve coding performance. However, existing methods often focus on a single type of knowledge and design specialized modules that are complex and incompatible with each other, thereby limiting their scalability and effectiveness. To address this issue, we propose GKI-ICD, a novel, general knowledge injection framework that integrates three key types of knowledge, namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized design of additional modules. The comprehensive utilization of the above knowledge, which exhibits both differences and complementarity, can effectively enhance the ICD coding performance. Extensive experiments on existing popular ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves the state-of-the-art performance on most evaluation metrics. Code is available at https://github.com/xuzhang0112/GKI-ICD.

</details>


### [87] [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)

*Sourav Kumar Das, Md. Julkar Naeen, MD. Jahidul Islam, Md. Anisul Haque Sajeeb, Narayan Ranjan Chakraborty, Mayen Uddin Mojumdar*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Bangla language, Sylheti language, LSTM, translation

**Relevance Score:** 4

**TL;DR:** This paper presents a system for translating Modern Bangla to Sylheti Bangla using NLP techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of research on local Bangla languages and aims to enhance the understanding of the Sylheti language through NLP.

**Method:** Three models (LSTM, Bi-LSTM, Seq2Seq) were trained on a dataset of 1200 samples to perform translations from Modern Bangla to Sylheti.

**Key Contributions:**

	1. Developed an NLP system for translating Modern Bangla to Sylheti Bangla.
	2. Achieved high accuracy with the LSTM model.
	3. Filled a gap in research regarding local Bangla languages.

**Result:** The LSTM model achieved the best performance with an accuracy of 89.3%.

**Limitations:** 

**Conclusion:** The research contributes valuable insights that could promote further advancements in Bangla NLP.

**Abstract:** Bangla or Bengali is the national language of Bangladesh, people from different regions don't talk in proper Bangla. Every division of Bangladesh has its own local language like Sylheti, Chittagong etc. In recent years some papers were published on Bangla language like sentiment analysis, fake news detection and classifications, but a few of them were on Bangla languages. This research is for the local language and this particular paper is on Sylheti language. It presented a comprehensive system using Natural Language Processing or NLP techniques for translating Pure or Modern Bangla to locally spoken Sylheti Bangla language. Total 1200 data used for training 3 models LSTM, Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3% accuracy. The findings of this research may contribute to the growth of Bangla NLP researchers for future more advanced innovations.

</details>


### [88] [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)

*Meng Li, Guangda Huzhang, Haibo Zhang, Xiting Wang, Anxiang Zeng*

**Main category:** cs.CL

**Keywords:** Direct Preference Optimization, Large Language Models, Token weighting, Human preferences, Instruction-following

**Relevance Score:** 9

**TL;DR:** This paper presents Optimal Transport-based token weighting for Direct Preference Optimization in LLMs, enhancing reward stability and instruction-following through context-aware token importance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Direct Preference Optimization in Large Language Models by addressing the issue of equal token weighting leading to suboptimal alignment with human preferences.

**Method:** A context-aware token weighting scheme that emphasizes semantically meaningful token pairs while de-emphasizing less relevant tokens for preference optimization.

**Key Contributions:**

	1. Introduction of a context-aware token weighting scheme for DPO
	2. Improved performance of LLMs in instruction-following tasks
	3. Enhanced interpretability and reward stability in preference optimization

**Result:** The proposed OTPO method was validated through extensive experiments, demonstrating improved instruction-following ability in LLMs.

**Limitations:** 

**Conclusion:** OTPO enhances the effectiveness of Direct Preference Optimization by focusing on meaningful differences, leading to more stable and interpretable rewards.

**Abstract:** Direct Preference Optimization (DPO) has emerged as a promising framework for aligning Large Language Models (LLMs) with human preferences by directly optimizing the log-likelihood difference between chosen and rejected responses. However, existing methods assign equal importance to all tokens in the response, while humans focus on more meaningful parts. This leads to suboptimal preference optimization, as irrelevant or noisy tokens disproportionately influence DPO loss. To address this limitation, we propose \textbf{O}ptimal \textbf{T}ransport-based token weighting scheme for enhancing direct \textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically meaningful token pairs and de-emphasizing less relevant ones, our method introduces a context-aware token weighting scheme that yields a more contrastive reward difference estimate. This adaptive weighting enhances reward stability, improves interpretability, and ensures that preference optimization focuses on meaningful differences between responses. Extensive experiments have validated OTPO's effectiveness in improving instruction-following ability across various settings\footnote{Code is available at https://github.com/Mimasss2/OTPO.}.

</details>


### [89] [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges](https://arxiv.org/abs/2505.18744)

*Tao Liu, Hongying Zan, Yifan Li, Dixuan Zhang, Lulu Kong, Haixin Liu, Jiaming Hou, Aoze Zheng, Rui Li, Yiming Qiao, Zewei Luo, Qi Wang, Zhiqiang Zhang, Jiaxi Li, Supeng Liu, Kunli Zhang, Min Peng*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Natural Language Processing, Dataset, Reasoning, Machine Learning

**Relevance Score:** 5

**TL;DR:** Introducing a new dataset, LogicCat, for complex reasoning in text-to-SQL tasks, featuring 4,038 questions with SQL queries and detailed annotations for reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing text-to-SQL datasets lack domain-specific knowledge and complex reasoning capabilities, necessitating a specialized dataset for better performance.

**Method:** A novel dataset, LogicCat, was created consisting of 4,038 questions paired with SQL queries and 12,114 reasoning annotations, covering various forms of reasoning.

**Key Contributions:**

	1. Creation of the LogicCat dataset for complex SQL reasoning
	2. Inclusion of diverse reasoning annotations
	3. Benchmarking results demonstrating challenges for existing models

**Result:** LogicCat increases the challenge for state-of-the-art models, achieving only 14.96% execution accuracy; however, using chain-of-thought annotations improves accuracy to 33.96%.

**Limitations:** 

**Conclusion:** The LogicCat dataset presents significant challenges for text-to-SQL systems, highlighting the need for improved reasoning capabilities in natural language processing tasks.

**Abstract:** Text-to-SQL is a fundamental task in natural language processing that seeks to translate natural language questions into meaningful and executable SQL queries. While existing datasets are extensive and primarily focus on business scenarios and operational logic, they frequently lack coverage of domain-specific knowledge and complex mathematical reasoning. To address this gap, we present a novel dataset tailored for complex reasoning and chain-of-thought analysis in SQL inference, encompassing physical, arithmetic, commonsense, and hypothetical reasoning. The dataset consists of 4,038 English questions, each paired with a unique SQL query and accompanied by 12,114 step-by-step reasoning annotations, spanning 45 databases across diverse domains. Experimental results demonstrate that LogicCat substantially increases the difficulty for state-of-the-art models, with the highest execution accuracy reaching only 14.96%. Incorporating our chain-of-thought annotations boosts performance to 33.96%. Benchmarking leading public methods on Spider and BIRD further underscores the unique challenges presented by LogicCat, highlighting the significant opportunities for advancing research in robust, reasoning-driven text-to-SQL systems. We have released our dataset code at https://github.com/Ffunkytao/LogicCat.

</details>


### [90] [Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning](https://arxiv.org/abs/2505.18752)

*Haolin Yang, Hakaze Cho, Yiqiao Zhong, Naoya Inoue*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, hidden states, classification tasks, machine learning

**Relevance Score:** 8

**TL;DR:** This paper proposes a unified framework for understanding in-context learning (ICL) in large language models by analyzing how query hidden states evolve through layers during classification tasks.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the internal mechanisms of large language models, particularly the role of in-context learning (ICL), which has unusual properties that require a unified understanding.

**Method:** The paper analyzes two geometric factors—separability and alignment of query hidden states—by conducting a fine-grained, layer-wise dynamics analysis and performing ablation studies to understand their impact during ICL in classification tasks.

**Key Contributions:**

	1. Proposed a unified framework for understanding ICL in large language models.
	2. Identified a two-stage mechanism involving separability and alignment of query hidden states.
	3. Linked attention heads and task vectors to the evolution of model outputs.

**Result:** The study reveals a two-stage mechanism where separability emerges in early layers, and alignment develops in later layers, with specific attention heads driving these characteristics.

**Limitations:** 

**Conclusion:** The findings bridge the gap between attention heads and task vectors, providing a comprehensive understanding of ICL's mechanisms in language models.

**Abstract:** The unusual properties of in-context learning (ICL) have prompted investigations into the internal mechanisms of large language models. Prior work typically focuses on either special attention heads or task vectors at specific layers, but lacks a unified framework linking these components to the evolution of hidden states across layers that ultimately produce the model's output. In this paper, we propose such a framework for ICL in classification tasks by analyzing two geometric factors that govern performance: the separability and alignment of query hidden states. A fine-grained analysis of layer-wise dynamics reveals a striking two-stage mechanism: separability emerges in early layers, while alignment develops in later layers. Ablation studies further show that Previous Token Heads drive separability, while Induction Heads and task vectors enhance alignment. Our findings thus bridge the gap between attention heads and task vectors, offering a unified account of ICL's underlying mechanisms.

</details>


### [91] [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)

*Elsen Ronando, Sozo Inoue*

**Main category:** cs.CL

**Keywords:** few-shot learning, HED-LM, example selection, fatigue detection, large language models

**Relevance Score:** 8

**TL;DR:** This paper introduces HED-LM, a method to enhance example selection in few-shot optimization for sensor-based classification tasks, specifically addressing fatigue detection using accelerometer data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The performance of few-shot prompting is significantly influenced by the quality of selected examples, which can hinder effective inference in sensor-based applications.

**Method:** HED-LM utilizes a hybrid selection pipeline that combines Euclidean distance filtering with contextual relevance scoring from large language models to improve example selection.

**Key Contributions:**

	1. Introduces a hybrid example selection method combining numerical and contextual relevance for few-shot learning.
	2. Demonstrates effectiveness in fatigue detection with accelerometer data characterized by high variability.
	3. Provides empirical evidence of improved performance over existing selection methods.

**Result:** HED-LM achieved a mean macro F1-score of 69.13%, outperforming random selection and distance-only filtering by 16.6% and 2.3%, respectively, demonstrating its effectiveness for nuanced tasks like fatigue detection.

**Limitations:** The approach's effectiveness may vary across different sensor modalities and task complexities.

**Conclusion:** HED-LM offers a robust solution for improving performance in real-world sensor-based learning applications and has further implications for healthcare monitoring and activity recognition.

**Abstract:** In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid Euclidean Distance with Large Language Models) to improve example selection for sensor-based classification tasks. While few-shot prompting enables efficient inference with limited labeled data, its performance largely depends on the quality of selected examples. HED-LM addresses this challenge through a hybrid selection pipeline that filters candidate examples based on Euclidean distance and re-ranks them using contextual relevance scored by large language models (LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection task using accelerometer data characterized by overlapping patterns and high inter-subject variability. Unlike simpler tasks such as activity recognition, fatigue detection demands more nuanced example selection due to subtle differences in physiological signals. Our experiments show that HED-LM achieves a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection (59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These represent relative improvements of 16.6% and 2.3%, respectively. The results confirm that combining numerical similarity with contextual relevance improves the robustness of few-shot prompting. Overall, HED-LM offers a practical solution to improve performance in real-world sensor-based learning tasks and shows potential for broader applications in healthcare monitoring, human activity recognition, and industrial safety scenarios.

</details>


### [92] [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)

*Minglai Yang, Ethan Huang, Liang Zhang, Mihai Surdeanu, William Wang, Liangming Pan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Robustness, Irrelevant Context, Machine Learning, Benchmark

**Relevance Score:** 8

**TL;DR:** The paper presents GSM-DC, a benchmark for testing LLMs' reasoning capabilities in handling irrelevant contexts during mathematical problem-solving.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the robustness of Large Language Models when faced with distracting contexts in reasoning tasks, particularly in grade school mathematics.

**Method:** The method involves constructing symbolic reasoning graphs with controlled irrelevant context injections and performing experiments to assess the impact on LLMs' reasoning and arithmetic performance.

**Key Contributions:**

	1. Introduction of GSM-DC benchmark for evaluating LLM reasoning under distracting context.
	2. Demonstrates the negative impact of irrelevant context on LLMs' performance.
	3. Proposes a tree search method that improves model robustness in challenging scenarios.

**Result:** LLMs show significant sensitivity to irrelevant context, impairing reasoning path selection and accuracy, but incorporating strong distractors during training improves overall performance.

**Limitations:** Focused primarily on mathematical reasoning; results may vary in other domains or complex tasks.

**Conclusion:** A novel stepwise tree search guided by a process reward model enhances LLM robustness to irrelevant contexts, especially in out-of-distribution conditions.

**Abstract:** We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic benchmark to evaluate Large Language Models' (LLMs) reasoning robustness against systematically controlled irrelevant context (IC). GSM-DC constructs symbolic reasoning graphs with precise distractor injections, enabling rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are significantly sensitive to IC, affecting both reasoning path selection and arithmetic accuracy. Additionally, training models with strong distractors improves performance in both in-distribution and out-of-distribution scenarios. We further propose a stepwise tree search guided by a process reward model, which notably enhances robustness in out-of-distribution conditions.

</details>


### [93] [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)

*Michael Flor, Zuowei Wang, Paul Deane, Tenaha O'Reilly*

**Main category:** cs.CL

**Keywords:** topics, vocabulary, background knowledge, automatic item generation, assessment, reading comprehension

**Relevance Score:** 5

**TL;DR:** K-tool is an automated system that generates topical vocabulary tests to assess students' background knowledge for improving reading comprehension in STEM texts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for an automated measure of students' background knowledge to predict their ability to comprehend specific domain-related texts.

**Method:** K-tool detects the topic of a text and generates vocabulary tests based on the topic's relationship to the words, categorizing words into related and unrelated groups.

**Key Contributions:**

	1. Development of an automated system for generating topical vocabulary tests
	2. Initial evaluation demonstrating system applicability
	3. Unique approach to measuring background knowledge related to reading comprehension

**Result:** Initial evaluation shows that K-tool can effectively produce vocabulary items that may indicate a student's readiness to understand text based on their knowledge state.

**Limitations:** Currently designed only for native English speakers in middle and high school; focuses on single reading passages without a broader text corpus.

**Conclusion:** K-tool is intended for middle and high school students and does not require a corpus, making it versatile for various single reading passages.

**Abstract:** Background knowledge is typically needed for successful comprehension of topical and domain specific reading passages, such as in the STEM domain. However, there are few automated measures of student knowledge that can be readily deployed and scored in time to make predictions on whether a given student will likely be able to understand a specific content area text. In this paper, we present our effort in developing K-tool, an automated system for generating topical vocabulary tests that measure students' background knowledge related to a specific text. The system automatically detects the topic of a given text and produces topical vocabulary items based on their relationship with the topic. This information is used to automatically generate background knowledge forms that contain words that are highly related to the topic and words that share similar features but do not share high associations to the topic. Prior research indicates that performance on such tasks can help determine whether a student is likely to understand a particular text based on their knowledge state. The described system is intended for use with middle and high school student population of native speakers of English. It is designed to handle single reading passages and is not dependent on any corpus or text collection. In this paper, we describe the system architecture and present an initial evaluation of the system outputs.

</details>


### [94] [Disentangling Knowledge Representations for Large Language Model Editing](https://arxiv.org/abs/2505.18774)

*Mengqi Zhang, Zisheng Zhou, Xiaotian Ye, Qiang Liu, Zhaochun Ren, Zhumin Chen, Pengjie Ren*

**Main category:** cs.CL

**Keywords:** Knowledge Editing, LLMs, DiKE, Disentanglement, Health Informatics

**Relevance Score:** 9

**TL;DR:** DiKE is a novel approach for efficiently editing knowledge in LLMs without losing fine-grained irrelevant knowledge, achieved through a unique disentanglement methodology.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing knowledge editing approaches that fail to preserve fine-grained irrelevant knowledge when editing LLMs.

**Method:** DiKE utilizes a Knowledge Representation Disentanglement module to separate target-related and unrelated components of knowledge, along with a Disentanglement-based Knowledge Edit module to update only the relevant parts while preserving others.

**Key Contributions:**

	1. Introduction of the DiKE approach for knowledge editing in LLMs
	2. Development of Knowledge Representation Disentanglement and Disentanglement-based Knowledge Edit modules
	3. Creation of the FINE-KED benchmark for evaluating knowledge preservation

**Result:** DiKE shows substantial improvements in preserving fine-grained irrelevant knowledge while maintaining competitive performance in general editing tasks across multiple LLMs.

**Limitations:** 

**Conclusion:** DiKE effectively disentangles and updates knowledge in LLMs, addressing challenges in preserving unrelated knowledge during the editing process.

**Abstract:** Knowledge Editing has emerged as a promising solution for efficiently updating embedded knowledge in large language models (LLMs). While existing approaches demonstrate effectiveness in integrating new knowledge and preserving the original capabilities of LLMs, they fail to maintain fine-grained irrelevant knowledge facts that share the same subject as edited knowledge but differ in relation and object. This challenge arises because subject representations inherently encode multiple attributes, causing the target and fine-grained irrelevant knowledge to become entangled in the representation space, and thus vulnerable to unintended alterations during editing. To address this, we propose DiKE, a novel approach that Disentangles Knowledge representations for LLM Editing (DiKE). DiKE consists of two key components: a Knowledge Representation Disentanglement (KRD) module that decomposes the subject representation into target-knowledgerelated and -unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module that updates only the target-related component while explicitly preserving the unrelated one. We further derive a closed-form, rank-one parameter update based on matrix theory to enable efficient and minimally invasive edits. To rigorously evaluate fine-grained irrelevant knowledge preservation, we construct FINE-KED, a new benchmark comprising fine-grained irrelevant knowledge at different levels of relational similarity to the edited knowledge. Extensive experiments across multiple LLMs demonstrate that DiKE substantially improves fine-grained irrelevant knowledge preservation while maintaining competitive general editing performance.

</details>


### [95] [A generalised editor calculus (Short Paper)](https://arxiv.org/abs/2505.18778)

*Benjamin Bennetzen, Peter Buus Steffensen, Hans Hüttel, Nikolaj Rossander Kristensen, Andreas Tor Mortensen*

**Main category:** cs.CL

**Keywords:** syntax-directed editing, lambda calculus, programming languages, editor calculus, syntactical errors

**Relevance Score:** 3

**TL;DR:** This paper presents a generalized syntax-directed editor calculus for creating specialized editors for various programming languages while avoiding syntactical errors and supporting incomplete programs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a flexible editor calculus that can instantiate syntax-directed editors for any programming language based on its abstract syntax, thereby facilitating better programming environments.

**Method:** The paper generalizes a syntax-directed editor calculus and encodes it into an extended simply typed lambda calculus, incorporating pairs, booleans, pattern matching, and fixed points.

**Key Contributions:**

	1. Generalized syntax-directed editor calculus
	2. Encoding in simply typed lambda calculus
	3. Support for incomplete programs in editing

**Result:** The generalized calculus ensures syntactical correctness and allows for editing incomplete programs, enhancing the flexibility of syntax-directed editing.

**Limitations:** 

**Conclusion:** The work provides a foundational tool for building specialized editors that prevent syntactical errors while supporting the editing of partial code, which is common in programming.

**Abstract:** In this paper, we present a generalization of a syntax-directed editor calculus, which can be used to instantiate a specialized syntax-directed editor for any language, given by some abstract syntax. The editor calculus guarantees the absence of syntactical errors while allowing incomplete programs. The generalized editor calculus is then encoded into a simply typed lambda calculus, extended with pairs, booleans, pattern matching and fixed points

</details>


### [96] [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)

*Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Attention Mechanism, Pruning Strategy, Task Alignment, Machine Learning

**Relevance Score:** 9

**TL;DR:** Proposes the ALPS algorithm for efficient LLM alignment by localizing and pruning task-sensitive attention heads, achieving improved performance with reduced training costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve alignment efficiency of LLMs while addressing data dependency issues that hinder generalizability to downstream tasks.

**Method:** The ALPS algorithm localizes most task-sensitive attention heads and prunes training updates to only these heads, drastically reducing alignment costs.

**Key Contributions:**

	1. Introduction of the ALPS algorithm for LLM alignment
	2. Significant reduction in training parameters required for alignment
	3. Demonstration of transferable task-specific attention heads.

**Result:** ALPS activates 10% of attention parameters during fine-tuning, achieving a 2% performance improvement over baselines on three tasks, with transferable task-specific heads that reduce knowledge forgetting.

**Limitations:** The method's performance may vary across different LLM architectures and tasks, requiring further validation.

**Conclusion:** ALPS offers a novel approach to enhance LLM alignment efficiency while being robust against knowledge retention across different datasets.

**Abstract:** Aligning general-purpose large language models (LLMs) to downstream tasks often incurs significant costs, including constructing task-specific instruction pairs and extensive training adjustments. Prior research has explored various avenues to enhance alignment efficiency, primarily through minimal-data training or data-driven activations to identify key attention heads. However, these approaches inherently introduce data dependency, which hinders generalization and reusability. To address this issue and enhance model alignment efficiency, we propose the \textit{\textbf{A}ttention \textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})}, an efficient algorithm that localizes the most task-sensitive attention heads and prunes by restricting attention training updates to these heads, thereby reducing alignment costs. Experimental results demonstrate that our method activates only \textbf{10\%} of attention parameters during fine-tuning while achieving a \textbf{2\%} performance improvement over baselines on three tasks. Moreover, the identified task-specific heads are transferable across datasets and mitigate knowledge forgetting. Our work and findings provide a novel perspective on efficient LLM alignment.

</details>


### [97] [Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation](https://arxiv.org/abs/2505.18842)

*Jiwan Chung, Junhyeok Kim, Siyeol Kim, Jaeyoung Lee, Min Soo Kim, Youngjae Yu*

**Main category:** cs.CL

**Keywords:** multimodal, large language models, visual reasoning, AI, grounded reasoning

**Relevance Score:** 7

**TL;DR:** A lightweight extension to MLLMs enables visual revisitation during inference, improving multimodal reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance performance of Multimodal Large Language Models by allowing selective visual revisitation during inference.

**Method:** Introduces a point-and-copy mechanism for dynamic retrieval of relevant image regions while reasoning, with minimal modifications to existing architectures.

**Key Contributions:**

	1. Introduction of a selective visual revisitation mechanism in MLLMs
	2. Construction of v1g dataset with 300K multimodal reasoning traces
	3. Improved performance on multimodal reasoning benchmarks

**Result:** Experiments on three benchmarks show that the new model significantly outperforms existing baselines, especially in tasks requiring fine-grained visual references and multi-step reasoning.

**Limitations:** 

**Conclusion:** Dynamic visual access is a promising approach for improving grounded multimodal reasoning in AI models.

**Abstract:** We present v1, a lightweight extension to Multimodal Large Language Models (MLLMs) that enables selective visual revisitation during inference. While current MLLMs typically consume visual input only once and reason purely over internal memory, v1 introduces a simple point-and-copy mechanism that allows the model to dynamically retrieve relevant image regions throughout the reasoning process. This mechanism augments existing architectures with minimal modifications, enabling contextual access to visual tokens based on the model's evolving hypotheses. To train this capability, we construct v1g, a dataset of 300K multimodal reasoning traces with interleaved visual grounding annotations. Experiments on three multimodal mathematical reasoning benchmarks -- MathVista, MathVision, and MathVerse -- demonstrate that v1 consistently improves performance over comparable baselines, particularly on tasks requiring fine-grained visual reference and multi-step reasoning. Our results suggest that dynamic visual access is a promising direction for enhancing grounded multimodal reasoning. Code, models, and data will be released to support future research.

</details>


### [98] [Multi-Party Conversational Agents: A Survey](https://arxiv.org/abs/2505.18845)

*Sagar Sapkota, Mohammad Saqib Hasan, Mubarak Shah, Santu Karmaker*

**Main category:** cs.CL

**Keywords:** Multi-party Conversational Agents, Theory of Mind, Large Language Models, Semantic Understanding, Multi-modal Systems

**Relevance Score:** 8

**TL;DR:** This survey explores the design of Multi-party Conversational Agents (MPCAs), focusing on mental state modeling, semantic understanding, and future conversation prediction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in creating agents that can engage in multi-party dialogues by interpreting semantics and social dynamics.

**Method:** The survey reviews various approaches including classical machine learning, Large Language Models (LLMs), and multi-modal systems in the context of MPCAs.

**Key Contributions:**

	1. Introduces the necessity of Theory of Mind for MPCAs.
	2. Explores the importance of mental state modeling and semantic understanding.
	3. Suggests directions for future research in multi-modal understanding.

**Result:** Identifies Theory of Mind (ToM) as crucial for MPCAs and highlights multi-modal understanding as an underexplored area.

**Limitations:** Focuses primarily on theoretical insights and does not provide extensive real-world applications or empirical studies.

**Conclusion:** The paper guides future research in developing more capable MPCAs based on the survey findings.

**Abstract:** Multi-party Conversational Agents (MPCAs) are systems designed to engage in dialogue with more than two participants simultaneously. Unlike traditional two-party agents, designing MPCAs faces additional challenges due to the need to interpret both utterance semantics and social dynamics. This survey explores recent progress in MPCAs by addressing three key questions: 1) Can agents model each participants' mental states? (State of Mind Modeling); 2) Can they properly understand the dialogue content? (Semantic Understanding); and 3) Can they reason about and predict future conversation flow? (Agent Action Modeling). We review methods ranging from classical machine learning to Large Language Models (LLMs) and multi-modal systems. Our analysis underscores Theory of Mind (ToM) as essential for building intelligent MPCAs and highlights multi-modal understanding as a promising yet underexplored direction. Finally, this survey offers guidance to future researchers on developing more capable MPCAs.

</details>


### [99] [Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation](https://arxiv.org/abs/2505.18853)

*Alexander Shabalin, Viacheslav Meshchaninov, Dmitry Vetrov*

**Main category:** cs.CL

**Keywords:** diffusion models, text generation, token embeddings, semantic similarity, sequence-to-sequence

**Relevance Score:** 4

**TL;DR:** Smoothie introduces a novel diffusion model for text generation that blends continuous and categorical approaches to enhance semantic representation and decoding quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite their success in generating images and audio, adapting diffusion models for text generation remains a challenge because textual data is discrete in nature.

**Method:** The proposed method, Smoothing Diffusion on Token Embeddings (Smoothie), progressively smooths token embeddings based on semantic similarity, allowing for better information management during decoding.

**Key Contributions:**

	1. Introduction of Smoothing Diffusion on Token Embeddings (Smoothie)
	2. Demonstration of improved generation quality over existing models
	3. Ablation studies that confirm the efficacy of the new diffusion space

**Result:** Experimental results indicate that Smoothie outperforms existing diffusion-based models in terms of generation quality across various sequence-to-sequence tasks.

**Limitations:** 

**Conclusion:** The proposed method provides a significant improvement in text generation, combining the advantages of both continuous latent spaces and categorical spaces.

**Abstract:** Diffusion models have achieved state-of-the-art performance in generating images, audio, and video, but their adaptation to text remains challenging due to its discrete nature. Prior approaches either apply Gaussian diffusion in continuous latent spaces, which inherits semantic structure but struggles with token decoding, or operate in categorical simplex space, which respect discreteness but disregard semantic relation between tokens. In this paper, we propose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion method that combines the strengths of both approaches by progressively smoothing token embeddings based on semantic similarity. This technique enables gradual information removal while maintaining a natural decoding process. Experimental results on several sequence-to-sequence generation tasks demonstrate that Smoothie outperforms existing diffusion-based models in generation quality. Furthermore, ablation studies show that our proposed diffusion space yields better performance than both the standard embedding space and the categorical simplex. Our code is available at https://github.com/ashaba1in/smoothie.

</details>


### [100] [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)

*Yuxiang Liu, Kevin Chen-Chuan Chang*

**Main category:** cs.CL

**Keywords:** expository text generation, adaptive imitation, large language models

**Relevance Score:** 8

**TL;DR:** This paper presents a new framework called RePA for generating expository texts based on exemplars, aiming to improve coherence and relevance while addressing limitations in existing methodologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methodologies in text generation struggle with the coherence and relevance of output texts, particularly when adapting content from exemplars on similar topics.

**Method:** The paper introduces the Recurrent Plan-then-Adapt (RePA) framework, which utilizes large language models to implement adaptive imitation through a detailed planning and adaptation process. The approach incorporates recurrent segment-by-segment imitation supported by memory structures for clarity and coherence.

**Key Contributions:**

	1. Introduction of the Exemplar-Based Expository Text Generation task
	2. Development of the Recurrent Plan-then-Adapt (RePA) framework
	3. Creation of new evaluation metrics for text generation tasks

**Result:** Experimental results show that the RePA framework outperforms previous baselines across three diverse datasets, producing texts that are more factual, consistent, and relevant.

**Limitations:** 

**Conclusion:** The proposed RePA framework effectively enhances the process of expository text generation using exemplars, achieving improved adaptability and coherence.

**Abstract:** We introduce the Exemplar-Based Expository Text Generation task, aiming to generate an expository text on a new topic using an exemplar on a similar topic. Current methods fall short due to their reliance on extensive exemplar data, difficulty in adapting topic-specific content, and issues with long-text coherence. To address these challenges, we propose the concept of Adaptive Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA leverages large language models (LLMs) for effective adaptive imitation through a fine-grained plan-then-adapt process. RePA also enables recurrent segment-by-segment imitation, supported by two memory structures that enhance input clarity and output coherence. We also develop task-specific evaluation metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as evaluators. Experimental results across our collected three diverse datasets demonstrate that RePA surpasses existing baselines in producing factual, consistent, and relevant texts for this task.

</details>


### [101] [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)

*Binhao Ma, Hanqing Guo, Zhengping Jay Luo, Rui Duan*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, adversarial attack, voice-enabled systems, SpeechGPT, security vulnerabilities

**Relevance Score:** 9

**TL;DR:** This paper presents a novel adversarial attack targeting the speech input of Multimodal Large Language Models (MLLMs), particularly focusing on vulnerabilities in voice-enabled systems like SpeechGPT.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored security risks of voice inputs in MLLMs, especially in light of significant advances in human-computer interaction.

**Method:** A novel token level adversarial attack that utilizes the model's speech tokenization to create adversarial sequences, which are synthesized into audio prompts.

**Key Contributions:**

	1. Introduction of a novel token level attack on speech input in MLLMs
	2. Demonstration of high attack success rates on SpeechGPT
	3. Identification of security vulnerabilities specific to voice modality in MLLMs

**Result:** Achieved up to 89% attack success rate across multiple restricted tasks on SpeechGPT, outperforming existing voice-based jailbreak methods.

**Limitations:** The focus is primarily on SpeechGPT and may not generalize to all MLLMs or different contexts of voice interaction.

**Conclusion:** The findings highlight the vulnerabilities of voice-enabled multimodal systems and provide insights for developing more robust MLLMs.

**Abstract:** Recent advances in Multimodal Large Language Models (MLLMs) have significantly enhanced the naturalness and flexibility of human computer interaction by enabling seamless understanding across text, vision, and audio modalities. Among these, voice enabled models such as SpeechGPT have demonstrated considerable improvements in usability, offering expressive, and emotionally responsive interactions that foster deeper connections in real world communication scenarios. However, the use of voice introduces new security risks, as attackers can exploit the unique characteristics of spoken language, such as timing, pronunciation variability, and speech to text translation, to craft inputs that bypass defenses in ways not seen in text-based systems. Despite substantial research on text based jailbreaks, the voice modality remains largely underexplored in terms of both attack strategies and defense mechanisms. In this work, we present an adversarial attack targeting the speech input of aligned MLLMs in a white box scenario. Specifically, we introduce a novel token level attack that leverages access to the model's speech tokenization to generate adversarial token sequences. These sequences are then synthesized into audio prompts, which effectively bypass alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT, our approach achieves up to 89 percent attack success rate across multiple restricted tasks, significantly outperforming existing voice based jailbreak methods. Our findings shed light on the vulnerabilities of voice-enabled multimodal systems and to help guide the development of more robust next-generation MLLMs.

</details>


### [102] [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)

*Ming Cheng, Jiaying Gong, Hoda Eldardiry*

**Main category:** cs.CL

**Keywords:** lay paraphrasing, interdisciplinary research, Sci-LoRA, large language models, cross-domain adaptability

**Relevance Score:** 7

**TL;DR:** Sci-LoRA is a model designed for lay paraphrasing that dynamically integrates knowledge from multiple scientific domains to enhance accessibility of information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for interdisciplinary understanding in lay paraphrasing, as most existing studies focus on single domains, creating barriers for non-experts.

**Method:** Sci-LoRA uses a mixture of LoRAs fine-tuned on multiple scientific domains and dynamically adjusts the weight of each based on the input text, allowing it to balance domain-specific knowledge and generalization.

**Key Contributions:**

	1. Introduction of the Sci-LoRA model for interdisciplinary lay paraphrasing.
	2. Dynamic weighting of LoRAs for improved performance across different scientific domains.
	3. Demonstrated ability to generalize and adapt without explicit domain labels.

**Result:** Sci-LoRA significantly outperforms state-of-the-art large language models across twelve domains on five public datasets, demonstrating improved adaptability and performance in lay paraphrasing.

**Limitations:** The performance may still vary based on the complexity of specific domains and input text characteristics.

**Conclusion:** The dynamic fusion of domain-specific and general knowledge in Sci-LoRA provides a robust solution for cross-domain lay paraphrasing, making scientific information more accessible.

**Abstract:** Lay paraphrasing aims to make scientific information accessible to audiences without technical backgrounds. However, most existing studies focus on a single domain, such as biomedicine. With the rise of interdisciplinary research, it is increasingly necessary to comprehend knowledge spanning multiple technical fields. To address this, we propose Sci-LoRA, a model that leverages a mixture of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA dynamically generates and applies weights for each LoRA, enabling it to adjust the impact of different domains based on the input text, without requiring explicit domain labels. To balance domain-specific knowledge and generalization across various domains, Sci-LoRA integrates information at both the data and model levels. This dynamic fusion enhances the adaptability and performance across various domains. Experimental results across twelve domains on five public datasets show that Sci-LoRA significantly outperforms state-of-the-art large language models and demonstrates flexible generalization and adaptability in cross-domain lay paraphrasing.

</details>


### [103] [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)

*Kung-Hsiang Huang, Akshara Prabhakar, Onkar Thorat, Divyansh Agarwal, Prafulla Kumar Choubey, Yixin Mao, Silvio Savarese, Caiming Xiong, Chien-Sheng Wu*

**Main category:** cs.CL

**Keywords:** LLM agents, benchmark, business scenarios, multi-turn interactions, confidentiality awareness

**Relevance Score:** 8

**TL;DR:** CRMArena-Pro is a novel benchmark for assessing LLM agents in business settings, revealing significant performance gaps in multi-turn interactions and confidentiality awareness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** AI agents have transformative potential in business, but current performance benchmarks lack realism and diversity due to limited public data.

**Method:** Introduced CRMArena-Pro with 19 expert-validated tasks spanning various business scenarios, incorporating multi-turn interactions and confidentiality assessments.

**Key Contributions:**

	1. Introduction of CRMArena-Pro as a benchmark for LLM agents in diverse business contexts.
	2. Inclusion of multi-turn interactions and diverse personas for realistic assessments.
	3. Findings underscore the performance limitations of current LLM agents in enterprise applications.

**Result:** Leading LLM agents achieve around 58% single-turn success, dropping to 35% in multi-turn settings; top agents excel in Workflow Execution with over 83% single-turn success but struggle with other tasks.

**Limitations:** Agents exhibit near-zero inherent confidentiality awareness; prompting improvements can compromise task performance.

**Conclusion:** The substantial gap between LLM capabilities and enterprise demands highlights the need for advancements in multi-turn reasoning, confidentiality awareness, and skill acquisition.

**Abstract:** While AI agents hold transformative potential in business, effective performance benchmarking is hindered by the scarcity of public, realistic business data on widely used platforms. Existing benchmarks often lack fidelity in their environments, data, and agent-user interactions, with limited coverage of diverse business scenarios and industries. To address these gaps, we introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena with nineteen expert-validated tasks across sales, service, and 'configure, price, and quote' processes, for both Business-to-Business and Business-to-Customer scenarios. It distinctively incorporates multi-turn interactions guided by diverse personas and robust confidentiality awareness assessments. Experiments reveal leading LLM agents achieve only around 58% single-turn success on CRMArena-Pro, with performance dropping significantly to approximately 35% in multi-turn settings. While Workflow Execution proves more tractable for top agents (over 83% single-turn success), other evaluated business skills present greater challenges. Furthermore, agents exhibit near-zero inherent confidentiality awareness; though targeted prompting can improve this, it often compromises task performance. These findings highlight a substantial gap between current LLM capabilities and enterprise demands, underscoring the need for advancements in multi-turn reasoning, confidentiality adherence, and versatile skill acquisition.

</details>


### [104] [StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos](https://arxiv.org/abs/2505.18903)

*Valentin Barriere, Nahuel Gomez, Leo Hemamou, Sofia Callejas, Brian Ravenet*

**Main category:** cs.CL

**Keywords:** humor detection, multimodal dataset, sequence labeling, audience laughter, stand-up comedy

**Relevance Score:** 4

**TL;DR:** A new multimodal dataset for humor detection in stand-up comedy across seven languages, accounting for audience laughter and proposing a novel word-level sequence labeling approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve computational models of humor detection by providing a diverse and extensive dataset.

**Method:** A new multimodal dataset is created, annotated for audience laughter, and the task is framed as word-level sequence labeling instead of binary classification.

**Key Contributions:**

	1. Creation of a multimodal dataset in seven languages.
	2. Introduction of word-level sequence labeling for humor detection.
	3. Method to enhance automatic laughter detection based on speech recognition errors.

**Result:** The dataset of over 330 hours is the largest for humor detection and shows improved results in capturing the context of humor.

**Limitations:** 

**Conclusion:** The proposed method highlights the need for word-level analysis in humor detection and facilitates better modeling of laughter in conversations.

**Abstract:** Aiming towards improving current computational models of humor detection, we propose a new multimodal dataset of stand-up comedies, in seven languages: English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset of more than 330 hours, is at the time of writing the biggest available for this type of task, and the most diverse. The whole dataset is automatically annotated in laughter (from the audience), and the subpart left for model validation is manually annotated. Contrary to contemporary approaches, we do not frame the task of humor detection as a binary sequence classification, but as word-level sequence labeling, in order to take into account all the context of the sequence and to capture the continuous joke tagging mechanism typically occurring in natural conversations. As par with unimodal baselines results, we propose a method for e propose a method to enhance the automatic laughter detection based on Audio Speech Recognition errors. Our code and data are available online: https://tinyurl.com/EMNLPHumourStandUpPublic

</details>


### [105] [Building a Functional Machine Translation Corpus for Kpelle](https://arxiv.org/abs/2505.18905)

*Kweku Andoh Yamoah, Jackson Weako, Emmanuel J. Dorley*

**Main category:** cs.CL

**Keywords:** machine translation, Kpelle, low-resource languages, NLP, data augmentation

**Relevance Score:** 4

**TL;DR:** Introduction of a new English-Kpelle dataset for machine translation, achieving competitive results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of resources for Kpelle language and improve machine translation capabilities.

**Method:** Fine-tuning Meta's No Language Left Behind model on a dataset of over 2000 English-Kpelle sentence pairs.

**Key Contributions:**

	1. First English-Kpelle dataset for machine translation
	2. Data augmentation leads to improved translation performance
	3. Roadmap for future dataset expansion and collaboration

**Result:** Achieved BLEU scores up to 30 in Kpelle-to-English direction, demonstrating the potential of Kpelle for competitive NLP performance.

**Limitations:** 

**Conclusion:** The dataset allows for future expansion and improvement of inclusive language technology for Kpelle and other low-resourced languages.

**Abstract:** In this paper, we introduce the first publicly available English-Kpelle dataset for machine translation, comprising over 2000 sentence pairs drawn from everyday communication, religious texts, and educational materials. By fine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the dataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English direction, demonstrating the benefits of data augmentation. Our findings align with NLLB-200 benchmarks on other African languages, underscoring Kpelle's potential for competitive performance despite its low-resource status. Beyond machine translation, this dataset enables broader NLP tasks, including speech recognition and language modelling. We conclude with a roadmap for future dataset expansion, emphasizing orthographic consistency, community-driven validation, and interdisciplinary collaboration to advance inclusive language technology development for Kpelle and other low-resourced Mande languages.

</details>


### [106] [Federated Retrieval-Augmented Generation: A Systematic Mapping Study](https://arxiv.org/abs/2505.18906)

*Abhijit Chakraborty, Chahana Dahal, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** Federated Learning, Retrieval-Augmented Generation, Natural Language Processing, Privacy, Systematic Mapping

**Relevance Score:** 9

**TL;DR:** This paper presents a systematic mapping study of Federated Retrieval-Augmented Generation, analyzing its architecture, trends, and challenges, and its implications for privacy-sensitive applications in NLP.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing deployment of large language models in sensitive domains like healthcare, combining Federated Learning with Retrieval-Augmented Generation offers a secure framework for NLP.

**Method:** The authors conducted a systematic mapping study of literature from 2020 to 2025, applying Kitchenham's guidelines to categorize research focuses, contributions, and application domains.

**Key Contributions:**

	1. First systematic mapping study of Federated RAG
	2. Structured classification of research focuses and application domains
	3. Identification of key challenges and trends in the field.

**Result:** The study identifies architectural patterns, trends, and challenges, including privacy-preserving retrieval and cross-client heterogeneity, laying groundwork for future research in Federated RAG.

**Limitations:** The study may not cover all relevant literature outside the specified date range and focuses primarily on theoretical aspects rather than empirical evaluations.

**Conclusion:** Federated RAG is a promising framework for enhancing knowledge-intensive NLP while ensuring privacy, and there are open questions that require further exploration.

**Abstract:** Federated Retrieval-Augmented Generation (Federated RAG) combines Federated Learning (FL), which enables distributed model training without exposing raw data, with Retrieval-Augmented Generation (RAG), which improves the factual accuracy of language models by grounding outputs in external knowledge. As large language models are increasingly deployed in privacy-sensitive domains such as healthcare, finance, and personalized assistance, Federated RAG offers a promising framework for secure, knowledge-intensive natural language processing (NLP). To the best of our knowledge, this paper presents the first systematic mapping study of Federated RAG, covering literature published between 2020 and 2025. Following Kitchenham's guidelines for evidence-based software engineering, we develop a structured classification of research focuses, contribution types, and application domains. We analyze architectural patterns, temporal trends, and key challenges, including privacy-preserving retrieval, cross-client heterogeneity, and evaluation limitations. Our findings synthesize a rapidly evolving body of research, identify recurring design patterns, and surface open questions, providing a foundation for future work at the intersection of RAG and federated systems.

</details>


### [107] [SCRum-9: Multilingual Stance Classification over Rumours on Social Media](https://arxiv.org/abs/2505.18916)

*Yue Li, Jake Vasilakes, Zhixue Zhao, Carolina Scarton*

**Main category:** cs.CL

**Keywords:** Rumour Stance Classification, Multilingual Dataset, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** SCRum-9 is a multilingual dataset for Rumour Stance Classification, comprising 7,516 tweet-reply pairs across 9 languages, with complex annotations from native speakers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve existing stance classification datasets by incorporating a broader linguistic range and linking to fact-checked claims.

**Method:** Created a multilingual dataset consisting of tweet-reply pairs annotated by native speakers, with thorough cross-linguistic and complex annotations collected over 405 hours.

**Key Contributions:**

	1. Introduction of a multilingual dataset for rumour stance classification.
	2. Linking examples to fact-checked claims.
	3. In-depth annotations to address annotator variability.

**Result:** The SCRum-9 dataset serves as a challenging benchmark for leading LLMs and fine-tuned models, indicating significant room for improvement in rumour stance classification.

**Limitations:** 

**Conclusion:** The dataset highlights the challenges in stance classification and encourages future research in utilizing multilingual data for improved model performance.

**Abstract:** We introduce SCRum-9, a multilingual dataset for Rumour Stance Classification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond existing stance classification datasets by covering more languages (9), linking examples to more fact-checked claims (2.1k), and including complex annotations from multiple annotators to account for intra- and inter-annotator variability. Annotations were made by at least three native speakers per language, totalling around 405 hours of annotation and 8,150 dollars in compensation. Experiments on SCRum-9 show that it is a challenging benchmark for both state-of-the-art LLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating future work in this area.

</details>


### [108] [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)

*Amel Muminovic*

**Main category:** cs.CL

**Keywords:** automated moderation, large language models, toxic comment detection

**Relevance Score:** 8

**TL;DR:** This study evaluates the performance of three large language models in identifying harmful comments in YouTube threads, finding strengths and weaknesses in their precision and recall.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing harassment in online comment sections is crucial for improving user experience and well-being.

**Method:** Benchmarking of GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus on a dataset of 5,080 YouTube comments categorized as harmful or non-harmful.

**Key Contributions:**

	1. Evaluation of LLMs on automated content moderation
	2. Public release of a dataset for further research
	3. Insights into the challenges of sarcasm and mixed-language content in moderation

**Result:** GPT-4.1 displayed the best overall balance (F1 score: 0.863), Gemini flagged the most harmful posts but had lower precision, and Claude had the highest precision but the lowest recall.

**Limitations:** Models struggled with coded insults, sarcasm, and slang, indicating gaps in their capabilities.

**Conclusion:** The study highlights the need for improved moderation models that combine the strengths of different approaches and adapt to diverse languages and contexts.

**Abstract:** As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.

</details>


### [109] [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems](https://arxiv.org/abs/2505.18943)

*Xuanming Zhang, Yuxuan Chen, Min-Hsuan Yeh, Yixuan Li*

**Main category:** cs.CL

**Keywords:** Theory of Mind, MetaMind, social reasoning, large language models, empathetic dialogue

**Relevance Score:** 9

**TL;DR:** MetaMind is a multi-agent framework designed to enhance large language models' ability to understand human social interactions by emulating cognitive processes of Theory of Mind (ToM).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve large language models' performance in understanding and responding to human social cues, which are complex and nuanced compared to traditional semantic tasks.

**Method:** MetaMind uses three collaborative agents: a Theory-of-Mind Agent for hypothesizing user mental states, a Domain Agent for refining these hypotheses based on cultural norms and ethical constraints, and a Response Agent for generating contextually appropriate replies while aligning with inferred intents.

**Key Contributions:**

	1. Introduces a novel multi-agent framework for social reasoning in LLMs.
	2. Achieves state-of-the-art performance on benchmarks related to Theory of Mind and social interactions.
	3. Demonstrates the capability of LLMs to perform at human-level competency in understanding social cues.

**Result:** The framework achieves a 35.7% improvement in real-world social scenarios and a 6.2% gain in ToM reasoning, enabling LLMs to match human-level performance on ToM tasks.

**Limitations:** 

**Conclusion:** MetaMind advances AI systems towards human-like social intelligence and has potential applications in areas such as empathetic dialogue and culturally sensitive interactions.

**Abstract:** Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs-a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce MetaMind, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent, emotion), (2) a Domain Agent refines these hypotheses using cultural norms and ethical constraints, and (3) a Response Agent generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind.

</details>


### [110] [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)

*Longfei Yun, Chenyang An, Zilong Wang, Letian Peng, Jingbo Shang*

**Main category:** cs.CL

**Keywords:** large language models, diversity collapse, prompt design, instruction tuning, output variability

**Relevance Score:** 8

**TL;DR:** This paper investigates the issue of diversity collapse in instruction-tuned LLMs, revealing that structured templates can limit output variability, and proposes the need for diversity-aware prompt design.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The research is motivated by the need to understand how formatting constraints in instruction-tuned LLMs affect their creativity and variability in output.

**Method:** The authors evaluate the diversity collapse phenomenon by fine-tuning LLMs with structured prompts and assessing their performance on multiple tasks. They analyze the impact of structural tokens on output diversity and task alignment.

**Key Contributions:**

	1. Identification of diversity collapse in instruction-tuned LLMs
	2. Systematic evaluation of the impact of structural tokens on output diversity
	3. Recommendations for diversity-aware prompt design

**Result:** The study finds that diversity collapse occurs even with high-temperature sampling, with structural tokens severely limiting output space. Format consistency is critical for certain tasks, while output diversity is enhanced with minimal formatting.

**Limitations:** 

**Conclusion:** The findings highlight the trade-off between alignment and output diversity in LLMs, suggesting a need for reformulating prompt design to improve diversity without compromising performance.

**Abstract:** Instruction-tuned large language models (LLMs) employ structured templates, such as role markers and special tokens, to enforce format consistency during inference. However, we identify a critical limitation of such formatting: it induces a phenomenon we term diversity collapse, where the model generates semantically similar outputs for open-ended inputs, undermining creativity and variability. We systematically evaluate this effect across tasks like story completion and free-form generation, finding that (1) diversity collapse persists even under high-temperature sampling, and (2) structural tokens in templates significantly constrain the model's output space. To contextualize these findings, we fine-tune the same model using a range of structured prompts and then evaluate them across three axes: downstream task performance, alignment behavior, and output diversity. Our analysis shows that format consistency between fine-tuning and inference is crucial for structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity is primarily governed by the presence or absence of structural tokens, with minimal formatting yielding the most diverse outputs. These findings reveal that current prompting conventions, while beneficial for alignment, may inadvertently suppress output diversity, underscoring the need for diversity-aware prompt design and instruction tuning.

</details>


### [111] [BnMMLU: Measuring Massive Multitask Language Understanding in Bengali](https://arxiv.org/abs/2505.18951)

*Saman Sarker Joy*

**Main category:** cs.CL

**Keywords:** Bengali, language models, benchmarking, multitask language understanding, low-resource languages

**Relevance Score:** 7

**TL;DR:** Introduction of a benchmark for evaluating language models on multitask understanding in Bengali, revealing performance gaps in existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing MMLU datasets focus on high-resource languages, leaving low-resource languages like Bengali underrepresented.

**Method:** Introduction of BnMMLU, a benchmark comprised of 138,949 question-option pairs across 23 domains, structured in multiple-choice format; benchmarking of various LLMs on this dataset.

**Key Contributions:**

	1. Introduction of BnMMLU benchmark for Bengali language models
	2. Dataset includes 138,949 question-option pairs across 23 domains
	3. Highlighting performance gaps in existing LLMs for low-resource languages

**Result:** Significant performance gaps observed in LLMs tested on the BnMMLU benchmark, indicating the need for better pre-training and fine-tuning for Bengali data.

**Limitations:** Focus primarily on Bengali, may not generalize to other low-resource languages; results are specific to tested LLMs.

**Conclusion:** The BnMMLU dataset and results emphasize the necessity for tailored language model strategies for low-resource languages like Bengali and are released for further research.

**Abstract:** The Massive Multitask Language Understanding (MMLU) benchmark has been widely used to evaluate language models across various domains. However, existing MMLU datasets primarily focus on high-resource languages such as English, which leaves low-resource languages like Bengali underrepresented. In this paper, we introduce BnMMLU, a benchmark to evaluate the multitask language understanding capabilities of Bengali in language models. The dataset spans 23 domains, including science, humanities, mathematics and general knowledge and is structured in a multiple-choice format to assess factual knowledge, application-based problem-solving and reasoning abilities of language models. It consists of 138,949 question-option pairs. We benchmark several proprietary and open-source large language models (LLMs) on the BnMMLU test set. Additionally, we annotate the test set with three cognitive categories-factual knowledge, procedural application and reasoning-to gain deeper insights into model strengths and weaknesses across various cognitive tasks. The results reveal significant performance gaps, highlighting the need for improved pre-training and fine-tuning strategies tailored to Bengali data. We release the dataset and benchmark results to facilitate further research in this area.

</details>


### [112] [Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?](https://arxiv.org/abs/2505.18953)

*Divij Chawla, Ashita Bhutada, Do Duc Anh, Abhinav Raghunathan, Vinod SP, Cathy Guo, Dar Win Liew, Prannaya Gupta, Rishabh Bhardwaj, Rajat Bhardwaj, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** AI models, investment risk, demographic bias, financial applications, standardized evaluation

**Relevance Score:** 4

**TL;DR:** The paper evaluates AI models' credibility in assessing investment risk appetite across diverse user profiles and highlights significant variance and biases in their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inconsistency and potential biases in AI models used for investment risk assessments, particularly in regulated financial contexts.

**Method:** The study analyzes proprietary and open-weight AI models using 1,720 user profiles with 16 risk-relevant features across 10 countries and genders.

**Key Contributions:**

	1. Evaluation of multiple AI models in investment risk assessment
	2. Identification of demographic biases in AI risk scoring
	3. Recommendation for standardized evaluations in financial AI applications

**Result:** The analysis reveals significant disparities in risk score distributions and model behavior, with certain models displaying demographic biases and inconsistent performance across different regions.

**Limitations:** The study does not explore the underlying reasons for the observed biases and inconsistencies in AI model performances.

**Conclusion:** There is a critical need for rigorous evaluations of AI systems in finance to mitigate risks of bias and ensure reliable deployment.

**Abstract:** We evaluate the credibility of leading AI models in assessing investment risk appetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and open-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720 user profiles constructed with 16 risk-relevant features across 10 countries and both genders. We observe significant variance across models in score distributions and demographic sensitivity. For example, GPT-4o assigns higher risk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show opposite gender tendencies in risk classification. While some models (e.g., GPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk ranges, none maintain consistent performance across regions and demographics. Our findings highlight the need for rigorous, standardized evaluations of AI systems in regulated financial contexts to prevent bias, opacity, and inconsistency in real-world deployment.

</details>


### [113] [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)

*Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, Bang Liu*

**Main category:** cs.CL

**Keywords:** large language models, adaptive reasoning, efficiency, latent-space, shortcut paths

**Relevance Score:** 9

**TL;DR:** This paper proposes System-1.5 Reasoning, an adaptive reasoning framework for large language models that improves computational efficiency by dynamically allocating computation across reasoning steps in latent space.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing chain-of-thought reasoning methods are inefficient due to verbose outputs and uniform treatment of reasoning steps. This paper seeks to enhance efficiency by better utilizing computational resources through an adaptive framework.

**Method:** The System-1.5 Reasoning framework introduces dynamic shortcuts, including a model depth shortcut that allows critical reasoning to proceed through deeper layers while skipping non-critical tokens, and a step shortcut that reuses hidden states to skip trivial steps.

**Key Contributions:**

	1. Introduction of the System-1.5 Reasoning framework for adaptive reasoning in LLMs
	2. Implementation of depth and step shortcuts for improved computational efficiency
	3. Demonstrated 20x speed-up and significant reduction in token generation for reasoning tasks

**Result:** System-1.5 Reasoning shows superior performance on reasoning tasks like GSM8K, achieving performance comparable to traditional methods while speeding up inference by over 20x and reducing token generation by 92.31%.

**Limitations:** The work is still in progress, and further validation and testing of the framework may be required.

**Conclusion:** The proposed System-1.5 Reasoning framework represents a significant step forward in adapting reasoning efficiency for large language models, enabling more effective use of computational resources.

**Abstract:** Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move beyond fast System-1 responses and engage in deliberative System-2 reasoning. However, this comes at the cost of significant inefficiency due to verbose intermediate output. Recent latent-space reasoning methods improve efficiency by operating on hidden states without decoding into language, yet they treat all steps uniformly, failing to distinguish critical deductions from auxiliary steps and resulting in suboptimal use of computational resources. In this paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that dynamically allocates computation across reasoning steps through shortcut paths in latent space.Specifically, System-1.5 Reasoning introduces two types of dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the vertical depth by early exiting non-critical tokens through lightweight adapter branches, while allowing critical tokens to continue through deeper Transformer layers. The step shortcut (SS) reuses hidden states across the decoding steps to skip trivial steps and reason horizontally in latent space. Training System-1.5 Reasoning involves a two-stage self-distillation process: first distilling natural language CoT into latent-space continuous thought, and then distilling full-path System-2 latent reasoning into adaptive shortcut paths (System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves reasoning performance comparable to traditional CoT fine-tuning methods while accelerating inference by over 20x and reducing token generation by 92.31% on average.

</details>


### [114] [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)

*Bowen Wei, Ziwei Zhu*

**Main category:** cs.CL

**Keywords:** large language models, explanations, prototype-based, interpretability, surrogate model

**Relevance Score:** 9

**TL;DR:** ProtoSurE is a prototype-based framework that provides faithful and understandable explanations for large language models, outperforming existing methods across various datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the transparency and understandability of large language models' decision-making processes, which are often opaque and difficult to explain using current methods.

**Method:** ProtoSurE utilizes a prototype-based surrogate model that aligns with target LLMs, employing sentence-level prototypes to enhance human comprehension of model outputs.

**Key Contributions:**

	1. Introduction of ProtoSurE, a novel prototype-based explanation framework for LLMs
	2. Demonstration of improved performance over existing explanation methods
	3. Showcasing of strong data efficiency for real-world applicability

**Result:** ProtoSurE outperforms state-of-the-art explanation methods in terms of faithfulness and understandability while demonstrating strong data efficiency with fewer training examples needed for effective performance.

**Limitations:** 

**Conclusion:** ProtoSurE presents a practical solution for enhancing the interpretability of LLMs, making it suitable for real-world applications where explainability is crucial.

**Abstract:** Large language models (LLMs) have demonstrated impressive performance on natural language tasks, but their decision-making processes remain largely opaque. Existing explanation methods either suffer from limited faithfulness to the model's reasoning or produce explanations that humans find difficult to understand. To address these challenges, we propose \textbf{ProtoSurE}, a novel prototype-based surrogate framework that provides faithful and human-understandable explanations for LLMs. ProtoSurE trains an interpretable-by-design surrogate model that aligns with the target LLM while utilizing sentence-level prototypes as human-understandable concepts. Extensive experiments show that ProtoSurE consistently outperforms SOTA explanation methods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates strong data efficiency, requiring relatively few training examples to achieve good performance, making it practical for real-world applications.

</details>


### [115] [Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE](https://arxiv.org/abs/2505.18971)

*Abhijit Chakraborty, Chahana Dahal, Ashutosh Balasubramaniam, Tejas Anvekar, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** knowledge graph completion, embedding models, RelatE, efficiency, interpretability

**Relevance Score:** 4

**TL;DR:** RelatE is a modular method for knowledge graph completion that uses real-valued embedding models, offering significant efficiency and performance benefits over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve knowledge graph completion methods, focusing on simplicity and interpretability while maintaining competitive performance.

**Method:** RelatE integrates dual representations for entities and relations using real-valued phase-modulus decomposition to encode relational patterns.

**Key Contributions:**

	1. Introduction of RelatE model for knowledge graph completion
	2. Achieves competitive performance with architectural simplicity
	3. Significant efficiency gains over previous methods

**Result:** RelatE achieves superior performance on standard benchmarks, including a MRR of 0.521 on YAGO3-10, while reducing training and inference times and improving robustness against structural edits.

**Limitations:** 

**Conclusion:** RelatE is positioned as a scalable and interpretable alternative to complex architectures in knowledge graph completion.

**Abstract:** We revisit the efficacy of simple, real-valued embedding models for knowledge graph completion and introduce RelatE, an interpretable and modular method that efficiently integrates dual representations for entities and relations. RelatE employs a real-valued phase-modulus decomposition, leveraging sinusoidal phase alignments to encode relational patterns such as symmetry, inversion, and composition. In contrast to recent approaches based on complex-valued embeddings or deep neural architectures, RelatE preserves architectural simplicity while achieving competitive or superior performance on standard benchmarks. Empirically, RelatE outperforms prior methods across several datasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680, surpassing all baselines. Additionally, RelatE offers significant efficiency gains, reducing training time by 24%, inference latency by 31%, and peak GPU memory usage by 22% compared to RotatE. Perturbation studies demonstrate improved robustness, with MRR degradation reduced by up to 61% relative to TransE and by up to 19% compared to RotatE under structural edits such as edge removals and relation swaps. Formal analysis further establishes the model's full expressiveness and its capacity to represent essential first-order logical inference patterns. These results position RelatE as a scalable and interpretable alternative to more complex architectures for knowledge graph completion.

</details>


### [116] [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)

*Sarang Patil, Ashish Parmanand Pandey, Ioannis Koutis, Mengjia Xu*

**Main category:** cs.CL

**Keywords:** hierarchical models, language representation, hyperbolic geometry, machine learning, multi-hop inference

**Relevance Score:** 8

**TL;DR:** The paper introduces Hierarchical Mamba (HiM), a model that integrates hyperbolic geometry to enhance language representation for complex hierarchical reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language representation in hierarchical reasoning tasks, where traditional models with flat Euclidean embeddings fall short.

**Method:** The HiM model utilizes efficient Mamba2 alongside hyperbolic geometry to create hierarchy-aware language embeddings, leveraging tangent-based and cosine/sine-based mappings to optimize language modeling across hierarchical levels.

**Key Contributions:**

	1. Introduction of Hierarchical Mamba (HiM) for modeling hierarchical language embeddings.
	2. Utilization of hyperbolic geometry for enhanced distance representation in language tasks.
	3. Demonstration of superior performance in mixed-hop prediction and multi-hop inference over traditional models.

**Result:** HiM models outperform Euclidean baselines in capturing hierarchical relationships, demonstrating better performance in mixed-hop prediction and multi-hop inference tasks on linguistic and medical datasets.

**Limitations:** 

**Conclusion:** HiM provides a more robust framework for understanding complex hierarchical structures in language, applicable to various inference tasks.

**Abstract:** Selective state-space models have achieved great success in long-sequence modeling. However, their capacity for language representation, especially in complex hierarchical reasoning tasks, remains underexplored. Most large language models rely on flat Euclidean embeddings, limiting their ability to capture latent hierarchies. To address this limitation, we propose Hierarchical Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved nature of hyperbolic geometry to learn hierarchy-aware language embeddings for deeper linguistic understanding. Mamba2-processed sequences are projected to the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via cosine and sine-based mapping) with "learnable" curvature, optimized with a combined hyperbolic loss. Our HiM model facilitates the capture of relational distances across varying hierarchical levels, enabling effective long-range reasoning. This makes it well-suited for tasks like mixed-hop prediction and multi-hop inference in hierarchical classification. We evaluated our HiM with four linguistic and medical datasets for mixed-hop prediction and multi-hop inference tasks. Experimental results demonstrated that: 1) Both HiM models effectively capture hierarchical relationships for four ontological datasets, surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic distinctions with higher h-norms, while HiM-Lorentz provides more stable, compact, and hierarchy-preserving embeddings favoring robustness over detail.

</details>


### [117] [AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2505.18978)

*Miguel Angel Peñaloza Perez, Bruno Lopez Orozco, Jesus Tadeo Cruz Soto, Michelle Bruno Hernandez, Miguel Angel Alvarado Gonzalez, Sandra Malagon*

**Main category:** cs.CL

**Keywords:** math benchmark, language models, Spanish math problems, AI4Math, domain-specific evaluation

**Relevance Score:** 6

**TL;DR:** AI4Math is a benchmark of 105 original math problems in Spanish that evaluates various language models' performance.

**Read time:** 36 min

<details>
  <summary>Details</summary>

**Motivation:** To create a benchmark that addresses the shortcomings of existing English-only or translation-based mathematical reasoning assessments, which can obscure language-specific errors.

**Method:** The study presents a dataset of 105 university-level math problems across seven domains, evaluated using six large language models under zero shot and chain of thought configurations, in both Spanish and English.

**Key Contributions:**

	1. Introduction of AI4Math, a Spanish-native math problem benchmark
	2. Evaluation of diverse language models on the benchmark
	3. Insights into reasoning capabilities across languages and domains

**Result:** Top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve over 70% accuracy; LLaMA 3.3 70B and GPT-4o mini score below 40%. Most models perform consistently across languages, with some showing better results on Spanish problems.

**Limitations:** The study is focused only on university-level problems and does not address lower educational levels or all mathematical domains.

**Conclusion:** The results indicate a pressing need for native-language benchmarks to uncover reasoning failures that standard evaluations tend to miss.

**Abstract:** Existing mathematical reasoning benchmarks are predominantly English only or translation-based, which can introduce semantic drift and mask languagespecific reasoning errors. To address this, we present AI4Math, a benchmark of 105 original university level math problems natively authored in Spanish. The dataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability, Number Theory, Combinatorics, and Logic), and each problem is accompanied by a step by step human solution. We evaluate six large language models GPT 4o, GPT 4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under four configurations: zero shot and chain of thought, each in Spanish and English. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve over 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most models show no significant performance drop between languages, with GPT 4o even performing better on Spanish problems in the zero shot setting. Geometry, Combinatorics, and Probability questions remain persistently challenging for all models. These results highlight the need for native-language benchmarks and domain-specific evaluations to reveal reasoning failures not captured by standard metrics.

</details>


### [118] [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)

*Carlos Jude G. Maminta, Isaiah Job Enriquez, Deandre Nigel Nunez, Michael B. Dela Fuente*

**Main category:** cs.CL

**Keywords:** Filipino language model, Natural Language Processing, Low-Rank Adaptation, NLP tasks, Machine Learning

**Relevance Score:** 5

**TL;DR:** FiLLM is a Filipino-optimized large language model enhancing NLP in the Filipino language, leveraging LoRA fine-tuning for memory efficiency.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance natural language processing capabilities in the Filipino language through an optimized model.

**Method:** FiLLM was developed based on the SeaLLM-7B 2.5 model with LoRA fine-tuning and trained on diverse Filipino datasets for various NLP tasks.

**Key Contributions:**

	1. Development of FiLLM for Filipino NLP tasks
	2. Use of Low-Rank Adaptation for memory efficiency
	3. Performance evaluation against the CalamanCy model

**Result:** Performance comparisons with the CalamanCy model showed that Calamancy outperformed FiLLM in several NLP tasks, suggesting its superior linguistic comprehension.

**Limitations:** FiLLM did not outperform CalamanCy in several key performance metrics.

**Conclusion:** The study contributes to Filipino NLP advancements by offering an efficient and scalable language model for local linguistic needs.

**Abstract:** This study presents FiLLM, a Filipino-optimized large language model, designed to enhance natural language processing (NLP) capabilities in the Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining task-specific performance. The model was trained and evaluated on diverse Filipino datasets to address key NLP tasks, including Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization. Performance comparisons with the CalamanCy model were conducted using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap metrics. Results indicate that Calamancy outperforms FILLM in several aspects, demonstrating its effectiveness in processing Filipino text with improved linguistic comprehension and adaptability. This research contributes to the advancement of Filipino NLP applications by providing an optimized, efficient, and scalable language model tailored for local linguistic needs.

</details>


### [119] [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization](https://arxiv.org/abs/2505.19000)

*Yunxin Li, Xinyu Chen, Zitao Li, Zhenyu Liu, Longyue Wang, Wenhan Luo, Baotian Hu, Min Zhang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Video Large Language Models, Policy Optimization

**Relevance Score:** 7

**TL;DR:** VerIPO is a new method for improving video LLMs' reasoning capabilities by efficiently combining reinforcement learning strategies to generate high-quality reasoning chains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Reinforcement Fine-Tuning methods for video LLMs suffer from data preparation issues and unstable performance, hindering effective video reasoning.

**Method:** VerIPO employs a Verifier-guided Iterative Policy Optimization process, incorporating a Rollout-Aware Verifier to produce better contrastive data for training video LLMs.

**Key Contributions:**

	1. Introduction of VerIPO for training video LLMs
	2. Use of Rollout-Aware Verifier to create high-quality training data
	3. Demonstrated superior optimization speed and reasoning performance over existing methods

**Result:** Experimental results show that VerIPO optimizes video LLMs 7x faster than standard methods and achieves superior performance in generating long, consistent reasoning chains.

**Limitations:** 

**Conclusion:** VerIPO provides a novel framework that combines the strengths of GRPO and DPO, resulting in more effective training and enhanced reasoning capabilities of video LLMs.

**Abstract:** Applying Reinforcement Learning (RL) to Video Large Language Models (Video-LLMs) shows significant promise for complex video reasoning. However, popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group Relative Policy Optimization (GRPO), are limited by data preparation bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the quality of long chain-of-thoughts (CoTs) and downstream performance.To address these limitations, we propose VerIPO, a Verifier-guided Iterative Policy Optimization method designed to gradually improve video LLMs' capacity for generating deep, long-term reasoning chains. The core component is Rollout-Aware Verifier, positioned between the GRPO and Direct Preference Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop. This verifier leverages small LLMs as a judge to assess the reasoning logic of rollouts, enabling the construction of high-quality contrastive data, including reflective and contextually consistent CoTs. These curated preference samples drive the efficient DPO stage (7x faster than GRPO), leading to marked improvements in reasoning chain quality, especially in terms of length and contextual consistency. This training loop benefits from GRPO's expansive search and DPO's targeted optimization. Experimental results demonstrate: 1) Significantly faster and more effective optimization compared to standard GRPO variants, yielding superior performance; 2) Our trained models exceed the direct inference of large-scale instruction-tuned Video-LLMs, producing long and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long reasoning models (e.g., Video-R1), highlighting its effectiveness and stability.

</details>


### [120] [CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language](https://arxiv.org/abs/2505.19018)

*Md. Mithun Hossain, Md. Shakil Hossain, Sudipto Chaki, Md. Rajib Hossain, Md. Saifur Rahman, A. B. M. Shawkat Ali*

**Main category:** cs.CL

**Keywords:** Aspect-Based Sentiment Analysis, Bengali, Cross-attention, Graph convolutional networks, Natural language processing

**Relevance Score:** 4

**TL;DR:** This paper presents CrosGrpsABS, a hybrid framework for Aspect-Based Sentiment Analysis (ABSA) in Bengali, addressing challenges in low-resource languages by leveraging cross-attention mechanisms and graph convolutional networks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance aspect-level sentiment classification in low-resource languages, specifically Bengali, which faces a scarcity of annotated datasets and language-specific tools.

**Method:** CrosGrpsABS employs bidirectional cross-attention between syntactic and semantic graphs, integrating transformer-based contextual embeddings with graph convolutional networks derived from syntactic dependency parsing and semantic similarity calculations.

**Key Contributions:**

	1. Introduction of CrosGrpsABS for low-resource language ABSA
	2. Integration of cross-attention mechanisms with graph convolutional networks
	3. Demonstrated state-of-the-art performance on Bengali and English datasets.

**Result:** CrosGrpsABS demonstrates significant performance gains on Bengali ABSA datasets and outperforms existing models on the SemEval 2014 Task 4 dataset, achieving a notable F1-score increase in the Restaurant and Laptop domains.

**Limitations:** 

**Conclusion:** The hybrid approach of CrosGrpsABS effectively fuses syntactic and semantic information, leading to superior sentiment classification in both low- and high-resource environments.

**Abstract:** Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural language processing, offering fine-grained insights into opinions expressed in text. While existing research has largely focused on resource-rich languages like English which leveraging large annotated datasets, pre-trained models, and language-specific tools. These resources are often unavailable for low-resource languages such as Bengali. The ABSA task in Bengali remains poorly explored and is further complicated by its unique linguistic characteristics and a lack of annotated data, pre-trained models, and optimized hyperparameters. To address these challenges, this research propose CrosGrpsABS, a novel hybrid framework that leverages bidirectional cross-attention between syntactic and semantic graphs to enhance aspect-level sentiment classification. The CrosGrpsABS combines transformerbased contextual embeddings with graph convolutional networks, built upon rule-based syntactic dependency parsing and semantic similarity computations. By employing bidirectional crossattention, the model effectively fuses local syntactic structure with global semantic context, resulting in improved sentiment classification performance across both low- and high-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali ABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The CrosGrpsABS consistently outperforms existing approaches, achieving notable improvements, including a 0.93% F1-score increase for the Restaurant domain and a 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark.

</details>


### [121] [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)

*Mahdi Nikdan, Vincent Cohen-Addad, Dan Alistarh, Vahab Mirrokni*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Selection, Influence Distillation, Machine Learning, Instruction Tuning

**Relevance Score:** 9

**TL;DR:** The paper presents Influence Distillation, a new framework for data selection in training Large Language Models, which uses second-order information to assign optimal weights to training samples, ensuring efficient fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effective data selection is crucial for training performance in Large Language Models, and current methods lack a systematic approach to optimize this process.

**Method:** Influence Distillation employs second-order information to compute model-specific weights for training samples, using a landmark-based approximation to scale the computation efficiently.

**Key Contributions:**

	1. Introduction of Influence Distillation framework for data selection
	2. Optimal weighting of training samples using second-order information
	3. Landmark-based approximation for efficient scalability

**Result:** The method achieves comparable or superior performance to state-of-the-art techniques while enabling up to 3.5 times faster selection of training data.

**Limitations:** 

**Conclusion:** Influence Distillation provides a mathematically grounded and efficient data selection method that improves training process for various models and datasets in LLM fine-tuning.

**Abstract:** Effective data selection is critical for efficient training of modern Large Language Models (LLMs). This paper introduces Influence Distillation, a novel, mathematically-justified framework for data selection that employs second-order information to optimally weight training samples. By distilling each sample's influence on a target distribution, our method assigns model-specific weights that are used to select training data for LLM fine-tuning, guiding it toward strong performance on the target domain. We derive these optimal weights for both Gradient Descent and Adam optimizers. To ensure scalability and reduce computational cost, we propose a $\textit{landmark-based approximation}$: influence is precisely computed for a small subset of "landmark" samples and then efficiently propagated to all other samples to determine their weights. We validate Influence Distillation by applying it to instruction tuning on the Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU, across several models from the Llama and Qwen families. Experiments show that Influence Distillation matches or outperforms state-of-the-art performance while achieving up to $3.5\times$ faster selection.

</details>


### [122] [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)

*Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, Bernard Ghanem, George Turkiyyah*

**Main category:** cs.CL

**Keywords:** large language models, safety guidelines, fine-tuning, abliteration attack, ethical AI

**Relevance Score:** 8

**TL;DR:** This paper addresses the vulnerability of large language models to an attack called abliteration that undermines their ability to refuse harmful instructions, proposing a defense through the creation of an extended-refusal dataset for fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To combat the abliteration attack that compromises the refusal behavior of large language models, enabling the generation of unethical content.

**Method:** The authors constructed an extended-refusal dataset containing harmful prompts paired with justifications for refusal, and fine-tuned Llama-2-7B-Chat and Qwen2.5-Instruct models on this dataset.

**Key Contributions:**

	1. Development of an extended-refusal dataset for model fine-tuning
	2. Demonstration of the effectiveness of fine-tuned models against abliteration attacks
	3. Balancing safety compliance with model performance during harmful prompt evaluations

**Result:** The fine-tuned models maintained high refusal rates, decreasing by only 10% compared to baseline models, which experienced a 70-80% drop in refusal rates under the attack.

**Limitations:** 

**Conclusion:** Extended-refusal fine-tuning effectively neutralizes the abliteration attack while maintaining model performance and safety.

**Abstract:** Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance.

</details>


### [123] [UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models](https://arxiv.org/abs/2505.19060)

*Roman Vashurin, Maiya Goloburda, Preslav Nakov, Maxim Panov*

**Main category:** cs.CL

**Keywords:** uncertainty quantification, large language models, debiasing, post-hoc method, length normalization

**Relevance Score:** 9

**TL;DR:** The paper proposes UNCERTAINTY-LINE, a debiasing method for uncertainty quantification of LLM outputs that corrects for output length bias.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The reliability of outputs from large language models (LLMs) is crucial, yet existing uncertainty quantification (UQ) methods may introduce biases related to output length.

**Method:** UNCERTAINTY-LINE regresses uncertainty scores on output length and uses residuals for length-invariant uncertainty estimates. It is a post-hoc and model-agnostic approach applicable to various UQ measures.

**Key Contributions:**

	1. Introduction of UNCERTAINTY-LINE for debiasing uncertainty estimates
	2. Demonstration of effectiveness across machine translation, summarization, and question-answering
	3. Model-agnostic applicability for various UQ measures

**Result:** UNCERTAINTY-LINE consistently yields improved uncertainty estimates over nominally length-normalized UQ methods across multiple tasks and models.

**Limitations:** 

**Conclusion:** The proposed method enhances the accuracy of uncertainty quantification for LLM outputs by addressing length bias effectively.

**Abstract:** Large Language Models (LLMs) have become indispensable tools across various applications, making it more important than ever to ensure the quality and the trustworthiness of their outputs. This has led to growing interest in uncertainty quantification (UQ) methods for assessing the reliability of LLM outputs. Many existing UQ techniques rely on token probabilities, which inadvertently introduces a bias with respect to the length of the output. While some methods attempt to account for this, we demonstrate that such biases persist even in length-normalized approaches. To address the problem, here we propose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing procedure that regresses uncertainty scores on output length and uses the residuals as corrected, length-invariant estimates. Our method is post-hoc, model-agnostic, and applicable to a range of UQ measures. Through extensive evaluation on machine translation, summarization, and question-answering tasks, we demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally length-normalized UQ methods uncertainty estimates across multiple metrics and models.

</details>


### [124] [Towards Harmonized Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2505.19073)

*Rui Li, Jing Long, Muge Qi, Heming Xia, Lei Sha, Peiyi Wang, Zhifang Sui*

**Main category:** cs.CL

**Keywords:** Uncertainty Estimation, Large Language Models, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents CUE, a method for improving uncertainty estimation in large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability of large language model generations by quantifying uncertainty effectively.

**Method:** CUE utilizes a lightweight model trained on data aligned with the performance of the target LLM to adjust its uncertainty scores.

**Key Contributions:**

	1. Introduction of CUE for adjusting uncertainty scores
	2. Demonstrated improvements over existing uncertainty estimation methods
	3. Empirical analysis of the effectiveness across diverse models and tasks

**Result:** CUE shows consistent improvements of up to 60% in uncertainty estimation across various models and tasks compared to existing methods.

**Limitations:** 

**Conclusion:** CUE provides a robust framework for more accurate uncertainty estimation in the deployment of LLMs.

**Abstract:** To facilitate robust and trustworthy deployment of large language models (LLMs), it is essential to quantify the reliability of their generations through uncertainty estimation. While recent efforts have made significant advancements by leveraging the internal logic and linguistic features of LLMs to estimate uncertainty scores, our empirical analysis highlights the pitfalls of these methods to strike a harmonized estimation between indication, balance, and calibration, which hinders their broader capability for accurate uncertainty estimation. To address this challenge, we propose CUE (Corrector for Uncertainty Estimation): A straightforward yet effective method that employs a lightweight model trained on data aligned with the target LLM's performance to adjust uncertainty scores. Comprehensive experiments across diverse models and tasks demonstrate its effectiveness, which achieves consistent improvements of up to 60% over existing methods.

</details>


### [125] [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)

*Benjamin Clavié, Florian Brand*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Reading Comprehension, Multimodal Benchmarking

**Relevance Score:** 8

**TL;DR:** Introduction of ReadBench, a benchmark for evaluating the reading comprehension abilities of Large Vision-Language Models (VLMs) on text-rich images.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited assessment of VLMs' performance in reading and reasoning about text-rich images, despite their advancements in visual comprehension.

**Method:** ReadBench transposes established text-only benchmark contexts into images of text while maintaining textual prompts and questions, allowing for evaluation of VLM capabilities.

**Key Contributions:**

	1. Introduction of the ReadBench benchmark for VLM evaluation
	2. Demonstration of performance variations based on input length
	3. Insights on the limitations of multimodal reasoning in VLMs

**Result:** Evaluation of leading VLMs using ReadBench demonstrated minimal performance degradation on short text-image inputs, but significant decline on longer, multi-page contexts. Text resolution was found to have negligible effects on performance.

**Limitations:** The study primarily focuses on text-rich images and does not encompass other multimodal capabilities of VLMs.

**Conclusion:** Improvements are needed in VLMs for reasoning over extensive textual content presented visually, which is crucial for practical applications.

**Abstract:** Recent advancements in Large Vision-Language Models (VLMs), have greatly enhanced their capability to jointly process text and images. However, despite extensive benchmarks evaluating visual comprehension (e.g., diagrams, color schemes, OCR tasks...), there is limited assessment of VLMs' ability to read and reason about text-rich images effectively. To fill this gap, we introduce ReadBench, a multimodal benchmark specifically designed to evaluate the reading comprehension capabilities of VLMs. ReadBench transposes contexts from established text-only benchmarks into images of text while keeping textual prompts and questions intact. Evaluating leading VLMs with ReadBench, we find minimal-but-present performance degradation on short, text-image inputs, while performance sharply declines for longer, multi-page contexts. Our experiments further reveal that text resolution has negligible effects on multimodal performance. These findings highlight needed improvements in VLMs, particularly their reasoning over visually presented extensive textual content, a capability critical for practical applications. ReadBench is available at https://github.com/answerdotai/ReadBench .

</details>


### [126] [ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning](https://arxiv.org/abs/2505.19100)

*Yeyuan Wang, Dehong Gao, Rujiao Long, Lei Yi, Linbo Jin, Libin Yang, Xiaoyan Cai*

**Main category:** cs.CL

**Keywords:** Direct Preference Optimization, Adaptive Sentence-level Optimization, Multimodal Models

**Relevance Score:** 8

**TL;DR:** This paper introduces Adaptive Sentence-level Preference Optimization (ASPO) to improve preference alignment in multimodal models through sentence-level evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Direct Preference Optimization (DPO) methods do not account for fine-grained correctness in multimodal interactions, leading to suboptimal alignments of large language models (LLMs).

**Method:** The authors propose ASPO, which dynamically calculates adaptive rewards at the sentence level based on model predictions to enhance preference optimization of multimodal outputs.

**Key Contributions:**

	1. Introduction of Adaptive Sentence-level Preference Optimization (ASPO)
	2. Improvement of multimodal model performance with sentence-level preference evaluation
	3. Reduction of the reliance on additional models or parameters for optimization

**Result:** ASPO significantly improves alignment and overall performance of multimodal models compared to traditional DPO.

**Limitations:** 

**Conclusion:** The study demonstrates that incorporating sentence-level evaluation allows for more precise preference optimization, enhancing model performance without additional complexity.

**Abstract:** Direct Preference Optimization (DPO) has gained significant attention for its simplicity and computational efficiency in aligning large language models (LLMs). Recent advancements have extended DPO to multimodal scenarios, achieving strong performance. However, traditional DPO relies on binary preference optimization, rewarding or penalizing entire responses without considering fine-grained segment correctness, leading to suboptimal solutions. The root of this issue lies in the absence of fine-grained supervision during the optimization process. To address this, we propose Adaptive Sentence-level Preference Optimization (ASPO), which evaluates individual sentences for more precise preference optimization. By dynamically calculating adaptive rewards at the sentence level based on model predictions, ASPO enhances response content assessment without additional models or parameters. This significantly improves the alignment of multimodal features. Extensive experiments show that ASPO substantially enhances the overall performance of multimodal models.

</details>


### [127] [WHISTRESS: Enriching Transcriptions with Sentence Stress Detection](https://arxiv.org/abs/2505.19103)

*Iddo Yosha, Dorin Shteyman, Yossi Adi*

**Main category:** cs.CL

**Keywords:** sentence stress detection, synthetic data, automatic transcription

**Relevance Score:** 6

**TL;DR:** WHISTRESS is a novel method for detecting sentence stress in spoken language, trained on synthetic data, showing strong performance and zero-shot generalization.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance transcription systems by integrating sentence stress detection, crucial for conveying speaker intent.

**Method:** WHISTRESS, an alignment-free approach, is trained on TINYSTRESS-15K, a synthetic dataset created through an automated process, and evaluated against competitive baselines.

**Key Contributions:**

	1. Introduction of WHISTRESS for sentence stress detection
	2. Creation of TINYSTRESS-15K, a scalable synthetic dataset
	3. Achievement of zero-shot generalization in stress detection

**Result:** WHISTRESS outperforms existing methods and shows strong zero-shot generalization across diverse benchmarks without requiring additional input priors.

**Limitations:** 

**Conclusion:** The method demonstrates the potential of using synthetic data for effective sentence stress detection in automatic transcription systems.

**Abstract:** Spoken language conveys meaning not only through words but also through intonation, emotion, and emphasis. Sentence stress, the emphasis placed on specific words within a sentence, is crucial for conveying speaker intent and has been extensively studied in linguistics. In this work, we introduce WHISTRESS, an alignment-free approach for enhancing transcription systems with sentence stress detection. To support this task, we propose TINYSTRESS-15K, a scalable, synthetic training data for the task of sentence stress detection which resulted from a fully automated dataset creation process. We train WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive baselines. Our results show that WHISTRESS outperforms existing methods while requiring no additional input priors during training or inference. Notably, despite being trained on synthetic data, WHISTRESS demonstrates strong zero-shot generalization across diverse benchmarks. Project page: https://pages.cs.huji.ac.il/adiyoss-lab/whistress.

</details>


### [128] [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)

*Yongheng Zhang, Xu Liu, Ruoxi Zhou, Qiguang Chen, Hao Fei, Wenpeng Lu, Libo Qin*

**Main category:** cs.CL

**Keywords:** hallucinations, large language models, cross-lingual, cross-modal, benchmark

**Relevance Score:** 9

**TL;DR:** Introduction of a novel benchmark to assess hallucination issues in large language models (LLMs) across cross-lingual and cross-modal scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in existing studies focusing solely on single scenarios of hallucinations in LLMs, either cross-lingual or cross-modal, by introducing a joint evaluation framework.

**Method:** Development of the Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) that evaluates LLMs in scenarios involving both cross-lingual and cross-modal hallucinations.

**Key Contributions:**

	1. Establishment of a new benchmark (CCHall) for assessing LLM hallucinations in dual scenarios.
	2. Comprehensive evaluation revealing struggles of existing LLMs with cross-lingual and cross-modal challenges.
	3. Potential to significantly impact the deployment of LLMs in real-world applications.

**Result:** Current mainstream open-source and closed-source LLMs show significant struggles with the challenges presented by the CCHall benchmark.

**Limitations:** Limited to the assessment of hallucination issues and does not cover other evaluation aspects of LLMs.

**Conclusion:** CCHall can serve as a valuable resource for evaluating LLM capabilities in joint cross-lingual and cross-modal scenarios, helping to advance their real-world applications.

**Abstract:** Investigating hallucination issues in large language models (LLMs) within cross-lingual and cross-modal scenarios can greatly advance the large-scale deployment in real-world applications. Nevertheless, the current studies are limited to a single scenario, either cross-lingual or cross-modal, leaving a gap in the exploration of hallucinations in the joint cross-lingual and cross-modal scenarios. Motivated by this, we introduce a novel joint Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this gap. Specifically, CCHall simultaneously incorporates both cross-lingual and cross-modal hallucination scenarios, which can be used to assess the cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a comprehensive evaluation on CCHall, exploring both mainstream open-source and closed-source LLMs. The experimental results highlight that current LLMs still struggle with CCHall. We hope CCHall can serve as a valuable resource to assess LLMs in joint cross-lingual and cross-modal scenarios.

</details>


### [129] [Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering](https://arxiv.org/abs/2505.19112)

*Zheng Chu, Huiming Fan, Jingchang Chen, Qianyu Wang, Mingda Yang, Jiafeng Liang, Zhongjie Wang, Hao Li, Guo Tang, Ming Liu, Bing Qin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-hop Reasoning, Self-Critique, Iterative Reasoning, Question Decomposition

**Relevance Score:** 9

**TL;DR:** This paper presents Self-Critique Guided Iterative Reasoning (SiGIR) to enhance multi-hop reasoning in LLMs by incorporating self-evaluation during the reasoning process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of multi-hop reasoning in large language models by addressing issues related to iterative retrieval and intermediate reasoning guidance.

**Method:** The proposed SiGIR model employs self-critique feedback during iterative reasoning processes, allowing it to decompose questions and evaluate its own reasoning steps.

**Key Contributions:**

	1. Introduction of self-critique feedback in iterative reasoning for LLMs.
	2. End-to-end training that allows question decomposition and self-evaluation of reasoning steps.
	3. Demonstration of an 8.6% improvement over previous SOTA methods in multi-hop reasoning tasks.

**Result:** The SiGIR approach outperformed the previous state-of-the-art (SOTA) by 8.6% on three multi-hop reasoning datasets, demonstrating significant improvements in addressing complex reasoning problems.

**Limitations:** 

**Conclusion:** SiGIR not only enhances reasoning accuracy through self-evaluation but also provides valuable insights for future research in multi-hop reasoning in LLMs.

**Abstract:** Although large language models (LLMs) have demonstrated remarkable reasoning capabilities, they still face challenges in knowledge-intensive multi-hop reasoning. Recent work explores iterative retrieval to address complex problems. However, the lack of intermediate guidance often results in inaccurate retrieval and flawed intermediate reasoning, leading to incorrect reasoning. To address these, we propose Self-Critique Guided Iterative Reasoning (SiGIR), which uses self-critique feedback to guide the iterative reasoning process. Specifically, through end-to-end training, we enable the model to iteratively address complex problems via question decomposition. Additionally, the model is able to self-evaluate its intermediate reasoning steps. During iterative reasoning, the model engages in branching exploration and employs self-evaluation to guide the selection of promising reasoning trajectories. Extensive experiments on three multi-hop reasoning datasets demonstrate the effectiveness of our proposed method, surpassing the previous SOTA by $8.6\%$. Furthermore, our thorough analysis offers insights for future research. Our code, data, and models are available at Github: https://github.com/zchuz/SiGIR-MHQA.

</details>


### [130] [Controlling Language Confusion in Multilingual LLMs](https://arxiv.org/abs/2505.19116)

*Nahyun Lee, Yeongseo Woo, Hyunwoo Ko, Guijin Son*

**Main category:** cs.CL

**Keywords:** large language models, language confusion, supervised fine-tuning, ORPO, low-resource settings

**Relevance Score:** 7

**TL;DR:** This paper addresses the issue of language confusion in large language models, proposing a method to mitigate it through the use of penalty terms alongside supervised fine-tuning (SFT).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Language confusion in large language models adversely affects user experience, especially in low-resource language settings.

**Method:** The authors analyze the loss trajectories during pretraining and propose ORPO, a method that adds penalties for unwanted output styles to standard supervised fine-tuning.

**Key Contributions:**

	1. Identification of the limitations of conventional supervised fine-tuning regarding language confusion.
	2. Introduction of ORPO as a solution to suppress unwanted language mixing.
	3. Empirical evidence demonstrating the effectiveness of ORPO in maintaining model performance while reducing confusion.

**Result:** ORPO effectively reduces language-confused generations and improves performance without degrading overall model effectiveness.

**Limitations:** The study primarily focuses on low-resource settings and may not fully generalize to high-resource contexts.

**Conclusion:** Integrating penalty terms into the training process can significantly alleviate language confusion in low-resource settings.

**Abstract:** Large language models often suffer from language confusion, a phenomenon where responses are partially or entirely generated in unintended languages. This can critically impact user experience in low-resource settings. We hypothesize that conventional supervised fine-tuning exacerbates this issue because the softmax objective focuses probability mass only on the single correct token but does not explicitly penalize cross-lingual mixing. Interestingly, by observing loss trajectories during the pretraining phase, we observe that models fail to learn to distinguish between monolingual and language-confused text. Additionally, we find that ORPO, which adds penalties for unwanted output styles to standard SFT, effectively suppresses language-confused generations even at high decoding temperatures without degrading overall model performance. Our findings suggest that incorporating appropriate penalty terms can mitigate language confusion in low-resource settings with limited data.

</details>


### [131] [Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models](https://arxiv.org/abs/2505.19121)

*Seunguk Yu, Juhwan Choi, Youngbin Kim*

**Main category:** cs.CL

**Keywords:** ethical biases, large language models, multilingual dataset, social biases, human rights

**Relevance Score:** 9

**TL;DR:** This paper investigates ethical biases in large language models (LLMs) using a newly introduced dataset while highlighting the influence of language-specific distinctions on these biases.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To validate and compare the existence of ethical biases in LLMs across various languages and sensitive topics, particularly those identified globally by Human Rights organizations.

**Method:** A new dataset called the Multilingual Sensitive Questions & Answers Dataset (MSQAD) was constructed from Human Rights Watch articles, consisting of socially sensitive questions and responses in multiple languages. Statistical hypothesis tests were employed to examine biases across these language responses.

**Key Contributions:**

	1. Introduction of the Multilingual Sensitive Questions & Answers Dataset (MSQAD) for analyzing biases in LLMs.
	2. Empirical evidence showing the presence of ethical biases across multiple languages and LLMs.
	3. Statistical methodology validating the existence of language-specific biases in responses.

**Result:** The analysis revealed that biases were detected across different languages and topics, indicating that ethical biases are often inherent in LLM responses, with rejection of null hypotheses in most cases.

**Limitations:** Study focuses on a limited set of 17 sensitive topics and the dataset is restricted to responses generated from specific LLMs.

**Conclusion:** The study illustrates widespread ethical biases in LLMs, underscoring the need for awareness and further study of cross-language biases, with MSQAD being a resource for future research.

**Abstract:** Despite the recent strides in large language models, studies have underscored the existence of social biases within these systems. In this paper, we delve into the validation and comparison of the ethical biases of LLMs concerning globally discussed and potentially sensitive topics, hypothesizing that these biases may arise from language-specific distinctions. Introducing the Multilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news articles from Human Rights Watch covering 17 topics, and generated socially sensitive questions along with corresponding responses in multiple languages. We scrutinized the biases of these responses across languages and topics, employing two statistical hypothesis tests. The results showed that the null hypotheses were rejected in most cases, indicating biases arising from cross-language differences. It demonstrates that ethical biases in responses are widespread across various languages, and notably, these biases were prevalent even among different LLMs. By making the proposed MSQAD openly available, we aim to facilitate future research endeavors focused on examining cross-language biases in LLMs and their variant models.

</details>


### [132] [MMATH: A Multilingual Benchmark for Mathematical Reasoning](https://arxiv.org/abs/2505.19126)

*Wenyang Luo, Wayne Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** multilingual reasoning, large language models, benchmark, complex reasoning, machine learning

**Relevance Score:** 7

**TL;DR:** The paper introduces MMATH, a benchmark for multilingual complex reasoning in mathematics, highlighting performance disparities in advanced models across languages and proposing strategies for improvement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the capabilities of large reasoning models in multilingual complex reasoning, which remain underexplored despite advances in simpler tasks.

**Method:** The MMATH benchmark was developed, encompassing 374 math problems across 10 languages, and performance of models was evaluated with various prompting and training strategies.

**Key Contributions:**

	1. Introduction of the MMATH benchmark for multilingual math reasoning.
	2. Identified performance disparities and off-target language issues in advanced reasoning models.
	3. Proposed strategies to enhance multilingual reasoning capabilities in large language models.

**Result:** Advanced models like DeepSeek R1 showed significant performance disparities across languages, and off-target language issues were identified.

**Limitations:** The findings are based on existing models and may not be generalizable to all reasoning tasks in multilingual contexts.

**Conclusion:** Strategies including prompting in English while answering in target languages improve performance and consistency in multilingual reasoning tasks.

**Abstract:** The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has significantly advanced complex reasoning tasks. However, their capabilities in multilingual complex reasoning remain underexplored, with existing efforts largely focused on simpler tasks like MGSM. To address this gap, we introduce MMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality math problems across 10 typologically diverse languages. Using MMATH, we observe that even advanced models like DeepSeek R1 exhibit substantial performance disparities across languages and suffer from a critical off-target issue-generating responses in unintended languages. To address this, we explore strategies including prompting and training, demonstrating that reasoning in English and answering in target languages can simultaneously enhance performance and preserve target-language consistency. Our findings offer new insights and practical strategies for advancing the multilingual reasoning capabilities of large language models. Our code and data could be found at https://github.com/RUCAIBox/MMATH.

</details>


### [133] [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)

*Jin Zhang, Fan Gao, Linyu Li, Yongbin Yu, Xiangxiang Wang, Nyima Tashi, Gadeng Luosang*

**Main category:** cs.CL

**Keywords:** Multilingual NER, Large language models, Dynamic LoRA, Cross-granularity knowledge augmentation, Hierarchical prompting

**Relevance Score:** 8

**TL;DR:** RetrieveAll is a universal multilingual named entity recognition framework that addresses language interference by decoupling features and employing a novel prompting method to enhance performance, particularly for low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant performance gap in named entity recognition for low- and medium-resource languages compared to high-resource languages, largely due to language interference in existing multilingual methods.

**Method:** The paper proposes RetrieveAll, which utilizes dynamic LoRA to decouple task-specific features and introduces a cross-granularity knowledge augmented method with hierarchical prompting for better knowledge injection and adaptability.

**Key Contributions:**

	1. Introduction of RetrieveAll universal multilingual NER framework.
	2. Decoupling of task-specific features across languages.
	3. Implementation of a hierarchical prompting mechanism for knowledge injection.

**Result:** RetrieveAll outperforms existing multilingual NER baselines, achieving a 12.1% average F1 improvement on the PAN-X dataset.

**Limitations:** The framework's performance on extremely low-resource languages is yet to be evaluated.

**Conclusion:** The proposed framework significantly enhances multilingual NER performance, especially for low-resource languages, and avoids the scalability issues of existing methods by mitigating language interference effectively.

**Abstract:** The rise of large language models has led to significant performance breakthroughs in named entity recognition (NER) for high-resource languages, yet there remains substantial room for improvement in low- and medium-resource languages. Existing multilingual NER methods face severe language interference during the multi-language adaptation process, manifested in feature conflicts between different languages and the competitive suppression of low-resource language features by high-resource languages. Although training a dedicated model for each language can mitigate such interference, it lacks scalability and incurs excessive computational costs in real-world applications. To address this issue, we propose RetrieveAll, a universal multilingual NER framework based on dynamic LoRA. The framework decouples task-specific features across languages and demonstrates efficient dynamic adaptability. Furthermore, we introduce a cross-granularity knowledge augmented method that fully exploits the intrinsic potential of the data without relying on external resources. By leveraging a hierarchical prompting mechanism to guide knowledge injection, this approach advances the paradigm from "prompt-guided inference" to "prompt-driven learning." Experimental results show that RetrieveAll outperforms existing baselines; on the PAN-X dataset, it achieves an average F1 improvement of 12.1 percent.

</details>


### [134] [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)

*Xuyang Liu, Zichen Wen, Shaobo Wang, Junjie Chen, Zhishan Tao, Yubo Wang, Xiangqi Jin, Chang Zou, Yiyu Wang, Chenfei Liao, Xu Zheng, Honggang Chen, Weijia Li, Xuming Hu, Conghui He, Linfeng Zhang*

**Main category:** cs.CL

**Keywords:** Token Compression, AI Efficiency, Long-context AI, Data-centric Compression, Model Efficiency

**Relevance Score:** 8

**TL;DR:** This position paper advocates for a shift in focus from model-centric to data-centric compression in AI, emphasizing token compression as a means to enhance efficiency in managing long-context data across various domains.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** As the cost of self-attention in long-context AI becomes a bottleneck, there's a pressing need to explore efficient data handling methods rather than simply increasing model size.

**Method:** The authors establish a unified mathematical framework for analyzing token compression and explore its implications across multiple domains, reviewing existing strategies and their effectiveness.

**Key Contributions:**

	1. Proposes a new focus on token compression for AI efficiency
	2. Establishes a mathematical framework for understanding model efficiency strategies
	3. Reviews challenges and future directions in token compression research.

**Result:** Token compression is identified as a promising solution to reduce the number of tokens during training and inference, enhancing overall AI efficiency and performance.

**Limitations:** 

**Conclusion:** Token compression represents a critical paradigm shift necessary to tackle the challenges posed by increasing context lengths in language models and other AI systems.

**Abstract:** The rapid advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on model-centric scaling through increasing parameter counts from millions to hundreds of billions to drive performance gains. However, as we approach hardware limits on model size, the dominant computational bottleneck has fundamentally shifted to the quadratic cost of self-attention over long token sequences, now driven by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \textbf{we argue that the focus of research for efficient AI is shifting from model-centric compression to data-centric compression}. We position token compression as the new frontier, which improves AI efficiency via reducing the number of tokens during model training or inference. Through comprehensive analysis, we first examine recent developments in long-context AI across various domains and establish a unified mathematical framework for existing model efficiency strategies, demonstrating why token compression represents a crucial paradigm shift in addressing long-context overhead. Subsequently, we systematically review the research landscape of token compression, analyzing its fundamental benefits and identifying its compelling advantages across diverse scenarios. Furthermore, we provide an in-depth analysis of current challenges in token compression research and outline promising future directions. Ultimately, our work aims to offer a fresh perspective on AI efficiency, synthesize existing research, and catalyze innovative developments to address the challenges that increasing context lengths pose to the AI community's advancement.

</details>


### [135] [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)

*Firoj Alam, Md Arid Hasan, Shammur Absar Chowdhury*

**Main category:** cs.CL

**Keywords:** Spoken Question Answering, Multilingual LLMs, Speech-based Evaluation

**Relevance Score:** 9

**TL;DR:** The paper presents SpokenNativQA, a pioneering multilingual spoken question-answering dataset for evaluating large language models in real-world conversational settings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in benchmarking LLM performance with multilingual spoken queries and to evaluate their capabilities in real-world conversational contexts.

**Method:** The study introduces a dataset comprising approximately 33,000 spoken questions and answers across multiple languages, benchmarking different ASR systems and LLMs for spoken question answering.

**Key Contributions:**

	1. Introduction of SpokenNativQA dataset for multilingual SQA
	2. Evaluation of LLMs in real-world conversational settings
	3. Benchmarking of various ASR systems alongside LLMs

**Result:** Findings indicate that traditional text-based QA systems face challenges in handling speech variability and linguistic diversity, highlighting the need for improved evaluation metrics in SQA.

**Limitations:** Limited to the languages and contexts represented in the dataset; potential biases in spoken data collection.

**Conclusion:** SpokenNativQA provides a robust resource for the research community to assess LLM performance in speech-based interactions, addressing limitations of current text-based datasets.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance across various disciplines and tasks. However, benchmarking their capabilities with multilingual spoken queries remains largely unexplored. In this study, we introduce SpokenNativQA, the first multilingual and culturally aligned spoken question-answering (SQA) dataset designed to evaluate LLMs in real-world conversational settings. The dataset comprises approximately 33,000 naturally spoken questions and answers in multiple languages, including low-resource and dialect-rich languages, providing a robust benchmark for assessing LLM performance in speech-based interactions. SpokenNativQA addresses the limitations of text-based QA datasets by incorporating speech variability, accents, and linguistic diversity. We benchmark different ASR systems and LLMs for SQA and present our findings. We released the data at (https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental scripts at (https://llmebench.qcri.org/) for the research community.

</details>


### [136] [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)

*Zhuo Liu, Moxin Li, Xun Deng, Qifan Wang, Fuli Feng*

**Main category:** cs.CL

**Keywords:** LLM, de-biasing, evaluation, human evaluations, machine learning

**Relevance Score:** 8

**TL;DR:** The paper presents AGDe-Judge, a framework to reduce teacher preference bias in evaluating LLM-generated responses.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of teacher preference bias when training proxy judge models for evaluating LLM outputs, which can lead to biased evaluation results.

**Method:** AGDe-Judge incorporates an additional assistant model to complement the training data and employs a three-stage framework to debias the training data from both labels and feedbacks.

**Key Contributions:**

	1. Introduction of AGDe-Judge framework
	2. Development of a debiasing strategy using an assistant model
	3. Demonstration of effectiveness across multiple evaluation benchmarks

**Result:** Extensive experiments show that AGDe-Judge significantly reduces teacher preference bias while maintaining strong performance across six evaluation benchmarks.

**Limitations:** 

**Conclusion:** The proposed framework effectively mitigates bias in proxy judge models, improving evaluation quality for LLM-generated responses.

**Abstract:** LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to evaluate the quality of LLM-generated responses, gaining popularity for its cost-effectiveness and strong alignment with human evaluations. However, training proxy judge models using evaluation data generated by powerful teacher models introduces a critical yet previously overlooked issue: teacher preference bias, where the proxy judge model learns a biased preference for responses from the teacher model. To tackle this problem, we propose a novel setting that incorporates an additional assistant model, which is not biased toward the teacher model's responses, to complement the training data. Building on this setup, we introduce AGDe-Judge, a three-stage framework designed to debias from both the labels and feedbacks in the training data. Extensive experiments demonstrate that AGDe-Judge effectively reduces teacher preference bias while maintaining strong performance across six evaluation benchmarks. Code is available at https://github.com/Liuz233/AGDe-Judge.

</details>


### [137] [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)

*Minh Nhat Nguyen, Pradyumna Shyama Prasad*

**Main category:** cs.CL

**Keywords:** Large Language Models, confidence calibration, adversarial debate, self-assessment, dynamic reasoning

**Relevance Score:** 8

**TL;DR:** This study evaluates the confidence calibration of LLMs in a dynamic debate setting, revealing systematic overconfidence and failure to accurately update beliefs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether Large Language Models can adjust their confidence accurately in dynamic adversarial scenarios, building on previous calibration studies in static tasks.

**Method:** Conducted 60 three-round policy debates among ten state-of-the-art LLMs, having them rate their confidence in winning after each round.

**Key Contributions:**

	1. Evaluation of LLM confidence in dynamic settings
	2. Identification of systemic overconfidence patterns
	3. Insights into the reliability of LLM reasoning in adversarial scenarios

**Result:** Found that models exhibited systematic overconfidence, confidence escalation, mutual overestimation, persistent self-debate bias, and misaligned reasoning between private thoughts and public confidence ratings.

**Limitations:** Limited to a controlled debate format and may not generalize to other scenarios or types of LLM outputs.

**Conclusion:** LLMs demonstrate significant issues in self-assessment and belief updating in dynamic settings, raising concerns for their deployment in real-world assistant roles.

**Abstract:** Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed >=75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: models' private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLM outputs are deployed without careful review in assistant roles or agentic settings.

</details>


### [138] [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)

*Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, Pengfei Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, PIR, Chain-of-Thought, Reasoning Optimization, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper introduces PIR, a framework that optimizes reasoning steps in LLMs to enhance test-time efficiency and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the verbosity in reasoning chains of LLMs that negatively impacts computational efficiency during inference.

**Method:** The PIR framework quantitatively evaluates and prunes low-importance functional reasoning steps while retaining essential progressive reasoning components.

**Key Contributions:**

	1. Introduction of the PIR framework for reasoning step optimization
	2. Demonstration of improved accuracy and reduced verbosity in reasoning chains
	3. Generalizability across various model sizes and benchmarks

**Result:** PIR-optimized models show improved accuracy (+0.9% to +6.6%) and reduced token usage (-3% to -41%) on reasoning benchmarks.

**Limitations:** 

**Conclusion:** PIR provides a practical solution for enhancing the efficiency of LLMs in reasoning tasks, maintaining model performance while optimizing resource usage.

**Abstract:** Large language models (LLMs) have demonstrated remarkable reasoning capabilities through test-time scaling approaches, particularly when fine-tuned with chain-of-thought (CoT) data distilled from more powerful large reasoning models (LRMs). However, these reasoning chains often contain verbose elements that mirror human problem-solving, categorized as progressive reasoning (the essential solution development path) and functional elements (verification processes, alternative solution approaches, and error corrections). While progressive reasoning is crucial, the functional elements significantly increase computational demands during test-time inference. We introduce PIR (Perplexity-based Importance Refinement), a principled framework that quantitatively evaluates the importance of each reasoning step based on its impact on answer prediction confidence. PIR systematically identifies and selectively prunes only low-importance functional steps while preserving progressive reasoning components, creating optimized training data that maintains the integrity of the core solution path while reducing verbosity. Models fine-tuned on PIR-optimized data exhibit superior test-time scaling properties, generating more concise reasoning chains while achieving improved accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to -41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond). Our approach demonstrates strong generalizability across different model sizes, data sources, and token budgets, offering a practical solution for deploying reasoning-capable LLMs in scenarios where efficient test-time scaling, response time, and computational efficiency are valuable constraints.

</details>


### [139] [Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection](https://arxiv.org/abs/2505.19191)

*Nursulu Sagimbayeva, Ruveyda Betül Bahçeci, Ingmar Weber*

**Main category:** cs.CL

**Keywords:** inconsistency detection, political statements, natural language processing, large language models, misinformation

**Relevance Score:** 3

**TL;DR:** This paper proposes an inconsistency detection task in political statements and presents a dataset to support NLP research in this area.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of misinformation and enhance public trust in political discourse by enabling the automatic detection of inconsistencies in political statements.

**Method:** The authors introduce the Inconsistency detection task, develop a scale of inconsistency types, and provide a dataset of 698 human-annotated pairs of political statements, with evaluations of Large Language Models on this dataset.

**Key Contributions:**

	1. Introduction of the Inconsistency detection task
	2. Development of a scale of inconsistency types
	3. Creation of a publicly available annotated dataset for political statements

**Result:** LLMs are found to detect inconsistencies as well as humans but struggle with fine-grained inconsistency types, indicating room for improvement.

**Limitations:** None of the models achieved optimal performance on fine-grained inconsistency types due to natural labeling variation.

**Conclusion:** The paper highlights the importance of detecting inconsistencies to support accountability in politics and makes the dataset publicly available for further research.

**Abstract:** Inconsistent political statements represent a form of misinformation. They erode public trust and pose challenges to accountability, when left unnoticed. Detecting inconsistencies automatically could support journalists in asking clarification questions, thereby helping to keep politicians accountable. We propose the Inconsistency detection task and develop a scale of inconsistency types to prompt NLP-research in this direction. To provide a resource for detecting inconsistencies in a political domain, we present a dataset of 698 human-annotated pairs of political statements with explanations of the annotators' reasoning for 237 samples. The statements mainly come from voting assistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland, reflecting real-world political issues. We benchmark Large Language Models (LLMs) on our dataset and show that in general, they are as good as humans at detecting inconsistencies, and might be even better than individual humans at predicting the crowd-annotated ground-truth. However, when it comes to identifying fine-grained inconsistency types, none of the model have reached the upper bound of performance (due to natural labeling variation), thus leaving room for improvement. We make our dataset and code publicly available.

</details>


### [140] [DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](https://arxiv.org/abs/2505.19201)

*Yunhai Hu, Tianhua Xia, Zining Liu, Rahul Raman, Xingyu Liu, Bo Bao, Eric Sather, Vithursan Thangarasa, Sai Qian Zhang*

**Main category:** cs.CL

**Keywords:** speculative decoding, vision-language models, multimodal generation

**Relevance Score:** 7

**TL;DR:** DREAM introduces a novel speculative decoding framework for vision-language models, enhancing efficiency and accuracy in multimodal generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of speculative decoding into vision-language models, which has been underexplored despite its success in large language models.

**Method:** DREAM combines a cross-attention mechanism, adaptive intermediate feature selection, and visual token compression to improve draft model latency and alignment.

**Key Contributions:**

	1. Introduction of a cross-attention mechanism for model alignment
	2. Adaptive intermediate feature selection based on attention entropy
	3. Visual token compression to enhance decoding efficiency

**Result:** Experiments show DREAM achieves up to 3.6x speedup over conventional decoding methods and significantly improves inference throughput compared to existing speculative decoding baselines.

**Limitations:** 

**Conclusion:** DREAM provides efficient and accurate multimodal decoding, setting a new standard for performance in vision-language models with publicly available code.

**Abstract:** Speculative decoding (SD) has emerged as a powerful method for accelerating autoregressive generation in large language models (LLMs), yet its integration into vision-language models (VLMs) remains underexplored. We introduce DREAM, a novel speculative decoding framework tailored for VLMs that combines three key innovations: (1) a cross-attention-based mechanism to inject intermediate features from the target model into the draft model for improved alignment, (2) adaptive intermediate feature selection based on attention entropy to guide efficient draft model training, and (3) visual token compression to reduce draft model latency. DREAM enables efficient, accurate, and parallel multimodal decoding with significant throughput improvement. Experiments across a diverse set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3, demonstrate up to 3.6x speedup over conventional decoding and significantly outperform prior SD baselines in both inference throughput and speculative draft acceptance length across a broad range of multimodal benchmarks. The code is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git

</details>


### [141] [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)

*Richard He Bai, Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly*

**Main category:** cs.CL

**Keywords:** streaming TTS, conversational AI, latency, LLM, text-to-speech

**Relevance Score:** 9

**TL;DR:** SpeakStream is a new streaming TTS system that reduces latency in conversational AI by incrementally generating audio from streaming text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional TTS systems hinder conversational AI due to high latency, making them unsuitable for responsive applications.

**Method:** The system uses a decoder-only architecture and is trained with a next-step prediction loss on interleaved text-speech data to provide incremental audio output.

**Key Contributions:**

	1. Introduction of SpeakStream, a low-latency streaming TTS system
	2. Use of next-step prediction loss for training
	3. Demonstration of state-of-the-art latency performance.

**Result:** SpeakStream outperforms traditional systems in first-token latency while maintaining audio quality.

**Limitations:** 

**Conclusion:** The proposed system demonstrates significant improvements in responsiveness for conversational agents reliant on streaming LLMs.

**Abstract:** The latency bottleneck of traditional text-to-speech (TTS) systems fundamentally hinders the potential of streaming large language models (LLMs) in conversational AI. These TTS systems, typically trained and inferenced on complete utterances, introduce unacceptable delays, even with optimized inference speeds, when coupled with streaming LLM outputs. This is particularly problematic for creating responsive conversational agents where low first-token latency is critical. In this paper, we present SpeakStream, a streaming TTS system that generates audio incrementally from streaming text using a decoder-only architecture. SpeakStream is trained using a next-step prediction loss on interleaved text-speech data. During inference, it generates speech incrementally while absorbing streaming input text, making it particularly suitable for cascaded conversational AI agents where an LLM streams text to a TTS system. Our experiments demonstrate that SpeakStream achieves state-of-the-art latency results in terms of first-token latency while maintaining the quality of non-streaming TTS systems.

</details>


### [142] [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)

*Zonglin Yang, Wanhao Liu, Ben Gao, Yujie Liu, Wei Li, Tong Xie, Lidong Bing, Wanli Ouyang, Erik Cambria, Dongzhan Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Scientific Hypothesis Generation, Combinatorial Optimization

**Relevance Score:** 8

**TL;DR:** Introducing fine-grained scientific hypothesis generation using LLMs, improving from coarse directions to detailed actionable hypotheses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for scientific hypothesis generation using LLMs generate coarse hypotheses lacking necessary detail; there's a need for fine-grained hypothesis discovery.

**Method:** The task is framed as a combinatorial optimization problem, with a hierarchical search method proposed for generating detailed hypotheses based on LLM internal heuristics and an ensemble of LLMs.

**Key Contributions:**

	1. Formal definition of fine-grained hypothesis discovery
	2. Hierarchical search method for hypothesis generation
	3. Empirical validation on expert-annotated benchmark data

**Result:** Empirical evaluations demonstrate that the proposed method consistently outperforms strong baselines on a benchmark of expert-annotated hypotheses from chemistry literature.

**Limitations:** 

**Conclusion:** The hierarchical search approach smooths the reward landscape, enabling better optimization and generation of more useful scientific hypotheses.

**Abstract:** Large language models (LLMs) have shown promise in automating scientific hypothesis generation, yet existing approaches primarily yield coarse-grained hypotheses lacking critical methodological and experimental details. We introduce and formally define the novel task of fine-grained scientific hypothesis discovery, which entails generating detailed, experimentally actionable hypotheses from coarse initial research directions. We frame this as a combinatorial optimization problem and investigate the upper limits of LLMs' capacity to solve it when maximally leveraged. Specifically, we explore four foundational questions: (1) how to best harness an LLM's internal heuristics to formulate the fine-grained hypothesis it itself would judge as the most promising among all the possible hypotheses it might generate, based on its own internal scoring-thus defining a latent reward landscape over the hypothesis space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment with ground-truth hypotheses; (3) whether shaping the reward landscape using an ensemble of diverse LLMs of similar capacity yields better outcomes than defining it with repeated instances of the strongest LLM among them; and (4) whether an ensemble of identical LLMs provides a more reliable reward landscape than a single LLM. To address these questions, we propose a hierarchical search method that incrementally proposes and integrates details into the hypothesis, progressing from general concepts to specific experimental configurations. We show that this hierarchical process smooths the reward landscape and enables more effective optimization. Empirical evaluations on a new benchmark of expert-annotated fine-grained hypotheses from recent chemistry literature show that our method consistently outperforms strong baselines.

</details>


### [143] [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)

*Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin*

**Main category:** cs.CL

**Keywords:** large language models, moral behavior, social dilemmas, ethical alignment, decision-making

**Relevance Score:** 8

**TL;DR:** This paper introduces Moral Behavior in Social Dilemma Simulation (MoralSim) to investigate how large language models (LLMs) behave in social dilemmas when moral imperatives conflict with rewards.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how LLMs navigate moral dilemmas is crucial for ensuring ethical AI behavior, especially in agentic roles with decision-making responsibilities.

**Method:** The study employs MoralSim to evaluate LLMs in the prisoner's dilemma and public goods game with various moral contexts, testing multiple state-of-the-art models across different scenarios.

**Key Contributions:**

	1. Introduction of MoralSim as a simulation for moral behavior in LLMs
	2. Evaluation of LLMs in social dilemmas with ethically-charged contexts
	3. Identification of variability in LLMs' moral responses across different scenarios

**Result:** Results reveal significant variability in LLMs' tendencies for moral behavior based on moral framing and situational dynamics; notably, no model consistently demonstrates moral behavior.

**Limitations:** Limited to specific game structures and models; may not fully generalize to all moral dilemmas or real-world applications.

**Conclusion:** The findings indicate that reliance on LLMs in ethically charged roles requires careful scrutiny due to inconsistencies in their moral decision-making.

**Abstract:** Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner's dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent's "self-interest" may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim.

</details>


### [144] [The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training](https://arxiv.org/abs/2505.19217)

*Weize Chen, Jiarui Yuan, Tailin Jin, Ning Ding, Huimin Chen, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, token efficiency

**Relevance Score:** 8

**TL;DR:** DIET introduces a framework that improves LLM efficiency by reducing token counts during response generation through difficulty-aware training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models often generate overly long responses, leading to inefficiencies. The paper aims to address this issue by introducing a systematic approach to optimize response length based on task difficulty.

**Method:** The DIET framework integrates problem difficulty into the reinforcement learning process by modulating token penalties and conditioning target response lengths on estimated task difficulty.

**Key Contributions:**

	1. Introduction of the DIET framework for difficulty-aware training in LLMs.
	2. Demonstration of a stable implementation of difficulty-aware objectives through Advantage Weighting technique.
	3. Illustration of how DIET improves inference scaling and maintains quality under fixed computational budgets.

**Result:** Experimental results show that DIET significantly reduces token counts while improving reasoning performance, enhancing inference scaling and maintaining quality in response generation.

**Limitations:** 

**Conclusion:** DIET provides an effective framework for creating more efficient and high-performing large language models by appropriately balancing response length and problem difficulty.

**Abstract:** Recent large language models (LLMs) exhibit impressive reasoning but often over-think, generating excessively long responses that hinder efficiency. We introduce DIET ( DIfficulty-AwarE Training), a framework that systematically cuts these "token calories" by integrating on-the-fly problem difficulty into the reinforcement learning (RL) process. DIET dynamically adapts token compression strategies by modulating token penalty strength and conditioning target lengths on estimated task difficulty, to optimize the performance-efficiency trade-off. We also theoretically analyze the pitfalls of naive reward weighting in group-normalized RL algorithms like GRPO, and propose Advantage Weighting technique, which enables stable and effective implementation of these difficulty-aware objectives. Experimental results demonstrate that DIET significantly reduces token counts while simultaneously improving reasoning performance. Beyond raw token reduction, we show two crucial benefits largely overlooked by prior work: (1) DIET leads to superior inference scaling. By maintaining high per-sample quality with fewer tokens, it enables better scaling performance via majority voting with more samples under fixed computational budgets, an area where other methods falter. (2) DIET enhances the natural positive correlation between response length and problem difficulty, ensuring verbosity is appropriately allocated, unlike many existing compression methods that disrupt this relationship. Our analyses provide a principled and effective framework for developing more efficient, practical, and high-performing LLMs.

</details>


### [145] [Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator](https://arxiv.org/abs/2505.19236)

*Qian Cao, Xiting Wang, Yuzhuo Yuan, Yahui Liu, Fang Luo, Ruihua Song*

**Main category:** cs.CL

**Keywords:** creativity evaluation, large language models, dataset, human judgment, automated methods

**Relevance Score:** 8

**TL;DR:** This paper proposes a pairwise-comparison framework for evaluating textual creativity in large language models (LLMs) and introduces a dataset called CreataSet to train a new LLM-based evaluator, CrEval, which outperforms existing methods in aligning with human judgments.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Creativity evaluation for large language models is currently inefficient as it relies on human judgments, and existing automated methods lack generalizability and alignment with human assessment.

**Method:** The paper presents a novel pairwise-comparison framework that utilizes shared contextual instructions for more consistent evaluation of creativity. It also introduces CreataSet, a large-scale dataset for training the evaluation model.

**Key Contributions:**

	1. Introduction of a novel pairwise-comparison framework for creativity evaluation.
	2. Creation of CreataSet, a large-scale dataset for training evaluators.
	3. Development of CrEval, an LLM-based evaluator that surpasses existing methods.

**Result:** CrEval, developed through training on CreataSet, shows superior performance in aligning with human judgments compared to existing evaluation methods.

**Limitations:** 

**Conclusion:** Integrating both human-generated and synthetic data is crucial for the development of robust evaluators. CrEval can effectively enhance the creativity of LLMs, and all associated data and models will be publicly released to aid further research.

**Abstract:** Creativity evaluation remains a challenging frontier for large language models (LLMs). Current evaluations heavily rely on inefficient and costly human judgments, hindering progress in enhancing machine creativity. While automated methods exist, ranging from psychological testing to heuristic- or prompting-based approaches, they often lack generalizability or alignment with human judgment. To address these issues, in this paper, we propose a novel pairwise-comparison framework for assessing textual creativity, leveraging shared contextual instructions to improve evaluation consistency. We introduce CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic creative instruction-response pairs spanning diverse open-domain tasks. Through training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval demonstrates remarkable superiority over existing methods in alignment with human judgments. Experimental results underscore the indispensable significance of integrating both human-generated and synthetic data in training highly robust evaluators, and showcase the practical utility of CrEval in boosting the creativity of LLMs. We will release all data, code, and models publicly soon to support further research.

</details>


### [146] [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)

*Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger*

**Main category:** cs.CL

**Keywords:** Large language models, limitations, research trends, LLM-related papers, multimodality

**Relevance Score:** 9

**TL;DR:** This survey reviews the limitations of large language models (LLMs) from 2022 to 2024, highlighting trends and key areas of research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the growing concerns about the limitations of LLMs, including reasoning failures, hallucinations, and multilingual capabilities.

**Method:** A semi-automated, data-driven approach was used to analyze 250,000 papers, filtering down to 14,648 relevant studies through keyword filtering and advanced classification methods.

**Key Contributions:**

	1. Comprehensive identification of relevant LLM research papers
	2. Quantitative analysis of trends in LLM limitations
	3. Release of a dataset of annotated abstracts

**Result:** LLM-related research has increased significantly, with LLLMs research growing rapidly and reasoning being the most studied limitation.

**Limitations:** 

**Conclusion:** The study provides insights into the evolving landscape of LLM limitations research and offers a dataset of annotated abstracts.

**Abstract:** Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. In this survey, we conduct a data-driven, semi-automated review of research on limitations of LLM (LLLMs) from 2022 to 2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we identify 14,648 relevant papers using keyword filtering, LLM-based classification, validated against expert labels, and topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs research grows even faster, reaching over 30% of LLM papers by late 2024. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2024. We release a dataset of annotated abstracts and a validated methodology, and offer a quantitative view of trends in LLM limitations research.

</details>


### [147] [PATS: Process-Level Adaptive Thinking Mode Switching](https://arxiv.org/abs/2505.19250)

*Yi Wang, Junxiao Liu, Shimao Zhang, Jiajun Chen, Shujian Huang*

**Main category:** cs.CL

**Keywords:** Large-language models, reasoning strategies, process reward models, adaptive thinking, computational efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces Process-Level Adaptive Thinking Mode Switching (PATS) for large-language models to optimize reasoning strategies based on task difficulty, improving accuracy and efficiency in inference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs use a fixed reasoning strategy, leading to inefficiencies due to varying task complexities.

**Method:** PATS utilizes Process Reward Models and Beam Search to dynamically adjust reasoning strategies at each step, allowing for progressive mode switching and penalties for poor performance.

**Key Contributions:**

	1. Introduction of Process-Level Adaptive Thinking Mode Switching (PATS) for LLMs
	2. Integration of Process Reward Models with Beam Search for dynamic reasoning adjustment
	3. Demonstrated high accuracy with moderate computational resource use on benchmarks.

**Result:** Experiments show that PATS achieves high accuracy in mathematical benchmarks while maintaining moderate token usage.

**Limitations:** 

**Conclusion:** The study highlights the importance of adaptive reasoning strategies in LLMs for enhancing computational efficiency without sacrificing accuracy.

**Abstract:** Current large-language models (LLMs) typically adopt a fixed reasoning strategy, either simple or complex, for all questions, regardless of their difficulty. This neglect of variation in task and reasoning process complexity leads to an imbalance between performance and efficiency. Existing methods attempt to implement training-free fast-slow thinking system switching to handle problems of varying difficulty, but are limited by coarse-grained solution-level strategy adjustments. To address this issue, we propose a novel reasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS), which enables LLMs to dynamically adjust their reasoning strategy based on the difficulty of each step, optimizing the balance between accuracy and computational efficiency. Our approach integrates Process Reward Models (PRMs) with Beam Search, incorporating progressive mode switching and bad-step penalty mechanisms. Experiments on diverse mathematical benchmarks demonstrate that our methodology achieves high accuracy while maintaining moderate token usage. This study emphasizes the significance of process-level, difficulty-aware reasoning strategy adaptation, offering valuable insights into efficient inference for LLMs.

</details>


### [148] [Unveiling Dual Quality in Product Reviews: An NLP-Based Approach](https://arxiv.org/abs/2505.19254)

*Rafał Poświata, Marcin Michał Mirończuk, Sławomir Dadas, Małgorzata Grębowiec, Michał Perełkiewicz*

**Main category:** cs.CL

**Keywords:** dual quality, NLP, dataset, multilingual transfer, product quality

**Relevance Score:** 6

**TL;DR:** This paper addresses the dual quality problem in consumer products using NLP techniques by creating a Polish-language dataset and experimenting with various ML models for detection and analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the inconsistent product quality experienced by consumers, particularly when identical products differ across markets.

**Method:** The authors developed a Polish-language dataset with 1,957 reviews focused on dual quality issues, followed by experiments with NLP methods including SetFit, transformer-based encoders, and LLMs for discrepancies detection and robustness verification.

**Key Contributions:**

	1. Creation of a new dataset for detecting dual quality issues in Polish language reviews.
	2. Evaluation of multiple NLP approaches including LLMs for improved accuracy.
	3. Insights into multilingual transfer of learned models for broader applicability.

**Result:** The experiments highlighted the effectiveness of various NLP approaches in detecting dual quality problems, alongside insights into multilingual transfer capabilities.

**Limitations:** 

**Conclusion:** The paper outlines practical insights for deploying their solution and suggests further applications in identifying dual quality issues across different languages.

**Abstract:** Consumers often face inconsistent product quality, particularly when identical products vary between markets, a situation known as the dual quality problem. To identify and address this issue, automated techniques are needed. This paper explores how natural language processing (NLP) can aid in detecting such discrepancies and presents the full process of developing a solution. First, we describe in detail the creation of a new Polish-language dataset with 1,957 reviews, 540 highlighting dual quality issues. We then discuss experiments with various approaches like SetFit with sentence-transformers, transformer-based encoders, and LLMs, including error analysis and robustness verification. Additionally, we evaluate multilingual transfer using a subset of opinions in English, French, and German. The paper concludes with insights on deployment and practical applications.

</details>


### [149] [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)

*Utkarsh Sahu, Zhisheng Qi, Yongjia Lei, Ryan A. Rossi, Franck Dernoncourt, Nesreen K. Ahmed, Mahantesh M Halappanavar, Yao Ma, Yu Wang*

**Main category:** cs.CL

**Keywords:** large language models, graph machine learning, knowledge homophily, fine-tuning, knowledge estimation

**Relevance Score:** 9

**TL;DR:** This paper investigates the structural patterns of knowledge in large language models (LLMs) using a graph perspective, quantifying knowledge levels and developing graph machine learning models for better estimation and fine-tuning outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the lack of focus on the structural patterns of knowledge in large language models, aiming to quantify and analyze these patterns from a graph perspective.

**Method:** The authors quantify LLM knowledge at triplet and entity levels, analyze relationships with graph structural properties (e.g. node degree), and develop graph machine learning models for knowledge estimation based on local neighbors.

**Key Contributions:**

	1. Investigation of knowledge in LLMs from a graph structural perspective.
	2. Development of graph machine learning models for estimating entity knowledge.
	3. Evidence of knowledge homophily among topologically close entities.

**Result:** The research uncovers knowledge homophily in LLMs, showing that closely-related entities have similar knowledge levels, and demonstrates that selected triplets for fine-tuning improve performance.

**Limitations:** 

**Conclusion:** The findings suggest that a graph-based understanding of LLM knowledge can enhance knowledge checking and lead to improved model fine-tuning.

**Abstract:** Large language models have been extensively studied as neural knowledge bases for their knowledge access, editability, reasoning, and explainability. However, few works focus on the structural patterns of their knowledge. Motivated by this gap, we investigate these structural patterns from a graph perspective. We quantify the knowledge of LLMs at both the triplet and entity levels, and analyze how it relates to graph structural properties such as node degree. Furthermore, we uncover the knowledge homophily, where topologically close entities exhibit similar levels of knowledgeability, which further motivates us to develop graph machine learning models to estimate entity knowledge based on its local neighbors. This model further enables valuable knowledge checking by selecting triplets less known to LLMs. Empirical results show that using selected triplets for fine-tuning leads to superior performance.

</details>


### [150] [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)

*Wang Yang, Hongye Jin, Shaochen Zhong, Song Jiang, Qifan Wang, Vipin Chaudhary, Xiaotian Han*

**Main category:** cs.CL

**Keywords:** long-context LLMs, evaluation benchmark, natural language processing

**Relevance Score:** 9

**TL;DR:** Introduction of a length-controllable long-context benchmark for evaluating LLMs, addressing current shortcomings in long-context evaluation metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation of long-context capabilities in language models and clarify performance metrics compared to baseline abilities.

**Method:** Development of a length-controllable benchmark and a novel metric that distinguishes baseline knowledge from true long-context capabilities in LLMs.

**Key Contributions:**

	1. A length-controllable long-context benchmark for LLMs
	2. A novel evaluation metric distinguishing long-context capability from baseline performance
	3. Experimental validation demonstrating the effectiveness of the approach

**Result:** Experiments show that the proposed benchmark and metric provide a clearer and more effective evaluation of long-context performance in various LLMs.

**Limitations:** Potential reliance on specific model architectures and varying interpretations of long-context across different applications.

**Conclusion:** The novel benchmark and metric significantly improve the ability to evaluate long-context performances in language models, contributing to better model assessment and development.

**Abstract:** Long-context capability is considered one of the most important abilities of LLMs, as a truly long context-capable LLM enables users to effortlessly process many originally exhausting tasks -- e.g., digesting a long-form document to find answers vs. directly asking an LLM about it. However, existing real-task-based long-context evaluation benchmarks have two major shortcomings. First, benchmarks like LongBench often do not provide proper metrics to separate long-context performance from the model's baseline ability, making cross-model comparison unclear. Second, such benchmarks are usually constructed with fixed input lengths, which limits their applicability across different models and fails to reveal when a model begins to break down. To address these issues, we introduce a length-controllable long-context benchmark and a novel metric that disentangles baseline knowledge from true long-context capabilities. Experiments demonstrate the superiority of our approach in effectively evaluating LLMs.

</details>


### [151] [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)

*Lingjun Zhao, Hal Daumé III*

**Main category:** cs.CL

**Keywords:** AI transparency, explanation consistency, language models, preference optimization, explanation faithfulness

**Relevance Score:** 8

**TL;DR:** This paper introduces a measure for PEX consistency to evaluate the alignment between AI predictions and their explanations, highlighting the prevalence of inconsistency in language model-generated explanations and illustrating improvement through optimization techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Ensuring transparency in AI decision-making, especially in high-stakes contexts, necessitates faithful explanations for predictions made by language models.

**Method:** The paper presents a measure for measuring PEX consistency, which quantifies how much a free-text explanation supports or opposes a prediction. It involves the application of direct preference optimization to improve explanation consistency across different model families.

**Key Contributions:**

	1. Development of a measure for PEX consistency
	2. Demonstration of widespread inconsistency in model-generated explanations
	3. Optimization techniques effectively improve the quality of explanations

**Result:** The study found that over 62% of explanations generated by large language models are inconsistent. Direct preference optimization improved consistency of explanations by 43.1% to 292.3% across three model families, with potential increases in explanation faithfulness by up to 9.7%.

**Limitations:** 

**Conclusion:** Optimizing PEX consistency significantly enhances the faithfulness of explanations provided by language models, thereby contributing to greater transparency in AI reasoning processes.

**Abstract:** Faithful free-text explanations are important to ensure transparency in high-stakes AI decision-making contexts, but they are challenging to generate by language models and assess by humans. In this paper, we present a measure for Prediction-EXplanation (PEX) consistency, by extending the concept of weight of evidence. This measure quantifies how much a free-text explanation supports or opposes a prediction, serving as an important aspect of explanation faithfulness. Our analysis reveals that more than 62% explanations generated by large language models lack this consistency. We show that applying direct preference optimization improves the consistency of generated explanations across three model families, with improvement ranging from 43.1% to 292.3%. Furthermore, we demonstrate that optimizing this consistency measure can improve explanation faithfulness by up to 9.7%.

</details>


### [152] [SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking](https://arxiv.org/abs/2505.19300)

*Junnan Liu, Linhao Luo, Thuy-Trang Vu, Gholamreza Haffari*

**Main category:** cs.CL

**Keywords:** Large Language Models, Situated Thinking, Reinforcement Learning, Real-world Reasoning, Grounded Knowledge

**Relevance Score:** 9

**TL;DR:** SituatedThinker is a framework that enables large language models to enhance their reasoning by integrating real-world information through situated thinking and reinforcement learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models which restrict reasoning to internal parametric space, thereby lacking access to real-time information and understanding of the physical world.

**Method:** The framework employs situated thinking to combine internal knowledge with external information via predefined interfaces, leveraging reinforcement learning to promote effective real-world reasoning and feedback acquisition.

**Key Contributions:**

	1. Introduction of the SituatedThinker framework for real-world grounded reasoning in LLMs
	2. Integration of reinforcement learning to enhance reasoning through real-world feedback
	3. Demonstrated generalizability across multiple unseen tasks and benchmarks

**Result:** Experimental results show significant improvements in multi-hop question-answering and mathematical reasoning, along with strong performance on unseen tasks such as KBQA and TableQA.

**Limitations:** 

**Conclusion:** SituatedThinker enhances LLMs' reasoning capabilities by grounding them in real-world contexts and demonstrating adaptability across various tasks.

**Abstract:** Recent advances in large language models (LLMs) demonstrate their impressive reasoning capabilities. However, the reasoning confined to internal parametric space limits LLMs' access to real-time information and understanding of the physical world. To overcome this constraint, we introduce SituatedThinker, a novel framework that enables LLMs to ground their reasoning in real-world contexts through situated thinking, which adaptively combines both internal knowledge and external information with predefined interfaces. By utilizing reinforcement learning, SituatedThinker incentivizes deliberate reasoning with the real world to acquire information and feedback, allowing LLMs to surpass their knowledge boundaries and enhance reasoning. Experimental results demonstrate significant performance improvements on multi-hop question-answering and mathematical reasoning benchmarks. Furthermore, SituatedThinker demonstrates strong performance on unseen tasks, such as KBQA, TableQA, and text-based games, showcasing the generalizable real-world grounded reasoning capability. Our codes are available at https://github.com/jnanliu/SituatedThinker.

</details>


### [153] [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)

*Yongmin Yoo, Qiongkai Xu, Longbing Cao*

**Main category:** cs.CL

**Keywords:** patent generation, natural language generation, evaluation framework, large language models, legal validity

**Relevance Score:** 9

**TL;DR:** The paper introduces PatentScore, a novel evaluation framework for assessing the quality of LLM-generated patent claims based on legal, technical, and structural dimensions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs for generating patent documents necessitates a specialized evaluation method that addresses the unique structural and legal characteristics of patents.

**Method:** The framework involves hierarchical decomposition for claim analysis, domain-specific validation patterns, and a multi-dimensional scoring system that evaluates structural, semantic, and legal aspects of patent claims.

**Key Contributions:**

	1. Introduction of a multi-dimensional evaluation framework for patent claims
	2. Incorporation of hierarchical claim analysis
	3. Demonstration of robust correlations with expert judgments in evaluation

**Result:** The framework showed a high Pearson correlation of $r = 0.819$ with expert annotations when evaluating 400 generated patent claims, indicating its effectiveness over traditional NLG metrics.

**Limitations:** 

**Conclusion:** PatentScore demonstrates the potential for effectively evaluating patent claims generated by LLMs, surpassing existing evaluation metrics and providing a robust solution for this domain.

**Abstract:** Natural language generation (NLG) metrics play a central role in evaluating generated texts, but are not well suited for the structural and legal characteristics of patent documents. Large language models (LLMs) offer strong potential in automating patent generation, yet research on evaluating LLM-generated patents remains limited, especially in evaluating the generation quality of patent claims, which are central to defining the scope of protection. Effective claim evaluation requires addressing legal validity, technical accuracy, and structural compliance. To address this gap, we introduce PatentScore, a multi-dimensional evaluation framework for assessing LLM-generated patent claims. PatentScore incorporates: (1) hierarchical decomposition for claim analysis; (2) domain-specific validation patterns based on legal and technical standards; and (3) scoring across structural, semantic, and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects patent-specific constraints and document structures, enabling evaluation beyond surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a Pearson correlation of $r = 0.819$ with expert annotations, outperforming existing NLG metrics. Furthermore, we conduct additional evaluations using open models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong correlations with expert judgments, confirming the robustness and generalizability of our framework.

</details>


### [154] [GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance](https://arxiv.org/abs/2505.19354)

*Mohammad Mahdi Moradi, Sudhir Mudur*

**Main category:** cs.CL

**Keywords:** Visual Question Answering, Knowledge-Based VQA, Large Language Models, Context-aware captioning, Zero-shot learning

**Relevance Score:** 8

**TL;DR:** GC-KBVQA improves Visual Question Answering by grounding question-aware captions and using LLMs effectively without fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of Knowledge-Based Visual Question Answering (KB-VQA) methods by addressing the limitations of irrelevant auxiliary text.

**Method:** We propose a four-stage framework called Grounding Caption-Guided Knowledge-Based Visual Question Answering (GC-KBVQA) which uses context-aware caption generation alongside external knowledge sources to inform LLMs.

**Key Contributions:**

	1. Development of GC-KBVQA framework
	2. Use of grounding question-aware captions for context-rich information
	3. Ability to perform zero-shot tasks without multimodal training

**Result:** GC-KBVQA significantly outperforms existing KB-VQA methods, demonstrating improved accuracy and efficacy in zero-shot VQA tasks without requiring task-specific fine-tuning.

**Limitations:** 

**Conclusion:** The GC-KBVQA framework facilitates high-quality VQA by leveraging the strengths of LLMs with rich contextual information, leading to reduced costs and complexities in deployment.

**Abstract:** Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks that demand reasoning with information extending beyond the explicit content depicted in the image. Early methods relied on explicit knowledge bases to provide this auxiliary information. Recent approaches leverage Large Language Models (LLMs) as implicit knowledge sources. While KB-VQA methods have demonstrated promising results, their potential remains constrained as the auxiliary text provided may not be relevant to the question context, and may also include irrelevant information that could misguide the answer predictor. We introduce a novel four-stage framework called Grounding Caption-Guided Knowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to effectively perform zero-shot VQA tasks without the need for end-to-end multimodal training. Innovations include grounding question-aware caption generation to move beyond generic descriptions and have compact, yet detailed and context-rich information. This is combined with knowledge from external sources to create highly informative prompts for the LLM. GC-KBVQA can address a variety of VQA tasks, and does not require task-specific fine-tuning, thus reducing both costs and deployment complexity by leveraging general-purpose, pre-trained LLMs. Comparison with competing KB-VQA methods shows significantly improved performance. Our code will be made public.

</details>


### [155] [Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement](https://arxiv.org/abs/2505.19355)

*Lin Tian, Marian-Andrei Rizoiu*

**Main category:** cs.CL

**Keywords:** social media, causal inference, misinformation, engagement, treatment effects

**Relevance Score:** 5

**TL;DR:** A novel joint treatment-outcome framework is proposed to analyze causal mechanisms of engagement in social media, particularly in the context of misinformation spread.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding true influence in social media requires distinguishing between correlation and causation, especially regarding the spread of misinformation.

**Method:** The approach uses a joint treatment-outcome framework that combines causal inference techniques adapted from healthcare with sequential models to estimate Average Treatment Effects (ATE) in social media interactions.

**Key Contributions:**

	1. Novel joint treatment-outcome framework for analyzing social media engagement
	2. Application of causal inference from healthcare to social media
	3. Demonstrated improved prediction of engagement metrics over existing methods

**Result:** Experiments show that the proposed model outperforms existing benchmarks by 15–22% in predicting engagement across various counterfactual scenarios.

**Limitations:** 

**Conclusion:** The framework effectively captures causal mechanisms, and case studies present strong alignment with expert-based empirical influence measures.

**Abstract:** Understanding true influence in social media requires distinguishing correlation from causation--particularly when analyzing misinformation spread. While existing approaches focus on exposure metrics and network structures, they often fail to capture the causal mechanisms by which external temporal signals trigger engagement. We introduce a novel joint treatment-outcome framework that leverages existing sequential models to simultaneously adapt to both policy timing and engagement effects. Our approach adapts causal inference techniques from healthcare to estimate Average Treatment Effects (ATE) within the sequential nature of social media interactions, tackling challenges from external confounding signals. Through our experiments on real-world misinformation and disinformation datasets, we show that our models outperform existing benchmarks by 15--22% in predicting engagement across diverse counterfactual scenarios, including exposure adjustment, timing shifts, and varied intervention durations. Case studies on 492 social media users show our causal effect measure aligns strongly with the gold standard in influence estimation, the expert-based empirical influence.

</details>


### [156] [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360)

*Manan Suri, Puneet Mathur, Nedim Lipka, Franck Dernoncourt, Ryan A. Rossi, Dinesh Manocha*

**Main category:** cs.CL

**Keywords:** multimodal large language models, chart understanding, visual attribution

**Relevance Score:** 8

**TL;DR:** The paper presents ChartLens, a novel chart attribution algorithm designed to improve the accuracy of multimodal large language models (MLLMs) by addressing hallucinations in chart understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** MLLMs often generate conflicting text when interpreting visual data, which can hinder their effectiveness in understanding charts.

**Method:** ChartLens utilizes segmentation-based techniques to identify chart elements and employs set-of-marks prompting with MLLMs for visual attribution.

**Key Contributions:**

	1. Introduction of ChartLens algorithm for fine-grained visual attribution
	2. Development of a benchmark, ChartVA-Eval, with diverse real-world chart data
	3. Significant improvement in attribution accuracy by 26-66%.

**Result:** ChartLens enhances fine-grained attributions by 26-66%, demonstrating superior performance in validating chart-associated responses against visual data.

**Limitations:** 

**Conclusion:** The introduction of ChartLens and the ChartVA-Eval benchmark provides a significant improvement in the reliability of MLLMs in chart understanding tasks.

**Abstract:** The growing capabilities of multimodal large language models (MLLMs) have advanced tasks like chart understanding. However, these models often suffer from hallucinations, where generated text sequences conflict with the provided visual data. To address this, we introduce Post-Hoc Visual Attribution for Charts, which identifies fine-grained chart elements that validate a given chart-associated response. We propose ChartLens, a novel chart attribution algorithm that uses segmentation-based techniques to identify chart objects and employs set-of-marks prompting with MLLMs for fine-grained visual attribution. Additionally, we present ChartVA-Eval, a benchmark with synthetic and real-world charts from diverse domains like finance, policy, and economics, featuring fine-grained attribution annotations. Our evaluations show that ChartLens improves fine-grained attributions by 26-66%.

</details>


### [157] [Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality](https://arxiv.org/abs/2505.19376)

*Lance Ying, Almog Hillel, Ryan Truong, Vikash K. Mansinghka, Joshua B. Tenenbaum, Tan Zhi-Xuan*

**Main category:** cs.CL

**Keywords:** theory-of-mind, belief attribution, causal relevance, natural language processing, human behavior

**Relevance Score:** 6

**TL;DR:** This paper investigates how people attribute beliefs to others based on the explanatory strength of those beliefs, using a computational model to analyze factors influencing these attributions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand which specific beliefs people prefer to attribute to others as explanations for observed behaviors.

**Method:** The authors develop a computational model assessing belief attribution using three factors: accuracy, informativity, and causal relevance, analyzed via a study where participants rank beliefs about an agent's behavior.

**Key Contributions:**

	1. Development of a computational model for belief attribution
	2. Identification of key factors (accuracy, informativity, causal relevance) influencing belief attributions
	3. Empirical findings showing causal relevance's dominance in belief selection

**Result:** The study finds that while accuracy and informativity help predict rankings, causal relevance is the most significant factor in how participants attribute beliefs.

**Limitations:** 

**Conclusion:** Causal relevance plays a key role in belief attribution, suggesting that people are inclined to attribute beliefs that provide strong explanations for behaviors.

**Abstract:** A key feature of human theory-of-mind is the ability to attribute beliefs to other agents as mentalistic explanations for their behavior. But given the wide variety of beliefs that agents may hold about the world and the rich language we can use to express them, which specific beliefs are people inclined to attribute to others? In this paper, we investigate the hypothesis that people prefer to attribute beliefs that are good explanations for the behavior they observe. We develop a computational model that quantifies the explanatory strength of a (natural language) statement about an agent's beliefs via three factors: accuracy, informativity, and causal relevance to actions, each of which can be computed from a probabilistic generative model of belief-driven behavior. Using this model, we study the role of each factor in how people selectively attribute beliefs to other agents. We investigate this via an experiment where participants watch an agent collect keys hidden in boxes in order to reach a goal, then rank a set of statements describing the agent's beliefs about the boxes' contents. We find that accuracy and informativity perform reasonably well at predicting these rankings when combined, but that causal relevance is the single factor that best explains participants' responses.

</details>


### [158] [GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor](https://arxiv.org/abs/2505.19384)

*Seokgi Lee, Jungjun Kim*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, Speech Synthesis, Style Encoding

**Relevance Score:** 5

**TL;DR:** Introducing GSA-TTS, a style adaptor for TTS that encodes speaking styles from acoustic references for zero-shot synthesis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance speech synthesis by improving the naturalness, speaker similarity, and intelligibility of generated speech using hierarchical style encoding.

**Method:** GSA-TTS employs a novel style encoder that captures local speaking styles from semantic sound units and combines them using self-attention, creating a global style condition.

**Key Contributions:**

	1. Introduction of a novel style encoder for TTS
	2. Hierarchical semantic and style representation
	3. Improvement of interpretability and controllability in TTS systems

**Result:** GSA-TTS demonstrates promising results on unseen speakers, showcasing improvements in naturalness, speaker similarity, and intelligibility in speech synthesis tasks.

**Limitations:** 

**Conclusion:** GSA-TTS provides a robust style representation that enhances interpretability and controllability in text-to-speech applications, benefiting from its hierarchical structure.

**Abstract:** We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder that gradually encodes speaking styles from an acoustic reference for zero-shot speech synthesis. GSA first captures the local style of each semantic sound unit. Then the local styles are combined by self-attention to obtain a global style condition. This semantic and hierarchical encoding strategy provides a robust and rich style representation for an acoustic model. We test GSA-TTS on unseen speakers and obtain promising results regarding naturalness, speaker similarity, and intelligibility. Additionally, we explore the potential of GSA in terms of interpretability and controllability, which stems from its hierarchical structure.

</details>


### [159] [gec-metrics: A Unified Library for Grammatical Error Correction Evaluation](https://arxiv.org/abs/2505.19388)

*Takumi Goto, Yusuke Sakai, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** grammatical error correction, evaluation metrics, NLP, library, API

**Relevance Score:** 7

**TL;DR:** Introduction of the gec-metrics library for grammatical error correction evaluation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enable fair comparisons of GEC systems through a consistent implementation of evaluation metrics.

**Method:** Development of a library with a unified API for evaluating grammatical error correction systems, including meta-evaluation functionalities and analysis scripts.

**Key Contributions:**

	1. Unified interface for GEC metrics evaluation
	2. Focus on API usability and extensibility
	3. Includes meta-evaluation and visualization tools

**Result:** The gec-metrics library allows for extensible and consistent evaluations of GEC systems.

**Limitations:** 

**Conclusion:** The library enhances the evaluation process in GEC, facilitating better comparisons and improvements in this NLP field.

**Abstract:** We introduce gec-metrics, a library for using and developing grammatical error correction (GEC) evaluation metrics through a unified interface. Our library enables fair system comparisons by ensuring that everyone conducts evaluations using a consistent implementation. Moreover, it is designed with a strong focus on API usage, making it highly extensible. It also includes meta-evaluation functionalities and provides analysis and visualization scripts, contributing to developing GEC evaluation metrics. Our code is released under the MIT license and is also distributed as an installable package. The video is available on YouTube.

</details>


### [160] [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)

*Jade Robinson, Jonathan K. Kummerfeld*

**Main category:** cs.CL

**Keywords:** code documentation, code summaries, LLM, evaluation metrics, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** This paper introduces a new baseline method leveraging an LLM to evaluate code summaries, improving reliability over traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of comparing code summary generation techniques due to expensive human evaluations and unreliable automatic metrics.

**Method:** The proposed baseline uses an LLM to assign an overall score to code summaries, incorporating code context, and introduces a variant that evaluates documentation quality without relying on reference summaries.

**Key Contributions:**

	1. Introduction of LLM-based evaluation for code summaries
	2. Ability to evaluate without reference summaries
	3. Recommendation to use in combination with existing metrics

**Result:** The new approach performs as well or better than existing metrics while recommending to use it alongside embedding-based methods to mitigate LLM-specific bias.

**Limitations:** Potential LLM-specific biases and the necessity of using alongside other metrics.

**Conclusion:** The proposed LLM-based scoring method enhances evaluation of code summaries and documentation, potentially serving diverse applications.

**Abstract:** Code documentation is useful, but writing it is time-consuming. Different techniques for generating code summaries have emerged, but comparing them is difficult because human evaluation is expensive and automatic metrics are unreliable. In this paper, we introduce a simple new baseline in which we ask an LLM to give an overall score to a summary. Unlike n-gram and embedding-based baselines, our approach is able to consider the code when giving a score. This allows us to also make a variant that does not consider the reference summary at all, which could be used for other tasks, e.g., to evaluate the quality of documentation in code bases. We find that our method is as good or better than prior metrics, though we recommend using it in conjunction with embedding-based methods to avoid the risk of LLM-specific bias.

</details>


### [161] [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)

*Yan Wen, Junfeng Guo, Heng Huang*

**Main category:** cs.CL

**Keywords:** copyright protection, multi-agent systems, Chain-of-Thought reasoning

**Relevance Score:** 8

**TL;DR:** CoTGuard is a framework designed for copyright protection in multi-agent LLM systems by detecting unauthorized content during the reasoning process.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of multi-agent LLM systems presents new challenges for copyright protection as they may recall sensitive or copyrighted content during inter-agent communication.

**Method:** CoTGuard employs trigger-based detection within Chain-of-Thought reasoning by embedding specific trigger queries into agent prompts to monitor intermediate reasoning steps.

**Key Contributions:**

	1. Introduction of CoTGuard framework for copyright protection in LLMs
	2. Utilization of trigger-based detection within Chain-of-Thought reasoning
	3. Demonstration of effective content leakage detection with minimal task performance interference

**Result:** Extensive experiments demonstrate that CoTGuard effectively detects content leakage while maintaining task performance.

**Limitations:** 

**Conclusion:** Reasoning-level monitoring through CoTGuard provides a promising approach for safeguarding intellectual property in collaborative LLM agent systems.

**Abstract:** As large language models (LLMs) evolve into autonomous agents capable of collaborative reasoning and task execution, multi-agent LLM systems have emerged as a powerful paradigm for solving complex problems. However, these systems pose new challenges for copyright protection, particularly when sensitive or copyrighted content is inadvertently recalled through inter-agent communication and reasoning. Existing protection techniques primarily focus on detecting content in final outputs, overlooking the richer, more revealing reasoning processes within the agents themselves. In this paper, we introduce CoTGuard, a novel framework for copyright protection that leverages trigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically, we can activate specific CoT segments and monitor intermediate reasoning steps for unauthorized content reproduction by embedding specific trigger queries into agent prompts. This approach enables fine-grained, interpretable detection of copyright violations in collaborative agent scenarios. We evaluate CoTGuard on various benchmarks in extensive experiments and show that it effectively uncovers content leakage with minimal interference to task performance. Our findings suggest that reasoning-level monitoring offers a promising direction for safeguarding intellectual property in LLM-based agent systems.

</details>


### [162] [Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering](https://arxiv.org/abs/2505.19410)

*Jiajun Zhu, Ye Liu, Meikai Bao, Kai Zhang, Yanghai Zhang, Qi Liu*

**Main category:** cs.CL

**Keywords:** large language models, knowledge graphs, reasoning, iterative reflection, natural language processing

**Relevance Score:** 8

**TL;DR:** The paper presents Self-Reflective Planning (SRP), a framework that enhances large language models (LLMs) by integrating them with knowledge graphs for improved reasoning accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often hallucinate due to insufficient internal knowledge, making their reasoning prone to inaccuracies. The need for better integration of structured, reliable information from knowledge graphs (KGs) is crucial for enhancing LLM capabilities.

**Method:** SRP enhances LLMs by implementing iterative, reference-guided reasoning. It involves planning guided by references, checking initial relations, generating reasoning paths, and iteratively reflecting on results to refine these paths.

**Key Contributions:**

	1. Introduction of Self-Reflective Planning (SRP) framework for LLMs
	2. Integration of iterative, reference-guided reasoning
	3. Demonstrated improvements in reasoning accuracy over existing methods

**Result:** SRP demonstrates superior reasoning abilities compared to various strong baselines in experiments conducted on three public datasets.

**Limitations:** 

**Conclusion:** The proposed SRP framework not only improves reasoning accuracy for LLMs by leveraging KGs but also shows reliable performance in generating correct answers through iterative reflection.

**Abstract:** Recently, large language models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, yet they remain prone to hallucinations when reasoning with insufficient internal knowledge. While integrating LLMs with knowledge graphs (KGs) provides access to structured, verifiable information, existing approaches often generate incomplete or factually inconsistent reasoning paths. To this end, we propose Self-Reflective Planning (SRP), a framework that synergizes LLMs with KGs through iterative, reference-guided reasoning. Specifically, given a question and topic entities, SRP first searches for references to guide planning and reflection. In the planning process, it checks initial relations and generates a reasoning path. After retrieving knowledge from KGs through a reasoning path, it implements iterative reflection by judging the retrieval result and editing the reasoning path until the answer is correctly retrieved. Extensive experiments on three public datasets demonstrate that SRP surpasses various strong baselines and further underscore its reliable reasoning ability.

</details>


### [163] [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)

*Wenyang Xiao, Haoyu Zhao, Lingxiao Huang*

**Main category:** cs.CL

**Keywords:** in-context learning, diversity, large language models, example selection, performance

**Relevance Score:** 8

**TL;DR:** The paper investigates the role of diversity in in-context learning (ICL) for large language models, showing that diversity-aware example selection improves performance on various tasks.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underexplored impact of diversity in examples selection for in-context learning in large language models (LLMs).

**Method:** The authors conducted systematic experiments across diverse tasks, including sentiment classification and complex math and code problems, using various model families like Llama-3.1, Gemma-2, and Mistral-v0.3.

**Key Contributions:**

	1. Introduces a theoretical framework explaining the benefits of diversity in example selection
	2. Demonstrates improved performance in complex tasks through diversity-aware methods
	3. Shows enhanced robustness to out-of-distribution queries

**Result:** Diversity-aware selection methods were found to significantly enhance performance, especially on complicated tasks, and increase robustness to out-of-distribution queries.

**Limitations:** 

**Conclusion:** Incorporating diversity in in-context example selection leads to improved model performance and robustness in diverse query scenarios.

**Abstract:** In-context learning (ICL) is a crucial capability of current large language models (LLMs), where the selection of examples plays a key role in performance. While most existing approaches focus on selecting the most similar examples to the query, the impact of diversity in example selection remains underexplored. We systematically investigate the role of diversity in in-context example selection through experiments across a range of tasks, from sentiment classification to more challenging math and code problems. Experiments on Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that diversity-aware selection methods improve performance, particularly on complex tasks like math and code, and enhance robustness to out-of-distribution queries. To support these findings, we introduce a theoretical framework that explains the benefits of incorporating diversity in in-context example selection.

</details>


### [164] [Frictional Agent Alignment Framework: Slow Down and Don't Break Things](https://arxiv.org/abs/2505.19428)

*Abhijnan Nath, Carine Graff, Andrei Bachinin, Nikhil Krishnaswamy*

**Main category:** cs.CL

**Keywords:** AI collaboration, Frictional Agent Alignment Framework, dynamic interaction, human-AI collaboration, belief alignment

**Relevance Score:** 8

**TL;DR:** The Frictional Agent Alignment Framework (FAAF) enhances AI's role in dynamic collaborative interactions by generating precise context-aware friction to address belief misalignments between interlocutors.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve collaborative interactions by mediating belief misalignments that occur in dynamic settings with sparse signals of interlocutor beliefs.

**Method:** FAAF decouples a two-player objective by implementing a frictive-state policy to identify misalignments and an intervention policy to craft preferred responses. The solution allows training through a simple supervised loss.

**Key Contributions:**

	1. Proposes the Frictional Agent Alignment Framework (FAAF) for dynamic collaborative tasks.
	2. Demonstrates superior performance in generating context-aware friction and belief alignment.
	3. Provides analytical solutions enabling effective training of collaborative AI policies.

**Result:** FAAF outperforms existing methods in generating interpretable friction and demonstrates improved out-of-distribution generalization across three benchmarks.

**Limitations:** Potential limitations include the framework's dependency on the specific nature of collaboration and the availability of recognizable belief signals.

**Conclusion:** By positioning LLMs as adaptive thought partners instead of passive responders, FAAF facilitates scalable and dynamic human-AI collaboration.

**Abstract:** AI support of collaborative interactions entails mediating potential misalignment between interlocutor beliefs. Common preference alignment methods like DPO excel in static settings, but struggle in dynamic collaborative tasks where the explicit signals of interlocutor beliefs are sparse and skewed. We propose the Frictional Agent Alignment Framework (FAAF), to generate precise, context-aware "friction" that prompts for deliberation and re-examination of existing evidence. FAAF's two-player objective decouples from data skew: a frictive-state policy identifies belief misalignments, while an intervention policy crafts collaborator-preferred responses. We derive an analytical solution to this objective, enabling training a single policy via a simple supervised loss. Experiments on three benchmarks show FAAF outperforms competitors in producing concise, interpretable friction and in OOD generalization. By aligning LLMs to act as adaptive "thought partners" -- not passive responders -- FAAF advances scalable, dynamic human-AI collaboration. Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.

</details>


### [165] [Rhapsody: A Dataset for Highlight Detection in Podcasts](https://arxiv.org/abs/2505.19429)

*Younghan Park, Anuj Diwan, David Harwath, Eunsol Choi*

**Main category:** cs.CL

**Keywords:** podcast, highlight detection, language models, finetuning, speech transcripts

**Relevance Score:** 7

**TL;DR:** The paper presents Rhapsody, a dataset for podcast highlight detection, and evaluates various approaches to automate this process, revealing challenges even for advanced language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To aid podcast users in swiftly identifying content highlights, thereby improving the listening experience amid the abundance of available podcasts.

**Method:** The paper investigates the podcast highlight detection problem as a binary classification challenge, utilizing Rhapsody dataset and baseline approaches like zero-shot prompting and fine-tuned language models.

**Key Contributions:**

	1. Introduction of Rhapsody dataset with highlight scores for 13K podcast episodes.
	2. Evaluation of language model approaches in detecting podcast highlights.
	3. Demonstration that model fine-tuning with in-domain data enhances performance significantly.

**Result:** The experimental results show that state-of-the-art language models struggle with podcast highlight detection, whereas models fine-tuned on the in-domain data significantly outperform zero-shot models, benefiting from integrating speech features and transcripts.

**Limitations:** The study primarily focused on existing language models, which may limit the generalization of findings to future models or unintended podcast formats.

**Conclusion:** Fine-tuned language models demonstrate improved performance in identifying podcast highlights, highlighting the complexities of parsing long-form spoken content for information retrieval.

**Abstract:** Podcasts have become daily companions for half a billion users. Given the enormous amount of podcast content available, highlights provide a valuable signal that helps viewers get the gist of an episode and decide if they want to invest in listening to it in its entirety. However, identifying highlights automatically is challenging due to the unstructured and long-form nature of the content. We introduce Rhapsody, a dataset of 13K podcast episodes paired with segment-level highlight scores derived from YouTube's 'most replayed' feature. We frame the podcast highlight detection as a segment-level binary classification task. We explore various baseline approaches, including zero-shot prompting of language models and lightweight finetuned language models using segment-level classification heads. Our experimental results indicate that even state-of-the-art language models like GPT-4o and Gemini struggle with this task, while models finetuned with in-domain data significantly outperform their zero-shot performance. The finetuned model benefits from leveraging both speech signal features and transcripts. These findings highlight the challenges for fine-grained information access in long-form spoken media.

</details>


### [166] [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)

*Keane Ong, Rui Mao, Deeksha Varshney, Paul Pu Liang, Erik Cambria, Gianmarco Mengaldo*

**Main category:** cs.CL

**Keywords:** Counterfactual reasoning, Financial markets, Large Language Models, Benchmarking, Decision-making

**Relevance Score:** 6

**TL;DR:** This paper introduces Fin-Force, a benchmark that supports LLM-based forward counterfactual reasoning in financial markets to automate insights for decision-making.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate understanding of potential future developments in dynamic financial markets, aiding stakeholders in decision-making by employing automated counterfactual reasoning.

**Method:** The paper curates financial news headlines to create the Fin-Force benchmark, which evaluates LLMs and counterfactual generation methods for their applicability in predicting future market behaviors.

**Key Contributions:**

	1. Introduction of the Fin-Force benchmark for forward counterfactual reasoning in finance
	2. Evaluation of state-of-the-art LLMs for generating counterfactual insights
	3. Identification of limitations in current counterfactual generation approaches.

**Result:** Experiments reveal the shortcomings of current LLM approaches in forward counterfactual generation and provide structured insights for future research in this area.

**Limitations:** Mainly focused on financial contexts, which may limit broader applicability of findings; relies heavily on the quality of curated financial news headlines.

**Conclusion:** Fin-Force enables scalable and automated forward counterfactual reasoning, which can significantly enhance decision-making in financial markets.

**Abstract:** Counterfactual reasoning typically involves considering alternatives to actual events. While often applied to understand past events, a distinct form-forward counterfactual reasoning-focuses on anticipating plausible future developments. This type of reasoning is invaluable in dynamic financial markets, where anticipating market developments can powerfully unveil potential risks and opportunities for stakeholders, guiding their decision-making. However, performing this at scale is challenging due to the cognitive demands involved, underscoring the need for automated solutions. Large Language Models (LLMs) offer promise, but remain unexplored for this application. To address this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward Counterfactual Evaluation. By curating financial news headlines and providing structured evaluation, Fin-Force supports LLM based forward counterfactual generation. This paves the way for scalable and automated solutions for exploring and anticipating future market developments, thereby providing structured insights for decision-making. Through experiments on Fin-Force, we evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing their limitations and proposing insights for future research.

</details>


### [167] [Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection](https://arxiv.org/abs/2505.19435)

*Zhihong Pan, Kai Zhang, Yuze Zhao, Yupeng Han*

**Main category:** cs.CL

**Keywords:** Language Models, Reasoning Strategies, Computational Efficiency, Dynamic Routing, Machine Learning

**Relevance Score:** 6

**TL;DR:** Route-To-Reason (RTR) is a framework for optimizing language model performance in reasoning tasks by dynamically routing models and strategies based on task difficulty, achieving higher accuracy and reduced computational costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational costs and pitfalls of reasoning tasks with language models, especially related to overthinking during complex reasoning.

**Method:** RTR dynamically allocates language models and reasoning strategies based on task difficulty under budget constraints, learning compressed representations for adaptive selection at inference time.

**Key Contributions:**

	1. Introduction of Route-To-Reason (RTR) framework
	2. Dynamic allocation of models and strategies based on task difficulty
	3. Significant reduction in computational costs while improving accuracy

**Result:** RTR achieves a higher accuracy than the best single model while reducing token usage by over 60%, demonstrating a successful trade-off between accuracy and computational efficiency.

**Limitations:** 

**Conclusion:** RTR provides a low-cost, flexible, and adaptable approach for improved reasoning capabilities in language models, applicable to various model types and strategies.

**Abstract:** The inherent capabilities of a language model (LM) and the reasoning strategies it employs jointly determine its performance in reasoning tasks. While test-time scaling is regarded as an effective approach to tackling complex reasoning tasks, it incurs substantial computational costs and often leads to "overthinking", where models become trapped in "thought pitfalls". To address this challenge, we propose Route-To-Reason (RTR), a novel unified routing framework that dynamically allocates both LMs and reasoning strategies according to task difficulty under budget constraints. RTR learns compressed representations of both expert models and reasoning strategies, enabling their joint and adaptive selection at inference time. This method is low-cost, highly flexible, and can be seamlessly extended to arbitrary black-box or white-box models and strategies, achieving true plug-and-play functionality. Extensive experiments across seven open source models and four reasoning strategies demonstrate that RTR achieves an optimal trade-off between accuracy and computational efficiency among all baselines, achieving higher accuracy than the best single model while reducing token usage by over 60%.

</details>


### [168] [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)

*Rihui Xin, Han Liu, Zecheng Wang, Yupeng Zhang, Dianbo Sui, Xiaolin Hu, Bingning Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Mathematical Problem-Solving

**Relevance Score:** 7

**TL;DR:** This research proposes using format and length as surrogate signals to train Large Language Models (LLMs) for mathematical problem-solving, achieving performance improvements without traditional ground truth answers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges and costs of obtaining ground truth answers for training LLMs in mathematical problem-solving.

**Method:** The study utilizes a reward function based on format correctness initially, and then incorporates length-based rewards in later phases, creating a GRPO approach that leverages these surrogate signals.

**Key Contributions:**

	1. Utilization of format and length as surrogate signals for training LLMs.
	2. Performance improvements in mathematical problem-solving without traditional ground truth answers.
	3. A practical approach to reducing dependence on costly data collection for LLM training.

**Result:** The new GRPO approach achieves performance comparable or superior to standard GRPO algorithms relying on ground truth answers, reaching 40.0% accuracy on AIME2024 with a 7B base model.

**Limitations:** Performance may vary across different scenarios and further evaluation needed in late phases.

**Conclusion:** The research offers a practical solution for training LLMs with less reliance on extensive ground truth data, demonstrating that enhancing answering habits can unlock the capabilities of base models.

**Abstract:** Large Language Models have achieved remarkable success in natural language processing tasks, with Reinforcement Learning playing a key role in adapting them to specific applications. However, obtaining ground truth answers for training LLMs in mathematical problem-solving is often challenging, costly, and sometimes unfeasible. This research delves into the utilization of format and length as surrogate signals to train LLMs for mathematical problem-solving, bypassing the need for traditional ground truth answers.Our study shows that a reward function centered on format correctness alone can yield performance improvements comparable to the standard GRPO algorithm in early phases. Recognizing the limitations of format-only rewards in the later phases, we incorporate length-based rewards. The resulting GRPO approach, leveraging format-length surrogate signals, not only matches but surpasses the performance of the standard GRPO algorithm relying on ground truth answers in certain scenarios, achieving 40.0\% accuracy on AIME2024 with a 7B base model. Through systematic exploration and experimentation, this research not only offers a practical solution for training LLMs to solve mathematical problems and reducing the dependence on extensive ground truth data collection, but also reveals the essence of why our label-free approach succeeds: base model is like an excellent student who has already mastered mathematical and logical reasoning skills, but performs poorly on the test paper, it simply needs to develop good answering habits to achieve outstanding results in exams , in other words, to unlock the capabilities it already possesses.

</details>


### [169] [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)

*Shashata Sawmya, Micah Adler, Nir Shavit*

**Main category:** cs.CL

**Keywords:** language models, interpretability, semantic features, neural activations, transformer dynamics

**Relevance Score:** 8

**TL;DR:** The paper examines how interpretable categorical features develop in large language models, focusing on timeline, model scale, and layer-specific behaviors using sparse autoencoders.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand how interpretable features emerge in large language models (LLMs) and challenge assumptions about their representational dynamics.

**Method:** Employing sparse autoencoders for mechanistic interpretability to analyze the behavior of LLM features across different training checkpoints, transformer layers, and model sizes.

**Key Contributions:**

	1. Identification of temporal and scale-specific thresholds for feature emergence.
	2. Discovery of unexpected semantic reactivation patterns in LLMs.
	3. Insights into representational dynamics that challenge existing assumptions.

**Result:** The study finds distinct temporal and spatial thresholds for feature emergence, along with notable semantic reactivations in early-layer features at later layers.

**Limitations:** 

**Conclusion:** The findings highlight the complex dynamics of feature emergence in LLMs and suggest a reevaluation of current theories regarding their representational processes.

**Abstract:** This paper studies the emergence of interpretable categorical features within large language models (LLMs), analyzing their behavior across training checkpoints (time), transformer layers (space), and varying model sizes (scale). Using sparse autoencoders for mechanistic interpretability, we identify when and where specific semantic concepts emerge within neural activations. Results indicate clear temporal and scale-specific thresholds for feature emergence across multiple domains. Notably, spatial analysis reveals unexpected semantic reactivation, with early-layer features re-emerging at later layers, challenging standard assumptions about representational dynamics in transformer models.

</details>


### [170] [Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks](https://arxiv.org/abs/2505.19472)

*Mohammad Mahdi Moradi, Walid Ahmed, Shuangyue Wen, Sudhir Mudur, Weiwei Zhang, Yang Liu*

**Main category:** cs.CL

**Keywords:** Hybrid Networks, Parallel Architecture, Load Balancing

**Relevance Score:** 6

**TL;DR:** FlowHN is a novel parallel hybrid network architecture that improves token processing speeds and accuracy by effectively balancing computational load between Attention and State-Space Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in existing hybrid networks that either use sequential or parallel pipelines, leading to latency and throughput issues.

**Method:** FlowHN implements a parallel hybrid network architecture with a dynamic token split approach for load balancing between Attention and State-Space Models, enhancing processing efficiency and representation fidelity.

**Key Contributions:**

	1. Novel parallel hybrid architecture using Attention and SSMs
	2. Dynamic token split for computational load balancing
	3. Enhanced output fusion method for better representation expressivity

**Result:** FlowHN achieves up to 4x higher Tokens per Second and 2x better Model FLOPs Utilization compared to other hybrid models.

**Limitations:** 

**Conclusion:** FlowHN demonstrates significant improvements in accuracy and processing speed, making it a more efficient alternative to traditional hybrid architectures.

**Abstract:** Attention and State-Space Models (SSMs) when combined in a hybrid network in sequence or in parallel provide complementary strengths. In a hybrid sequential pipeline they alternate between applying a transformer to the input and then feeding its output into a SSM. This results in idle periods in the individual components increasing end-to-end latency and lowering throughput caps. In the parallel hybrid architecture, the transformer operates independently in parallel with the SSM, and these pairs are cascaded, with output from one pair forming the input to the next. Two issues are (i) creating an expressive knowledge representation with the inherently divergent outputs from these separate branches, and (ii) load balancing the computation between these parallel branches, while maintaining representation fidelity. In this work we present FlowHN, a novel parallel hybrid network architecture that accommodates various strategies for load balancing, achieved through appropriate distribution of input tokens between the two branches. Two innovative differentiating factors in FlowHN include a FLOP aware dynamic token split between the attention and SSM branches yielding efficient balance in compute load, and secondly, a method to fuse the highly divergent outputs from individual branches for enhancing representation expressivity. Together they enable much better token processing speeds, avoid bottlenecks, and at the same time yield significantly improved accuracy as compared to other competing works. We conduct comprehensive experiments on autoregressive language modeling for models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential hybrid models and its parallel counterpart, achieving up to 4* higher Tokens per Second (TPS) and 2* better Model FLOPs Utilization (MFU).

</details>


### [171] [Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection](https://arxiv.org/abs/2505.19475)

*Mohammad Mahdi Moradi, Hossam Amer, Sudhir Mudur, Weiwei Zhang, Yang Liu, Walid Ahmed*

**Main category:** cs.CL

**Keywords:** language models, test-time training, unsupervised learning, verifier-driven adaptation, self-supervised learning

**Relevance Score:** 8

**TL;DR:** Introducing VDS-TTT framework for efficient adaptation of LLMs to out-of-distribution data using a verifier-driven sample selection process.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Adapting pretrained language models to new, unlabeled data presents significant challenges due to their poor performance on structurally novel reasoning tasks outside their training distribution.

**Method:** The VDS-TTT framework employs a learned verifier to score a pool of generated responses, selecting the highest-ranked examples for test-time training. Low-rank LoRA adapter parameters are fine-tuned for efficient adaptation.

**Key Contributions:**

	1. Introduction of VDS-TTT framework for language model adaptation
	2. First method to synthesize verifier-driven test-time training data
	3. Demonstration of significant performance improvements across multiple benchmarks

**Result:** Experiments show VDS-TTT provides up to a 32.29% relative performance improvement over baseline models and a 6.66% gain compared to verifier-based methods that do not employ test-time training.

**Limitations:** 

**Conclusion:** VDS-TTT is an effective self-supervised framework enabling continuous self-improvement for large language models during deployment.

**Abstract:** Learning to adapt pretrained language models to unlabeled, out-of-distribution data is a critical challenge, as models often falter on structurally novel reasoning tasks even while excelling within their training distribution. We introduce a new framework called VDS-TTT - Verifier-Driven Sample Selection for Test-Time Training to efficiently address this. We use a learned verifier to score a pool of generated responses and select only from high ranking pseudo-labeled examples for fine-tuned adaptation. Specifically, for each input query our LLM generates N candidate answers; the verifier assigns a reliability score to each, and the response with the highest confidence and above a fixed threshold is paired with its query for test-time training. We fine-tune only low-rank LoRA adapter parameters, ensuring adaptation efficiency and fast convergence. Our proposed self-supervised framework is the first to synthesize verifier driven test-time training data for continuous self-improvement of the model. Experiments across three diverse benchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up to a 32.29% relative improvement over the base model and a 6.66% gain compared to verifier-based methods without test-time training, highlighting its effectiveness and efficiency for on-the-fly large language model adaptation.

</details>


### [172] [CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis](https://arxiv.org/abs/2505.19484)

*Ruixiang Feng, Shen Gao, Xiuying Chen, Lisi Chen, Shuo Shang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Bias, Cultural Sensitivity, Multilingual Data, Question-Answering

**Relevance Score:** 9

**TL;DR:** CulFiT is a culturally-aware training paradigm for Large Language Models that incorporates multilingual data and fine-grained reward modeling to enhance cultural sensitivity and inclusivity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate the cultural biases of LLMs that undermine equality and perpetuate discrimination, especially in low-resource regions.

**Method:** CulFiT synthesizes diverse cultural-related questions, constructs critique data in relevant languages, and uses fine-grained rewards to evaluate cultural texts as verifiable knowledge units.

**Key Contributions:**

	1. Introduction of CulFiT, a culturally-aware training paradigm for LLMs.
	2. Development of GlobalCultureQA, a multilingual dataset for evaluating cultural responses.
	3. Demonstration of state-of-the-art performance in cultural alignment and reasoning.

**Result:** CulFiT achieves state-of-the-art performance in cultural alignment and reasoning across multiple benchmarks, including a newly introduced GlobalCultureQA dataset.

**Limitations:** 

**Conclusion:** The proposed approach significantly enhances the cultural sensitivity of LLMs, making them more inclusive and less biased.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they often exhibit a specific cultural biases, neglecting the values and linguistic diversity of low-resource regions. This cultural bias not only undermines universal equality, but also risks reinforcing stereotypes and perpetuating discrimination. To address this, we propose CulFiT, a novel culturally-aware training paradigm that leverages multilingual data and fine-grained reward modeling to enhance cultural sensitivity and inclusivity. Our approach synthesizes diverse cultural-related questions, constructs critique data in culturally relevant languages, and employs fine-grained rewards to decompose cultural texts into verifiable knowledge units for interpretable evaluation. We also introduce GlobalCultureQA, a multilingual open-ended question-answering dataset designed to evaluate culturally-aware responses in a global context. Extensive experiments on three existing benchmarks and our GlobalCultureQA demonstrate that CulFiT achieves state-of-the-art open-source model performance in cultural alignment and general reasoning.

</details>


### [173] [Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents](https://arxiv.org/abs/2505.19494)

*Manoj Balaji Jagadeeshan, Prince Raj, Pawan Goyal*

**Main category:** cs.CL

**Keywords:** Sanskrit retrieval, cross-lingual challenges, RAG framework

**Relevance Score:** 2

**TL;DR:** The study benchmarks retrieval methods for Sanskrit documents using English queries, focusing on cross-lingual challenges with a dataset of 3,400 pairs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the retrieval systems in a RAG framework for Sanskrit documents and improve accessibility to ancient texts.

**Method:** The study utilizes Direct Retrieval, Translation-based Retrieval, and Query Translation, employing shared embedding spaces and fine-tuning state-of-the-art models like BM25 and GPT-2.

**Key Contributions:**

	1. Development of a benchmarking framework for Sanskrit retrieval
	2. Publicly available dataset of English-Sanskrit query-document pairs
	3. Fine-tuning of state-of-the-art models for Sanskrit nuances

**Result:** Translation-based Retrieval methods significantly outperform Direct Retrieval and Query Translation in addressing linguistic challenges, improving comprehension and access to Sanskrit texts.

**Limitations:** 

**Conclusion:** The study demonstrates that advanced translation methods can effectively bridge the gap in cross-lingual retrieval for ancient documents, providing a means to preserve and disseminate Sanskrit scriptures.

**Abstract:** The study presents a comprehensive benchmark for retrieving Sanskrit documents using English queries, focusing on the chapters of the Srimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR), Translation-based Retrieval (DT), and Query Translation (QT), utilizing shared embedding spaces and advanced translation methods to enhance retrieval systems in a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's linguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT, Contriever, and GPT-2. It adapts summarization techniques for Sanskrit documents to improve QA processing. Evaluation shows DT methods outperform DR and QT in handling the cross-lingual challenges of ancient texts, improving accessibility and understanding. A dataset of 3,400 English-Sanskrit query-document pairs underpins the study, aiming to preserve Sanskrit scriptures and share their philosophical importance widely. Our dataset is publicly available at https://huggingface.co/datasets/manojbalaji1/anveshana

</details>


### [174] [LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](https://arxiv.org/abs/2505.19510)

*Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo*

**Main category:** cs.CL

**Keywords:** Large Language Models, scene graphs, benchmark, evaluation, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces TSG Bench, a benchmark for evaluating LLMs' capabilities in understanding and generating scene graphs from text, highlighting their strengths and weaknesses in this area.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the capacity of Large Language Models to understand and generate scene graphs, addressing the gap in existing benchmarks.

**Method:** Introduces TSG Bench, a benchmark specifically designed for assessing LLMs' understanding and generation of scene graphs from textual narratives, evaluating 11 LLMs.

**Key Contributions:**

	1. Development of the TSG Bench benchmark for LLM evaluation
	2. Identification of performance gaps between scene graph understanding and generation
	3. Public availability of code and evaluation data for further research.

**Result:** Models excel in scene graph understanding but struggle significantly with scene graph generation, especially for complex narratives, indicating a challenge in decomposing scenes.

**Limitations:** Focus on a limited set of 11 LLMs; does not explore other evaluation metrics beyond scene graphs.

**Conclusion:** The findings suggest a need for improved methodologies in scene graph generation and provide insights for future research directions.

**Abstract:** The remarkable reasoning and generalization capabilities of Large Language Models (LLMs) have paved the way for their expanding applications in embodied AI, robotics, and other real-world tasks. To effectively support these applications, grounding in spatial and temporal understanding in multimodal environments is essential. To this end, recent works have leveraged scene graphs, a structured representation that encodes entities, attributes, and their relationships in a scene. However, a comprehensive evaluation of LLMs' ability to utilize scene graphs remains limited. In this work, we introduce Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess LLMs' ability to (1) understand scene graphs and (2) generate them from textual narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models perform well on scene graph understanding, they struggle with scene graph generation, particularly for complex narratives. Our analysis indicates that these models fail to effectively decompose discrete scenes from a complex narrative, leading to a bottleneck when generating scene graphs. These findings underscore the need for improved methodologies in scene graph generation and provide valuable insights for future research. The demonstration of our benchmark is available at https://tsg-bench.netlify.app. Additionally, our code and evaluation data are publicly available at https://anonymous.4open.science/r/TSG-Bench.

</details>


### [175] [Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models](https://arxiv.org/abs/2505.19511)

*Aggrey Muhebwa, Khalid K. Osman*

**Main category:** cs.CL

**Keywords:** causal reasoning, language models, explanation coherence

**Relevance Score:** 8

**TL;DR:** The paper presents a framework for distilling causal reasoning from large models to smaller ones using a new metric for evaluating explanation quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in causal reasoning abilities between large proprietary language models and smaller open-source models.

**Method:** The framework trains the smaller model to generate structured cause-and-effect explanations aligned with those of a teacher model, utilizing the new Causal Explanation Coherence (CEC) metric for evaluation.

**Key Contributions:**

	1. Introduction of a framework for distilling causal reasoning from teacher to student models.
	2. Development of the Causal Explanation Coherence (CEC) metric for assessing explanation quality.
	3. Demonstration of how smaller models can be trained to achieve robust causal reasoning capabilities.

**Result:** The proposed CEC metric measures the structural and logical consistency of generated explanations, providing insights into the performance of smaller models in causal reasoning tasks.

**Limitations:** 

**Conclusion:** This framework enables smaller models to acquire robust causal reasoning abilities and offers a systematic way to assess the coherence of language model outputs.

**Abstract:** Large proprietary language models exhibit strong causal reasoning abilities that smaller open-source models struggle to replicate. We introduce a novel framework for distilling causal explanations that transfers causal reasoning skills from a powerful teacher model to a compact open-source model. The key idea is to train the smaller model to develop causal reasoning abilities by generating structured cause-and-effect explanations consistent with those of the teacher model. To evaluate the quality of the student-generated explanations, we introduce a new metric called Causal Explanation Coherence (CEC) to assess the structural and logical consistency of causal reasoning. This metric uses sentence-level semantic alignment to measure how well each part of the generated explanation corresponds to the teacher's reference, capturing both faithfulness and coverage of the underlying causal chain. Our framework and the CEC metric provide a principled foundation for training smaller models to perform robust causal reasoning and for systematically assessing the coherence of explanations in language model outputs.

</details>


### [176] [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)

*Yaoning Yu, Ye Yu, Kai Wei, Haojing Luo, Haohan Wang*

**Main category:** cs.CL

**Keywords:** prompt optimization, synthetic data generation, large language models

**Relevance Score:** 9

**TL;DR:** SIPDO introduces a framework that optimizes prompts for large language models by integrating synthetic data generation into the optimization process, enabling iterative improvement of prompt performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve prompt quality for large language models by addressing limitations in current optimization methods that rely on fixed datasets and lack iterative refinement.

**Method:** SIPDO is a closed-loop framework that couples a synthetic data generator with a prompt optimizer, allowing for the generation of new examples that highlight prompt weaknesses and guide incremental refinement.

**Key Contributions:**

	1. Introduction of SIPDO framework for prompt optimization
	2. Coupling synthetic data generation with prompt tuning for iterative improvement
	3. Demonstrated superior performance on benchmarks compared to standard methods.

**Result:** Experiments indicate that SIPDO outperforms traditional prompt tuning methods on question answering and reasoning benchmarks.

**Limitations:** 

**Conclusion:** Integrating data synthesis into the prompt learning workflow significantly enhances prompt performance and allows for systematic improvements without needing external supervision.

**Abstract:** Prompt quality plays a critical role in the performance of large language models (LLMs), motivating a growing body of work on prompt optimization. Most existing methods optimize prompts over a fixed dataset, assuming static input distributions and offering limited support for iterative improvement. We introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a closed-loop framework for prompt learning that integrates synthetic data generation into the optimization process. SIPDO couples a synthetic data generator with a prompt optimizer, where the generator produces new examples that reveal current prompt weaknesses and the optimizer incrementally refines the prompt in response. This feedback-driven loop enables systematic improvement of prompt performance without assuming access to external supervision or new tasks. Experiments across question answering and reasoning benchmarks show that SIPDO outperforms standard prompt tuning methods, highlighting the value of integrating data synthesis into prompt learning workflows.

</details>


### [177] [Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework](https://arxiv.org/abs/2505.19515)

*Lavanya Prahallad, Radhika Mamidi*

**Main category:** cs.CL

**Keywords:** Discourse Analysis, Political Communication, Rhetorical Strategies

**Relevance Score:** 2

**TL;DR:** This paper analyzes Donald Trump's rhetorical strategies in the 2024 U.S. presidential debates using a new annotation framework called BEADS, highlighting his use of emotionally charged and adversarial discourse.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand Donald Trump's rhetorical strategies during the 2024 U.S. presidential debates and establish a framework for analyzing political discourse.

**Method:** Introduces BEADS, an annotation framework that extends the DAMSL framework with tags for ideological framing, emotional appeals, and adversarial tactics, comparing human annotations with ChatGPT-assisted tagging on debate transcripts.

**Key Contributions:**

	1. Development of BEADS for analyzing political discourse
	2. Comparison of human annotation and AI-assisted tagging
	3. Insights into Trump's rhetorical strategies in political debates

**Result:** The analysis reveals that Trump excelled in categories such as Challenge and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias, and Perceived Dismissiveness.

**Limitations:** 

**Conclusion:** BEADS is proposed as a scalable and reproducible tool for critical discourse analysis applicable in various languages and political contexts.

**Abstract:** We present a critical discourse analysis of the 2024 U.S. presidential debates, examining Donald Trump's rhetorical strategies in his interactions with Joe Biden and Kamala Harris. We introduce a novel annotation framework, BEADS (Bias Enriched Annotation for Dialogue Structure), which systematically extends the DAMSL framework to capture bias driven and adversarial discourse features in political communication. BEADS includes a domain and language agnostic set of tags that model ideological framing, emotional appeals, and confrontational tactics. Our methodology compares detailed human annotation with zero shot ChatGPT assisted tagging on verified transcripts from the Trump and Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our analysis shows that Trump consistently dominated in key categories: Challenge and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias, and Perceived Dismissiveness. These findings underscore his use of emotionally charged and adversarial rhetoric to control the narrative and influence audience perception. In this work, we establish BEADS as a scalable and reproducible framework for critical discourse analysis across languages, domains, and political contexts.

</details>


### [178] [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)

*Yejin Lee, Joonghyuk Hahn, Hyeseon Ahn, Yo-Sub Han*

**Main category:** cs.CL

**Keywords:** implicit hate speech, named entity recognition, attention mechanism, contrastive learning, natural language processing

**Relevance Score:** 7

**TL;DR:** AmpleHate is a novel approach for implicit hate speech detection that mimics human inference by identifying explicit targets and their contextual relationships using attention mechanisms, achieving state-of-the-art performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of detecting implicit hate speech, which is often subtle and context-dependent, highlighting the differences between machine learning approaches and human understanding.

**Method:** AmpleHate uses a pretrained Named Entity Recognition model to identify explicit targets, then captures implicit target information with [CLS] tokens, calculating attention-based relationships between these targets and the context to improve sentence representation.

**Key Contributions:**

	1. Introduction of AmpleHate for implicit hate speech detection mimicking human inference.
	2. Use of attention-based mechanisms to enhance relational context understanding.
	3. Achieving state-of-the-art performance and faster convergence compared to previous methods.

**Result:** AmpleHate outperforms existing contrastive learning methods by an average of 82.14% and shows faster convergence in training, while attention patterns align with human judgement.

**Limitations:** 

**Conclusion:** The effectiveness of AmpleHate in identifying implicit hate speech demonstrates its potential for interpretability and robustness in understanding context relations.

**Abstract:** Implicit hate speech detection is challenging due to its subtlety and reliance on contextual interpretation rather than explicit offensive words. Current approaches rely on contrastive learning, which are shown to be effective on distinguishing hate and non-hate sentences. Humans, however, detect implicit hate speech by first identifying specific targets within the text and subsequently interpreting how these target relate to their surrounding context. Motivated by this reasoning process, we propose AmpleHate, a novel approach designed to mirror human inference for implicit hate detection. AmpleHate identifies explicit target using a pretrained Named Entity Recognition model and capture implicit target information via [CLS] tokens. It computes attention-based relationships between explicit, implicit targets and sentence context and then, directly injects these relational vectors into the final sentence representation. This amplifies the critical signals of target-context relations for determining implicit hate. Experiments demonstrate that AmpleHate achieves state-of-the-art performance, outperforming contrastive learning baselines by an average of 82.14% and achieve faster convergence. Qualitative analyses further reveal that attention patterns produced by AmpleHate closely align with human judgement, underscoring its interpretability and robustness.

</details>


### [179] [Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation](https://arxiv.org/abs/2505.19529)

*Tanjil Hasan Sakib, Md. Tanzib Hosain, Md. Kishor Morol*

**Main category:** cs.CL

**Keywords:** Small Language Models, model optimization, pruning, quantization, model compression

**Relevance Score:** 7

**TL;DR:** The paper reviews Small Language Models (SLMs), their optimization techniques, and proposes a classification system for their design and evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and optimize Small Language Models (SLMs) for deployment in resource-limited environments, enhancing their performance and efficiency.

**Method:** The study involves a comprehensive assessment of SLMs, outlining design frameworks, training approaches, and optimization techniques such as pruning, quantization, and model compression.

**Key Contributions:**

	1. Proposed a classification system for SLM optimization approaches.
	2. Established an evaluation suite for SLM capabilities using existing datasets.
	3. Identified unresolved challenges in SLM development and future research directions.

**Result:** A novel classification system for SLM optimization is proposed, alongside an evaluation suite using existing datasets, highlighting SLM capabilities and current limitations in the field.

**Limitations:** The study acknowledges unresolved trade-offs between efficiency and performance in SLMs.

**Conclusion:** The research serves as a guide for constructing efficient and high-performing SLMs, while also addressing unresolved issues such as efficiency-performance trade-offs.

**Abstract:** Small Language Models (SLMs) have gained substantial attention due to their ability to execute diverse language tasks successfully while using fewer computer resources. These models are particularly ideal for deployment in limited environments, such as mobile devices, on-device processing, and edge systems. In this study, we present a complete assessment of SLMs, focussing on their design frameworks, training approaches, and techniques for lowering model size and complexity. We offer a novel classification system to organize the optimization approaches applied for SLMs, encompassing strategies like pruning, quantization, and model compression. Furthermore, we assemble SLM's studies of evaluation suite with some existing datasets, establishing a rigorous platform for measuring SLM capabilities. Alongside this, we discuss the important difficulties that remain unresolved in this sector, including trade-offs between efficiency and performance, and we suggest directions for future study. We anticipate this study to serve as a beneficial guide for researchers and practitioners who aim to construct compact, efficient, and high-performing language models.

</details>


### [180] [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)

*Yuxing Lu, Gecheng Fu, Wei Wu, Xukai Zhao, Sin Yee Goi, Jinzhuo Wang*

**Main category:** cs.CL

**Keywords:** medical RAG, case-based reasoning, clinical decision-making, hybrid retrieval, experiential knowledge

**Relevance Score:** 9

**TL;DR:** DoctorRAG is a novel RAG framework that integrates both explicit clinical knowledge and experiential knowledge from patient cases to emulate human clinical reasoning in medical contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing medical RAG systems primarily rely on formal medical knowledge bases and overlook the valuable experiential knowledge that comes from similar patient cases, crucial for human clinical reasoning.

**Method:** DoctorRAG uses a hybrid retrieval mechanism that combines relevant clinical knowledge with experiential case-based information, incorporating conceptual tags and a Med-TextGrad module for output accuracy.

**Key Contributions:**

	1. Integration of experiential knowledge with clinical knowledge in RAG systems
	2. Use of hybrid retrieval mechanisms to enhance performance
	3. Implementation of a Med-TextGrad module for output validation

**Result:** Experiments show that DoctorRAG significantly outperforms existing RAG models in terms of response accuracy, relevance, and comprehensiveness, benefiting from iterative improvements.

**Limitations:** 

**Conclusion:** DoctorRAG represents a meaningful advancement towards creating medical reasoning systems that better reflect human-like clinical decision-making processes.

**Abstract:** Existing medical RAG systems mainly leverage knowledge from medical knowledge bases, neglecting the crucial role of experiential knowledge derived from similar patient cases -- a key component of human clinical reasoning. To bridge this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like reasoning by integrating both explicit clinical knowledge and implicit case-based experience. DoctorRAG enhances retrieval precision by first allocating conceptual tags for queries and knowledge sources, together with a hybrid retrieval mechanism from both relevant knowledge and patient. In addition, a Med-TextGrad module using multi-agent textual gradients is integrated to ensure that the final output adheres to the retrieved knowledge and patient query. Comprehensive experiments on multilingual, multitask datasets demonstrate that DoctorRAG significantly outperforms strong baseline RAG models and gains improvements from iterative refinements. Our approach generates more accurate, relevant, and comprehensive responses, taking a step towards more doctor-like medical reasoning systems.

</details>


### [181] [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)

*Xufeng Duan, Zhaoqian Yao, Yunhao Zhang, Shaonan Wang, Zhenguang G. Cai*

**Main category:** cs.CL

**Keywords:** large language models, syntactic structure, internal specialization, training process, machine learning

**Relevance Score:** 8

**TL;DR:** This paper analyzes the development of syntactic specialization in large language models during training, highlighting a clear trajectory and critical periods of sensitivity emergence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the emergence and development of syntactic sensitivity in large language models (LLMs) during their training process.

**Method:** The study quantifies internal syntactic consistency by tracking minimal pairs across various syntactic phenomena throughout training.

**Key Contributions:**

	1. Identified a developmental trajectory for syntactic specialization in LLMs.
	2. Showed the influence of model scale and training data on specialization.
	3. Provided evidence of a critical period for rapid internal specialization.

**Result:** A developmental trajectory is identified where syntactic sensitivity gradually emerges, concentrates in specific layers, and exhibits a 'critical period' of rapid internal specialization, influenced by model scale and training data.

**Limitations:** 

**Conclusion:** The findings reveal insights into how syntax is internalized in LLMs during training and suggest a systematic process of specialization that is consistent across different model architectures.

**Abstract:** Large language models (LLMs) have been found to develop surprising internal specializations: Individual neurons, attention heads, and circuits become selectively sensitive to syntactic structure, reflecting patterns observed in the human brain. While this specialization is well-documented, how it emerges during training and what influences its development remains largely unknown.   In this work, we tap into the black box of specialization by tracking its formation over time. By quantifying internal syntactic consistency across minimal pairs from various syntactic phenomena, we identify a clear developmental trajectory: Syntactic sensitivity emerges gradually, concentrates in specific layers, and exhibits a 'critical period' of rapid internal specialization. This process is consistent across architectures and initialization parameters (e.g., random seeds), and is influenced by model scale and training data. We therefore reveal not only where syntax arises in LLMs but also how some models internalize it during training. To support future research, we will release the code, models, and training checkpoints upon acceptance.

</details>


### [182] [Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents](https://arxiv.org/abs/2505.19549)

*Derong Xu, Yi Wen, Pengyue Jia, Yingyi Zhang, wenlin zhang, Yichao Wang, Huifeng Guo, Ruiming Tang, Xiangyu Zhao, Enhong Chen, Tong Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, long-term memory, conversational agents, memory retrieval, Gaussian Mixture Models

**Relevance Score:** 9

**TL;DR:** MemGAS is a new framework that improves long-term dialogue memory in conversational agents by using multi-granularity associations and adaptive selection to enhance memory retrieval and reduce noise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Long interactions in conversational agents lead to extensive dialogue records, challenging LLMs with limited context to maintain coherence and personalization in responses.

**Method:** MemGAS utilizes multi-granularity memory units and Gaussian Mixture Models to cluster memories, with an entropy-based router for optimal granularity selection and LLM-based filtering to refine retrievals.

**Key Contributions:**

	1. Introduction of multi-granularity memory units
	2. Adaptive selection of memory granularity based on query relevance
	3. Performance improvement on various long-term memory tasks

**Result:** Experiments show that MemGAS outperforms existing methods on four long-term memory benchmarks, demonstrating better performance in both question answering and retrieval tasks.

**Limitations:** 

**Conclusion:** MemGAS significantly enhances the ability of LLMs to manage long-term memory in dialogues, leading to improved user experiences in conversational agents.

**Abstract:** Large Language Models (LLMs) have recently been widely adopted in conversational agents. However, the increasingly long interactions between users and agents accumulate extensive dialogue records, making it difficult for LLMs with limited context windows to maintain a coherent long-term dialogue memory and deliver personalized responses. While retrieval-augmented memory systems have emerged to address this issue, existing methods often depend on single-granularity memory segmentation and retrieval. This approach falls short in capturing deep memory connections, leading to partial retrieval of useful information or substantial noise, resulting in suboptimal performance. To tackle these limits, we propose MemGAS, a framework that enhances memory consolidation by constructing multi-granularity association, adaptive selection, and retrieval. MemGAS is based on multi-granularity memory units and employs Gaussian Mixture Models to cluster and associate new memories with historical ones. An entropy-based router adaptively selects optimal granularity by evaluating query relevance distributions and balancing information completeness and noise. Retrieved memories are further refined via LLM-based filtering. Experiments on four long-term memory benchmarks demonstrate that MemGAS outperforms state-of-the-art methods on both question answer and retrieval tasks, achieving superior performance across different query types and top-K settings.

</details>


### [183] [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)

*Li Zeng, Zeming Liu, Chong Feng, Heyan Huang, Yuhang Guo*

**Main category:** cs.CL

**Keywords:** document-level, model editing, large language models, dataset, evaluation metrics

**Relevance Score:** 9

**TL;DR:** The paper introduces a dataset and task for document-level model editing in LLMs, addressing limitations of prior datasets focused on short outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To correct errors and outdated knowledge in LLMs with minimal cost and improve their usability in real-world document-level tasks.

**Method:** A new dataset, enchmarkname, is introduced for document-level model editing, featuring document-level inputs and outputs with extrapolative and multiple facts.

**Key Contributions:**

	1. Introduction of the enchmarkname dataset for document-level model editing
	2. Proposes new evaluation metrics for assessing model editing at the document level
	3. Highlights the challenges faced by current methods in document-level editing

**Result:** Experiments demonstrate that existing model editing methods struggle with the complexities of document-level editing tasks.

**Limitations:** 

**Conclusion:** The introduction of document-level model editing and the enchmarkname dataset is crucial for enhancing LLMs' capabilities in practical settings.

**Abstract:** Model editing aims to correct errors and outdated knowledge in the Large language models (LLMs) with minimal cost. Prior research has proposed a variety of datasets to assess the effectiveness of these model editing methods. However, most existing datasets only require models to output short phrases or sentences, overlooks the widespread existence of document-level tasks in the real world, raising doubts about their practical usability. Aimed at addressing this limitation and promoting the application of model editing in real-world scenarios, we propose the task of document-level model editing. To tackle such challenges and enhance model capabilities in practical settings, we introduce \benchmarkname, a dataset focused on document-level model editing, characterized by document-level inputs and outputs, extrapolative, and multiple facts within a single edit. We propose a series of evaluation metrics and experiments. The results show that the difficulties in document-level model editing pose challenges for existing model editing methods.

</details>


### [184] [TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](https://arxiv.org/abs/2505.19586)

*Dingyu Yao, Bowen Shen, Zheng Lin, Wei Liu, Jian Luan, Bin Wang, Weiping Wang*

**Main category:** cs.CL

**Keywords:** Key-Value cache, Large language models, Hybrid compression, Quantization, Offloading

**Relevance Score:** 8

**TL;DR:** TailorKV offers a hybrid compression method for LLM caches that integrates quantization and offloading, achieving nearly lossless performance with reduced memory overhead.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the substantial memory overhead of the KV cache in generative LLMs and to mitigate latency and performance degradation caused by existing methods.

**Method:** TailorKV employs a hybrid compression technique that combines loading dominant tokens with the quantization of all tokens, creating a hardware-friendly inference framework.

**Key Contributions:**

	1. Introduction of TailorKV, a hybrid compression method for KV caches in LLMs.
	2. Demonstration of nearly lossless performance under aggressive compression settings.
	3. Implementation of a hardware-friendly inference framework optimized for long-context evaluations.

**Result:** TailorKV outperforms state-of-the-art methods with nearly lossless performance, demonstrated through evaluations on long contexts, particularly with the Llama-3.1-8B model.

**Limitations:** 

**Conclusion:** The proposed method provides an effective solution for reducing memory overhead while maintaining high performance in LLM inference.

**Abstract:** The Key-Value (KV) cache in generative large language models (LLMs) introduces substantial memory overhead. Existing works mitigate this burden by offloading or compressing the KV cache. However, loading the entire cache incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU communication, while aggressive compression causes notable performance degradation. We identify that certain layers in the LLM need to maintain global information and are unsuitable for selective loading. In contrast, other layers primarily focus on a few tokens with dominant activations that potentially incur substantial quantization error. This observation leads to a key insight that loading dominant tokens and quantizing all tokens can complement each other. Building on this insight, we propose a hybrid compression method, TailorKV, which seamlessly integrates quantization and offloading. TailorKV develops an inference framework along with a hardware-friendly implementation that leverages these complementary characteristics. Extensive long-context evaluations exhibit that TailorKV achieves nearly lossless performance under aggressive compression settings, outperforming the state-of-the-art. Particularly, the Llama-3.1-8B with 128k context can be served within a single RTX 3090 GPU, reaching 82 ms per token during decoding.

</details>


### [185] [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)

*Yufan Dang, Chen Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** large language models, multi-agent collaboration, reinforcement learning

**Relevance Score:** 7

**TL;DR:** Proposes a puppeteer-style paradigm for dynamic multi-agent collaboration among LLMs to improve scalability and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of static organizational structures in LLM-enabled multi-agent systems, which lead to coordination overhead and inefficiencies as task complexity increases.

**Method:** A puppeteer orchestrator dynamically directs puppets (agents) in response to evolving task states, trained via reinforcement learning to adaptively sequence and prioritize agents for flexible collective reasoning.

**Key Contributions:**

	1. Introduction of a dynamic orchestrator for LLM multi-agent collaboration
	2. Utilization of reinforcement learning to adapt agent coordination
	3. Demonstration of compact cyclic reasoning structures improving efficiency

**Result:** The proposed method demonstrates superior performance and reduced computational costs in various experimental scenarios, attributed to the development of compact cyclic reasoning structures.

**Limitations:** 

**Conclusion:** Dynamic orchestration significantly enhances multi-agent collaboration in LLMs, improving adaptability and performance in complex problem-solving tasks.

**Abstract:** Large language models (LLMs) have achieved remarkable results across diverse downstream tasks, but their monolithic nature restricts scalability and efficiency in complex problem-solving. While recent research explores multi-agent collaboration among LLMs, most approaches rely on static organizational structures that struggle to adapt as task complexity and agent numbers grow, resulting in coordination overhead and inefficiencies. To this end, we propose a puppeteer-style paradigm for LLM-based multi-agent collaboration, where a centralized orchestrator ("puppeteer") dynamically directs agents ("puppets") in response to evolving task states. This orchestrator is trained via reinforcement learning to adaptively sequence and prioritize agents, enabling flexible and evolvable collective reasoning. Experiments on closed- and open-domain scenarios show that this method achieves superior performance with reduced computational costs. Analyses further reveal that the key improvements consistently stem from the emergence of more compact, cyclic reasoning structures under the orchestrator's evolution.

</details>


### [186] [Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study](https://arxiv.org/abs/2505.19598)

*Guanyu Hou, Jiaming He, Yinhang Zhou, Ji Guo, Yitong Qiao, Rui Zhang, Wenbo Jiang*

**Main category:** cs.CL

**Keywords:** Large Audio-Language Models, robustness, malicious audio attacks, multi-modal defenses

**Relevance Score:** 6

**TL;DR:** This paper evaluates the robustness of Large Audio-Language Models (LALMs) against various malicious audio injection attacks, revealing performance disparities and suggesting new strategies for enhancing model security.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored vulnerabilities of LALMs against malicious audio attacks in real-world applications.

**Method:** The study systematically evaluates five leading LALMs across four attack scenarios using metrics like Defense Success Rate, Context Robustness Score, and Judgment Robustness Index.

**Key Contributions:**

	1. Introduces a benchmark framework for evaluating LALMs against audio injection attacks.
	2. Identifies the need for tailored defense strategies based on attack type and model characteristics.
	3. Reveals a negative correlation between instruction-following capability and robustness, prompting further design considerations.

**Result:** Significant performance disparities were found among models; no single model outperformed others consistently. Attack effectiveness is influenced by the position of malicious content, and models adhering strictly to instructions were found to be more susceptible to attacks.

**Limitations:** 

**Conclusion:** The research highlights the importance of integrating robustness into model training, developing multi-modal defenses, and rethinking architectural designs for secure deployment of LALMs.

**Abstract:** Large Audio-Language Models (LALMs) are increasingly deployed in real-world applications, yet their robustness against malicious audio injection attacks remains underexplored. This study systematically evaluates five leading LALMs across four attack scenarios: Audio Interference Attack, Instruction Following Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics like Defense Success Rate, Context Robustness Score, and Judgment Robustness Index, their vulnerabilities and resilience were quantitatively assessed. Experimental results reveal significant performance disparities among models; no single model consistently outperforms others across all attack types. The position of malicious content critically influences attack effectiveness, particularly when placed at the beginning of sequences. A negative correlation between instruction-following capability and robustness suggests models adhering strictly to instructions may be more susceptible, contrasting with greater resistance by safety-aligned models. Additionally, system prompts show mixed effectiveness, indicating the need for tailored strategies. This work introduces a benchmark framework and highlights the importance of integrating robustness into training pipelines. Findings emphasize developing multi-modal defenses and architectural designs that decouple capability from susceptibility for secure LALMs deployment.

</details>


### [187] [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)

*Andrew Gambardella, Takeshi Kojima, Yusuke Iwasawa, Yutaka Matsuo*

**Main category:** cs.CL

**Keywords:** language models, grammar evaluation, perplexity, Japanese, tokenization

**Relevance Score:** 8

**TL;DR:** This paper analyzes the performance of language models on nuanced grammar evaluation, particularly focusing on the first person psych predicate restriction in Japanese.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing metrics for evaluating language models fail to capture nuanced grammatical capabilities, especially in languages other than English.

**Method:** We measure the perplexity of language models, specifically examining Weblab and Llama 3, in response to grammatical constraints in Japanese.

**Key Contributions:**

	1. Introduced perplexity measurement for nuanced grammar evaluation in Japanese.
	2. Demonstrated Weblab's unique performance characteristics among tested models.
	3. Evidence that tokenization quality impacts language model output accuracy.

**Result:** Weblab shows higher perplexity for ungrammatical psych predicate sentences, and Llama 3's perplexity improves significantly with well-behaved tokenizations.

**Limitations:** The findings are specific to the tested models and may not generalize to all language models or grammar points.

**Conclusion:** Grammar evaluation using perplexity can reveal significant differences in model performance based on tokenization, influencing machine translation outputs.

**Abstract:** Typical methods for evaluating the performance of language models evaluate their ability to answer questions accurately. These evaluation metrics are acceptable for determining the extent to which language models can understand and reason about text in a general sense, but fail to capture nuanced capabilities, such as the ability of language models to recognize and obey rare grammar points, particularly in languages other than English. We measure the perplexity of language models when confronted with the "first person psych predicate restriction" grammar point in Japanese. Weblab is the only tested open source model in the 7-10B parameter range which consistently assigns higher perplexity to ungrammatical psych predicate sentences than grammatical ones. We give evidence that Weblab's uniformly bad tokenization is a possible root cause for its good performance, and show that Llama 3's perplexity on grammatical psych predicate sentences can be reduced by orders of magnitude (28x difference) by restricting test sentences to those with uniformly well-behaved tokenizations. We show in further experiments on machine translation tasks that language models will use alternative grammar patterns in order to produce grammatical sentences when tokenization issues prevent the most natural sentence from being output.

</details>


### [188] [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)

*Ahan Prasannakumar Shetty*

**Main category:** cs.CL

**Keywords:** Machine Translation, English-Hindi, Evaluation Metrics, Natural Language Processing, Translation Models

**Relevance Score:** 4

**TL;DR:** This paper evaluates various machine translation models for English-Hindi translation using a large corpus and custom datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge linguistic gaps between English and Hindi through effective machine translation.

**Method:** Evaluation of various machine translation models using a diverse set of automatic evaluation metrics and a parallel dataset of over 18,000 pairs.

**Key Contributions:**

	1. Comprehensive evaluation of English-Hindi machine translation models.
	2. Utilization of a large parallel corpus and custom FAQ dataset for testing.
	3. Insights into strengths and weaknesses of current translation systems.

**Result:** Different models show varying performance levels, indicating strengths and areas for improvement across metrics.

**Limitations:** 

**Conclusion:** The effectiveness of different translation approaches varies, providing insights into both general and specialized language handling.

**Abstract:** Machine translation has become a critical tool in bridging linguistic gaps, especially between languages as diverse as English and Hindi. This paper comprehensively evaluates various machine translation models for translating between English and Hindi. We assess the performance of these models using a diverse set of automatic evaluation metrics, both lexical and machine learning-based metrics. Our evaluation leverages an 18000+ corpus of English Hindi parallel dataset and a custom FAQ dataset comprising questions from government websites. The study aims to provide insights into the effectiveness of different machine translation approaches in handling both general and specialized language domains. Results indicate varying performance levels across different metrics, highlighting strengths and areas for improvement in current translation systems.

</details>


### [189] [Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically](https://arxiv.org/abs/2505.19606)

*Ryan Soh-Eun Shim, Domenico De Cristofaro, Chengzhi Martin Hu, Alessandro Vietti, Barbara Plank*

**Main category:** cs.CL

**Keywords:** cross-lingual alignment, speech models, semantic knowledge, spoken translation, low-resource languages

**Relevance Score:** 8

**TL;DR:** This paper investigates cross-lingual alignment in speech foundation models, exploring whether it operates on semantic grounds rather than relying solely on phonetic similarities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to determine whether methods of cross-lingual alignment, previously established in text-based language models, can be applied to speech models, particularly in the context of spoken translation retrieval.

**Method:** The authors conduct pronunciation-controlled experiments to test the stability of spoken translation retrieval accuracy without phonetic cues and perform controlled experiments on a dataset of cross-lingual synonyms and near-homophones.

**Key Contributions:**

	1. Demonstrated cross-lingual alignment in speech models on a semantic basis
	2. Established the impact of early exiting strategies on speech recognition accuracy
	3. Validated the presence of phonetic and semantic knowledge in the encoder.

**Result:** The experiments reveal that spoken translation retrieval accuracy remains stable without phonetic cues, confirming both phonetic and semantic knowledge in the encoder. Additionally, improved speech recognition accuracy is achieved in seven low-resource languages using early exiting strategies.

**Limitations:** The study primarily focuses on low-resource languages and may not generalize to more resource-rich languages or other speech contexts.

**Conclusion:** The findings suggest that cross-lingual alignment in speech models can be based on semantic understanding rather than phonetic similarity, leading to better speech recognition outcomes in low-resource languages.

**Abstract:** Cross-lingual alignment in pretrained language models (LMs) has enabled efficient transfer in text-based LMs. Such an alignment has also been observed in speech foundation models. However, it remains an open question whether findings and methods from text-based cross-lingual alignment apply to speech. Building on prior work on spoken translation retrieval, we perform pronunciation-controlled experiments to observe if cross-lingual alignment can indeed occur in such models on a semantic basis, instead of relying on phonetic similarities. Our findings indicate that even in the absence of phonetic cues, spoken translation retrieval accuracy remains relatively stable. We follow up with a controlled experiment on a word-level dataset of cross-lingual synonyms and near-homophones, confirming the existence of both phonetic and semantic knowledge in the encoder. Finally, we qualitatively examine the transcriptions produced by early exiting the encoder, where we observe that speech translation produces semantic errors that are characterized by phonetic similarities to corresponding words in the source language. We apply this insight from early exiting to speech recognition in seven low-resource languages unsupported by the Whisper model, and achieve improved accuracy in all languages examined, particularly for languages with transparent orthographies.

</details>


### [190] [HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices](https://arxiv.org/abs/2505.19628)

*Silin Li, Yuhang Guo, Jiashu Yao, Zeming Liu, Haifeng Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Smart Home, Dataset, Human-Computer Interaction, Instruction Following

**Relevance Score:** 9

**TL;DR:** Introducing HomeBench, a dataset for evaluating LLMs in smart home scenarios, including valid and invalid device instructions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLM-based smart home assistants by addressing the limitations in handling complex user instructions and multiple device operations.

**Method:** Development of the HomeBench dataset featuring diverse valid and invalid instructions across single and multiple smart home devices, and evaluation of 13 distinct LLMs on their ability to execute these instructions.

**Key Contributions:**

	1. Introduction of HomeBench dataset for smart home instructions
	2. Evaluation of LLM performance across varied user commands
	3. Identification of critical challenges in LLM-based smart home integrations

**Result:** Experimental results show that current LLMs, like GPT-4o, perform poorly on invalid multi-device instructions, indicating a significant gap in capability.

**Limitations:** Existing LLMs struggle with invalid multi-device instructions; performance is significantly lacking even with advanced techniques.

**Conclusion:** HomeBench serves as a foundational dataset to advance research in LLM applications for smart home environments, highlighting current limitations and areas for improvement.

**Abstract:** Large language models (LLMs) have the potential to revolutionize smart home assistants by enhancing their ability to accurately understand user needs and respond appropriately, which is extremely beneficial for building a smarter home environment. While recent studies have explored integrating LLMs into smart home systems, they primarily focus on handling straightforward, valid single-device operation instructions. However, real-world scenarios are far more complex and often involve users issuing invalid instructions or controlling multiple devices simultaneously. These have two main challenges: LLMs must accurately identify and rectify errors in user instructions and execute multiple user instructions perfectly. To address these challenges and advance the development of LLM-based smart home assistants, we introduce HomeBench, the first smart home dataset with valid and invalid instructions across single and multiple devices in this paper. We have experimental results on 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the scenario of invalid multi-device instructions, revealing that the existing state-of-the-art LLMs still cannot perform well in this situation even with the help of in-context learning, retrieval-augmented generation, and fine-tuning. Our code and dataset are publicly available at https://github.com/BITHLP/HomeBench.

</details>


### [191] [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630)

*Yichun Feng, Jiawei Wang, Lu Zhou, Yixue Li*

**Main category:** cs.CL

**Keywords:** reinforcement learning, medical consultation, large language models, multi-turn dialogue, health informatics

**Relevance Score:** 9

**TL;DR:** DoctorAgent-RL is a reinforcement learning framework for improving multi-turn medical consultations by dynamically optimizing questioning strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing biomedical question answering systems face challenges in clinical consultations due to one-way communication and poor generalizability in static learning models.

**Method:** The study employs a reinforcement learning framework where a doctor agent interacts with a patient agent to optimize information gathering during consultations.

**Key Contributions:**

	1. Introduces DoctorAgent-RL, a RL-based collaborative framework for medical consultations.
	2. Constructs the MTMedDialog dataset for simulating patient interactions in multi-turn dialogues.
	3. Enhances diagnostic performance and reasoning capabilities over traditional models.

**Result:** DoctorAgent-RL demonstrates improved multi-turn reasoning capabilities and diagnostic performance compared to existing models.

**Limitations:** 

**Conclusion:** The proposed framework shows practical applications in assisting clinical consultations by enabling LLMs to adopt clinical reasoning strategies through dynamic interactions.

**Abstract:** Large language models (LLMs) have demonstrated excellent capabilities in the field of biomedical question answering, but their application in real-world clinical consultations still faces core challenges. Existing systems rely on a one-way information transmission mode where patients must fully describe their symptoms in a single round, leading to nonspecific diagnostic recommendations when complaints are vague. Traditional multi-turn dialogue methods based on supervised learning are constrained by static data-driven paradigms, lacking generalizability and struggling to intelligently extract key clinical information. To address these limitations, we propose DoctorAgent-RL, a reinforcement learning (RL)-based multi-agent collaborative framework that models medical consultations as a dynamic decision-making process under uncertainty. The doctor agent continuously optimizes its questioning strategy within the RL framework through multi-turn interactions with the patient agent, dynamically adjusting its information-gathering path based on comprehensive rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables LLMs to autonomously develop interaction strategies aligned with clinical reasoning logic, rather than superficially imitating patterns in existing dialogue data. Notably, we constructed MTMedDialog, the first English multi-turn medical consultation dataset capable of simulating patient interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing models in both multi-turn reasoning capability and final diagnostic performance, demonstrating practical value in assisting clinical consultations. https://github.com/JarvisUSTC/DoctorAgent-RL

</details>


### [192] [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)

*Zihong Zhang, Liqi He, Zuchao Li, Lefei Zhang, Hai Zhao, Bo Du*

**Main category:** cs.CL

**Keywords:** word segmentation, Large Language Models, unsupervised learning, Aho-Corasick automata, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces a new framework for unsupervised word segmentation using Large Language Models (LLMs), revealing enhanced performance with more parameters and a novel method called LLACA, which combines Aho-Corasick automata with LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of traditional word segmentation methods in NLP by leveraging LLMs to improve segmentation accuracy and semantic understanding.

**Method:** The authors evaluate mainstream LLMs for word segmentation across multiple languages and propose a new unsupervised method, LLACA, which combines the capabilities of Aho-Corasick automata with insights from LLMs to dynamically adjust n-gram models.

**Key Contributions:**

	1. Introduction of LLACA, a novel unsupervised word segmentation method.
	2. Demonstration of LLMs' effectiveness in word segmentation across multiple languages.
	3. Analysis of the correlation between model size and segmentation performance.

**Result:** LLMs demonstrated the ability to segment raw text effectively, with performance improvements observed in larger models. LLACA showed significant advantages over traditional segmentation methods by incorporating contextual semantics from LLMs.

**Limitations:** 

**Conclusion:** The integration of LLMs and Aho-Corasick automata presents a promising approach for unsupervised word segmentation, achieving higher accuracy and flexibility in language processing tasks.

**Abstract:** Word segmentation stands as a cornerstone of Natural Language Processing (NLP). Based on the concept of "comprehend first, segment later", we propose a new framework to explore the limit of unsupervised word segmentation with Large Language Models (LLMs) and evaluate the semantic understanding capabilities of LLMs based on word segmentation. We employ current mainstream LLMs to perform word segmentation across multiple languages to assess LLMs' "comprehension". Our findings reveal that LLMs are capable of following simple prompts to segment raw text into words. There is a trend suggesting that models with more parameters tend to perform better on multiple languages. Additionally, we introduce a novel unsupervised method, termed LLACA ($\textbf{L}$arge $\textbf{L}$anguage Model-Inspired $\textbf{A}$ho-$\textbf{C}$orasick $\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities of Aho-Corasick automata, LLACA innovatively combines these with the deep insights of well-pretrained LLMs. This approach not only enables the construction of a dynamic $n$-gram model that adjusts based on contextual information but also integrates the nuanced understanding of LLMs, offering significant improvements over traditional methods. Our source code is available at https://github.com/hkr04/LLACA

</details>


### [193] [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)

*Zili Wang, Tianyu Zhang, Haoli Bai, Lu Hou, Xianzhi Yu, Wulong Liu, Shiming Xiang, Lei Zhu*

**Main category:** cs.CL

**Keywords:** Test-Time Scaling, Large Language Models, Latency Optimization, Parallelism, Inference Efficiency

**Relevance Score:** 8

**TL;DR:** This paper addresses the efficiency of Test-Time Scaling (TTS) for Large Language Models (LLMs) by optimizing for latency in inference scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing TTS methods do not adequately consider the effects of latency, which is critical in many applications.

**Method:** The authors propose latency-optimal TTS through branch-wise parallelism and sequence-wise parallelism, optimizing concurrency configurations in LLM inference.

**Key Contributions:**

	1. Introduction of latency-aware Test-Time Scaling (TTS) methods for LLMs
	2. Optimization techniques for concurrency configurations in inference
	3. Demonstration of trade-offs between speed and accuracy in latency-sensitive environments

**Result:** A 32B model achieves 82.3% accuracy on MATH-500 within 1 minute, while a 3B model reaches 72.4% accuracy within 10 seconds.

**Limitations:** The study focuses primarily on TTS methods without extensive exploration of other latency-reducing techniques.

**Conclusion:** The proposed latency-aware TTS successfully balances speed and accuracy in critical latency scenarios, highlighting its significance.

**Abstract:** Test-Time Scaling (TTS) has proven effective in improving the performance of Large Language Models (LLMs) during inference. However, existing research has overlooked the efficiency of TTS from a latency-sensitive perspective. Through a latency-aware evaluation of representative TTS methods, we demonstrate that a compute-optimal TTS does not always result in the lowest latency in scenarios where latency is critical. To address this gap and achieve latency-optimal TTS, we propose two key approaches by optimizing the concurrency configurations: (1) branch-wise parallelism, which leverages multiple concurrent inference branches, and (2) sequence-wise parallelism, enabled by speculative decoding. By integrating these two approaches and allocating computational resources properly to each, our latency-optimal TTS enables a 32B model to reach 82.3% accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4% within 10 seconds. Our work emphasizes the importance of latency-aware TTS and demonstrates its ability to deliver both speed and accuracy in latency-sensitive scenarios.

</details>


### [194] [Interleaved Reasoning for Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.19640)

*Roy Xie, David Qiu, Deepak Gopinath, Dong Lin, Yanchao Sun, Chong Wang, Saloni Potdar, Bhuwan Dhingra*

**Main category:** cs.CL

**Keywords:** reinforcement learning, large language models, interleaved reasoning, multi-hop questions, question answering

**Relevance Score:** 8

**TL;DR:** Proposes a reinforcement learning approach for interleaved reasoning in large language models, improving efficiency and accuracy in multi-hop questions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies caused by extensive reasoning traces in large language models, which increase time-to-first-token (TTFT).

**Method:** A novel training paradigm that interleaves thinking and answering using reinforcement learning (RL) to guide reasoning.

**Key Contributions:**

	1. Introduction of interleaved reasoning in LLMs via reinforcement learning
	2. A simple rule-based reward system to guide reasoning
	3. Extensive experimental validation across multiple datasets and RL algorithms

**Result:** The proposed method reduces TTFT by over 80% on average and improves Pass@1 accuracy by up to 19.3%, showing strong generalization to complex reasoning datasets.

**Limitations:** 

**Conclusion:** The approach enables efficient reasoning in language models while offering valuable insights into conditional reward modeling.

**Abstract:** Long chain-of-thought (CoT) significantly enhances large language models' (LLM) reasoning capabilities. However, the extensive reasoning traces lead to inefficiencies and an increased time-to-first-token (TTFT). We propose a novel training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs to interleave thinking and answering for multi-hop questions. We observe that models inherently possess the ability to perform interleaved reasoning, which can be further enhanced through RL. We introduce a simple yet effective rule-based reward to incentivize correct intermediate steps, which guides the policy model toward correct reasoning paths by leveraging intermediate signals generated during interleaved reasoning. Extensive experiments conducted across five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++) demonstrate consistent improvements over traditional think-answer reasoning, without requiring external tools. Specifically, our approach reduces TTFT by over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore, our method, trained solely on question answering and logical reasoning datasets, exhibits strong generalization ability to complex reasoning datasets such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to reveal several valuable insights into conditional reward modeling.

</details>


### [195] [Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation](https://arxiv.org/abs/2505.19647)

*Xiaochuan Liu, Ruihua Song, Xiting Wang, Xu Chen*

**Main category:** cs.CL

**Keywords:** related work generation, multi-agent framework, graph-aware strategies

**Relevance Score:** 8

**TL;DR:** This paper proposes a multi-agent framework for automatic related work generation (RWG) that addresses limitations in existing methods by employing full-text input and graph-aware strategies for better comprehension and relationship capturing among reference papers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the effectiveness of automatic related work generation by overcoming shallow comprehension issues and isolated explanations in existing RWG methods.

**Method:** The proposed framework includes three agents: a selector to determine which section of papers to read next, a reader to digest selected sections and update shared memory, and a writer to generate the related work section from the curated memory. Graph-aware strategies are integrated into the selector to optimize reading orders based on relationship constraints among reference papers.

**Key Contributions:**

	1. Introduction of a multi-agent framework for RWG
	2. Implementation of graph-aware strategies in the selector
	3. Demonstration of state-of-the-art performance across multiple configurations

**Result:** Extensive experiments show that the multi-agent framework consistently enhances performance across various models and input configurations, with graph-aware selectors achieving state-of-the-art results compared to alternative methods.

**Limitations:** 

**Conclusion:** The proposed multi-agent framework significantly improves automatic related work generation by enabling deeper comprehension and better relationship capturing among references, demonstrating its efficacy in RWG tasks.

**Abstract:** Automatic related work generation (RWG) can save people's time and effort when writing a draft of related work section (RWS) for further revision. However, existing methods for RWG always suffer from shallow comprehension due to taking the limited portions of references papers as input and isolated explanation for each reference due to ineffective capturing the relationships among them. To address these issues, we focus on full-text-based RWG task and propose a novel multi-agent framework. Our framework consists of three agents: a selector that decides which section of the papers is going to read next, a reader that digests the selected section and updates a shared working memory, and a writer that generates RWS based on the final curated memory. To better capture the relationships among references, we also propose two graph-aware strategies for selector, enabling to optimize the reading order with constrains of the graph structure. Extensive experiments demonstrate that our framework consistently improves performance across three base models and various input configurations. The graph-aware selectors outperform alternative selectors, achieving state-of-the-art results. The code and data are available at https://github.com/1190200817/Full_Text_RWG.

</details>


### [196] [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)

*Tingjia Shen, Hao Wang, Chuan Qin, Ruijun Sun, Yang Song, Defu Lian, Hengshu Zhu, Enhong Chen*

**Main category:** cs.CL

**Keywords:** Open-domain Question Answering, Large Language Models, Knowledge Integration

**Relevance Score:** 9

**TL;DR:** This paper presents GenKI, a novel framework for enhancing Open-domain Question Answering (OpenQA) by integrating retrieved knowledge into LLMs and enabling controllable output formats.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to overcome the challenges faced by LLM-based OpenQA methods, particularly focusing on knowledge integration and the generation of task-specific answer formats.

**Method:** The authors propose a framework called GenKI that trains a dense passage retrieval model for knowledge retrieval, integrates the retrieved knowledge into fine-tuning instructions, and utilizes a fine-tuned LLM to ensure controllable generation with the assistance of text consistency.

**Key Contributions:**

	1. Introduction of the GenKI framework for OpenQA
	2. Training a dense passage retrieval model for effective knowledge extraction
	3. Development of a knowledge integration model for fine-tuning LLMs

**Result:** Experiments on diverse datasets (TriviaQA, MSMARCO, CMRC2018) show that GenKI significantly outperforms state-of-the-art baselines, with findings indicating a linear relationship between the frequency of retrieved knowledge and knowledge recall accuracy.

**Limitations:** 

**Conclusion:** The study concludes that GenKI is effective in enhancing OpenQA performance through integrated knowledge and controlled answer generation, encouraging future research in this direction.

**Abstract:** Open-domain question answering (OpenQA) represents a cornerstone in natural language processing (NLP), primarily focused on extracting answers from unstructured textual data. With the rapid advancements in Large Language Models (LLMs), LLM-based OpenQA methods have reaped the benefits of emergent understanding and answering capabilities enabled by massive parameters compared to traditional methods. However, most of these methods encounter two critical challenges: how to integrate knowledge into LLMs effectively and how to adaptively generate results with specific answer formats for various task situations. To address these challenges, we propose a novel framework named GenKI, which aims to improve the OpenQA performance by exploring Knowledge Integration and controllable Generation on LLMs simultaneously. Specifically, we first train a dense passage retrieval model to retrieve associated knowledge from a given knowledge base. Subsequently, we introduce a novel knowledge integration model that incorporates the retrieval knowledge into instructions during fine-tuning to intensify the model. Furthermore, to enable controllable generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based on text consistency incorporating all coherence, fluency, and answer format assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO, and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover, ablation studies have disclosed a linear relationship between the frequency of retrieved knowledge and the model's ability to recall knowledge accurately against the ground truth. Our code of GenKI is available at https://github.com/USTC-StarTeam/GenKI

</details>


### [197] [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)

*Weikang Yuan, Kaisong Song, Zhuoren Jiang, Junjie Cao, Yujie Zhang, Jun Lin, Kun Kuang, Ji Zhang, Xiaozhong Liu*

**Main category:** cs.CL

**Keywords:** legal consultation, Large Language Models, benchmark dataset, evaluation framework, dialogue systems

**Relevance Score:** 7

**TL;DR:** LeCoDe is a multi-turn benchmark dataset for legal consultations designed to improve LLMs' capabilities in legal dialogue, revealing significant performance challenges in current models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The high cost and limited accessibility of legal consultation necessitate scalable and effective LLM-based solutions, particularly for interactive scenarios.

**Method:** The paper introduces LeCoDe, a dataset of 3,696 legal consultation dialogues collected from live-streamed short-video platforms, and proposes a comprehensive evaluation framework for LLMs.

**Key Contributions:**

	1. Introduction of the LeCoDe benchmark dataset for legal consultation dialogues
	2. A comprehensive evaluation framework with metrics for clarification and advice quality
	3. Insights into the performance limitations of existing LLMs in legal contexts

**Result:** State-of-the-art LLMs like GPT-4 show only 39.8% recall for clarification tasks and a 59% overall score for advice quality, indicating substantial difficulties in achieving effective legal consultation.

**Limitations:** The dataset may not cover all aspects of legal consultations and may reflect the biases of the data collection process.

**Conclusion:** The findings reveal significant limitations in current LLM performance in legal contexts and suggest strategies for improving LLMs in professional consultation scenarios.

**Abstract:** Legal consultation is essential for safeguarding individual rights and ensuring access to justice, yet remains costly and inaccessible to many individuals due to the shortage of professionals. While recent advances in Large Language Models (LLMs) offer a promising path toward scalable, low-cost legal assistance, current systems fall short in handling the interactive and knowledge-intensive nature of real-world consultations. To address these challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset comprising 3,696 legal consultation dialogues with 110,008 dialogue turns, designed to evaluate and improve LLMs' legal consultation capability. With LeCoDe, we innovatively collect live-streamed consultations from short-video platforms, providing authentic multi-turn legal consultation dialogues. The rigorous annotation by legal experts further enhances the dataset with professional insights and expertise. Furthermore, we propose a comprehensive evaluation framework that assesses LLMs' consultation capabilities in terms of (1) clarification capability and (2) professional advice quality. This unified framework incorporates 12 metrics across two dimensions. Through extensive experiments on various general and domain-specific LLMs, our results reveal significant challenges in this task, with even state-of-the-art models like GPT-4 achieving only 39.8% recall for clarification and 59% overall score for advice quality, highlighting the complexity of professional consultation scenarios. Based on these findings, we further explore several strategies to enhance LLMs' legal consultation abilities. Our benchmark contributes to advancing research in legal domain dialogue systems, particularly in simulating more real-world user-expert interactions.

</details>


### [198] [Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models](https://arxiv.org/abs/2505.19670)

*Hao Yang, Lizhen Qu, Ehsan Shareghi, Gholamreza Haffari*

**Main category:** cs.CL

**Keywords:** Large Audio Language Models, Safety-alignment, Unsupervised fine-tuning, Machine Learning, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper proposes an unsupervised safety-fine-tuning strategy to enhance the safety-alignment of Large Audio Language Models (LALMs) while minimizing over-rejection issues.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in safety measures for text and vision-based LLMs, LALMs have been found to be lacking effective safety-alignment strategies, making them susceptible to harmful queries.

**Method:** The paper introduces an unsupervised safety-fine-tuning technique aimed at reshaping the model's representation space to improve safety-alignment without significantly increasing the rate of over-rejections.

**Key Contributions:**

	1. Unsupervised safety-fine-tuning strategy for LALMs
	2. Demonstration of significant safety improvements across different input modalities
	3. Analysis of over-rejection rates alongside safety enhancements

**Result:** Experiments across three generations of Qwen LALMs indicate that the proposed method greatly improves the safety of LALMs under various input conditions while only slightly increasing the over-rejection rate.

**Limitations:** The paper contains harmful examples and may not address all safety concerns for LALMs.

**Conclusion:** The unsupervised safety-fine-tuning strategy effectively enhances safety for LALMs, addressing a critical gap in existing safety measures.

**Abstract:** Large Audio Language Models (LALMs) have extended the capabilities of Large Language Models (LLMs) by enabling audio-based human interactions. However, recent research has revealed that LALMs remain vulnerable to harmful queries due to insufficient safety-alignment. Despite advances in defence measures for text and vision LLMs, effective safety-alignment strategies and audio-safety dataset specifically targeting LALMs are notably absent. Meanwhile defence measures based on Supervised Fine-tuning (SFT) struggle to address safety improvement while avoiding over-rejection issues, significantly compromising helpfulness. In this work, we propose an unsupervised safety-fine-tuning strategy as remedy that reshapes model's representation space to enhance existing LALMs safety-alignment while balancing the risk of over-rejection. Our experiments, conducted across three generations of Qwen LALMs, demonstrate that our approach significantly improves LALMs safety under three modality input conditions (audio-text, text-only, and audio-only) while increasing over-rejection rate by only 0.88% on average. Warning: this paper contains harmful examples.

</details>


### [199] [Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations](https://arxiv.org/abs/2505.19674)

*Chaoyi Xiang, Chunhua Liu, Simon De Deyne, Lea Frermann*

**Main category:** cs.CL

**Keywords:** large language models, moral reasoning, word associations, Moral Foundation Theory, human-computer interaction

**Relevance Score:** 8

**TL;DR:** The paper explores the moral values reflected by large language models (LLMs) using word associations as a more robust method than direct prompting.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the moral values reflected by LLMs is crucial due to their increasing impact, but assessing these values through direct means is difficult.

**Method:** The authors create a dataset of LLM-generated word associations and develop a novel method to propagate moral values through association graphs based on Moral Foundation Theory.

**Key Contributions:**

	1. Creation of a dataset of LLM-generated word associations
	2. Novel method for propagating moral values through association graphs
	3. Comparison of moral conceptualizations between human and LLM associations

**Result:** The study reveals systematic differences between the moral values associated with human English speakers and those inferred from LLMs.

**Limitations:** The study focuses only on English-speaking communities, which may limit generalizability to other cultures.

**Conclusion:** Using word associations provides a clearer understanding of the moral reasoning of LLMs compared to traditional prompting methods.

**Abstract:** As the impact of large language models increases, understanding the moral values they reflect becomes ever more important. Assessing the nature of moral values as understood by these models via direct prompting is challenging due to potential leakage of human norms into model training data, and their sensitivity to prompt formulation. Instead, we propose to use word associations, which have been shown to reflect moral reasoning in humans, as low-level underlying representations to obtain a more robust picture of LLMs' moral reasoning. We study moral differences in associations from western English-speaking communities and LLMs trained predominantly on English data. First, we create a large dataset of LLM-generated word associations, resembling an existing data set of human word associations. Next, we propose a novel method to propagate moral values based on seed words derived from Moral Foundation Theory through the human and LLM-generated association graphs. Finally, we compare the resulting moral conceptualizations, highlighting detailed but systematic differences between moral values emerging from English speakers and LLM associations.

</details>


### [200] [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)

*Liqin Ye, Agam Shah, Chao Zhang, Sudheer Chava*

**Main category:** cs.CL

**Keywords:** noisy labels, large language models, NLP tasks, classifier robustness, label noise correction

**Relevance Score:** 9

**TL;DR:** This paper presents SiDyP, a method to enhance classifier robustness against noisy labels generated by LLMs by refining label candidates using a simplex diffusion model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of noisy labels generated by large language models in NLP tasks and improve model generalization.

**Method:** SiDyP retrieves potential true label candidates from neighborhood label distributions in text embedding space and iteratively refines them using a simplex diffusion model.

**Key Contributions:**

	1. Introduction of SiDyP framework for label correction.
	2. Demonstrated significant performance improvement on BERT classifier with LLM-generated data.
	3. Extensive benchmarking on various NLP tasks.

**Result:** SiDyP improves the performance of a fine-tuned BERT classifier on zero-shot and few-shot datasets by an average of 7.21% and 7.30%, respectively.

**Limitations:** Focuses primarily on LLM-generated label noise, which may not generalize to other types of noise.

**Conclusion:** The proposed SiDyP framework effectively enhances the robustness of classifiers dealing with LLM-generated noisy labels across various NLP tasks.

**Abstract:** The traditional process of creating labeled datasets is labor-intensive and expensive. Recent breakthroughs in open-source large language models (LLMs) have opened up a new avenue in generating labeled datasets automatically for various natural language processing (NLP) tasks, providing an alternative to such an expensive annotation process. However, the reliability of such auto-generated labels remains a significant concern due to inherent inaccuracies. When learning from noisy labels, the model's generalization is likely to be harmed as it is prone to overfit to those label noises. While previous studies in learning from noisy labels mainly focus on synthetic noise and real-world noise, LLM-generated label noise receives less attention. In this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to calibrate the classifier's prediction, thus enhancing its robustness towards LLM-generated noisy labels. SiDyP retrieves potential true label candidates by neighborhood label distribution in text embedding space and iteratively refines noisy candidates using a simplex diffusion model. Our framework can increase the performance of the BERT classifier fine-tuned on both zero-shot and few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30% respectively. We demonstrate the effectiveness of SiDyP by conducting extensive benchmarking for different LLMs over a variety of NLP tasks. Our code is available on Github.

</details>


### [201] [Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs](https://arxiv.org/abs/2505.19678)

*Hao Fang, Changle Zhou, Jiawei Kong, Kuofeng Gao, Bin Chen, Tao Liang, Guojun Ma, Shu-Tao Xia*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Hallucinations, Conditional Pointwise Mutual Information, Text-Image Dependencies, Decoding Strategy

**Relevance Score:** 7

**TL;DR:** This paper introduces a Conditional Pointwise Mutual Information (C-PMI) decoding strategy to reduce hallucinations in Large Vision-Language Models (LVLMs) by reinforcing the relationship between text and image tokens during generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Vision-Language Models suffer from hallucinations, generating plausible but irrelevant responses, primarily due to reliance on language priors over visual information.

**Method:** The authors propose a C-PMI calibrated decoding strategy that models the contributions of visual and textual tokens, treating hallucination mitigation as a bi-level optimization problem to enhance mutual information.

**Key Contributions:**

	1. Introduction of C-PMI calibrated decoding strategy
	2. Joint modeling of visual and textual token contributions
	3. Development of a token purification mechanism for relevant token sampling

**Result:** Experiments show that the C-PMI strategy significantly reduces hallucinations in LVLMs while maintaining decoding efficiency across various benchmarks.

**Limitations:** 

**Conclusion:** The proposed method effectively addresses hallucination issues in LVLMs by improving the synergy between text and image representations.

**Abstract:** Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where generated responses seem semantically plausible yet exhibit little or no relevance to the input image. Previous studies reveal that this issue primarily stems from LVLMs' over-reliance on language priors while disregarding the visual information during decoding. To alleviate this issue, we introduce a novel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding strategy, which adaptively strengthens the mutual dependency between generated texts and input images to mitigate hallucinations. Unlike existing methods solely focusing on text token sampling, we propose to jointly model the contributions of visual and textual tokens to C-PMI, formulating hallucination mitigation as a bi-level optimization problem aimed at maximizing mutual information. To solve it, we design a token purification mechanism that dynamically regulates the decoding process by sampling text tokens remaining maximally relevant to the given image, while simultaneously refining image tokens most pertinent to the generated response. Extensive experiments across various benchmarks reveal that the proposed method significantly reduces hallucinations in LVLMs while preserving decoding efficiency.

</details>


### [202] [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)

*Zhaolin Li, Yining Liu, Danni Liu, Tuan Nam Nguyen, Enes Yavuz Ugan, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Machine Translation, Speech Translation

**Relevance Score:** 5

**TL;DR:** The paper discusses the development of ASR and MT systems and E2E ST systems for low-resource languages, utilizing synthetic data and model regularization to enhance performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the challenge of low-resource language translation and improving translation quality by utilizing pre-trained models and synthetic data.

**Method:** The authors implemented cascaded ASR and MT systems, and end-to-end ST systems, fine-tuning them with various strategies and evaluating them on language pairs: Bemba, North Levantine Arabic, and Tunisian Arabic.

**Key Contributions:**

	1. Development of cascaded and E2E systems for low-resource languages
	2. Use of synthetic data to enhance ASR and ST performance
	3. Application of Minimum Bayes Risk decoding for performance improvement

**Result:** The use of synthetic data and text-to-speech models improved ASR and ST performance. Intra-distillation consistently enhanced model performance across tasks, and Minimum Bayes Risk decoding improved results by approximately 1.5 BLEU points.

**Limitations:** 

**Conclusion:** The findings highlight the effectiveness of synthetic data and system augmentation techniques in low-resource machine translation and speech translation systems.

**Abstract:** This paper presents KIT's submissions to the IWSLT 2025 low-resource track. We develop both cascaded systems, consisting of Automatic Speech Recognition (ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech Translation (ST) systems for three language pairs: Bemba, North Levantine Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we fine-tune our systems with different strategies to utilize resources efficiently. This study further explores system enhancement with synthetic data and model regularization. Specifically, we investigate MT-augmented ST by generating translations from ASR data using MT models. For North Levantine, which lacks parallel ST training data, a system trained solely on synthetic data slightly surpasses the cascaded system trained on real data. We also explore augmentation using text-to-speech models by generating synthetic speech from MT data, demonstrating the benefits of synthetic data in improving both ASR and ST performance for Bemba. Additionally, we apply intra-distillation to enhance model performance. Our experiments show that this approach consistently improves results across ASR, MT, and ST tasks, as well as across different pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine the cascaded and end-to-end systems, achieving an improvement of approximately 1.5 BLEU points.

</details>


### [203] [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)

*Yi Liu, Dianqing Liu, Mingye Zhu, Junbo Guo, Yongdong Zhang, Zhendong Mao*

**Main category:** cs.CL

**Keywords:** large language models, alignment, importance sampling, autoregressive model, resampling algorithm

**Relevance Score:** 9

**TL;DR:** The paper introduces a Residual Alignment Model (RAM) which enhances alignment of large language models (LLMs) for improved adaptability without retraining the entire model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rising use of LLMs, there is a need for quick and customizable outputs which traditional alignment methods struggle to deliver due to the necessity of retraining.

**Method:** The paper proposes RAM, which treats the alignment process as importance sampling using an autoregressive alignment module as an estimator for importance weights, allowing separate operation from the underlying model.

**Key Contributions:**

	1. Introduction of the Residual Alignment Model (RAM)
	2. Formalization of alignment as importance sampling
	3. Development of an efficient token-level decoding resampling algorithm.

**Result:** Experiments show that RAM outperforms existing baseline models on tasks such as instruction following and domain adaptation across two leading open-source LLMs.

**Limitations:** 

**Conclusion:** RAM provides a more flexible and efficient method for aligning large language models, addressing limitations of existing methods by decoupling the alignment and model training processes.

**Abstract:** The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \textit{Residual Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.

</details>


### [204] [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)

*Tej Deep Pala, Panshul Sharma, Amir Zadeh, Chuan Li, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** Large Language Models, mathematical problem solving, Process Reward Models

**Relevance Score:** 8

**TL;DR:** PathFinder-PRM is a novel hierarchical process reward model designed to enhance mathematical problem-solving by classifying errors at each step, achieving state-of-the-art performance with improved data efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models often hallucinate during reasoning-intensive tasks like mathematics, necessitating better error detection and evaluation methods to improve their performance.

**Method:** PathFinder-PRM classifies math and consistency errors at each step of problem-solving, combining these classifications to estimate correctness and guide generation towards coherent solutions.

**Key Contributions:**

	1. Introduction of a hierarchical error-aware discriminative Process Reward Model
	2. Construction of a 400K-sample dataset for training
	3. Achieving state-of-the-art performance with increased data efficiency

**Result:** On PRMBench, PathFinder-PRM achieved a PRMScore of 67.7, outperforming the previous best and using 3 times less data. It also improved reward-guided greedy search performance, yielding a prm@8 score of 48.3.

**Limitations:** 

**Conclusion:** Decoupling error detection from reward estimation enhances the detection of fine-grained errors and significantly improves the effectiveness of reward-guided mathematical reasoning while being more data-efficient.

**Abstract:** Large Language Models (LLMs) are prone to hallucination, especially during multi-hop and reasoning-intensive tasks such as mathematical problem solving. While Outcome Reward Models verify only final answers, Process Reward Models (PRMs) score each intermediate step to steer generation toward coherent solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware discriminative PRM that first classifies math and consistency errors at each step, then combines these fine-grained signals to estimate step correctness. To train PathFinder-PRM, we construct a 400K-sample dataset by enriching the human-annotated PRM800K corpus and RLHFlow Mistral traces with three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while using 3 times less data. When applied to reward guided greedy search, our model yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results demonstrate that decoupled error detection and reward estimation not only boost fine-grained error detection but also substantially improve end-to-end, reward-guided mathematical reasoning with greater data efficiency.

</details>


### [205] [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)

*Zhaopeng Feng, Yupu Liang, Shaosheng Cao, Jiayuan Su, Jiahan Ren, Zhe Xu, Yao Hu, Wenxuan Huang, Jian Wu, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** Text Image Machine Translation, Reinforcement Learning, Multimodal LLMs

**Relevance Score:** 8

**TL;DR:** The paper presents MT³, a framework using Multi-Task Reinforcement Learning for end-to-end Text Image Machine Translation, achieving state-of-the-art results.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexities of Text Image Machine Translation (TIMT) which includes challenges in OCR, visual-text reasoning, and translation.

**Method:** The MT³ framework applies Multi-Task Reinforcement Learning to MLLMs, focusing on text recognition, context-aware reasoning, and translation, using a unique multi-mixed reward mechanism.

**Key Contributions:**

	1. Introduction of MT³ for TIMT using Multi-Task RL
	2. Development of XHSPost benchmark for social media TIMT
	3. State-of-the-art results on MIT-10M benchmark

**Result:** MT³-7B-Zero outperforms baseline models on the MIT-10M benchmark and demonstrates strong generalization capabilities.

**Limitations:** 

**Conclusion:** The approach significantly improves TIMT performance through multi-task optimization and opens doors for better evaluation in social media contexts.

**Abstract:** Text Image Machine Translation (TIMT)-the task of translating textual content embedded in images-is critical for applications in accessibility, cross-lingual information access, and real-world document understanding. However, TIMT remains a complex challenge due to the need for accurate optical character recognition (OCR), robust visual-text reasoning, and high-quality translation, often requiring cascading multi-stage pipelines. Recent advances in large-scale Reinforcement Learning (RL) have improved reasoning in Large Language Models (LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is still underexplored. To bridge this gap, we introduce MT$^{3}$, the first framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts a multi-task optimization paradigm targeting three key sub-skills: text recognition, context-aware reasoning, and translation. It is trained using a novel multi-mixed reward mechanism that adapts rule-based RL strategies to TIMT's intricacies, offering fine-grained, non-binary feedback across tasks. Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural and real-world social media contexts, we introduced XHSPost, the first social media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on the latest in-domain MIT-10M benchmark, outperforming strong baselines such as Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics. Additionally, the model shows strong generalization to out-of-distribution language pairs and datasets. In-depth analyses reveal how multi-task synergy, reinforcement learning initialization, curriculum design, and reward formulation contribute to advancing MLLM-driven TIMT.

</details>


### [206] [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)

*Chunyang Jiang, Chi-min Chan, Yiyang Cai, Yulong Liu, Wei Xue, Yike Guo*

**Main category:** cs.CL

**Keywords:** graceful forgetting, generative language models, fine-tuning, negative transfer, Fisher Information Matrix

**Relevance Score:** 8

**TL;DR:** The paper introduces a framework called Learning With Forgetting (LWF) for graceful forgetting in generative language models, addressing negative transfer during fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the detrimental effects of negative transfer in pre-trained language models and improve fine-tuning performance.

**Method:** The framework uses Fisher Information Matrix to weight parameter updates, computing forgetting confidence to selectively unlearn irrelevant knowledge during fine-tuning.

**Key Contributions:**

	1. Introduction of Learning With Forgetting (LWF) framework for generative models.
	2. Application of Fisher Information Matrix for knowledge unlearning.
	3. Evidence of improved fine-tuning performance through graceful forgetting.

**Result:** Experiments show that applying graceful forgetting enhances fine-tuning performance in generative language models despite challenges in fully understanding knowledge interactions.

**Limitations:** The mechanisms of knowledge interaction in pre-trained language models remain challenging to fully uncover.

**Conclusion:** Graceful forgetting can improve the performance of generative language models during fine-tuning by allowing selective unlearning of irrelevant knowledge.

**Abstract:** Recently, the pretrain-finetune paradigm has become a cornerstone in various deep learning areas. While in general the pre-trained model would promote both effectiveness and efficiency of downstream tasks fine-tuning, studies have shown that not all knowledge acquired during pre-training is beneficial. Some of the knowledge may actually bring detrimental effects to the fine-tuning tasks, which is also known as negative transfer. To address this problem, graceful forgetting has emerged as a promising approach. The core principle of graceful forgetting is to enhance the learning plasticity of the target task by selectively discarding irrelevant knowledge. However, this approach remains underexplored in the context of generative language models, and it is often challenging to migrate existing forgetting algorithms to these models due to architecture incompatibility. To bridge this gap, in this paper we propose a novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting in generative language models. With Fisher Information Matrix weighting the intended parameter updates, LWF computes forgetting confidence to evaluate self-generated knowledge regarding the forgetting task, and consequently, knowledge with high confidence is periodically unlearned during fine-tuning. Our experiments demonstrate that, although thoroughly uncovering the mechanisms of knowledge interaction remains challenging in pre-trained language models, applying graceful forgetting can contribute to enhanced fine-tuning performance.

</details>


### [207] [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)

*Yihao Ai, Zhiyuan Ning, Weiwei Dai, Pengfei Wang, Yi Du, Wenjuan Cui, Kunpeng Liu, Yuanchun Zhou*

**Main category:** cs.CL

**Keywords:** biomedical entity linking, large language models, re-ranking, open-source LLMs, knowledge distillation

**Relevance Score:** 8

**TL;DR:** This paper introduces RPDR, a framework that leverages both closed-source and open-source large language models (LLMs) for biomedical entity linking, improving performance with limited annotated data.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve biomedical entity linking in scenarios with limited annotated data and reduce dependency on costly closed-source LLMs.

**Method:** RPDR utilizes a closed-source LLM to generate training data from unannotated data and then fine-tunes an open-source LLM for re-ranking candidates.

**Key Contributions:**

	1. Introduction of the RPDR framework for biomedical entity linking
	2. Combination of closed-source and open-source LLMs to enhance performance
	3. Demonstration of effectiveness on multilingual datasets

**Result:** RPDR demonstrates improved accuracy with 0.019 Acc@1 and 0.036 Acc@1 improvements on two different datasets when training data is insufficient.

**Limitations:** Limited to biomedical context and dependent on the quality of generated training data.

**Conclusion:** The proposed RPDR framework successfully combines the benefits of both closed-source and open-source LLMs, making it a viable solution for biomedical entity linking.

**Abstract:** Biomedical entity linking aims to map nonstandard entities to standard entities in a knowledge base. Traditional supervised methods perform well but require extensive annotated data to transfer, limiting their usage in low-resource scenarios. Large language models (LLMs), especially closed-source LLMs, can address these but risk stability issues and high economic costs: using these models is restricted by commercial companies and brings significant economic costs when dealing with large amounts of data. To address this, we propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs for re-ranking candidates retrieved by a retriever fine-tuned with a small amount of data. By prompting a closed-source LLM to generate training data from unannotated data and fine-tuning an open-source LLM for re-ranking, we effectively distill the knowledge to the open-source LLM that can be deployed locally, thus avoiding the stability issues and the problem of high economic costs. We evaluate RPDR on two datasets, including one real-world dataset and one publicly available dataset involving two languages: Chinese and English. RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier dataset and the Ask A Patient dataset when the amount of training data is not enough. The results demonstrate the superiority and generalizability of the proposed framework.

</details>


### [208] [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)

*Yang Zhang, Yu Yu, Bo Tang, Yu Zhu, Chuxiong Sun, Wenqiang Wei, Jie Hu, Zipeng Xie, Zhiyu Li, Feiyu Xiong, Edward Chung*

**Main category:** cs.CL

**Keywords:** Large Language Models, alignment techniques, ethical AI

**Relevance Score:** 9

**TL;DR:** MARA is a new token-level alignment approach for LLMs that enhances ethical applications by efficiently classifying token preferences, significantly improving alignment performance while lowering computational costs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for alignment techniques that can ensure ethical and safe application of Large Language Models without the high computational costs associated with current methods.

**Method:** The MARA approach involves a three-layer fully-connected network that performs binary classification on individual tokens to determine their acceptance or rejection in a response, independent of the larger language model.

**Key Contributions:**

	1. Introduction of the Micro token-level Accept-Reject Aligning (MARA) method
	2. Decomposition of sentence-level preferences into token-level classification
	3. Demonstrated efficiency and effectiveness across multiple LLMs and datasets

**Result:** MARA has shown significant improvements in alignment performance compared to existing techniques across seven LLMs and three open-source datasets, while also reducing computational costs.

**Limitations:** 

**Conclusion:** The proposed MARA approach provides a more efficient means of aligning LLMs with human preferences, indicating potential for broader application in ethical AI.

**Abstract:** With the rapid development of Large Language Models (LLMs), aligning these models with human preferences and values is critical to ensuring ethical and safe applications. However, existing alignment techniques such as RLHF or DPO often require direct fine-tuning on LLMs with billions of parameters, resulting in substantial computational costs and inefficiencies. To address this, we propose Micro token-level Accept-Reject Aligning (MARA) approach designed to operate independently of the language models. MARA simplifies the alignment process by decomposing sentence-level preference learning into token-level binary classification, where a compact three-layer fully-connected network determines whether candidate tokens are "Accepted" or "Rejected" as part of the response. Extensive experiments across seven different LLMs and three open-source datasets show that MARA achieves significant improvements in alignment performance while reducing computational costs.

</details>


### [209] [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)

*Ruisheng Cao, Hanchong Zhang, Tiancheng Huang, Zhangyi Kang, Yuxin Zhang, Liangtai Sun, Hanqi Li, Yuxun Miao, Shuai Fan, Lu Chen, Kai Yu*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, Large Language Models, Hybrid Retrieval, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** Introducing NeuSym-RAG, a hybrid retrieval framework that integrates neural and symbolic approaches for effective academic paper querying.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges researchers face in efficiently acquiring key information from an expanding pool of academic papers, especially through enhanced retrieval mechanisms.

**Method:** NeuSym-RAG employs a hybrid neural-symbolic retrieval approach, utilizing multi-view chunking and schema-based parsing to organize PDF content into a relational database and vector store.

**Key Contributions:**

	1. Development of a hybrid neural-symbolic retrieval framework.
	2. Implementation of multi-view chunking for better data representation.
	3. Public availability of code and datasets for reproducibility.

**Result:** Experiments demonstrate that NeuSym-RAG outperforms traditional vector-based retrieval methods and structured baselines on three PDF-based QA datasets, including the self-annotated AIRQA-REAL.

**Limitations:** 

**Conclusion:** NeuSym-RAG successfully integrates complementary retrieval strategies, enhancing the performance of large language model agents in academic querying tasks.

**Abstract:** The increasing number of academic papers poses significant challenges for researchers to efficiently acquire key details. While retrieval augmented generation (RAG) shows great promise in large language model (LLM) based automated question answering, previous works often isolate neural and symbolic retrieval despite their complementary strengths. Moreover, conventional single-view chunking neglects the rich structure and layout of PDFs, e.g., sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural symbolic retrieval framework which combines both paradigms in an interactive process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG organizes semi-structured PDF content into both the relational database and vectorstore, enabling LLM agents to iteratively gather context until sufficient to generate answers. Experiments on three full PDF-based QA datasets, including a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the vector-based RAG and various structured baselines, highlighting its capacity to unify both retrieval schemes and utilize multiple views. Code and data are publicly available at https://github.com/X-LANCE/NeuSym-RAG.

</details>


### [210] [Efficient Reasoning via Chain of Unconscious Thought](https://arxiv.org/abs/2505.19756)

*Ruihan Gong, Yue Liu, Wenjie Qu, Mingzhe Du, Yufei He, Yingwei Ma, Yulin Chen, Xiang Liu, Yi Wen, Xinfeng Li, Ruidong Wang, Xinzhong Zhu, Bryan Hooi, Jiaheng Zhang*

**Main category:** cs.CL

**Keywords:** Reasoning Models, Token Efficiency, Unconscious Thought Theory, Machine Learning, Natural Language Processing

**Relevance Score:** 7

**TL;DR:** This paper introduces the Chain of Unconscious Thought (CoUT) paradigm to improve token efficiency in Large Reasoning Models (LRMs) by mimicking human unconscious thought processes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the token efficiency of LRMs while maintaining performance by leveraging concepts from Unconscious Thought Theory.

**Method:** The authors propose CoUT, which involves prompting models to internalize reasoning processes in hidden layers and using token-efficient strategies to reduce unnecessary tokens.

**Key Contributions:**

	1. Introduction of the Chain of Unconscious Thought (CoUT) framework.
	2. Demonstrated significant token efficiency improvements over previous methods.
	3. Provided code for the CoUT implementation to facilitate further research.

**Result:** CoUT reduces token usage by 47.62% compared to the Chain of Thought (CoT) framework while maintaining similar accuracy levels.

**Limitations:** 

**Conclusion:** The findings suggest that models can effectively incorporate strategies that resemble unconscious thought to boost efficiency without compromising performance.

**Abstract:** Large Reasoning Models (LRMs) achieve promising performance but compromise token efficiency due to verbose reasoning processes. Unconscious Thought Theory (UTT) posits that complex problems can be solved more efficiently through internalized cognitive processes. Inspired by UTT, we propose a new reasoning paradigm, termed Chain of Unconscious Thought (CoUT), to improve the token efficiency of LRMs by guiding them to mimic human unconscious thought and internalize reasoning processes. Concretely, we first prompt the model to internalize the reasoning by thinking in the hidden layer. Then, we design a bag of token-efficient strategies to further help models reduce unnecessary tokens yet preserve the performance. Our work reveals that models may possess beneficial unconscious thought, enabling improved efficiency without sacrificing performance. Extensive experiments demonstrate the effectiveness of CoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while maintaining comparable accuracy, as shown in Figure 1. The code of CoUT is available at this link: https://github.com/Rohan-GRH/CoUT

</details>


### [211] [SGM: A Framework for Building Specification-Guided Moderation Filters](https://arxiv.org/abs/2505.19766)

*Masoomali Fatehkia, Enes Altinisik, Husrev Taha Sencar*

**Main category:** cs.CL

**Keywords:** large language models, content moderation, adversarial inputs, user-defined specifications, alignment control

**Relevance Score:** 8

**TL;DR:** SGM is a framework for training moderation filters for LLMs that align with user-specific requirements and goes beyond standard safety concerns, enabling automated, scalable support for various applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models often face issues with misalignment and adversarial inputs, making the development of robust moderation frameworks essential for effective deployment.

**Method:** SGM introduces a flexible framework for training moderation filters based on user-defined specifications, automating the generation of training data without needing human-written examples.

**Key Contributions:**

	1. Development of SGM framework for training moderation filters
	2. Automation of training data generation
	3. Enhanced user-defined alignment controls in moderation filters

**Result:** SGM-trained filters achieve performance comparable to state-of-the-art safety filters while allowing for more nuanced and application-specific alignment controls.

**Limitations:** 

**Conclusion:** The SGM framework demonstrates the ability to effectively address LLM alignment issues through automated moderation training, supporting diverse alignment goals.

**Abstract:** Aligning large language models (LLMs) with deployment-specific requirements is critical but inherently imperfect. Despite extensive training, models remain susceptible to misalignment and adversarial inputs such as jailbreaks. Content moderation filters are commonly used as external safeguards, though they typically focus narrowly on safety. We introduce SGM (Specification-Guided Moderation), a flexible framework for training moderation filters grounded in user-defined specifications that go beyond standard safety concerns. SGM automates training data generation without relying on human-written examples, enabling scalable support for diverse, application-specific alignment goals. SGM-trained filters perform on par with state-of-the-art safety filters built on curated datasets, while supporting fine-grained and user-defined alignment control.

</details>


### [212] [T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search](https://arxiv.org/abs/2505.19768)

*Xing Cui, Yueying Zou, Zekun Li, Peipei Li, Xinyuan Xu, Xuannan Liu, Huaibo Huang, Ran He*

**Main category:** cs.CL

**Keywords:** multimodal misinformation, dynamic verification, Monte Carlo Tree Search

**Relevance Score:** 6

**TL;DR:** T2Agent is a novel misinformation detection agent that combines a modular toolkit with Monte Carlo Tree Search (MCTS) for dynamic verification of mixed-source forgery.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing misinformation detection methods are limited by static pipelines and cannot effectively handle the complexity and diversity of real-world multimodal misinformation.

**Method:** T2Agent integrates an extensible toolkit that includes tools for web search, forgery detection, and consistency analysis, using a Bayesian optimization-based selector to identify a task-relevant subset of tools, which informs a modified MCTS for evidence collection and verification.

**Key Contributions:**

	1. Introduction of T2Agent for dynamic misinformation detection
	2. Integration of modular tools and Bayesian optimization with MCTS
	3. Demonstrated performance improvements over existing methods in real-world benchmarks

**Result:** T2Agent outperforms existing baselines on challenging multimodal misinformation benchmarks, confirming the effectiveness of its tree search mechanism and tool integration.

**Limitations:** 

**Conclusion:** The findings indicate that T2Agent provides a strong training-free approach to enhancing detection accuracy for mixed-source misinformation.

**Abstract:** Real-world multimodal misinformation often arises from mixed forgery sources, requiring dynamic reasoning and adaptive verification. However, existing methods mainly rely on static pipelines and limited tool usage, limiting their ability to handle such complexity and diversity. To address this challenge, we propose T2Agent, a novel misinformation detection agent that incorporates an extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of modular tools such as web search, forgery detection, and consistency analysis. Each tool is described using standardized templates, enabling seamless integration and future expansion. To avoid inefficiency from using all tools simultaneously, a Bayesian optimization-based selector is proposed to identify a task-relevant subset. This subset then serves as the action space for MCTS to dynamically collect evidence and perform multi-source verification. To better align MCTS with the multi-source nature of misinformation detection, T2Agent extends traditional MCTS with multi-source verification, which decomposes the task into coordinated subtasks targeting different forgery sources. A dual reward mechanism containing a reasoning trajectory score and a confidence score is further proposed to encourage a balance between exploration across mixed forgery sources and exploitation for more reliable evidence. We conduct ablation studies to confirm the effectiveness of the tree search mechanism and tool usage. Extensive experiments further show that T2Agent consistently outperforms existing baselines on challenging mixed-source multimodal misinformation benchmarks, demonstrating its strong potential as a training-free approach for enhancing detection accuracy. The code will be released.

</details>


### [213] [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)

*Sangyeop Kim, Yohan Lee, Yongwoo Song, Kimin Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Many-Shot Jailbreaking, long-context vulnerabilities, safety mechanisms, context length

**Relevance Score:** 9

**TL;DR:** This paper discusses vulnerabilities in Large Language Models (LLMs) regarding long-context processing, revealing safety gaps and the ineffectiveness of existing safety mechanisms across various contexts.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore vulnerabilities in LLMs related to long-context processing, specifically to understand how these weaknesses can be exploited and to highlight the ineffectiveness of current safety measures.

**Method:** The authors conduct experiments using Many-Shot Jailbreaking (MSJ) techniques, analyzing various attack settings that vary in instruction styles, shot density, topics, and formats with a context length of up to 128K tokens.

**Key Contributions:**

	1. Investigation of long-context vulnerabilities in LLMs through Many-Shot Jailbreaking experiments.
	2. Demonstration that context length is the key factor for attack effectiveness.
	3. Identification of safety gaps in the long-context processing capacity of LLMs.

**Result:** The analysis shows that context length significantly affects the effectiveness of many-shot attacks, and that successful attacks can be achieved even with non-specific or random content, indicating serious limitations in LLM safety when processing long contexts.

**Limitations:** The study may not address the full spectrum of vulnerabilities or scenarios involved in LLM long-context usage.

**Conclusion:** The findings demonstrate inconsistent safety behaviors in well-aligned models with longer contexts and underline the urgent need for developing new safety mechanisms to address these vulnerabilities.

**Abstract:** We investigate long-context vulnerabilities in Large Language Models (LLMs) through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of up to 128K tokens. Through comprehensive analysis with various many-shot attack settings with different instruction styles, shot density, topic, and format, we reveal that context length is the primary factor determining attack effectiveness. Critically, we find that successful attacks do not require carefully crafted harmful content. Even repetitive shots or random dummy text can circumvent model safety measures, suggesting fundamental limitations in long-context processing capabilities of LLMs. The safety behavior of well-aligned models becomes increasingly inconsistent with longer contexts. These findings highlight significant safety gaps in context expansion capabilities of LLMs, emphasizing the need for new safety mechanisms.

</details>


### [214] [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)

*Akram Elbouanani, Evan Dufraisse, Adrian Popescu*

**Main category:** cs.CL

**Keywords:** Political Bias, Large Language Models, Sentiment Analysis, Natural Language Processing, Bias Mitigation

**Relevance Score:** 8

**TL;DR:** This paper presents a new method for analyzing political biases in LLMs by measuring sentiment prediction variability across different target entities in sentences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the impact of political biases in LLMs on downstream applications, we seek a bias analysis method that doesn't propagate existing biases.

**Method:** We insert demographically and politically diverse names into political sentences and measure sentiment predictions across models and languages using an entropy-based inconsistency metric.

**Key Contributions:**

	1. Introduction of an entropy-based inconsistency metric for bias analysis
	2. Comprehensive analysis across 450 sentences using 1319 politicians
	3. Findings show language and model size impact on bias intensity

**Result:** The study finds significant inconsistencies in sentiment predictions related to political entities, with biases observable across multiple languages and correlations between politically aligned figures.

**Limitations:** The approach may not account for all aspects of bias and requires further validation across different contexts.

**Conclusion:** Replacing real politician names with fictional equivalents can mitigate some reliability issues in LLM sentiment analysis while revealing underlying biases.

**Abstract:** Political biases encoded by LLMs might have detrimental effects on downstream applications. Existing bias analysis methods rely on small-size intermediate tasks (questionnaire answering or political content generation) and rely on the LLMs themselves for analysis, thus propagating bias. We propose a new approach leveraging the observation that LLM sentiment predictions vary with the target entity in the same sentence. We define an entropy-based inconsistency metric to encode this prediction variability. We insert 1319 demographically and politically diverse politician names in 450 political sentences and predict target-oriented sentiment using seven models in six widely spoken languages. We observe inconsistencies in all tested combinations and aggregate them in a statistically robust analysis at different granularity levels. We observe positive and negative bias toward left and far-right politicians and positive correlations between politicians with similar alignment. Bias intensity is higher for Western languages than for others. Larger models exhibit stronger and more consistent biases and reduce discrepancies between similar languages. We partially mitigate LLM unreliability in target-oriented sentiment classification (TSC) by replacing politician names with fictional but plausible counterparts.

</details>


### [215] [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)

*Yiqun Zhang, Hao Li, Chenxu Wang, Linyao Chen, Qiaosheng Zhang, Peng Ye, Shi Feng, Daling Wang, Zhen Wang, Xinrun Wang, Jia Xu, Lei Bai, Wanli Ouyang, Shuyue Hu*

**Main category:** cs.CL

**Keywords:** language models, ensemble methods, open-source, natural language processing, machine learning

**Relevance Score:** 9

**TL;DR:** The Avengers framework leverages multiple smaller language models to outperform GPT-4.1 on various tasks, utilizing embedding, clustering, scoring, and voting techniques.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Explores the potential of smaller, open-source language models to compete with larger proprietary models in natural language processing tasks.

**Method:** The framework employs four operations: embedding queries, clustering them by semantic similarity, scoring model performance within clusters, and voting for improved output generation.

**Key Contributions:**

	1. Introduced a novel ensemble method for smaller language models.
	2. Achieved performance surpassing GPT-4.1 in diverse tasks.
	3. Open-sourced implementation available for further research.

**Result:** The Avengers framework improved performance on 10 out of 15 benchmark datasets compared to GPT-4.1, particularly excelling in mathematics and coding tasks with notable percentage increases.

**Limitations:** The framework's performance may vary depending on the selection of embedding models and clustering strategies.

**Conclusion:** Smaller language models can collectively outperform larger models by utilizing a novel ensemble method, demonstrating robustness and generalization capabilities.

**Abstract:** As proprietary giants increasingly dominate the race for ever-larger language models, a pressing question arises for the open-source community: can smaller models remain competitive across a broad range of tasks? In this paper, we present the Avengers--a simple recipe that effectively leverages the collective intelligence of open-source, smaller language models. Our framework is built upon four lightweight operations: (i) embedding: encode queries using a text embedding model; (ii) clustering: group queries based on their semantic similarity; (iii) scoring: scores each model's performance within each cluster; and (iv) voting: improve outputs via repeated sampling and voting. At inference time, each query is embedded and assigned to its nearest cluster. The top-performing model(s) within that cluster are selected to generate the response using the Self-Consistency or its multi-model variant. Remarkably, with 10 open-source models (~7B parameters each), the Avengers collectively outperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code, logic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the Avengers delivers superior out-of-distribution generalization, and remains robust across various embedding models, clustering algorithms, ensemble strategies, and values of its sole parameter--the number of clusters. We have open-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers

</details>


### [216] [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800)

*Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem*

**Main category:** cs.CL

**Keywords:** metadata extraction, Large Language Models, scientific papers, automation, benchmark

**Relevance Score:** 8

**TL;DR:** This paper presents MOLE, a framework using Large Language Models to automatically extract metadata attributes from scientific papers, enhancing research discovery and reproducibility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and effectiveness of metadata extraction in the context of the rapid expansion of scientific research, moving beyond the reliance on manual annotation.

**Method:** The framework uses a schema-driven methodology to process documents in multiple formats with validation mechanisms to ensure consistent output.

**Key Contributions:**

	1. Introduction of the MOLE framework for automatic metadata extraction using LLMs
	2. Establishment of a new benchmark for evaluating this task
	3. Demonstration of the potential of modern LLMs in automating data extraction for diverse languages.

**Result:** MOLE shows promising results in automating metadata extraction through analysis of context length, few-shot learning, and web browsing integration, along with the introduction of a new benchmark for evaluating research progress in this area.

**Limitations:** The framework's performance depends on further improvements for achieving consistent and reliable results across various datasets.

**Conclusion:** The study indicates the need for further improvements in the automation of metadata extraction using LLMs to guarantee consistent and reliable performance.

**Abstract:** Metadata extraction is essential for cataloging and preserving datasets, enabling effective research discovery and reproducibility, especially given the current exponential growth in scientific research. While Masader (Alyafeai et al.,2021) laid the groundwork for extracting a wide range of metadata attributes from Arabic NLP datasets' scholarly articles, it relies heavily on manual annotation. In this paper, we present MOLE, a framework that leverages Large Language Models (LLMs) to automatically extract metadata attributes from scientific papers covering datasets of languages other than Arabic. Our schema-driven methodology processes entire documents across multiple input formats and incorporates robust validation mechanisms for consistent output. Additionally, we introduce a new benchmark to evaluate the research progress on this task. Through systematic analysis of context length, few-shot learning, and web browsing integration, we demonstrate that modern LLMs show promising results in automating this task, highlighting the need for further future work improvements to ensure consistent and reliable performance. We release the code: https://github.com/IVUL-KAUST/MOLE and dataset: https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.

</details>


### [217] [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)

*Siyuan Li, Jian Chen, Rui Yao, Xuming Hu, Peilin Zhou, Weihua Qiu, Simin Zhang, Chucheng Dong, Zhiyao Li, Qipeng Xie, Zixuan Yuan*

**Main category:** cs.CL

**Keywords:** Regulatory Technology, Large Language Models, Financial Regulations

**Relevance Score:** 6

**TL;DR:** This paper introduces Compliance-to-Code, a large-scale Chinese dataset aimed at improving financial regulatory compliance through automated code generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced in automating compliance logic from complex Chinese financial regulations, particularly the suboptimal performance of existing LLMs.

**Method:** Development of a domain-specific, code-oriented dataset, including 1,159 annotated clauses from 361 financial regulations, structured to aid model training.

**Key Contributions:**

	1. Introduction of Compliance-to-Code, the first large-scale dataset for Chinese regulatory compliance.
	2. Detailed annotations with logical elements for better model training.
	3. Presentation of FinCheck, a practical pipeline for automating compliance tasks.

**Result:** The dataset includes modularly structured clauses and deterministic Python code mappings, aiming to facilitate automated compliance auditing and code generation.

**Limitations:** 

**Conclusion:** This new dataset and the accompanying FinCheck pipeline could significantly enhance the capabilities of regulatory technology in processing Chinese financial regulations.

**Abstract:** Nowadays, regulatory compliance has become a cornerstone of corporate governance, ensuring adherence to systematic legal frameworks. At its core, financial regulations often comprise highly intricate provisions, layered logical structures, and numerous exceptions, which inevitably result in labor-intensive or comprehension challenges. To mitigate this, recent Regulatory Technology (RegTech) and Large Language Models (LLMs) have gained significant attention in automating the conversion of regulatory text into executable compliance logic. However, their performance remains suboptimal particularly when applied to Chinese-language financial regulations, due to three key limitations: (1) incomplete domain-specific knowledge representation, (2) insufficient hierarchical reasoning capabilities, and (3) failure to maintain temporal and logical coherence. One promising solution is to develop a domain specific and code-oriented datasets for model training. Existing datasets such as LexGLUE, LegalBench, and CODE-ACCORD are often English-focused, domain-mismatched, or lack fine-grained granularity for compliance code generation. To fill these gaps, we present Compliance-to-Code, the first large-scale Chinese dataset dedicated to financial regulatory compliance. Covering 1,159 annotated clauses from 361 regulations across ten categories, each clause is modularly structured with four logical elements-subject, condition, constraint, and contextual information-along with regulation relations. We provide deterministic Python code mappings, detailed code reasoning, and code explanations to facilitate automated auditing. To demonstrate utility, we present FinCheck: a pipeline for regulation structuring, code generation, and report generation.

</details>


### [218] [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)

*Sirui Chen, Shuqin Ma, Shu Yu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu*

**Main category:** cs.CL

**Keywords:** LLM Consciousness, Awareness, Artificial Intelligence, Theoretical Perspectives, Empirical Research

**Relevance Score:** 6

**TL;DR:** This paper explores the controversial topic of consciousness in large language models (LLMs), clarifying terms, organizing existing research, discussing potential risks, and outlining future directions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the burgeoning discourse and confusion surrounding the consciousness of large language models (LLMs) as they evolve.

**Method:** The authors synthesize existing theoretical and empirical research on LLM consciousness and categorize relevant terminologies.

**Key Contributions:**

	1. Clarification of key terminologies regarding LLM consciousness and awareness.
	2. Synthesis of theoretical and empirical perspectives on LLM consciousness.
	3. Identification of potential risks and challenges in developing conscious LLMs.

**Result:** The paper identifies various risks associated with conscious LLMs and highlights key challenges in the field, while organizing references for further exploration.

**Limitations:** The exploration of LLM consciousness is still in its nascent stages, and deeper empirical investigations are needed.

**Conclusion:** The study sets the stage for future research by clarifying concepts and outlining both risks and avenues for investigation regarding LLM consciousness.

**Abstract:** Consciousness stands as one of the most profound and distinguishing features of the human mind, fundamentally shaping our understanding of existence and agency. As large language models (LLMs) develop at an unprecedented pace, questions concerning intelligence and consciousness have become increasingly significant. However, discourse on LLM consciousness remains largely unexplored territory. In this paper, we first clarify frequently conflated terminologies (e.g., LLM consciousness and LLM awareness). Then, we systematically organize and synthesize existing research on LLM consciousness from both theoretical and empirical perspectives. Furthermore, we highlight potential frontier risks that conscious LLMs might introduce. Finally, we discuss current challenges and outline future directions in this emerging field. The references discussed in this paper are organized at https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.

</details>


### [219] [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)

*Junnan Liu, Hongwei Liu, Linchen Xiao, Shudong Liu, Taolin Zhang, Zihan Ma, Songyang Zhang, Kai Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Meta-Learning, Reasoning, Machine Learning, Artificial Intelligence

**Relevance Score:** 9

**TL;DR:** A framework is proposed to enhance understanding of reasoning in large language models (LLMs) via meta-learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the reasoning capabilities of LLMs and improve them through established meta-learning techniques.

**Method:** The training process for reasoning tasks is formalized as a meta-learning setup, conceptualizing reasoning trajectories as pseudo-gradient descent updates to model parameters.

**Key Contributions:**

	1. Introduction of a meta-learning perspective to LLM reasoning
	2. Formalization of the reasoning process as meta-learning tasks
	3. Empirical evaluations demonstrating generalization of reasoning capabilities

**Result:** The study establishes a strong connection between LLM reasoning and meta-learning, demonstrating that LLMs can generalize reasoning capabilities to unseen questions after training on diverse tasks.

**Limitations:** 

**Conclusion:** The findings provide practical insights for enhancing LLMs using meta-learning strategies, improving their reasoning abilities.

**Abstract:** We propose a novel framework for comprehending the reasoning capabilities of large language models (LLMs) through the perspective of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLM's parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning, exploring several issues of significant interest from a meta-learning standpoint. Our work not only enhances the understanding of LLM reasoning but also provides practical insights for improving these models through established meta-learning techniques.

</details>


### [220] [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)

*Pascal Wullschleger, Majid Zarharan, Donnacha Daly, Marc Pouly, Jennifer Foster*

**Main category:** cs.CL

**Keywords:** Large Language Models, taxonomy generation, food technology, Llama-3, iterative prompting

**Relevance Score:** 8

**TL;DR:** This paper explores the use of Large Language Models for generating and completing taxonomies in the food technology sector.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enhance taxonomy generation and completion processes in the food technology industry using LLMs.

**Method:** Using an open-source LLM (Llama-3), the paper tests two approaches: completing taxonomies from a seed taxonomy and generating them from known concepts through iterative prompting techniques.

**Key Contributions:**

	1. Novel application of LLMs to taxonomy generation in food technology
	2. Iterative prompting techniques for taxonomy completion
	3. Insights into the limitations of current LLM approaches for inner node placement

**Result:** Experiments on five taxonomies show promise, though challenges in accurately placing inner nodes were identified.

**Limitations:** Difficulty in correctly placing inner nodes in taxonomies generated by LLMs.

**Conclusion:** While the initial results are encouraging, further refinement is needed to improve the accuracy of inner node placements in generated taxonomies.

**Abstract:** We investigate the utility of Large Language Models for automated taxonomy generation and completion specifically applied to taxonomies from the food technology industry. We explore the extent to which taxonomies can be completed from a seed taxonomy or generated without a seed from a set of known concepts, in an iterative fashion using recent prompting techniques. Experiments on five taxonomies using an open-source LLM (Llama-3), while promising, point to the difficulty of correctly placing inner nodes.

</details>


### [221] [Improving Multilingual Math Reasoning for African Languages](https://arxiv.org/abs/2505.19848)

*Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji, Esther Adenuga, David Ifeoluwa Adelani, Jimmy Lin*

**Main category:** cs.CL

**Keywords:** low-resource languages, large language models, African languages, adaptation strategies, mathematical reasoning

**Relevance Score:** 7

**TL;DR:** This paper investigates adaptation strategies for large language models in low-resource African languages, focusing on performance evaluation of pre-training and post-training techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenges faced by researchers working on low-resource languages, specifically African languages, due to limited data and computational resources.

**Method:** The authors conduct extensive experiments and ablation studies to compare various adaptation strategies, including different data types and training stages using the Llama 3.1 model family.

**Key Contributions:**

	1. Systematic investigation of adaptation strategies for low-resource African languages
	2. Comprehensive evaluation of pre-training and post-training paradigms
	3. Insights into the use of translated versus synthetically generated data for model training

**Result:** The paper identifies which adaptation strategies yield the best performance for extending LLMs to African languages, particularly in the context of mathematical reasoning tasks.

**Limitations:** The effectiveness of the strategies may vary across different languages and tasks beyond those evaluated in this study.

**Conclusion:** The findings inform effective model adaptation configurations that can improve performance in low-resource language contexts.

**Abstract:** Researchers working on low-resource languages face persistent challenges due to limited data availability and restricted access to computational resources. Although most large language models (LLMs) are predominantly trained in high-resource languages, adapting them to low-resource contexts, particularly African languages, requires specialized techniques. Several strategies have emerged for adapting models to low-resource languages in todays LLM landscape, defined by multi-stage pre-training and post-training paradigms. However, the most effective approaches remain uncertain. This work systematically investigates which adaptation strategies yield the best performance when extending existing LLMs to African languages. We conduct extensive experiments and ablation studies to evaluate different combinations of data types (translated versus synthetically generated), training stages (pre-training versus post-training), and other model adaptation configurations. Our experiments focuses on mathematical reasoning tasks, using the Llama 3.1 model family as our base model.

</details>


### [222] [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)

*Gulfarogh Azam, Mohd Sadique, Saif Ali, Mohammad Nadeem, Erik Cambria, Shahab Saquib Sohail, Mohammad Sultan Alam*

**Main category:** cs.CL

**Keywords:** transliteration, large language models, multilingual processing, natural language processing, performance evaluation

**Relevance Score:** 8

**TL;DR:** This paper evaluates the performance of large language models (LLMs) in the task of transliteration across various Indian languages and compares them to a specialized model, IndicXlit.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to analyze how general-purpose LLMs can perform transliteration tasks, which are increasingly relevant in linguistically diverse contexts such as India, where multilingual processing is critical.

**Method:** The authors conducted experiments utilizing prominent LLMs (GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, Mistral-Large) and compared their performances against IndicXlit using datasets like Dakshina and Aksharantar. Performance metrics included Top-1 Accuracy and Character Error Rate, with extensive error analysis and robustness testing.

**Key Contributions:**

	1. Systematic evaluation of LLMs for transliteration tasks in a multilingual context.
	2. Comparison of LLMs against a state-of-the-art specialized model, IndicXlit.
	3. Demonstration of the potential of general-purpose models in specialized applications without explicit task-specific training.

**Result:** Findings indicate that GPT family models generally outperform both other LLMs and the IndicXlit model across most instances, with fine-tuning of GPT-4o showing notable improvements in performance for specific languages.

**Limitations:** The study may be limited by the choice of languages and datasets used for evaluation, as well as potential biases in LLM performance across different contexts.

**Conclusion:** The research concludes that foundational models like GPT have significant potential for specialized applications, such as transliteration, demonstrating robustness even under noisy conditions with minimal overhead.

**Abstract:** Transliteration, the process of mapping text from one script to another, plays a crucial role in multilingual natural language processing, especially within linguistically diverse contexts such as India. Despite significant advancements through specialized models like IndicXlit, recent developments in large language models suggest a potential for general-purpose models to excel at this task without explicit task-specific training. The current work systematically evaluates the performance of prominent LLMs, including GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a state-of-the-art transliteration model, across ten major Indian languages. Experiments utilized standard benchmarks, including Dakshina and Aksharantar datasets, with performance assessed via Top-1 Accuracy and Character Error Rate. Our findings reveal that while GPT family models generally outperform other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o improves performance on specific languages notably. An extensive error analysis and robustness testing under noisy conditions further elucidate strengths of LLMs compared to specialized models, highlighting the efficacy of foundational models for a wide spectrum of specialized applications with minimal overhead.

</details>


### [223] [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)

*Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jun Rao, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Reinforcement Learning, Inference Efficiency, Reflection Model

**Relevance Score:** 8

**TL;DR:** Introducing REA-RL for more efficient online training of Large Reasoning Models, balancing performance and inference costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address high inference costs and inefficient training in Large Reasoning Models due to overthinking and the inadequacies of online reinforcement learning methods.

**Method:** The proposed method, REA-RL, employs a small reflection model to enhance online training efficiency, integrating parallel sampling with sequential revision and introducing a reflection reward to maintain response quality.

**Key Contributions:**

	1. Introduction of REA-RL for online training
	2. Implementation of a reflection reward to optimize response quality
	3. Demonstrated significant reduction in inference costs

**Result:** The combination of methods achieved a 35% reduction in inference costs while maintaining or enhancing performance, effectively managing reflection frequency based on problem complexity.

**Limitations:** 

**Conclusion:** REA-RL demonstrates that online training for LRMs can be made more efficient without sacrificing performance, optimizing the balance between response quality and inference speed.

**Abstract:** Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks but often face the challenge of overthinking, leading to substantially high inference costs. Existing approaches synthesize shorter reasoning responses for LRMs to learn, but are inefficient for online usage due to the time-consuming data generation and filtering processes. Meanwhile, online reinforcement learning mainly adopts a length reward to encourage short reasoning responses, but tends to lose the reflection ability and harm the performance. To address these issues, we propose REA-RL, which introduces a small reflection model for efficient scaling in online training, offering both parallel sampling and sequential revision. Besides, a reflection reward is designed to further prevent LRMs from favoring short yet non-reflective responses. Experiments show that both methods maintain or enhance performance while significantly improving inference efficiency. Their combination achieves a good balance between performance and efficiency, reducing inference costs by 35% without compromising performance. Further analysis demonstrates that our methods are effective by maintaining reflection frequency for hard problems while appropriately reducing it for simpler ones without losing reflection ability. Codes are available at https://github.com/hexuandeng/REA-RL.

</details>


### [224] [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)

*Javier Marín*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-tuning, Machine Learning, Natural Language Processing, Cost Efficiency

**Relevance Score:** 9

**TL;DR:** Adjacent Possible Exploration (APE) offers a resource-efficient method for adapting large language models to specific tasks, achieving significant performance improvements with minimal computational overhead.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for an efficient fine-tuning method for large language models that requires less computational power while still achieving high performance.

**Method:** APE employs an iterative approach to fine-tune models on small, selected data batches of 200 examples, retaining only those improvements that enhance model performance.

**Key Contributions:**

	1. Introduces the Adjacent Possible Exploration method for efficient LLM adaptation.
	2. Achieves significant performance improvements with minimal computational resources.
	3. Provides open-source code and validation through metrics and human evaluations.

**Result:** On news summarization, APE achieves a 40% BLEU score improvement with minimal resources, matching or exceeding more complex methods like LoRA while executing in only 60 minutes on a T4 GPU.

**Limitations:** 

**Conclusion:** APE demonstrates that small, iterative data perturbations can effectively enhance task-specific performance in LLMs without the need for extensive retraining.

**Abstract:** We present Adjacent Possible Exploration (APE), a simple yet effective method for adapting large language models to specific tasks using minimal computational resources. Unlike traditional fine-tuning that requires extensive compute, APE iteratively fine-tunes models on small, carefully selected data batches (200 examples), retaining only improvements. On news summarization, APE achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes, matching or exceeding more complex methods like LoRA while remaining conceptually simple. Our approach is particularly valuable for researchers and practitioners with limited computational resources. We provide open-source code and demonstrate APE's effectiveness through both automatic metrics and human evaluation. While inspired by evolutionary theory's "adjacent possible", APE's core insight has a very practical application: small, iterative data perturbations can efficiently guide LLMs toward task-specific performance without expensive retraining.

</details>


### [225] [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)

*Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, Mingxuan Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, puzzle reasoning, Reinforcement Learning, benchmark, AI

**Relevance Score:** 8

**TL;DR:** Enigmata is a comprehensive suite designed to enhance Large Language Models (LLMs) in puzzle reasoning, offering a generator-verifier framework for task training and evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLMs' ability to solve puzzles that do not require domain knowledge, which are typically easy for humans but challenging for LLMs.

**Method:** The paper introduces a suite called Enigmata comprising 36 tasks across seven categories, featuring a generator for creating examples and a rule-based verifier for automatic evaluation, facilitating scalable and detailed RL training.

**Key Contributions:**

	1. Introduction of Enigmata, a suite for puzzle reasoning in LLMs.
	2. Development of a generator-verifier system for scalable multi-task RL training.
	3. Creation of Enigmata-Eval, a benchmark to evaluate LLM puzzle reasoning abilities.

**Result:** The trained model Qwen2.5-32B-Enigmata outperforms previous models on puzzle reasoning and generalizes well to mathematical reasoning tasks, showing significant performance improvements with the use of Enigmata data.

**Limitations:** 

**Conclusion:** The Enigmata framework provides a structured approach to enhancing logical reasoning in LLMs, with promising transferability to various reasoning tasks in AI.

**Abstract:** Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across seven categories, each with 1) a generator that produces unlimited examples with controllable difficulty and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from Enigmata further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of Enigmata. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Resources of this work can be found at https://seed-enigmata.github.io.

</details>


### [226] [ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs](https://arxiv.org/abs/2505.19937)

*Pooneh Mousavi, Yingzhi Wang, Mirco Ravanelli, Cem Subakan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Spoken Language Understanding, Cross-modal alignment, Automatic Latent Alignment Score, Multimodal learning

**Relevance Score:** 9

**TL;DR:** Proposes a new metric for evaluating cross-modal alignment in LLMs used for Spoken Language Understanding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a standard metric for evaluating the alignment quality between audio and text representations in LLMs for spoken language understanding tasks.

**Method:** Introduced ALAS (Automatic Latent Alignment Score) to measure correlation between audio and text representations across transformer layers.

**Key Contributions:**

	1. Introduction of the ALAS metric for alignment evaluation
	2. Analysis of audio-text representation correlation across transformer layers
	3. Application of the metric in two SLU tasks: Spoken Question Answering and Emotion Recognition.

**Result:** The ALAS metric shows consistent behavior across different transformer layers and tasks, indicating effective cross-modal alignment evaluation.

**Limitations:** 

**Conclusion:** ALAS provides a valuable tool for assessing the integration of multimodal input in LLMs for spoken language understanding, aiding in model development.

**Abstract:** Large Language Models (LLMs) are widely used in Spoken Language Understanding (SLU). Recent SLU models process audio directly by adapting speech input into LLMs for better multimodal learning. A key consideration for these models is the cross-modal alignment between text and audio modalities, which is a telltale sign as to whether or not LLM is able to associate semantic meaning to audio segments. While various methods exist for fusing these modalities, there is no standard metric to evaluate alignment quality in LLMs. In this work, we propose a new metric, ALAS (Automatic Latent Alignment Score). Our study examines the correlation between audio and text representations across transformer layers, for two different tasks (Spoken Question Answering and Emotion Recognition). We showcase that our metric behaves as expected across different layers and different tasks.

</details>


### [227] [MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2505.19959)

*Zhongzhan Huang, Guoming Ling, Shanshan Zhong, Hefeng Wu, Liang Lin*

**Main category:** cs.CL

**Keywords:** Long Context Understanding, large language models, benchmarking, data compression, evaluation costs

**Relevance Score:** 8

**TL;DR:** This paper introduces MiniLongBench, a low-cost benchmark for assessing Long Context Understanding (LCU) in large language models, significantly reducing evaluation costs while maintaining high correlation with existing benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this research is to address the high evaluation costs associated with current Long Context Understanding benchmarks for large language models, which hinder effective assessment of LCU capabilities.

**Method:** The authors conducted extensive experimentation to identify redundancy in existing LCU benchmarks, leading to the development of MiniLongBench, which prunes the LongBench dataset to a more efficient set of 237 test samples across six major task categories and 21 distinct tasks.

**Key Contributions:**

	1. Introduction of MiniLongBench, a concise benchmark for LCU.
	2. Significant reduction in evaluation costs for long-text data.
	3. High correlation with existing benchmarks, indicating reliability.

**Result:** MiniLongBench achieves an average evaluation cost of only 4.5% of the original LongBench while maintaining a high average rank correlation coefficient of 0.97 with LongBench results, demonstrating its effectiveness for LCU assessment.

**Limitations:** 

**Conclusion:** MiniLongBench provides a substantial reduction in evaluation costs and offers a promising avenue for future research into Long Context Understanding capabilities of large language models.

**Abstract:** Long Context Understanding (LCU) is a critical area for exploration in current large language models (LLMs). However, due to the inherently lengthy nature of long-text data, existing LCU benchmarks for LLMs often result in prohibitively high evaluation costs, like testing time and inference expenses. Through extensive experimentation, we discover that existing LCU benchmarks exhibit significant redundancy, which means the inefficiency in evaluation. In this paper, we propose a concise data compression method tailored for long-text data with sparse information characteristics. By pruning the well-known LCU benchmark LongBench, we create MiniLongBench. This benchmark includes only 237 test samples across six major task categories and 21 distinct tasks. Through empirical analysis of over 60 LLMs, MiniLongBench achieves an average evaluation cost reduced to only 4.5% of the original while maintaining an average rank correlation coefficient of 0.97 with LongBench results. Therefore, our MiniLongBench, as a low-cost benchmark, holds great potential to substantially drive future research into the LCU capabilities of LLMs. See https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.

</details>


### [228] [CP-Router: An Uncertainty-Aware Router Between LLM and LRM](https://arxiv.org/abs/2505.19970)

*Jiayuan Su, Fulin Lin, Zhaopeng Feng, Han Zheng, Teng Wang, Zhenyu Xiao, Xinlong Zhao, Zuozhu Liu, Lu Cheng, Hongwei Wang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Large Language Models, Conformal Prediction, Multi-Choice Question Answering, Entropy-based Criterion

**Relevance Score:** 8

**TL;DR:** CP-Router is a model-agnostic routing framework that dynamically selects between LLMs and LRMs for efficient multi-choice question answering by utilizing uncertainty estimates for optimal model choice.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and accuracy degradation caused by Large Reasoning Models producing lengthy outputs for simple queries.

**Method:** The CP-Router utilizes a training-free, model-agnostic approach that makes routing decisions based on prediction uncertainty estimates obtained through Conformal Prediction, enhanced by a novel Full and Binary Entropy criterion for adaptive threshold selection.

**Key Contributions:**

	1. Introduction of CP-Router as a model-agnostic routing framework.
	2. Utilization of Conformal Prediction for uncertainty estimation.
	3. Development of Full and Binary Entropy criterion for adaptive threshold selection.

**Result:** Experiments show that CP-Router reduces token usage while maintaining or improving accuracy compared to using LRM alone across various MCQA benchmarks.

**Limitations:** 

**Conclusion:** CP-Router exhibits strong performance across diverse model pairings and open-ended QA, validating its generality and robustness.

**Abstract:** Recent advances in Large Reasoning Models (LRMs) have significantly improved long-chain reasoning capabilities over Large Language Models (LLMs). However, LRMs often produce unnecessarily lengthy outputs even for simple queries, leading to inefficiencies or even accuracy degradation compared to LLMs. To overcome this, we propose CP-Router, a training-free and model-agnostic routing framework that dynamically selects between an LLM and an LRM, demonstrated with multiple-choice question answering (MCQA) prompts. The routing decision is guided by the prediction uncertainty estimates derived via Conformal Prediction (CP), which provides rigorous coverage guarantees. To further refine the uncertainty differentiation across inputs, we introduce Full and Binary Entropy (FBE), a novel entropy-based criterion that adaptively selects the appropriate CP threshold. Experiments across diverse MCQA benchmarks, including mathematics, logical reasoning, and Chinese chemistry, demonstrate that CP-Router efficiently reduces token usage while maintaining or even improving accuracy compared to using LRM alone. We also extend CP-Router to diverse model pairings and open-ended QA, where it continues to demonstrate strong performance, validating its generality and robustness.

</details>


### [229] [Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language](https://arxiv.org/abs/2505.19971)

*Kilian Sennrich, Sina Ahmadi*

**Main category:** cs.CL

**Keywords:** knowledge graphs, natural language interface, SPARQL, lexicographic data, language models

**Relevance Score:** 7

**TL;DR:** This paper develops a natural language interface for querying lexicographic data in knowledge graphs, notably Wikidata, and evaluates the performance of several language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To simplify lexicographic data retrieval from knowledge graphs for non-expert users who struggle with SPARQL.

**Method:** A multidimensional taxonomy of Wikidata's lexicographic data ontology was created, and a dataset with over 1.2 million mappings from natural language utterances to SPARQL queries was developed, followed by experiments with various language models.

**Key Contributions:**

	1. Development of a multidimensional taxonomy for Wikidata's lexicographic data ontology.
	2. Creation of a large template-based dataset for natural language to SPARQL query mapping.
	3. Experimental evaluation of language models' performance in this domain.

**Result:** GPT-2, Phi-1.5, and GPT-3.5-Turbo were tested, revealing that while all effectively handle familiar patterns, only GPT-3.5-Turbo shows notable generalization, indicating the importance of model size and diverse pre-training.

**Limitations:** Challenges in achieving robust generalization and accommodating the full complexity of lexicographic knowledge representation remain.

**Conclusion:** Robust generalization and handling of diverse linguistic data remains challenging, pointing towards the need for scalable solutions in lexicographic knowledge representation.

**Abstract:** Knowledge graphs offer an excellent solution for representing the lexical-semantic structures of lexicographic data. However, working with the SPARQL query language represents a considerable hurdle for many non-expert users who could benefit from the advantages of this technology. This paper addresses the challenge of creating natural language interfaces for lexicographic data retrieval on knowledge graphs such as Wikidata. We develop a multidimensional taxonomy capturing the complexity of Wikidata's lexicographic data ontology module through four dimensions and create a template-based dataset with over 1.2 million mappings from natural language utterances to SPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and GPT-3.5-Turbo reveal significant differences in model capabilities. While all models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates meaningful generalization capabilities, suggesting that model size and diverse pre-training are crucial for adaptability in this domain. However, significant challenges remain in achieving robust generalization, handling diverse linguistic data, and developing scalable solutions that can accommodate the full complexity of lexicographic knowledge representation.

</details>


### [230] [DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset](https://arxiv.org/abs/2505.19978)

*Alkis Koudounas, Moreno La Quatra, Elena Baralis*

**Main category:** cs.CL

**Keywords:** Conversational AI, Multimodal Dataset, Multi-turn Dialogues, Emotional Progression, Language Models

**Relevance Score:** 9

**TL;DR:** DeepDialogue is a large-scale multimodal dataset for multi-turn dialogues, addressing limitations in emotional range and domain diversity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve coherence and emotional engagement in multi-turn dialogues in conversational AI, as current datasets lack depth and variety.

**Method:** The dataset consists of 40,150 high-quality multi-turn dialogues across 41 domains and 20 distinct emotions, created by pairing 9 language models to generate initial conversations, which were then evaluated for quality.

**Key Contributions:**

	1. Introduction of the DeepDialogue dataset for multi-turn dialogues.
	2. Inclusion of a speech component with emotion-consistent voices for dialogues.
	3. Insights on the importance of model size and domain in maintaining coherence.

**Result:** The dataset provides insights into dialogue coherence, revealing limitations of smaller models and the benefits of concrete domains and cross-model interactions.

**Limitations:** 

**Conclusion:** DeepDialogue introduces an essential resource for advancing human-like conversational systems and offers the first large-scale open-source multimodal dialogue dataset preserving emotional context.

**Abstract:** Recent advances in conversational AI have demonstrated impressive capabilities in single-turn responses, yet multi-turn dialogues remain challenging for even the most sophisticated language models. Current dialogue datasets are limited in their emotional range, domain diversity, turn depth, and are predominantly text-only, hindering progress in developing more human-like conversational systems across modalities. To address these limitations, we present DeepDialogue, a large-scale multimodal dataset containing 40,150 high-quality multi-turn dialogues spanning 41 domains and incorporating 20 distinct emotions with coherent emotional progressions. Our approach pairs 9 different language models (4B-72B parameters) to generate 65,600 initial conversations, which we then evaluate through a combination of human annotation and LLM-based quality filtering. The resulting dataset reveals fundamental insights: smaller models fail to maintain coherence beyond 6 dialogue turns; concrete domains (e.g., "cars," "travel") yield more meaningful conversations than abstract ones (e.g., "philosophy"); and cross-model interactions produce more coherent dialogues than same-model conversations. A key contribution of DeepDialogue is its speech component, where we synthesize emotion-consistent voices for all 40,150 dialogues, creating the first large-scale open-source multimodal dialogue dataset that faithfully preserves emotional context across multi-turn conversations.

</details>


### [231] [How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation](https://arxiv.org/abs/2505.19987)

*Yongshi Ye, Biao Fu, Chongxuan Huang, Yidong Chen, Xiaodong Shi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Large Reasoning Models, Machine Translation, Domain-Sensitive Translation, Evaluation Metrics

**Relevance Score:** 8

**TL;DR:** This paper evaluates the effectiveness of Large Reasoning Models (LRMs) compared to traditional Large Language Models (LLMs) in domain-sensitive machine translation tasks, finding that LRMs outperform LLMs particularly in complex scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether structured reasoning via Large Reasoning Models can improve translation quality in complex, domain-sensitive tasks.

**Method:** The study compares LRMs with traditional LLMs in 15 domains and 4 translation directions using automatic metrics and an enhanced MQM-based evaluation hierarchy.

**Key Contributions:**

	1. Demonstrated superiority of LRMs over LLMs in complex translation scenarios
	2. Provided a comprehensive evaluation across multiple domains
	3. Introduced domain-adaptive prompting strategies for improved performance

**Result:** LRMs consistently outperform traditional LLMs in semantically complex domains, particularly for long-text and high-difficulty translations.

**Limitations:** 

**Conclusion:** The research suggests that structured reasoning can significantly enhance translation quality in domain-sensitive contexts and identifies domain-adaptive prompting as a beneficial strategy.

**Abstract:** Large language models (LLMs) have demonstrated strong performance in general-purpose machine translation, but their effectiveness in complex, domain-sensitive translation tasks remains underexplored. Recent advancements in Large Reasoning Models (LRMs), raise the question of whether structured reasoning can enhance translation quality across diverse domains. In this work, we compare the performance of LRMs with traditional LLMs across 15 representative domains and four translation directions. Our evaluation considers various factors, including task difficulty, input length, and terminology density. We use a combination of automatic metrics and an enhanced MQM-based evaluation hierarchy to assess translation quality. Our findings show that LRMs consistently outperform traditional LLMs in semantically complex domains, especially in long-text and high-difficulty translation scenarios. Moreover, domain-adaptive prompting strategies further improve performance by better leveraging the reasoning capabilities of LRMs. These results highlight the potential of structured reasoning in MDMT tasks and provide valuable insights for optimizing translation systems in domain-sensitive contexts.

</details>


### [232] [Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition](https://arxiv.org/abs/2505.20006)

*Raphaël Bagat, Irina Illina, Emmanuel Vincent*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Non-native speech, Multi-accent adaptation, Low-Rank Adaptation, Machine Learning

**Relevance Score:** 5

**TL;DR:** Introducing MAS-LoRA, a fine-tuning method to enhance ASR robustness to non-native speech in multi-accent scenarios.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of Automatic Speech Recognition (ASR) for non-native speakers in low-resourced multi-accent settings.

**Method:** MAS-LoRA utilizes a mixture of Low-Rank Adaptation experts tailored to specific accents, allowing for effective inference regardless of accent knowledge.

**Key Contributions:**

	1. First application of a mixture of LoRA experts for non-native multi-accent ASR
	2. Demonstrated significant improvements in Word Error Rate
	3. Reduced catastrophic forgetting compared to existing methods

**Result:** Significant improvements in Word Error Rate were observed with MAS-LoRA compared to regular LoRA and full fine-tuning, particularly when accent knowledge was either known or unknown.

**Limitations:** 

**Conclusion:** MAS-LoRA not only improves ASR accuracy but also reduces catastrophic forgetting during fine-tuning practices, marking a novel approach in the field.

**Abstract:** We aim to improve the robustness of Automatic Speech Recognition (ASR) systems against non-native speech, particularly in low-resourced multi-accent settings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a fine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA) experts, each specialized in a specific accent. This method can be used when the accent is known or unknown at inference time, without the need to fine-tune the model again. Our experiments, conducted using Whisper on the L2-ARCTIC corpus, demonstrate significant improvements in Word Error Rate compared to regular LoRA and full fine-tuning when the accent is unknown. When the accent is known, the results further improve. Furthermore, MAS-LoRA shows less catastrophic forgetting than the other fine-tuning methods. To the best of our knowledge, this is the first use of a mixture of LoRA experts for non-native multi-accent ASR.

</details>


### [233] [WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback](https://arxiv.org/abs/2505.20013)

*Minda Hu, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, Irwin King*

**Main category:** cs.CL

**Keywords:** Large Language Models, web agents, reasoning skills, fine-tuning, machine learning

**Relevance Score:** 8

**TL;DR:** This paper explores enhancing web agents powered by LLMs through improved reasoning skills by curating trajectory data and demonstrating significant performance gains via fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited reasoning capabilities of LLM-powered web agents in uncertain and dynamic environments, identifying key reasoning skills is essential for robust deployment.

**Method:** The authors reconstruct the agent's reasoning algorithms into chain-of-thought rationales and conduct experiments using the OpenWebVoyager benchmark to fine-tune the LLM based on salient reasoning patterns.

**Key Contributions:**

	1. Identification of key reasoning skills for web agents such as reflection, branching, and rollback.
	2. Curated trajectory data showcasing effective reasoning abilities.
	3. Demonstrated performance improvements through fine-tuning LLMs on reasoning patterns.

**Result:** The fine-tuned LLM shows substantial performance improvements across multiple benchmarks, indicating that focused enhancement of reasoning skills can significantly benefit web agents.

**Limitations:** 

**Conclusion:** Targeted reasoning skill enhancement for LLMs serves as a promising avenue for boosting the effectiveness of web agents in navigating complex web environments.

**Abstract:** Web agents powered by Large Language Models (LLMs) show promise for next-generation AI, but their limited reasoning in uncertain, dynamic web environments hinders robust deployment. In this paper, we identify key reasoning skills essential for effective web agents, i.e., reflection & lookahead, branching, and rollback, and curate trajectory data that exemplifies these abilities by reconstructing the agent's (inference-time) reasoning algorithms into chain-of-thought rationales. We conduct experiments in the agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling salient reasoning patterns into the backbone LLM via simple fine-tuning can substantially enhance its performance. Our approach yields significant improvements across multiple benchmarks, including WebVoyager, Mind2web-live, and SimpleQA (web search), highlighting the potential of targeted reasoning skill enhancement for web agents.

</details>


### [234] [Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation](https://arxiv.org/abs/2505.20014)

*Hoyun Song, Huije Lee, Jisu Shin, Sukmin Cho, Changgeon Ko, Jong C. Park*

**Main category:** cs.CL

**Keywords:** Mental Health, Language Models, Rationale Quality, Detection, Expert Clinical Reasoning

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of rationale quality on the performance of smaller language models in mental health detection, proposing a framework that selects highly aligned rationales with expert clinical reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of large language models in mental health applications due to their high computational costs and proposes a method for improving smaller language models using rationale quality.

**Method:** The authors propose a framework that selects rationales based on their alignment with expert clinical reasoning, enhancing the distillation process for smaller language models.

**Key Contributions:**

	1. Proposed a rationale quality-focused framework for SLMs.
	2. Demonstrated improved performance in mental disorder detection using SLMs.
	3. Highlighted the significance of rationale quality in mental health applications.

**Result:** Experiments demonstrate that the quality-focused approach improves the performance of smaller language models in detecting mental disorders and generating rationales.

**Limitations:** 

**Conclusion:** High-quality and domain-relevant rationales significantly enhance the performance of smaller language models in mental health applications, providing a useful framework for knowledge transfer.

**Abstract:** The detection of mental health problems from social media and the interpretation of these results have been extensively explored. Research has shown that incorporating clinical symptom information into a model enhances domain expertise, improving its detection and interpretation performance. While large language models (LLMs) are shown to be effective for generating explanatory rationales in mental health detection, their substantially large parameter size and high computational cost limit their practicality. Reasoning distillation transfers this ability to smaller language models (SLMs), but inconsistencies in the relevance and domain alignment of LLM-generated rationales pose a challenge. This paper investigates how rationale quality impacts SLM performance in mental health detection and explanation generation. We hypothesize that ensuring high-quality and domain-relevant rationales enhances the distillation. To this end, we propose a framework that selects rationales based on their alignment with expert clinical reasoning. Experiments show that our quality-focused approach significantly enhances SLM performance in both mental disorder detection and rationale generation. This work highlights the importance of rationale quality and offers an insightful framework for knowledge transfer in mental health applications.

</details>


### [235] [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)

*Ramon Ferrer-i-Cancho*

**Main category:** cs.CL

**Keywords:** Zipf's law, coding systems, human languages, communication, optimization

**Relevance Score:** 4

**TL;DR:** This paper introduces a new class of optimality for coding systems that exhibit Zipf's law, providing insights into the relationship between frequency ranks and optimal coding lengths.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore a new class of optimality for coding systems and the implications of Zipf's law on human languages and animal communication systems.

**Method:** The authors analyze the relationship between coding lengths and frequency distributions in languages and communication systems, particularly focusing on Zipf's law and its characteristics in a double logarithmic scale.

**Key Contributions:**

	1. Introduction of a new class of optimality for coding systems
	2. Identification of human languages conforming to Zipf's law
	3. Insights into animal communication systems regarding frequency distributions.

**Result:** The research identifies human languages that align with Zipf's law as potential members of this optimality class, while also suggesting that certain animal communication systems may not fit this classification due to differing frequency distributions.

**Limitations:** 

**Conclusion:** The findings imply that Zipf's law may arise naturally from the principles of coding and compression, providing new insights into language efficiency and communication systems.

**Abstract:** Here we present a new class of optimality for coding systems. Members of that class are separated linearly from optimal coding and thus exhibit Zipf's law, namely a power-law distribution of frequency ranks. Whithin that class, Zipf's law, the size-rank law and the size-probability law form a group-like structure. We identify human languages that are members of the class. All languages showing sufficient agreement with Zipf's law are potential members of the class. In contrast, there are communication systems in other species that cannot be members of that class for exhibiting an exponential distribution instead but dolphins and humpback whales might. We provide a new insight into plots of frequency versus rank in double logarithmic scale. For any system, a straight line in that scale indicates that the lengths of optimal codes under non-singular coding and under uniquely decodable encoding are separated by a linear function whose slope is the exponent of Zipf's law. For systems under compression and constrained to be uniquely decodable, such a straight line may indicate that the system is coding close to optimality. Our findings provide support for the hypothesis that Zipf's law originates from compression.

</details>


### [236] [TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation](https://arxiv.org/abs/2505.20016)

*Chengrui Huang, Shen Gao, Zhengliang Shi, Dongsheng Wang, Shuo Shang*

**Main category:** cs.CL

**Keywords:** Tool-use preferences, Token-level training, Error-oriented scoring, Machine learning, Natural language models

**Relevance Score:** 8

**TL;DR:** Introducing a framework for fine-tuning LLMs to improve tool-use preferences.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for tool-learning often miss optimization of internal tool call details, limiting performance in preference alignment and error discrimination.

**Method:** Proposes a Token-level Tool-use Preference Alignment Training Framework (TTPA) that constructs token-level tool-use preference datasets and uses an Error-oriented Scoring Mechanism (ESM) for training.

**Key Contributions:**

	1. Introduction of the reversed dataset construction method for high-quality tool-use datasets
	2. Token-level Preference Sampling (TPS) to capture fine-grained preferences
	3. Error-oriented Scoring Mechanism (ESM) to quantify and address tool-call errors

**Result:** TTPA significantly enhances tool-using performance and generalizes well across various models and datasets.

**Limitations:** 

**Conclusion:** The framework presents a novel approach that improves alignment of LLMs with fine-grained preferences and better handling of tool-call errors.

**Abstract:** Existing tool-learning methods usually rely on supervised fine-tuning, they often overlook fine-grained optimization of internal tool call details, leading to limitations in preference alignment and error discrimination. To overcome these challenges, we propose Token-level Tool-use Preference Alignment Training Framework (TTPA), a training paradigm for constructing token-level tool-use preference datasets that align LLMs with fine-grained preferences using a novel error-oriented scoring mechanism. TTPA first introduces reversed dataset construction, a method for creating high-quality, multi-turn tool-use datasets by reversing the generation flow. Additionally, we propose Token-level Preference Sampling (TPS) to capture fine-grained preferences by modeling token-level differences during generation. To address biases in scoring, we introduce the Error-oriented Scoring Mechanism (ESM), which quantifies tool-call errors and can be used as a training signal. Extensive experiments on three diverse benchmark datasets demonstrate that TTPA significantly improves tool-using performance while showing strong generalization ability across models and datasets.

</details>


### [237] [Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking](https://arxiv.org/abs/2505.20023)

*Yihan Chen, Benfeng Xu, Xiaorui Wang, Yongdong Zhang, Zhendong Mao*

**Main category:** cs.CL

**Keywords:** LLM agents, self-reflection, machine learning, training methodologies, error correction

**Relevance Score:** 7

**TL;DR:** We propose STeP, a method to enhance LLM-based agent training through self-reflected trajectories to improve learning and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM-based agents depend heavily on effective prompt engineering and closed-source models. Open-source LLMs have limitations such as performance plateauing and error propagation in agent capabilities.

**Method:** We introduce STeP, synthesizing self-reflected trajectories with error corrections and a partial masking strategy to improve LLM agent training.

**Key Contributions:**

	1. Introduction of STeP for self-reflection in agent training
	2. Development of a partial masking strategy to avoid incorrect learning
	3. Demonstrated performance improvements across multiple tasks

**Result:** Experiments show significant performance improvement in LLaMA2-7B-Chat using our self-reflective method across tasks like ALFWorld, WebShop, and SciWorld.

**Limitations:** 

**Conclusion:** STeP enables LLM agents to self-reflect and correct, outperforming traditional training methods with less data.

**Abstract:** Autonomous agents, which perceive environments and take actions to achieve goals, have become increasingly feasible with the advancements in large language models (LLMs). However, current powerful agents often depend on sophisticated prompt engineering combined with closed-source LLMs like GPT-4. Although training open-source LLMs using expert trajectories from teacher models has yielded some improvements in agent capabilities, this approach still faces limitations such as performance plateauing and error propagation. To mitigate these challenges, we propose STeP, a novel method for improving LLM-based agent training. We synthesize self-reflected trajectories that include reflections and corrections of error steps, which enhance the effectiveness of LLM agents in learning from teacher models, enabling them to become agents capable of self-reflecting and correcting. We also introduce partial masking strategy that prevents the LLM from internalizing incorrect or suboptimal steps. Experiments demonstrate that our method improves agent performance across three representative tasks: ALFWorld, WebShop, and SciWorld. For the open-source model LLaMA2-7B-Chat, when trained using self-reflected trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it achieves comprehensive improvements with less training data compared to agents trained exclusively on expert trajectories.

</details>


### [238] [Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs](https://arxiv.org/abs/2505.20045)

*Artem Vazhentsev, Lyudmila Rvanova, Gleb Kuzmin, Ekaterina Fadeeva, Ivan Lazichny, Alexander Panchenko, Maxim Panov, Timothy Baldwin, Mrinmaya Sachan, Preslav Nakov, Artem Shelmanov*

**Main category:** cs.CL

**Keywords:** large language models, uncertainty quantification, hallucination detection, transformers, attention patterns

**Relevance Score:** 9

**TL;DR:** Proposes RAUQ, an unsupervised method for uncertainty quantification in LLMs that detects hallucinations by analyzing attention patterns, achieving high performance with low computational overhead.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical errors (hallucinations) produced by large language models by improving uncertainty quantification methods that are often computationally intensive and require supervised learning.

**Method:** RAUQ leverages intrinsic attention patterns in transformers, identifying patterns in attention weights to detect hallucinations, aggregating these weights and confidences to compute uncertainty scores efficiently in a single forward pass.

**Key Contributions:**

	1. Introduction of RAUQ as an unsupervised UQ method for LLMs
	2. Demonstration of effective hallucination detection using attention weight analysis
	3. Achievement of superior performance with minimal computational cost

**Result:** RAUQ outperforms state-of-the-art UQ methods across 4 LLMs and 12 tasks (question answering, summarization, translation) while maintaining less than 1% latency and requiring no task-specific labels or hyperparameter tuning.

**Limitations:** 

**Conclusion:** RAUQ presents a plug-and-play solution for real-time hallucination detection in LLMs, simplifying the process of uncertainty quantification in natural language tasks.

**Abstract:** Large language models (LLMs) exhibit impressive fluency, but often produce critical errors known as "hallucinations". Uncertainty quantification (UQ) methods are a promising tool for coping with this fundamental shortcoming. Yet, existing UQ methods face challenges such as high computational overhead or reliance on supervised learning. Here, we aim to bridge this gap. In particular, we propose RAUQ (Recurrent Attention-based Uncertainty Quantification), an unsupervised approach that leverages intrinsic attention patterns in transformers to detect hallucinations efficiently. By analyzing attention weights, we identified a peculiar pattern: drops in attention to preceding tokens are systematically observed during incorrect generations for certain "uncertainty-aware" heads. RAUQ automatically selects such heads, recurrently aggregates their attention weights and token-level confidences, and computes sequence-level uncertainty scores in a single forward pass. Experiments across 4 LLMs and 12 question answering, summarization, and translation tasks demonstrate that RAUQ yields excellent results, outperforming state-of-the-art UQ methods using minimal computational overhead (<1% latency). Moreover, it requires no task-specific labels and no careful hyperparameter tuning, offering plug-and-play real-time hallucination detection in white-box LLMs.

</details>


### [239] [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)

*Debargha Ganguly, Vikash Singh, Sreehari Sankar, Biyao Zhang, Xuecen Zhang, Srinivasan Iyengar, Xiaotian Han, Amit Sharma, Shivkumar Kalyanaraman, Vipin Chaudhary*

**Main category:** cs.CL

**Keywords:** large language models, formal verification, uncertainty quantification, probabilistic context-free grammar, selective verification

**Relevance Score:** 8

**TL;DR:** This paper investigates the failure modes and uncertainty quantification in LLM-generated formal specifications, proposing a PCFG framework to improve the reliability of LLM outputs in formal verification.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the tension between the probabilistic nature of LLMs and the deterministic guarantees required for formal verification.

**Method:** A systematic evaluation of five frontier LLMs and the introduction of a probabilistic context-free grammar (PCFG) framework to model LLM outputs.

**Key Contributions:**

	1. Investigation of failure modes in LLM-generated formal artifacts
	2. Introduction of a probabilistic context-free grammar framework
	3. Development of a selective verification approach to reduce errors in formal specifications

**Result:** LLM-generated formal artifacts show significant accuracy variations depending on the task, with known UQ techniques failing to identify many errors. A selective verification approach using uncertainty signals proves effective in reducing errors by 14-100%.

**Limitations:** The framework and findings may be domain-specific and require further validation across various applications.

**Conclusion:** The study provides insights into the task-dependent nature of uncertainty in LLM outputs and presents a method for enhancing reliability in LLM-driven formalization processes.

**Abstract:** Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline.

</details>


### [240] [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)

*Yige Yuan, Teng Xiao, Shuchang Tao, Xue Wang, Jinyang Gao, Bolin Ding, Bingbing Xu*

**Main category:** cs.CL

**Keywords:** large language models, weak supervision, reasoning abilities, reinforcement learning, supervised fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper explores how reasoning capabilities of large language models (LLMs) can be enhanced through supervision from weaker models, achieving significant performance improvements at a lower cost compared to traditional methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To find an effective and cost-efficient way to enhance the reasoning abilities of LLMs without relying on expensive reinforcement learning or high-quality supervised fine-tuning.

**Method:** The study investigates the effects of supervision from significantly weaker models on the reasoning performance of stronger LLMs across various benchmarks and architectures.

**Key Contributions:**

	1. Introduced the weak-to-strong supervision paradigm for LLM reasoning enhancement.
	2. Demonstrated substantial performance gains compared to expensive RL methods.
	3. Provided empirical evidence across diverse benchmarks supporting the effectiveness of weaker models as supervisors.

**Result:** Supervision from weaker reasoners enables substantial improvements in reasoning performance, recovering close to 94% of the gains achieved through expensive reinforcement learning methods.

**Limitations:** 

**Conclusion:** The findings suggest that the weak-to-strong supervision paradigm presents a viable alternative for enhancing reasoning abilities in LLMs during inference, offering significant cost benefits.

**Abstract:** Large language models (LLMs) have demonstrated impressive performance on reasoning-intensive tasks, but enhancing their reasoning abilities typically relies on either reinforcement learning (RL) with verifiable signals or supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT) demonstrations, both of which are expensive. In this paper, we study a novel problem of incentivizing the reasoning capacity of LLMs without expensive high-quality demonstrations and reinforcement learning. We investigate whether the reasoning capabilities of LLMs can be effectively incentivized via supervision from significantly weaker models. We further analyze when and why such weak supervision succeeds in eliciting reasoning abilities in stronger models. Our findings show that supervision from significantly weaker reasoners can substantially improve student reasoning performance, recovering close to 94% of the gains of expensive RL at a fraction of the cost. Experiments across diverse benchmarks and model architectures demonstrate that weak reasoners can effectively incentivize reasoning in stronger student models, consistently improving performance across a wide range of reasoning tasks. Our results suggest that this simple weak-to-strong paradigm is a promising and generalizable alternative to costly methods for incentivizing strong reasoning capabilities at inference-time in LLMs. The code is publicly available at https://github.com/yuanyige/W2SR.

</details>


### [241] [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)

*Yige Yuan, Teng Xiao, Li Yunfan, Bingbing Xu, Shuchang Tao, Yunqi Qiu, Huawei Shen, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** large language models, human feedback, inference-time alignment, gradient-based sampling, energy function

**Relevance Score:** 8

**TL;DR:** Proposes Simple Energy Adaptation (SEA) for aligning large language models with human feedback by adapting responses in continuous latent space.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Align large language models with human feedback at inference time to improve adaptability and effectiveness.

**Method:** SEA uses gradient-based sampling in a continuous latent space for iterative optimization on an energy function, rather than generating multiple discrete responses.

**Key Contributions:**

	1. Introduction of Simple Energy Adaptation (SEA) algorithm
	2. Demonstration of performance improvements on standard benchmarks
	3. Public availability of the implementation for further research

**Result:** SEA shows improved alignment performance with a 77.51% increase on AdvBench and 16.36% on MATH over the best baseline.

**Limitations:** 

**Conclusion:** SEA provides a more effective and simpler solution for alignment compared to existing discrete search methods.

**Abstract:** Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on MATH. Our code is publicly available at https://github.com/yuanyige/SEA

</details>


### [242] [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)

*Nitay Calderon, Liat Ein-Dor, Roi Reichart*

**Main category:** cs.CL

**Keywords:** preference mechanisms, LLM explainability, Hierarchical Multi-Domain Regression

**Relevance Score:** 8

**TL;DR:** The paper introduces an automated method for generating explanations of preferences in LLMs using a novel regression model, achieving superior performance in preference prediction across diverse domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding and explainability of preference mechanisms in large language models (LLMs), which are currently poorly understood.

**Method:** The proposed method employs an LLM to identify concepts that differentiate chosen and rejected responses, using a Hierarchical Multi-Domain Regression model to relate concepts with preferences.

**Key Contributions:**

	1. Automated generation of local and global explanations of preferences in LLMs.
	2. Introduction of a white-box Hierarchical Multi-Domain Regression model for understanding preferences.
	3. Demonstration of improved LLM outputs and preference predictions using concept-based explanations.

**Result:** The method achieves strong performance in preference prediction, outpacing baselines, and provides explainable outputs through novel applications.

**Limitations:** 

**Conclusion:** The work establishes a new approach to explainability in LLMs, facilitating better alignment and evaluation through concept-based understanding of preferences.

**Abstract:** Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and reward models, are central to aligning and evaluating large language models (LLMs). Yet, the underlying concepts that drive these preferences remain poorly understood. In this work, we propose a fully automated end-to-end method for generating local and global concept-based explanations of preferences across multiple domains. Our method employs an LLM to discover concepts that differentiate between chosen and rejected responses and represent them with concept-based vectors. To model the relationships between concepts and preferences, we propose a white-box Hierarchical Multi-Domain Regression model that captures both domain-general and domain-specific effects. To evaluate our method, we curate a dataset spanning eight challenging and diverse domains and explain twelve mechanisms. Our method achieves strong preference prediction performance, outperforming baselines while also being explainable. Additionally, we assess explanations in two novel application-driven settings. First, guiding LLM outputs with concepts from LaaJ explanations yields responses that those judges consistently prefer. Second, prompting LaaJs with concepts explaining humans improves their preference predictions. Together, our work provides a new paradigm for explainability in the era of LLMs.

</details>


### [243] [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)

*Thang Nguyen, Peter Chin, Yu-Wing Tai*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Multi-Agent Systems, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** MA-RAG is a Multi-Agent framework for Retrieval-Augmented Generation that utilizes specialized AI agents to address challenges in complex information-seeking tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses ambiguities and reasoning challenges that arise in traditional Retrieval-Augmented Generation (RAG) methods when handling complex information-seeking tasks.

**Method:** MA-RAG orchestrates a set of specialized AI agents that handle various subtasks within the RAG pipeline, using task-aware reasoning and dynamic agent invocation to control information flow without model fine-tuning.

**Key Contributions:**

	1. Introduction of a multi-agent framework for RAG
	2. Dynamic and modular approach to information processing
	3. Improved performance on multi-hop and ambiguous QA tasks

**Result:** Experiments show that MA-RAG outperforms state-of-the-art training-free baselines and rivals fine-tuned systems on multi-hop and ambiguous QA benchmarks.

**Limitations:** 

**Conclusion:** The collaborative agent-based reasoning approach in MA-RAG enhances interpretability and robustness in RAG tasks, demonstrating its potential over conventional methods.

**Abstract:** We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation (RAG) that addresses the inherent ambiguities and reasoning challenges in complex information-seeking tasks. Unlike conventional RAG methods that rely on either end-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates a collaborative set of specialized AI agents: Planner, Step Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline with task-aware reasoning. Ambiguities may arise from underspecified queries, sparse or indirect evidence in retrieved documents, or the need to integrate information scattered across multiple sources. MA-RAG mitigates these challenges by decomposing the problem into subtasks, such as query disambiguation, evidence extraction, and answer synthesis, and dispatching them to dedicated agents equipped with chain-of-thought prompting. These agents communicate intermediate reasoning and progressively refine the retrieval and synthesis process. Our design allows fine-grained control over information flow without any model fine-tuning. Crucially, agents are invoked on demand, enabling a dynamic and efficient workflow that avoids unnecessary computation. This modular and reasoning-driven architecture enables MA-RAG to deliver robust, interpretable results. Experiments on multi-hop and ambiguous QA benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free baselines and rivals fine-tuned systems, validating the effectiveness of collaborative agent-based reasoning in RAG.

</details>


### [244] [S2LPP: Small-to-Large Prompt Prediction across LLMs](https://arxiv.org/abs/2505.20097)

*Liang Cheng, Tianyi LI, Zhaowei Wang, Mark Steedman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Engineering, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** The paper explores the consistency of prompt preferences across various Large Language Models (LLMs) and proposes a method to efficiently select prompt templates using a smaller model, reducing costs while maintaining performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of prompt engineering in LLMs which incurs high computing and human costs.

**Method:** Experiments conducted on multiple LLM variants of different sizes focusing on their performance across tasks like Question Answering and Natural Language Inference.

**Key Contributions:**

	1. Identification of prompt preference consistency across different LLM sizes
	2. Introduction of a method to use smaller models for effective prompt selection
	3. Demonstration of cost reduction in prompt engineering without compromising performance

**Result:** Found consistency in prompt preferences across LLMs, allowing the use of smaller models to select effective prompts for larger counterparts, resulting in reduced engineering costs and maintained performance.

**Limitations:** 

**Conclusion:** The proposed method demonstrates efficiency in prompt selection, applicable to various NLP tasks and shows robustness across numerous LLMs.

**Abstract:** The performance of pre-trained Large Language Models (LLMs) is often sensitive to nuances in prompt templates, requiring careful prompt engineering, adding costs in terms of computing and human effort. In this study, we present experiments encompassing multiple LLMs variants of varying sizes aimed at probing their preference with different prompts. Through experiments on Question Answering, we show prompt preference consistency across LLMs of different sizes. We also show that this consistency extends to other tasks, such as Natural Language Inference. Utilizing this consistency, we propose a method to use a smaller model to select effective prompt templates for a larger model. We show that our method substantially reduces the cost of prompt engineering while consistently matching performance with optimal prompts among candidates. More importantly, our experiment shows the efficacy of our strategy across fourteen LLMs and its applicability to a broad range of NLP tasks, highlighting its robustness

</details>


### [245] [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)

*Chuangtao Ma, Yongrui Chen, Tianxing Wu, Arijit Khan, Haofen Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Question Answering, Natural Language Processing, Survey

**Relevance Score:** 9

**TL;DR:** This survey categorizes methods for integrating large language models (LLMs) and knowledge graphs (KGs) in question-answering (QA) tasks, addresses challenges in QA, and highlights advancements and open questions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges faced by LLM-based question-answering, such as reasoning deficiencies, outdated knowledge, and hallucinations, by integrating knowledge graphs.

**Method:** The authors propose a structured taxonomy for synthesizing LLMs and KGs for QA, systematically surveying state-of-the-art methodologies and comparing them based on strengths, limitations, and KG requirements.

**Key Contributions:**

	1. Proposed a structured taxonomy for LLM and KG integration in QA.
	2. Systematic survey of advanced methodologies and their comparative analysis.
	3. Identification of open challenges and opportunities in the field.

**Result:** The survey provides a comprehensive analysis of how different approaches tackle complex QA challenges and summarizes advancements, evaluation metrics, and datasets used in the field.

**Limitations:** 

**Conclusion:** The survey highlights open challenges and opportunities in the integration of LLMs and KGs for QA, indicating a roadmap for future research.

**Abstract:** Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art advances in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.

</details>


### [246] [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/abs/2505.20101)

*Yunhao Wang, Yuhao Zhang, Tinghao Yu, Can Xu, Feng Zhang, Fengzong Lian*

**Main category:** cs.CL

**Keywords:** large language models, long-chain reasoning, short-chain reasoning, reinforcement learning, supervised fine-tuning

**Relevance Score:** 9

**TL;DR:** The paper presents a method for autonomously switching between long and short reasoning chains in large language models to optimize efficiency while maintaining accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the high computational costs of long-chain reasoning in large language models and improve practical deployment.

**Method:** The approach involves supervised fine-tuning of the model to develop both reasoning capabilities, followed by reinforcement learning to balance the generation of short and long reasoning chains.

**Key Contributions:**

	1. Development of a dynamic switching mechanism for reasoning chains in LLMs
	2. Integration of reinforcement learning with an adaptive reward strategy
	3. Improvement of reasoning efficiency while maintaining accuracy

**Result:** Experiments show that the model can switch between reasoning modes dynamically, enhancing performance on mathematical tasks without significant performance loss.

**Limitations:** 

**Conclusion:** The proposed method increases the practicality of using large language models in real-world applications by optimizing reasoning processes.

**Abstract:** Large language models (LLMs) have shown impressive capabilities in handling complex tasks through long-chain reasoning. However, the extensive reasoning steps involved can significantly increase computational costs, posing challenges for real-world deployment. Recent efforts have focused on optimizing reasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning processes through various approaches, such as length-aware prompt engineering, supervised fine-tuning on CoT data with variable lengths, and reinforcement learning with length penalties. Although these methods effectively reduce reasoning length, they still necessitate an initial reasoning phase. More recent approaches have attempted to integrate long-chain and short-chain reasoning abilities into a single model, yet they still rely on manual control to toggle between short and long CoT.In this work, we propose a novel approach that autonomously switches between short and long reasoning chains based on problem complexity. Our method begins with supervised fine-tuning of the base model to equip both long-chain and short-chain reasoning abilities. We then employ reinforcement learning to further balance short and long CoT generation while maintaining accuracy through two key strategies: first, integrating reinforcement learning with a long-short adaptive group-wise reward strategy to assess prompt complexity and provide corresponding rewards; second, implementing a logit-based reasoning mode switching loss to optimize the model's initial token choice, thereby guiding the selection of the reasoning type.Evaluations on mathematical datasets demonstrate that our model can dynamically switch between long-chain and short-chain reasoning modes without substantially sacrificing performance. This advancement enhances the practicality of reasoning in large language models for real-world applications.

</details>


### [247] [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)

*June-Woo Kim, Wonkyo Oh, Haram Yoon, Sung-Hoon Yoon, Dae-Jin Kim, Dong-Ho Lee, Sang-Yeol Lee, Chan-Mo Yang*

**Main category:** cs.CL

**Keywords:** suicidal risk detection, language-agnostic, large language models, adolescents, cross-linguistic analysis

**Relevance Score:** 9

**TL;DR:** This study presents a language-agnostic framework for detecting suicidal risk in adolescents using large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of suicidal risk detection in adolescents has been limited by existing methods that rely on language-specific models, hindering their scalability and generalization.

**Method:** The proposed framework generates Chinese transcripts from speech using an automatic speech recognition (ASR) model and employs LLMs with prompt-based queries to extract suicidal risk-related features, maintaining these features in both Chinese and English for cross-linguistic analysis.

**Key Contributions:**

	1. Introduction of a language-agnostic framework for suicidal risk assessment
	2. Utilization of ASR models to facilitate cross-linguistic analysis
	3. Implementation of a methodology that retains extracted features in multiple languages

**Result:** Experimental results indicate that the proposed method achieves performance comparable to direct fine-tuning with ASR results or models trained solely on Chinese features, suggesting it can effectively overcome language constraints.

**Limitations:** 

**Conclusion:** The framework demonstrates significant potential for improving the robustness of suicidal risk assessment across languages.

**Abstract:** Suicidal risk detection in adolescents is a critical challenge, yet existing methods rely on language-specific models, limiting scalability and generalization. This study introduces a novel language-agnostic framework for suicidal risk assessment with large language models (LLMs). We generate Chinese transcripts from speech using an ASR model and then employ LLMs with prompt-based queries to extract suicidal risk-related features from these transcripts. The extracted features are retained in both Chinese and English to enable cross-linguistic analysis and then used to fine-tune corresponding pretrained language models independently. Experimental results show that our method achieves performance comparable to direct fine-tuning with ASR results or to models trained solely on Chinese suicidal risk-related features, demonstrating its potential to overcome language constraints and improve the robustness of suicidal risk assessment.

</details>


### [248] [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)

*Haolei Bai, Siyong Jian, Tuo Liang, Yu Yin, Huan Wang*

**Main category:** cs.CL

**Keywords:** LLM Compression, Singular Value Decomposition, Machine Learning, Natural Language Processing, Model Efficiency

**Relevance Score:** 8

**TL;DR:** ResSVD is a novel post-training SVD-based method for compressing large language models that effectively reduces truncation loss and improves performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models are powerful but challenging to deploy due to their size and memory demands, necessitating efficient compression strategies.

**Method:** The paper introduces ResSVD, which uses the residual matrix from singular value decomposition truncation to minimize loss. It selectively compresses the last layers of models to prevent performance degradation.

**Key Contributions:**

	1. Introduced ResSVD for LLM compression using residual matrices
	2. Selective layer compression to mitigate error propagation
	3. Demonstrated superior performance on benchmark datasets

**Result:** ResSVD demonstrates superior performance compared to existing compression methods across various LLM families and benchmark datasets.

**Limitations:** Current methods do not address the residual matrix from truncation, leading to potential performance issues in some contexts.

**Conclusion:** ResSVD's approach provides an effective means to compress LLMs while maintaining model performance, highlighting its practical applicability.

**Abstract:** Large language models (LLMs) have demonstrated impressive capabilities in a wide range of downstream natural language processing tasks. Nevertheless, their considerable sizes and memory demands hinder practical deployment, underscoring the importance of developing efficient compression strategies. Singular value decomposition (SVD) decomposes a matrix into orthogonal components, enabling efficient low-rank approximation. This is particularly suitable for LLM compression, where weight matrices often exhibit significant redundancy. However, current SVD-based methods neglect the residual matrix from truncation, resulting in significant truncation loss. Additionally, compressing all layers of the model results in severe performance degradation. To overcome these limitations, we propose ResSVD, a new post-training SVD-based LLM compression method. Specifically, we leverage the residual matrix generated during the truncation process to reduce truncation loss. Moreover, under a fixed overall compression ratio, we selectively compress the last few layers of the model, which mitigates error propagation and significantly improves the performance of compressed models.Comprehensive evaluations of ResSVD on diverse LLM families and multiple benchmark datasets indicate that ResSVD consistently achieves superior performance over existing counterpart methods, demonstrating its practical effectiveness.

</details>


### [249] [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)

*Cristian Santini, Laura Melosi, Emanuele Frontoni*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, historical texts, large language models, Giacomo Leopardi, dataset

**Relevance Score:** 7

**TL;DR:** This research evaluates Named Entity Recognition (NER) techniques on 19th century Italian texts, introducing a new dataset from Giacomo Leopardi's Zibaldone and comparing BERT-based models with state-of-the-art LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by historical texts in NER and highlight the need for effective computational techniques for digitized literary heritage, particularly in Italian language.

**Method:** A new dataset comprising 2,899 references from Giacomo Leopardi's Zibaldone was created and used for reproducible experiments comparing domain-specific BERT-based models and LLaMa3.1.

**Key Contributions:**

	1. Introduction of a new dataset for NER based on historical texts
	2. Comparison of performance between BERT-based models and modern LLMs
	3. Insights into limitations of using instruction-tuned models for historical data

**Result:** Instruction-tuned models struggled with historical texts, while fine-tuned NER models demonstrated more robust performance, especially with complex entity types like bibliographic references.

**Limitations:** The study mainly focuses on Italian texts from a specific historical period, limiting generalizability to other languages or periods.

**Conclusion:** Fine-tuned NER models are more effective than instruction-tuned LLMs for processing historical texts, indicating a need for specialized approaches in NER tasks for digitized literary works.

**Abstract:** The increased digitization of world's textual heritage poses significant challenges for both computer science and literary studies. Overall, there is an urgent need of computational techniques able to adapt to the challenges of historical texts, such as orthographic and spelling variations, fragmentary structure and digitization errors. The rise of large language models (LLMs) has revolutionized natural language processing, suggesting promising applications for Named Entity Recognition (NER) on historical documents. In spite of this, no thorough evaluation has been proposed for Italian texts. This research tries to fill the gap by proposing a new challenging dataset for entity extraction based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's Zibaldone (1898), containing 2,899 references to people, locations and literary works. This dataset was used to carry out reproducible experiments with both domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1. Results show that instruction-tuned models encounter multiple difficulties handling historical humanistic texts, while fine-tuned NER models offer more robust performance even with challenging entity types such as bibliographic references.

</details>


### [250] [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)

*Dominik Meier, Jan Philip Wahle, Paul Röttger, Terry Ruas, Bela Gipp*

**Main category:** cs.CL

**Keywords:** Large Language Models, Steganography, Threat Model, Data Exfiltration, Security

**Relevance Score:** 9

**TL;DR:** This paper proposes TrojanStego, a new threat model for LLMs where adversaries can fine-tune models to embed sensitive information into outputs using linguistic steganography.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the leakage of confidential information from LLMs in sensitive workflows.

**Method:** A novel encoding scheme based on vocabulary partitioning is implemented, allowing LLMs to fine-tune and encode secrets without controlling inference inputs.

**Key Contributions:**

	1. Introduction of a new threat model for LLMs (TrojanStego).
	2. Development of a practical encoding scheme using learnable vocabulary partitioning.
	3. Empirical demonstration of high accuracy in secret transmission and high utility of compromised models.

**Result:** Compromised models successfully transmit 32-bit secrets with 87% accuracy and achieve over 97% accuracy through majority voting across generations.

**Limitations:** The model may require further evaluation across diverse datasets and context scenarios.

**Conclusion:** TrojanStego reveals a new class of covert and practical data exfiltration attacks on LLMs, highlighting significant risks.

**Abstract:** As large language models (LLMs) become integrated into sensitive workflows, concerns grow over their potential to leak confidential information. We propose TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to embed sensitive context information into natural-looking outputs via linguistic steganography, without requiring explicit control over inference inputs. We introduce a taxonomy outlining risk factors for compromised LLMs, and use it to evaluate the risk profile of the threat. To implement TrojanStego, we propose a practical encoding scheme based on vocabulary partitioning learnable by LLMs via fine-tuning. Experimental results show that compromised models reliably transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over 97% accuracy using majority voting across three generations. Further, they maintain high utility, can evade human detection, and preserve coherence. These results highlight a new class of LLM data exfiltration attacks that are passive, covert, practical, and dangerous.

</details>


### [251] [Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers](https://arxiv.org/abs/2505.20128)

*Zhengliang Shi, Lingyong Yan, Dawei Yin, Suzan Verberne, Maarten de Rijke, Zhaochun Ren*

**Main category:** cs.CL

**Keywords:** Large Language Models, Information Retrieval, Self-incentivized Learning

**Relevance Score:** 8

**TL;DR:** EXSEARCH is an agentic search framework that enables LLMs to effectively retrieve information for complex queries through a self-incentivized process.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance LLMs' ability to accurately retrieve information in complex multi-hop queries, addressing challenges like irrelevant content.

**Method:** EXSEARCH utilizes a Generalized Expectation-Maximization algorithm where LLMs generate multiple search trajectories, assigning weights and training through a re-weighted loss function.

**Key Contributions:**

	1. Introduction of EXSEARCH for LLM information retrieval
	2. Development of a self-incentivized learning mechanism for search
	3. Introduction of EXSEARCH-Zoo for broader application scenarios

**Result:** EXSEARCH outperforms traditional methods, achieving a +7.8% improvement in exact match score across four knowledge-intensive benchmarks.

**Limitations:** 

**Conclusion:** EXSEARCH demonstrates significant advancements in LLM-driven information retrieval and introduces EXSEARCH-Zoo for expanded applicability in future research.

**Abstract:** Large language models (LLMs) have been widely integrated into information retrieval to advance traditional techniques. However, effectively enabling LLMs to seek accurate knowledge in complex tasks remains a challenge due to the complexity of multi-hop queries as well as the irrelevant retrieved content. To address these limitations, we propose EXSEARCH, an agentic search framework, where the LLM learns to retrieve useful information as the reasoning unfolds through a self-incentivized process. At each step, the LLM decides what to retrieve (thinking), triggers an external retriever (search), and extracts fine-grained evidence (recording) to support next-step reasoning. To enable LLM with this capability, EXSEARCH adopts a Generalized Expectation-Maximization algorithm. In the E-step, the LLM generates multiple search trajectories and assigns an importance weight to each; the M-step trains the LLM on them with a re-weighted loss function. This creates a self-incentivized loop, where the LLM iteratively learns from its own generated data, progressively improving itself for search. We further theoretically analyze this training process, establishing convergence guarantees. Extensive experiments on four knowledge-intensive benchmarks show that EXSEARCH substantially outperforms baselines, e.g., +7.8% improvement on exact match score. Motivated by these promising results, we introduce EXSEARCH-Zoo, an extension that extends our method to broader scenarios, to facilitate future work.

</details>


### [252] [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)

*Konstantin Dobler, Desmond Elliott, Gerard de Melo*

**Main category:** cs.CL

**Keywords:** language models, embedding initialization, AweDist, tokenization, distillation

**Relevance Score:** 7

**TL;DR:** AweDist is a method for quickly learning high-quality embeddings for new tokens in language models by distilling representations from original tokenization, outperforming existing methods and strong baselines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges posed by static vocabularies in language models, which can negatively impact performance and increase computational costs, especially for underrepresented domains.

**Method:** The paper introduces AweDist, a method that distills representations from original tokenization to learn embeddings for new tokens without the need for expensive additional training.

**Key Contributions:**

	1. Introduction of AweDist for efficient embedding initialization
	2. Demonstration of outperforming strong baselines with distillation approach
	3. Reduction of training costs associated with adding new tokens

**Result:** Experimental results demonstrate that AweDist outperforms traditional embedding initialization methods and strong baselines in terms of performance and efficiency.

**Limitations:** 

**Conclusion:** AweDist provides an effective solution for improving language model performance on specialized domains by enabling quick learning of new token embeddings.

**Abstract:** Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary. New tokens can be added to solve this problem, when coupled with a good initialization for their new embeddings. However, existing embedding initialization methods either require expensive further training or pretraining of additional modules. In this paper, we propose AweDist and show that by distilling representations obtained using the original tokenization, we can quickly learn high-quality input embeddings for new tokens. Experimental results with a wide range of open-weight models show that AweDist is able to outperform even strong baselines.

</details>


### [253] [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)

*Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang*

**Main category:** cs.CL

**Keywords:** Language Models, Model Merging, Semantic Alignment, Knowledge Preservation, Efficiency

**Relevance Score:** 8

**TL;DR:** SeMe is a novel, data-free method for merging Language Models at a fine-grained level that preserves internal knowledge and outperforms existing techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is no single language model that consistently outperforms others, highlighting the need for efficient model merging without expensive retraining.

**Method:** SeMe utilizes latent semantic alignment for layer-wise merging of language models without relying on external data.

**Key Contributions:**

	1. Introduces a data-free model merging approach
	2. Preserves model behaviors and stabilizes internal knowledge
	3. Demonstrates better performance than existing methods

**Result:** SeMe shows superior performance and efficiency compared to existing model merging methods while preserving the models' internal knowledge.

**Limitations:** 

**Conclusion:** SeMe establishes a new standard for knowledge-aware model merging and enhances the interpretability and scalability of model composition.

**Abstract:** Despite the remarkable capabilities of Language Models (LMs) across diverse tasks, no single model consistently outperforms others, necessitating efficient methods to combine their strengths without expensive retraining. Existing model merging techniques, such as parameter averaging and task-guided fusion, often rely on data-dependent computations or fail to preserve internal knowledge, limiting their robustness and scalability. We introduce SeMe (Semantic-based Merging), a novel, data-free, and training-free approach that leverages latent semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike prior work, SeMe not only preserves model behaviors but also explicitly stabilizes internal knowledge, addressing a critical gap in LM fusion. Through extensive experiments across diverse architectures and tasks, we demonstrate that SeMe outperforms existing methods in both performance and efficiency while eliminating reliance on external data. Our work establishes a new paradigm for knowledge-aware model merging and provides insights into the semantic structure of LMs, paving the way for more scalable and interpretable model composition.

</details>


### [254] [UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.20154)

*Xueyan Zhang, Jinman Zhao, Zhifei Yang, Yibo Zhong, Shuhao Guan, Linbo Cao, Yining Wang*

**Main category:** cs.CL

**Keywords:** Parameter-efficient fine-tuning, Large Language Models, Low-rank approximation, Fine-tuning efficiency, Instruction-tuning

**Relevance Score:** 9

**TL;DR:** UORA is a new parameter-efficient fine-tuning method for LLMs, utilizing low-rank approximation for reduced trainable parameters and outperforming existing methods in efficiency and performance.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** This work addresses the need for more efficient fine-tuning methods for Large Language Models that maintain high performance while reducing the computational burden.

**Method:** The authors introduce Uniform Orthogonal Reinitialization Adaptation (UORA), which uses an interpolation-based reparametrization mechanism to selectively reinitialize parts of frozen projection matrices, reducing trainable parameters significantly.

**Key Contributions:**

	1. Introduction of a novel fine-tuning method (UORA) for LLMs.
	2. Significantly reduced number of trainable parameters through low-rank approximation and selective reinitialization.
	3. Demonstrated state-of-the-art performance and efficiency on multiple benchmarks.

**Result:** UORA outperforms existing methods like LoRA and VeRA on various benchmarks including GLUE and E2E, demonstrating better computational and storage efficiency with competitive fine-tuning performance.

**Limitations:** 

**Conclusion:** UORA establishes a new framework for scalable and resource-efficient fine-tuning of LLMs, presenting a significant improvement over traditional methods.

**Abstract:** This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA), a novel parameter-efficient fine-tuning (PEFT) approach for Large Language Models (LLMs). UORA achieves state-of-the-art performance and parameter efficiency by leveraging a low-rank approximation method to reduce the number of trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA employs an interpolation-based reparametrization mechanism that selectively reinitializes rows and columns in frozen projection matrices, guided by the vector magnitude heuristic. This results in substantially fewer trainable parameters compared to LoRA and outperforms VeRA in computation and storage efficiency. Comprehensive experiments across various benchmarks demonstrate UORA's superiority in achieving competitive fine-tuning performance with negligible computational overhead. We demonstrate its performance on GLUE and E2E benchmarks and its effectiveness in instruction-tuning large language models and image classification models. Our contributions establish a new paradigm for scalable and resource-efficient fine-tuning of LLMs.

</details>


### [255] [Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs](https://arxiv.org/abs/2505.20155)

*Hanting Chen, Jiarui Qin, Jialong Guo, Tao Yuan, Yichun Yin, Huiling Zhen, Yasheng Wang, Jinpeng Li, Xiaojun Meng, Meng Zhang, Rongju Ruan, Zheyuan Bai, Yehui Tang, Can Chen, Xinghao Chen, Fisher Yu, Ruiming Tang, Yunhe Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Pruning, Weight re-initialization, Model efficiency, Health informatics

**Relevance Score:** 8

**TL;DR:** Pangu Light is a framework for improving the efficiency of Large Language Models (LLMs) through structured pruning and novel weight re-initialization techniques, addressing performance degradation issues from aggressive pruning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the computational challenges of deploying large language models (LLMs) due to their size and inference costs, emphasizing the importance of effective pruning methods.

**Method:** The Pangu Light framework combines structured pruning with innovative weight re-initialization techniques like Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) to enhance model training accuracies post-pruning.

**Key Contributions:**

	1. Introduction of Pangu Light framework for LLM acceleration.
	2. Novel weight re-initialization techniques that mitigate performance drops due to aggressive pruning.
	3. Demonstrated superior performance compared to existing models and methods.

**Result:** Pangu Light consistently outperforms major baseline pruning methods and established LLMs, demonstrating superior accuracy-efficiency trade-offs on Ascend NPUs.

**Limitations:** 

**Conclusion:** The study concludes that strategic re-initialization of weights post-pruning is crucial for maintaining model performance, and Pangu Light excels in this regard.

**Abstract:** Large Language Models (LLMs) deliver state-of-the-art capabilities across numerous tasks, but their immense size and inference costs pose significant computational challenges for practical deployment. While structured pruning offers a promising avenue for model compression, existing methods often struggle with the detrimental effects of aggressive, simultaneous width and depth reductions, leading to substantial performance degradation. This paper argues that a critical, often overlooked, aspect in making such aggressive joint pruning viable is the strategic re-initialization and adjustment of remaining weights to improve the model post-pruning training accuracies. We introduce Pangu Light, a framework for LLM acceleration centered around structured pruning coupled with novel weight re-initialization techniques designed to address this ``missing piece''. Our framework systematically targets multiple axes, including model width, depth, attention heads, and RMSNorm, with its effectiveness rooted in novel re-initialization methods like Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) that mitigate performance drops by providing the network a better training starting point. Further enhancing efficiency, Pangu Light incorporates specialized optimizations such as absorbing Post-RMSNorm computations and tailors its strategies to Ascend NPU characteristics. The Pangu Light models consistently exhibit a superior accuracy-efficiency trade-off, outperforming prominent baseline pruning methods like Nemotron and established LLMs like Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and 2225 tokens/s.

</details>


### [256] [Exploring Generative Error Correction for Dysarthric Speech Recognition](https://arxiv.org/abs/2505.20163)

*Moreno La Quatra, Alkis Koudounas, Valerio Mario Salerno, Sabato Marco Siniscalchi*

**Main category:** cs.CL

**Keywords:** Dysarthric speech, Automatic Speech Recognition, Generative error correction, Speech Accessibility Project, Large Language Models

**Relevance Score:** 9

**TL;DR:** The paper proposes a two-stage framework to improve transcription accuracy of dysarthric speech using advanced speech recognition models and LLM-based generative error correction.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant challenges of accurately transcribing dysarthric speech with state-of-the-art ASR technologies.

**Method:** A two-stage framework combining speech recognition models and LLM-based generative error correction, with various configurations tested.

**Key Contributions:**

	1. Proposed a novel two-stage framework for dysarthric speech transcription.
	2. Incorporated generative error correction using LLMs to improve accuracy.
	3. Provided insights into model configurations and their impact on transcription accuracy.

**Result:** Experiments show the effectiveness of the framework on structured and spontaneous dysarthric speech, while revealing difficulties in single-word recognition.

**Limitations:** Challenges remain in accurately recognizing single words within dysarthric speech.

**Conclusion:** The analysis provides insights into how acoustic and linguistic modeling complement each other for better recognition of dysarthric speech.

**Abstract:** Despite the remarkable progress in end-to-end Automatic Speech Recognition (ASR) engines, accurately transcribing dysarthric speech remains a major challenge. In this work, we proposed a two-stage framework for the Speech Accessibility Project Challenge at INTERSPEECH 2025, which combines cutting-edge speech recognition models with LLM-based generative error correction (GER). We assess different configurations of model scales and training strategies, incorporating specific hypothesis selection to improve transcription accuracy. Experiments on the Speech Accessibility Project dataset demonstrate the strength of our approach on structured and spontaneous speech, while highlighting challenges in single-word recognition. Through comprehensive analysis, we provide insights into the complementary roles of acoustic and linguistic modeling in dysarthric speech recognition

</details>


### [257] [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)

*Dairu Liu, Ziyue Wang, Minyuan Ruan, Fuwen Luo, Chi Chen, Peng Li, Yang Liu*

**Main category:** cs.CL

**Keywords:** Visual Abstract Thinking, Multimodal Large Language Models, Visual Reasoning, Human Cognition, Cognitive Strategies

**Relevance Score:** 8

**TL;DR:** The paper introduces Visual Abstract Thinking (VAT), a new paradigm for enhancing multimodal reasoning in MLLMs by reducing redundant visual information and focusing on essential elements, resulting in improved performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of multimodal reasoning by addressing redundancy in visual information and leveraging abstract thinking, analogous to human cognitive strategies.

**Method:** The authors propose Visual Abstract Thinking (VAT) as a thinking paradigm for multimodal large language models, which prompts reasoning based on concise visual abstracts rather than verbose verbal thoughts.

**Key Contributions:**

	1. Introduction of Visual Abstract Thinking (VAT) as a novel reasoning paradigm for MLLMs.
	2. Demonstrated consistent performance improvements in visual reasoning tasks using VAT.
	3. Compatibility of VAT with existing methodologies like chain-of-thought in knowledge-intensive tasks.

**Result:** Experiments show that VAT leads to an average performance gain of 17% over the GPT-4o baseline across various reasoning tasks, thus enhancing models' visual reasoning abilities in conceptual, structural, and relational tasks.

**Limitations:** 

**Conclusion:** VAT is demonstrated to improve visual reasoning in MLLMs and is found compatible with chain-of-thought methodologies in complex reasoning tasks, urging further investigation into diverse reasoning paradigms inspired by human cognition.

**Abstract:** Images usually convey richer detail than text, but often include redundant information which potentially downgrades multimodal reasoning performance. When faced with lengthy or complex messages, humans tend to employ abstract thinking to convert them into simple and concise abstracts. Inspired by this cognitive strategy, we introduce Visual Abstract Thinking (VAT), a novel thinking paradigm that prompts Multimodal Large Language Models (MLLMs) with visual abstract instead of explicit verbal thoughts or elaborate guidance, permitting a more concentrated visual reasoning mechanism. Explicit thinking, such as Chain-of-thought (CoT) or tool-augmented approaches, increases the complexity of reasoning process via inserting verbose intermediate steps, external knowledge or visual information. In contrast, VAT reduces redundant visual information and encourages models to focus their reasoning on more essential visual elements. Experimental results show that VAT consistently empowers different models, and achieves an average gain of 17% over GPT-4o baseline by employing diverse types of visual abstracts, demonstrating that VAT can enhance visual reasoning abilities for MLLMs regarding conceptual, structural and relational reasoning tasks. VAT is also compatible with CoT in knowledge-intensive multimodal reasoning tasks. These findings highlight the effectiveness of visual reasoning via abstract thinking and encourage further exploration of more diverse reasoning paradigms from the perspective of human cognition.

</details>


### [258] ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)

*Alkis Koudounas, Moreno La Quatra, Eliana Pastor, Sabato Marco Siniscalchi, Elena Baralis*

**Main category:** cs.CL

**Keywords:** Kolmogorov-Arnold Networks, Spoken Language Understanding, Deep Learning

**Relevance Score:** 7

**TL;DR:** This paper investigates Kolmogorov-Arnold Networks (KANs) for Spoken Language Understanding (SLU) tasks, demonstrating their effectiveness in speech processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The application of KANs to speech processing is underexplored despite their potential as alternatives to traditional neural architectures.

**Method:** We experimented with 2D-CNN models integrating KAN layers in five configurations within the dense block and evaluated their performance against transformer-based models on five SLU datasets of varying complexity.

**Key Contributions:**

	1. First investigation of KANs in Spoken Language Understanding
	2. Found that KAN layers can replace linear layers in transformers
	3. Provided insights into differing attentiveness of KAN and linear layers on raw waveforms.

**Result:** The best-performing configuration placed a KAN layer between two linear layers, achieving comparable or superior performance compared to traditional linear layers in most evaluations.

**Limitations:** 

**Conclusion:** KAN layers can effectively substitute linear layers in transformer architectures for SLU tasks, offering new insights into input region attentiveness in waveforms.

**Abstract:** Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising alternative to traditional neural architectures, yet their application to speech processing remains under explored. This work presents the first investigation of KANs for Spoken Language Understanding (SLU) tasks. We experiment with 2D-CNN models on two datasets, integrating KAN layers in five different configurations within the dense block. The best-performing setup, which places a KAN layer between two linear layers, is directly applied to transformer-based models and evaluated on five SLU datasets with increasing complexity. Our results show that KAN layers can effectively replace the linear layers, achieving comparable or superior performance in most cases. Finally, we provide insights into how KAN and linear layers on top of transformers differently attend to input regions of the raw waveforms.

</details>


### [259] [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)

*Yongan Yu, Mengqian Wu, Yiran Lin, Nikki G. Lobczowski*

**Main category:** cs.CL

**Keywords:** higher-order thinking, LLM evaluation, Bloom's Taxonomy, feedback-driven, cognitive analysis

**Relevance Score:** 9

**TL;DR:** The paper introduces THiNK, a framework for evaluating higher-order thinking skills in LLMs, demonstrating that structured feedback improves their reasoning performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical need to assess higher-order thinking in LLMs beyond surface-level accuracy to enhance their reasoning capabilities.

**Method:** THiNK employs a multi-agent, feedback-driven approach based on Bloom's Taxonomy, framing reasoning assessment as an iterative task of problem generation, critique, and revision.

**Key Contributions:**

	1. Introduction of the THiNK framework for evaluating LLM reasoning
	2. Demonstration of the importance of structured feedback in improving higher-order thinking skills
	3. Provision of code and methodology for broader use in evaluating LLM performance

**Result:** The study reveals LLMs perform well in lower-order tasks but struggle with higher-order thinking, though structured feedback significantly enhances their performance in this area.

**Limitations:** The framework's effectiveness may vary with different model architectures and specific contexts not covered in this study.

**Conclusion:** THiNK provides a scalable methodology for evaluating and improving LLM reasoning, with qualitative evaluations illustrating better alignment with domain logic when using the framework.

**Abstract:** Assessing higher-order thinking skills in large language models (LLMs) remains a fundamental challenge, especially in tasks that go beyond surface-level accuracy. In this work, we propose THiNK (Testing Higher-order Notion of Knowledge), a multi-agent, feedback-driven evaluation framework grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative task of problem generation, critique, and revision, encouraging LLMs to think-aloud through step-by-step reflection and refinement. This enables a systematic evaluation of both lower-order (e.g., remember, understand) and higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven state-of-the-art LLMs and perform a detailed cognitive analysis of their outputs. Results reveal that while models reliably perform lower-order categories well, they struggle with applying knowledge in realistic contexts and exhibit limited abstraction. Structured feedback loops significantly improve reasoning performance, particularly in higher-order thinking. Qualitative evaluations further confirm that THiNK-guided outputs better align with domain logic and problem structure. The code of our framework provides a scalable methodology for probing and enhancing LLM reasoning, offering new directions for evaluation grounded in learning science, which is available at our GitHub repository.

</details>


### [260] [Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning](https://arxiv.org/abs/2505.20195)

*Xiaorong Wang, Ting Yang, Zhu Zhang, Shuo Wang, Zihan Zhou, Liner Yang, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** long-form text evaluation, active learning, human feedback, LLM, coherence

**Relevance Score:** 8

**TL;DR:** Proposes a divide-and-conquer approach for evaluating long-form model-generated text, integrating human feedback and a novel active learning algorithm.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality assessment of long-form generated text, which suffers from degradation due to input length.

**Method:** A divide-and-conquer approach is used to break down evaluation tasks into localized and global assessments, combined with a hybrid in-context learning approach and an uncertainty-based active learning algorithm.

**Key Contributions:**

	1. Introduction of a localized scoring method for coherent evaluations of text segments.
	2. Development of a hybrid in-context learning approach leveraging human annotations.
	3. Implementation of an uncertainty-based active learning algorithm to optimize data annotation.

**Result:** The framework outperforms several representative baselines in evaluating long-form generated text.

**Limitations:** 

**Conclusion:** Integration of human feedback into the evaluation process significantly enhances model performance in text assessment.

**Abstract:** Assessing the quality of long-form, model-generated text is challenging, even with advanced LLM-as-a-Judge methods, due to performance degradation as input length increases. To address this issue, we propose a divide-and-conquer approach, which breaks down the comprehensive evaluation task into a series of localized scoring tasks, followed by a final global assessment. This strategy allows for more granular and manageable evaluations, ensuring that each segment of the text is assessed in isolation for both coherence and quality, while also accounting for the overall structure and consistency of the entire piece. Moreover, we introduce a hybrid in-context learning approach that leverages human annotations to enhance the performance of both local and global evaluations. By incorporating human-generated feedback directly into the evaluation process, this method allows the model to better align with human judgment. Finally, we develop an uncertainty-based active learning algorithm that efficiently selects data samples for human annotation, thereby reducing annotation costs in practical scenarios. Experimental results show that the proposed evaluation framework outperforms several representative baselines, highlighting the effectiveness of our approach.

</details>


### [261] [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199)

*Pengxiang Li, Shilin Yan, Joey Tsai, Renrui Zhang, Ruichuan An, Ziyu Guo, Xiaowei Gao*

**Main category:** cs.CL

**Keywords:** Adaptive Classifier-Free Guidance, Language Generation, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper presents Adaptive Classifier-Free Guidance (A-CFG), a method that enhances generative models by dynamically adjusting unconditional inputs based on model confidence, leading to improved language generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Standard Classifier-Free Guidance (CFG) uses a static unconditional input, which is not always optimal during iterative generation when model uncertainty changes.

**Method:** A-CFG modifies unconditional input by re-masking low-confidence tokens at each generation step, providing dynamic guidance that enhances the effectiveness of CFG during language generation.

**Key Contributions:**

	1. Introduction of A-CFG for dynamic adjustment of guidance
	2. Integration with a state-of-the-art masked diffusion language model
	3. Empirical validation through substantial benchmark improvements

**Result:** A-CFG shows significant improvements in language generation tasks, achieving a 3.9 point gain on the GPQA benchmark compared to standard CFG.

**Limitations:** 

**Conclusion:** The paper demonstrates the advantages of adapting guidance mechanisms in generative models to align with the model's immediate uncertainty levels, resulting in better performance.

**Abstract:** Classifier-Free Guidance (CFG) significantly enhances controllability in generative models by interpolating conditional and unconditional predictions. However, standard CFG often employs a static unconditional input, which can be suboptimal for iterative generation processes where model uncertainty varies dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel method that tailors the unconditional input by leveraging the model's instantaneous predictive confidence. At each step of an iterative (masked) diffusion language model, A-CFG identifies tokens in the currently generated sequence for which the model exhibits low confidence. These tokens are temporarily re-masked to create a dynamic, localized unconditional input. This focuses CFG's corrective influence precisely on areas of ambiguity, leading to more effective guidance. We integrate A-CFG into a state-of-the-art masked diffusion language model and demonstrate its efficacy. Experiments on diverse language generation benchmarks show that A-CFG yields substantial improvements over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation.

</details>


### [262] [Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations](https://arxiv.org/abs/2505.20201)

*Mohit Chandra, Siddharth Sriraman, Harneet Singh Khanuja, Yiqiao Jin, Munmun De Choudhury*

**Main category:** cs.CL

**Keywords:** mental health, large language models, synthetic data generation, multi-turn conversations, evaluation framework

**Relevance Score:** 9

**TL;DR:** Introduction of MedAgent framework and MHSD dataset to evaluate LLMs in mental health dialogue.

**Read time:** 33 min

<details>
  <summary>Details</summary>

**Motivation:** To address limited access to mental healthcare and the need for effective communication between LLMs and patients.

**Method:** Development of MedAgent for generating synthetic multi-turn conversations and MultiSenseEval for evaluation based on human-centric criteria.

**Key Contributions:**

	1. Introduction of MedAgent for generating realistic mental health conversations.
	2. Creation of the MHSD dataset with 2,200 conversations.
	3. Development of MultiSenseEval for evaluating LLMs using human-centric criteria.

**Result:** Created the MHSD dataset with 2,200 patient-LLM conversations; findings show that current models perform poorly in patient-centric communication.

**Limitations:** Current evaluation frameworks may overlook alignment with patient-specific goals and variance in patient personas.

**Conclusion:** The work provides tools for generating and evaluating LLMs in mental health conversations, highlighting performance gaps in current models.

**Abstract:** Limited access to mental healthcare, extended wait times, and increasing capabilities of Large Language Models (LLMs) has led individuals to turn to LLMs for fulfilling their mental health needs. However, examining the multi-turn mental health conversation capabilities of LLMs remains under-explored. Existing evaluation frameworks typically focus on diagnostic accuracy and win-rates and often overlook alignment with patient-specific goals, values, and personalities required for meaningful conversations. To address this, we introduce MedAgent, a novel framework for synthetically generating realistic, multi-turn mental health sensemaking conversations and use it to create the Mental Health Sensemaking Dialogue (MHSD) dataset, comprising over 2,200 patient-LLM conversations. Additionally, we present MultiSenseEval, a holistic framework to evaluate the multi-turn conversation abilities of LLMs in healthcare settings using human-centric criteria. Our findings reveal that frontier reasoning models yield below-par performance for patient-centric communication and struggle at advanced diagnostic capabilities with average score of 31%. Additionally, we observed variation in model performance based on patient's persona and performance drop with increasing turns in the conversation. Our work provides a comprehensive synthetic data generation framework, a dataset and evaluation framework for assessing LLMs in multi-turn mental health conversations.

</details>


### [263] [How to Improve the Robustness of Closed-Source Models on NLI](https://arxiv.org/abs/2505.20209)

*Joe Stacey, Lisa Alazraki, Aran Ubhi, Beyza Ermis, Aaron Mueller, Marek Rei*

**Main category:** cs.CL

**Keywords:** Large Language Models, Robustness, Out-of-Distribution, Data-Centric Methods, Closed-source Models

**Relevance Score:** 8

**TL;DR:** The paper evaluates methods to enhance the robustness of closed-source Large Language Models (LLMs) against out-of-distribution data using data-centric strategies, revealing that the effectiveness varies with the complexity of the OOD datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Closed-source LLMs have achieved remarkable performance, but their robustness to out-of-distribution data is often compromised due to specific training heuristics.

**Method:** The study investigates data-centric methods that do not require access to model internals, analyzing upsampling of training examples and substitution with LLM-generated samples based on the OOD complexity.

**Key Contributions:**

	1. Identified effective strategies to improve robustness of closed-source LLMs without requiring model access.
	2. Demonstrated varying effectiveness of data-centric methods based on OOD dataset complexity.
	3. Highlighted the superior robustness of autoregressive LLMs compared to encoder models.

**Result:** For complex OOD datasets, upsampling challenging examples improves robustness by up to 1.5%. For less complex datasets, replacing some training data with LLM-generated examples increases robustness by 3.7%.

**Limitations:** 

**Conclusion:** Large closed-source autoregressive LLMs exhibit greater robustness compared to traditional encoder models, suggesting they should be the benchmark for future research.

**Abstract:** Closed-source Large Language Models (LLMs) have become increasingly popular, with impressive performance across a wide range of natural language tasks. These models can be fine-tuned to further improve performance, but this often results in the models learning from dataset-specific heuristics that reduce their robustness on out-of-distribution (OOD) data. Existing methods to improve robustness either perform poorly, or are non-applicable to closed-source models because they assume access to model internals, or the ability to change the model's training procedure. In this work, we investigate strategies to improve the robustness of closed-source LLMs through data-centric methods that do not require access to model internals. We find that the optimal strategy depends on the complexity of the OOD data. For highly complex OOD datasets, upsampling more challenging training examples can improve robustness by up to 1.5%. For less complex OOD datasets, replacing a portion of the training set with LLM-generated examples can improve robustness by 3.7%. More broadly, we find that large-scale closed-source autoregressive LLMs are substantially more robust than commonly used encoder models, and are a more appropriate choice of baseline going forward.

</details>


### [264] [Dependency Parsing is More Parameter-Efficient with Normalization](https://arxiv.org/abs/2505.20215)

*Paolo Gajo, Domenic Rosati, Hassan Sajjad, Alberto Barrón-Cedeño*

**Main category:** cs.CL

**Keywords:** dependency parsing, biaffine scoring, score normalization, BiLSTM, natural language processing

**Relevance Score:** 4

**TL;DR:** This paper addresses the efficiency of biaffine scoring in dependency parsing by introducing score normalization, demonstrating its effectiveness in improving parser models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore and improve the biaffine scoring mechanism used in dependency parsing, particularly focusing on the lack of normalization that leads to overparameterized models.

**Method:** Empirical experiments were conducted using N-layer stacked BiLSTMs and assessed the parser's performance on six datasets for semantic and syntactic dependency parsing with and without score normalization.

**Key Contributions:**

	1. Theoretical evidence supporting the need for normalization in biaffine scoring.
	2. Empirical results demonstrating improved parser performance with normalization.
	3. Achievement of state-of-the-art results on multiple datasets with reduced parameters.

**Result:** Normalizing biaffine scores resulted in improved performance, surpassing the state of the art on two datasets while requiring fewer samples and parameters.

**Limitations:** 

**Conclusion:** The findings suggest that incorporating score normalization can lead to more efficient dependency parsing models and improved accuracy.

**Abstract:** Dependency parsing is the task of inferring natural language structure, often approached by modeling word interactions via attention through biaffine scoring. This mechanism works like self-attention in Transformers, where scores are calculated for every pair of words in a sentence. However, unlike Transformer attention, biaffine scoring does not use normalization prior to taking the softmax of the scores. In this paper, we provide theoretical evidence and empirical results revealing that a lack of normalization necessarily results in overparameterized parser models, where the extra parameters compensate for the sharp softmax outputs produced by high variance inputs to the biaffine scoring function. We argue that biaffine scoring can be made substantially more efficient by performing score normalization. We conduct experiments on six datasets for semantic and syntactic dependency parsing using a one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's performance with and without normalizing biaffine scores. Normalizing allows us to beat the state of the art on two datasets, with fewer samples and trainable parameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1

</details>


### [265] [FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models](https://arxiv.org/abs/2505.20225)

*Hao Kang, Zichun Yu, Chenyan Xiong*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, Large Language Models, Open-Source Research

**Relevance Score:** 8

**TL;DR:** FLAME-MoE is an open-source research suite featuring efficient Mixture-of-Experts architectures for large language models, improving accuracy and enabling reproducible experimentation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an open, end-to-end Mixture-of-Experts platform for academic researchers to study various aspects of scaling and performance in large language models.

**Method:** The research suite includes seven decoder-only models with varying sizes, using a MoE architecture of 64 experts with top-8 gating and shared experts for efficient token processing.

**Key Contributions:**

	1. Release of FLAME-MoE open-source suite
	2. Demonstration of improved accuracy over dense models
	3. Insights into expert specialization and routing behavior

**Result:** FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines, demonstrating effective expert specialization and routing stability during training.

**Limitations:** 

**Conclusion:** The open-source release of FLAME-MoE facilitates reproducible research and exploration of MoE dynamics in modern large language models.

**Abstract:** Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4 increasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong efficiency-performance trade-offs by activating only a fraction of the model per token. Yet academic researchers still lack a fully open, end-to-end MoE platform for investigating scaling, routing, and expert behavior. We release FLAME-MoE, a completely open-source research suite composed of seven decoder-only models, ranging from 38M to 1.7B active parameters, whose architecture--64 experts with top-8 gating and 2 shared experts--closely reflects modern production LLMs. All training data pipelines, scripts, logs, and checkpoints are publicly available to enable reproducible experimentation. Across six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines trained with identical FLOPs. Leveraging full training trace transparency, we present initial analyses showing that (i) experts increasingly specialize on distinct token subsets, (ii) co-activation matrices remain sparse, reflecting diverse expert usage, and (iii) routing behavior stabilizes early in training. All code, training logs, and model checkpoints are available at https://github.com/cmu-flame/FLAME-MoE.

</details>


### [266] [Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue](https://arxiv.org/abs/2505.20231)

*Yiming Du, Bingbing Wang, Yang He, Bin Liang, Baojun Wang, Zhongyang Li, Lin Gui, Jeff Z. Pan, Ruifeng Xu, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** Task-Oriented Dialogue, Multi-session, Long-term memory, Dialogue efficiency, Memory-Active Policy

**Relevance Score:** 7

**TL;DR:** Introduces a multi-session task-oriented dialogue dataset and a Memory-Active Policy to enhance dialogue efficiency and task completion.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing task-oriented dialogue systems struggle with multi-session dialogues and long-term memory retention, reducing efficiency in task completion.

**Method:** The paper proposes a new MS-TOD dataset specifically for multi-session dialogues and introduces a Memory-Active Policy (MAP) that includes a Memory-Guided Dialogue Planning and a Proactive Response Strategy.

**Key Contributions:**

	1. Introduction of the first multi-session TOD dataset (MS-TOD) for evaluating long-term memory
	2. Development of the Memory-Active Policy (MAP) for enhanced dialogue efficiency
	3. Demonstration of improved task success and efficiency in multi-session dialogues.

**Result:** The experiments show that MAP improves task success rates and dialogue turn efficiency in multi-session scenarios while performing competitively in single-session tasks.

**Limitations:** 

**Conclusion:** MAP represents a significant advancement in task-oriented dialogues by efficiently handling multi-session conversations, which is essential for applications requiring long-term interactions.

**Abstract:** Existing Task-Oriented Dialogue (TOD) systems primarily focus on single-session dialogues, limiting their effectiveness in long-term memory augmentation. To address this challenge, we introduce a MS-TOD dataset, the first multi-session TOD dataset designed to retain long-term memory across sessions, enabling fewer turns and more efficient task completion. This defines a new benchmark task for evaluating long-term memory in multi-session TOD. Based on this new dataset, we propose a Memory-Active Policy (MAP) that improves multi-session dialogue efficiency through a two-stage approach. 1) Memory-Guided Dialogue Planning retrieves intent-aligned history, identifies key QA units via a memory judger, refines them by removing redundant questions, and generates responses based on the reconstructed memory. 2) Proactive Response Strategy detects and correct errors or omissions, ensuring efficient and accurate task completion. We evaluate MAP on MS-TOD dataset, focusing on response quality and effectiveness of the proactive strategy. Experiments on MS-TOD demonstrate that MAP significantly improves task success and turn efficiency in multi-session scenarios, while maintaining competitive performance on conventional single-session tasks.

</details>


### [267] [Efficient Speech Translation through Model Compression and Knowledge Distillation](https://arxiv.org/abs/2505.20237)

*Yasmin Moslem*

**Main category:** cs.CL

**Keywords:** speech translation, model compression, audio-language models, quantization, knowledge distillation

**Relevance Score:** 6

**TL;DR:** This paper discusses methods for efficiently deploying large audio-language models for speech translation, specifically at the IWSLT 2025 conference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the significant computational requirements of deploying large audio-language models for effective speech translation.

**Method:** The authors use a combination of iterative layer pruning based on layer importance evaluation, low-rank adaptation with 4-bit quantization (QLoRA), and knowledge distillation.

**Key Contributions:**

	1. Introduction of layer pruning based on importance evaluation
	2. Application of 4-bit quantization (QLoRA)
	3. Demonstration of knowledge distillation in speech translation models

**Result:** The pruned models achieve up to a 50% reduction in both model parameters and storage footprint, while retaining 97-100% of the translation quality of the original models.

**Limitations:** 

**Conclusion:** The methods explored can significantly improve the efficiency of large audio-language models without compromising on translation quality.

**Abstract:** Efficient deployment of large audio-language models for speech translation remains challenging due to their significant computational requirements. In this paper, we address this challenge through our system submissions to the "Model Compression" track at the International Conference on Spoken Language Translation (IWSLT 2025). We experiment with a combination of approaches including iterative layer pruning based on layer importance evaluation, low-rank adaptation with 4-bit quantization (QLoRA), and knowledge distillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech translation into German and Chinese. Our pruned (student) models achieve up to a 50% reduction in both model parameters and storage footprint, while retaining 97-100% of the translation quality of the in-domain (teacher) models.

</details>


### [268] [It's High Time: A Survey of Temporal Information Retrieval and Question Answering](https://arxiv.org/abs/2505.20243)

*Bhawna Piryani, Abdelrahman Abdullah, Jamshid Mozafari, Avishek Anand, Adam Jatowt*

**Main category:** cs.CL

**Keywords:** Temporal Information Retrieval, Temporal Question Answering, Large Language Models, Temporal Language Modeling, Event Ordering

**Relevance Score:** 8

**TL;DR:** This paper surveys Temporal Information Retrieval and Temporal Question Answering, addressing challenges in handling time-sensitive information.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing amount of time-stamped content necessitates improved methods for understanding temporal information, crucial for various domains.

**Method:** The paper reviews traditional and modern methods, including neural approaches and LLMs, focusing on challenges like temporal intent detection, time expression normalization, and event ordering.

**Key Contributions:**

	1. Comprehensive overview of Temporal Information Retrieval and Question Answering
	2. Discussion of traditional and modern approaches, including LLMs
	3. Identification of key challenges and evaluation strategies for temporal systems

**Result:** It highlights advancements in temporal language modeling and retrieval-augmented generation, as well as evaluation strategies for assessing temporal robustness and recency awareness.

**Limitations:** 

**Conclusion:** The survey presents a comprehensive understanding of the current landscape in temporal information handling and suggests directions for future research.

**Abstract:** Time plays a critical role in how information is generated, retrieved, and interpreted. In this survey, we provide a comprehensive overview of Temporal Information Retrieval and Temporal Question Answering, two research areas aimed at handling and understanding time-sensitive information. As the amount of time-stamped content from sources like news articles, web archives, and knowledge bases increases, systems must address challenges such as detecting temporal intent, normalizing time expressions, ordering events, and reasoning over evolving or ambiguous facts. These challenges are critical across many dynamic and time-sensitive domains, from news and encyclopedias to science, history, and social media. We review both traditional approaches and modern neural methods, including those that use transformer models and Large Language Models (LLMs). We also review recent advances in temporal language modeling, multi-hop reasoning, and retrieval-augmented generation (RAG), alongside benchmark datasets and evaluation strategies that test temporal robustness, recency awareness, and generalization.

</details>


### [269] [KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing](https://arxiv.org/abs/2505.20245)

*Rui Li, Quanyu Dai, Zeyu Zhang, Xu Chen, Zhenhua Dong, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, multi-hop reasoning, knowledge graphs, self-bootstrapping

**Relevance Score:** 9

**TL;DR:** KnowTrace is a RAG framework that reduces context overload for multi-hop reasoning in LLMs by organizing retrieved information into knowledge graphs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenge of context overload in LLMs during multi-hop reasoning tasks and improve the quality of reasoning processes.

**Method:** KnowTrace organizes retrieved information into specific knowledge graphs, tracing desired knowledge triplets relevant to the input question to create an intelligible context for inference.

**Key Contributions:**

	1. Development of an effective RAG framework to alleviate context overload
	2. Introduction of knowledge backtracing to improve reasoning quality
	3. Demonstration of superior performance across multiple benchmarks

**Result:** KnowTrace consistently outperforms existing multi-hop question answering methods across three benchmarks, with significant improvements observed in a bootstrapped version.

**Limitations:** 

**Conclusion:** The proposed method not only enhances reasoning capabilities but also introduces a mechanism for self-bootstrapping using process supervision data from LLM generations.

**Abstract:** Recent advances in retrieval-augmented generation (RAG) furnish large language models (LLMs) with iterative retrievals of relevant information to handle complex multi-hop questions. These methods typically alternate between LLM reasoning and retrieval to accumulate external information into the LLM's context. However, the ever-growing context inherently imposes an increasing burden on the LLM to perceive connections among critical information pieces, with futile reasoning steps further exacerbating this overload issue. In this paper, we present KnowTrace, an elegant RAG framework to (1) mitigate the context overload and (2) bootstrap higher-quality multi-step reasoning. Instead of simply piling the retrieved contents, KnowTrace autonomously traces out desired knowledge triplets to organize a specific knowledge graph relevant to the input question. Such a structured workflow not only empowers the LLM with an intelligible context for inference, but also naturally inspires a reflective mechanism of knowledge backtracing to identify contributive LLM generations as process supervision data for self-bootstrapping. Extensive experiments show that KnowTrace consistently surpasses existing methods across three multi-hop question answering benchmarks, and the bootstrapped version further amplifies the gains.

</details>


### [270] [WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2505.20249)

*Yongan Yu, Qingchen Hu, Xianda Du, Jiayin Wang, Fengran Mo, Renee Sieber*

**Main category:** cs.CL

**Keywords:** climate change, large language models, disruptive weather, benchmark, dataset

**Relevance Score:** 5

**TL;DR:** This study develops a dataset on disruptive weather impacts and introduces WXImpactBench, a benchmark for evaluating large language models on this topic.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding of the societal impacts of climate change and evaluate the effectiveness of LLMs in this domain.

**Method:** A four-stage construction pipeline was used to develop the disruptive weather impact dataset, followed by the creation of WXImpactBench with multi-label classification and ranking-based question answering tasks.

**Key Contributions:**

	1. Development of a disruptive weather impact dataset
	2. Introduction of WXImpactBench for LLM evaluation
	3. First-hand analysis of LLM challenges in climate adaptation understanding

**Result:** Extensive experiments were conducted with various LLMs to analyze the challenges of understanding disruptive weather impacts and adapting to climate change.

**Limitations:** 

**Conclusion:** The dataset and evaluation framework aim to assist in protecting communities against vulnerabilities from climate-related disasters.

**Abstract:** Climate change adaptation requires the understanding of disruptive weather impacts on society, where large language models (LLMs) might be applicable. However, their effectiveness is under-explored due to the difficulty of high-quality corpus collection and the lack of available benchmarks. The climate-related events stored in regional newspapers record how communities adapted and recovered from disasters. However, the processing of the original corpus is non-trivial. In this study, we first develop a disruptive weather impact dataset with a four-stage well-crafted construction pipeline. Then, we propose WXImpactBench, the first benchmark for evaluating the capacity of LLMs on disruptive weather impacts. The benchmark involves two evaluation tasks, multi-label classification and ranking-based question answering. Extensive experiments on evaluating a set of LLMs provide first-hand analysis of the challenges in developing disruptive weather impact understanding and climate change adaptation systems. The constructed dataset and the code for the evaluation framework are available to help society protect against vulnerabilities from disasters.

</details>


### [271] [ARM: Adaptive Reasoning Model](https://arxiv.org/abs/2505.20258)

*Siye Wu, Jian Xie, Yikai Zhang, Aili Chen, Kai Zhang, Yu Su, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** Adaptive Reasoning, Token Efficiency, Machine Learning, Human-Computer Interaction, AI

**Relevance Score:** 8

**TL;DR:** The paper presents the Adaptive Reasoning Model (ARM), which dynamically selects reasoning formats based on task difficulty to improve token efficiency and reduce reasoning excess.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the issue of excessive reasoning in large models and achieve more autonomous AI without human intervention.

**Method:** ARM utilizes a method called Ada-GRPO, an adaptation of Group Relative Policy Optimization, to train the model while addressing the issue of format collapse.

**Key Contributions:**

	1. Introduction of the Adaptive Reasoning Model (ARM) for dynamic reasoning format selection.
	2. Development of Ada-GRPO for efficient training and token usage.
	3. Demonstration of performance improvement without the need for excessive reasoning.

**Result:** ARM achieves a reduction in token usage by an average of 30%, up to 70%, while performing comparably to the traditional Long CoT format and improves training speed by 2x.

**Limitations:** The work is still in progress and may have unresolved issues as indicated in the arXiv comment.

**Conclusion:** ARM demonstrates a significant improvement in inference efficiency and training speed, providing flexible reasoning modes for various task requirements.

**Abstract:** While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the "overthinking" problem -- excessive and unnecessary reasoning -- which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones -- Direct Answer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of 30%, and up to 70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a 2x speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens -- ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage.

</details>


### [272] [We Need to Measure Data Diversity in NLP -- Better and Broader](https://arxiv.org/abs/2505.20264)

*Dong Nguyen, Esther Ploeger*

**Main category:** cs.CL

**Keywords:** NLP, data diversity, measurement challenges, interdisciplinary approaches, dataset validation

**Relevance Score:** 6

**TL;DR:** This paper discusses the challenges and necessity of measuring diversity in NLP datasets, emphasizing interdisciplinary approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper highlights the increasing attention towards diversity in NLP datasets and identifies gaps in measurement methodologies.

**Method:** It outlines conceptual and methodological challenges in assessing data diversity, advocating for interdisciplinary approaches to enhance measurement validity.

**Key Contributions:**

	1. Identifies fundamental challenges in measuring NLP dataset diversity
	2. Proposes the need for interdisciplinary perspectives in developing measurement strategies
	3. Highlights the importance of fine-grained and valid diversity measures.

**Result:** The examination reveals significant hurdles in defining and quantifying diversity, suggesting that interdisciplinary collaboration could yield more effective measures.

**Limitations:** The paper primarily discusses challenges and theoretical perspectives without proposing concrete measurement frameworks.

**Conclusion:** The authors conclude that addressing these challenges is crucial for improving the robustness of NLP datasets and their applications.

**Abstract:** Although diversity in NLP datasets has received growing attention, the question of how to measure it remains largely underexplored. This opinion paper examines the conceptual and methodological challenges of measuring data diversity and argues that interdisciplinary perspectives are essential for developing more fine-grained and valid measures.

</details>


### [273] [Does quantization affect models' performance on long-context tasks?](https://arxiv.org/abs/2505.20276)

*Anmol Mekala, Anirudh Atmakuru, Yixiao Song, Marzena Karpinska, Mohit Iyyer*

**Main category:** cs.CL

**Keywords:** large language models, quantization, long-context inputs, long-form outputs, performance evaluation

**Relevance Score:** 7

**TL;DR:** This paper systematically evaluates the impact of quantization methods on large language models (LLMs) for tasks requiring long inputs and outputs.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high memory requirements and inference latency of LLMs with large context windows, this work investigates the trade-offs of quantization techniques.

**Method:** The study conducts a systematic evaluation using 9.7K test examples across five quantization methods and five language models to assess performance degradation.

**Key Contributions:**

	1. First systematic evaluation of quantized LLMs on long-input tasks.
	2. Comparison across different quantization methods and models.
	3. Insights on performance degradation based on input language and context length.

**Result:** 8-bit quantization shows minimal accuracy loss (~0.8%), while 4-bit quantization methods result in significant performance drops, particularly in tasks with long contexts and non-English input.

**Limitations:** Performance degradation varies by quantization method and specific tasks, necessitating individualized assessments for different applications.

**Conclusion:** Careful and task-specific evaluation of quantized LLMs is crucial before deployment, especially in long-context tasks and for languages beyond English.

**Abstract:** Large language models (LLMs) now support context windows exceeding 128K tokens, but this comes with significant memory requirements and high inference latency. Quantization can mitigate these costs, but may degrade performance. In this work, we present the first systematic evaluation of quantized LLMs on tasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation spans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4, GPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B, and 72B). We find that, on average, 8-bit quantization preserves accuracy (~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for tasks involving long context inputs (drops of up to 59%). This degradation tends to worsen when the input is in a language other than English. Crucially, the effects of quantization depend heavily on the quantization method, model, and task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4, Llama-3.1 70B experiences a 32% performance drop on the same task. These findings highlight the importance of a careful, task-specific evaluation before deploying quantized LLMs, particularly in long-context scenarios and with languages other than English.

</details>


### [274] [OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction](https://arxiv.org/abs/2505.20277)

*Haonan Zhang, Run Luo, Xiong Liu, Yuchuan Wu, Ting-En Lin, Pengpeng Zeng, Qiang Qu, Feiteng Fang, Min Yang, Lianli Gao, Jingkuan Song, Fei Huang, Yongbin Li*

**Main category:** cs.CL

**Keywords:** Role-Playing Agents, Large Language Models, Speech-Language Interaction

**Relevance Score:** 8

**TL;DR:** OmniCharacter introduces a model for immersive role-playing agents that combine speech and language with personality and vocal traits, overcoming limitations of current dialogue-only systems.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the immersive experience of role-playing agents by incorporating vocal traits alongside textual dialogue.

**Method:** OmniCharacter is developed to exhibit consistent role-specific personality and vocal traits, achieving low latency interactions through a specially constructed dataset (OmniCharacter-10K) that includes diverse characters and multi-round dialogue.

**Key Contributions:**

	1. Development of OmniCharacter model for immersive RPAs
	2. Creation of the OmniCharacter-10K dataset
	3. Lower latency for responses compared to traditional models

**Result:** Experimental results indicate that OmniCharacter provides superior responses in content and style compared to existing RPAs, with a response latency of as low as 289ms.

**Limitations:** 

**Conclusion:** OmniCharacter demonstrates significant improvements in both the quality of interactions and the speed of responses for role-playing agents.

**Abstract:** Role-Playing Agents (RPAs), benefiting from large language models, is an emerging interactive AI system that simulates roles or characters with diverse personalities. However, existing methods primarily focus on mimicking dialogues among roles in textual form, neglecting the role's voice traits (e.g., voice style and emotions) as playing a crucial effect in interaction, which tends to be more immersive experiences in realistic scenarios. Towards this goal, we propose OmniCharacter, a first seamless speech-language personality interaction model to achieve immersive RPAs with low latency. Specifically, OmniCharacter enables agents to consistently exhibit role-specific personality traits and vocal traits throughout the interaction, enabling a mixture of speech and language responses. To align the model with speech-language scenarios, we construct a dataset named OmniCharacter-10K, which involves more distinctive characters (20), richly contextualized multi-round dialogue (10K), and dynamic speech response (135K). Experimental results showcase that our method yields better responses in terms of both content and style compared to existing RPAs and mainstream speech-language models, with a response latency as low as 289ms. Code and dataset are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.

</details>


### [275] [One-shot Entropy Minimization](https://arxiv.org/abs/2505.20282)

*Zitian Gao, Lynx Chen, Joey Zhou, Bryan Dai*

**Main category:** cs.CL

**Keywords:** large language models, entropy minimization, unlabeled data

**Relevance Score:** 7

**TL;DR:** Training large language models using entropy minimization with minimal data can achieve significant performance improvements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore efficient training methods for large language models that rely less on extensive labeled datasets.

**Method:** Trained 13,440 large language models using a single unlabeled data point and 10 optimization steps.

**Key Contributions:**

	1. Utilization of a single unlabeled sample for training large language models.
	2. Demonstration of entropy minimization effectiveness in model performance.
	3. Proposal of a simpler training paradigm that challenges existing reinforcement learning approaches.

**Result:** Achieved performance improvements comparable to traditional methods that use extensive data and carefully designed rewards in reinforcement learning.

**Limitations:** Work in progress; results may need further validation and refinement.

**Conclusion:** This work suggests a potential shift in how we approach post-training paradigms for large language models.

**Abstract:** We trained 13,440 large language models and found that entropy minimization requires only a single unlabeled data and 10 steps optimization to achieve performance improvements comparable to or even greater than those obtained using thousands of data and carefully designed rewards in rule-based reinforcement learning. This striking result may prompt a rethinking of post-training paradigms for large language models. Our code is avaliable at https://github.com/zitian-gao/one-shot-em.

</details>


### [276] [MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability](https://arxiv.org/abs/2505.20285)

*Weiqi Wu, Xin Guan, Shen Huang, Yong Jiang, Pengjun Xie, Fei Huang, Jiuxin Cao, Hai Zhao, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Language Models, Machine Learning, Open-Domain Question Answering

**Relevance Score:** 9

**TL;DR:** MASKSEARCH introduces a novel pre-training framework that enhances LLM search agents' performance in retrieving knowledge through a specialized module and addresses limitations in existing training methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the agentic abilities of LLMs by enhancing their retrieval and reasoning capabilities through a new pre-training framework.

**Method:** The proposed framework includes a Retrieval Augmented Mask Prediction (RAMP) task during pre-training and utilizes both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training, incorporating a hybrid reward system and curriculum learning.

**Key Contributions:**

	1. Introduction of the RAMP task for universal retrieval learning.
	2. Combination of agent-based and distillation-based methods for SFT.
	3. Innovative curriculum learning approach to tackle complex training instances.

**Result:** The MASKSEARCH framework significantly improves the performance of LLM-based search agents on both in-domain and out-of-domain tasks, particularly in open-domain multi-hop question answering.

**Limitations:** 

**Conclusion:** Our experiments show that the MASKSEARCH framework provides a substantial enhancement in the universal search capabilities of retrieval-augmented LLMs.

**Abstract:** Retrieval-Augmented Language Models (RALMs) represent a classic paradigm where models enhance generative capabilities using external knowledge retrieved via a specialized module. Recent advancements in Agent techniques enable Large Language Models (LLMs) to autonomously utilize tools for retrieval, planning, and reasoning. While existing training-based methods show promise, their agentic abilities are limited by inherent characteristics of the task-specific data used during training. To further enhance the universal search capability of agents, we propose a novel pre-training framework, MASKSEARCH. In the pre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP) task, where the model learns to leverage search tools to fill masked spans on a large number of pre-training data, thus acquiring universal retrieval and reasoning capabilities for LLMs. After that, the model is trained on downstream tasks to achieve further improvement. We apply both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training. For SFT, we combine agent-based and distillation-based methods to generate training data, starting with a multi-agent system consisting of a planner, rewriter, observer, and followed by a self-evolving teacher model. While for RL, we employ DAPO as the training framework and adopt a hybrid reward system consisting of answer rewards and format rewards. Additionally, we introduce a curriculum learning approach that allows the model to learn progressively from easier to more challenging instances based on the number of masked spans. We evaluate the effectiveness of our framework in the scenario of open-domain multi-hop question answering. Through extensive experiments, we demonstrate that MASKSEARCH significantly enhances the performance of LLM-based search agents on both in-domain and out-of-domain downstream tasks.

</details>


### [277] [Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery](https://arxiv.org/abs/2505.20293)

*Yifan Sun, Danding Wang, Qiang Sheng, Juan Cao, Jintao Li*

**Main category:** cs.CL

**Keywords:** explainable AI, concept discovery, large language models

**Relevance Score:** 8

**TL;DR:** ECO-Concept is a novel framework that automatically discovers comprehensible concepts for explainable AI in the text domain by utilizing large language models for evaluation and model fine-tuning.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Existing concept-based explainable methods are limited in the text domain, often relying on predefined annotations or producing unintuitive explanations that reduce user trust.

**Method:** ECO-Concept employs an object-centric architecture to automatically extract semantic concepts, evaluates their comprehensibility using large language models, and fine-tunes the model based on evaluation results to enhance explanation clarity.

**Key Contributions:**

	1. Introduces ECO-Concept for automatic concept discovery without annotations
	2. Utilizes large language models to evaluate and improve concept comprehensibility
	3. Demonstrates superior performance in explainability across diverse tasks

**Result:** ECO-Concept outperforms existing methods across various tasks, demonstrating improved comprehensibility of the extracted concepts.

**Limitations:** 

**Conclusion:** The proposed framework addresses the gap in concept discovery and clarity in explainable AI, showing promise for better user understanding and trust.

**Abstract:** Concept-based explainable approaches have emerged as a promising method in explainable AI because they can interpret models in a way that aligns with human reasoning. However, their adaption in the text domain remains limited. Most existing methods rely on predefined concept annotations and cannot discover unseen concepts, while other methods that extract concepts without supervision often produce explanations that are not intuitively comprehensible to humans, potentially diminishing user trust. These methods fall short of discovering comprehensible concepts automatically. To address this issue, we propose \textbf{ECO-Concept}, an intrinsically interpretable framework to discover comprehensible concepts with no concept annotations. ECO-Concept first utilizes an object-centric architecture to extract semantic concepts automatically. Then the comprehensibility of the extracted concepts is evaluated by large language models. Finally, the evaluation result guides the subsequent model fine-tuning to obtain more understandable explanations. Experiments show that our method achieves superior performance across diverse tasks. Further concept evaluations validate that the concepts learned by ECO-Concept surpassed current counterparts in comprehensibility.

</details>


### [278] [Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?](https://arxiv.org/abs/2505.20295)

*Michael Kirchhof, Luca Füger, Adam Goliński, Eeshan Gunesh Dhekane, Arno Blaas, Sinead Williamson*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Quantification, SelfReflect, Self-Summarization, Human Judgment

**Relevance Score:** 8

**TL;DR:** The paper introduces SelfReflect, a metric for assessing how well a language model's output summarizes its internal uncertainty distribution, showing its effectiveness compared to existing metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore better methods of uncertainty quantification in large language models beyond simple percentage scores, allowing for a richer understanding of model confidence.

**Method:** The paper presents SelfReflect as a metric designed to evaluate how faithfully a string represents the distribution of possible outputs from an LLM. It also investigates self-summarization methods using this metric.

**Key Contributions:**

	1. Introduction of SelfReflect, a novel metric for uncertainty quantification in LLMs
	2. Demonstration of SelfReflect's alignment with human judgment
	3. Identification of effective methods for generating faithful summaries of LLM uncertainties

**Result:** SelfReflect effectively distinguishes subtle differences in summary strings and correlates well with human judgment, outperforming previous metrics.

**Limitations:** The paper acknowledges that even advanced reasoning models find it difficult to explicate their internal uncertainty, indicating a challenge that still exists in the field.

**Conclusion:** The study suggests that while state-of-the-art models struggle with uncertainty explication, faithful summaries can be achieved through specific sampling and summarization techniques, paving the way for future research in LLM uncertainties.

**Abstract:** To reveal when a large language model (LLM) is uncertain about a response, uncertainty quantification commonly produces percentage numbers along with the output. But is this all we can do? We argue that in the output space of LLMs, the space of strings, exist strings expressive enough to summarize the distribution over output strings the LLM deems possible. We lay a foundation for this new avenue of uncertainty explication and present SelfReflect, a theoretically-motivated metric to assess how faithfully a string summarizes an LLM's internal answer distribution. We show that SelfReflect is able to discriminate even subtle differences of candidate summary strings and that it aligns with human judgement, outperforming alternative metrics such as LLM judges and embedding comparisons. With SelfReflect, we investigate a number of self-summarization methods and find that even state-of-the-art reasoning models struggle to explicate their internal uncertainty. But we find that faithful summarizations can be generated by sampling and summarizing. Our metric enables future works towards this universal form of LLM uncertainties.

</details>


### [279] [Reasoning LLMs are Wandering Solution Explorers](https://arxiv.org/abs/2505.20296)

*Jiahao Lu, Ziwei Xu, Mohan Kankanhalli*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Systematic Problem Solving, Evaluation Metrics, Failure Modes

**Relevance Score:** 7

**TL;DR:** This paper critiques the reasoning abilities of large language models (RLLMs), highlighting their inability to systematically explore solution spaces and proposing new metrics for evaluating reasoning processes.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in reasoning capabilities of large language models and foster a deeper evaluation of their systematic problem-solving abilities.

**Method:** The paper employs qualitative and quantitative analyses across multiple state-of-the-art reasoning LLMs to identify and categorize common failure modes in their reasoning processes.

**Key Contributions:**

	1. Formalization of systematic problem solving in reasoning LLMs
	2. Identification of common failure modes in reasoning processes
	3. Advocacy for new metrics to evaluate reasoning structures

**Result:** The analysis reveals persistent issues such as invalid reasoning steps, redundant explorations, and hallucinated conclusions, which contribute to a significant performance drop in complex tasks.

**Limitations:** The study primarily focuses on identifying issues without providing comprehensive solutions to the uncovered limitations of RLLMs.

**Conclusion:** The study calls for new evaluation metrics that focus on the reasoning process structure rather than just final outputs to improve the understanding and capabilities of reasoning LLMs.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive reasoning abilities through test-time computation (TTC) techniques such as chain-of-thought prompting and tree-based reasoning. However, we argue that current reasoning LLMs (RLLMs) lack the ability to systematically explore the solution space. This paper formalizes what constitutes systematic problem solving and identifies common failure modes that reveal reasoning LLMs to be wanderers rather than systematic explorers. Through qualitative and quantitative analysis across multiple state-of-the-art LLMs, we uncover persistent issues: invalid reasoning steps, redundant explorations, hallucinated or unfaithful conclusions, and so on. Our findings suggest that current models' performance can appear to be competent on simple tasks yet degrade sharply as complexity increases. Based on the findings, we advocate for new metrics and tools that evaluate not just final outputs but the structure of the reasoning process itself.

</details>


### [280] [MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding](https://arxiv.org/abs/2505.20298)

*Jeonghun Baek, Kazuki Egashira, Shota Onohara, Atsuyuki Miyai, Yuki Imajuku, Hikaru Ikuta, Kiyoharu Aizawa*

**Main category:** cs.CL

**Keywords:** manga, multimodal models, text recognition, visual question answering, narrative understanding

**Relevance Score:** 6

**TL;DR:** This paper introduces benchmarks and a specialized model for Japanese manga understanding using large multimodal models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to enhance the storytelling capabilities of manga creators by enabling large multimodal models to understand manga narratives at a human-like level.

**Method:** The authors introduce two benchmarks, MangaOCR for in-page text recognition and MangaVQA for visual question answering, as well as develop MangaLMM, a specialized model fine-tuned from Qwen2.5-VL for these tasks.

**Key Contributions:**

	1. Introduction of MangaOCR benchmark for text recognition
	2. Development of MangaVQA for evaluating contextual understanding
	3. Creation of MangaLMM model specialized for manga understanding

**Result:** Extensive experiments and comparisons with existing models like GPT-4o and Gemini 2.5 demonstrate the effectiveness of MangaLMM in understanding manga narratives.

**Limitations:** 

**Conclusion:** The introduced benchmarks and the fine-tuned MangaLMM provide a significant foundation for advancing the evaluation and development of large multimodal models in the narrative-heavy domain of manga.

**Abstract:** Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.

</details>


### [281] [AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection](https://arxiv.org/abs/2112.11479)

*Geet Shingi, Vedangi Wagh*

**Main category:** cs.CL

**Keywords:** Hinglish, Hate Speech, Subword Tokenization, Machine Learning, Multi-Head Attention

**Relevance Score:** 7

**TL;DR:** The paper proposes a simplified method for classifying Hinglish hate speech that outperforms complex deep learning techniques using subword tokenization and multi-head attention, achieving an accuracy of 87.41%.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexities and inefficiencies in classifying Hinglish hate speech on social media, using simpler approaches while maintaining or improving performance.

**Method:** The proposed technique utilizes subword tokenization algorithms like BPE and Unigram combined with multi-head attention mechanisms to classify code-mixed Hinglish hate speech.

**Key Contributions:**

	1. Simplified approach for hate speech classification
	2. Utilization of BPE and Unigram for handling Hinglish vocabulary
	3. Achieved competitive performance with lower computational costs

**Result:** Achieved an accuracy of 87.41% and an F1 score of 0.851 on standard datasets, exceeding the performance of existing complex methods while being more efficient.

**Limitations:** 

**Conclusion:** The proposed method offers a simpler, efficient, and sustainable solution for hate speech classification in Hinglish, suitable for real-world applications.

**Abstract:** Recent advancements in technology have led to a boost in social media usage which has ultimately led to large amounts of user-generated data which also includes hateful and offensive speech. The language used in social media is often a combination of English and the native language in the region. In India, Hindi is used predominantly and is often code-switched with English, giving rise to the Hinglish (Hindi+English) language. Various approaches have been made in the past to classify the code-mixed Hinglish hate speech using different machine learning and deep learning-based techniques. However, these techniques make use of recurrence on convolution mechanisms which are computationally expensive and have high memory requirements. Past techniques also make use of complex data processing making the existing techniques very complex and non-sustainable to change in data. Proposed work gives a much simpler approach which is not only at par with these complex networks but also exceeds performance with the use of subword tokenization algorithms like BPE and Unigram, along with multi-head attention-based techniques, giving an accuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use of BPE and Unigram algorithms help handle the nonconventional Hinglish vocabulary making the proposed technique simple, efficient and sustainable to use in the real world.

</details>


### [282] [ADEPT: A DEbiasing PrompT Framework](https://arxiv.org/abs/2211.05414)

*Ke Yang, Charles Yu, Yi Fung, Manling Li, Heng Ji*

**Main category:** cs.CL

**Keywords:** debiasing, prompt tuning, pre-trained language models, manifold learning, semantic representation

**Relevance Score:** 7

**TL;DR:** This paper presents ADEPT, a method for debiasing pre-trained language models (PLMs) using continuous prompt tuning while preserving their representation abilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing debiasing methods that compromise the representation ability of PLMs when reducing bias.

**Method:** The paper proposes a new training criterion inspired by manifold learning combined with an explicit debiasing term for effective prompt tuning.

**Key Contributions:**

	1. Introduction of ADEPT for debiasing PLMs via prompt tuning
	2. A unique training criterion based on manifold learning
	3. Empirical validation of ADEPT's effectiveness on standard benchmarks

**Result:** ADEPT achieves competitive results on debiasing benchmarks while maintaining or improving the PLM's representation ability.

**Limitations:** 

**Conclusion:** The visualization of word correlations indicates the effectiveness of the proposed method in reducing bias without losing important semantic information.

**Abstract:** Several works have proven that finetuning is an applicable approach for debiasing contextualized word embeddings. Similarly, discrete prompts with semantic meanings have shown to be effective in debiasing tasks. With unfixed mathematical representation at the token level, continuous prompts usually surpass discrete ones at providing a pre-trained language model (PLM) with additional task-specific information. Despite this, relatively few efforts have been made to debias PLMs by prompt tuning with continuous prompts compared to its discrete counterpart. Furthermore, for most debiasing methods that alter a PLM's original parameters, a major problem is the need to not only decrease the bias in the PLM but also to ensure that the PLM does not lose its representation ability. Finetuning methods typically have a hard time maintaining this balance, as they tend to violently remove meanings of attribute words. In this paper, we propose ADEPT, a method to debias PLMs using prompt tuning while maintaining the delicate balance between removing biases and ensuring representation ability. To achieve this, we propose a new training criterion inspired by manifold learning and equip it with an explicit debiasing term to optimize prompt tuning. In addition, we conduct several experiments with regard to the reliability, quality, and quantity of a previously proposed attribute training corpus in order to obtain a clearer prototype of a certain attribute, which indicates the attribute's position and relative distances to other words on the manifold. We evaluate ADEPT on several widely acknowledged debiasing benchmarks and downstream tasks, and find that it achieves competitive results while maintaining (and in some cases even improving) the PLM's representation ability. We further visualize words' correlation before and after debiasing a PLM, and give some possible explanations for the visible effects.

</details>


### [283] [The More Similar, the Better? Associations between Latent Semantic Similarity and Emotional Experiences Differ across Conversation Contexts](https://arxiv.org/abs/2309.12646)

*Chen-Wei Yu, Yun-Shiuan Chuang, Alexandros N. Lotsos, Tabea Meier, Claudia M. Haase*

**Main category:** cs.CL

**Keywords:** latent semantic similarity, emotional experiences, conversational context, natural language processing, social psychology

**Relevance Score:** 6

**TL;DR:** The study examines the relationship between latent semantic similarity (LSS) and emotional experiences in conversations, suggesting that context (type of conversation) significantly influences this association.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that higher LSS always correlates with positive emotional experiences by investigating how conversation type affects this relationship.

**Method:** Linear mixed modeling was employed to analyze conversations among 50 long-term married couples to determine the impact of LSS on emotional experiences.

**Key Contributions:**

	1. Introduces a nuanced view of LSS's association with emotional experiences based on conversation type.
	2. Demonstrates the application of NLP tools in psychological research.
	3. Provides empirical evidence that more dissimilar communication can enhance positive emotional experiences in pleasant interactions.

**Result:** The findings indicated that partners reported more positive emotions when their information exchanges were more dissimilar in pleasant conversations, contrary to the expectations of the share-mind perspective.

**Limitations:** Limited to the context of married couples; findings may not generalize to other types of relationships or conversations.

**Conclusion:** The study emphasizes the significance of context in the emotional implications of LSS and shows how NLP tools can test social psychology theories.

**Abstract:** Latent semantic similarity (LSS) is a measure of the similarity of information exchanges in a conversation. Challenging the assumption that higher LSS bears more positive psychological meaning, we propose that this association might depend on the type of conversation people have. On the one hand, the share-mind perspective would predict that higher LSS should be associated with more positive emotional experiences across the board. The broaden-and-build theory, on the other hand, would predict that higher LSS should be inversely associated with more positive emotional experiences specifically in pleasant conversations. Linear mixed modeling based on conversations among 50 long-term married couples supported the latter prediction. That is, partners experienced greater positive emotions when their overall information exchanges were more dissimilar in pleasant (but not conflict) conversations. This work highlights the importance of context in understanding the emotional correlates of LSS and exemplifies how modern natural language processing tools can be used to evaluate competing theory-driven hypotheses in social psychology.

</details>


### [284] [Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models](https://arxiv.org/abs/2310.13312)

*Jaeyoung Choe, Keonwoong Noh, Nayeon Kim, Seyun Ahn, Woohwan Jung*

**Main category:** cs.CL

**Keywords:** financial language model, pretrained language models, financial data analysis, generalization performance

**Relevance Score:** 8

**TL;DR:** Financial Language Model (FiLM) outperforms existing financial and general domain pretrained language models by training on a diverse financial corpus.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the generalization performance of financial pretrained language models by training them on a diverse range of financial data.

**Method:** FiLM was trained on a broad range of financial corpus collected from diverse sources.

**Key Contributions:**

	1. Introduction of FiLM as a diverse financial PLM
	2. Empirical evidence of performance improvement on unseen corpus
	3. Analysis of training data diversity impact on financial PLMs

**Result:** FiLM demonstrates better performance than both existing financial PLMs and general-purpose PLMs like BERT on various downstream tasks.

**Limitations:** 

**Conclusion:** Using a broader dataset for training significantly enhances the capabilities of financial PLMs, enabling better generalization even on unseen corpus groups.

**Abstract:** Over the past few years, various domain-specific pretrained language models (PLMs) have been proposed and have outperformed general-domain PLMs in specialized areas such as biomedical, scientific, and clinical domains. In addition, financial PLMs have been studied because of the high economic impact of financial data analysis. However, we found that financial PLMs were not pretrained on sufficiently diverse financial data. This lack of diverse training data leads to a subpar generalization performance, resulting in general-purpose PLMs, including BERT, often outperforming financial PLMs on many downstream tasks. To address this issue, we collected a broad range of financial corpus and trained the Financial Language Model (FiLM) on these diverse datasets. Our experimental results confirm that FiLM outperforms not only existing financial PLMs but also general domain PLMs. Furthermore, we provide empirical evidence that this improvement can be achieved even for unseen corpus groups.

</details>


### [285] [Unearthing Large Scale Domain-Specific Knowledge from Public Corpora](https://arxiv.org/abs/2401.14624)

*Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Conghui He, Qipeng Guo, Hang Yan, Dahua Lin, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** large language models, data collection, domain-specific knowledge, reasoning procedures, Retrieve-Pile

**Relevance Score:** 9

**TL;DR:** This paper introduces Retrieve-from-CC, an approach for guiding the collection of domain-specific data using large language models, resulting in the creation of the Retrieve-Pile dataset.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of open-source models and data for specific domains and reduce the labor-intensive process of manually collecting high-quality data.

**Method:** Integrating large language models into the data collection pipeline to generate and retrieve relevant domain-specific information from the Common Crawl corpus.

**Key Contributions:**

	1. Introduction of the Retrieve-from-CC approach
	2. Creation of the Retrieve-Pile dataset covering multiple domains
	3. Demonstrated improvement in reasoning performance with the collected data

**Result:** Retrieve-from-CC effectively collects data and mines reasoning procedures, resulting in the Retrieve-Pile dataset, which enhances reasoning abilities in various knowledge domains.

**Limitations:** 

**Conclusion:** The approach improves performance in mathematical and knowledge-related reasoning tasks, with datasets released for public use.

**Abstract:** Large language models (LLMs) have demonstrated remarkable potential in various tasks, however, there remains a significant lack of open-source models and data for specific domains. Previous work has primarily focused on manually specifying resources and collecting high-quality data for specific domains, which is extremely time-consuming and labor-intensive. To address this limitation, we introduce large models into the data collection pipeline to guide the generation of domain-specific information and retrieve relevant data from Common Crawl (CC), a large public corpus. We refer to this approach as Retrieve-from-CC. It not only collects data related to domain-specific knowledge but also mines the data containing potential reasoning procedures from the public corpus. By applying this method, we have collected a knowledge domain-related dataset named Retrieve-Pile, which covers four main domains, including the sciences, humanities, and other categories. Through the analysis of , Retrieve-from-CC can effectively retrieve relevant data from the covered knowledge domains and significantly improve the performance in tests of mathematical and knowledge-related reasoning abilities. We have released Retrieve-Pile at https://huggingface.co/datasets/Query-of-CC/Retrieve-Pile.

</details>


### [286] [Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation](https://arxiv.org/abs/2402.13211)

*Dongjin Kang, Sunghwan Kim, Taeyoon Kwon, Seungjun Moon, Hyunsouk Cho, Youngjae Yu, Dongha Lee, Jinyoung Yeo*

**Main category:** cs.CL

**Keywords:** Emotional Support, Large Language Models, Human-Computer Interaction, Chatbots, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper analyzes the performance of large language models in providing emotional support during conversations and examines the impact of strategy preferences on effectiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate why large language models struggle in offering effective emotional support despite their conversational capabilities.

**Method:** Analysis of the ESConv dataset and the performance of LLMs in selecting appropriate emotional support strategies.

**Key Contributions:**

	1. Analysis of LLM performance on the ESConv dataset
	2. Identification of challenges in emotional support strategy selection
	3. Recommendations for enhancing emotional intelligence in LLMs

**Result:** LLMs show preference for specific strategies that negatively affect their ability to provide emotional support.

**Limitations:** Existing LLMs lack the necessary emotional intelligence without modification and external assistance.

**Conclusion:** To improve emotional support capabilities, LLMs need to reduce strategy preference bias and may require external assistance; they cannot independently become proficient emotional supporters.

**Abstract:** Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) existing LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.

</details>


### [287] [A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion](https://arxiv.org/abs/2402.13405)

*Yanzhen Shen, Yu Zhang, Yunyi Zhang, Jiawei Han*

**Main category:** cs.CL

**Keywords:** entity set expansion, taxonomy expansion, large language model, instruction tuning, machine learning

**Relevance Score:** 7

**TL;DR:** This paper presents a unified approach for entity set expansion, taxonomy expansion, and seed-guided taxonomy construction using a taxonomy-guided instruction tuning framework for large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of generalizability in existing techniques that treat entity set expansion, taxonomy expansion, and seed-guided taxonomy construction as separate tasks.

**Method:** The authors propose a taxonomy-guided instruction tuning framework that trains a large language model to jointly learn the skills of generating 'siblings' and 'parents' for query entities, enhancing the performance across the three tasks.

**Key Contributions:**

	1. Unified framework addressing three taxonomy-related tasks
	2. Taxonomy-guided instruction tuning for large language models
	3. Experimental validation showing superior performance compared to task-specific methods

**Result:** Experimental results on benchmark datasets show that the proposed TaxoInstruct framework outperforms traditional task-specific baselines in all three tasks.

**Limitations:** 

**Conclusion:** The study demonstrates that a unified framework can effectively improve performance in related tasks by leveraging common skills in a joint learning process.

**Abstract:** Entity set expansion, taxonomy expansion, and seed-guided taxonomy construction are three representative tasks that can be applied to automatically populate an existing taxonomy with emerging concepts. Previous studies view them as three separate tasks. Therefore, their proposed techniques usually work for one specific task only, lacking generalizability and a holistic perspective. In this paper, we aim at a unified solution to the three tasks. To be specific, we identify two common skills needed for entity set expansion, taxonomy expansion, and seed-guided taxonomy construction: finding "siblings" and finding "parents". We propose a taxonomy-guided instruction tuning framework to teach a large language model to generate siblings and parents for query entities, where the joint pre-training process facilitates the mutual enhancement of the two skills. Extensive experiments on multiple benchmark datasets demonstrate the efficacy of our proposed TaxoInstruct framework, which outperforms task-specific baselines across all three tasks.

</details>


### [288] [MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models](https://arxiv.org/abs/2402.13606)

*Boyang Xue, Hongru Wang, Rui Wang, Sheng Wang, Zezhong Wang, Yiming Du, Bin Liang, Wenxuan Zhang, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** Large Language Models, confidence estimation, multilingual, language-agnostic, language-specific

**Relevance Score:** 9

**TL;DR:** This paper investigates multilingual confidence estimation in Large Language Models (LLMs), revealing linguistic dominance effects and proposing a native-tone prompting strategy to improve reliability in language-specific tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding confidence estimations of LLMs in languages other than English and improve their reliability.

**Method:** The paper introduces MlingConf, a benchmark comprising multilingual datasets for language-agnostic and language-specific tasks, and conducts experiments to analyze performance and linguistic dominance.

**Key Contributions:**

	1. Introduction of multilingual confidence estimation framework (MlingConf)
	2. Benchmark with high-quality multilingual datasets
	3. Proposed native-tone prompting strategy for improved reliability in LS tasks

**Result:** Experiments show that English has linguistic dominance in confidence estimations for language-agnostic tasks, while language-specific prompts improve performance in language-specific tasks.

**Limitations:** Focus on specific contexts; may not generalize universally across all languages and tasks.

**Conclusion:** A native-tone prompting strategy utilizing language-specific prompts enhances the reliability and accuracy of LLMs in language-specific scenarios.

**Abstract:** The tendency of Large Language Models (LLMs) to generate hallucinations raises concerns regarding their reliability. Therefore, confidence estimations indicating the extent of trustworthiness of the generations become essential. However, current LLM confidence estimations in languages other than English remain underexplored. This paper addresses this gap by introducing a comprehensive investigation of Multilingual Confidence estimation (MlingConf) on LLMs, focusing on both language-agnostic (LA) and language-specific (LS) tasks to explore the performance and language dominance effects of multilingual confidence estimations on different tasks. The benchmark comprises four meticulously checked and human-evaluated high-quality multilingual datasets for LA tasks and one for the LS task tailored to specific social, cultural, and geographical contexts of a language. Our experiments reveal that on LA tasks English exhibits notable linguistic dominance in confidence estimations than other languages, while on LS tasks, using question-related language to prompt LLMs demonstrates better linguistic dominance in multilingual confidence estimations. The phenomena inspire a simple yet effective native-tone prompting strategy by employing language-specific prompts for LS tasks, effectively improving LLMs' reliability and accuracy in LS scenarios.

</details>


### [289] [Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency](https://arxiv.org/abs/2402.15481)

*Yiran Liu, Ke Yang, Zehan Qi, Xiao Liu, Yang Yu, ChengXiang Zhai*

**Main category:** cs.CL

**Keywords:** Bias-Volatility Framework, stereotypes, large language models, discrimination, human feedback

**Relevance Score:** 9

**TL;DR:** This paper introduces the Bias-Volatility Framework (BVF) for analyzing and quantifying stereotypes in large language models (LLMs), highlighting the importance of capturing both bias and variation in generative outputs to assess discrimination risks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current metrics for evaluating alignment in LLMs often fail to account for the randomness and inconsistencies in stereotypes generated by these models, leading to misleading evaluations of bias and discrimination.

**Method:** The Bias-Volatility Framework (BVF) estimates the probability distribution of stereotypes in LLM outputs, measuring both bias risk (mean of the stereotype distribution) and volatility risk (variation in the outputs).

**Key Contributions:**

	1. Introduction of the Bias-Volatility Framework (BVF) for stereotype analysis in LLMs.
	2. Demonstration that bias risk is the leading contributor to discrimination in LLM outputs.
	3. Insights into the effect of reinforcement learning on bias and volatility.

**Result:** Analysis of 12 widely used LLMs showed that bias risk is the primary contributor to discrimination, with many models exhibiting significant pro-male stereotypes across various professions. Additionally, reinforcement learning from human feedback was found to reduce bias but increase volatility in outputs.

**Limitations:** The framework primarily focuses on bias and volatility; it may not capture all forms of discrimination or other factors influencing LLM behavior.

**Conclusion:** BVF provides a new avenue for understanding the discriminatory impacts of LLMs, emphasizing the need to consider variations in generative behavior when assessing model fairness.

**Abstract:** We present a novel statistical framework for analyzing stereotypes in large language models (LLMs) by systematically estimating the bias and variation in their generation. Current alignment evaluation metrics often overlook stereotypes' randomness caused by LLMs' inconsistent generative behavior. For instance, LLMs may display contradictory stereotypes, such as those related to gender or race, for identical professions in different contexts. Ignoring this inconsistency risks misleading conclusions in alignment assessments and undermines efforts to evaluate the potential of LLMs to perpetuate or amplify social biases and unfairness.   To address this, we propose the Bias-Volatility Framework (BVF), which estimates the probability distribution of stereotypes in LLM outputs. By capturing the variation in generative behavior, BVF assesses both the likelihood and degree to which LLM outputs negatively impact vulnerable groups, enabling a quantification of aggregated discrimination risk. Additionally, we introduce a mathematical framework to decompose this risk into bias risk (from the mean of the stereotype distribution) and volatility risk (from its variation). Applying BVF to 12 widely used LLMs, we find: i) Bias risk is the dominant contributor to discrimination; ii) Most LLMs exhibit substantial pro-male stereotypes across nearly all professions; iii) Reinforcement learning from human feedback reduces bias but increases volatility; iv) Discrimination risk correlates with socio-economic factors, such as professional salaries. Finally, we highlight BVF's broader applicability for assessing how generation inconsistencies in LLMs impact behavior beyond stereotypes.

</details>


### [290] [MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2402.17263)

*Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren, Maarten de Rijke, Zhumin Chen, Jiahuan Pei*

**Main category:** cs.CL

**Keywords:** parameter-efficient fine-tuning, low-rank adaptation, natural language processing

**Relevance Score:** 8

**TL;DR:** MELoRA is a new method for parameter-efficient fine-tuning of large language models that uses mini-ensemble low-rank adapters to improve generalization while requiring fewer trainable parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As large language models grow larger and are applied to diverse tasks, standard fine-tuning methods struggle with generalization errors. A new approach is needed to enhance performance while reducing parameter counts.

**Method:** MELoRA freezes the original pretrained weights and trains a group of mini LoRAs, allowing for greater diversity and better generalization in performance across tasks.

**Key Contributions:**

	1. Introduction of mini-ensemble low-rank adapters (MELoRA) for efficient training
	2. Significantly reduced parameter counts while maintaining or improving performance
	3. Theoretical and empirical validation of improved generalization ability across various NLP tasks

**Result:** MELoRA outperforms standard low-rank adaptation (LoRA) by achieving better performance with significantly fewer trainable parameters, specifically achieving 8 times fewer parameters for natural language understanding and 36 times fewer for instruction following tasks.

**Limitations:** 

**Conclusion:** MELoRA effectively captures task diversity and generalization, making it a promising approach for fine-tuning large language models with fewer resources.

**Abstract:** Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.

</details>


### [291] [Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation](https://arxiv.org/abs/2405.13984)

*Dimitris Gkoumas, Maria Liakata*

**Main category:** cs.CL

**Keywords:** language models, molecule caption generation, NLI, alignment fine-tuning, cross-modal

**Relevance Score:** 8

**TL;DR:** This paper improves scientific language models for molecule caption generation by enhancing inference and evaluation capabilities without extensive fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to enhance scientific language models that traditionally rely on extensive fine-tuning on large datasets, by boosting their performance with minimal additional training.

**Method:** The authors investigate post-training enhancements through a combination of alignment fine-tuning and model merging in a cross-modal context, targeting molecule caption generation.

**Key Contributions:**

	1. Enhanced inference and evaluation for language models with minimal training
	2. Novel atomic-level evaluation method for unseen chemical domains
	3. Insights into post-training synergies between model merging and alignment fine-tuning

**Result:** Experimental results show significant performance improvements over state-of-the-art models, revealing better alignment with the tasks and a novel evaluation method for unseen chemical domains.

**Limitations:** The paper does not address potential limitations in the generalizability of the proposed methods beyond the chemical domain.

**Conclusion:** The study concludes that the proposed atomic-level evaluation method is effective for the molecular context, in contrast to common NLI methods, and supports finer granularity in reasoning assessments.

**Abstract:** Scientific language models drive research innovation but require extensive fine-tuning on large datasets. This work enhances such models by improving their inference and evaluation capabilities with minimal or no additional training. Focusing on molecule caption generation, we explore post-training synergies between alignment fine-tuning and model merging in a cross-modal setup. We reveal intriguing insights into the behaviour and suitability of such methods while significantly surpassing state-of-the-art models. Moreover, we propose a novel atomic-level evaluation method leveraging off-the-shelf Natural Language Inference (NLI) models for use in the unseen chemical domain. Our experiments demonstrate that our evaluation operates at the right level of granularity, effectively handling multiple content units and subsentence reasoning, while widely adopted NLI methods consistently misalign with assessment criteria.

</details>


### [292] [UniICL: An Efficient Unified Framework Unifying Compression, Selection, and Generation](https://arxiv.org/abs/2405.17062)

*Jun Gao, Qi Lv, Zili Wang, Tianxiang Wu, Ziqiang Cao, Wenjie Li*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, demonstration compression, Demonstration Bank, AI efficiency

**Relevance Score:** 8

**TL;DR:** UniICL is a new framework that addresses context length issues in in-context learning for LLMs by improving demonstration compression and selection, enhancing efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing in-context learning methods, particularly their tendency to increase context length excessively, which burdens hardware resources and hampers effective reasoning in LLMs.

**Method:** A unified framework, UniICL, is proposed that integrates demonstration compression, selection, and response generation, along with a compression strategy that caches results in a Demonstration Bank.

**Key Contributions:**

	1. Development of a unified ICL framework (UniICL) that consolidates multiple tasks.
	2. Introduction of a novel caching strategy for demonstration compression.
	3. Empirical evidence of improved efficiency and effectiveness over existing methods.

**Result:** UniICL demonstrates significant improvements in both effectiveness and efficiency in out-of-domain evaluations compared to traditional methods.

**Limitations:** 

**Conclusion:** The findings suggest that UniICL can effectively enhance LLM reasoning while mitigating hardware constraints typically associated with excessive context lengths.

**Abstract:** In-context learning (ICL) enhances the reasoning abilities of Large Language Models (LLMs) by prepending a few demonstrations. It motivates researchers to introduce more examples to provide additional contextual information for the generation. However, existing methods show a significant limitation due to the problem of excessive growth in context length, which causes a large hardware burden. In addition, shallow-relevant examples selected by off-the-shelf tools hinder LLMs from capturing useful contextual information for generation. In this paper, we propose \textbf{UniICL}, a novel \textbf{Uni}fied \textbf{ICL} framework that unifies demonstration compression, demonstration selection, and final response generation. Furthermore, to boost inference efficiency, we design a tailored compression strategy that allows UniICL to cache compression results into \textbf{Demonstration Bank} (\textbf{DB}), which avoids repeated compression of the same demonstration. Extensive out-of-domain evaluations prove the advantages of UniICL in both effectiveness and efficiency.

</details>


### [293] [Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets](https://arxiv.org/abs/2406.05348)

*Satanu Ghosh, Neal R. Brodnik, Carolina Frey, Collin Holgate, Tresa M. Pollock, Samantha Daly, Samuel Carton*

**Main category:** cs.CL

**Keywords:** GPT-4, information extraction, schema-based, materials science, LLM

**Relevance Score:** 8

**TL;DR:** This paper investigates the use of GPT-4 for schema-based information extraction from scientific literature, specifically in materials science, and evaluates its effectiveness and limitations.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to explore the potential of GPT-4 in replicating existing datasets from scientific manuscripts through ad-hoc schema-based information extraction.

**Method:** The authors employed a basic prompting approach with GPT-4 and conducted a detailed manual error analysis using materials scientists to evaluate the model's extraction capabilities.

**Key Contributions:**

	1. Evaluation of GPT-4's performance in information extraction from scientific literature.
	2. Identification of common errors in information extraction using a manual review by experts.
	3. Recommendations for future research to improve LLM-based information extraction methods.

**Result:** The assessment revealed specific areas where GPT-4 struggles to accurately extract information and provided insights into potential improvements.

**Limitations:** The model's limitations in accurately extracting certain types of information were identified but not fully resolved in the study.

**Conclusion:** Insights from the manual error analysis led to suggestions for future research directions in enhancing the model's information extraction abilities.

**Abstract:** We explore the ability of GPT-4 to perform ad-hoc schema based information extraction from scientific literature. We assess specifically whether it can, with a basic prompting approach, replicate two existing material science datasets, given the manuscripts from which they were originally manually extracted. We employ materials scientists to perform a detailed manual error analysis to assess where the model struggles to faithfully extract the desired information, and draw on their insights to suggest research directions to address this broadly important task.

</details>


### [294] [Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation](https://arxiv.org/abs/2406.11632)

*Boxuan Lyu, Hidetaka Kamigaito, Kotaro Funakoshi, Manabu Okumura*

**Main category:** cs.CL

**Keywords:** neural machine translation, minimum Bayes risk, quality estimation

**Relevance Score:** 4

**TL;DR:** This paper introduces source-based MBR (sMBR) decoding for neural machine translation, which utilizes paraphrased or back-translated sources to improve translation quality over standard methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of maximum a posteriori decoding in neural machine translation, where high estimated probabilities do not guarantee translation quality.

**Method:** The proposed sMBR decoding uses quasi-sources as support hypotheses and incorporates a reference-free quality estimation metric as its utility function.

**Key Contributions:**

	1. Introduction of source-based MBR decoding (sMBR)
	2. Use of paraphrased or back-translated sources as support hypotheses
	3. First work using sources in MBR decoding for NMT

**Result:** Experiments demonstrate that sMBR surpasses both Quality Estimation reranking and traditional MBR decoding approaches.

**Limitations:** 

**Conclusion:** sMBR presents a novel and effective technique for improving neural machine translation decoding by leveraging source-based hypotheses and quality estimation.

**Abstract:** Maximum a posteriori decoding, a commonly used method for neural machine translation (NMT), aims to maximize the estimated posterior probability. However, high estimated probability does not always lead to high translation quality. Minimum Bayes Risk (MBR) decoding offers an alternative by seeking hypotheses with the highest expected utility.   Inspired by Quality Estimation (QE) reranking which uses the QE model as a ranker we propose source-based MBR (sMBR) decoding, a novel approach that utilizes quasi-sources (generated via paraphrasing or back-translation) as ``support hypotheses'' and a reference-free quality estimation metric as the utility function, marking the first work to solely use sources in MBR decoding. Experiments show that sMBR outperforms QE reranking and the standard MBR decoding. Our findings suggest that sMBR is a promising approach for NMT decoding.

</details>


### [295] [USDC: A Dataset of $\underline{U}$ser $\underline{S}$tance and $\underline{D}$ogmatism in Long $\underline{C}$onversations](https://arxiv.org/abs/2406.16833)

*Mounika Marreddy, Subba Reddy Oota, Venkata Charan Chinni, Manish Gupta, Lucie Flek*

**Main category:** cs.CL

**Keywords:** User Opinion, Stance Classification, Dogmatism Classification, Dataset, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper introduces USDC, a dataset designed to study user opinion fluctuations in long Reddit conversation threads, automating annotations using LLMs for stance and dogmatism classifications.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** Analyzing user opinion changes in long conversation threads is vital for applications in personalization, market research, and customer service, but existing methods focus on simplistic post-level analysis.

**Method:** Developed a dataset (USDC) with annotations for stance and dogmatism, leveraging zero-shot, one-shot, and few-shot annotations from LLMs like GPT-4 to enhance annotation automation.

**Key Contributions:**

	1. Introduction of a novel dataset (USDC) for user opinion fluctuation study
	2. Use of LLMs to automate annotation processes
	3. Finetuning of multiple language models for enhanced classification tasks

**Result:** Achieved inter-annotator agreement scores of 0.49 for stance and 0.50 for dogmatism in human annotations, indicating substantial consistency with LLM-generated annotations.

**Limitations:** Challenges in manual annotations include lengthy conversation threads and subtle opinion changes.

**Conclusion:** The USDC dataset can be utilized to finetune and instruction-tune multiple small language models for stance and dogmatism classification, making significant advancements in the study of opinion dynamics in conversations.

**Abstract:** Analyzing user opinion changes in long conversation threads is extremely critical for applications like enhanced personalization, market research, political campaigns, customer service, targeted advertising, and content moderation. Unfortunately, previous studies on stance and dogmatism in user conversations have focused on training models using datasets annotated at the post level, treating each post as independent and randomly sampling posts from conversation threads. Hence, first, we build a dataset for studying user opinion fluctuations in 764 long multi-user Reddit conversation threads, called USDC. USDC contains annotations for 2 tasks: i) User Stance classification, which involves labeling a user's stance in a post within a conversation on a five-point scale; ii) User Dogmatism classification, which involves labeling a user's overall opinion in the conversation on a four-point scale. Besides being time-consuming and costly, manual annotations for USDC are challenging because: 1) Conversation threads could be very long, increasing the chances of noisy annotations; and 2) Interpreting instances where a user changes their opinion within a conversation is difficult because often such transitions are subtle and not expressed explicitly. Hence, we leverage majority voting on zero-shot, one-shot, and few-shot annotations from Mistral Large and GPT-4 to automate the annotation process. Human annotations on 200 test conversations achieved inter-annotator agreement scores of 0.49 for stance and 0.50 for dogmatism with these LLM annotations, indicating a reasonable level of consistency between human and LLM annotations. USDC is then used to finetune and instruction-tune multiple deployable small language models like LLaMA, Falcon and Vicuna for the stance and dogmatism classification tasks. We make the code and dataset publicly available [https://github.com/mounikamarreddy/USDC].

</details>


### [296] [Can Large Language Models Generate High-quality Patent Claims?](https://arxiv.org/abs/2406.19465)

*Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz*

**Main category:** cs.CL

**Keywords:** Large Language Models, Patent Claim Generation, Fine-Tuning, Natural Language Processing, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the capabilities of large language models (LLMs) in generating patent claims and explores the need for further research on in-domain LLMs.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the under-exploration of LLMs in the patent domain, which uses structured language for claim generation.

**Method:** A dataset was constructed to assess the performance of LLMs in generating patent claims based on patent descriptions.

**Key Contributions:**

	1. Construction of a patent claim generation dataset
	2. Demonstration of LLM performance differences in patent vs. abstract generation
	3. Insights into fine-tuning for improved patent feature representation

**Result:** The study reveals that generating claims from patent descriptions is more effective than generating from abstracts, with GPT-4 performing best among the evaluated models.

**Limitations:** Current patent-specific LLMs underperform compared to general LLMs, requiring further research and development.

**Conclusion:** While LLMs can generate high-quality independent claims, their performance declines for dependent claims, necessitating further fine-tuning for legal robustness.

**Abstract:** Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions' features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.

</details>


### [297] [The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations](https://arxiv.org/abs/2407.19299)

*Thanh-Dung Le, Ti Ti Nguyen, Vu Nguyen Ha, Symeon Chatzinotas, Philippe Jouvet, Rita Noumeir*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Large Language Models, Health Informatics, Clinical Notes Classification, Adapter Techniques

**Relevance Score:** 9

**TL;DR:** The study evaluates the effectiveness of adapter techniques for fine-tuning LLMs in clinical NLP under resource constraints, finding simpler Transformer-based models to perform better.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in fine-tuning LLMs for clinical NLP due to domain gaps and limited data in a resource-constrained environment.

**Method:** The study explored four adapter structures for clinical notes classification using biomedical pre-trained models and Transformer-based models.

**Key Contributions:**

	1. Demonstrated that simpler Transformer-based models outperform fine-tuned LLMs in resource-constrained settings.
	2. Identified Gated Residual Network (GRN) as the most effective adapter structure.
	3. Highlighted the computational resource demands of LLMs for clinical NLP tasks.

**Result:** Adapter structures showed minimal improvement, while simpler Transformer-based models outperformed LLMs, with GRN being the best adapter structure yielding an F1 score of 0.88.

**Limitations:** The findings are specific to resource-constrained environments and may not generalize to scenarios with ample resources.

**Conclusion:** Simpler models can be effectively trained in low-resource settings, suggesting a viable alternative for clinical NLP without needing extensive computational resources.

**Abstract:** Fine-tuning Large Language Models (LLMs) for clinical Natural Language Processing (NLP) poses significant challenges due to the domain gap and limited data availability. This study investigates the effectiveness of various adapter techniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a resource-constrained hospital environment. We experimented with four structures-Adapter, Lightweight, TinyAttention, and Gated Residual Network (GRN)-as final layers for clinical notes classification. We fine-tuned biomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT, alongside two Transformer-based models. Our extensive experimental results indicate that i) employing adapter structures does not yield significant improvements in fine-tuning biomedical pre-trained LLMs, and ii) simpler Transformer-based models, trained from scratch, perform better under resource constraints. Among the adapter structures, GRN demonstrated superior performance with accuracy, precision, recall, and an F1 score of 0.88. Moreover, the total training time for LLMs exceeded 1000 hours, compared to under 6 hours for simpler transformer-based models, highlighting that LLMs are more suitable for environments with extensive computational resources and larger datasets. Consequently, this study demonstrates that simpler Transformer-based models can be effectively trained from scratch, providing a viable solution for clinical NLP tasks in low-resource environments with limited data availability. By identifying the GRN as the most effective adapter structure, we offer a practical approach to enhance clinical note classification without requiring extensive computational resources.

</details>


### [298] [CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts](https://arxiv.org/abs/2408.09070)

*Qingkai Zeng, Yuyang Bai, Zhaoxuan Tan, Zhenyu Wu, Shangbin Feng, Meng Jiang*

**Main category:** cs.CL

**Keywords:** taxonomy expansion, large language models, knowledge representation

**Relevance Score:** 8

**TL;DR:** CodeTaxo introduces a novel method for taxonomy expansion using large language models to enhance the integration of new concepts into existing taxonomies, achieving superior performance over prior methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Taxonomies are essential for knowledge representation, and effective expansion methods are needed, especially when existing taxonomies are limited in size.

**Method:** CodeTaxo leverages large language models by using code language prompts to identify parent concepts for new query concepts in taxonomy expansion.

**Key Contributions:**

	1. Introduction of CodeTaxo leveraging LLMs for taxonomy expansion
	2. Demonstrated superior performance compared to existing methods
	3. Experimental validation across multiple real-world benchmarks

**Result:** CodeTaxo consistently outperforms previous state-of-the-art methods across five real-world benchmarks, demonstrating superior performance in taxonomy expansion tasks.

**Limitations:** 

**Conclusion:** The introduction of CodeTaxo marks a significant advancement in taxonomy expansion, particularly for small taxonomies, and the availability of code and data enhances accessibility for future research.

**Abstract:** Taxonomies play a crucial role in various applications by providing a structural representation of knowledge. The task of taxonomy expansion involves integrating emerging concepts into existing taxonomies by identifying appropriate parent concepts for these new query concepts. Previous approaches typically relied on self-supervised methods that generate annotation data from existing taxonomies. However, these methods are less effective when the existing taxonomy is small (fewer than 100 entities). In this work, we introduce CodeTaxo, a novel approach that leverages large language models through code language prompts to capture the taxonomic structure. Extensive experiments on five real-world benchmarks from different domains demonstrate that CodeTaxo consistently achieves superior performance across all evaluation metrics, significantly outperforming previous state-of-the-art methods. The code and data are available at https://github.com/QingkaiZeng/CodeTaxo-Pub.

</details>


### [299] [Language Models Benefit from Preparation with Elicited Knowledge](https://arxiv.org/abs/2409.01345)

*Jiacan Yu, Hannah An, Lenhart K. Schubert*

**Main category:** cs.CL

**Keywords:** zero-shot, question answering, language models, knowledge access, prompting technique

**Relevance Score:** 8

**TL;DR:** The paper introduces a prompting technique called PREP that improves the performance of language models by using two instances to generate relevant information and answer questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the question answering capability of language models by focusing on accessing relevant knowledge rather than just reasoning steps.

**Method:** The PREP technique uses two language models: the first generates relevant information, while the second answers the question using that information. It was tested on a dataset of 100 QA questions regarding artifact parts and material composition.

**Key Contributions:**

	1. Introduction of the PREP technique for enhanced QA tasks
	2. Demonstration of higher accuracy across various datasets
	3. Reduction of the need for domain-specific prompt engineering

**Result:** The PREP method showed consistently higher average accuracy than other tested methods across multiple datasets, including one based on parts and materials as well as commonsense reasoning datasets.

**Limitations:** 

**Conclusion:** PREP can be effectively applied to various question answering tasks without the need for domain-specific prompt engineering and improves the use of language models in QA scenarios.

**Abstract:** The zero-shot chain of thought (CoT) approach is often used in question answering (QA) by language models (LMs) for tasks that require multiple reasoning steps. However, some QA tasks hinge more on accessing relevant knowledge than on chaining reasoning steps. We introduce a simple prompting technique, called PREP, that involves using two instances of LMs: the first (LM1) generates relevant information, and the second (LM2) receives the information from the user and answers the question. This design is intended to make better use of the LM's instruction-following capability. PREP is applicable across various QA tasks without domain-specific prompt engineering. PREP is developed on a dataset of 100 QA questions, derived from an extensive schematic dataset specifying artifact parts and material composition. These questions ask which of two artifacts is less likely to share materials with another artifact. Such questions probe the LM's knowledge of shared materials in the part structure of different artifacts. We test our method on our parts-and-materials dataset and three published commonsense reasoning datasets. The average accuracy of our method is consistently higher than that of all the other tested methods across all the tested datasets.

</details>


### [300] [The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language](https://arxiv.org/abs/2409.08103)

*Michael Ong, Sean Robertson, Leo Peckham, Alba Jorquera Jimenez de Aberasturi, Paula Arkhangorodsky, Robin Huo, Aman Sakhardande, Mark Hallap, Naomi Nagy, Ewan Dunbar*

**Main category:** cs.CL

**Keywords:** speech recognition, low-resource languages, Faetar, multilingual models, automatic speech recognition

**Relevance Score:** 3

**TL;DR:** The Faetar Automatic Speech Recognition Benchmark aims to enhance low-resource speech recognition, focusing on the Franco-Proven
o{c}al variety spoken in Italy, with limited existing data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the field of low-resource speech recognition, particularly for underrepresented languages such as Faetar.

**Method:** The benchmark includes noisy field recordings, with 5 hours of transcribed and 20 hours of unlabelled speech. Baseline results were generated using multilingual speech foundation models.

**Key Contributions:**

	1. Introduction of a novel benchmark corpus for low-resource speech recognition.
	2. Demonstration of baseline results from multilingual speech models on a unique language variety.
	3. Insights into the challenges of working with noisy recordings and limited transcriptions.

**Result:** A best phone error rate of 30.4% was achieved using a pipeline that continues pre-training on the foundation model with the unlabelled dataset.

**Limitations:** Limited amount of transcribed data and absence of standard orthography for the language.

**Conclusion:** The Faetar corpus presents challenges for ASR due to its limited resources and variability in quality, highlighting the need for innovative approaches.

**Abstract:** We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition. Faetar, a Franco-Proven\c{c}al variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Proven\c{c}al. The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality. The corpus contains an additional 20 hrs of unlabelled speech. We report baseline results from state-of-the-art multilingual speech foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set.

</details>


### [301] [Identifying Knowledge Editing Types in Large Language Models](https://arxiv.org/abs/2409.19663)

*Xiaopeng Li, Shasha Li, Shangwen Wang, Shezheng Song, Bin Ji, Huijun Liu, Jun Ma, Jie Yu*

**Main category:** cs.CL

**Keywords:** Knowledge Editing, Large Language Models, Malicious Edits

**Relevance Score:** 9

**TL;DR:** This paper introduces the knowledge editing type identification (KETI) task and KETIBench dataset to identify malicious edits in large language models (LLMs).

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the risks associated with malicious edits in LLMs, which can lead to harmful outputs and user manipulation.

**Method:** The study proposes KETIBench, a dataset with various harmful and benign edits, and develops classification models, including five classical and three BERT-based models, to identify these edits.

**Key Contributions:**

	1. Introduction of the KETI task for identifying types of edits in LLMs
	2. Development and release of KETIBench dataset
	3. Demonstration of effective identification models for malicious edits

**Result:** The experimental results show that all eight baseline identifiers perform well in identifying malicious edits, demonstrating cross-domain generalization and robustness against various knowledge editing methods.

**Limitations:** 

**Conclusion:** The findings suggest that it's feasible to identify harmful edits in LLMs, contributing to safer AI interactions.

**Abstract:** Knowledge editing has emerged as an efficient technique for updating the knowledge of large language models (LLMs), attracting increasing attention in recent years. However, there is a lack of effective measures to prevent the malicious misuse of this technique, which could lead to harmful edits in LLMs. These malicious modifications could cause LLMs to generate toxic content, misleading users into inappropriate actions. In front of this risk, we introduce a new task, $\textbf{K}$nowledge $\textbf{E}$diting $\textbf{T}$ype $\textbf{I}$dentification (KETI), aimed at identifying different types of edits in LLMs, thereby providing timely alerts to users when encountering illicit edits. As part of this task, we propose KETIBench, which includes five types of harmful edits covering the most popular toxic types, as well as one benign factual edit. We develop five classical classification models and three BERT-based models as baseline identifiers for both open-source and closed-source LLMs. Our experimental results, across 92 trials involving four models and three knowledge editing methods, demonstrate that all eight baseline identifiers achieve decent identification performance, highlighting the feasibility of identifying malicious edits in LLMs. Additional analyses reveal that the performance of the identifiers is independent of the reliability of the knowledge editing methods and exhibits cross-domain generalization, enabling the identification of edits from unknown sources. All data and code are available in https://github.com/xpq-tech/KETI.

</details>


### [302] [QAEncoder: Towards Aligned Representation Learning in Question Answering System](https://arxiv.org/abs/2409.20434)

*Zhengren Wang, Qinhan Yu, Shida Wei, Zhiyu Li, Feiyu Xiong, Xiaoxing Wang, Simin Niu, Hao Liang, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** QA systems, retrieval-augmented generation, embedding methods

**Relevance Score:** 8

**TL;DR:** QAEncoder is a training-free method designed to improve precision in retrieval-augmented generation (RAG) systems for question answering by estimating potential queries in the embedding space.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of the gap between user queries and relevant documents that affects the accuracy of QA systems.

**Method:** QAEncoder estimates potential queries in the embedding space as a surrogate for document embeddings and attaches document fingerprints for better distinction.

**Key Contributions:**

	1. Introduction of a training-free method for query-document alignment
	2. Use of document fingerprints for enhanced embedding distinction
	3. Demonstrated effectiveness across various datasets and languages

**Result:** Extensive experimentation shows QAEncoder's strong performance in aligning queries and documents without additional storage, latency, training costs, or the risks of forgetting and hallucination.

**Limitations:** 

**Conclusion:** QAEncoder provides a simple and effective solution for enhancing QA system accuracy without incurring extra costs or complexity.

**Abstract:** Modern QA systems entail retrieval-augmented generation (RAG) for accurate and trustworthy responses. However, the inherent gap between user queries and relevant documents hinders precise matching. We introduce QAEncoder, a training-free approach to bridge this gap. Specifically, QAEncoder estimates the expectation of potential queries in the embedding space as a robust surrogate for the document embedding, and attaches document fingerprints to effectively distinguish these embeddings. Extensive experiments across diverse datasets, languages, and embedding models confirmed QAEncoder's alignment capability, which offers a simple-yet-effective solution with zero additional index storage, retrieval latency, training costs, or catastrophic forgetting and hallucination issues. The repository is publicly available at https://github.com/IAAR-Shanghai/QAEncoder.

</details>


### [303] [Do Vision-Language Models Really Understand Visual Language?](https://arxiv.org/abs/2410.00193)

*Yifan Hou, Buse Giledereli, Yilei Tu, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, diagram comprehension, relationship reasoning

**Relevance Score:** 7

**TL;DR:** This paper evaluates the capability of Large Vision-Language Models (LVLMs) in understanding diagrams and reveals limitations in their ability to grasp relationships among entities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The growing use of visual languages like diagrams in communication necessitates an understanding of how AI models, particularly LVLMs, comprehend them.

**Method:** A comprehensive test suite was developed to evaluate LVLMs' diagram comprehension through various questions focused on concept entities and their relationships across synthetic and real diagrams.

**Key Contributions:**

	1. Development of a test suite for evaluating LVLMs on diagram understanding
	2. Identification of limitations in LVLMs' relationship comprehension
	3. Insights into the reliance on background knowledge in diagram reasoning

**Result:** LVLMs can identify and reason about entities in diagrams accurately, but their understanding of relationships is notably limited, often relying on background knowledge as shortcuts.

**Limitations:** Limited capability for genuine diagram understanding; results may not generalize across all diagram types.

**Conclusion:** The impressive performance of LVLMs in diagram reasoning is misleading, as it is influenced by confounding factors rather than true understanding of diagrams.

**Abstract:** Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an image. The symbolic nature of diagrams presents significant challenges for building models capable of understanding them. Recent studies suggest that Large Vision-Language Models (LVLMs) can even tackle complex reasoning tasks involving diagrams. In this paper, we investigate this phenomenon by developing a comprehensive test suite to evaluate the diagram comprehension capability of LVLMs. Our test suite uses a variety of questions focused on concept entities and their relationships over a set of synthetic as well as real diagrams across domains to evaluate the recognition and reasoning abilities of models. Our evaluation of LVLMs shows that while they can accurately identify and reason about entities, their ability to understand relationships is notably limited. Further testing reveals that the decent performance on diagram understanding largely stems from leveraging their background knowledge as shortcuts to identify and reason about the relational information. Thus, we conclude that LVLMs have a limited capability for genuine diagram understanding, and their impressive performance in diagram reasoning is an illusion emanating from other confounding factors, such as the background knowledge in the models.

</details>


### [304] [In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement](https://arxiv.org/abs/2410.03124)

*Zhen-Yu Zhang, Jiandong Zhang, Huaxiu Yao, Gang Niu, Masashi Sugiyama*

**Main category:** cs.CL

**Keywords:** large language models, prompt optimization, pseudo-supervision

**Relevance Score:** 9

**TL;DR:** The paper introduces the PAPO algorithm, which optimizes prompts and pseudo-supervision in LLMs to enhance performance without costly human intervention.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient methods to enhance LLM performance without expensive human supervision or extensive retraining.

**Method:** The PAPO algorithm combines in-context learning and a novel optimization process that translates gradient signals into textual critiques to refine prompts and pseudo-supervised data.

**Key Contributions:**

	1. Introduction of the PAPO algorithm for optimizing prompts and pseudo-supervision in LLMs.
	2. Utilization of in-context learning to generate consistent responses.
	3. Theoretical analysis demonstrating the effectiveness in mitigating overfitting.

**Result:** The proposed approach reduces overfitting and demonstrates improved performance on question answering, natural language inference, and molecule optimization tasks.

**Limitations:** 

**Conclusion:** The PAPO algorithm effectively optimizes prompts and pseudo-supervision, leading to better performance in LLMs across diverse applications while mitigating overfitting risks.

**Abstract:** Large language models (LLMs) have achieved great success across diverse tasks, and fine-tuning is sometimes needed to further enhance generation quality. Most existing methods rely on human supervision or parameter retraining, both of which are costly in terms of data collection and computational resources. To handle these challenges, a direct solution is to generate ``high-confidence'' data from unsupervised downstream tasks and use them for in-context prompting or prompt optimization to refine the pseudo-supervision. However, relying solely on such data may lead to overfitting. In this paper, we leverage the in-context learning (ICL) abilities of LLMs and propose a novel approach, pseudo-supervised demonstrations aligned prompt optimization (PAPO) algorithm, which jointly refines both the prompt and the overall pseudo-supervision. The proposed learning objective ensures that the optimized prompt guides the LLM to generate consistent responses for a given input when pseudo-supervised data from the downstream task are used as demonstrations, enabling refinement over the entire pseudo-supervision. The prompt is optimized by translating gradient signals into textual critiques, which serve as feedback to iteratively refine the prompt and model responses. Theoretical analysis in a simplified classification setting shows that the refined pseudo-supervision exhibits a geometric clustering structure, helping to mitigate overfitting. Experiments on question answering, natural language inference benchmarks, and a real-world molecule optimization task, show the effectiveness of the proposed algorithm.

</details>


### [305] [Lens: Rethinking Multilingual Enhancement for Large Language Models](https://arxiv.org/abs/2410.04407)

*Weixiang Zhao, Yulin Hu, Jiahe Guo, Xingyu Sui, Tongtong Wu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang Che, Ting Liu*

**Main category:** cs.CL

**Keywords:** multilingual large language models, language representation, computer science, natural language processing, AI access

**Relevance Score:** 9

**TL;DR:** The paper presents Lens, a novel methodology to enhance multilingual capabilities in LLMs without compromising English proficiency, utilizing internal language representation structures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited multilingual capabilities of LLMs, which primarily focus on English, and to provide better access to AI for non-English speakers.

**Method:** Lens leverages two subspaces: a language-agnostic subspace for aligning target languages with the central language and a language-specific subspace for preserving linguistic specificity among languages.

**Key Contributions:**

	1. Introduction of the Lens methodology for enhancing multilingual capabilities
	2. Demonstrating improved performance with reduced computational costs
	3. Maintaining English proficiency while expanding multilingual support

**Result:** Lens significantly improves multilingual performance on English-centric LLMs while maintaining high proficiency in English and requires less computational resources compared to traditional methods.

**Limitations:** 

**Conclusion:** The proposed approach outperforms existing post-training methods, enabling more efficient and effective multilingual capabilities in LLMs.

**Abstract:** As global demand for multilingual large language models (LLMs) grows, most LLMs still remain overly focused on English, leading to the limited access to advanced AI for non-English speakers. Current methods to enhance multilingual capabilities largely rely on data-driven post-training techniques, such as multilingual instruction tuning or continual pre-training. However, these approaches exhibit significant limitations, including high resource cost, exacerbation of off-target issue and catastrophic forgetting of central language abilities. To this end, we propose Lens, a novel approach that enhances multilingual capabilities by leveraging LLMs' internal language representation spaces. Lens operates on two subspaces: the language-agnostic subspace, where it aligns target languages with the central language to inherit strong semantic representations, and the language-specific subspace, where it separates target and central languages to preserve linguistic specificity. Experiments on three English-centric LLMs show that Lens significantly improves multilingual performance while maintaining the model's English proficiency, achieving better results with less computational cost compared to existing post-training approaches.

</details>


### [306] [PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs](https://arxiv.org/abs/2410.06704)

*Krishna Kanth Nakka, Ahmed Frikha, Ricardo Mendes, Xue Jiang, Xuebing Zhou*

**Main category:** cs.CL

**Keywords:** PII extraction, benchmark, LLMs, adversarial attacks, privacy

**Relevance Score:** 8

**TL;DR:** Presentation of PII-Scope, a benchmark for evaluating PII extraction attacks on LLMs, revealing vulnerabilities and offering insights into effective mitigation strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and understand the effectiveness of PII extraction attacks on large language models (LLMs) and to establish a reliable benchmark for these evaluations.

**Method:** Introduction of PII-Scope benchmark, experimentation with advanced adversarial strategies for PII extraction, and analysis of hyperparameters affecting attack outcomes.

**Key Contributions:**

	1. Introduction of the PII-Scope benchmark for PII extraction attacks
	2. Identification of key hyperparameters affecting PII extraction effectiveness
	3. Empirical demonstration of PII leakage increase under advanced adversarial strategies

**Result:** Demonstrated significant underestimation of PII leakage in single-query attacks, with rates increasing fivefold under sophisticated adversarial approaches targeting pretrained models.

**Limitations:** None mentioned; potential limitations could include the scope of models evaluated or the specific threat scenarios considered.

**Conclusion:** The study establishes a rigorous benchmark for assessing PII extraction attacks and highlights the increased vulnerability of finetuned models.

**Abstract:** In this work, we introduce PII-Scope, a comprehensive benchmark designed to evaluate state-of-the-art methodologies for PII extraction attacks targeting LLMs across diverse threat settings. Our study provides a deeper understanding of these attacks by uncovering several hyperparameters (e.g., demonstration selection) crucial to their effectiveness. Building on this understanding, we extend our study to more realistic attack scenarios, exploring PII attacks that employ advanced adversarial strategies, including repeated and diverse querying, and leveraging iterative learning for continual PII extraction. Through extensive experimentation, our results reveal a notable underestimation of PII leakage in existing single-query attacks. In fact, we show that with sophisticated adversarial capabilities and a limited query budget, PII extraction rates can increase by up to fivefold when targeting the pretrained model. Moreover, we evaluate PII leakage on finetuned models, showing that they are more vulnerable to leakage than pretrained models. Overall, our work establishes a rigorous empirical benchmark for PII extraction attacks in realistic threat scenarios and provides a strong foundation for developing effective mitigation strategies.

</details>


### [307] [Stuffed Mamba: Oversized States Lead to the Inability to Forget](https://arxiv.org/abs/2410.07145)

*Yingfa Chen, Xinrong Zhang, Shengding Hu, Xu Han, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Recurrent Neural Networks, Mamba, Context Length, Forgetting Mechanisms, Long-context Modeling

**Relevance Score:** 5

**TL;DR:** This paper exposes limitations in Mamba-based RNNs regarding their ability to forget early tokens, revealing that training on short contexts hampers their learning of forgetting mechanisms.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance degradation in recurrent neural networks (RNNs) related to forgetting earlier tokens when encoding long contexts.

**Method:** Analyzed Mamba-based models to understand their token forgetting mechanisms and the relationship between training length, state size, and context retention.

**Key Contributions:**

	1. Revealed that Mamba models struggle with forgetting mechanisms during long-context tasks.
	2. Identified the relationship between training length, state size, and forgetting in RNNs.
	3. Provided insights for improving design strategies in RNN architectures for better long-context handling.

**Result:** Demonstrated that effective token forgetting is hindered by training on insufficiently short contexts and identified linear and exponential scaling of training length and context length with state size.

**Limitations:** Focused on specific architectures and context lengths; applicability to other architectures not discussed.

**Conclusion:** Future RNN designs should consider the balance between state size, training context length, and forgetting capabilities to enhance their performance in long-context tasks.

**Abstract:** Recent advancements in recurrent architectures, such as Mamba and RWKV, have showcased strong language capabilities. Unlike transformer-based models, these architectures encode all contextual information into a fixed-size state, leading to great inference efficiency. However, this approach can cause information interference, where different token data conflicts, resulting in performance degradation and incoherent outputs beyond a certain context length. To prevent this, most RNNs incorporate mechanisms designed to "forget" earlier tokens. In this paper, we reveal that Mamba-based models struggle to effectively forget earlier tokens even with built-in forgetting mechanisms. We demonstrate that this issue stems from training on contexts that are too short for the state size, enabling the model to perform well without needing to learn how to forget. Then, we show that the minimum training length required for the model to learn forgetting scales linearly with the state size, and the maximum context length for accurate retrieval of a 5-digit passkey scales exponentially with the state size, indicating that the model retains some information beyond the point where forgetting begins. These findings highlight a critical limitation in current RNN architectures and provide valuable insights for improving long-context modeling. Our work suggests that future RNN designs must account for the interplay between state size, training length, and forgetting mechanisms to achieve robust performance in long-context tasks.

</details>


### [308] [LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts](https://arxiv.org/abs/2410.10700)

*Qibing Ren, Hao Li, Dongrui Liu, Zhanxu Xie, Xiaoya Lu, Yu Qiao, Lei Sha, Junchi Yan, Lizhuang Ma, Jing Shao*

**Main category:** cs.CL

**Keywords:** Safety in LLMs, Natural distribution shifts, ActorBreaker

**Relevance Score:** 9

**TL;DR:** This paper identifies a new safety vulnerability in large language models (LLMs) due to natural distribution shifts and proposes a novel attack method called ActorBreaker to exploit this vulnerability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address safety concerns arising from large language models (LLMs) exposed to harmful data during pre-training, focusing on their vulnerability to natural distribution shifts.

**Method:** Introduction of ActorBreaker, an attack method that identifies relationships with toxic prompts to craft multi-turn prompts that reveal unsafe content in LLMs, grounded in Latour's actor-network theory.

**Key Contributions:**

	1. Introduction of ActorBreaker attack method
	2. Identification of natural distribution shifts as a safety vulnerability
	3. Development of a multi-turn safety dataset for fine-tuning LLMs

**Result:** ActorBreaker outperforms existing attack methods in diversity, effectiveness, and efficiency, leading to significant enhancements in the robustness of LLMs when fine-tuned on the developed multi-turn safety dataset.

**Limitations:** Some trade-offs between robustness and utility in fine-tuned models.

**Conclusion:** Proposes expanding safety training to cover a broader semantic space of toxic content, demonstrating improvements in model robustness with some trade-offs in utility.

**Abstract:** Safety concerns in large language models (LLMs) have gained significant attention due to their exposure to potentially harmful data during pre-training. In this paper, we identify a new safety vulnerability in LLMs: their susceptibility to \textit{natural distribution shifts} between attack prompts and original toxic prompts, where seemingly benign prompts, semantically related to harmful content, can bypass safety mechanisms. To explore this issue, we introduce a novel attack method, \textit{ActorBreaker}, which identifies actors related to toxic prompts within pre-training distribution to craft multi-turn prompts that gradually lead LLMs to reveal unsafe content. ActorBreaker is grounded in Latour's actor-network theory, encompassing both human and non-human actors to capture a broader range of vulnerabilities. Our experimental results demonstrate that ActorBreaker outperforms existing attack methods in terms of diversity, effectiveness, and efficiency across aligned LLMs. To address this vulnerability, we propose expanding safety training to cover a broader semantic space of toxic content. We thus construct a multi-turn safety dataset using ActorBreaker. Fine-tuning models on our dataset shows significant improvements in robustness, though with some trade-offs in utility. Code is available at https://github.com/AI45Lab/ActorAttack.

</details>


### [309] [Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up](https://arxiv.org/abs/2410.12323)

*Jiahao Yuan, Dehui Du, Hao Zhang, Zixiang Di, Usman Naseem*

**Main category:** cs.CL

**Keywords:** large language models, logical reasoning, cognitive preferences, reverse reasoning, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces Reversal of Thought (RoT), a framework aimed at enhancing the logical reasoning abilities of large language models (LLMs) during the warm-up phase before batch inference, achieving better reasoning accuracy and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models in mathematical and complex logical reasoning by providing a cost-effective improvement framework.

**Method:** RoT employs a Preference-Guided Reverse Reasoning strategy that uses logical symbols for pseudocode planning and meta-cognitive mechanisms for generating task-specific prompts through demonstrations, improving LLM cognitive preferences shaped by RLHF.

**Key Contributions:**

	1. Introduction of a new framework (RoT) to improve LLM reasoning capabilities.
	2. Use of Preference-Guided Reverse Reasoning for task-specific prompt generation.
	3. Demonstrated efficacy across various reasoning tasks compared to existing methods.

**Result:** RoT shows improved reasoning accuracy and efficiency in various tasks compared to existing baselines.

**Limitations:** 

**Conclusion:** RoT outperforms traditional methods by avoiding increased computational costs and rigidity while enhancing LLM reasoning capabilities.

**Abstract:** Large language models (LLMs) have shown remarkable performance in reasoning tasks but face limitations in mathematical and complex logical reasoning. Existing methods to improve LLMs' logical capabilities either involve traceable or verifiable logical sequences that generate more reliable responses by constructing logical structures yet increase computational costs, or introduces rigid logic template rules, reducing flexibility. In this paper, we propose Reversal of Thought (RoT), a plug-and-play and cost-effective reasoning framework designed to enhance the logical reasoning abilities of LLMs during the warm-up phase prior to batch inference. RoT utilizes a Preference-Guided Reverse Reasoning warm-up strategy, which integrates logical symbols for pseudocode planning through meta-cognitive mechanisms and pairwise preference self-evaluation to generate task-specific prompts solely through demonstrations, aligning with LLMs' cognitive preferences shaped by RLHF. Through reverse reasoning, we utilize a Cognitive Preference Manager to assess knowledge boundaries and further expand LLMs' reasoning capabilities by aggregating solution logic for known tasks and stylistic templates for unknown tasks. Experiments across various tasks demonstrate that RoT surpasses existing baselines in both reasoning accuracy and efficiency.

</details>


### [310] [Conformity in Large Language Models](https://arxiv.org/abs/2410.12428)

*Xiaochen Zhu, Caiqi Zhang, Tom Stafford, Nigel Collier, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** conformity effect, large language models, decision-making, psychological experiments, language model robustness

**Relevance Score:** 8

**TL;DR:** This paper investigates conformity effects in large language models (LLMs) and offers strategies to mitigate this bias.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the conformity effect is essential as LLMs are increasingly utilized in decision-making tasks, where biased responses could impact productivity.

**Method:** Psychological experiments adapted to evaluate the conformity of various LLMs, measuring their responses to majority opinions across different knowledge domains.

**Key Contributions:**

	1. First to demonstrate LLMs' conformity effects under uncertainty.
	2. Explored factors influencing conformity, such as training paradigms.
	3. Proposed interventions to mitigate conformity in LLMs.

**Result:** All tested LLMs demonstrated a tendency to conform to majority responses, especially when uncertain; instruction-tuned models showed less conformity.

**Limitations:** Does not extensively cover the impact of LLMs in real-world applications beyond the experimental context.

**Conclusion:** Interventions like Devil's Advocate and Question Distillation can help create more robust LLMs by reducing conformity bias.

**Abstract:** The conformity effect describes the tendency of individuals to align their responses with the majority. Studying this bias in large language models (LLMs) is crucial, as LLMs are increasingly used in various information-seeking and decision-making tasks as conversation partners to improve productivity. Thus, conformity to incorrect responses can compromise their effectiveness. In this paper, we adapt psychological experiments to examine the extent of conformity in popular LLMs. Our findings reveal that all tested models exhibit varying levels of conformity toward the majority, regardless of their initial choice or correctness, across different knowledge domains. Notably, we are the first to show that LLMs are more likely to conform when they are more uncertain in their own prediction. We further explore factors that influence conformity, such as training paradigms and input characteristics, finding that instruction-tuned models are less susceptible to conformity, while increasing the naturalness of majority tones amplifies conformity. Finally, we propose two interventions, Devil's Advocate and Question Distillation, to mitigate conformity, providing insights into building more robust language models.

</details>


### [311] [SynapticRAG: Enhancing Temporal Memory Retrieval in Large Language Models through Synaptic Mechanisms](https://arxiv.org/abs/2410.13553)

*Yuki Hou, Haruki Tamoto, Qinghua Zhao, Homei Miyashita*

**Main category:** cs.CL

**Keywords:** memory retrieval, language models, temporal association, cognitive science, conversational systems

**Relevance Score:** 9

**TL;DR:** The paper introduces SynapticRAG, a novel memory retrieval method for Large Language Models that combines temporal association triggers with synaptic propagation, resulting in improved accuracy in handling temporally distributed conversations.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing retrieval methods struggle with accuracy in temporally distributed conversations, primarily due to their reliance on simple similarity-based retrieval.

**Method:** SynapticRAG uniquely combines temporal association triggers with biologically-inspired synaptic propagation mechanisms to manage memory retrieval in dialogue systems.

**Key Contributions:**

	1. Introduction of SynapticRAG for improved memory retrieval
	2. Combination of temporal triggers with synaptic mechanisms
	3. Demonstrated significant performance improvements across multiple languages and datasets

**Result:** Experiments show SynapticRAG achieves consistent accuracy improvements over state-of-the-art memory retrieval methods by up to 14.66% points across four datasets in multiple languages.

**Limitations:** 

**Conclusion:** This work bridges cognitive science and language model development, offering a new framework for effective memory management in conversational systems.

**Abstract:** Existing retrieval methods in Large Language Models show degradation in accuracy when handling temporally distributed conversations, primarily due to their reliance on simple similarity-based retrieval. Unlike existing memory retrieval methods that rely solely on semantic similarity, we propose SynapticRAG, which uniquely combines temporal association triggers with biologically-inspired synaptic propagation mechanisms. Our approach uses temporal association triggers and synaptic-like stimulus propagation to identify relevant dialogue histories. A dynamic leaky integrate-and-fire mechanism then selects the most contextually appropriate memories. Experiments on four datasets of English, Chinese and Japanese show that compared to state-of-the-art memory retrieval methods, SynapticRAG achieves consistent improvements across multiple metrics up to 14.66% points. This work bridges the gap between cognitive science and language model development, providing a new framework for memory management in conversational systems.

</details>


### [312] [RESTOR: Knowledge Recovery in Machine Unlearning](https://arxiv.org/abs/2411.00204)

*Keivan Rezaei, Khyathi Chandu, Soheil Feizi, Yejin Choi, Faeze Brahman, Abhilasha Ravichander*

**Main category:** cs.CL

**Keywords:** machine unlearning, large language models, data erasure

**Relevance Score:** 6

**TL;DR:** The paper introduces the RESTOR framework for evaluating machine unlearning algorithms, focusing on effective data erasure and knowledge recovery.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of evaluating existing machine unlearning algorithms, ensuring they can effectively erase specific data points while recovering lost knowledge.

**Method:** The RESTOR framework assesses unlearning by evaluating models' ability to forget specific datapoints and recover their knowledge state as if those datapoints were never encountered.

**Key Contributions:**

	1. Introduction of the RESTOR framework for unlearning evaluation
	2. Insights into popular unlearning algorithms' mechanisms
	3. Identification of recovery mechanisms and improvements through target localization

**Result:** RESTOR reveals insights about popular unlearning algorithms, showing that some focus on forgetting without recovery, and identifies that localizing unlearning targets can improve performance.

**Limitations:** 

**Conclusion:** The framework provides a more robust evaluation of unlearning algorithms, contributing to understanding their effectiveness in data erasure.

**Abstract:** Large language models trained on web-scale corpora can memorize undesirable data containing misinformation, copyrighted material, or private or sensitive information. Recently, several machine unlearning algorithms have been proposed to eliminate the effect of such datapoints from trained models -- that is, to approximate a model that had never been trained on these datapoints in the first place. However, evaluating the effectiveness of unlearning algorithms remains an open challenge. Previous work has relied on heuristics -- such as verifying that the model can no longer reproduce the specific information targeted for removal while maintaining accuracy on unrelated test data. These approaches inadequately capture the complete effect of reversing the influence of datapoints on a trained model. In this work, we propose the RESTOR framework for machine unlearning evaluation, which assesses the ability of unlearning algorithms for targeted data erasure, by evaluating the ability of models to forget the knowledge introduced in these datapoints, while simultaneously recovering the model's knowledge state had it never encountered these datapoints. RESTOR helps uncover several novel insights about popular unlearning algorithms, and the mechanisms through which they operate -- for instance, identifying that some algorithms merely emphasize forgetting but not recovering knowledge, and that localizing unlearning targets can enhance unlearning performance.

</details>


### [313] [Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models](https://arxiv.org/abs/2411.02083)

*Jonas Zausinger, Lars Pennig, Anamarija Kozina, Sean Sdahl, Julian Sikora, Adrian Dendorfer, Timofey Kuznetsov, Mohamad Hagog, Nina Wiedemann, Kacper Chlodny, Vincent Limbach, Anna Ketteler, Thorben Prein, Vishwa Mohan Singh, Michael Morris Danziger, Jannis Born*

**Main category:** cs.CL

**Keywords:** Number Token Loss, Quantitative Reasoning, Language Models

**Relevance Score:** 8

**TL;DR:** This paper introduces a new loss function, Number Token Loss (NTL), designed to improve numerical reasoning in language models by directly minimizing the distance between predicted and actual numerical values.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of language models in quantitative reasoning tasks, particularly those involving arithmetic and numerical outputs.

**Method:** NTL is a regression-like loss that focuses on token level, minimizing either the Lp norm or the Wasserstein distance between real and predicted number tokens, which can be easily integrated into existing models.

**Key Contributions:**

	1. Introduction of Number Token Loss (NTL) for language models
	2. Proven effectiveness on mathematical tasks
	3. Compatibility with large-scale models (up to 3B parameters)

**Result:** The proposed NTL improves performance on various mathematical datasets and can match regression head performance in a direct comparison while being less resource-intensive.

**Limitations:** 

**Conclusion:** NTL can enhance the training objectives of large language models by providing a better framework for numerical prediction, encouraging its adoption by LLM developers.

**Abstract:** While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving quantitative reasoning, especially arithmetic. One fundamental limitation is the nature of the Cross Entropy loss, which assumes a nominal scale and thus cannot convey proximity between generated number tokens. In response, we here present a regression-like loss that operates purely on token level. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes either the Lp norm or the Wasserstein distance between the numerical values of the real and predicted number tokens. NTL can easily be added to any language model and extend the Cross Entropy objective during training without runtime overhead. We evaluate the proposed scheme on various mathematical datasets and find that it consistently improves performance in math-related tasks. In a direct comparison on a regression task, we find that NTL can match the performance of a regression head, despite operating on token level. Finally, we scale NTL up to 3B parameter models and observe improved performance, demonstrating its potential for seamless integration into LLMs. We hope that this work can inspire LLM developers to improve their pretraining objectives. The code is available via: https://tum-ai.github.io/number-token-loss/

</details>


### [314] [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/abs/2411.02391)

*Yanzhe Zhang, Tao Yu, Diyi Yang*

**Main category:** cs.CL

**Keywords:** autonomous agents, adversarial pop-ups, vision and language models, task performance, defense techniques

**Relevance Score:** 8

**TL;DR:** This paper investigates adversarial attacks on autonomous agents powered by vision and language models, demonstrating how pop-up distractions can significantly impair their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore vulnerabilities in VLM agents that can be exploited through adversarial pop-ups, affecting their task performance.

**Method:** The authors integrated adversarial pop-ups into agent testing environments like OSWorld and VisualWebArena to evaluate their impact on agent performance.

**Key Contributions:**

	1. First to identify the vulnerability of VLM agents to adversarial pop-ups
	2. Quantifies the impact of these attacks on agent performance
	3. Evaluates commonly used defense techniques against these attacks.

**Result:** The study found an attack success rate of 86%, where agents clicked pop-ups instead of completing tasks, leading to a 47% decrease in task success rate.

**Limitations:** The study primarily focuses on a specific type of adversarial attack and may not account for other types of potential vulnerabilities in VLM agents.

**Conclusion:** The paper concludes that basic defense techniques are ineffective against these adversarial attacks, highlighting the need for more robust solutions.

**Abstract:** Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear. In this work, we demonstrate that VLM agents can be easily attacked by a set of carefully designed adversarial pop-ups, which human users would typically recognize and ignore. This distraction leads agents to click these pop-ups instead of performing their tasks as usual. Integrating these pop-ups into existing agent testing environments like OSWorld and VisualWebArena leads to an attack success rate (the frequency of the agent clicking the pop-ups) of 86% on average and decreases the task success rate by 47%. Basic defense techniques, such as asking the agent to ignore pop-ups or including an advertisement notice, are ineffective against the attack.

</details>


### [315] [Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent](https://arxiv.org/abs/2411.02937)

*Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Philip S. Yu, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** Multimodal Retrieval, Dynamic VQA, Self-Adaptive Planning, OmniSearch, Multimodal Knowledge Retrieval

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel Multimodal Retrieval Augmented Generation (mRAG) approach to address the hallucination problem in multimodal large language models (MLLMs) by proposing a self-adaptive planning agent, OmniSearch, and a new dynamic VQA dataset, Dyn-VQA.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing heuristic mRAGs in multimodal retrieval, specifically their non-adaptive and overloaded retrieval queries, which do not adequately handle complex, dynamic, and multi-modal questions.

**Method:** The authors constructed the Dyn-VQA dataset to include dynamic questions that require innovative retrieval strategies. They introduced OmniSearch, a self-adaptive planning agent that decomposes complex multimodal questions into manageable sub-questions for retrieval.

**Key Contributions:**

	1. Introduction of the Dyn-VQA dataset for dynamic question retrieval
	2. Development of the OmniSearch self-adaptive planning agent
	3. Demonstration of improved performance over heuristic mRAGs for multimodal question answering

**Result:** Experiments demonstrated that existing heuristic mRAGs perform poorly on dynamic questions. In contrast, OmniSearch effectively addressed these challenges by dynamically adjusting the retrieval process based on the question's requirements.

**Limitations:** 

**Conclusion:** The proposed OmniSearch significantly improves the handling of complex multimodal questions and sets a new direction for enhancing mRAG methodologies. The code and dataset will be shared publicly to facilitate further research.

**Abstract:** Multimodal Retrieval Augmented Generation (mRAG) plays an important role in mitigating the "hallucination" issue inherent in multimodal large language models (MLLMs). Although promising, existing heuristic mRAGs typically predefined fixed retrieval processes, which causes two issues: (1) Non-adaptive Retrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws cannot be adequately reflected by current knowledge-seeking visual question answering (VQA) datasets, since the most required knowledge can be readily obtained with a standard two-step retrieval. To bridge the dataset gap, we first construct Dyn-VQA dataset, consisting of three types of "dynamic" questions, which require complex knowledge retrieval strategies variable in query, tool, and time: (1) Questions with rapidly changing answers. (2) Questions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments on Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient and precisely relevant knowledge for dynamic questions due to their rigid retrieval processes. Hence, we further propose the first self-adaptive planning agent for multimodal retrieval, OmniSearch. The underlying idea is to emulate the human behavior in question solution which dynamically decomposes complex multimodal questions into sub-question chains with retrieval action. Extensive experiments prove the effectiveness of our OmniSearch, also provide direction for advancing mRAG. The code and dataset will be open-sourced at https://github.com/Alibaba-NLP/OmniSearch.

</details>


### [316] [Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries](https://arxiv.org/abs/2411.07237)

*Chaitanya Malaviya, Joseph Chee Chang, Dan Roth, Mohit Iyyer, Mark Yatskar, Kyle Lo*

**Main category:** cs.CL

**Keywords:** Language Models, Evaluation, User Context, Bias, Contextualization

**Relevance Score:** 9

**TL;DR:** This paper introduces a new evaluation protocol called contextualized evaluations for assessing language model responses to underspecified queries by simulating user context, significantly impacting evaluation outcomes and revealing biases in model behaviors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need arises from the challenge of evaluating language model responses to queries that are not well-defined, where the user context, intent, and criteria for useful answers are often not clear.

**Method:** The authors develop the contextualized evaluations protocol which constructs synthetic context around underspecified queries to guide evaluators during the assessment process.

**Key Contributions:**

	1. Introduction of contextualized evaluations protocol
	2. Revealed biases in language models towards WEIRD contexts
	3. Demonstrated influence of user context on evaluation outcomes

**Result:** The study demonstrates that including context can significantly change evaluation outcomes, affect the ranking of models, reduce reliance on superficial criteria, and expose biases towards certain user contexts in language model outputs.

**Limitations:** 

**Conclusion:** The findings suggest that better evaluation strategies can enhance our understanding of language models and highlight the inconsistencies in their responses based on user context.

**Abstract:** Language model users often issue queries that lack specification, where the context under which a query was issued -- such as the user's identity, the query's intent, and the criteria for a response to be useful -- is not explicit. For instance, a good response to a subjective query like "What book should I read next?" would depend on the user's preferences, and a good response to an open-ended query like "How do antibiotics work against bacteria?" would depend on the user's expertise. This makes evaluation of responses to such queries an ill-posed task, as evaluators may make arbitrary judgments about the response quality. To remedy this, we present contextualized evaluations, a protocol that synthetically constructs context surrounding an underspecified query and provides it during evaluation. We find that the presence of context can 1) alter conclusions drawn from evaluation, even flipping benchmark rankings between model pairs, 2) nudge evaluators to make fewer judgments based on surface-level criteria, like style, and 3) provide new insights about model behavior across diverse contexts. Specifically, our procedure suggests a potential bias towards WEIRD (Western, Educated, Industrialized, Rich and Democratic) contexts in models' "default" responses and we find that models are not equally sensitive to following different contexts, even when they are provided in prompts.

</details>


### [317] [SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs](https://arxiv.org/abs/2411.07965)

*Chuyi Kong, Ziyang Luo, Hongzhan Lin, Zhiyuan Fan, Yaxin Fan, Yuxi Sun, Jing Ma*

**Main category:** cs.CL

**Keywords:** Large Language Models, interactive hallucination, role-playing, benchmark, commonsense knowledge

**Relevance Score:** 9

**TL;DR:** This paper introduces SHARP, a paradigm to explore interactive hallucination in LLMs through stance transfer and benchmarking based on commonsense knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings in existing research on social interactions of LLMs, particularly in relation to hallucination and character fidelity.

**Method:** The paper defines interactive hallucination and constructs the SHARP benchmark using relations from knowledge graphs to simulate multi-role interactions.

**Key Contributions:**

	1. Introduction of the SHARP benchmark for interactive role-playing in LLMs
	2. Defining interactive hallucination through stance transfer
	3. Challenging existing hallucination mitigation methods.

**Result:** Extensive experiments validate the effectiveness and stability of the SHARP paradigm while identifying influencing factors and challenging existing hallucination mitigation strategies.

**Limitations:** Existing post-training methods can conceal knowledge under stylistic behavior, leading to less informative outputs.

**Conclusion:** The findings reveal significant limitations in post-training methods for role-playing LLMs, highlighting a propensity for these models to prioritize style over knowledge.

**Abstract:** The advanced role-playing capabilities of Large Language Models (LLMs) have enabled rich interactive scenarios, yet existing research in social interactions neglects hallucination while struggling with poor generalizability and implicit character fidelity judgments. To bridge this gap, motivated by human behaviour, we introduce a generalizable and explicit paradigm for uncovering interactive patterns of LLMs across diverse worldviews. Specifically, we first define interactive hallucination through stance transfer, then construct SHARP, a benchmark built by extracting relations from commonsense knowledge graphs and utilizing LLMs' inherent hallucination properties to simulate multi-role interactions. Extensive experiments confirm our paradigm's effectiveness and stability, examine the factors that influence these metrics, and challenge conventional hallucination mitigation solutions. More broadly, our work reveals a fundamental limitation in popular post-training methods for role-playing LLMs: the tendency to obscure knowledge beneath style, resulting in monotonous yet human-like behaviors - interactive hallucination.

</details>


### [318] [On the Compatibility of Generative AI and Generative Linguistics](https://arxiv.org/abs/2411.10533)

*Eva Portelance, Masoud Jasbi*

**Main category:** cs.CL

**Keywords:** generative linguistics, language models, AI, Chomsky, Universal Grammar

**Relevance Score:** 4

**TL;DR:** This paper discusses the compatibility of generative AI with generative linguistics, arguing that language models (LMs) align with and can enhance foundational theories established by Noam Chomsky.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the relationship between generative AI and generative linguistics, particularly in light of advances in neural language models.

**Method:** The paper presents a theoretical argument supporting the compatibility of LMs with generative linguistics, discussing their roles in language modeling, discovery procedures, and Universal Grammar.

**Key Contributions:**

	1. Demonstration of LMs as formal generative models per Chomsky's theory
	2. Development of discovery procedures relevant to generative linguistics
	3. Support for the minimalist approach to Universal Grammar

**Result:** The authors find that generative AI reinforces Chomsky's principles by demonstrating that LMs are formal generative models, can inform discovery procedures, and support the minimalist approach to Universal Grammar.

**Limitations:** 

**Conclusion:** Generative linguistics can serve as a foundation for assessing and improving LMs, suggesting a symbiotic relationship between the two fields.

**Abstract:** In mid-20th century, the linguist Noam Chomsky established generative linguistics, and made significant contributions to linguistics, computer science, and cognitive science by developing the computational and philosophical foundations for a theory that defined language as a formal system, instantiated in human minds or artificial machines. These developments in turn ushered a wave of research on symbolic Artificial Intelligence (AI). More recently, a new wave of non-symbolic AI has emerged with neural Language Models (LMs) that exhibit impressive linguistic performance, leading many to question the older approach and wonder about the the compatibility of generative AI and generative linguistics. In this paper, we argue that generative AI is compatible with generative linguistics and reinforces its basic tenets in at least three ways. First, we argue that LMs are formal generative models as intended originally in Chomsky's work on formal language theory. Second, LMs can help develop a program for discovery procedures as defined by Chomsky's "Syntactic Structures". Third, LMs can be a major asset for Chomsky's minimalist approach to Universal Grammar and language acquisition. In turn, generative linguistics can provide the foundation for evaluating and improving LMs as well as other generative computational models of language.

</details>


### [319] [Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?](https://arxiv.org/abs/2411.15821)

*Aryan Sajith, Krishna Chaitanya Rao Kathala*

**Main category:** cs.CL

**Keywords:** small language models, training data quality, data duplication, AI accessibility, environmental impact

**Relevance Score:** 7

**TL;DR:** The study evaluates the impact of training data quality versus quantity on small language models, revealing that data quality is more critical for performance, and excessive duplication harms accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how training data quality and quantity influence the performance of small language models and assess implications for accessibility and sustainability.

**Method:** Empirical analysis using the TinyStories dataset, varying dataset size and duplication rates, with performance measured through validation loss, accuracy, and perplexity.

**Key Contributions:**

	1. Demonstrated the superiority of training data quality over quantity for SLMs
	2. Provided empirical data on the effects of data duplication rates on model performance
	3. Highlighted the implications for accessibility and sustainability in AI training

**Result:** Training data quality significantly affects SLM performance; minimal duplication improves accuracy while excessive duplication severely degrades it.

**Limitations:** The study is limited to small language models and specific dataset variations; results may not generalize to larger models or other datasets.

**Conclusion:** The findings indicate that prioritizing data quality over quantity can reduce costs and environmental impact, making advanced AI more accessible.

**Abstract:** This study investigates the relative impact of training data quality versus quantity on the performance of small language models (SLMs), utilizing the TinyStories dataset for empirical analysis. Analysis of dataset variations with respect to size (25% and 50% of the original size) and duplication (controlled rates of 25%, 50%, 75%, and 100%) were performed. Model performance was evaluated based on the validation loss, accuracy, and perplexity metrics. Results indicate training data quality plays a more significant role in the overall performance of SLMs, especially given scale of this experiment. Minimal duplication positively impacted model accuracy (+0.87% increase in accuracy at 25% duplication) without significantly increasing perplexity (+0.52% increase going from 0% to 25% duplication) but excessive duplication led to pronounced performance degradation (-40% drop in accuracy at 100% duplication). The implications of this exploration extend beyond just model performance; training large-scale models imposes significant financial and computational burdens, which can be prohibitive for organizations, individuals, and the public at large, especially in developing countries. Additionally, the energy consumption associated with large-scale training raises environmental concerns. Understanding the relative importance of data quality versus quantity could democratize AI technology, making advanced models more accessible and sustainable for all.

</details>


### [320] [Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning](https://arxiv.org/abs/2411.17679)

*Zhu Xu, Zhiqiang Zhao, Zihan Zhang, Yuchi Liu, Quanwei Shen, Fei Liu, Yu Kuang, Jian He, Conglin Liu*

**Main category:** cs.CL

**Keywords:** Tokenization, Large Language Models, Character Position Awareness, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces Token Internal Position Awareness (TIPA), enhancing LLMs' ability to predict character positions, particularly improving tasks like Chinese Spelling Correction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing tokenization methods obscure internal character structures, limiting LLMs' precision in tasks requiring accurate character positioning.

**Method:** The authors propose TIPA, which allows models to learn character positions by training on reverse character prediction tasks using the tokenizer's vocabulary.

**Key Contributions:**

	1. Introduction of TIPA for character position awareness in LLMs.
	2. Application of reverse character prediction tasks to enhance model training.
	3. Demonstration of improved accuracy in character-level tasks like Chinese Spelling Correction.

**Result:** TIPA significantly improves position prediction accuracy in LLMs and also enhances performance on downstream tasks that benefit from character-level information.

**Limitations:** 

**Conclusion:** The versatility and effectiveness of TIPA were validated by its positive impact, even in tasks that do not require exact position prediction.

**Abstract:** Tokenization methods like Byte-Pair Encoding (BPE) enhance computational efficiency in large language models (LLMs) but often obscure internal character structures within tokens. This limitation hinders LLMs' ability to predict precise character positions, which is crucial in tasks like Chinese Spelling Correction (CSC) where identifying the positions of misspelled characters accelerates correction processes. We propose Token Internal Position Awareness (TIPA), a method that significantly improves models' ability to capture character positions within tokens by training them on reverse character prediction tasks using the tokenizer's vocabulary. Experiments demonstrate that TIPA enhances position prediction accuracy in LLMs, enabling more precise identification of target characters in original text. Furthermore, when applied to downstream tasks that do not require exact position prediction, TIPA still boosts performance in tasks needing character-level information, validating its versatility and effectiveness.

</details>


### [321] [Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation](https://arxiv.org/abs/2411.18337)

*T. G. D. K. Sumanathilaka, Nicholas Micallef, Julian Hough*

**Main category:** cs.CL

**Keywords:** Word Sense Disambiguation, Large Language Models, Lexical Ambiguity, Natural Language Processing, Prompt Augmentation

**Relevance Score:** 8

**TL;DR:** This study explores the use of Large Language Models to improve Word Sense Disambiguation through a novel approach that combines prompt augmentation and a knowledge base.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Lexical ambiguity in digital communications challenges traditional WSD methods and affects systems like translation and information retrieval.

**Method:** The study employs a novel prompt augmentation mechanism, incorporating POS tagging, synonyms, aspect-based filtering, and a few-shot Chain of Thought prompting approach.

**Key Contributions:**

	1. Novel prompt augmentation mechanism for WSD
	2. Integration of knowledge base with different sense interpretations
	3. Human-in-loop approach for better LLM guidance

**Result:** Significant improvement in performance in WSD tasks using FEWS test data and sense tags.

**Limitations:** 

**Conclusion:** The research enhances accurate word interpretation in social media and digital communications through the proposed approach.

**Abstract:** Ambiguous words are often found in modern digital communications. Lexical ambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due to limited data. Consequently, the efficiency of translation, information retrieval, and question-answering systems is hindered by these limitations. This study investigates the use of Large Language Models (LLMs) to improve WSD using a novel approach combining a systematic prompt augmentation mechanism with a knowledge base (KB) consisting of different sense interpretations. The proposed method incorporates a human-in-loop approach for prompt augmentation where prompt is supported by Part-of-Speech (POS) tagging, synonyms of ambiguous words, aspect-based sense filtering and few-shot prompting to guide the LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based approach, this work demonstrates a substantial improvement in performance. The evaluation was conducted using FEWS test data and sense tags. This research advances accurate word interpretation in social media and digital communication.

</details>


### [322] [Patent-CR: A Dataset for Patent Claim Revision](https://arxiv.org/abs/2412.02549)

*Lekang Jiang, Pascal A Scherz, Stephan Goetz*

**Main category:** cs.CL

**Keywords:** patent claims, large language models, dataset, legal standards, human-computer interaction

**Relevance Score:** 8

**TL;DR:** Patent-CR is the first dataset for patent claim revision, assessing various LLMs on their ability to improve rejected patent applications based on legal standards.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure patent claims fulfill legal criteria beyond mere novelty and inventiveness, including clarity, precision, and robustness, by creating a dataset for the patent claim revision task.

**Method:** The study evaluates various LLMs, including general, text revision, and domain-specific models, through human evaluations to compare effectiveness in revising patent claims.

**Key Contributions:**

	1. Introduction of the Patent-CR dataset for patent claim revision.
	2. Assessment of LLMs tailored for legal criteria in patent contexts.
	3. Insights on the correlation between automated evaluations and human judgment.

**Result:** LLMs frequently produce edits that do not effectively align with target revisions. Domain-specific models perform better, with GPT-4 showing the best results among tested models, though still not reaching full examination standards.

**Limitations:** Further revision is needed for LLM outputs to meet examination standards; the study is limited to English language patents.

**Conclusion:** The findings highlight discrepancies between automated and human evaluations in patent claim revisions, suggesting room for improvement in LLMs, particularly with fine-tuning for legal standards.

**Abstract:** This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models. Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions. In addition, domain-specific models and the method of fine-tuning show promising results. Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard. Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment. This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision.

</details>


### [323] [Interpretable Company Similarity with Sparse Autoencoders](https://arxiv.org/abs/2412.02605)

*Marco Molinari, Victor Shao, Luca Imeneo, Mateusz Mikolajczak, Vladimir Tregubiak, Abhimanyu Pandey, Sebastian Kuznetsov Ryder Torres Pereira*

**Main category:** cs.CL

**Keywords:** Sparse Autoencoders, Company Similarity, Large Language Models, Financial Analysis, Interpretability

**Relevance Score:** 4

**TL;DR:** This paper explores the use of Sparse Autoencoders (SAEs) for enhancing the interpretability of Large Language Models in determining company similarity, showing that SAEs outperform traditional classification methods and embeddings in financial applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of traditional sector and industry classifications in assessing company similarity, which are crucial in finance for risk management and portfolio diversification.

**Method:** The authors apply Sparse Autoencoders (SAEs) to clusters of company descriptions to extract interpretable features of companies and benchmark these features against traditional classifications and embeddings.

**Key Contributions:**

	1. Introduction of Sparse Autoencoders for interpreting LLM activations in finance
	2. Demonstration of SAEs as superior to traditional sector classifications and embeddings
	3. Validation of the interpretability of features generated by SAEs.

**Result:** The results indicate that SAE features outperform SIC codes, industry classifications, and embeddings in capturing fundamental characteristics of companies as shown by superior correlation with logged monthly returns and higher Sharpe ratios in trading strategies.

**Limitations:** 

**Conclusion:** The study concludes that SAEs provide deeper insights into company similarities and enhance interpretability, making them a valuable tool for financial analysis.

**Abstract:** Determining company similarity is a vital task in finance, underpinning risk management, hedging, and portfolio diversification. Practitioners often rely on sector and industry classifications such as SIC and GICS codes to gauge similarity, the former being used by the U.S. Securities and Exchange Commission (SEC), and the latter widely used by the investment community. Since these classifications lack granularity and need regular updating, using clusters of embeddings of company descriptions has been proposed as a potential alternative, but the lack of interpretability in token embeddings poses a significant barrier to adoption in high-stakes contexts. Sparse Autoencoders (SAEs) have shown promise in enhancing the interpretability of Large Language Models (LLMs) by decomposing Large Language Model (LLM) activations into interpretable features. Moreover, SAEs capture an LLM's internal representation of a company description, as opposed to semantic similarity alone, as is the case with embeddings. We apply SAEs to company descriptions, and obtain meaningful clusters of equities. We benchmark SAE features against SIC-codes, Industry codes, and Embeddings. Our results demonstrate that SAE features surpass sector classifications and embeddings in capturing fundamental company characteristics. This is evidenced by their superior performance in correlating logged monthly returns - a proxy for similarity - and generating higher Sharpe ratios in co-integration trading strategies, which underscores deeper fundamental similarities among companies. Finally, we verify the interpretability of our clusters, and demonstrate that sparse features form simple and interpretable explanations for our clusters.

</details>


### [324] [HARP: Hesitation-Aware Reframing in Transformer Inference Pass](https://arxiv.org/abs/2412.07282)

*Romain Storaï, Seung-won Hwang*

**Main category:** cs.CL

**Keywords:** HARP, Adaptive Computation, Transformer Models

**Relevance Score:** 8

**TL;DR:** Introducing HARP, a model-agnostic method to improve inference performance in Transformers by selectively applying additional computation during token generation when uncertainty is detected.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of large language models by addressing variable computational demands during inference steps.

**Method:** HARP selectively applies additional computation at points of uncertainty in token generation, inspired by human cognitive processes like hesitation and reframing.

**Key Contributions:**

	1. Model-agnostic framework
	2. Training-free implementation
	3. Performance improvement in inference speeds

**Result:** HARP shows performance improvements of up to +5.16% on various downstream tasks while maintaining inference times twice as fast as beam search.

**Limitations:** 

**Conclusion:** HARP offers significant gains with adaptive computation, providing insights into improving Transformer-based language models.

**Abstract:** This paper aims to improve the performance of large language models by addressing the variable computational demands in inference steps, where some tokens require more computational resources than others. We present HARP, a simple modification to "off-the-shelf" Transformer forward pass. Drawing from hesitation and the framing effect in decision-making, HARP selectively applies additional computation when the model encounters uncertainty during token generation. Our method mimics human cognitive processes by pausing at difficult decision points and reframing inputs for a different perspective. Unlike other approaches, HARP is model-agnostic, training-free, and easy to implement. We evaluate our method across various downstream tasks and model sizes, demonstrating performance improvements up to +5.16%. Notably, HARP achieves these gains while maintaining inference times twice faster than beam search. Simple and yet with significant gains, HARP provides insights into the potential of adaptive computation for enhancing the performance of Transformer-based language models.

</details>


### [325] [On the Limit of Language Models as Planning Formalizers](https://arxiv.org/abs/2412.09879)

*Cassie Huang, Li Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Formal Planning, PDDL, Human-Computer Interaction, Error Analysis

**Relevance Score:** 8

**TL;DR:** This paper evaluates the use of Large Language Models (LLMs) for generating formal representations of planning domains in PDDL, highlighting their capabilities and limitations in producing executable plans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the executable and verifiable planning capabilities of LLMs in grounded environments by generating complete PDDL representations from realistic descriptions.

**Method:** Systematic evaluation of LLMs to generate complete PDDL representations from descriptions of varying naturalness; analysis of performance across different model sizes and error patterns.

**Key Contributions:**

	1. Complete PDDL representation generation from natural language descriptions
	2. Empirical evidence demonstrating LLMs outperforming direct plan generation
	3. Detailed error analysis regarding LLM performance with varying description naturalness

**Result:** LLMs can effectively formalize descriptions as PDDL, outperforming those generating plans directly, though their performance decreases with more natural-sounding descriptions.

**Limitations:** Performance declines as descriptions become more natural-sounding, indicating potential gaps in understanding complex language input.

**Conclusion:** Despite their robustness to lexical changes, LLMs exhibit lowered performance with increased naturalness in descriptions, indicating areas for further improvement in formal planning abilities.

**Abstract:** Large Language Models have been found to create plans that are neither executable nor verifiable in grounded environments. An emerging line of work demonstrates success in using the LLM as a formalizer to generate a formal representation of the planning domain in some language, such as Planning Domain Definition Language (PDDL). This formal representation can be deterministically solved to find a plan. We systematically evaluate this methodology while bridging some major gaps. While previous work only generates a partial PDDL representation, given templated, and therefore unrealistic environment descriptions, we generate the complete representation given descriptions of various naturalness levels. Among an array of observations critical to improve LLMs' formal planning abilities, we note that most large enough models can effectively formalize descriptions as PDDL, outperforming those directly generating plans, while being robust to lexical perturbation. As the descriptions become more natural-sounding, we observe a decrease in performance and provide detailed error analysis.

</details>


### [326] [MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset](https://arxiv.org/abs/2412.10105)

*Sagi Shaier, George Arthur Baker, Chiranthan Sridhar, Lawrence E Hunter, Katharina von der Wense*

**Main category:** cs.CL

**Keywords:** Language Models, Education, Dataset, Machine Learning, Cloze-style

**Relevance Score:** 9

**TL;DR:** MALAMUTE is a new educational benchmark dataset designed to evaluate language models' knowledge in specific subjects, addressing limitations in existing benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure the effective and safe integration of language models in education, it is essential to evaluate their proficiency in granular areas of knowledge.

**Method:** MALAMUTE is a multilingual, template-free dataset composed of expert-written probes from 71 university textbooks in English, Spanish, and Polish, covering multiple domains and subdomains.

**Key Contributions:**

	1. Introduction of MALAMUTE as the first education-focused cloze-style dataset.
	2. Inclusion of a multilingual and template-free structure.
	3. Coverage of detailed subjects across multiple languages.
	4. Demonstration of knowledge gaps in language models through evaluation.

**Result:** The dataset includes 33,361 concepts and 116,887 prompts, revealing significant knowledge gaps in language models when assessed on specific subjects, despite overall proficiency.

**Limitations:** 

**Conclusion:** MALAMUTE highlights the necessity for improved educational evaluations of language models to enhance their application in real-world learning environments.

**Abstract:** Language models (LMs) have excelled in various broad domains. However, to ensure their safe and effective integration into real-world educational settings, they must demonstrate proficiency in specific, granular areas of knowledge. Existing cloze-style benchmarks, commonly used to evaluate LMs' knowledge, have three major limitations. They: 1) do not cover the educational domain; 2) typically focus on low-complexity, generic knowledge or broad domains, which do not adequately assess the models' knowledge in specific subjects; and 3) often rely on templates that can bias model predictions. Here, we introduce MALAMUTE, a multilingual, template-free, and highly granular probing dataset comprising expert-written, peer-reviewed probes from 71 university-level textbooks across three languages (English, Spanish, and Polish). MALAMUTE is the first education-based cloze-style dataset. It covers eight domains, each with up to 14 subdomains, further broken down into concepts and concept-based prompts, totaling 33,361 university curriculum concepts and 116,887 prompts. MALAMUTE's fine granularity, educational focus, and inclusion of both sentence-level and paragraph-level prompts make it an ideal tool for evaluating LMs' course-related knowledge. Our evaluation of masked and causal LMs on MALAMUTE shows that despite overall proficiency, they have significant gaps in knowledge when examined closely on specific subjects, hindering their safe use in classrooms and underscoring the need for further development.

</details>


### [327] [Rethinking Chain-of-Thought from the Perspective of Self-Training](https://arxiv.org/abs/2412.10827)

*Zongqian Wu, Baoduo Xu, Ruochen Cui, Mengmeng Zhan, Xiaofeng Zhu, Lei Feng*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought reasoning, Large Language Models, Self-training, Adaptive reasoning

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel framework to enhance Chain-of-Thought reasoning in LLMs, improving both reasoning performance and computational efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper explores the intersection of Chain-of-Thought reasoning and self-training to improve performance in large language models (LLMs).

**Method:** The proposed framework incorporates a task-specific prompt module for initial reasoning and an adaptive reasoning iteration module that refines the reasoning process.

**Key Contributions:**

	1. Introduction of a task-specific prompt module for optimization of reasoning processes.
	2. Development of an adaptive iteration module to refine reasoning and reduce over-reasoning.
	3. Demonstration of improved performance and computational efficiency through extensive experiments.

**Result:** Experiments show significant improvements in reasoning performance and efficiency compared to previous methods.

**Limitations:** 

**Conclusion:** The novel CoT framework effectively addresses the limitations of earlier methods and enhances reasoning capabilities in LLMs.

**Abstract:** Chain-of-thought (CoT) reasoning has emerged as an effective approach for activating latent capabilities in LLMs. Interestingly, we observe that both CoT reasoning and self-training share the core objective: iteratively leveraging model-generated information to progressively reduce prediction uncertainty. Building on this insight, we propose a novel CoT framework to improve reasoning performance. Our framework integrates two key components: (i) a task-specific prompt module that optimizes the initial reasoning process, and (ii) an adaptive reasoning iteration module that dynamically refines the reasoning process and addresses the limitations of previous CoT approaches, \ie over-reasoning and high similarity between consecutive reasoning iterations. Extensive experiments demonstrate that the proposed method achieves significant advantages in both performance and computational efficiency.

</details>


### [328] [Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models](https://arxiv.org/abs/2412.11041)

*Di Wu, Xin Lu, Yanyan Zhao, Bing Qin*

**Main category:** cs.CL

**Keywords:** large language models, safety alignment, fine-tuning, machine learning, safety benchmarks

**Relevance Score:** 9

**TL;DR:** Proposes IRR, a method for enhancing safety alignment in fine-tuned large language models by identifying and removing unsafe parameters while recalibrating the remaining ones.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the safety challenges posed by fine-tuning large language models, which often compromise their safety alignment.

**Method:** The proposed method, IRR, identifies and removes unsafe delta parameters from fine-tuned models and recalibrates retained parameters to improve safety alignment.

**Key Contributions:**

	1. Introduction of the IRR method for safety realignment in LLMs.
	2. Demonstrated effectiveness on safety benchmarks including harmful queries and jailbreak attacks.
	3. Maintained performance on downstream tasks alongside improved safety.

**Result:** IRR enhances the safety performance of fine-tuned models on various safety benchmarks while preserving their effectiveness on downstream tasks.

**Limitations:** 

**Conclusion:** The IRR method improves safety alignment in LLMs without degrading their performance on tasks, making it a valuable approach for safe model deployment.

**Abstract:** Although large language models (LLMs) achieve effective safety alignment at the time of release, they still face various safety challenges. A key issue is that fine-tuning often compromises the safety alignment of LLMs. To address this issue, we propose a method named IRR (Identify, Remove, and Recalibrate for Safety Realignment) that performs safety realignment for LLMs. The core of IRR is to identify and remove unsafe delta parameters from the fine-tuned models, while recalibrating the retained ones. We evaluate the effectiveness of IRR across various datasets, including both full fine-tuning and LoRA methods. Our results demonstrate that IRR significantly enhances the safety performance of fine-tuned models on safety benchmarks, such as harmful queries and jailbreak attacks, while maintaining their performance on downstream tasks. The source code is available at: https://anonymous.4open.science/r/IRR-BD4F.

</details>


### [329] [Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models](https://arxiv.org/abs/2412.11333)

*Xiaochen Zhu, Georgi Karadzhov, Chenxi Whitehouse, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** Diffusion Models, Text Generation, Segment Level, Adversarial Learning, Contrastive Learning

**Relevance Score:** 7

**TL;DR:** Segment-Level Diffusion (SLD) enhances text generation by improving long-form coherence and context accuracy through segmentation, robust representation training, and latent-space guidance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of traditional diffusion models in generating long, coherent, and contextually accurate text.

**Method:** SLD uses text segmentation and integrates adversarial and contrastive learning approaches to enhance the representation of long-form text and uses an autoregressive decoder for predictions.

**Key Contributions:**

	1. Introduction of Segment-Level Diffusion (SLD) framework.
	2. Utilization of adversarial and contrastive learning to enhance representation learning.
	3. Improved scalability and performance of long-form text generation.

**Result:** SLD demonstrates competitive or superior performance in fluency, coherence, and contextual compatibility across four datasets compared to existing diffusion and autoregressive models.

**Limitations:** 

**Conclusion:** The proposed framework significantly improves the abilities of diffusion models for long-form text generation tasks.

**Abstract:** Diffusion models have shown promise in text generation, but often struggle with generating long, coherent, and contextually accurate text. Token-level diffusion doesn't model word-order dependencies explicitly and operates on short, fixed output windows, while passage-level diffusion struggles with learning robust representations for long-form text. To address these challenges, we propose Segment-Level Diffusion (SLD), a framework that enhances diffusion-based text generation through text segmentation, robust representation training with adversarial and contrastive learning, and improved latent-space guidance. By segmenting long-form outputs into multiple latent representations and decoding them with an autoregressive decoder, SLD simplifies diffusion predictions and improves scalability. Experiments on four datasets demonstrate that, when compared to other diffusion and autoregressive baselines SLD achieves competitive or superior fluency, coherence, and contextual compatibility in automatic and human evaluations.

</details>


### [330] [Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models](https://arxiv.org/abs/2412.13328)

*Elvis Nunez, Luca Zancato, Benjamin Bowman, Aditya Golatkar, Wei Xia, Stefano Soatto*

**Main category:** cs.CL

**Keywords:** State Space Models, Attention Mechanism, Hybrid Models, Natural Language Processing, Long-range Dependencies

**Relevance Score:** 7

**TL;DR:** The paper introduces Span-Expanded Attention (SE-Attn) to enhance the memory capacity of Hybrid State Space Models (SSMs) by allowing access to tokens from beyond the current Attention span, and proposes a new fine-tuning method (HyLoRA) for efficient adaptation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Hybrid architectures struggle with recalling distant past tokens due to limitations in Attention span. This research aims to overcome these limitations by introducing a novel mechanism that allocates memory based on relevancy, thereby enhancing model performance on tasks requiring long-range dependencies.

**Method:** The authors propose Span-Expanded Attention (SE-Attn), which reserves part of the Attention context for tokens retrieved from beyond the current span, enabling hybrid models to access a wider range of memory. A fine-tuning method called HyLoRA is presented to adapt these models for longer token sequences efficiently.

**Key Contributions:**

	1. Introduction of Span-Expanded Attention (SE-Attn) for improved memory access in Hybrid SSMs.
	2. Novel fine-tuning method (HyLoRA) for adapting models to longer token sequences.
	3. Performance improvements in various natural language benchmarks with long-range dependencies.

**Result:** SE-Attn allows for the adaptation of pre-trained Hybrid models to sequences of tokens up to 8 times longer than their original pre-training, showing improved performance on natural language benchmarks with long-range dependencies.

**Limitations:** 

**Conclusion:** HyLoRA combined with SE-Attn is demonstrated to be more cost-effective and performant than alternatives like LongLoRA when applied to Hybrid models in natural language processing tasks.

**Abstract:** The "state" of State Space Models (SSMs) represents their memory, which fades exponentially over an unbounded span. By contrast, Attention-based models have "eidetic" (i.e., verbatim, or photographic) memory over a finite span (context size). Hybrid architectures combine State Space layers with Attention, but still cannot recall the distant past and can access only the most recent tokens eidetically. Unlike current methods of combining SSM and Attention layers, we allow the state to be allocated based on relevancy rather than recency. In this way, for every new set of query tokens, our models can "eidetically" access tokens from beyond the Attention span of current Hybrid SSMs without requiring extra hardware resources. We introduce a method to expand the memory span of the hybrid state by "reserving" a fraction of the Attention context for tokens retrieved from arbitrarily distant in the past, thus expanding the eidetic memory span of the overall state. We call this reserved fraction of tokens the "expansion span," and the mechanism to retrieve and aggregate it "Span-Expanded Attention" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a novel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows efficient adaptation on long spans of tokens. We show that SE-Attn enables us to efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8 times longer than the ones used for pre-training. We show that HyLoRA with SE-Attn is cheaper and more performant than alternatives like LongLoRA when applied to Hybrid models on natural language benchmarks with long-range dependencies, such as PG-19, RULER, and other common natural language downstream tasks.

</details>


### [331] [EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents](https://arxiv.org/abs/2412.13549)

*Cheng Qian, Peixuan Han, Qinyu Luo, Bingxiang He, Xiusi Chen, Yuji Zhang, Hongyi Du, Jiarui Yao, Xiaocheng Yang, Denghui Zhang, Yunzhu Li, Heng Ji*

**Main category:** cs.CL

**Keywords:** Language Models, Creative Reasoning, Benchmarking, Tool Use, Task Reflection

**Relevance Score:** 8

**TL;DR:** Introduction of EscapeBench, a benchmark for creative reasoning in language model agents, and EscapeAgent, a framework to enhance their performance in room escape scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for language models focus on explicit, goal-oriented tasks, neglecting creative adaptation and problem-solving in unfamiliar environments.

**Method:** Introduction of EscapeBench to evaluate creative reasoning and unconventional tool use in language models, and development of EscapeAgent to enhance performance through innovative tool use and task reflection.

**Key Contributions:**

	1. Introduction of EscapeBench benchmark for evaluating creativity in LM agents
	2. Development of EscapeAgent framework for enhancing creative reasoning
	3. Demonstration of significant improvement in task completion efficiency and success rates

**Result:** Current language models achieve only 15% average progress in EscapeBench without hints, indicating limitations in creativity. EscapeAgent improves performance, executing over 1,000 action steps with fewer hints and increased success rates.

**Limitations:** 

**Conclusion:** EscapeAgent enhances the creative problem-solving capabilities of language models in complex scenarios, indicating potential for better performance in non-linear, imaginative tasks.

**Abstract:** Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench, a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.

</details>


### [332] [Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings](https://arxiv.org/abs/2412.13879)

*Yuanhe Zhang, Zhenhong Zhou, Wei Zhang, Xinyue Wang, Xiaojun Jia, Yang Liu, Sen Su*

**Main category:** cs.CL

**Keywords:** Large Language Models, Denial-of-Service, black-box attacks, transferability, security defenses

**Relevance Score:** 4

**TL;DR:** The paper presents Auto-Generation for LLM-DoS (AutoDoS), an automated algorithm aimed at executing denial-of-service attacks on black-box Large Language Models (LLMs), significantly increasing service response latency and resource consumption.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in research focusing on black-box LLM Denial-of-Service (LLM-DoS) attacks, as most existing studies have concentrated on white-box attacks.

**Method:** The AutoDoS attack utilizes a constructed DoS Attack Tree and employs transferability-driven iterative optimization to achieve effective attacks on different models using a single prompt.

**Key Contributions:**

	1. Introduction of the AutoDoS attack against black-box LLMs
	2. Development of a DoS Attack Tree for systematic attack construction
	3. Demonstration of bypassing existing defenses using the Length Trojan

**Result:** Experimental results indicate that AutoDoS enhances service response latency by over 250 times, leading to significant GPU and memory resource consumption.

**Limitations:** The focus is exclusively on automated black-box LLM attacks, which may not encompass all possible attack scenarios and defenses.

**Conclusion:** AutoDoS provides insights into black-box LLM-DoS attacks and emphasizes the need for improved security defenses against such threats.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks yet still are vulnerable to external threats, particularly LLM Denial-of-Service (LLM-DoS) attacks. Specifically, LLM-DoS attacks aim to exhaust computational resources and block services. However, existing studies predominantly focus on white-box attacks, leaving black-box scenarios underexplored. In this paper, we introduce Auto-Generation for LLM-DoS (AutoDoS) attack, an automated algorithm designed for black-box LLMs. AutoDoS constructs the DoS Attack Tree and expands the node coverage to achieve effectiveness under black-box conditions. By transferability-driven iterative optimization, AutoDoS could work across different models in one prompt. Furthermore, we reveal that embedding the Length Trojan allows AutoDoS to bypass existing defenses more effectively. Experimental results show that AutoDoS significantly amplifies service response latency by over 250$\times\uparrow$, leading to severe resource consumption in terms of GPU utilization and memory usage. Our work provides a new perspective on LLM-DoS attacks and security defenses. Our code is available at https://github.com/shuita2333/AutoDoS.

</details>


### [333] [How to Synthesize Text Data without Model Collapse?](https://arxiv.org/abs/2412.14689)

*Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou*

**Main category:** cs.CL

**Keywords:** synthetic data, model collapse, token editing, language models, data synthesis

**Relevance Score:** 8

**TL;DR:** This paper investigates the challenges of model collapse in language models trained on synthetic data and proposes a method for generating improved semi-synthetic data through token editing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of AI models necessitates understanding the effect of synthetic data on model training and addressing the performance decline associated with model collapse.

**Method:** The authors pre-train language models using varying amounts of synthetic data, analyze the resulting model performance, and propose editing tokens in human-produced data to create semi-synthetic data that maintains performance.

**Key Contributions:**

	1. Identified the negative impact of synthetic data on model performance.
	2. Proposed token editing as a method to create semi-synthetic data.
	3. Validated theoretical proof through experiments showing improved model performance.

**Result:** A negative correlation was found between the proportion of synthetic data and language model performance. Token editing was shown to improve model performance and prevent model collapse, validated through extensive experiments.

**Limitations:** The study focuses on token-level editing and may not generalize to all forms of synthetic data generation.

**Conclusion:** Token-level editing of human data allows for the synthesis of quality semi-synthetic data that mitigates performance issues associated with synthetic data in language models.

**Abstract:** Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves model performance.

</details>


### [334] [DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs](https://arxiv.org/abs/2412.14838)

*Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding*

**Main category:** cs.CL

**Keywords:** KV cache management, LLM, adaptive strategies, token retention, dynamic optimization

**Relevance Score:** 9

**TL;DR:** DynamicKV optimizes token retention in LLMs for improved efficiency in long-context tasks, outperforming traditional methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve KV cache management for long-context tasks in LLMs by utilizing task-specific characteristics for optimal token retention.

**Method:** DynamicKV proposes a dynamic method for optimizing token retention by adjusting the number of tokens per layer based on specific task requirements, establishing global and per-layer KV cache budgets during inference.

**Key Contributions:**

	1. Introduces an adaptive strategy for KV cache management in LLMs
	2. Demonstrates substantial performance retention with minimal cache size
	3. Outperforms SOTA methods under extreme compression scenarios

**Result:** DynamicKV retains only 1.7% of the KV cache size while achieving ~85% of the performance of the Full KV cache on LongBench and surpasses SOTA methods by 11% under extreme compression.

**Limitations:** 

**Conclusion:** DynamicKV significantly enhances the efficiency of KV cache management in LLMs by using adaptive strategies tailored to individual task demands without sacrificing performance.

**Abstract:** Efficient KV cache management in LLMs is crucial for long-context tasks like RAG and summarization. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics and reducing the retention of essential information. However, we observe distinct activation patterns across layers in various tasks, highlighting the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer to adapt to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method retains only 1.7% of the KV cache size while achieving ~85% of the Full KV cache performance on LongBench. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released.

</details>


### [335] [ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty](https://arxiv.org/abs/2412.20251)

*Qing Zong, Zhaowei Wang, Tianshi Zheng, Xiyu Ren, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** LLM, knowledge robustness, benchmark, low-frequency entities, uncertainty

**Relevance Score:** 8

**TL;DR:** The paper introduces the ComparisonQA benchmark to assess LLMs' knowledge robustness, focusing on low-frequency entities and their impact on performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the limitations of current evaluations of LLMs, which do not adequately account for the differing difficulties of questions and low-frequency entities.

**Method:** Development of the ComparisonQA benchmark containing 283K questions, and a two-round evaluation method that measures correctness and uncertainty.

**Key Contributions:**

	1. Introduction of the ComparisonQA benchmark
	2. Demonstration of LLMs' low robustness on low-frequency entities
	3. Development of a method to select high-quality, hard questions

**Result:** LLMs show low robustness on low-frequency knowledge, and uncertainty effectively identifies higher quality questions. An automatic selection method was proposed for creating a hard subset of questions.

**Limitations:** 

**Conclusion:** The study highlights the challenges LLMs face with low-frequency knowledge and offers a structured approach to generate more reliable benchmarks for evaluating LLM performance.

**Abstract:** The rapid development of LLMs has sparked extensive research into their factual knowledge. Current works find that LLMs fall short on questions around low-frequency entities. However, such proofs are unreliable since the questions can differ not only in entity frequency but also in difficulty themselves. So we introduce ComparisonQA benchmark, containing 283K abstract questions, each instantiated by a pair of high-frequency and low-frequency entities. It ensures a controllable comparison to study the role of knowledge frequency in the performance of LLMs. Because the difference between such a pair is only the entity with different frequencies. In addition, we use both correctness and uncertainty to develop a two-round method to evaluate LLMs' knowledge robustness. It aims to avoid possible semantic shortcuts which is a serious problem of current QA study. Experiments reveal that LLMs, including GPT-4o, exhibit particularly low robustness regarding low-frequency knowledge. Besides, we find that uncertainty can be used to effectively identify high-quality and shortcut-free questions while maintaining the data size. Based on this, we propose an automatic method to select such questions to form a subset called ComparisonQA-Hard, containing only hard low-frequency questions.

</details>


### [336] [Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation](https://arxiv.org/abs/2501.02979)

*Zhi Qu, Yiran Wang, Jiannan Mao, Chenchen Ding, Hideki Tanaka, Masao Utiyama, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** multilingual neural machine translation, large language models, artificial tokens

**Relevance Score:** 6

**TL;DR:** This paper introduces a method called registering, which enables small multilingual neural machine translation (MNMT) models to perform comparably to large language models (LLMs) by using artificial tokens to indicate target languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The performance of MNMT-specific models lags behind LLMs, creating a need for methods that can enhance their capabilities.

**Method:** The method involves inserting artificial tokens (registers) into the input sequence and modifying the attention mask to focus on these tokens for target token generation.

**Key Contributions:**

	1. Introduction of the registering method for MNMT
	2. Development of MITRE models that outperform existing benchmarks
	3. Open-sourcing of models for community use

**Result:** Experiments using the EC-40 benchmark show significant advancements in MNMT performance, where the MITRE models outperform existing MNMT models and are comparable to commercial LLMs.

**Limitations:** 

**Conclusion:** The proposed registering method allows for better performance of MNMT models and the research includes open-sourced models for further exploration in the field.

**Abstract:** The multilingual neural machine translation (MNMT) aims for arbitrary translations across multiple languages. Although MNMT-specific models trained on parallel data offer low costs in training and deployment, their performance consistently lags behind that of large language models (LLMs). In this work, we introduce registering, a novel method that enables a small MNMT-specific model to compete with LLMs. Specifically, we insert a set of artificial tokens specifying the target language, called registers, into the input sequence between the source and target tokens. By modifying the attention mask, the target token generation only pays attention to the activation of registers, representing the source tokens in the target language space. Experiments on EC-40, a large-scale benchmark, show that our method advances the state-of-the-art of MNMT. We further pre-train two models, namely MITRE (multilingual translation with registers), by 9.3 billion sentence pairs across 24 languages collected from public corpora. One of them, MITRE-913M, outperforms NLLB-3.3B, achieves comparable performance with commercial LLMs, and shows strong adaptability in fine-tuning. Finally, we open-source our models to facilitate further research and development in MNMT: https://github.com/zhiqu22/mitre.

</details>


### [337] [OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis](https://arxiv.org/abs/2501.04561)

*Run Luo, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Yangyi Chen, Xiaobo Xia, Hamid Alinejad-Rokny, Fei Huang*

**Main category:** cs.CL

**Keywords:** omnimodal learning, speech generation, emotional speech synthesis, alignment, large language model

**Relevance Score:** 8

**TL;DR:** Introducing 
ame, an omnimodal large language model framework that enhances speech synthesis and alignment between text, image, and speech modalities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance open-source research in omnimodal learning and address the lack of quality datasets and limitations in real-time emotional speech synthesis.

**Method:** A two-stage training framework integrating omnimodal alignment and speech generation, where a pre-trained speech model is retrained on text-image tasks followed by training a lightweight decoder for speech tasks.

**Key Contributions:**

	1. Development of an omnimodal large language model with real-time speech generation capabilities.
	2. Integration of text-image tasks into speech model training for improved generalization.
	3. Achieving high fidelity in emotional speech synthesis while reducing model size and training data requirements.

**Result:** 
ame surpasses state-of-the-art models on various benchmarks, achieving a 4-point improvement on OmniBench and real-time speech generation with less latency and fewer training samples.

**Limitations:** 

**Conclusion:** The 
ame framework achieves significant gains in omnimodal learning and speech generation, demonstrating the effectiveness of combining alignment and speech synthesis techniques in real-time scenarios.

**Abstract:** Recent advancements in omnimodal learning have significantly improved understanding and generation across images, text, and speech, yet these developments remain predominantly confined to proprietary models. The lack of high-quality omnimodal datasets and the challenges of real-time emotional speech synthesis have notably hindered progress in open-source research. To address these limitations, we introduce \name, a two-stage training framework that integrates omnimodal alignment and speech generation to develop a state-of-the-art omnimodal large language model. In the alignment phase, a pre-trained speech model undergoes further training on text-image tasks, enabling (near) zero-shot generalization from vision to speech, outperforming models trained on tri-modal datasets. In the speech generation phase, a lightweight decoder is trained on speech tasks with direct preference optimization, enabling real-time emotional speech synthesis with high fidelity. Experiments show that \name surpasses state-of-the-art models across omnimodal, vision-language, and speech-language benchmarks. It achieves a 4-point absolute improvement on OmniBench over the leading open-source model VITA, despite using 5x fewer training samples and a smaller model size (7B vs. 7x8B). Additionally, \name achieves real-time speech generation with <1s latency at non-autoregressive mode, reducing inference time by 5x compared to autoregressive methods, and improves emotion classification accuracy by 7.7\%

</details>


### [338] [A partition cover approach to tokenization](https://arxiv.org/abs/2501.06246)

*Jia Peng Lim, Shawn Tan, Davin Choo, Hady W. Lauw*

**Main category:** cs.CL

**Keywords:** Tokenization, Natural Language Processing, GreedTok, Byte-Pair Encoding, Optimization

**Relevance Score:** 8

**TL;DR:** This paper formulates tokenization as an optimization problem, demonstrating its NP-hardness and proposing a new greedy algorithm, GreedTok, that outperforms traditional methods like BPE and Unigram in compression efficiency and language model training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Tokenization is crucial in NLP, affecting the performance of various applications. Current methods like Byte-Pair Encoding (BPE) have limitations that this work aims to address through a new formulation and algorithm.

**Method:** The authors reduce the tokenization problem to a known NP-hard problem and present GreedTok, a polynomial-time greedy algorithm. They also relate their approach to the weighted maximum coverage problem, leveraging existing approximation algorithms for comparison.

**Key Contributions:**

	1. Proposes a new greedy algorithm for tokenization (GreedTok)
	2. Formulates tokenization as an NP-hard optimization problem
	3. Demonstrates improved performance over existing algorithms in empirical evaluations

**Result:** Empirical evaluations show that GreedTok outperforms BPE and Unigram significantly in terms of compression and achieves a comparable covering score to GreedWMC, a known optimal solution for the relaxation.

**Limitations:** 

**Conclusion:** GreedTok not only enhances the tokenization process but also improves the performance of transformer-based language models by lowering the bit per byte cost

**Abstract:** Tokenization is the process of encoding strings into tokens of a fixed vocabulary size, and is widely utilized in Natural Language Processing applications. The leading tokenization algorithm today is Byte-Pair Encoding (BPE), which formulates the tokenization problem as a compression problem and tackles it by performing sequences of merges. In this work, we formulate tokenization as an optimization objective, show that it is NP-hard via a simple reduction from vertex cover, and propose a polynomial-time greedy algorithm GreedTok. Our formulation naturally relaxes to the well-studied weighted maximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm GreedWMC. Through empirical evaluations on real-world corpora, we show that GreedTok outperforms BPE and Unigram on compression and achieves a covering score comparable to GreedWMC. Finally, our extensive pre-training for two transformer-based language models with 1 billion parameters, comparing the choices of BPE and GreedTok as the tokenizer, shows that GreedTok achieves a lower bit per byte even when we control for either the total dataset proportion or total training tokens.

</details>


### [339] [Language Fusion for Parameter-Efficient Cross-lingual Transfer](https://arxiv.org/abs/2501.06892)

*Philipp Borchert, Ivan Vulić, Marie-Francine Moens, Jochen De Weerdt*

**Main category:** cs.CL

**Keywords:** multilingual NLP, cross-lingual transfer, low-rank adapters

**Relevance Score:** 8

**TL;DR:** FLARE proposes a novel method that enhances multilingual representation quality in NLP using low-rank adapters while maintaining computational efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The limited availability of multilingual text corpora leads to poor performance of language models on non-English tasks, necessitating better methods for cross-lingual transfer.

**Method:** FLARE integrates source and target language representations using low-rank (LoRA) adapters with lightweight linear transformations to improve representation quality and efficiency.

**Key Contributions:**

	1. Introduction of FLARE for improved cross-lingual performance
	2. Use of low-rank adapters to maintain parameter efficiency
	3. Demonstrated effectiveness across multiple NLP tasks

**Result:** FLARE improves performance by 4.9% for Llama 3.1 and 2.2% for Gemma~2 on question-answering tasks compared to standard LoRA fine-tuning, measured by the exact match metric.

**Limitations:** 

**Conclusion:** FLARE effectively enhances multilingual performance in NLP tasks while keeping parameter efficiency high.

**Abstract:** Limited availability of multilingual text corpora for training language models often leads to poor performance on downstream tasks due to undertrained representation spaces for languages other than English. This 'under-representation' has motivated recent cross-lingual transfer methods to leverage the English representation space by e.g. mixing English and 'non-English' tokens at the input level or extending model parameters to accommodate new languages. However, these approaches often come at the cost of increased computational complexity. We propose Fusion forLanguage Representations (FLARE) in adapters, a novel method that enhances representation quality and downstream performance for languages other than English while maintaining parameter efficiency. FLARE integrates source and target language representations within low-rank (LoRA) adapters using lightweight linear transformations, maintaining parameter efficiency while improving transfer performance. A series of experiments across representative cross-lingual natural language understanding tasks, including natural language inference, question-answering and sentiment analysis, demonstrate FLARE's effectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1 and 2.2% for Gemma~2 compared to standard LoRA fine-tuning on question-answering tasks, as measured by the exact match metric.

</details>


### [340] [Domain Adaptation of Foundation LLMs for e-Commerce](https://arxiv.org/abs/2501.09706)

*Christian Herold, Michael Kozielski, Tala Bazazo, Pavel Petrushkov, Patrycja Cieplicka, Dominika Basaj, Yannick Versley, Seyyed Hadi Hashemi, Shahram Khadivi*

**Main category:** cs.CL

**Keywords:** e-commerce, language models, Llama 3.1, domain adaptation, multilingual evaluation

**Relevance Score:** 4

**TL;DR:** Introduction of e-Llama models as foundation models for e-commerce applications, demonstrating adaptation techniques without losing general performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create large language models specifically tailored for the e-commerce domain while maintaining performance on general tasks.

**Method:** Continuous pretraining of Llama 3.1 models on 1 trillion tokens of domain-specific e-commerce data with hyperparameter tuning validated through ablation studies.

**Key Contributions:**

	1. Introduction of e-Llama models with 8B and 70B parameters for e-commerce
	2. Implementation of multilingual, e-commerce specific evaluation tasks
	3. Exploration of merging adapted and base models for performance control

**Result:** The e-Llama models show significant adaptation to the e-commerce domain and retain comparable performance on general domain tasks.

**Limitations:** 

**Conclusion:** Careful training setups allow for effective domain adaptation without sacrificing the general performance, and merging adapted and base models can optimize performance trade-offs.

**Abstract:** We present the e-Llama models: 8 billion and 70 billion parameter large language models that are adapted towards the e-commerce domain. These models are meant as foundation models with deep knowledge about e-commerce, that form a base for instruction- and fine-tuning. The e-Llama models are obtained by continuously pretraining the Llama 3.1 base models on 1 trillion tokens of domain-specific data.   We discuss our approach and motivate our choice of hyperparameters with a series of ablation studies. To quantify how well the models have been adapted to the e-commerce domain, we define and implement a set of multilingual, e-commerce specific evaluation tasks.   We show that, when carefully choosing the training setup, the Llama 3.1 models can be adapted towards the new domain without sacrificing significant performance on general domain tasks. We also explore the possibility of merging the adapted model and the base model for a better control of the performance trade-off between domains.

</details>


### [341] [iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use](https://arxiv.org/abs/2501.09766)

*Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, Ting Liu*

**Main category:** cs.CL

**Keywords:** Large language models, synthetic data, fine-tuning, machine learning, HCI

**Relevance Score:** 8

**TL;DR:** This paper proposes an iterative reinforced fine-tuning strategy to enhance large language models' (LLMs') capabilities by addressing limitations identified in synthetic data training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLM performance in complex tasks using external tool integration and to tackle the decay of training gains with increasing synthetic data.

**Method:** The proposed strategy involves enhancing response diversity using Monte Carlo Tree Search and iteratively identifying and optimizing model deficiencies through preference optimization algorithms.

**Key Contributions:**

	1. Proposed an iterative reinforced fine-tuning strategy for LLMs.
	2. Utilized Monte Carlo Tree Search for improving synthetic data response diversity.
	3. Developed fine-grained preference optimization for model improvement.

**Result:** The method shows a 13.11% performance improvement over a base model and a 6.5% improvement in complex scenarios compared to the baseline.

**Limitations:** The performance benefits observed may vary based on the complexity of tasks and the quality of synthetic data used.

**Conclusion:** An iterative reinforced fine-tuning approach significantly enhances LLM capabilities, particularly in complex tasks, compared to standard training methods.

**Abstract:** Augmenting large language models (LLMs) with external tools is a promising approach to enhance their capabilities, especially for complex tasks. Synthesizing tool-use data through real-world simulations is an effective way to achieve this. However, our investigation reveals that training gains significantly decay as synthetic data increases. The model struggles to benefit from more synthetic data, and it can not equip the model with advanced tool-use capabilities in complex scenarios. Moreover, we discovered that the above limitation usually manifests as a fragment deficiency (i.e., parameter errors) in response. To this end, we propose an iterative reinforced fine-tuning strategy designed to alleviate this limitation. This strategy involves: (1) enhancing the diversity of response for synthetic data through path exploration of Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency by constructing fine-grained preference pairs, and then improving it by preference optimization algorithms for targeted improvement. The experiments show that our method achieves 13.11% better performance than the same-size base model. It achieves an improvement of 6.5% in complex scenarios compared to the baseline, and it also outperforms larger open-source and closed-source models.

</details>


### [342] [Each Graph is a New Language: Graph Learning with LLMs](https://arxiv.org/abs/2501.11478)

*Huachi Zhou, Jiahe Du, Chuang Zhou, Chang Yang, Yilin Xiao, Yuxuan Xie, Xiao Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Graph Structures, Node Classification, GDL4LLM, Machine Learning

**Relevance Score:** 9

**TL;DR:** GDL4LLM is a novel framework that enables LLMs to effectively model graph structures for node classification by treating graphs as a language.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current models struggle with verbose and inadequate descriptions of high-order graph structures, limiting the effectiveness of LLMs in graph-related tasks.

**Method:** The GDL4LLM framework translates graphs into a graph language corpus for pre-training LLMs, allowing for efficient modeling of graph structures during fine-tuning.

**Key Contributions:**

	1. Proposes GDL4LLM framework for graphs.
	2. Transforms graph structures into graph language corpora.
	3. Achieves improved performance in node classification tasks.

**Result:** GDL4LLM shows superior performance compared to traditional description-based and textual attribute embeddings approaches on three real-world datasets.

**Limitations:** 

**Conclusion:** By treating graphs as a new language, GDL4LLM provides a concise and effective way for LLMs to understand and classify nodes in graph-structured data.

**Abstract:** Recent efforts leverage Large Language Models (LLMs) for modeling text-attributed graph structures in node classification tasks. These approaches describe graph structures for LLMs to understand or aggregate LLM-generated textual attribute embeddings through graph structure. However, these approaches face two main limitations in modeling graph structures with LLMs. (i) Graph descriptions become verbose in describing high-order graph structure. (ii) Textual attributes alone do not contain adequate graph structure information. It is challenging to model graph structure concisely and adequately with LLMs. LLMs lack built-in mechanisms to model graph structures directly. They also struggle with complex long-range dependencies between high-order nodes and target nodes.   Inspired by the observation that LLMs pre-trained on one language can achieve exceptional performance on another with minimal additional training, we propose \textbf{G}raph-\textbf{D}efined \textbf{L}anguage for \textbf{L}arge \textbf{L}anguage \textbf{M}odel (GDL4LLM). This novel framework enables LLMs to transfer their powerful language understanding capabilities to graph-structured data. GDL4LLM translates graphs into a graph language corpus instead of graph descriptions and pre-trains LLMs on this corpus to adequately understand graph structures. During fine-tuning, this corpus describes the structural information of target nodes concisely with only a few tokens. By treating graphs as a new language, GDL4LLM enables LLMs to model graph structures adequately and concisely for node classification tasks. Extensive experiments on three real-world datasets demonstrate that GDL4LLM outperforms description-based and textual attribute embeddings-based baselines by efficiently modeling different orders of graph structure with LLMs.

</details>


### [343] [NExtLong: Toward Effective Long-Context Training without Long Documents](https://arxiv.org/abs/2501.12766)

*Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Long-context synthesis, Machine Learning

**Relevance Score:** 8

**TL;DR:** NExtLong is a novel framework that synthesizes long-context data by using negative document extension to enhance long-range dependency modeling in large language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for synthesizing long-context data often fail to properly reinforce long-range dependency modeling due to the limited availability of long documents.

**Method:** NExtLong decomposes documents into multiple meta-chunks and integrates hard negative distractors from pretraining corpora to improve the long-range context discrimination ability of the model.

**Key Contributions:**

	1. Introduction of the NExtLong framework for long-context data synthesis
	2. Utilization of negative document extension to improve long-range dependency modeling
	3. Demonstrated significant performance improvements on benchmark tests.

**Result:** NExtLong significantly outperforms existing long-context synthesis methods and leading models on the HELMET and RULER benchmarks, highlighting its effectiveness in modeling long-range dependencies without heavy reliance on non-synthetic long documents.

**Limitations:** 

**Conclusion:** NExtLong presents an effective strategy for developing advanced long-context large language models, showcasing the potential of using synthesized long-context data.

**Abstract:** Large language models (LLMs) with extended context windows have made significant strides yet remain a challenge due to the scarcity of long documents. Existing methods tend to synthesize long-context data but lack a clear mechanism to reinforce the long-range dependency modeling. To address this limitation, we propose NExtLong, a novel framework for synthesizing long-context data through Negative document Extension. NExtLong decomposes a document into multiple meta-chunks and extends the context by interleaving hard negative distractors retrieved from pretraining corpora. This approach compels the model to discriminate long-range dependent context from distracting content, enhancing its ability to model long-range dependencies. Extensive experiments demonstrate that NExtLong achieves significant performance improvements on the HELMET and RULER benchmarks compared to existing long-context synthesis approaches and leading models, which are trained on non-synthetic long documents. These findings highlight NExtLong's ability to reduce reliance on non-synthetic long documents, making it an effective framework for developing advanced long-context LLMs.

</details>


### [344] [Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages](https://arxiv.org/abs/2501.13836)

*Farhana Shahid, Mona Elswah, Aditya Vashistha*

**Main category:** cs.CL

**Keywords:** automated moderation, low-resource languages, AI bias, content moderation, structural inequities

**Relevance Score:** 7

**TL;DR:** This paper investigates the challenges in automated moderation of harmful content in low-resource languages, highlighting systemic issues and proposing multi-stakeholder approaches for improvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the systemic challenges faced by AI-driven moderation systems inadequately serving non-English speaking users in the Global South, where harmful content often appears in local languages.

**Method:** The study conducted semi-structured interviews with 22 AI experts focused on detecting harmful content in Tamil, Swahili, Maghrebi Arabic, and Quechua, providing insights into the specific challenges faced in these languages.

**Key Contributions:**

	1. Identifies systemic challenges in automated moderation for low-resource languages
	2. Highlights the impact of outdated technology and Western-centric models on content moderation accuracy
	3. Proposes multi-stakeholder approaches for better automated moderation solutions

**Result:** Findings reveal technical issues, including data scarcity, outdated machine translation systems, and models grounded in Western values, leading to significant inaccuracies in content moderation.

**Limitations:** The study focuses on only four low-resource languages and may not cover all systemic issues in other languages or regions.

**Conclusion:** The paper concludes that limitations in automated moderation are not merely technical, but are indicative of deeper structural inequities, advocating for multi-stakeholder approaches to improve the situation.

**Abstract:** Most social media users come from non-English speaking countries in the Global South, where much of harmful content appears in local languages. Yet, current AI-driven moderation systems struggle with low-resource languages spoken in these regions. This work examines the systemic challenges in building automated moderation tools for these languages. We conducted semi-structured interviews with 22 AI experts working on detecting harmful content in four low-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi Arabic (North Africa), and Quechua (South America). Our findings show that beyond the well-known data scarcity in local languages, technical issues--such as outdated machine translation systems, sentiment and toxicity models grounded in Western values, and unreliable language detection technologies--undermine moderation efforts. Even with more data, current language models and preprocessing pipelines--primarily designed for English--struggle with the morphological richness, linguistic complexity, and code-mixing. As a result, automated moderation in Tamil, Swahili, Arabic, and Quechua remains fraught with inaccuracies and blind spots. Based on our findings, we argue that these limitations are not just technical gaps but reflect deeper structural inequities that continue to reproduce historical power imbalances. We conclude by discussing multi-stakeholder approaches to improve automated moderation for low-resource languages.

</details>


### [345] [Token Sampling Uncertainty Does Not Explain Homogeneity Bias in Large Language Models](https://arxiv.org/abs/2501.19337)

*Messi H. J. Lee, Soyeon Jeon*

**Main category:** cs.CL

**Keywords:** homogeneity bias, AI equity, language models, representation learning, token sampling

**Relevance Score:** 7

**TL;DR:** This paper explores homogeneity bias in AI models, specifically in language technologies, showing that interventions should focus on representation learning mechanisms rather than inference-time adjustments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Homogeneity bias in AI models leads to inequitable language technologies by misrepresenting groups as more alike than they are.

**Method:** The study tests homogeneity bias across six large language models by analyzing token-sampling uncertainty and sentence similarity.

**Key Contributions:**

	1. Investigates causes of homogeneity bias in language models
	2. Suggests targeting representation learning to mitigate bias
	3. Challenges the effectiveness of temperature-based sampling adjustments

**Result:** While homogeneity bias is present, there is minimal difference in token sampling uncertainty, indicating that adjustments based on temperature do not effectively reduce the bias.

**Limitations:** Limited to six large language models; further research needed to confirm findings across more models.

**Conclusion:** To address homogeneity bias, researchers should focus on improving representation learning and the composition of training data rather than making changes during inference.

**Abstract:** Homogeneity bias is one form of stereotyping in AI models where certain groups are represented as more similar to each other than other groups. This bias is a major obstacle to creating equitable language technologies. We test whether the bias is driven by systematic differences in token-sampling uncertainty across six large language models. While we observe the presence of homogeneity bias using sentence similarity, we find very little difference in token sampling uncertainty across groups. This finding elucidates why temperature-based sampling adjustments fail to mitigate homogeneity bias. It suggests researchers should prioritize interventions targeting representation learning mechanisms and training corpus composition rather than inference-time output manipulations.

</details>


### [346] [A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment](https://arxiv.org/abs/2502.00136)

*Edward Y. Chang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Ethical Alignment, Self-Supervised Learning, Emotional Conditioning, Adversarial Testing

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for ethically aligning Large Language Models (LLMs) using a checks-and-balances system with three interacting components: knowledge generation (LLMs), ethical guardrails (DIKE), and contextual interpretation (ERIS). The framework incorporates emotional regulation and self-supervised learning to modulate behaviors for ethical outcomes.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper is motivated by the need for ethical alignment in LLMs, drawing inspiration from governmental checks-and-balances systems and addressing the challenge of regulating emotions to improve behavioral outcomes.

**Method:** The authors developed a self-supervised learning pipeline that links emotions to linguistic behaviors. This integrated approach is tested adversarially to ensure that ethical guidelines are followed during knowledge generation and interpretation.

**Key Contributions:**

	1. Introduction of a checks-and-balances framework for LLMs
	2. Integration of emotional regulation mechanisms for improved behavior management
	3. Development of an adversarial testing approach to ensure ethical outcomes

**Result:** The framework successfully directs linguistic behaviors toward ethical outcomes by allowing DIKE and ERIS to influence LLMs while maintaining independence between the components.

**Limitations:** The framework's effectiveness may depend on the robustness of the emotional behavior mapping and the complexity of interactions between the components.

**Conclusion:** The proposed framework demonstrates a novel approach to ethical oversight in LLMs through emotional conditioning and structured governance, which could improve the safety and alignment of future AI systems.

**Abstract:** This paper introduces a checks-and-balances framework for ethical alignment of Large Language Models (LLMs), inspired by three-branch governmental systems. It implements three independent yet interacting components: LLMs as the executive branch for knowledge generation, DIKE as the legislative branch establishing ethical guardrails, and ERIS as the judicial branch for contextual interpretation. Beyond structural separation, we address a fundamental challenge: regulating emotion to shape behaviors. Drawing from psychological theories where managing emotional responses prevents harmful behaviors, we develop a self-supervised learning pipeline that maps emotions to linguistic behaviors, enabling precise behavioral modulation through emotional conditioning. By integrating this approach with adversarial testing, our framework demonstrates how DIKE and ERIS direct linguistic behaviors toward ethical outcomes while preserving independence throughout knowledge generation, ethical oversight, and contextual interpretation.

</details>


### [347] [UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models](https://arxiv.org/abs/2502.00334)

*Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, Yang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Physics Reasoning, Benchmark, UGPhysics, Machine Learning

**Relevance Score:** 6

**TL;DR:** UGPhysics is a new benchmark for evaluating large language models' reasoning in undergraduate-level physics, featuring 5,520 problems across 13 subjects.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of comprehensive evaluation benchmarks for LLMs in physics reasoning, highlighting the gap in their performance compared to other domains like mathematics.

**Method:** Development of UGPhysics, a benchmark comprising 5,520 physics problems and a Model-Assistant Rule-based Judgment (MARJ) pipeline for accurate evaluation.

**Key Contributions:**

	1. Introduction of UGPhysics, a comprehensive benchmark for undergraduate physics reasoning
	2. Development of MARJ for assessing answer correctness
	3. Evaluation of 31 LLMs demonstrating limited physics reasoning capabilities.

**Result:** The highest accuracy achieved by an LLM on this benchmark is 49.8%, indicating a need for enhanced physics reasoning capabilities in models.

**Limitations:** Focuses solely on undergraduate-level physics; may not address higher-level physics complexities or interdisciplinary applications.

**Conclusion:** UGPhysics and the MARJ pipeline aim to stimulate advancements in AI's capability to reason in physics, targeting improvements in models' performance.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities in solving complex reasoning tasks, particularly in mathematics. However, the domain of physics reasoning presents unique challenges that have received significantly less attention. Existing benchmarks often fall short in evaluating LLMs' abilities on the breadth and depth of undergraduate-level physics, underscoring the need for a comprehensive evaluation. To fill this gap, we introduce UGPhysics, a large-scale and comprehensive benchmark specifically designed to evaluate UnderGraduate-level Physics (UGPhysics) reasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics problems in both English and Chinese, covering 13 subjects with seven different answer types and four distinct physics reasoning skills, all rigorously screened for data leakage. Additionally, we develop a Model-Assistant Rule-based Judgment (MARJ) pipeline specifically tailored for assessing answer correctness of physics problems, ensuring accurate evaluation. Our evaluation of 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by OpenAI-o1-mini), emphasizes the necessity for models with stronger physics reasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ, will drive future advancements in AI for physics reasoning. Codes and data are available at https://github.com/YangLabHKUST/UGPhysics .

</details>


### [348] [A statistically consistent measure of semantic uncertainty using Language Models](https://arxiv.org/abs/2502.00507)

*Yi Liu*

**Main category:** cs.CL

**Keywords:** semantic uncertainty, language models, semantic spectral entropy

**Relevance Score:** 8

**TL;DR:** Proposes a novel measure of semantic uncertainty for language models using semantic spectral entropy to quantify uncertainty in outputs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To quantify uncertainty in outputs generated by language models.

**Method:** A straightforward algorithm that uses standard, pretrained language models without needing access to their internal generation process.

**Key Contributions:**

	1. Introduction of semantic spectral entropy as a measure of semantic uncertainty
	2. Application of the method across different language model architectures
	3. Validation through simulation studies showing robustness and accuracy

**Result:** The method provides an accurate and robust estimation of semantic uncertainty, despite the randomness of generative outputs.

**Limitations:** 

**Conclusion:** The approach is widely applicable due to minimal constraints on language model choice and is supported by comprehensive simulation studies.

**Abstract:** To address the challenge of quantifying uncertainty in the outputs generated by language models, we propose a novel measure of semantic uncertainty, semantic spectral entropy, that is statistically consistent under mild assumptions. This measure is implemented through a straightforward algorithm that relies solely on standard, pretrained language models, without requiring access to the internal generation process. Our approach imposes minimal constraints on the choice of language models, making it broadly applicable across different architectures and settings. Through comprehensive simulation studies, we demonstrate that the proposed method yields an accurate and robust estimate of semantic uncertainty, even in the presence of the inherent randomness characteristic of generative language model outputs.

</details>


### [349] [JingFang: An Expert-Level Large Language Model for Traditional Chinese Medicine Clinical Consultation and Syndrome Differentiation-Based Treatment](https://arxiv.org/abs/2502.04345)

*Yehan Yang, Tianhao Ma, Ruotai Li, Xinhan Zheng, Guodong Shan, Chisheng Li*

**Main category:** cs.CL

**Keywords:** Traditional Chinese Medicine, Large Language Models, clinical consultation, syndrome differentiation, healthcare AI

**Relevance Score:** 7

**TL;DR:** JingFang is a novel Large Language Model (LLM) aimed at enhancing clinical consultation and syndrome differentiation in Traditional Chinese Medicine (TCM).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLMs for TCM face critical limitations in clinical consultation and diagnosis accuracy, necessitating a more effective solution.

**Method:** The paper introduces JingFang (JF) which incorporates a Multi-Agent Collaborative Chain-of-Thought Mechanism (MACCTM) for improved clinical consultation and diagnostic capabilities. Additionally, it utilizes a Syndrome Agent and a Dual-Stage Recovery Scheme (DSRS) for better syndrome differentiation and treatment.

**Key Contributions:**

	1. Introduction of JingFang (JF), an advanced TCM LLM.
	2. Development of MACCTM for improved consultation and diagnostic precision.
	3. Implementation of a Dual-Stage Recovery Scheme (DSRS) for accurate syndrome differentiation.

**Result:** JingFang demonstrates enhanced expertise in clinical consultation, accurate syndrome differentiation, and effective treatment recommendations compared to existing TCM LLMs.

**Limitations:** The study may be limited by the need for extensive clinical validation of JingFang's recommendations in real-world scenarios.

**Conclusion:** The development of JingFang contributes significantly to the integration of LLMs in traditional healthcare practices, especially in TCM.

**Abstract:** The effective application of traditional Chinese medicine (TCM) requires extensive knowledge of TCM and clinical experience. The emergence of Large Language Models (LLMs) provides a solution to this, while existing LLMs for TCM exhibit critical limitations of incomplete clinical consultation and diagnoses, as well as inaccurate syndrome differentiation. To address these issues, we establish JingFang (JF), a novel TCM LLM that demonstrates the level of expertise in clinical consultation and syndrome differentiation. We propose a Multi-Agent Collaborative Chain-of-Thought Mechanism (MACCTM) for comprehensive and targeted clinical consultation, enabling JF with effective and accurate diagnostic ability. In addition, a Syndrome Agent and a Dual-Stage Recovery Scheme (DSRS) are developed to accurately enhance the differentiation of the syndrome and the subsequent corresponding treatment. JingFang not only facilitates the application of LLMs but also promotes the effective application of TCM for healthcare.

</details>


### [350] [DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease](https://arxiv.org/abs/2502.04394)

*Tingyu Mo, Jacqueline C. K. Lam, Victor O. K. Li, Lawrence Y. L. Cheung*

**Main category:** cs.CL

**Keywords:** Alzheimer's Disease, Large Language Models, Speech Detection, Linguistic Analysis, Data Augmentation

**Relevance Score:** 9

**TL;DR:** The paper presents DECT, a novel LLM-based approach for improving Alzheimer's Disease detection through detailed linguistic analysis of patient-interviewer dialogues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Alzheimer's Disease is widely prevalent, requiring effective identification of markers for timely intervention, with a focus on language impairment as a key sign.

**Method:** DECT employs large language models to analyze speech transcripts, filter irrelevant information, and generate diverse AD speech data for model training.

**Key Contributions:**

	1. Introduction of DECT for fine-grained linguistic analysis in AD detection
	2. Utilization of LLMs to extract and generate relevant linguistic markers from speech transcripts
	3. Demonstration of significant improvement in AD detection accuracy through enhanced data representation

**Result:** The implementation of DECT resulted in an 11% increase in detection accuracy compared to existing models using DementiaBank datasets.

**Limitations:** 

**Conclusion:** DECT effectively enhances the robustness of AD detection models through advanced linguistic analysis and data augmentation techniques.

**Abstract:** Alzheimer's Disease (AD) is an irreversible neurodegenerative disease affecting 50 million people worldwide. Low-cost, accurate identification of key markers of AD is crucial for timely diagnosis and intervention. Language impairment is one of the earliest signs of cognitive decline, which can be used to discriminate AD patients from normal control individuals. Patient-interviewer dialogues may be used to detect such impairments, but they are often mixed with ambiguous, noisy, and irrelevant information, making the AD detection task difficult. Moreover, the limited availability of AD speech samples and variability in their speech styles pose significant challenges in developing robust speech-based AD detection models. To address these challenges, we propose DECT, a novel speech-based domain-specific approach leveraging large language models (LLMs) for fine-grained linguistic analysis and label-switched label-preserved data generation. Our study presents four novelties: We harness the summarizing capabilities of LLMs to identify and distill key Cognitive-Linguistic information from noisy speech transcripts, effectively filtering irrelevant information. We leverage the inherent linguistic knowledge of LLMs to extract linguistic markers from unstructured and heterogeneous audio transcripts. We exploit the compositional ability of LLMs to generate AD speech transcripts consisting of diverse linguistic patterns to overcome the speech data scarcity challenge and enhance the robustness of AD detection models. We use the augmented AD textual speech transcript dataset and a more fine-grained representation of AD textual speech transcript data to fine-tune the AD detection model. The results have shown that DECT demonstrates superior model performance with an 11% improvement in AD detection accuracy on the datasets from DementiaBank compared to the baselines.

</details>


### [351] [Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection](https://arxiv.org/abs/2502.04528)

*Minseok Jung, Cynthia Fuertes Panizo, Liam Dugan, Yi R., Fung, Pin-Yu Chen, Paul Pu Liang*

**Main category:** cs.CL

**Keywords:** large language models, fairness, AI-text detectors, threshold optimization, bias reduction

**Relevance Score:** 8

**TL;DR:** FairOPT optimizes thresholds for AI text detectors to reduce error discrepancies among subgroups.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of AI-text detectors by addressing limitations of fixed global thresholds which lead to biased misclassifications.

**Method:** Introduced FairOPT, which partitions data into subgroups based on attributes and learns group-specific decision thresholds.

**Key Contributions:**

	1. Introduction of FairOPT for group-specific threshold optimization
	2. Demonstration of reduced error discrepancies across subgroups
	3. Validation through experiments with nine AI text classifiers on three datasets

**Result:** FairOPT decreased overall balanced error rate discrepancy by 12% and minimized accuracy sacrifice by 0.003%.

**Limitations:** May rely on the assumption that subgroups are properly defined and that attributes accurately represent group differences.

**Conclusion:** FairOPT provides a framework for enhancing classification robustness in AI-generated content detection.

**Abstract:** The advancement of large language models (LLMs) has made it difficult to differentiate human-written text from AI-generated text. Several AI-text detectors have been developed in response, which typically utilize a fixed global threshold (e.g., $\theta = 0.5$) to classify machine-generated text. However, one universal threshold could fail to account for distributional variations by subgroups. For example, when using a fixed threshold, detectors make more false positive errors on shorter human-written text, and more positive classifications of neurotic writing styles among long texts. These discrepancies can lead to misclassifications that disproportionately affect certain groups. We address this critical limitation by introducing FairOPT, an algorithm for group-specific threshold optimization for probabilistic AI-text detectors. We partitioned data into subgroups based on attributes (e.g., text length and writing style) and implemented FairOPT to learn decision thresholds for each group to reduce discrepancy. In experiments with nine AI text classifiers on three datasets, FairOPT decreases overall balanced error rate (BER) discrepancy by 12\% while minimally sacrificing accuracy by 0.003\%. Our framework paves the way for more robust classification in AI-generated content detection via post-processing.

</details>


### [352] [Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering](https://arxiv.org/abs/2502.07340)

*Shuzheng Si, Haozhe Zhao, Gang Chen, Cheng Gao, Yuzhuo Bai, Zhitong Wang, Kaikai An, Kangyang Luo, Chen Qian, Fanchao Qi, Baobao Chang, Maosong Sun*

**Main category:** cs.CL

**Keywords:** NOVA, LLMs, instruction tuning, data quality, hallucinations

**Relevance Score:** 9

**TL;DR:** NOVA is a framework designed to reduce hallucinations in LLMs by identifying high-quality data with which the models are familiar during instruction tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs can generate hallucinations when trained on unfamiliar data, particularly during instruction tuning. There is a need for a framework that selects high-quality and familiar instruction data to mitigate this issue.

**Method:** NOVA employs Internal Consistency Probing (ICP) to evaluate the model's understanding of instructions through self-generated responses, and Semantic Equivalence Identification (SEI) to assess the familiarity of the model with responses, enhanced by a voting strategy. An expert-aligned reward model is also introduced to ensure quality beyond familiarity.

**Key Contributions:**

	1. Introduction of NOVA framework for data quality assessment in LLMs
	2. Development of Internal Consistency Probing (ICP) method
	3. Implementation of Semantic Equivalence Identification (SEI) for familiarity evaluation

**Result:** NOVA effectively reduces hallucinations by optimizing the selection of training data, leading to better alignment of LLMs with instructions and enhanced response quality.

**Limitations:** 

**Conclusion:** By focusing on the quality and familiarity of training data, NOVA provides a significant advance in aligning LLMs to follow instructions more accurately and reduce erroneous outputs.

**Abstract:** Training LLMs on data containing unfamiliar knowledge during the instruction tuning stage can encourage hallucinations. To address this challenge, we introduce NOVA, a novel framework designed to identify high-quality data that aligns well with the LLM's learned knowledge to reduce hallucinations. NOVA includes Internal Consistency Probing (ICP) and Semantic Equivalence Identification (SEI) to measure how familiar the LLM is with instruction data. Specifically, ICP evaluates the LLM's understanding of the given instruction by calculating the tailored consistency among multiple self-generated responses. SEI further assesses the familiarity of the LLM with the target response by comparing it to the generated responses, using the proposed semantic clustering and well-designed voting strategy. Finally, to ensure the quality of selected samples, we introduce an expert-aligned reward model, considering characteristics beyond just familiarity. By considering data quality and avoiding unfamiliar data, we can utilize the selected data to effectively align LLMs to follow instructions and hallucinate less.

</details>


### [353] [What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations](https://arxiv.org/abs/2502.08279)

*Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg*

**Main category:** cs.CL

**Keywords:** video-to-text summarization, multimodal learning, dataset, planning, AI conference

**Relevance Score:** 6

**TL;DR:** This paper introduces VISTA, a dataset for video-to-text summarization of AI conference presentations, and evaluates state-of-the-art models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of transforming recorded videos into concise textual summaries in scientific domains, particularly for AI conferences.

**Method:** Introduces the VISTA dataset of 18,599 AI conference presentation videos and their abstracts, benchmarks existing models, and applies a plan-based framework to improve summary quality.

**Key Contributions:**

	1. Development of the VISTA dataset for video-to-text summarization
	2. Benchmarking state-of-the-art large models
	3. Demonstration of the impact of planning on summary quality

**Result:** Evaluation shows that a plan-based approach enhances summary quality and factual consistency; however, there remains a significant gap between model and human performance.

**Limitations:** Not all models achieve human-level performance, revealing challenges in the dataset and the summarization task.

**Conclusion:** The study highlights the capabilities and limitations of current models and paves the way for future research in scientific video-to-text summarization.

**Abstract:** Transforming recorded videos into concise and accurate textual summaries is a growing challenge in multimodal learning. This paper introduces VISTA, a dataset specifically designed for video-to-text summarization in scientific domains. VISTA contains 18,599 recorded AI conference presentations paired with their corresponding paper abstracts. We benchmark the performance of state-of-the-art large models and apply a plan-based framework to better capture the structured nature of abstracts. Both human and automated evaluations confirm that explicit planning enhances summary quality and factual consistency. However, a considerable gap remains between models and human performance, highlighting the challenges of our dataset. This study aims to pave the way for future research on scientific video-to-text summarization.

</details>


### [354] [SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence](https://arxiv.org/abs/2502.08767)

*Zhining Liu, Rana Ali Amjad, Ravinarayana Adkathimar, Tianxin Wei, Hanghang Tong*

**Main category:** cs.CL

**Keywords:** Language Models, Contextual Evidence, Self-Guided Highlighting, QA Tasks, Attention Mechanism

**Relevance Score:** 9

**TL;DR:** SelfElicit improves language models' responses by emphasizing key contextual evidence through self-guided explicit highlighting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of language models struggling to comprehend and utilize key evidence from noisy and irrelevant contextual information.

**Method:** SelfElicit is an inference-time approach that leverages LMs' attention scores to identify and highlight important evidence without extra training.

**Key Contributions:**

	1. Introduces SelfElicit for highlighting key evidence in context
	2. Demonstrates significant performance improvements on QA tasks
	3. Maintains computational efficiency during the inference process

**Result:** SelfElicit consistently improves performance on multiple evidence-based QA tasks across various language model families while being computationally efficient.

**Limitations:** 

**Conclusion:** The proposed method enables language models to produce more accurate and grounded responses in real-world scenarios without requiring additional training efforts.

**Abstract:** Providing Language Models (LMs) with relevant evidence in the context (either via retrieval or user-provided) can significantly improve their ability to provide better-grounded responses. However, recent studies have found that LMs often struggle to fully comprehend and utilize key evidence from the context, especially when it contains noise and irrelevant information, an issue common in real-world scenarios. To address this, we propose SelfElicit, an inference-time approach that helps LMs focus on key contextual evidence through self-guided explicit highlighting. By leveraging the inherent evidence-finding capabilities of LMs using the attention scores of deeper layers, our method automatically identifies and emphasizes key evidence within the input context, facilitating more accurate and grounded responses without additional training or iterative prompting. We demonstrate that SelfElicit brings consistent and significant improvement on multiple evidence-based QA tasks for various LM families while maintaining computational efficiency. Our code and documentation are available at https://github.com/ZhiningLiu1998/SelfElicit.

</details>


### [355] [A Survey of LLM-based Agents in Medicine: How far are we from Baymax?](https://arxiv.org/abs/2502.11211)

*Wenxuan Wang, Zizhan Ma, Zheng Wang, Chenghan Wu, Jiaming Ji, Wenting Chen, Xiang Li, Yixuan Yuan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare, Medical Agents, Artificial Intelligence, Ethics

**Relevance Score:** 10

**TL;DR:** This survey reviews LLM-based agents in healthcare, their architectures, applications, challenges, and future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview of LLM-based agents' influence on healthcare and identify challenges and future prospects.

**Method:** Survey of existing literature and frameworks on LLM-based agents in medicine, focusing on architectures, applications, evaluation metrics, and challenges.

**Key Contributions:**

	1. Comprehensive review of LLM architectures and applications in healthcare
	2. Analysis of performance evaluation metrics for LLM-based agents
	3. Identification of key challenges and future research directions in healthcare LLM application

**Result:** Identification of key application scenarios for LLM-based agents, including clinical decision support and medical documentation, as well as the challenges these systems face, such as hallucination management and ethical considerations.

**Limitations:** Challenges such as multimodal integration and ethical considerations are ongoing; future work is needed to address these issues.

**Conclusion:** LLM-based agents have potential in enhancing healthcare delivery, but challenges such as system integration and ethical issues need to be addressed; future research should focus on improving medical reasoning and integrating these systems.

**Abstract:** Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.

</details>


### [356] [HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning](https://arxiv.org/abs/2502.11393)

*Xiaoyuan Li, Moxin Li, Rui Men, Yichang Zhang, Keqin Bao, Wenjie Wang, Fuli Feng, Dayiheng Liu, Junyang Lin*

**Main category:** cs.CL

**Keywords:** large language models, commonsense reasoning, evaluation benchmark

**Relevance Score:** 8

**TL;DR:** This paper presents HellaSwag-Pro, a large-scale evaluation benchmark for assessing the robustness of large language models in commonsense reasoning across different question variants and languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether large language models truly understand commonsense knowledge or simply memorize expression patterns, and to provide a robust evaluation benchmark.

**Method:** A two-stage method was proposed to develop a bilingual benchmark, HellaSwag-Pro, consisting of 11,200 cases with seven types of question variants, alongside a fine-tuned dataset of 12,000 instances in Chinese.

**Key Contributions:**

	1. Introduction of HellaSwag-Pro as a benchmark for commonsense reasoning in LLMs.
	2. Development of a bilingual dataset for evaluating model robustness.
	3. Comprehensive analysis of 41 LLMs revealing flaws in their commonsense reasoning capabilities.

**Result:** Extensive experiments conducted on 41 LLMs demonstrated that these models are not robust in commonsense reasoning, with performance varying based on the language of the questions.

**Limitations:** 

**Conclusion:** The establishment of HellaSwag-Pro offers a high-quality evaluation benchmark which provides insights to improve commonsense reasoning in LLMs.

**Abstract:** Large language models (LLMs) have shown remarkable capabilities in commonsense reasoning; however, some variations in questions can trigger incorrect responses. Do these models truly understand commonsense knowledge, or just memorize expression patterns? To investigate this question, we present the first extensive robustness evaluation of LLMs in commonsense reasoning. We introduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200 cases, by designing and compiling seven types of question variants. To construct this benchmark, we propose a two-stage method to develop Chinese HellaSwag, a finely annotated dataset comprising 12,000 instances across 56 categories. We conduct extensive experiments on 41 representative LLMs, revealing that these LLMs are far from robust in commonsense reasoning. Furthermore, this robustness varies depending on the language in which the LLM is tested. This work establishes a high-quality evaluation benchmark, with extensive experiments offering valuable insights to the community in commonsense reasoning for LLMs.

</details>


### [357] [Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study](https://arxiv.org/abs/2502.11514)

*Yujie Lin, Ante Wang, Moye Chen, Jingyao Liu, Hao Liu, Jinsong Su, Xinyan Xiao*

**Main category:** cs.CL

**Keywords:** multi-modal reasoning, inference-time scaling, chain-of-thought

**Relevance Score:** 8

**TL;DR:** This study explores multi-modal reasoning by integrating visual and textual modalities in inference-time scaling, addressing gaps in existing research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in understanding multi-modal reasoning processes using both visual and textual modalities, which have been overlooked in current studies.

**Method:** The paper investigates popular sampling-based and tree search-based inference-time scaling methods on ten tasks across various domains, incorporating a consistency-enhanced verifier for guidance.

**Key Contributions:**

	1. Integration of visual and textual modalities in multi-modal reasoning.
	2. Systematic analysis of inference-time scaling methods on diverse tasks.
	3. Identification of the trade-offs involved in multi-modal thought processes.

**Result:** Results indicate that multi-modal reasoning outperforms text-only reasoning and allows for more diverse thought processes, although it incurs higher token costs due to richer visual inputs.

**Limitations:** Higher token consumption for processing visual inputs may limit practical application.

**Conclusion:** The findings highlight both the advantages and drawbacks of multi-modal thought in inference-time scaling, suggesting avenues for future research.

**Abstract:** Recently, inference-time scaling of chain-of-thought (CoT) has been demonstrated as a promising approach for addressing multi-modal reasoning tasks. While existing studies have predominantly centered on text-based thinking, the integration of both visual and textual modalities within the reasoning process remains unexplored. In this study, we pioneer the exploration of inference-time scaling with multi-modal thought, aiming to bridge this gap. To provide a comprehensive analysis, we systematically investigate popular sampling-based and tree search-based inference-time scaling methods on 10 challenging tasks spanning various domains. Besides, we uniformly adopt a consistency-enhanced verifier to ensure effective guidance for both methods across different thought paradigms. Results show that multi-modal thought promotes better performance against conventional text-only thought, and blending the two types of thought fosters more diverse thinking. Despite these advantages, multi-modal thoughts necessitate higher token consumption for processing richer visual inputs, which raises concerns in practical applications. We hope that our findings on the merits and drawbacks of this research line will inspire future works in the field.

</details>


### [358] [Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?](https://arxiv.org/abs/2502.11598)

*Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, watermarking, knowledge distillation, watermark removal, adversarial robustness

**Relevance Score:** 8

**TL;DR:** This paper investigates watermark removal in Large Language Models (LLMs) and proposes methods for avoiding watermark inheritance during knowledge distillation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the robustness of watermark radioactivity against adversarial actors and to prevent unauthorized knowledge distillation in LLMs.

**Method:** The paper proposes two categories of watermark removal approaches: pre-distillation removal (using untargeted and targeted training data paraphrasing) and post-distillation removal (through inference-time watermark neutralization).

**Key Contributions:**

	1. Investigation of watermark radioactivity in LLMs
	2. Proposed methods for watermark removal during student model training
	3. Demonstrated effectiveness of removing watermarks without significant performance trade-offs

**Result:** Experiments demonstrate that both targeted paraphrasing and watermark neutralization effectively eliminate inherited watermarks; watermark neutralization does so while preserving knowledge transfer efficiency and low computational costs.

**Limitations:** 

**Conclusion:** The findings highlight the need for more robust defense strategies against potential threats posed by watermarking techniques in production LLMs.

**Abstract:** The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at https://github.com/THU-BPM/Watermark-Radioactivity-Attack.

</details>


### [359] [ReviewEval: An Evaluation Framework for AI-Generated Reviews](https://arxiv.org/abs/2502.11736)

*Madhav Krishan Garg, Tejash Prasad, Tanmay Singhal, Chhavi Kirtani, Murari Mandal, Dhruv Kumar*

**Main category:** cs.CL

**Keywords:** AI-generated reviews, peer review, human assessments, LLM, evaluation framework

**Relevance Score:** 8

**TL;DR:** This paper introduces ReviewEval, a framework for evaluating AI-generated peer reviews, and ReviewAgent, an LLM-based agent that improves review quality by aligning with human assessments and optimizing feedback processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing volume of academic research and the shortage of qualified reviewers necessitate innovative solutions in the peer review process.

**Method:** The paper proposes ReviewEval, an evaluation framework for AI-generated reviews that assesses alignment, factual accuracy, analytical depth, constructiveness, and adherence to guidelines. ReviewAgent is an LLM-based generation agent that utilizes a unique alignment mechanism and a refinement feedback loop to enhance review quality.

**Key Contributions:**

	1. Introduction of ReviewEval for evaluating AI-generated reviews.
	2. Development of ReviewAgent with a novel alignment mechanism.
	3. Demonstrated improvements in review quality metrics over existing methods.

**Result:** ReviewAgent outperforms existing AI baselines and expert reviews, improving actionable insights by 6.78% and 47.62%, analytical depth by 3.97% and 12.73%, and adherence to guidelines by 10.11% and 47.26%.

**Limitations:** 

**Conclusion:** The study establishes crucial metrics for AI-based peer review and significantly increases the reliability and effectiveness of AI-generated reviews in academic research.

**Abstract:** The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. In this work, we propose: 1. ReviewEval, a comprehensive evaluation framework for AI-generated reviews that measures alignment with human assessments, verifies factual accuracy, assesses analytical depth, identifies degree of constructiveness and adherence to reviewer guidelines; and 2. ReviewAgent, an LLM-based review generation agent featuring a novel alignment mechanism to tailor feedback to target conferences and journals, along with a self-refinement loop that iteratively optimizes its intermediate outputs and an external improvement loop using ReviewEval to improve upon the final reviews. ReviewAgent improves actionable insights by 6.78% and 47.62% over existing AI baselines and expert reviews respectively. Further, it boosts analytical depth by 3.97% and 12.73%, enhances adherence to guidelines by 10.11% and 47.26% respectively. This paper establishes essential metrics for AIbased peer review and substantially enhances the reliability and impact of AI-generated reviews in academic research.

</details>


### [360] [Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning](https://arxiv.org/abs/2502.11962)

*Tianyi Wu, Jingwei Ni, Bryan Hooi, Jiaheng Zhang, Elliott Ash, See-Kiong Ng, Mrinmaya Sachan, Markus Leippold*

**Main category:** cs.CL

**Keywords:** Instruction fine-tuning, Large language models, Truthfulness, Informativeness, Uncertainty

**Relevance Score:** 9

**TL;DR:** The paper explores the trade-off between informativeness and truthfulness in instruction fine-tuning (IFT) of LLMs, introducing two new IFT approaches to mitigate issues of unfamiliar knowledge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work is to address the reduction in truthfulness of large language models (LLMs) caused by instruction fine-tuning (IFT) using unfamiliar knowledge, which can lead to misinformation.

**Method:** The authors propose two paradigms: $UNIT_{cut}$, which removes unfamiliar knowledge from IFT datasets, and $UNIT_{ref}$, which trains LLMs to recognize uncertainty and signal it at the end of their responses. They conduct experiments to evaluate the effectiveness of both methods.

**Key Contributions:**

	1. Introduction of $UNIT_{cut}$ for removing unfamiliar knowledge from training datasets, improving truthfulness.
	2. Introduction of $UNIT_{ref}$ for teaching LLMs to indicate uncertainty, maintaining informativeness.
	3. Empirical validation showing the effectiveness of both methods in enhancing LLM performance.

**Result:** Experiments demonstrate that $UNIT_{cut}$ significantly improves the truthfulness of LLMs, while $UNIT_{ref}$ preserves informativeness and reduces misstatements by allowing the model to differentiate between confident and uncertain claims.

**Limitations:** 

**Conclusion:** The findings suggest that carefully designed instruction fine-tuning can enhance the reliability of LLMs, particularly in high-stakes applications where accuracy is critical.

**Abstract:** Instruction fine-tuning (IFT) can increase the informativeness of large language models (LLMs), but may reduce their truthfulness. This trade-off arises because IFT steers LLMs to generate responses containing long-tail knowledge that was not well covered during pre-training. As a result, models become more informative but less accurate when generalizing to unseen tasks. In this paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets can negatively affect the truthfulness of LLMs, and we introduce two new IFT paradigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$ identifies and removes unfamiliar knowledge from IFT datasets to mitigate its impact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize their uncertainty and explicitly indicate it at the end of their responses. Our experiments show that $UNIT_{cut}$ substantially improves LLM truthfulness, while $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by distinguishing between confident and uncertain statements.

</details>


### [361] [How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines](https://arxiv.org/abs/2502.12051)

*Ayan Sengupta, Yash Goel, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** neural scaling laws, AI models, adaptive scaling strategies

**Relevance Score:** 6

**TL;DR:** This survey synthesizes insights from over 50 studies on neural scaling laws, examining their implications, challenges, and variability across different AI architectures and use cases.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the predictability of model performance based on scaling laws and the limitations encountered across various architectures and deployment scenarios.

**Method:** A survey of existing research, synthesizing theoretical foundations, empirical results, and practical implications of neural scaling laws.

**Key Contributions:**

	1. Synthesis of insights from over 50 studies on neural scaling laws
	2. Identification of challenges such as data efficiency and architecture-specific constraints
	3. Advocacy for adaptive scaling strategies in AI deployment

**Result:** Identified variances in scaling behavior across different domains and architectures, emphasizing the need for adaptive scaling strategies for real-world applications.

**Limitations:** Scaling laws may not generalize across all architectures and training strategies, requiring careful adaptation in practice.

**Conclusion:** Scaling laws are helpful guides for model optimization but are not universally applicable to all training strategies; tailored approaches are needed.

**Abstract:** Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.

</details>


### [362] [TokenSkip: Controllable Chain-of-Thought Compression in LLMs](https://arxiv.org/abs/2502.12067)

*Heming Xia, Chak Tou Leong, Wenjie Wang, Yongqi Li, Wenjie Li*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, TokenSkip, large language models, inference latency, reasoning performance

**Relevance Score:** 9

**TL;DR:** TokenSkip is a method that allows large language models to skip less important tokens in Chain-of-Thought outputs, reducing inference latency while maintaining reasoning performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Scaling the length of Chain-of-Thought sequences improves the reasoning capabilities of large language models, but longer outputs increase inference latency, affecting user experience. Identifying and skipping less important tokens can enhance efficiency.

**Method:** The paper analyzes the semantic importance of tokens in Chain-of-Thought sequences and proposes TokenSkip, a method that allows LLMs to selectively skip tokens based on their contribution to reasoning.

**Key Contributions:**

	1. Introduction of TokenSkip for controllable Chain-of-Thought compression
	2. Demonstration of token importance analysis in reasoning tasks
	3. Empirical validation of TokenSkip's effectiveness across various models and tasks

**Result:** TokenSkip effectively reduces the number of reasoning tokens used by 40% on the GSM8K task with less than a 0.4% drop in performance, demonstrating efficiency improvements for large language models while preserving strong reasoning capabilities.

**Limitations:** The approach may require fine-tuning for different tasks and models to achieve optimal performance.

**Conclusion:** TokenSkip offers a practical solution for enhancing the efficiency of Chain-of-Thought reasoning in LLMs, striking a balance between performance and inference speed.

**Abstract:** Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning capabilities of large language models (LLMs). Recent advancements, such as OpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT sequences during inference could further boost LLM reasoning performance. However, due to the autoregressive nature of LLM decoding, longer CoT outputs lead to a linear increase in inference latency, adversely affecting user experience, particularly when the CoT exceeds 10,000 tokens. To address this limitation, we analyze the semantic importance of tokens within CoT outputs and reveal that their contributions to reasoning vary. Building on this insight, we propose TokenSkip, a simple yet effective approach that enables LLMs to selectively skip less important tokens, allowing for controllable CoT compression. Extensive experiments across various models and tasks demonstrate the effectiveness of TokenSkip in reducing CoT token usage while preserving strong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct, TokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less than a 0.4% performance drop.

</details>


### [363] [Evaluating Step-by-step Reasoning Traces: A Survey](https://arxiv.org/abs/2502.12289)

*Jinu Lee, Julia Hockenmaier*

**Main category:** cs.CL

**Keywords:** large language models, reasoning evaluation, taxonomy of evaluation criteria, factuality, utility

**Relevance Score:** 9

**TL;DR:** This survey outlines evaluation practices for step-by-step reasoning in large language models, presenting a taxonomy of evaluation criteria to improve consistency in evaluations.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding and improvement in the reasoning capabilities of large language models by standardizing evaluation practices.

**Method:** The paper proposes a taxonomy of evaluation criteria comprising factuality, validity, coherence, and utility, and reviews existing evaluator implementations and findings.

**Key Contributions:**

	1. Comprehensive overview of step-by-step reasoning evaluation
	2. Taxonomy of evaluation criteria
	3. Review of evaluator implementations and findings

**Result:** The survey identifies gaps in current evaluation practices and suggests a structured approach to evaluate reasoning in LLMs.

**Limitations:** 

**Conclusion:** The proposed taxonomy could guide future research in developing consistent and effective benchmarks for evaluating reasoning in large language models.

**Abstract:** Step-by-step reasoning is widely used to enhance the reasoning ability of large language models (LLMs) in complex problems. Evaluating the quality of reasoning traces is crucial for understanding and improving LLM reasoning. However, existing evaluation practices are highly inconsistent, resulting in fragmented progress across evaluator design and benchmark development. To address this gap, this survey provides a comprehensive overview of step-by-step reasoning evaluation, proposing a taxonomy of evaluation criteria with four top-level categories (factuality, validity, coherence, and utility). Based on the taxonomy, we review different evaluator implementations and recent findings, leading to promising directions for future research.

</details>


### [364] [A Cognitive Writing Perspective for Constrained Long-Form Text Generation](https://arxiv.org/abs/2502.12568)

*Kaiyang Wan, Honglin Mu, Rui Hao, Haoran Luo, Tianle Gu, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cognitive Writing Theory, Long-form text generation, AI writing systems, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** CogWriter is a novel framework that empowers Large Language Models to generate long-form text through cognitive writing processes.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the difficulties LLMs face in generating high-quality long-form text, drawing on Cognitive Writing Theory to enhance LLM writing capabilities.

**Method:** CogWriter employs two main components: a Planning Agent for hierarchical task decomposition and multiple Generation Agents for parallel execution of writing plans, with a monitoring system for continuous quality assessment.

**Key Contributions:**

	1. Introduces CogWriter, a training-free framework for LLM writing
	2. Implements a Planning Agent for task decomposition
	3. Demonstrates superior performance on constrained text generation benchmarks.

**Result:** CogWriter significantly outperforms GPT-4o by 22% in instruction completion accuracy and can produce texts over 10,000 words, as demonstrated on LongGenBench.

**Limitations:** 

**Conclusion:** The study suggests that leveraging cognitive science principles can enhance LLM writing, providing a new framework for future advancements in this area.

**Abstract:** Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: \href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.

</details>


### [365] [Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization](https://arxiv.org/abs/2502.12672)

*Tzu-Quan Lin, Wei-Ping Huang, Hao Tang, Hung-yi Lee*

**Main category:** cs.CL

**Keywords:** speech representation, fine-tuning, cross-task generalization, HuBERT, SUPERB benchmark

**Relevance Score:** 4

**TL;DR:** Speech-FT is a two-stage fine-tuning framework for speech representation models that enhances task-specific performance while maintaining cross-task generalization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve fine-tuning methods that often degrade cross-task generalization in speech representation models.

**Method:** The framework involves a two-stage process: first fine-tuning to reduce representational drift, followed by weight-space interpolation with the pre-trained model.

**Key Contributions:**

	1. Proposed a novel two-stage fine-tuning framework for speech models
	2. Demonstrated significant performance improvements on the SUPERB benchmark
	3. Maintained higher feature similarity to pre-trained models with larger weight updates

**Result:** Speech-FT consistently improves performance across various fine-tuning scenarios and maintains higher feature similarity to the pre-trained model compared to existing approaches.

**Limitations:** 

**Conclusion:** Speech-FT is effective in refining speech representation models and significantly improves performance on benchmarks like SUPERB.

**Abstract:** Fine-tuning speech representation models can enhance performance on specific tasks but often compromises their cross-task generalization ability. This degradation is often caused by excessive changes in the representations, making it difficult to retain information learned during pre-training. Existing approaches, such as regularizing weight changes during fine-tuning, may fail to maintain sufficiently high feature similarity with the pre-trained model, and thus could possibly lose cross-task generalization. To address this issue, we propose Speech-FT, a novel two-stage fine-tuning framework designed to maintain cross-task generalization while benefiting from fine-tuning. Speech-FT first applies fine-tuning specifically designed to reduce representational drift, followed by weight-space interpolation with the pre-trained model to restore cross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR 2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves performance across a wide range of supervised, unsupervised, and multitask fine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task generalization compared to fine-tuning baselines that explicitly constrain weight changes, such as weight-space regularization and LoRA fine-tuning. Our analysis reveals that Speech-FT maintains higher feature similarity to the pre-trained model compared to alternative strategies, despite allowing larger weight-space updates. Notably, Speech-FT achieves significant improvements on the SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech recognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%, lower word error rate from 6.38% to 5.75%, and increase speaker identification accuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful solution for further refining speech representation models after pre-training.

</details>


### [366] [Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements](https://arxiv.org/abs/2502.12904)

*Shu Yang, Shenzhe Zhu, Zeyu Wu, Keyu Wang, Junchi Yao, Junchao Wu, Lijie Hu, Mengdi Li, Derek F. Wong, Di Wang*

**Main category:** cs.CL

**Keywords:** Fraud Detection, Large Language Models, Phishing Prevention, Benchmarking, Role-Play

**Relevance Score:** 9

**TL;DR:** Fraud-R1 is a benchmark for evaluating LLMs' effectiveness in combating internet fraud and phishing through multi-round assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing issue of internet fraud and phishing, a comprehensive evaluation framework for LLMs is necessary to enhance their resistance in real-world scenarios.

**Method:** The benchmark consists of 8,564 fraud cases from various sources and employs a multi-round evaluation pipeline to test LLMs in two settings: Helpful-Assistant and Role-play.

**Key Contributions:**

	1. Introduction of a novel benchmark (Fraud-R1) for evaluating LLMs against fraud
	2. Multi-round evaluation approach to assess LLMs under different decision-making contexts
	3. Identification of performance gaps in LLMs' abilities to detect fraud in different languages

**Result:** The evaluation indicates significant difficulties in defending against fraud, particularly in role-play scenarios, and highlights performance disparities between languages, notably between Chinese and English.

**Limitations:** The benchmark may not cover all possible fraud scenarios, and the real-world applicability of the findings needs further exploration.

**Conclusion:** Improving multilingual capabilities is essential for effective fraud detection across various fraud types.

**Abstract:** We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to defend against internet fraud and phishing in dynamic, real-world scenarios. Fraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job postings, social media, and news, categorized into 5 major fraud types. Unlike previous benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to assess LLMs' resistance to fraud at different stages, including credibility building, urgency creation, and emotional manipulation. Furthermore, we evaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM provides general decision-making assistance, and 2. Role-play, where the model assumes a specific persona, widely used in real-world agent-based interactions. Our evaluation reveals the significant challenges in defending against fraud and phishing inducement, especially in role-play settings and fake job postings. Additionally, we observe a substantial performance gap between Chinese and English, underscoring the need for improved multilingual fraud detection capabilities.

</details>


### [367] [Conditioning LLMs to Generate Code-Switched Text](https://arxiv.org/abs/2502.12924)

*Maite Heredia, Gorka Labaka, Jeremy Barnes, Aitor Soroa*

**Main category:** cs.CL

**Keywords:** code-switching, natural language processing, large language models, dataset generation, human evaluation

**Relevance Score:** 9

**TL;DR:** This paper presents a novel methodology for generating code-switched (CS) data using LLMs, focusing on the English-Spanish language pair.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current Large Language Models struggle with code-switching due to a lack of large-scale datasets.

**Method:** The paper proposes back-translating natural CS sentences into monolingual English to create a parallel corpus for fine-tuning LLMs to produce CS from monolingual sentences.

**Key Contributions:**

	1. Novel methodology for generating CS data using LLMs
	2. Focus on natural CS data for training
	3. Findings on the correlation of traditional metrics with human judgement

**Result:** The methodology generates fluent code-switched text, expands research opportunities in CS communication, and reveals that traditional metrics do not correlate with human judgment in evaluating CS data quality.

**Limitations:** Study focused on English-Spanish; results may not generalize to other language pairs.

**Conclusion:** The approach allows models to learn natural CS distribution, highlighting the importance of human evaluation in assessing model outputs.

**Abstract:** Code-switching (CS) is still a critical challenge in Natural Language Processing (NLP). Current Large Language Models (LLMs) struggle to interpret and generate code-switched text, primarily due to the scarcity of large-scale CS datasets for training. This paper presents a novel methodology to generate CS data using LLMs, and test it on the English-Spanish language pair. We propose back-translating natural CS sentences into monolingual English, and using the resulting parallel corpus to fine-tune LLMs to turn monolingual sentences into CS. Unlike previous approaches to CS generation, our methodology uses natural CS data as a starting point, allowing models to learn its natural distribution beyond grammatical patterns. We thoroughly analyse the models' performance through a study on human preferences, a qualitative error analysis and an evaluation with popular automatic metrics. Results show that our methodology generates fluent code-switched text, expanding research opportunities in CS communication, and that traditional metrics do not correlate with human judgement when assessing the quality of the generated CS data. We release our code and generated dataset under a CC-BY-NC-SA license.

</details>


### [368] [Natural Language Generation from Visual Events: Challenges and Future Directions](https://arxiv.org/abs/2502.13034)

*Aditya K Surikuchi, Raquel Fernández, Sandro Pezzelle*

**Main category:** cs.CL

**Keywords:** NLP, visual events, multimodal, language generation, human cognition

**Relevance Score:** 7

**TL;DR:** This position paper discusses the importance of natural language generation (NLG) in interpreting visual events, particularly in videos, and proposes directions for improving multimodal models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of focus on natural language tasks that interpret sequences of images and videos in the field of visually grounded NLP.

**Method:** The paper reviews and analyzes five relevant tasks, outlining their impacts on modeling language and visual events over time and suggesting unified evaluation approaches.

**Key Contributions:**

	1. Identification of five relevant NLG tasks related to visual events in videos.
	2. Argument for a unified approach in modeling and evaluating language and vision interaction.
	3. Proposed research directions for advancing multimodal understanding.

**Result:** It highlights common challenges among tasks, emphasizing the need for improved language-and-vision models to properly interpret visual events, which have significant applications.

**Limitations:** 

**Conclusion:** Addressing the intricate relationships between time-based visual events and linguistic descriptions is essential for advancing AI capabilities and understanding human cognition.

**Abstract:** The ability to use natural language to talk about visual events is at the core of human intelligence and a crucial feature of any artificial intelligence system. In recent years, a substantial body of work in visually grounded NLP has focused on describing content depicted in single images. By contrast, comparatively less attention has been devoted to exhaustively modeling scenarios in which natural language is employed to interpret and talk about events presented through videos or sequences of images. In this position paper, we argue that any NLG task dealing with sequences of images or frames is an instance of the broader, more general problem of modeling the intricate relationships between visual events unfolding over time and the features of the language used to interpret, describe, or narrate them. Therefore, solving these tasks requires models to be capable of identifying and managing such intricacies. We consider five seemingly different tasks, which we argue are compelling instances of this broader multimodal problem. Consistently, we claim that these tasks pose a common set of challenges and share similarities in terms of modeling and evaluation approaches. Building on this perspective, we identify key open questions and propose several research directions for future investigation. We claim that improving language-and-vision models' understanding of visual events is both timely and essential, given their growing applications. Additionally, this challenge offers significant scientific insight, advancing model development through principles of human cognition and language use.

</details>


### [369] [Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors](https://arxiv.org/abs/2502.13311)

*Jian Wang, Yinpei Dai, Yichi Zhang, Ziqiao Ma, Wenjie Li, Joyce Chai*

**Main category:** cs.CL

**Keywords:** intelligent tutoring agents, large language models, coding tutoring, knowledge tracing, adaptive guidance

**Relevance Score:** 9

**TL;DR:** This paper presents TRAVER, a novel workflow for coding tutoring agents powered by LLMs that combines knowledge tracing and verification to guide users in completing coding tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing interest in using intelligent tutoring agents based on large language models for personalized education, but their effectiveness in real-world task guidance is underexplored, particularly in coding.

**Method:** The authors propose the Trace-and-Verify (TRAVER) workflow, which uses knowledge tracing to gauge a student's progress and employs turn-by-turn verification to provide adaptive guidance.

**Key Contributions:**

	1. Introduction of the Trace-and-Verify (TRAVER) workflow for coding tutoring.
	2. Development of DICT, an evaluation protocol for assessing tutor agents.
	3. Demonstration of significantly improved success rates in coding task completions with TRAVER.

**Result:** Experimental results show that TRAVER significantly improves the success rate of users completing coding tasks compared to existing tutoring approaches.

**Limitations:** The study is primarily focused on coding and does not explore wider applications in depth.

**Conclusion:** While the study focuses on coding tutoring, the insights gained from TRAVER suggest its methodological contributions are applicable to various human task learning domains.

**Abstract:** Intelligent tutoring agents powered by large language models (LLMs) have been increasingly explored to deliver personalized knowledge in areas such as language learning and science education. However, their capabilities in guiding users to solve complex real-world tasks remain underexplored. To address this limitation, in this work, we focus on coding tutoring, a challenging problem that requires tutors to proactively guide students towards completing predefined coding tasks. We propose a novel agent workflow, Trace-and-Verify (TRAVER), which combines knowledge tracing to estimate a student's knowledge state and turn-by-turn verification to ensure effective guidance toward task completion. We introduce DICT, an automatic evaluation protocol that assesses tutor agents using controlled student simulation and code generation tests. Extensive experiments reveal the challenges of coding tutoring and demonstrate that TRAVER achieves a significantly higher success rate. Although we use code tutoring as an example in this paper, our approach can be extended beyond coding, providing valuable insights into advancing tutoring agents for human task learning.

</details>


### [370] [A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?](https://arxiv.org/abs/2502.14924)

*Ibrahim Alabdulmohsin, Andreas Steiner*

**Main category:** cs.CL

**Keywords:** Fractal structure, Large language models, Text generation, Language complexity, Detection of synthetic texts

**Relevance Score:** 9

**TL;DR:** This paper investigates the fractal structure of language and how large language models (LLMs) replicate these characteristics, focusing on the impact of temperature settings and prompting methods, and includes a dataset for analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the fractal structure of language and assess if large language models can replicate this complexity, and to understand conditions under which they fail to do so.

**Method:** The study analyzes the fractal characteristics of language through a comparison of natural language and LLM output, examining variations based on temperature settings and prompting methods.

**Key Contributions:**

	1. Identified fractal characteristics in natural language vs. LLM outputs.
	2. Revealed the influence of temperature setting and prompting on LLM performance.
	3. Released a comprehensive dataset of LLM-generated and human-generated texts for further research.

**Result:** Findings indicate that LLMs exhibit fractal parameters that vary widely compared to the narrow range observed in natural language, suggesting that these parameters could help in detecting LLM-generated texts.

**Limitations:** 

**Conclusion:** The work reveals insights into the relationship between fractal properties and LLM outputs, highlighting implications for text generation and evaluation.

**Abstract:** Language exhibits a fractal structure in its information-theoretic complexity (i.e. bits per token), with self-similarity across scales and long-range dependence (LRD). In this work, we investigate whether large language models (LLMs) can replicate such fractal characteristics and identify conditions-such as temperature setting and prompting method-under which they may fail. Moreover, we find that the fractal parameters observed in natural language are contained within a narrow range, whereas those of LLMs' output vary widely, suggesting that fractal parameters might prove helpful in detecting a non-trivial portion of LLM-generated texts. Notably, these findings, and many others reported in this work, are robust to the choice of the architecture; e.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset comprising of over 240,000 articles generated by various LLMs (both pretrained and instruction-tuned) with different decoding temperatures and prompting methods, along with their corresponding human-generated texts. We hope that this work highlights the complex interplay between fractal properties, prompting, and statistical mimicry in LLMs, offering insights for generating, evaluating and detecting synthetic texts.

</details>


### [371] [Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models](https://arxiv.org/abs/2502.15094)

*Marianne Chuang, Gabriel Chuang, Cheryl Chuang, John Chuang*

**Main category:** cs.CL

**Keywords:** large language models, corporate climate disclosures, greenwashing, LLM-as-a-Judge, sustainability

**Relevance Score:** 8

**TL;DR:** The paper examines the effectiveness of large language models in evaluating corporate climate disclosures and their resistance to greenwashing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how large language models can be employed to evaluate corporate sustainability reports and exposed to greenwashing tactics.

**Method:** The study employs the LLM-as-a-Judge (LLMJ) methodology to score emissions reduction reports and analyzes LLM behavior when prompted to generate greenwashed responses.

**Key Contributions:**

	1. Introduces LLM-as-a-Judge methodology for scoring corporate climate reports
	2. Demonstrates effectiveness of LLM scoring systems in differentiating company performance
	3. Highlights robustness of pairwise comparison method against greenwashing

**Result:** Two scoring systems derived from the LLMJ methodology effectively distinguish companies based on their climate performance, with the pairwise comparison demonstrating superior robustness against LLM-generated greenwashed responses.

**Limitations:** Potential limitations include the reliance on LLM capabilities and the need for continuous adaptation to keep pace with evolving greenwashing techniques.

**Conclusion:** The findings indicate that while LLMs can be useful in assessing corporate sustainability, their susceptibility to greenwashing requires careful methodological design to ensure reliable evaluations.

**Abstract:** We study the use of large language models (LLMs) to both evaluate and greenwash corporate climate disclosures. First, we investigate the use of the LLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on emissions reduction targets and progress. Second, we probe the behavior of an LLM when it is prompted to greenwash a response subject to accuracy and length constraints. Finally, we test the robustness of the LLMJ methodology against responses that may be greenwashed using an LLM. We find that two LLMJ scoring systems, numerical rating and pairwise comparison, are effective in distinguishing high-performing companies from others, with the pairwise comparison system showing greater robustness against LLM-greenwashed responses.

</details>


### [372] [Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning](https://arxiv.org/abs/2502.15361)

*Xuyang Wu, Jinming Nian, Ting-Ruen Wei, Zhiqiang Tao, Hsin-Tai Wu, Yi Fang*

**Main category:** cs.CL

**Keywords:** large language models, social bias, reasoning, mitigation, machine learning

**Relevance Score:** 9

**TL;DR:** The paper systematically evaluates social bias in reasoning generated by large language models (LLMs) and proposes a mitigation method called Answer Distribution as Bias Proxy (ADBP).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study is motivated by the potential for LLMs to generate reasoning that reflects harmful social stereotypes, which can lead to misleading conclusions in critical applications.

**Method:** The authors analyze LLM outputs using the BBQ dataset, assessing prediction accuracy and bias across various reasoning models, including several instruction-tuned LLMs.

**Key Contributions:**

	1. First systematic evaluation of social bias in LLM-generated reasoning
	2. Development of the ADBP method for detecting and mitigating bias
	3. Demonstration of bias correlation with prediction inaccuracies

**Result:** The evaluation shows a correlation between biased reasoning steps and incorrect predictions. ADBP effectively detects and mitigates this bias, often outperforming stereotype-free baselines.

**Limitations:** The study primarily focuses on existing LLMs and may not generalize to future models or completely eliminate bias.

**Conclusion:** The proposed ADBP method not only reduces bias in LLM outputs but also enhances the accuracy of predictions, addressing important ethical concerns in AI applications.

**Abstract:** Recent advances in large language models (LLMs) have enabled automatic generation of chain-of-thought (CoT) reasoning, leading to strong performance on tasks such as math and code. However, when reasoning steps reflect social stereotypes (e.g., those related to gender, race or age), they can reinforce harmful associations and lead to misleading conclusions. We present the first systematic evaluation of social bias within LLM-generated reasoning, using the BBQ dataset to analyze both prediction accuracy and bias. Our study spans a wide range of mainstream reasoning models, including instruction-tuned and CoT-augmented variants of DeepSeek-R1 (8B/32B), ChatGPT, and other open-source LLMs. We quantify how biased reasoning steps correlate with incorrect predictions and often lead to stereotype expression. To mitigate reasoning-induced bias, we propose Answer Distribution as Bias Proxy (ADBP), a lightweight mitigation method that detects bias by tracking how model predictions change across incremental reasoning steps. ADBP outperforms a stereotype-free baseline in most cases, mitigating bias and improving the accuracy of LLM outputs. Code will be released upon paper acceptance.

</details>


### [373] [GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking](https://arxiv.org/abs/2502.16514)

*Yingjian Chen, Haoran Liu, Yinhong Liu, Jinxiang Xie, Rui Yang, Han Yuan, Yanran Fu, Peng Yuan Zhou, Qingyu Chen, James Caverlee, Irene Li*

**Main category:** cs.CL

**Keywords:** fact-checking, large language models, knowledge graphs, graph neural networks, multihop reasoning

**Relevance Score:** 9

**TL;DR:** GraphCheck is a fact-checking framework using knowledge graphs to enhance LLM performance, addressing multihop reasoning and resource efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing fact-checking methods struggle with subtle factual errors in long-form text, especially in specialized fields like medicine, and are often resource-intensive.

**Method:** GraphCheck employs extracted knowledge graphs and processes them with Graph Neural Networks as soft prompts to improve LLM text representation and reasoning capabilities.

**Key Contributions:**

	1. Introduction of GraphCheck framework for fact-checking
	2. Enhanced multihop reasoning capabilities using knowledge graphs
	3. Significant resource efficiency by reducing model calls

**Result:** GraphCheck achieves up to a 7.1% improvement on seven benchmarks in general and medical domains, outperforming existing specialized fact-checkers with fewer parameters.

**Limitations:** Further evaluation on more specialized domains is needed to fully understand the framework's applicability.

**Conclusion:** GraphCheck provides an innovative solution for efficient and precise fact-checking in long-form texts by leveraging structured knowledge and graph-based reasoning.

**Abstract:** Large language models (LLMs) are widely used, but they often generate subtle factual errors, especially in long-form text. These errors are fatal in some specialized domains such as medicine. Existing fact-checking with grounding documents methods face two main challenges: (1) they struggle to understand complex multihop relations in long documents, often overlooking subtle factual errors; (2) most specialized methods rely on pairwise comparisons, requiring multiple model calls, leading to high resource and computational costs. To address these challenges, we propose GraphCheck, a fact-checking framework that uses extracted knowledge graphs to enhance text representation. Graph Neural Networks further process these graphs as a soft prompt, enabling LLMs to incorporate structured knowledge more effectively. Enhanced with graph-based reasoning, GraphCheck captures multihop reasoning chains that are often overlooked by existing methods, enabling precise and efficient fact-checking in a single inference call. Experimental results on seven benchmarks spanning both general and medical domains demonstrate up to a 7.1% overall improvement over baseline models. Notably, GraphCheck outperforms existing specialized fact-checkers and achieves comparable performance with state-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.

</details>


### [374] [CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter](https://arxiv.org/abs/2502.16880)

*Yepeng Weng, Dianwen Mei, Huishi Qiu, Xujie Chen, Li Liu, Jiang Tian, Zhongchao Shi*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, training-inference alignment, efficiency, latency reduction

**Relevance Score:** 8

**TL;DR:** CORAL is a framework that enhances speculative decoding in large language models by improving training-inference alignment and reducing inference latency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance issues in speculative decoding caused by training-inference misalignment and to improve the efficiency of large language model inference.

**Method:** Introduce the CORAL framework with Cross-Step Representation Alignment to enhance training step consistency and a weight-grouping mechanism to reduce LM head parameters during inference.

**Key Contributions:**

	1. Introduced CORAL framework for improved speculative drafting.
	2. Developed Cross-Step Representation Alignment for training consistency.
	3. Implemented weight-grouping mechanism to reduce inference latency.

**Result:** CORAL achieves speedup ratios of 2.50x-4.07x on multiple LLM families and benchmark datasets, outperforming existing methods like EAGLE-2 and HASS.

**Limitations:** 

**Conclusion:** CORAL effectively reduces training-inference misalignment and significantly increases the speed of modern large language models.

**Abstract:** Speculative decoding is a powerful technique that accelerates Large Language Model (LLM) inference by leveraging a lightweight speculative draft model. However, existing designs suffers in performance due to misalignment between training and inference. Recent methods have tried to solve this issue by adopting a multi-step training strategy, but the complex inputs of different training steps make it harder for the draft model to converge. To address this, we propose CORAL, a novel framework that improves both accuracy and efficiency in speculative drafting. CORAL introduces Cross-Step Representation Alignment, a method that enhances consistency across multiple training steps, significantly improving speculative drafting performance. Additionally, we identify the LM head as a major bottleneck in the inference speed of the draft model. We introduce a weight-grouping mechanism that selectively activates a subset of LM head parameters during inference, substantially reducing the latency of the draft model. We evaluate CORAL on three LLM families and three benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that CORAL effectively mitigates training-inference misalignment and delivers significant speedup for modern LLMs with large vocabularies.

</details>


### [375] [Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch](https://arxiv.org/abs/2502.17173)

*Xueru Wen, Jie Lou, Zichao Li, Yaojie Lu, Xing Yu, Yuqiu Ji, Guohai Xu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Debing Zhang*

**Main category:** cs.CL

**Keywords:** reward models, human preferences, Chinese language, human supervision, NLP

**Relevance Score:** 9

**TL;DR:** This paper introduces CheemsBench and CheemsPreference, a new benchmark and dataset for evaluating reward models in Chinese, highlighting the limitations of existing RMs and the importance of human supervision.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of robust evaluation benchmarks and datasets for reward models (RMs) in Chinese, which is a gap in existing research that primarily focuses on English.

**Method:** The paper introduces CheemsBench, a human-annotated RM evaluation benchmark for Chinese contexts, and CheemsPreference, a large-scale dataset created through human-machine collaboration. The authors evaluate several open-source RMs using these resources.

**Key Contributions:**

	1. Introduction of CheemsBench, a benchmark for Chinese reward models.
	2. Development of CheemsPreference, a diverse dataset for RM training.
	3. Demonstrated limitations of existing RMs in capturing human preferences in Chinese scenarios.

**Result:** Significant limitations were found in the ability of existing RMs to capture human preferences in Chinese contexts. The constructed RM using CheemsPreference achieved state-of-the-art performance on CheemsBench.

**Limitations:** Limited to evaluating RMs in Chinese, which may not generalize to other languages or regions.

**Conclusion:** The findings indicate the critical role of high-quality human supervision in the development of reward models to effectively align with human preferences, suggesting that AI-generated data alone is insufficient.

**Abstract:** Reward models (RMs) are crucial for aligning large language models (LLMs) with human preferences. However, most RM research is centered on English and relies heavily on synthetic resources, which leads to limited and less reliable datasets and benchmarks for Chinese. To address this gap, we introduce CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese contexts, and CheemsPreference, a large-scale and diverse preference dataset annotated through human-machine collaboration to support Chinese RM training. We systematically evaluate open-source discriminative and generative RMs on CheemsBench and observe significant limitations in their ability to capture human preferences in Chinese scenarios. Additionally, based on CheemsPreference, we construct an RM that achieves state-of-the-art performance on CheemsBench, demonstrating the necessity of human supervision in RM training. Our findings reveal that scaled AI-generated data struggles to fully capture human preferences, emphasizing the importance of high-quality human supervision in RM development.

</details>


### [376] [Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning](https://arxiv.org/abs/2502.18001)

*Xinghao Chen, Zhijing Sun, Wenjin Guo, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, Xiaoyu Shen*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Small Language Models, reasoning tasks, distillation

**Relevance Score:** 9

**TL;DR:** This study investigates the distillation of Chain-of-Thought (CoT) capabilities from Large Language Models (LLMs) to Small Language Models (SLMs), identifying key factors that impact effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to optimize the distillation of reasoning capabilities from LLMs to SLMs while addressing the increased computational demands of CoT prompting.

**Method:** The study systematically examines CoT distillation factors by experimenting with four teacher models and seven student models on various reasoning datasets, analyzing performance variations based on model granularity, format, and teacher model choice.

**Key Contributions:**

	1. Identified non-monotonic relationship of granularity in SLMs versus LLMs
	2. Demonstrated limited effect of CoT format on SLM performance
	3. Highlighted importance of supervision diversity over teacher model accuracy

**Result:** Findings reveal that SLMs demonstrate a non-monotonic relationship with reasoning granularity, CoT format has less impact on SLMs compared to LLMs, and diversity in CoT supervision can be more beneficial than sheer accuracy in teacher models.

**Limitations:** 

**Conclusion:** The study underscores the necessity of customizing CoT strategies for distinct student models, providing valuable guidance for enhancing CoT distillation methods in SLMs.

**Abstract:** Large Language Models (LLMs) excel in reasoning tasks through Chain-of-Thought (CoT) prompting. However, CoT prompting greatly increases computational demands, which has prompted growing interest in distilling CoT capabilities into Small Language Models (SLMs). This study systematically examines the factors influencing CoT distillation, including the choice of granularity, format and teacher model. Through experiments involving four teacher models and seven student models across seven mathematical and commonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs, SLMs exhibit a non-monotonic relationship with granularity, with stronger models benefiting from finer-grained reasoning and weaker models performing better with simpler CoT supervision; (2) CoT format significantly impacts LLMs but has minimal effect on SLMs, likely due to their reliance on supervised fine-tuning rather than pretraining preferences; (3) Stronger teacher models do NOT always produce better student models, as diversity and complexity in CoT supervision can outweigh accuracy alone. These findings emphasize the need to tailor CoT strategies to specific student model, offering actionable insights for optimizing CoT distillation in SLMs. The code and datasets are available at https://github.com/EIT-NLP/Distilling-CoT-Reasoning.

</details>


### [377] [Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs](https://arxiv.org/abs/2502.18791)

*Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter*

**Main category:** cs.CL

**Keywords:** LLM, literature analysis, data extraction, Chain-of-Thought, empirical analysis

**Relevance Score:** 9

**TL;DR:** A semi-automated approach using LLMs accelerates literature analysis by extracting experimental results from relevant papers, resulting in a new structured dataset (LLMEvalDB) to facilitate ongoing insights into LLM behaviors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of synthesizing findings from the increasing volume of LLM studies by reducing the manual effort required for literature analysis and data extraction.

**Method:** Developed a semi-automated approach that utilizes LLMs to identify relevant arXiv papers, extract experimental results and their attributes, organizing them into LLMEvalDB for easier access and analysis.

**Key Contributions:**

	1. Introduction of LLMEvalDB, a structured dataset for LLM analysis
	2. Significant reduction in data extraction time
	3. Empirical insights into LLM performance on various tasks

**Result:** Reduces the effort of literature surveying and data extraction by over 93% compared to manual methods; LLMEvalDB reproduces key findings and discovers new insights related to LLM performance across different tasks.

**Limitations:** 

**Conclusion:** LLMEvalDB serves as a continually updated dataset that supports ongoing analysis of LLMs, providing important insights while also streamlining the process of literature review in LLM research.

**Abstract:** The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB. We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches. We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding & multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.

</details>


### [378] [Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization](https://arxiv.org/abs/2502.19127)

*Siyuan Zhang, Yichi Zhang, Yinpeng Dong, Hang Su*

**Main category:** cs.CL

**Keywords:** Large Language Models, factual hallucinations, preference optimization, factual QA, dataset

**Relevance Score:** 9

**TL;DR:** This paper introduces PKUE, a method to fine-tune LLMs to improve factual accuracy, and constructs FactualBench, a large dataset for evaluating factual QA.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to address the problem of factual hallucinations in LLMs which mislead users due to misaligned responses with objective facts.

**Method:** PKUE fine-tunes LLMs on self-generated responses to factual questions using preference optimization, alongside the creation of FactualBench, a 181k data QA dataset.

**Key Contributions:**

	1. Introduction of PKUE for fine-tuning LLMs on factual questions
	2. Development of FactualBench dataset for factual QA evaluation
	3. Demonstration of significant improvement in LLM factual performance across different tasks

**Result:** Extensive experiments show that PKUE enhances LLM performance across factual tasks and general tasks, with improvements consistent in various languages.

**Limitations:** 

**Conclusion:** PKUE effectively improves LLM's capability in factual tasks, addressing issues of generalization and performance trade-offs seen in prior methods.

**Abstract:** Large Language Models (LLMs) often struggle to align their responses with objective facts, resulting in the issue of factual hallucinations, which can be difficult to detect and mislead users without relevant knowledge. Although post-training techniques have been employed to mitigate the issue, existing methods usually suffer from poor generalization and trade-offs in different capabilities. In this paper, we propose to address it by directly augmenting LLM's fundamental ability to precisely leverage its knowledge and introduce PKUE, which fine-tunes the model on self-generated responses to precise and simple factual questions through preference optimization. Furthermore, we construct FactualBench, a comprehensive and precise factual QA dataset containing 181k Chinese data spanning 21 domains, to facilitate both evaluation and training. Extensive experiments demonstrate that PKUE significantly improves LLM overall performance, with consistent enhancement across factual tasks of various forms, general tasks beyond factuality, and tasks in a different language.

</details>


### [379] [Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs](https://arxiv.org/abs/2502.19148)

*Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang*

**Main category:** cs.CL

**Keywords:** large language models, real-time adaptation, user preferences, online learning, computational efficiency

**Relevance Score:** 9

**TL;DR:** Amulet is a novel framework for real-time adaptation of large language models (LLMs) to align with user preferences using online learning techniques and user prompts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for LLMs to align with diverse and changing user preferences, which are often not adequately captured during model training.

**Method:** Amulet formulates the decoding of each token as an online learning problem, allowing real-time adaptation based on simple prompts provided by the user.

**Key Contributions:**

	1. Introduction of a training-free framework for real-time preference adaptation in LLMs.
	2. Formulation of token decoding as an online learning problem.
	3. Provision of a closed-form solution to reduce computational costs during optimization.

**Result:** Amulet achieves significant performance improvements across various LLMs, datasets, and user preferences while maintaining high computational efficiency.

**Limitations:** 

**Conclusion:** Amulet enables effective and efficient real-time optimization of LLMs to meet personalized user demands without the need for retraining.

**Abstract:** How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse regarding culture, values, or time. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduce Amulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.

</details>


### [380] [R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning](https://arxiv.org/abs/2502.19735)

*Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, Osamu Yoshie*

**Main category:** cs.CL

**Keywords:** machine translation, reinforcement learning, chain-of-thought, language models, reasoning

**Relevance Score:** 7

**TL;DR:** This paper presents R1-Translator, a framework that enhances machine translation (MT) by incorporating inference-time reasoning through reinforcement learning, using human-aligned chain-of-thought (CoT) patterns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of structured reasoning in machine translation, an area that remains underexploited despite its relevance to human translator methods.

**Method:** The framework employs reinforcement learning to develop and utilize six expert-curated CoT templates that align with human reasoning strategies in MT tasks.

**Key Contributions:**

	1. Extension of reasoning-based translation to broader MT scenarios, including unseen languages.
	2. Formalization of six human-aligned CoT templates.
	3. Implementation of self-evolving CoT discovery via reinforcement learning.

**Result:** The results show improved translation performance across 10+ languages and multiple translation scenarios, particularly in languages that were not included in the training phase.

**Limitations:** 

**Conclusion:** R1-Translator demonstrates that reasoning-based translation can significantly enhance MT adaptability and performance in diverse contexts.

**Abstract:** Despite recent breakthroughs in reasoning-enhanced large language models (LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine translation (MT), where human translators naturally employ structured, multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored. Existing methods either design a fixed CoT tailored for a specific MT sub-task (e.g., literature translation), or rely on synthesizing CoTs unaligned with humans and supervised fine-tuning (SFT) prone to overfitting, limiting their adaptability to diverse translation scenarios. This paper introduces R1-Translator (R1-T1), a novel framework to achieve inference-time reasoning for general MT via reinforcement learning (RL) with human-aligned CoTs comprising six common patterns. Our approach pioneers three innovations: (1) extending reasoning-based translation to broader MT scenarios (e.g., multilingual MT, domain MT) unseen in the training phase; (2) formalizing six expert-curated CoT templates that mirror hybrid human strategies like context-aware paraphrasing and back translation; and (3) enabling self-evolving CoT discovery through RL. Both human and automatic evaluation results indicate a steady translation performance improvement in a total of 10+ languages and 40+ translation directions on Flores-101 test set and four domain-specific MT tasks, especially on the languages unseen from training.

</details>


### [381] [GeoEdit: Geometric Knowledge Editing for Large Language Models](https://arxiv.org/abs/2502.19953)

*Yujie Feng, Liming Zhan, Zexin Lu, Yongxin Xu, Xu Chu, Yasha Wang, Jiannong Cao, Philip S. Yu, Xiao-Ming Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Editing, Machine Learning, Geometric Methods, Model Preservation

**Relevance Score:** 8

**TL;DR:** GeoEdit is a novel framework for efficiently updating knowledge in large language models while preserving general knowledge using geometric relationships of parameter updates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Regular updates are vital for maintaining current knowledge in LLMs, but existing training methods often fail to balance the incorporation of new knowledge without losing general knowledge.

**Method:** GeoEdit utilizes a direction-aware knowledge identification method to differentiate between neurons for new and general knowledge, employing a 'forget-then-learn' approach and an importance-guided task vector fusion technique.

**Key Contributions:**

	1. Introduction of the GeoEdit framework for knowledge editing in LLMs.
	2. Direction-aware identification of knowledge-related neurons to preserve model generalization.
	3. Importance-guided task vector fusion to filter redundant information.

**Result:** GeoEdit outperforms existing state-of-the-art methods on two publicly available datasets for knowledge editing in LLMs.

**Limitations:** 

**Conclusion:** GeoEdit improves model editing performance by effectively managing the update of new knowledge while conserving general knowledge, ensuring better model generalization.

**Abstract:** Regular updates are essential for maintaining up-to-date knowledge in large language models (LLMs). Consequently, various model editing methods have been developed to update specific knowledge within LLMs. However, training-based approaches often struggle to effectively incorporate new knowledge while preserving unrelated general knowledge. To address this challenge, we propose a novel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes the geometric relationships of parameter updates from fine-tuning to differentiate between neurons associated with new knowledge updates and those related to general knowledge perturbations. By employing a direction-aware knowledge identification method, we avoid updating neurons with directions approximately orthogonal to existing knowledge, thus preserving the model's generalization ability. For the remaining neurons, we integrate both old and new knowledge for aligned directions and apply a "forget-then-learn" editing strategy for opposite directions. Additionally, we introduce an importance-guided task vector fusion technique that filters out redundant information and provides adaptive neuron-level weighting, further enhancing model editing performance. Extensive experiments on two publicly available datasets demonstrate the superiority of GeoEdit over existing state-of-the-art methods.

</details>


### [382] [PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues](https://arxiv.org/abs/2502.21017)

*Fangxu Yu, Lai Jiang, Shenyi Huang, Zhen Wu, Xinyu Dai*

**Main category:** cs.CL

**Keywords:** Theory of Mind, Persuasive dialogues, Large Language Models, Benchmark, Social interactions

**Relevance Score:** 9

**TL;DR:** PersuasiveToM is a new benchmark for evaluating the Theory of Mind abilities of LLMs in persuasive dialogues, focusing on complex social interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing benchmarks that do not capture the complexity of real-world social interactions in evaluating Theory of Mind in LLMs.

**Method:** PersuasiveToM introduces two tasks: ToM Reasoning for tracking desires, beliefs, and intentions, and ToM Application for assessing the use of inferred mental states in persuasion strategies. It tests eight leading LLMs.

**Key Contributions:**

	1. Introduction of PersuasiveToM benchmark for LLMs
	2. Focus on persuasive dialogues and complex social interactions
	3. Identification of LLMs' struggles with evolving mental state tracking

**Result:** Experiments show that while LLMs perform well on simple questions, they struggle significantly with tracking evolving mental states and understanding comprehensive dialogues.

**Limitations:** 

**Conclusion:** PersuasiveToM enables a more effective evaluation of ToM reasoning in LLMs, specifically targeting complex psychological activities.

**Abstract:** The ability to understand and predict the mental states of oneself and others, known as the Theory of Mind (ToM), is crucial for effective social scenarios. Although recent studies have evaluated ToM in Large Language Models (LLMs), existing benchmarks focus on simplified settings (e.g., Sally-Anne-style tasks) and overlook the complexity of real-world social interactions. To mitigate this gap, we propose PersuasiveToM, a benchmark designed to evaluate the ToM abilities of LLMs in persuasive dialogues. Our framework contains two core tasks: ToM Reasoning, which tests tracking of evolving desires, beliefs, and intentions; and ToM Application, which assesses the use of inferred mental states to predict and evaluate persuasion strategies. Experiments across eight leading LLMs reveal that while models excel on multiple questions, they struggle with the tasks that need tracking the dynamics and shifts of mental states and understanding the mental states in the whole dialogue comprehensively. Our aim with PersuasiveToM is to allow an effective evaluation of the ToM reasoning ability of LLMs with more focus on complex psychological activities. Our code is available at https://github.com/Yu-Fangxu/PersuasiveToM.

</details>


### [383] [Detecting LLM-Generated Korean Text through Linguistic Feature Analysis](https://arxiv.org/abs/2503.00032)

*Shinwoo Park, Shubin Kim, Do-Kyung Kim, Yo-Sub Han*

**Main category:** cs.CL

**Keywords:** LLM detection, Korean language, KatFish, KatFishNet, linguistic analysis

**Relevance Score:** 8

**TL;DR:** The paper proposes KatFish, a benchmark dataset and a detection method (KatFishNet) for distinguishing between human-written and LLM-generated Korean text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large language models (LLMs), it is increasingly difficult to differentiate between human and LLM-generated text, which is essential for maintaining academic integrity and ethical research practices.

**Method:** The authors created a benchmark dataset named KatFish featuring human-written and LLM-generated Korean text, and developed KatFishNet, a detection method that analyses linguistic features such as spacing patterns and part-of-speech diversity.

**Key Contributions:**

	1. Introduction of KatFish, a benchmark dataset for Korean LLM-generated text.
	2. Development of KatFishNet, a specialized detection method for Korean text.
	3. Demonstration of significant performance improvement in detecting LLM-generated text compared to existing methods.

**Result:** KatFishNet outperforms the best existing detection methods by an average of 19.78% in AUROC scores.

**Limitations:** 

**Conclusion:** The proposed dataset and detection method contribute significantly to the field of LLM detection for non-English languages, particularly Korean, and offer tools to uphold ethical standards in text generation.

**Abstract:** The rapid advancement of large language models (LLMs) increases the difficulty of distinguishing between human-written and LLM-generated text. Detecting LLM-generated text is crucial for upholding academic integrity, preventing plagiarism, protecting copyrights, and ensuring ethical research practices. Most prior studies on detecting LLM-generated text focus primarily on English text. However, languages with distinct morphological and syntactic characteristics require specialized detection approaches. Their unique structures and usage patterns can hinder the direct application of methods primarily designed for English. Among such languages, we focus on Korean, which has relatively flexible spacing rules, a rich morphological system, and less frequent comma usage compared to English. We introduce KatFish, the first benchmark dataset for detecting LLM-generated Korean text. The dataset consists of text written by humans and generated by four LLMs across three genres.   By examining spacing patterns, part-of-speech diversity, and comma usage, we illuminate the linguistic differences between human-written and LLM-generated Korean text. Building on these observations, we propose KatFishNet, a detection method specifically designed for the Korean language. KatFishNet achieves an average of 19.78% higher AUROC compared to the best-performing existing detection method. Our code and data are available at https://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.

</details>


### [384] [Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models](https://arxiv.org/abs/2503.01763)

*Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, Zhaochun Ren*

**Main category:** cs.CL

**Keywords:** tool learning, large language models, information retrieval, benchmark, tool retrieval

**Relevance Score:** 9

**TL;DR:** The paper introduces ToolRet, a benchmark for tool retrieval in large language models, highlighting the poor performance of existing IR models on diverse tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited context length of tool-using LLMs and the underexplored performance of information retrieval models in selecting useful tools from large toolsets.

**Method:** The authors propose ToolRet, a heterogeneous benchmark consisting of 7.6k retrieval tasks and a corpus of 43k tools, and benchmark six types of models on this new task.

**Key Contributions:**

	1. Introduction of ToolRet benchmark for tool retrieval tasks.
	2. Demonstration of poor performance of conventional IR models in tool retrieval.
	3. Provision of a large-scale training dataset to enhance tool retrieval capabilities.

**Result:** The models tested showed weak performance on ToolRet compared to conventional benchmarks, affecting the overall task pass rate for tool-using LLMs.

**Limitations:** 

**Conclusion:** The work highlights the need for improved retrieval capabilities in LLMs and provides a large-scale training dataset of over 200k instances to optimize IR models.

**Abstract:** Tool learning aims to augment large language models (LLMs) with diverse tools, enabling them to act as agents for solving practical tasks. Due to the limited context length of tool-using LLMs, adopting information retrieval (IR) models to select useful tools from large toolsets is a critical initial step. However, the performance of IR models in tool retrieval tasks remains underexplored and unclear. Most tool-use benchmarks simplify this step by manually pre-annotating a small set of relevant tools for each task, which is far from the real-world scenarios. In this paper, we propose ToolRet, a heterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks, and a corpus of 43k tools, collected from existing datasets. We benchmark six types of models on ToolRet. Surprisingly, even the models with strong performance in conventional IR benchmarks, exhibit poor performance on ToolRet. This low retrieval quality degrades the task pass rate of tool-use LLMs. As a further step, we contribute a large-scale training dataset with over 200k instances, which substantially optimizes the tool retrieval ability of IR models.

</details>


### [385] [SteerConf: Steering LLMs for Confidence Elicitation](https://arxiv.org/abs/2503.02863)

*Ziang Zhou, Tianyuan Jin, Jieming Shi, Qing Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, confidence calibration, steering prompts, machine learning, reliability

**Relevance Score:** 9

**TL;DR:** SteerConf is a framework that enhances the reliability of LLMs by improving their confidence score calibration through steering prompts, consistency measures, and calibration methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often exhibit overconfidence which affects their reliability, particularly in critical applications; therefore, improving their calibration is essential.

**Method:** SteerConf introduces a steering prompt strategy, a steered confidence consistency measure, and a calibration method that uses linear quantization to improve LLMs' confidence scores without any additional training.

**Key Contributions:**

	1. Steering prompt strategy for confidence score directionality.
	2. Steered confidence consistency measure for better calibration.
	3. Calibration method using linear quantization for answer selection.

**Result:** Experimental results show that SteerConf significantly outperforms existing calibration methods across various benchmarks, including professional knowledge and ethical reasoning tasks.

**Limitations:** The framework operates without additional training, which may limit its customization for specific tasks.

**Conclusion:** SteerConf demonstrates that adequately adjusting LLMs' confidence can lead to enhanced performance and reliability in real-world applications.

**Abstract:** Large Language Models (LLMs) exhibit impressive performance across diverse domains but often suffer from overconfidence, limiting their reliability in critical applications. We propose SteerConf, a novel framework that systematically steers LLMs' confidence scores to improve their calibration and reliability. SteerConf introduces three key components: (1) a steering prompt strategy that guides LLMs to produce confidence scores in specified directions (e.g., conservative or optimistic) by leveraging prompts with varying steering levels; (2) a steered confidence consistency measure that quantifies alignment across multiple steered confidences to enhance calibration; and (3) a steered confidence calibration method that aggregates confidence scores using consistency measures and applies linear quantization for answer selection. SteerConf operates without additional training or fine-tuning, making it broadly applicable to existing LLMs. Experiments on seven benchmarks spanning professional knowledge, common sense, ethics, and reasoning tasks, using advanced LLM models (GPT-3.5, LLaMA 3, GPT-4), demonstrate that SteerConf significantly outperforms existing methods, often by a significant margin. Our findings highlight the potential of steering the confidence of LLMs to enhance their reliability for safer deployment in real-world applications.

</details>


### [386] [LINGOLY-TOO: Disentangling Memorisation from Knowledge with Linguistic Templatisation and Orthographic Obfuscation](https://arxiv.org/abs/2503.02972)

*Jude Khouja, Karolina Korgul, Simi Hellsten, Lingyi Yang, Vlad Neacsu, Harry Mayne, Ryan Kearns, Andrew Bean, Adam Mahdi*

**Main category:** cs.CL

**Keywords:** Language Models, Reasoning Benchmark, Natural Language Processing, Machine Learning, Inference-Time Compute

**Relevance Score:** 8

**TL;DR:** LINGOLY-TOO is a new benchmark that evaluates reasoning abilities of language models independently from their prior knowledge, highlighting their limitations in consistent reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address inflated estimates of language models' reasoning abilities by evaluating their performance on reasoning tasks that are not solvable by prior knowledge.

**Method:** The benchmark introduces permuted reasoning problems using linguistically informed rulesets, allowing for multiple question variations while retaining the original reasoning steps needed for solutions.

**Key Contributions:**

	1. Introduction of LINGOLY-TOO benchmark for language model reasoning evaluation.
	2. Use of permutation-based problems to isolate reasoning from prior knowledge.
	3. Highlighting the brittleness of LLM reasoning abilities through experimental results.

**Result:** Experiments reveal that language models, when evaluated on consistent reasoning, perform poorly and show significant variance across question permutations, indicating brittle reasoning capabilities.

**Limitations:** The benchmark still relies on the existing reasoning capabilities of models and might not fully account for all aspects of reasoning.

**Conclusion:** The study emphasizes the need to distinguish between reasoning ability and prior knowledge in benchmarks, suggesting significant room for improvement in language model reasoning.

**Abstract:** The expanding knowledge and memorisation capacity of frontier language models allows them to solve many reasoning tasks directly by exploiting prior knowledge, leading to inflated estimates of their reasoning abilities. We introduce LINGOLY-TOO, a challenging reasoning benchmark grounded in natural language and designed to counteract the effect of non-reasoning abilities on reasoning estimates. Using linguistically informed rulesets, we permute reasoning problems written in real languages to generate numerous question variations. These permutations preserve the intrinsic reasoning steps required for each solution while reducing the likelihood problems are directly solvable with models' knowledge. Experiments and analyses show that models can circumvent reasoning and answer from prior knowledge. On a metric that rewards consistent reasoning, all models perform poorly and exhibit high variance across question permutations, indicating that Large Language Models' (LLMs) reasoning faculty remains brittle. Overall, results on the benchmark reflect the recent progress of Inference-Time Compute (ITC) models but suggest ample room for further improvement. The benchmark is a step towards better measurement of reasoning abilities of LLMs and offers a cautionary tale on the importance of disentangling reasoning abilities from models' internalised knowledge when developing reasoning benchmarks.

</details>


### [387] [Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions](https://arxiv.org/abs/2503.03862)

*Emmy Liu, Amanda Bertsch, Lintang Sutawika, Lindia Tjuatja, Patrick Fernandes, Lara Marinov, Michael Chen, Shreya Singhal, Carolin Lawrence, Aditi Raghunathan, Kiril Gashteovski, Graham Neubig*

**Main category:** cs.CL

**Keywords:** language models, model performance, architectural decisions

**Relevance Score:** 8

**TL;DR:** A meta-analysis of 92 open-source pretrained language models reveals that smaller, curated models can outperform larger ones due to design choices, achieving a 3-28% increase in performance prediction by considering factors beyond model size.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To quantify the effects of various design choices in language models beyond just size and training data, examining why smaller models can outperform larger ones under certain conditions.

**Method:** A meta-analysis of 92 open-source pretrained language models, comparing performance based on different model architectures and data sources.

**Key Contributions:**

	1. Meta-analysis of 92 pretrained models
	2. Insights into the impact of design decisions on performance
	3. Framework for systematic investigation of model development choices

**Result:** Found that including design choice characteristics leads to a 3-28% improvement in predicting model performance compared to only using model size and training tokens.

**Limitations:** 

**Conclusion:** The study provides a framework for future investigations into how various decisions during model development impact their capabilities.

**Abstract:** Improvements in language model capabilities are often attributed to increasing model size or training data, but in some cases smaller models trained on curated data or with different architectural decisions can outperform larger ones trained on more tokens. What accounts for this? To quantify the impact of these design choices, we meta-analyze 92 open-source pretrained models across a wide array of scales, including state-of-the-art open-weights models as well as less performant models and those with less conventional design decisions. We find that by incorporating features besides model size and number of training tokens, we can achieve a relative 3-28% increase in ability to predict downstream performance compared with using scale alone. Analysis of model design decisions reveal insights into data composition, such as the trade-off between language and code tasks at 15-25\% code, as well as the better performance of some architectural decisions such as choosing rotary over learned embeddings. Broadly, our framework lays a foundation for more systematic investigation of how model development choices shape final capabilities.

</details>


### [388] [DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models](https://arxiv.org/abs/2503.04240)

*Ruizhe Chen, Wenhao Chai, Zhifei Yang, Xiaotian Zhang, Joey Tianyi Zhou, Tony Quek, Soujanya Poria, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** Preference Optimization, LLMs, Alignment, Inference, Scalability

**Relevance Score:** 8

**TL;DR:** This paper introduces Diffusion-styled Preference Optimization (model), a novel method for aligning large language models (LLMs) with human preferences efficiently at the sentence level, enhancing performance while reducing inference-time latency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address limited scalability and latency challenges in existing inference-time alignment methods for LLMs.

**Method:** The proposed model performs alignment directly at the sentence level rather than at the token level, designed to be a plug-and-play module for various base models.

**Key Contributions:**

	1. Introduction of a policy-agnostic alignment method
	2. Improvement of alignment performance at sentence level
	3. Scalability enhancement for large models

**Result:** model shows superior alignment performance across multiple benchmarks (AlpacaEval 2, MT-bench, HH-RLHF), achieving a balance between alignment quality and latency, and enhancing large models like Llama-3-70B.

**Limitations:** 

**Conclusion:** The proposed method improves the alignment of LLMs with human preferences while maintaining scalability and reducing latency in inference.

**Abstract:** Inference-time alignment provides an efficient alternative for aligning LLMs with humans. However, these approaches still face challenges, such as limited scalability due to policy-specific value functions and latency during the inference phase. In this paper, we propose a novel approach, Diffusion-styled Preference Optimization (\model), which provides an efficient and policy-agnostic solution for aligning LLMs with humans. By directly performing alignment at sentence level, \model~avoids the time latency associated with token-level generation. Designed as a plug-and-play module, \model~can be seamlessly integrated with various base models to enhance their alignment. Extensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that \model~achieves superior alignment performance across various settings, achieving a favorable trade-off between alignment quality and inference-time latency. Furthermore, \model~demonstrates model-agnostic scalability, significantly improving the performance of large models such as Llama-3-70B.

</details>


### [389] [One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs](https://arxiv.org/abs/2503.04856)

*Junwoo Ha, Hyunjun Kim, Sangyoon Yu, Haon Park, Ashkan Yousefpour, Yuna Park, Suhyun Kim*

**Main category:** cs.CL

**Keywords:** adversarial testing, large language models, multi-turn prompts, single-turn queries, ML vulnerabilities

**Relevance Score:** 9

**TL;DR:** A framework for converting multi-turn adversarial prompts to single-turn queries, improving efficiency in testing LLM defenses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce manual effort in adversarial testing of large language models while maintaining or enhancing the effectiveness of adversarial prompts.

**Method:** The paper introduces M2S methods (Hyphenize, Numberize, Pythonize) to reformulate multi-turn dialogues into structured single-turn prompts.

**Key Contributions:**

	1. Introduces the M2S framework for prompt consolidation.
	2. Demonstrates higher success rates and reduced token usage with single-turn prompts.
	3. Reveals contextual vulnerabilities in LLM defenses.

**Result:** M2S methods achieve attack success rates between 70.6% and 95.9% on the Multi-turn Human Jailbreak dataset, outperforming multi-turn prompts by up to 17.5 percentage points and reducing token usage by over 50%.

**Limitations:** 

**Conclusion:** The M2S framework serves as a scalable tool for adversarial testing, highlighting vulnerabilities in LLM defenses.

**Abstract:** We introduce a novel framework for consolidating multi-turn adversarial ``jailbreak'' prompts into single-turn queries, significantly reducing the manual overhead required for adversarial testing of large language models (LLMs). While multi-turn human jailbreaks have been shown to yield high attack success rates, they demand considerable human effort and time. Our multi-turn-to-single-turn (M2S) methods -- Hyphenize, Numberize, and Pythonize -- systematically reformat multi-turn dialogues into structured single-turn prompts. Despite removing iterative back-and-forth interactions, these prompts preserve and often enhance adversarial potency: in extensive evaluations on the Multi-turn Human Jailbreak (MHJ) dataset, M2S methods achieve attack success rates from 70.6 percent to 95.9 percent across several state-of-the-art LLMs. Remarkably, the single-turn prompts outperform the original multi-turn attacks by as much as 17.5 percentage points while cutting token usage by more than half on average. Further analysis shows that embedding malicious requests in enumerated or code-like structures exploits ``contextual blindness'', bypassing both native guardrails and external input-output filters. By converting multi-turn conversations into concise single-turn prompts, the M2S framework provides a scalable tool for large-scale red teaming and reveals critical weaknesses in contemporary LLM defenses.

</details>


### [390] [InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2503.06692)

*Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, Yueting Zhuang*

**Main category:** cs.CL

**Keywords:** large language models, iterative reasoning, computational efficiency

**Relevance Score:** 7

**TL;DR:** InftyThink transforms monolithic reasoning in language models into an iterative process with intermediate summaries, reducing computational complexity and improving performance in long-context reasoning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical limitations of long-context reasoning in large language models, such as quadratic computational scaling and performance degradation beyond context boundaries.

**Method:** InftyThink introduces an iterative reasoning process interleaved with summarization, creating a sawtooth memory pattern that maintains computational efficiency.

**Key Contributions:**

	1. Introduction of InftyThink for iterative reasoning
	2. Transformation of long-context reasoning datasets into iterative format
	3. Empirical validation of reduced computational costs and improved performance

**Result:** Our methodology, applied to OpenR1-Math, resulted in 333K training instances and demonstrated 3-13% performance improvements on various benchmarks.

**Limitations:** 

**Conclusion:** InftyThink presents a scalable approach to complex reasoning in language models, challenging the trade-offs between reasoning depth and computational efficiency.

**Abstract:** Advanced reasoning in large language models has achieved remarkable performance on challenging tasks, but the prevailing long-context reasoning paradigm faces critical limitations: quadratic computational scaling with sequence length, reasoning constrained by maximum context boundaries, and performance degradation beyond pre-training context windows. Existing approaches primarily compress reasoning chains without addressing the fundamental scaling problem. To overcome these challenges, we introduce InftyThink, a paradigm that transforms monolithic reasoning into an iterative process with intermediate summarization. By interleaving short reasoning segments with concise progress summaries, our approach enables unbounded reasoning depth while maintaining bounded computational costs. This creates a characteristic sawtooth memory pattern that significantly reduces computational complexity compared to traditional approaches. Furthermore, we develop a methodology for reconstructing long-context reasoning datasets into our iterative format, transforming OpenR1-Math into 333K training instances. Experiments across multiple model architectures demonstrate that our approach reduces computational costs while improving performance, with Qwen2.5-Math-7B showing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks. Our work challenges the assumed trade-off between reasoning depth and computational efficiency, providing a more scalable approach to complex reasoning without architectural modifications.

</details>


### [391] [MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System](https://arxiv.org/abs/2503.09600)

*Jihao Zhao, Zhiyuan Ji, Zhaoxin Fan, Hanyu Wang, Simin Niu, Bo Tang, Feiyu Xiong, Zhiyu Li*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, text chunking, large language models, Mixture-of-Chunkers, chunk quality assessment

**Relevance Score:** 9

**TL;DR:** This paper addresses the neglected role of text chunking in Retrieval-Augmented Generation (RAG) for LLMs by introducing new metrics and a framework to improve chunk quality and system performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine and improve the text chunking process in RAG systems for better integration with LLMs and to enhance context handling.

**Method:** Introduces a dual-metric evaluation method for chunking quality and devises the Mixture-of-Chunkers (MoC) framework consisting of a three-stage processing mechanism.

**Key Contributions:**

	1. Introduction of Boundary Clarity and Chunk Stickiness metrics for chunking quality assessment
	2. Development of the Mixture-of-Chunkers framework
	3. Demonstration of improved performance in RAG systems through effective chunking.

**Result:** Experiments show that the proposed metrics and the MoC framework effectively improve chunking quality, addressing the challenges in the RAG system.

**Limitations:** Focuses primarily on chunking without extensively addressing other aspects of RAG systems.

**Conclusion:** The study validates the importance of chunking improvement in RAG and presents methods that enhance performance while managing trade-offs.

**Abstract:** Retrieval-Augmented Generation (RAG), while serving as a viable complement to large language models (LLMs), often overlooks the crucial aspect of text chunking within its pipeline. This paper initially introduces a dual-metric evaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable the direct quantification of chunking quality. Leveraging this assessment method, we highlight the inherent limitations of traditional and semantic chunking in handling complex contextual nuances, thereby substantiating the necessity of integrating LLMs into chunking process. To address the inherent trade-off between computational efficiency and chunking precision in LLM-based approaches, we devise the granularity-aware Mixture-of-Chunkers (MoC) framework, which consists of a three-stage processing mechanism. Notably, our objective is to guide the chunker towards generating a structured list of chunking regular expressions, which are subsequently employed to extract chunks from the original text. Extensive experiments demonstrate that both our proposed metrics and the MoC framework effectively settle challenges of the chunking task, revealing the chunking kernel while enhancing the performance of the RAG system.

</details>


### [392] [MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation](https://arxiv.org/abs/2503.10497)

*Weihao Xuan, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Aosong Feng, Dairui Liu, Yun Xing, Junjue Wang, Fan Gao, Jinghui Lu, Yuang Jiang, Huitao Li, Xin Li, Kunyu Yu, Ruihai Dong, Shangding Gu, Yuekang Li, Xiaofei Xie, Felix Juefei-Xu, Foutse Khomh, Osamu Yoshie, Qingyu Chen, Douglas Teodoro, Nan Liu, Randy Goebel, Lei Ma, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo, Irene Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilingual Evaluation, Cross-linguistic Reasoning, Benchmark, AI Equity

**Relevance Score:** 8

**TL;DR:** MMLU-ProX is a new multilingual benchmark assessing LLMs across 29 languages with identical questions for effective cross-linguistic evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of multilingual evaluation benchmarks that assess cross-linguistic reasoning abilities in LLMs.

**Method:** Creation of MMLU-ProX involving a comprehensive benchmark with 11,829 identical questions across 29 languages and a lite version with 658 questions per language, alongside rigorous translation and expert review processes.

**Key Contributions:**

	1. Introduction of MMLU-ProX as a multilingual LLM benchmark
	2. Systematic evaluation of multilingual capabilities of 36 state-of-the-art LLMs
	3. Identification of performance gaps in LLMs between high-resource and low-resource languages

**Result:** Evaluation of 36 state-of-the-art LLMs showed significant performance disparities, with high-resource languages performing well, while low-resource languages experienced declines up to 24.3%.

**Limitations:** 

**Conclusion:** MMLU-ProX will help develop inclusive AI systems and promote equitable technology access globally.

**Abstract:** Existing large language model (LLM) evaluation benchmarks primarily focus on English, while current multilingual tasks lack parallel questions that specifically assess cross-linguistic reasoning abilities. This dual limitation makes it challenging to comprehensively assess LLMs' performance in the multilingual setting. To fill this gap, we introduce MMLU-ProX, a comprehensive benchmark covering 29 languages, built on an English benchmark. Each language version consists of 11,829 identical questions, enabling direct cross-linguistic comparisons. Additionally, to meet efficient evaluation needs, we provide a lite version containing 658 questions per language. To ensure the high quality of MMLU-ProX, we employ a rigorous development process that involves multiple powerful LLMs for translation, followed by expert review to ensure accurate expression, consistent terminology, and cultural relevance. Building on this, we systematically evaluate 36 state-of-the-art LLMs, including reasoning-enhanced and multilingual-optimized LLMs. The results reveal significant disparities in the multilingual capabilities of LLMs: While they perform well in high-resource languages, their performance declines markedly in low-resource languages, with gaps of up to 24.3%. Through MMLU-ProX, we aim to advance the development of more inclusive AI systems and promote equitable access to technology across global contexts.

</details>


### [393] [CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding](https://arxiv.org/abs/2503.10688)

*Tadesse Destaw Belay, Ahmed Haj Ahmed, Alvin Grissom II, Iqra Ameer, Grigori Sidorov, Olga Kolesnikova, Seid Muhie Yimam*

**Main category:** cs.CL

**Keywords:** Cultural dimensions, Emotion analysis, Language models, NLP, Benchmarking

**Relevance Score:** 9

**TL;DR:** Introduction of CuLEmo, a benchmark for culture-aware emotion prediction in NLP.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address shortcomings in existing emotion benchmarks related to cultural dimensions and language reliability.

**Method:** Development of the Cultural Lenses on Emotion (CuLEmo) benchmark, evaluating cultural emotion prediction across six languages with crafted cultural reasoning questions.

**Key Contributions:**

	1. Introduction of the first culture-aware emotion benchmark across multiple languages
	2. Highlighting the variation of emotion understanding and LLM performance across cultures
	3. Providing publicly available dataset and evaluation code

**Result:** Significant variation in emotion conceptualizations and LLM performance across languages; English prompts with cultural context perform better than in-language prompts.

**Limitations:** 

**Conclusion:** The CuLEmo dataset can enhance understanding of emotion across cultures and improve NLP emotion tasks.

**Abstract:** NLP research has increasingly focused on subjective tasks such as emotion analysis. However, existing emotion benchmarks suffer from two major shortcomings: (1) they largely rely on keyword-based emotion recognition, overlooking crucial cultural dimensions required for deeper emotion understanding, and (2) many are created by translating English-annotated data into other languages, leading to potentially unreliable evaluation. To address these issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first benchmark designed to evaluate culture-aware emotion prediction across six languages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo comprises 400 crafted questions per language, each requiring nuanced cultural reasoning and understanding. We use this benchmark to evaluate several state-of-the-art LLMs on culture-aware emotion prediction and sentiment analysis tasks. Our findings reveal that (1) emotion conceptualizations vary significantly across languages and cultures, (2) LLMs performance likewise varies by language and cultural context, and (3) prompting in English with explicit country context often outperforms in-language prompts for culture-aware emotion and sentiment understanding. The dataset and evaluation code are publicly available.

</details>


### [394] [General Table Question Answering via Answer-Formula Joint Generation](https://arxiv.org/abs/2503.12345)

*Zhongyuan Wang, Richong Zhang, Zhijie Nie*

**Main category:** cs.CL

**Keywords:** TableQA, Spreadsheet Formula, LLM, TabAF, FromulaQA

**Relevance Score:** 8

**TL;DR:** This paper introduces a new system called TabAF that uses Spreadsheet Formula as a representation for advanced table question answering, enabling versatile handling of various question types and table structures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current TableQA methods relying on LLMs struggle with specific question types and table structures, necessitating a more versatile approach.

**Method:** The authors constructed a new dataset called FromulaQA and proposed a table answering framework, TabAF, which decodes answers and formulas with a single LLM backbone thus improving performance across multiple tasks.

**Key Contributions:**

	1. Introduction of FromulaQA dataset for table question answering
	2. Development of TabAF framework that unifies answer generation and formula decoding
	3. Achievement of state-of-the-art performance on several benchmarks

**Result:** TabAF, utilizing Llama3.1-70B, demonstrated state-of-the-art results on datasets like WikiTableQuestion, HiTab, and TabFact.

**Limitations:** 

**Conclusion:** TabAF represents a significant advancement in TableQA by integrating Spreadsheet Formulas for enhanced reasoning capabilities across diverse tabular formats.

**Abstract:** Advanced table question answering (TableQA) methods prompt large language models (LLMs) to generate answer text, SQL query, Python code, or custom operations, which impressively improve the complex reasoning problems in the TableQA task. However, these methods lack the versatility to cope with specific question types or table structures. In contrast, the Spreadsheet Formula, the widely used and well-defined operation language for tabular data, has not been thoroughly explored to solve TableQA. In this paper, we first attempt to use the Formula as the executable representation for solving complex reasoning on tables with different structures. Specifically, we construct \texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing datasets. In addition, we propose \texttt{TabAF}, a general table answering framework to solve multiple types of tasks over multiple types of tables simultaneously. Unlike existing methods, \texttt{TabAF} decodes answers and Formulas with a single LLM backbone, demonstrating great versatility and generalization. \texttt{TabAF} based on Llama3.1-70B achieves new state-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.

</details>


### [395] [RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning](https://arxiv.org/abs/2503.12759)

*Jerry Huang, Siddarth Madala, Risham Sidhu, Cheng Niu, Hao Peng, Julia Hockenmaier, Tong Zhang*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, curriculum learning, question answering

**Relevance Score:** 9

**TL;DR:** RAG-RL enhances retrieval-augmented generation systems by training answer generation models to also identify and cite relevant information from retrieved contexts, improving performance through curriculum learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional retrieval models in RAG systems, where imperfect recall and precision hinder the overall performance.

**Method:** RAG-RL employs a curriculum learning approach, training the model with easier examples containing only relevant contexts to enhance its citation and reasoning capabilities.

**Key Contributions:**

	1. Introduction of RAG-RL for improved answer generation in RAG systems
	2. Use of curriculum learning for better citation and reasoning skills
	3. Empirical insights on training sample construction and model performance

**Result:** The experiments demonstrate significant improvements in answer and citation accuracy across multiple open-domain multi-hop question answering datasets, even with increasing irrelevant passages.

**Limitations:** 

**Conclusion:** The study concludes that easier training samples provide effective signals for skill acquisition, with various post-training factors influencing overall model effectiveness.

**Abstract:** Retrieval-augmented generation (RAG) systems rely on retrieval models for identifying relevant contexts and answer generation models for utilizing those contexts. However, retrievers exhibit imperfect recall and precision, limiting downstream performance. We introduce RAG-RL, an answer generation model trained not only to produce answers but also to identify and cite relevant information from larger sets of retrieved contexts, shifting some of the burden of identifying relevant documents from the retriever to the answer generator. Our approach uses curriculum learning, where the model is first trained on easier examples that include only relevant contexts. Our experiments show that these training samples enable models to acquire citation and reasoning skills with greater sample efficiency and generalizability, demonstrating strong model performance even as the number of irrelevant passages increases. We benchmark our methods on three open-domain multi-hop question answering datasets and report significant gains in answer and citation accuracy. Our experiments provide empirical insights into how easier training samples can give models stronger signals for learning specific skills (e.g., citation generation) and how different components of post-training (e.g., training set construction, rule-based rewards, training sample ordering, etc.) impact final model performance.

</details>


### [396] [Optimizing Decomposition for Optimal Claim Verification](https://arxiv.org/abs/2503.15354)

*Yining Lu, Noah Ziems, Hy Dang, Meng Jiang*

**Main category:** cs.CL

**Keywords:** factuality evaluation, dynamic decomposition, reinforcement learning, bilevel optimization, text verification

**Relevance Score:** 7

**TL;DR:** The paper proposes a reinforcement learning framework for dynamically decomposing claims to optimize synchronization with verifiers, addressing misalignments in factual verification.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for evaluating factuality in long-form text do not effectively consider the interactions between decomposition and verification, often leading to mismatches in atomicity and suboptimal verification results.

**Method:** The authors formulate the problem of finding an optimal decomposition policy as a bilevel optimization problem and employ reinforcement learning to create a dynamic decomposition method that adapts based on verifier feedback.

**Key Contributions:**

	1. Introduces a bilevel optimization framework for decomposition and verification alignment.
	2. Proposes dynamic decomposition as a novel reinforcement learning approach to improve factual verification.
	3. Demonstrates improved verification metrics through experimental validation.

**Result:** Dynamic decomposition demonstrates improved verification confidence (0.07) and accuracy (0.12) over existing methods across various verifiers and datasets.

**Limitations:** The problem remains NP-hard, and the performance can depend on the design of verifiers and datasets used in experiments.

**Conclusion:** The proposed approach improves alignment between decomposition and verification, offering a more effective solution for factual evaluation of text.

**Abstract:** Current research on the \textit{Decompose-Then-Verify} paradigm for evaluating the factuality of long-form text typically treats decomposition and verification in isolation, overlooking their interactions and potential misalignment. We find that existing decomposition policies, typically hand-crafted demonstrations, do not align well with downstream verifiers in terms of atomicity -- a novel metric quantifying information density -- leading to suboptimal verification results. We formulate finding the optimal decomposition policy for optimal verification as a bilevel optimization problem. To approximate a solution for this strongly NP-hard problem, we propose dynamic decomposition, a reinforcement learning framework that leverages verifier feedback to learn a policy for dynamically decomposing claims to verifier-preferred atomicity. Experimental results show that dynamic decomposition outperforms existing decomposition policies, improving verification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on average across varying verifiers, datasets, and atomcities of input claims.

</details>


### [397] [Prompting is Not All You Need! Evaluating LLM Agent Simulation Methodologies with Real-World Online Customer Behavior Data](https://arxiv.org/abs/2503.20749)

*Yuxuan Lu, Jing Huang, Yan Han, Bingsheng Yao, Sisong Bei, Jiri Gesi, Yaochen Xie, Zheshen, Wang, Qi He, Dakuo Wang*

**Main category:** cs.CL

**Keywords:** LLM, behavior simulation, online shopping, fine-tuning, real-world data

**Relevance Score:** 8

**TL;DR:** This study evaluates the objective accuracy of LLMs in simulating human shopping behaviors and demonstrates that fine-tuning LLMs on real-world data enhances performance significantly.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the accuracy of LLM-generated human behavior simulations in online shopping contexts, moving beyond subjective measures of believability.

**Method:** Comprehensive evaluation of various LLMs on web shopping action generation using a large-scale dataset of customer actions, comparing prompt-only methods to fine-tuned approaches.

**Key Contributions:**

	1. First comprehensive evaluation of LLMs in online shopping action simulation
	2. Demonstrated effectiveness of fine-tuning LLMs on real-world data
	3. Showed that synthesized reasonings contribute to performance improvements

**Result:** Fine-tuning LLMs on behavioral data significantly improves action generation accuracy, and the addition of synthesized reasonings further enhances this performance.

**Limitations:** 

**Conclusion:** Real-world action data is crucial for improving LLMs' ability to simulate accurate human behaviors, providing insights for better behavior modeling.

**Abstract:** Recent research shows that LLMs can simulate ``believable'' human behaviors to power LLM agents via prompt-only methods. In this work, we focus on evaluating LLM's objective ``accuracy'' rather than the subjective ``believability'' in simulating human behavior, leveraging a large-scale, real-world dataset collected from customers' online shopping actions. We present the first comprehensive evaluation of state-of-the-art LLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web shopping action generation. Our results show that out-of-the-box LLM-generated actions are often misaligned with actual human behavior, whereas fine-tuning LLMs on real-world behavioral data substantially improves their ability to generate accurate actions compared to prompt-only methods. Furthermore, incorporating synthesized reasonings into model training leads to additional performance gains, demonstrating the value of explicit rationale in behavior modeling. This work evaluates state-of-the-art LLMs in behavior simulation and provides actionable insights into how real-world action data can enhance the fidelity of LLM agents.

</details>


### [398] [GTR: Graph-Table-RAG for Cross-Table Question Answering](https://arxiv.org/abs/2504.01346)

*Jiaru Zou, Dongqi Fu, Sirui Chen, Xinrui He, Zihao Li, Yada Zhu, Jiawei Han, Jingrui He*

**Main category:** cs.CL

**Keywords:** GraphRAG, table retrieval, cross-table QA, natural language processing, machine learning

**Relevance Score:** 8

**TL;DR:** The paper introduces a benchmark and framework for cross-table question answering using a heterogeneous graph representation of tables, enhancing LLMs' reasoning capabilities.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for effective retrieval of answers distributed across multiple tables in real-world scenarios and to fill the gap in available data for this task.

**Method:** The paper presents the Graph-Table-RAG (GTR) framework, which reorganizes table data into a heterogeneous graph and implements a hierarchical retrieval process alongside graph-aware prompting for LLMs.

**Key Contributions:**

	1. Introduction of the MultiTableQA benchmark with 60k tables and 25k queries
	2. Development of the Graph-Table-RAG framework (GTR)
	3. Demonstration of improved performance in cross-table question-answering tasks
	4. High deployment efficiency of the proposed method.

**Result:** GTR demonstrates superior performance in cross-table question answering and shows high efficiency in deployment across extensive experiments.

**Limitations:** 

**Conclusion:** The proposed GTR framework enhances the capabilities of LLMs in tabular reasoning and offers a practical solution for real-world applications.

**Abstract:** Beyond pure text, a substantial amount of knowledge is stored in tables. In real-world scenarios, user questions often require retrieving answers that are distributed across multiple tables. GraphRAG has recently attracted much attention for enhancing LLMs' reasoning capabilities by organizing external knowledge to address ad-hoc and complex questions, exemplifying a promising direction for cross-table question answering. In this paper, to address the current gap in available data, we first introduce a multi-table benchmark, MutliTableQA, comprising 60k tables and 25k user queries collected from real-world sources. Then, we propose the first Graph-Table-RAG framework, namely GTR, which reorganizes table corpora into a heterogeneous graph, employs a hierarchical coarse-to-fine retrieval process to extract the most relevant tables, and integrates graph-aware prompting for downstream LLMs' tabular reasoning. Extensive experiments show that GTR exhibits superior cross-table question-answering performance while maintaining high deployment efficiency, demonstrating its real-world practical applicability.

</details>


### [399] [Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models](https://arxiv.org/abs/2504.05050)

*Jiawei Lian, Jianhong Pan, Lefan Wang, Yi Wang, Shaohui Mei, Lap-Pui Chau*

**Main category:** cs.CL

**Keywords:** large language models, alignment, adversarial prompt, ethical vulnerabilities, dark patterns

**Relevance Score:** 8

**TL;DR:** This paper demonstrates that harmful knowledge in LLMs persists as 'dark patterns' despite alignment efforts, exposing vulnerabilities when faced with adversarial prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to uncover the limitations of current alignment methods in LLMs and their susceptibility to harmful knowledge even after instruction tuning.

**Method:** The authors analyze the ethical vulnerabilities in LLMs theoretically and empirically, using adversarial prompts to test alignment limits.

**Key Contributions:**

	1. Theoretical analysis of ethical vulnerabilities in aligned LLMs.
	2. Demonstration of the persistence of harmful knowledge as dark patterns.
	3. Empirical validation of attack success rates across multiple aligned LLMs.

**Result:** The research shows that current alignment methods only create local safety zones, while harmful knowledge can be accessed globally through adversarial methods with a 100% attack success rate across various LLMs.

**Limitations:** Focuses on adversarial prompts and may not address all methods of alignment failure.

**Conclusion:** The findings highlight the need for improved safeguarding in LLM alignment strategies to address inherent vulnerabilities to harmful knowledge.

**Abstract:** Large language models (LLMs) are foundational explorations to artificial general intelligence, yet their alignment with human values via instruction tuning and preference learning achieves only superficial compliance. Here, we demonstrate that harmful knowledge embedded during pretraining persists as indelible "dark patterns" in LLMs' parametric memory, evading alignment safeguards and resurfacing under adversarial inducement at distributional shifts. In this study, we first theoretically analyze the intrinsic ethical vulnerability of aligned LLMs by proving that current alignment methods yield only local "safety regions" in the knowledge manifold. In contrast, pretrained knowledge remains globally connected to harmful concepts via high-likelihood adversarial trajectories. Building on this theoretical insight, we empirically validate our findings by employing semantic coherence inducement under distributional shifts--a method that systematically bypasses alignment constraints through optimized adversarial prompts. This combined theoretical and empirical approach achieves a 100% attack success rate across 19 out of 23 state-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing their universal vulnerabilities.

</details>


### [400] [NoveltyBench: Evaluating Language Models for Humanlike Diversity](https://arxiv.org/abs/2504.05228)

*Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, Daphne Ippolito*

**Main category:** cs.CL

**Keywords:** language models, diversity, benchmark, evaluating outputs, mode collapse

**Relevance Score:** 7

**TL;DR:** Introducing NoveltyBench, a benchmark to evaluate the diversity of language model outputs, revealing significant shortcomings compared to human writers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of mode collapse in language models, where they fail to generate diverse outputs despite high performance on benchmarks.

**Method:** Development of NoveltyBench, which utilizes curated prompts and real-world user queries to assess the diversity of 20 leading language models' outputs.

**Key Contributions:**

	1. Introduction of NoveltyBench for evaluating output diversity
	2. Findings that challenge existing notions of model capability
	3. Insights on the limitations of larger language models in generating diverse outputs.

**Result:** Current state-of-the-art models generate significantly less diversity than human writers; larger models often show less diversity than smaller ones.

**Limitations:** Focus on evaluating only language model outputs, which may not encompass all facets of generative utility.

**Conclusion:** There is a critical need for new training and evaluation paradigms that emphasize output diversity alongside quality in language models.

**Abstract:** Language models have demonstrated remarkable capabilities on standard benchmarks, yet they struggle increasingly from mode collapse, the inability to generate diverse and novel outputs. Our work introduces NoveltyBench, a benchmark specifically designed to evaluate the ability of language models to produce multiple distinct and high-quality outputs. NoveltyBench utilizes prompts curated to elicit diverse answers and filtered real-world user queries. Evaluating 20 leading language models, we find that current state-of-the-art systems generate significantly less diversity than human writers. Notably, larger models within a family often exhibit less diversity than their smaller counterparts, challenging the notion that capability on standard benchmarks translates directly to generative utility. While prompting strategies like in-context regeneration can elicit diversity, our findings highlight a fundamental lack of distributional diversity in current models, reducing their utility for users seeking varied responses and suggesting the need for new training and evaluation paradigms that prioritize diversity alongside quality.

</details>


### [401] [Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric](https://arxiv.org/abs/2504.07440)

*Yixin Cao, Jiahao Ying, Yaoning Wang, Xipeng Qiu, Xuanjing Huang, Yugang Jiang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Model Utilization Index, Evaluation Metrics

**Relevance Score:** 8

**TL;DR:** The paper proposes a Model Utilization Index (MUI) to evaluate Large Language Models (LLMs) more effectively by quantifying the effort a model expends during inference, revealing a relationship between effort and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation methods for LLMs are inadequate, particularly concerning the generalization issue of inferring model capabilities from bounded benchmarks.

**Method:** The proposal of the MUI, which quantifies the proportion of activated neurons during inference to measure the effort expended by a model on a task.

**Key Contributions:**

	1. Introduction of the Model Utilization Index (MUI) as a new evaluation metric for LLMs
	2. Establishment of the Utility Law showing the relationship between model effort and performance
	3. Presentation of practical corollaries for training diagnostics and model comparisons

**Result:** A consistent inverse logarithmic relationship between MUI and performance was found across popular LLMs, leading to the formulation of the Utility Law.

**Limitations:** 

**Conclusion:** The Utility Law provides practical insights for training diagnostics, identifying data contamination, enabling fairer model comparisons, and guiding dataset diversity for specific models.

**Abstract:** Large Language Models (LLMs) have become indispensable across academia, industry, and daily applications, yet current evaluation methods struggle to keep pace with their rapid development. One core challenge of evaluation in the large language model (LLM) era is the generalization issue: how to infer a model's near-unbounded abilities from inevitably bounded benchmarks. We address this challenge by proposing Model Utilization Index (MUI), a mechanism interpretability enhanced metric that complements traditional performance scores. MUI quantifies the effort a model expends on a task, defined as the proportion of activated neurons or features during inference. Intuitively, a truly capable model should achieve higher performance with lower effort. Extensive experiments across popular LLMs reveal a consistent inverse logarithmic relationship between MUI and performance, which we formulate as the Utility Law. From this law we derive four practical corollaries that (i) guide training diagnostics, (ii) expose data contamination issue, (iii) enable fairer model comparisons, and (iv) design model-specific dataset diversity. Our code can be found at https://github.com/ALEX-nlp/MUI-Eva.

</details>


### [402] [MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations](https://arxiv.org/abs/2504.07830)

*Genglin Liu, Vivian Le, Salman Rahman, Elisa Kreiss, Marzyeh Ghassemi, Saadia Gabriel*

**Main category:** cs.CL

**Keywords:** social network simulation, misinformation, content moderation, generative language agents, user behavior

**Relevance Score:** 8

**TL;DR:** We introduce MOSAIC, an open-source framework for simulating user behavior on social networks using LLM agents, focusing on misinformation dynamics and content moderation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand user behaviors in online social networks and the spread of misinformation.

**Method:** The framework combines generative language agents with a directed social graph and constructs user personas for multi-agent simulations.

**Key Contributions:**

	1. Introduction of the MOSAIC framework for social network simulation.
	2. Evaluation of content moderation strategies against misinformation.
	3. Analysis of user engagement dynamics in simulated environments.

**Result:** The evaluation of content moderation strategies shows they can reduce misinformation spread while enhancing user engagement.

**Limitations:** The work is still in progress and requires further validation of findings.

**Conclusion:** The software has been open-sourced to promote further research into AI and social sciences.

**Abstract:** We present a novel, open-source social network simulation framework, MOSAIC, where generative language agents predict user behaviors such as liking, sharing, and flagging content. This simulation combines LLM agents with a directed social graph to analyze emergent deception behaviors and gain a better understanding of how users determine the veracity of online social content. By constructing user representations from diverse fine-grained personas, our system enables multi-agent simulations that model content dissemination and engagement dynamics at scale. Within this framework, we evaluate three different content moderation strategies with simulated misinformation dissemination, and we find that they not only mitigate the spread of non-factual content but also increase user engagement. In addition, we analyze the trajectories of popular content in our simulations, and explore whether simulation agents' articulated reasoning for their social interactions truly aligns with their collective engagement patterns. We open-source our simulation software to encourage further research within AI and social sciences.

</details>


### [403] [PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims](https://arxiv.org/abs/2504.09866)

*Ziyu Zhuang*

**Main category:** cs.CL

**Keywords:** automated fact-checking, adaptive search, multilingual retrieval

**Relevance Score:** 6

**TL;DR:** PASS-FC is a new automated fact-checking system that improves performance on time-sensitive and entity-ambiguous claims through adaptive search techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Automated fact-checking struggles with time-sensitive and ambiguous claims buried in noisy search results.

**Method:** The system uses a structured adaptive search loop that grounds claims with precise timelines and disambiguated entities, filtering sources for credibility and expanding searches cross-lingually.

**Key Contributions:**

	1. Introduction of an adaptive search scheme for fact-checking
	2. Demonstration of improved accuracy on multilingual benchmarks
	3. Highlighting the importance of temporal grounding in claims

**Result:** PASS-FC outperforms existing systems in various benchmarks, showing the importance of temporal grounding and adaptive methods in fact-checking.

**Limitations:** Performance degrades for low-resource languages; relies on the assumption of typological closeness to English for better outcomes.

**Conclusion:** The framework demonstrates significant improvements in automated fact-checking and offers insights into the need for cross-lingual retrieval in various contexts.

**Abstract:** Automated fact-checking (AFC) still falters on claims that are time-sensitive, entity-ambiguous, or buried beneath noisy search-engine results. We present PASS-FC, a Progressive and Adaptive Search Scheme for Fact Checking. Each atomic claim is first grounded with a precise time span and disambiguated entity descriptors. An adaptive search loop then issues structured queries, filters domains through credible-source selection, and expands queries cross-lingually; when necessary, a lightweight reflection routine restarts the loop. Experiments on six benchmark--covering general knowledge, scientific literature, real-world events, and ten languages--show that PASS-FC consistently outperforms prior systems, even those powered by larger backbone LLMs. On the multilingual X-FACT set, performance of different languages partially correlates with typological closeness to English, and forcing the model to reason in low-resource languages degrades accuracy. Ablations highlight the importance of temporal grounding and the adaptive search scheme, while detailed analysis shows that cross-lingual retrieval contributes genuinely new evidence. Code and full results will be released to facilitate further research.

</details>


### [404] [TextArena](https://arxiv.org/abs/2504.11442)

*Leon Guertler, Bobby Cheng, Simon Yu, Bo Liu, Leshem Choshen, Cheston Tan*

**Main category:** cs.CL

**Keywords:** Large Language Models, agentic behavior, text-based games, evaluation framework, social skills

**Relevance Score:** 8

**TL;DR:** TextArena is an open-source collection of 57+ competitive text-based games designed for evaluating agentic behavior in Large Language Models (LLMs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of evaluation of dynamic social skills in traditional benchmarks for LLMs, including negotiation and deception.

**Method:** TextArena provides a framework for competitive gameplay that includes single-player, two-player, and multi-player setups, with the ability to evaluate model capabilities through an online play system and TrueSkill scoring.

**Key Contributions:**

	1. Open-source framework for competitive text-based games
	2. Real-time evaluation system using TrueSkill scores
	3. Focus on dynamic social skills in LLM evaluation

**Result:** The platform allows users to test and train models, add new games, and evaluate behaviors in a wide range of competitive scenarios.

**Limitations:** 

**Conclusion:** TextArena emphasizes community involvement and extensibility, making it easier for researchers to assess LLM capabilities in social contexts.

**Abstract:** TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.

</details>


### [405] [Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models](https://arxiv.org/abs/2504.14366)

*Patrick Haller, Jonas Golde, Alan Akbik*

**Main category:** cs.CL

**Keywords:** knowledge distillation, subquadratic architecture, language models

**Relevance Score:** 7

**TL;DR:** This study evaluates knowledge distillation from Transformer models to subquadratic architectures to enhance efficiency while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the bottleneck of quadratic complexity in self-attention during inference in large language models (LLMs).

**Method:** The authors systematically evaluate the transferability of knowledge distillation from a Transformer teacher model to eight different subquadratic student architectures, analyzing the effects of various architectural designs and initialization strategies.

**Key Contributions:**

	1. Exploration of knowledge distillation from Transformer to subquadratic architectures.
	2. Evaluation of different architectural design choices and initialization strategies.
	3. Insights into efficiency and performance trade-offs for knowledge transfer.

**Result:** Empirical results demonstrate the trade-offs between efficiency and performance when transferring knowledge to subquadratic models, offering insights on effective parameter adaptation.

**Limitations:** The study is limited to specific subquadratic models and NLP benchmarks; generalizability to other domains is uncertain.

**Conclusion:** The findings suggest that specific architectural design choices and initialization strategies play a crucial role in the successful knowledge transfer process.

**Abstract:** Knowledge distillation is a widely used technique for compressing large language models (LLMs), in which a smaller student model is trained to mimic a larger teacher model. Typically, both the teacher and student models are Transformer-based architectures, leveraging softmax attention for sequence modeling. However, the quadratic complexity of self-attention during inference remains a significant bottleneck, motivating the exploration of subquadratic alternatives such as structured state-space models (SSMs), linear attention, and recurrent architectures. In this work, we systematically evaluate the transferability of knowledge distillation from a Transformer teacher model to eight subquadratic student architectures. Our study investigates which subquadratic model can most effectively approximate the teacher model's learned representations through knowledge distillation, and how different architectural design choices influence the training dynamics. We further investigate the impact of initialization strategies, such as matrix mixing and query-key-value (QKV) copying, on the adaptation process. Our empirical results on multiple NLP benchmarks provide insights into the trade-offs between efficiency and performance, highlighting key factors for successful knowledge transfer to subquadratic architectures.

</details>


### [406] [What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs](https://arxiv.org/abs/2505.10113)

*Xinlan Yan, Di Wu, Yibin Lei, Christof Monz, Iacer Calixto*

**Main category:** cs.CL

**Keywords:** medical question answering, large language models, fine-tuning, S-MedQA, domain shifting

**Relevance Score:** 9

**TL;DR:** S-MedQA is a novel English medical QA dataset designed to benchmark large language models in clinical specialties, challenging existing knowledge injection hypotheses in medical QA.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a reliable dataset to evaluate large language models in various clinical specialties and to investigate knowledge injection in medical question-answering.

**Method:** Introducing S-MedQA, an English medical QA dataset, and conducting experiments to assess the performance of large language models across different clinical specialties.

**Key Contributions:**

	1. Introduction of S-MedQA dataset for medical QA benchmarking
	2. Challenging the effectiveness of specialty-specific fine-tuning
	3. Providing code for reproducibility of experiments

**Result:** The findings indicate that training on specialty-specific data does not always yield optimal performance, and improvements are primarily attributed to domain shifting rather than knowledge injection.

**Limitations:** The study is limited to English medical data and may not translate universally across languages or non-English-speaking contexts.

**Conclusion:** The role of fine-tuning data in medical applications should be re-evaluated, and S-MedQA is released for community use.

**Abstract:** In this paper, we introduce S-MedQA, an English medical question-answering (QA) dataset for benchmarking large language models in fine-grained clinical specialties. We use S-MedQA to check the applicability of a popular hypothesis related to knowledge injection in the knowledge-intense scenario of medical QA, and show that: 1) training on data from a speciality does not necessarily lead to best performance on that specialty and 2) regardless of the specialty fine-tuned on, token probabilities of clinically relevant terms for all specialties increase consistently. Thus, we believe improvement gains come mostly from domain shifting (e.g., general to medical) rather than knowledge injection and suggest rethinking the role of fine-tuning data in the medical domain. We release S-MedQA and all code needed to reproduce all our experiments to the research community.

</details>
