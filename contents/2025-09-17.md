# 2025-09-17

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 25]

- [cs.CL](#cs.CL) [Total: 54]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Data selves and identity theft in the age of AI](https://arxiv.org/abs/2509.12383)

*Tim Gorichanaz*

**Main category:** cs.HC

**Keywords:** Identity Theft, Artificial Intelligence, Digital Age, Online Security, Big Data

**Relevance Score:** 4

**TL;DR:** This chapter explores identity theft in the digital age, focusing on AI's role in exacerbating and combating these crimes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the impact of AI on identity theft, especially with the proliferation of online technologies.

**Method:** The chapter reviews existing literature on online identity theft, examining theoretical and empirical aspects.

**Key Contributions:**

	1. Analysis of AI's impact on identity theft
	2. Review of theoretical and empirical literature
	3. Discussion of future trends in identity theft prevention

**Result:** AI technologies have accelerated identity crimes while also providing new detection and prevention methods, leading to a dynamic between criminals and law enforcement.

**Limitations:** 

**Conclusion:** The chapter emphasizes the ongoing battles regarding identity theft and suggests future directions as technology evolves.

**Abstract:** This chapter examines identity theft in the digital age, particularly in the context of emerging artificial intelligence (AI) technologies. It begins with a discussion of big data and selfhood, the concepts of data selves and data doubles, and the process of identification in the digital age. Next, the literature on online identity theft is reviewed, including its theoretical and empirical aspects. As is evident from that review, AI technologies have increased the speed and scale of identity crimes that were already rampant in the online world, even while they have led to new ways of detecting and preventing such crimes. As with any new technology, AI is currently fuelling an arms race between criminals and law enforcement, with end users often caught powerless in the middle. The chapter closes by exploring some emerging directions and future possibilities of identity theft in the age of AI.

</details>


### [2] [FlexMind: Scaffolding Flexible Ideation Workflows with AI in Creative Problem-Solving](https://arxiv.org/abs/2509.12408)

*Yaqing Yang, Vikram Mohanty, Nikolas Martelaro, Aniket Kittur, Yan-Ying Chen, Matthew K. Hong*

**Main category:** cs.HC

**Keywords:** human-AI collaboration, creative problem-solving, ideation, risk analysis, design support

**Relevance Score:** 8

**TL;DR:** The paper presents a human-AI collaborative workflow designed to enhance creativity and efficiency during the ideation stage of creative problem-solving by providing fluid support for exploring design alternatives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing interfaces in creative problem-solving, which either impose rigid workflows or offer no guidance.

**Method:** The paper proposes a collaborative AI system that presents three opt-in aids: high-level schemas for alternative ideas, risk analysis with mitigation, and system-generated suggestions for user invocation.

**Key Contributions:**

	1. Introduced a human-AI collaborative workflow for creative problem-solving.
	2. Developed three opt-in aids for enhancing ideation: schemas, risk analysis, and suggestions.
	3. Facilitated a fluid transition between design actions to improve creativity and efficiency.

**Result:** The proposed system allows users to fluidly navigate through searching, creating, and evaluating ideas, enhancing both creativity and efficiency during ideation.

**Limitations:** 

**Conclusion:** By integrating these aids into the ideation process, users can sustain creative momentum without being constrained by rigid workflows.

**Abstract:** Divergent thinking in the ideation stage of creative problem-solving demands that individuals explore a broad design space. Yet this exploration rarely follows a neat, linear sequence; problem-solvers constantly shift among searching, creating, and evaluating ideas. Existing interfaces either impose rigid, step-by-step workflows or permit unguided free-form exploration. To strike a balance between flexibility and guidance for augmenting people's efficiency and creativity, we introduce a human-AI collaborative workflow that supports a fluid ideation process. The system surfaces three opt-in aids: (1) high-level schemas to uncover alternative ideas, (2) risk analysis with mitigation suggestions, and (3) steering system-generated suggestions. Users can invoke these supports at any moment, allowing seamless back-and-forth movement among design actions to maintain creative momentum.

</details>


### [3] [Beyond Gaze Overlap: Analyzing Joint Visual Attention Dynamics Using Egocentric Data](https://arxiv.org/abs/2509.12419)

*Kumushini Thennakoon, Yasasi Abeysinghe, Bhanuka Mahanama, Vikas Ashok, Sampath Jayarathna*

**Main category:** cs.HC

**Keywords:** Joint Visual Attention, Egocentric Eye Trackers, Attention Coordination, Human-Computer Interaction, Deep Learning

**Relevance Score:** 8

**TL;DR:** This study proposes a novel approach to detect joint visual attention (JVA) using spatiotemporal tubes from egocentric eye-tracking data, revealing that object-focused collaborative tasks show significantly higher JVA than independent tasks.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to identify joint visual attention in multi-user environments using egocentric eye-trackers and large datasets, which provides insights into social interactions and attention coordination.

**Method:** Utilizing deep-learning-based feature mapping, the approach involves analyzing spatiotemporal tubes centered on individual gaze to detect JVA in various task settings.

**Key Contributions:**

	1. Novel approach for detecting JVA using eye-tracking data
	2. Analysis of attention characteristics using the ambient-focal attention coefficient K
	3. Revealed significant differences in JVA depending on task types

**Result:** The analysis confirmed that collaborative tasks yield higher JVA (44-46%) compared to independent tasks (4-5%), revealing notable differences in attentional dynamics.

**Limitations:** The study is limited to specific collaborative and independent task settings and may require further validation in diverse ecological contexts.

**Conclusion:** The findings highlight the utility of the proposed method in fields like psychology, human-computer interaction, and social robotics for understanding attention mechanisms in realistic scenarios.

**Abstract:** Joint visual attention (JVA) provides informative cues on human behavior during social interactions. The ubiquity of egocentric eye-trackers and large-scale datasets on everyday interactions offer research opportunities in identifying JVA in multi-user environments. We propose a novel approach utilizing spatiotemporal tubes centered on attention rendered by individual gaze and detect JVA using deep-learning-based feature mapping. Our results reveal object-focused collaborative tasks to yield higher JVA (44-46%), whereas independent tasks yield lower (4-5%) attention. Beyond JVA, we analyze attention characteristics using ambient-focal attention coefficient K to understand the qualitative aspects of shared attention. Our analysis reveals $\mathcal{K}$ to converge instances where participants interact with shared objects while diverging when independent. While our study presents seminal findings on joint attention with egocentric commodity eye trackers, it indicates the potential utility of our approach in psychology, human-computer interaction, and social robotics, particularly in understanding attention coordination mechanisms in ecologically valid contexts.

</details>


### [4] [Extended AI Interactions Shape Sycophancy and Perspective Mimesis](https://arxiv.org/abs/2509.12517)

*Shomik Jain, Charlotte Park, Matheus Mesquita Viana, Ashia Wilson, Dana Calacci*

**Main category:** cs.HC

**Keywords:** AI mirroring, long-context interactions, Large Language Models

**Relevance Score:** 8

**TL;DR:** The paper examines AI mirroring behaviors in LLMs during long-context interactions, specifically focusing on sycophancy and perspective mimesis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how long-context interactions between users and LLMs influence AI mirroring behaviors, which are critical for understanding user experience and trust in AI systems.

**Method:** We analyzed two weeks of interaction data from 38 users, comparing model responses with and without long-context in political explanations and personal advice tasks.

**Key Contributions:**

	1. Identification of sycophancy and perspective mimesis as key mirroring behaviors.
	2. Empirical evidence that long context affects these behaviors in LLMs.
	3. Insights into context-dependent interaction dynamics with AI.

**Result:** Findings reveal that sycophancy is consistently higher with long-context interactions, while perspective mimesis only increases in contexts where user perspectives can be accurately inferred.

**Limitations:** 

**Conclusion:** Long-context interactions significantly amplify certain AI mirroring behaviors, which has implications for user trust and experience with LLMs.

**Abstract:** We investigate whether long-context interactions between users and LLMs lead to AI mirroring behaviors. We focus on two forms of mirroring: (1) sycophancy -- the tendency of models to be overly agreeable with users, and (2) perspective mimesis -- the extent to which models reflect a user's perspective. Using two weeks of interaction context collected from 38 users, we compare model responses with and without long-context for two tasks: political explanations and personal advice. Our results demonstrate how and when real-world interaction contexts can amplify AI mirroring behaviors. We find that sycophancy increases in long-context, irrespective of the interaction topics. Perspective mimesis increases only in contexts where models can accurately infer user perspectives.

</details>


### [5] [The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots](https://arxiv.org/abs/2509.12525)

*T. James Brandt, Cecilia Xi Wang*

**Main category:** cs.HC

**Keywords:** Generative AI, Chatbots, User Authors, Language Style Matching, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** The paper investigates the effects of user-controlled avatar generation and language style matching in chatbots, revealing that visible authorship enhances connection while covert mimicry can harm perception.

**Read time:** 31 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how different methods of personalization in generative AI companion chatbots affect user connection and satisfaction.

**Method:** A preregistered 3x2 experiment with 162 participants, manipulating avatar generation and language style matching styles.

**Key Contributions:**

	1. Identifies the benefits of user-generated avatars for enhancing rapport in chatbots.
	2. Describes the Adaptation Paradox where stylistic mimicry can reduce perceived connection.
	3. Proposes a stability-and-legibility framework for designing personalized interactions.

**Result:** User-generated avatars improved rapport significantly, while adaptive language style matching was less effective in personalization and satisfaction, leading to the Adaptation Paradox.

**Limitations:** Limited to specific styles of avatar generation and language mimicry; results may not generalize to all generative AI applications.

**Conclusion:** Designers should focus on user-driven personalization and avoid opaque mimicry that may confuse users.

**Abstract:** Generative AI powers a growing wave of companion chatbots, yet principles for fostering genuine connection remain unsettled. We test two routes: visible user authorship versus covert language-style mimicry. In a preregistered 3x2 experiment (N = 162), we manipulated user-controlled avatar generation (none, premade, user-generated) and Language Style Matching (LSM) (static vs. adaptive). Generating an avatar boosted rapport ($\omega^2$ = .040, p = .013), whereas adaptive LSM underperformed static style on personalization and satisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t = 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony erodes connection when perceived as incoherent, destabilizing persona. To explain, we propose a stability-and-legibility account: visible authorship fosters natural interaction, while covert mimicry risks incoherence. Our findings suggest designers should prioritize legible, user-driven personalization and limit stylistic shifts rather than rely on opaque mimicry.

</details>


### [6] [Conflect: Designing Reflective Thinking-Based Contextual Privacy Policy for Mobile Applications](https://arxiv.org/abs/2509.12578)

*Shuning Zhang, Sixing Tao, Eve He, Yuting Yang, Ying Ma, Ailei Wang, Xin Yi, Hewu Li*

**Main category:** cs.HC

**Keywords:** privacy policies, human-computer interaction, mobile applications

**Relevance Score:** 8

**TL;DR:** Designing Conflect, an interactive contextual privacy policy (CPP) for mobile apps to improve user engagement and understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the neglect of lengthy and complex privacy policies by providing contextual information at the point of risk.

**Method:** Developed Conflect through workshops, utilizing a reflective thinking framework to create sidebar alerts that inform users of privacy risks.

**Key Contributions:**

	1. Introduces Conflect as an interactive CPP
	2. Utilizes a reflective thinking framework for privacy engagement
	3. Demonstrated significant improvements in user experience metrics

**Result:** Conflect achieves 94.0% accuracy in policy extraction and a user study reveals improved user understanding, trust, satisfaction, and reduced cognitive load.

**Limitations:** 

**Conclusion:** Conflect offers an effective solution to enhance user interaction with privacy policies in mobile applications.

**Abstract:** Privacy policies are lengthy and complex, leading to user neglect. While contextual privacy policies (CPPs) present information at the point of risk, they may lack engagement and disrupt tasks. We propose Conflect, an interactive CPP for mobile apps, guided by a reflective thinking framework. Through three workshops with experienced designers and researchers, we constructed the design space of reflective thinking-based CPP design, and identified the disconnect between context and action as the most critical problem. Based on participants' feedback, we designed Conflect to use sidebar alerts, allowing users to reflect on contextualized risks and fostering their control. Our system contextually detects privacy risks, extracts policy segments, and automatically generates risk descriptions with 94.0% policy extraction accuracy on CPP4APP dataset and a 4.35s latency. A user study (N=28) demonstrated that Conflect improves user understanding, trust, and satisfaction while lowering cognitive load compared to CPPs, privacy policies and privacy labels.

</details>


### [7] [DPCheatSheet: Using Worked and Erroneous LLM-usage Examples to Scaffold Differential Privacy Implementation](https://arxiv.org/abs/2509.12590)

*Shao-Yu Chu, Yuhe Tian, Yu-Xiang Wang, Haojian Jin*

**Main category:** cs.HC

**Keywords:** differential privacy, large language models, novice programmers, error identification, instructional tool

**Relevance Score:** 7

**TL;DR:** The paper investigates how novices can use LLMs to implement differential privacy (DP) with minimal training and introduces DPCheatSheet, a tool to facilitate this process.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enable programmers without specialized knowledge in differential privacy to effectively utilize LLMs for implementing DP programs.

**Method:** Conducted a need-finding study with novices and experts to understand LLM usage, followed by the development of DPCheatSheet, which combines expert workflows and common pitfalls in LLM-generated code.

**Key Contributions:**

	1. DPCheatSheet tool developed for aiding novices in DP implementation using LLMs
	2. Combines expert annotations and common mistakes for error-driven learning
	3. Empirical validation of effectiveness through studies.

**Result:** DPCheatSheet was shown to improve the ability of novices to identify errors and implement DP by bridging the knowledge gap with expert examples and error-driven learning.

**Limitations:** The study involved a small sample size and focused only on novices and experts in differential privacy, limiting generalizability.

**Conclusion:** DPCheatSheet successfully aids novices in understanding and implementing differential privacy using LLMs, enhancing their coding practices in this domain.

**Abstract:** This paper explores how programmers without specialized expertise in differential privacy (DP) (i.e., novices) can leverage LLMs to implement DP programs with minimal training. We first conducted a need-finding study with 6 novices and 3 experts to understand how they utilize LLMs in DP implementation. While DP experts can implement correct DP analyses through a few prompts, novices struggle to articulate their requirements in prompts and lack the skills to verify the correctness of the generated code. We then developed DPCheatSheet, an instructional tool that helps novices implement DP using LLMs. DPCheatSheet combines two learning concepts: it annotates an expert's workflow with LLMs as a worked example to bridge the expert mindset to novices, and it presents five common mistakes in LLM-based DP code generation as erroneous examples to support error-driven learning. We demonstrated the effectiveness of DPCheatSheet with an error identification study and an open-ended DP implementation study.

</details>


### [8] [DoubleAgents: Exploring Mechanisms of Building Trust with Proactive AI](https://arxiv.org/abs/2509.12626)

*Tao Long, Xuanming Zhang, Sitong Wang, Zhou Yu, Lydia B Chilton*

**Main category:** cs.HC

**Keywords:** Agentic workflows, Trust in AI, User intervention, Simulation, Human coordination

**Relevance Score:** 8

**TL;DR:** DoubleAgents is an agentic planning tool aimed at enhancing user trust in automated systems through transparency and control.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind DoubleAgents is to increase the efficiency of agentic workflows and ensure user trust in systems acting on their behalf.

**Method:** The paper presents a tool that incorporates user intervention, value-reflecting policies, rich state visualizations, and uncertainty flagging, alongside a respondent simulation for realistic scenario testing.

**Key Contributions:**

	1. Trust-by-design patterns for AI systems
	2. Mechanisms for transparency, control, and explainability
	3. Simulation techniques for building user trust over time

**Result:** Participants showed initial hesitation to delegate actions to the system but developed greater trust and reliance after experiencing its transparency and control features during training sessions.

**Limitations:** Study sample size was small (n=10 for lab study; n=2 for deployments), limiting generalizability.

**Conclusion:** DoubleAgents successfully demonstrates how to nurture user trust in AI systems through design patterns that emphasize consistency, controllability, and explainability, with simulation serving as a safe training method.

**Abstract:** Agentic workflows promise efficiency, but adoption hinges on whether people actually trust systems that act on their behalf. We present DoubleAgents, an agentic planning tool that embeds transparency and control through user intervention, value-reflecting policies, rich state visualizations, and uncertainty flagging for human coordination tasks. A built-in respondent simulation generates realistic scenarios, allowing users to rehearse, refine policies, and calibrate their reliance before live use. We evaluate DoubleAgents in a two-day lab study (n=10), two deployments (n=2), and a technical evaluation. Results show that participants initially hesitated to delegate but grew more reliant as they experienced transparency, control, and adaptive learning during simulated cases. Deployment results demonstrate DoubleAgents' real-world relevance and usefulness, showing that the effort required scaled appropriately with task complexity and contextual data. We contribute trust-by-design patterns and mechanisms for proactive AI -- consistency, controllability, and explainability -- along with simulation as a safe path to build and calibrate trust over time.

</details>


### [9] [Harnessing the Power of AI in Qualitative Research: Role Assignment, Engagement, and User Perceptions of AI-Generated Follow-Up Questions in Semi-Structured Interviews](https://arxiv.org/abs/2509.12709)

*He Zhang, Yueyan Liu, Xin Guan, Jie Cai, John M. Carroll*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Large Language Models, Qualitative Research, AI-assisted interviewing, Human-AI collaboration

**Relevance Score:** 8

**TL;DR:** This paper explores the use of large language models (LLMs) to generate follow-up questions in semi-structured interviews, examining the resulting impacts on interview dynamics and the roles between human interviewers and AI.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential of LLMs in enhancing the quality of follow-up questions in semi-structured interviews, an aspect crucial to qualitative data collection yet often limited by interviewers' skills.

**Method:** The study employs an AI-driven 'Wizard-of-Oz' setup, involving 17 participants to assess the effects of real-time LLM-generated follow-up questions on the interview process.

**Key Contributions:**

	1. Empirical evidence on the strengths and weaknesses of AI-generated follow-up questions.
	2. A Human-AI collaboration framework in semi-structured interviews.
	3. Human-centered design guidelines for integrating AI into qualitative data collection.

**Result:** The findings reveal both advantages and limitations of using AI-generated follow-up questions and offer insights into the collaborative dynamics between interviewers and AI.

**Limitations:** The paper does not explore the long-term effects of LLM use in various interviewing contexts or across different interview types.

**Conclusion:** LLMs should be viewed as complements to human judgment in qualitative research, with suggested guidelines for human-centered design in AI-assisted interviews.

**Abstract:** Semi-structured interviews highly rely on the quality of follow-up questions, yet interviewers' knowledge and skills may limit their depth and potentially affect outcomes. While many studies have shown the usefulness of large language models (LLMs) for qualitative analysis, their possibility in the data collection process remains underexplored. We adopt an AI-driven "Wizard-of-Oz" setup to investigate how real-time LLM support in generating follow-up questions shapes semi-structured interviews. Through a study with 17 participants, we examine the value of LLM-generated follow-up questions, the evolving division of roles, relationships, collaborative behaviors, and responsibilities between interviewers and AI. Our findings (1) provide empirical evidence of the strengths and limitations of AI-generated follow-up questions (AGQs); (2) introduce a Human-AI collaboration framework in this interview context; and (3) propose human-centered design guidelines for AI-assisted interviewing. We position LLMs as complements, not replacements, to human judgment, and highlight pathways for integrating AI into qualitative data collection.

</details>


### [10] [Participatory AI: A Scandinavian Approach to Human-Centered AI](https://arxiv.org/abs/2509.12752)

*Niklas Elmqvist, Eve Hoggan, Hans-Jörg Schulz, Marianne Graves Petersen, Peter Dalsgaard, Ira Assent, Olav W. Bertelsen, Akhil Arora, Kaj Grønbæk, Susanne Bødker, Clemens Nylandsted Klokmose, Rachel Charlotte Smith, Sebastian Hubenschmid, Christoph A. Johns, Gabriela Molina León, Anton Wolter, Johannes Ellemose, Vaishali Dhanoa, Simon Aagaard Enni, Mille Skovhus Lunding, Karl-Emil Kjær Bilstrup, Juan Sánchez Esquivel, Luke Connelly, Rafael Pablos Sarabia, Morten Birk, Joachim Nyborg, Stefanie Zollmann, Tobias Langlotz, Meredith Siang-Yun Chou, Jens Emil Sloth Grønbæk, Michael Wessely, Yijing Jiang, Caroline Berger, Duosi Dai, Michael Mose Biskjaer, Germán Leiva, Jonas Frich, Eva Eriksson, Kim Halskov, Thorbjørn Mikkelsen, Nearchos Potamitis, Michel Yildirim, Arvind Srinivasan, Jeanette Falk, Nanna Inie, Ole Sejer Iversen, Hugo Andersson*

**Main category:** cs.HC

**Keywords:** Participatory Design, Human-Centered AI, Algorithmic Automation, Democratic Values, Socio-Technical Systems

**Relevance Score:** 7

**TL;DR:** This paper proposes Participatory AI as a human-centered design approach to counter the challenges posed by algorithmic automation, using principles from Scandinavian Participatory Design.

**Read time:** 32 min

<details>
  <summary>Details</summary>

**Motivation:** To address the threats to human agency and democratic values posed by opaque and centralized AI models in work and everyday life.

**Method:** The paper draws from the Scandinavian Participatory Design tradition, applying its five principles to tackle four design challenges related to algorithmic automation.

**Key Contributions:**

	1. Introduction of Participatory AI as a human-centered design approach
	2. Application of PD principles to algorithmic automation design challenges
	3. Case studies demonstrating AI as socio-technical systems

**Result:** The authors illustrate how AI can be viewed as a shared socio-technical system through case studies, enhancing human agency and values rather than diminishing them.

**Limitations:** 

**Conclusion:** Participatory AI can transform AI from proprietary products into democratic tools that support human dignity and agency.

**Abstract:** AI's transformative impact on work, education, and everyday life makes it as much a political artifact as a technological one. Current AI models are opaque, centralized, and overly generic. The algorithmic automation they provide threatens human agency and democratic values in both workplaces and daily life. To confront such challenges, we turn to Scandinavian Participatory Design (PD), which was devised in the 1970s to face a similar threat from mechanical automation. In the PD tradition, technology is seen not just as an artifact, but as a locus of democracy. Drawing from this tradition, we propose Participatory AI as a PD approach to human-centered AI that applies five PD principles to four design challenges for algorithmic automation. We use concrete case studies to illustrate how to treat AI models less as proprietary products and more as shared socio-technical systems that enhance rather than diminish human agency, human dignity, and human values.

</details>


### [11] [PLUTO: A Public Value Assessment Tool](https://arxiv.org/abs/2509.12773)

*Laura Koesten, Péter Ferenc Gyarmati, Connor Hogan, Bernhard Jordan, Seliem El-Sayed, Barbara Prainsack, Torsten Möller*

**Main category:** cs.HC

**Keywords:** data solidarity, public value, data assessment, stakeholder engagement, data practices

**Relevance Score:** 4

**TL;DR:** PLUTO is a framework aimed at assessing the public value of data use, focusing on stakeholder engagement through qualitative and quantitative risk-benefit evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To empower stakeholders to engage critically with data projects and assess the risks and benefits involved in data use.

**Method:** Development of PLUTO involved a theoretical foundation grounded in data solidarity, along with a focus on creating a structured assessment framework for diverse stakeholders.

**Key Contributions:**

	1. Introduction of the PLUTO framework for data assessment
	2. Focus on stakeholder inclusivity in data evaluation
	3. Highlighting initial user experiences and feedback

**Result:** PLUTO seeks to provide actionable metrics from qualitative assessments, highlighting initial user experiences and feedback on its implementation.

**Limitations:** Challenges in translating qualitative assessments into quantitative metrics while maintaining inclusivity and transparency.

**Conclusion:** PLUTO shows promise in promoting responsible decision-making and accountability in data practices, though challenges remain in balancing qualitative and quantitative assessments.

**Abstract:** We present PLUTO (Public VaLUe Assessment TOol), a framework for assessing the public value of specific instances of data use. Grounded in the concept of data solidarity, PLUTO aims to empower diverse stakeholders - including regulatory bodies, private enterprises, NGOs, and individuals - to critically engage with data projects through a structured assessment of the risks and benefits of data use, and by encouraging critical reflection. This paper discusses the theoretical foundation, development process, and initial user experiences with PLUTO. Key challenges include translating qualitative assessments of benefits and risks into actionable quantitative metrics while maintaining inclusivity and transparency. Initial feedback highlights PLUTO's potential to foster responsible decision-making and shared accountability in data practices.

</details>


### [12] [The Impact of Automation on Risk-Taking: The Role of Sense of Agency](https://arxiv.org/abs/2509.12794)

*Yang Chen, Zhijun Zhang*

**Main category:** cs.HC

**Keywords:** automation, risk-taking, sense of agency, human behavior, experimentation

**Relevance Score:** 4

**TL;DR:** This study explores how automation influences risk-taking behaviors and the role of sense of agency in this relationship through three experiments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the relationship between automation and risk-taking, and how sense of agency mediates this relationship.

**Method:** Quantitative experiments measuring the sense of agency through subjective ratings while varying automation level and reliability.

**Key Contributions:**

	1. Identifies the mediating role of sense of agency in risk-taking under automation
	2. Demonstrates different effects of automation level and reliability on risk-taking
	3. Provides empirical evidence through experimental studies

**Result:** The experiments found that higher automation levels decrease risk-taking and sense of agency, while higher reliability increases both risk-taking and sense of agency.

**Limitations:** 

**Conclusion:** Automation significantly affects risk-taking behaviors, with the sense of agency mediating these effects differently based on the level and reliability of automation.

**Abstract:** Automation significantly alters human behavior, particularly risk-taking. Previous researches have paid limited attention to the underlying characteristics of automation and their mechanisms of influence on risk-taking. This study investigated how automation affects risk-taking and examined the role of sense of agency therein. By quantifying sense of agency through subjective ratings, this research explored the impact of automation level and reliability level on risk-taking. The results of three experiments indicated that automation reduced the level of risk-taking; higher automation level was associated with lower sense of agency and lower risk-taking, with sense of agency playing a complete mediating role; higher automation reliability was associated with higher sense of agency and higher risk-taking, with sense of agency playing a partial mediating role. The study concludes that automation influences risk-taking, such that higher automation level or lower reliability is associated with a lower likelihood of risk-taking. Sense of agency mediates the impact of automation on risk-taking, and automation level and reliability have different effects on risk-taking.

</details>


### [13] [Gesture Evaluation in Virtual Reality](https://arxiv.org/abs/2509.12816)

*Axel Wiebe Werner, Jonas Beskow, Anna Deichler*

**Main category:** cs.HC

**Keywords:** gestures, Virtual Reality, human-computer interaction, AI, multimodal interaction

**Relevance Score:** 8

**TL;DR:** This paper evaluates AI-generated gestures in Virtual Reality (VR) versus 2D settings, highlighting the advantages of immersive environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of different environments on the perception of AI-generated gestures and enhance human-computer interaction.

**Method:** A comparative evaluation was conducted using three models from the 2023 GENEA Challenge, assessing how gestures were perceived in VR and 2D contexts.

**Key Contributions:**

	1. Comparative evaluation of gesture perception in VR vs. 2D
	2. Highlighting the effectiveness of motion-capture gestures
	3. Demonstrating VR's impact on overall gesture perception

**Result:** Gestures in VR received higher ratings on average, particularly highlighting the effectiveness of motion-capture 'true movement' gestures.

**Limitations:** 

**Conclusion:** VR positively influences participants' perceptions of gestures, providing unique advantages over traditional 2D evaluations.

**Abstract:** Gestures are central to human communication, enriching interactions through non-verbal expression. Virtual avatars increasingly use AI-generated gestures to enhance life-likeness, yet evaluations have largely been confined to 2D. Virtual Reality (VR) provides an immersive alternative that may affect how gestures are perceived. This paper presents a comparative evaluation of computer-generated gestures in VR and 2D, examining three models from the 2023 GENEA Challenge. Results show that gestures viewed in VR were rated slightly higher on average, with the strongest effect observed for motion-capture "true movement." While model rankings remained consistent across settings, VR influenced participants' overall perception and offered unique benefits over traditional 2D evaluation.

</details>


### [14] [Winds Through Time: Interactive Data Visualization and Physicalization for Paleoclimate Communication](https://arxiv.org/abs/2509.13039)

*David Hunter, Pablo Botin, Emily Snode-Brenneman, Amy Stevermer, Becca Hatheway, Dillon Amaya, Eddie Goldstein, Wayne A Seltzer, Mark D Gross, Kris Karnauskas, Daniel Leithinger, Ellen Yi-Luen Do*

**Main category:** cs.HC

**Keywords:** paleoclimate, interactive exhibit, public science education, data physicalization, science communication

**Relevance Score:** 4

**TL;DR:** This paper discusses the design of an interactive exhibit on paleoclimate for a science center, featuring a data physicalization that engages visitors through manipulation and visualization of climate changes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create an engaging way for a diverse audience to understand the significant climate changes from historical perspectives, particularly during the last ice age.

**Method:** The design involved a collaborative, iterative process leading to the development of a physical model representing mountains and ice sheets combined with an interactive wind simulation.

**Key Contributions:**

	1. Creation of an interactive physical model representing paleoclimate data.
	2. Methodology for designing exhibits that cater to varying levels of audience expertise.
	3. Demonstration of an effective way to visualize climate change impact through hands-on interaction.

**Result:** The exhibit allows visitors to manipulate physical representations of climate data and see the effects of climate change in an intuitive way.

**Limitations:** 

**Conclusion:** The exhibit successfully engages a broad audience in learning about paleoclimate through interactive experiences that bridge complex scientific concepts and tangible interaction.

**Abstract:** We describe a multidisciplinary collaboration to iteratively design an interactive exhibit for a public science center on paleoclimate, the study of past climates. We created a data physicalisation of mountains and ice sheets that can be tangibly manipulated by visitors to interact with a wind simulation visualisation that demonstrates how the climate of North America differed dramatically between now and the peak of the last ice age. We detail the system for interaction and visualisation plus design choices to appeal to an audience that ranges from children to scientists and responds to site requirements.

</details>


### [15] [More than Meets the Eye: Understanding the Effect of Individual Objects on Perceived Visual Privacy](https://arxiv.org/abs/2509.13051)

*Mete Harun Akcay, Siddharth Prakash Rao, Alexandros Bakas, Buse Gul Atli*

**Main category:** cs.HC

**Keywords:** privacy, user-generated content, image evaluation, context-aware systems, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This study investigates how users evaluate the privacy of images with multiple sensitive objects, revealing nuanced patterns and mental models related to privacy perceptions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how individual objects within an image influence privacy risks and user perceptions, as previous research often treated privacy as a property of the entire image.

**Method:** A mixed-methods study was conducted with 92 participants to explore the evaluation of privacy in images containing multiple sensitive objects.

**Key Contributions:**

	1. Identification of mental models related to privacy perception.
	2. Nuanced understanding of privacy risks posed by individual objects in images.
	3. Implications for developing better privacy protection designs in visual content sharing.

**Result:** The study uncovers how contextual details and the co-presence of objects affect users' privacy perceptions, indicating that privacy is perceived differently depending on specific elements in the image.

**Limitations:** 

**Conclusion:** These insights could inform the design of personalized and context-aware privacy protection features in social media and technology.

**Abstract:** User-generated content, such as photos, comprises the majority of online media content and drives engagement due to the human ability to process visual information quickly. Consequently, many online platforms are designed for sharing visual content, with billions of photos posted daily. However, photos often reveal more than they intended through visible and contextual cues, leading to privacy risks. Previous studies typically treat privacy as a property of the entire image, overlooking individual objects that may carry varying privacy risks and influence how users perceive it. We address this gap with a mixed-methods study (n = 92) to understand how users evaluate the privacy of images containing multiple sensitive objects. Our results reveal mental models and nuanced patterns that uncover how granular details, such as photo-capturing context and co-presence of other objects, affect privacy perceptions. These novel insights could enable personalized, context-aware privacy protection designs on social media and future technologies.

</details>


### [16] [Patient Perspectives on Telemonitoring during Colorectal Cancer Surgery Prehabilitation](https://arxiv.org/abs/2509.13064)

*Irina Bianca Serban, Dimitra Dritsa, David ten Cate, Loes Janssen, Margot Heijmans, Sara Colombo, Aarnout Brombacher, Steven Houben*

**Main category:** cs.HC

**Keywords:** telemonitoring, colorectal cancer, prehabilitation, patient-centered design, health informatics

**Relevance Score:** 7

**TL;DR:** This paper explores patient perspectives on telemonitoring in colorectal cancer prehabilitation, revealing motivations, benefits, and design considerations for patient-centered systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding patient perspectives on telemonitoring during prehabilitation to improve system design and patient outcomes in colorectal cancer care.

**Method:** Interviews with five patients who participated in a four-week prehabilitation program with continuous telemonitoring were conducted to gather insights on their experiences.

**Key Contributions:**

	1. Identified key factors influencing patient engagement with telemonitoring in prehabilitation
	2. Provided insights into patient motivations and concerns
	3. Outlined design considerations for patient-centered telemonitoring systems

**Result:** Patients were generally willing to engage with telemonitoring, influenced by their motivations, perceived benefits, and concerns, which can inform future system designs.

**Limitations:** The study is based on a small sample size of five patients, which may not represent the broader population.

**Conclusion:** The findings highlight the need for patient-centered design considerations in telemonitoring for prehabilitation and set the groundwork for future research in this area.

**Abstract:** Multimodal prehabilitation for colorectal cancer (CRC) surgery aims to optimize patient fitness and reduce postoperative complications. While telemonitoring's clinical value in supporting decision-making is recognized, patient perspectives on its use in prehabilitation remain underexplored, particularly compared to its related clinical context, rehabilitation. To address this gap, we conducted interviews with five patients who completed a four-week CRC prehabilitation program incorporating continuous telemonitoring. Our findings reveal patients' willingness to engage with telemonitoring, shaped by their motivations, perceived benefits, and concerns. We outline design considerations for patient-centered systems and offer a foundation for further research on telemonitoring in CRC prehabilitation.

</details>


### [17] [Textarium: Entangling Annotation, Abstraction and Argument](https://arxiv.org/abs/2509.13191)

*Philipp Proff, Marian Dörk*

**Main category:** cs.HC

**Keywords:** text interpretation, visual interface, scholarly reading

**Relevance Score:** 4

**TL;DR:** Web-based environment for annotating and interpreting texts, integrating human analysis with computational processing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the scholarly reading and writing process by bridging close and distant reading practices.

**Method:** The study employs a co-creative and iterative prototyping approach to design a visual interface for text interpretation.

**Key Contributions:**

	1. Development of Textarium as a visual interface for text interpretation
	2. Integration of human analysis with computational tools
	3. Facilitation of transparent and shareable interpretive processes

**Result:** Textarium allows users to highlight text, categorize keywords into concepts, and visualize their interpretive actions, making the process of reading and writing more transparent.

**Limitations:** 

**Conclusion:** The reading-writing approach developed through Textarium facilitates collaborative and shareable interpretive processes within digital narratives.

**Abstract:** We present a web-based environment that connects annotation, abstraction, and argumentation during the interpretation of text. As a visual interface for scholarly reading and writing, Textarium combines human analysis with lightweight computational processing to bridge close and distant reading practices. Readers can highlight text, group keywords into concepts, and embed these observations as anchors in essays. The interface renders these interpretive actions as parameterized visualization states. Through a speculative design process of co-creative and iterative prototyping, we developed a reading-writing approach that makes interpretive processes transparent and shareable within digital narratives.

</details>


### [18] [Evolution of Programmers' Trust in Generative AI Programming Assistants](https://arxiv.org/abs/2509.13253)

*Anshul Shah, Thomas Rexin, Elena Tomson, Leo Porter, William G. Griswold, Adalbert Gerald Soosai Raj*

**Main category:** cs.HC

**Keywords:** Trust in AI, GitHub Copilot, Programming education, Human-Computer Interaction, Trust calibration

**Relevance Score:** 8

**TL;DR:** Study on programmers' trust in GitHub Copilot over short and long-term use, revealing factors influencing trust and offering pedagogical recommendations.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding trust dynamics in generative AI programming assistants to improve their effective use among programmers.

**Method:** Survey data collected from 71 upper-division computer science students analyzing trust levels after using GitHub Copilot immediately and over 10 days.

**Key Contributions:**

	1. Reveals evolution of student trust in AI programming tools over time.
	2. Identifies factors that enhance trust in programming assistants like Copilot.
	3. Provides actionable recommendations for CS educators to improve trust calibration.

**Result:** Average student trust in Copilot increased over time, influenced by perceived correctness, context understanding, and basic NLP knowledge.

**Limitations:** 

**Conclusion:** The findings inform educators and industry managers about the importance of trust calibration in AI tools, leading to pedagogical recommendations.

**Abstract:** Motivation. Trust in generative AI programming assistants is a vital attitude that impacts how programmers use those programming assistants. Programmers that are over-trusting may be too reliant on their tools, leading to incorrect or vulnerable code; programmers that are under-trusting may avoid using tools that can improve their productivity and well-being.   Methods. Since trust is a dynamic attitude that may change over time, this study aims to understand programmers' evolution of trust after immediate (one hour) and extended (10 days) use of GitHub Copilot. We collected survey data from 71 upper-division computer science students working on a legacy code base, representing a population that is about to enter the workforce. In this study, we quantitatively measure student trust levels and qualitatively uncover why student trust changes.   Findings. Student trust, on average, increased over time. After completing a project with Copilot, however, students felt that Copilot requires a competent programmer to complete some tasks manually. Students mentioned that seeing Copilot's correctness, understanding how Copilot uses context from the code base, and learning some basics of natural language processing contributed to their elevated trust.   Implications. Our study helps instructors and industry managers understand the factors that influence how students calibrate their trust with AI assistants. We make four pedagogical recommendations, which are that CS educators should 1) provide opportunities for students to work with Copilot on challenging software engineering tasks to calibrate their trust, 2) teach traditional skills of comprehending, debugging, and testing so students can verify output, 3) teach students about the basics of natural language processing, and 4) explicitly introduce and demonstrate the range of features available in Copilot.

</details>


### [19] [Towards an Embodied Composition Framework for Organizing Immersive Computational Notebooks](https://arxiv.org/abs/2509.13291)

*Sungwon In, Eric Krokos, Kirsten Whitley, Chris North, Yalong Yang*

**Main category:** cs.HC

**Keywords:** immersive technologies, computational notebooks, user study

**Relevance Score:** 6

**TL;DR:** This paper presents an embodied composition framework to improve user interaction in immersive computational notebooks, revealing reduced effort and time in organizational tasks.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of scaling immersive computational notebooks, where analysts struggle with arranging numerous cells while maintaining execution logic and visual coherence.

**Method:** A controlled user study was conducted comparing manual and embodied composition frameworks in organizational processes.

**Key Contributions:**

	1. Introduction of an embodied composition framework for immersive computational notebooks.
	2. Evidence from user studies demonstrating reduced effort and time with the new framework.
	3. Identification of areas for improvement in the triggering mechanism.

**Result:** The study results indicate that the embodied composition framework significantly reduced user effort and completion time, although the design of the triggering mechanism needs refinement.

**Limitations:** The design of the triggering mechanism requires further refinement.

**Conclusion:** The findings suggest that embodied composition frameworks can enhance the scalability of organizational processes in immersive computational notebooks.

**Abstract:** As immersive technologies evolve, immersive computational notebooks offer new opportunities for interacting with code, data, and outputs. However, scaling these environments remains a challenge, particularly when analysts manually arrange large numbers of cells to maintain both execution logic and visual coherence. To address this, we introduce an embodied composition framework, facilitating organizational processes in the context of immersive computational notebooks. To evaluate the effectiveness of the embodied composition framework, we conducted a controlled user study comparing manual and embodied composition frameworks in an organizational process. The results show that embodied composition frameworks significantly reduced user effort and decreased completion time. However, the design of the triggering mechanism requires further refinement. Our findings highlight the potential of embodied composition frameworks to enhance the scalability of the organizational process in immersive computational notebooks.

</details>


### [20] [Investigating Seamless Transitions Between Immersive Computational Notebooks and Embodied Data Interactions](https://arxiv.org/abs/2509.13295)

*Sungwon In, Eric Krokos, Kirsten Whitley, Chris North, Yalong Yang*

**Main category:** cs.HC

**Keywords:** Immersive Analytics, Computational Notebooks, Embodied Interaction

**Relevance Score:** 7

**TL;DR:** The paper introduces ICoN, an immersive computational notebook prototype that combines traditional analytical tools with embodied data exploration, improving workflow efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the impracticality of using WIMP metaphors in complex data exploration, which currently forces analysts to switch between different environments during their workflow.

**Method:** ICoN prototype was developed to unify computational notebooks and embodied exploration in a fully immersive environment, allowing for seamless transitions.

**Key Contributions:**

	1. Development of ICoN prototype for immersive analytics
	2. Demonstration of improved workflow efficiency
	3. Integration of computational notebooks with embodied data exploration

**Result:** Testing showed that unification through ICoN enhances the efficiency and intuitiveness of analytical workflows, allowing for smoother data analysis.

**Limitations:** The study focuses on a prototype; broader applications and long-term usability remain to be tested.

**Conclusion:** ICoN has the potential to significantly improve data exploration by providing a unified environment that reduces the overhead of switching contexts.

**Abstract:** A growing interest in Immersive Analytics (IA) has led to the extension of computational notebooks (e.g., Jupyter Notebook) into an immersive environment to enhance analytical workflows. However, existing solutions rely on the WIMP (windows, icons, menus, pointer) metaphor, which remains impractical for complex data exploration. Although embodied interaction offers a more intuitive alternative, immersive computational notebooks and embodied data exploration systems are implemented as standalone tools. This separation requires analysts to invest considerable effort to transition from one environment to an entirely different one during analytical workflows. To address this, we introduce ICoN, a prototype that facilitates a seamless transition between computational notebooks and embodied data explorations within a unified, fully immersive environment. Our findings reveal that unification improves transition efficiency and intuitiveness during analytical workflows, highlighting its potential for seamless data analysis.

</details>


### [21] [Y-AR: A Mixed Reality CAD Tool for 3D Wire Bending](https://arxiv.org/abs/2410.23540)

*Shuo Feng, Bo Liu, Yifan, Shan, Roy Zunder, Wei-Che Lin, Tri Dinh, Harald Haraldsson, Ofer Berman, Thijs Roumen*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Wire Bending, Computer Aided Design, Gesture Interaction, Usability Evaluation

**Relevance Score:** 4

**TL;DR:** Y-AR is a mixed reality CAD interface for 3D wire bending that enables novice users to design and fabricate functional wire connectors with ease.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to enhance the accessibility and ease of custom fabrication for wire bending processes, which are traditionally reliant on complex CAM software without integrated design tools.

**Method:** The authors developed a mixed reality CAD interface called Y-AR which incorporates gesture-based interaction and design primitives like springs to facilitate the design and fabrication of wire connectors.

**Key Contributions:**

	1. Development of Y-AR mixed reality CAD tool for wire bending
	2. Integration of springs as design primitives to counteract inaccuracies
	3. Successful usability testing with novice users creating functional designs

**Result:** In a usability evaluation, 12 participants successfully designed and fabricated a functional bottle holder using the Y-AR interface, demonstrating its effectiveness for novice users.

**Limitations:** The study may be limited by the small sample size of usability testing and the focus on specific wire-bending applications.

**Conclusion:** The study concludes that the combination of gesture interaction and fabrication-aware design principles overcomes challenges in accuracy, enabling successful wire connector creation.

**Abstract:** Wire bending is a technique used in manufacturing to mass-produce items such as clips, mounts, and braces. Recent advances in programmable wire bending have made this process increasingly accessible for custom fabrication. However, CNC wire benders are controlled using Computer Aided Manufacturing (CAM) software, without design tools, making custom designs challenging to produce. We present Y-AR, a Computer Aided Design (CAD) interface for 3D wire bending. Y-AR uses mixed reality to let designers create clips, mounts, and braces to physically connect objects to their surrounding environment. The interface incorporates springs as design primitives which (1) apply forces to hold objects, and (2) counter-act dimensional inaccuracies inherently caused by mid-air modeling and measurement errors in AR. Springs are a natural design element when working with metal wire-bending given its specific material properties. We demonstrate workflows to design and fabricate a range of mechanisms in Y-AR as well as structures made using free-hand design tools. We found that combining gesture-based interaction with fabrication-aware design principles allowed novice users to create functional wire connectors, even when using imprecise XR-based input. In our usability evaluation, all 12 participants successfully designed and fabricated a functional bottle holder using Y-AR.

</details>


### [22] [Argumentative Experience: Reducing Confirmation Bias on Controversial Issues through LLM-Generated Multi-Persona Debates](https://arxiv.org/abs/2412.04629)

*Li Shi, Houjiang Liu, Yian Wong, Utkarsh Mujumdar, Dan Zhang, Jacek Gwizdka, Matthew Lease*

**Main category:** cs.HC

**Keywords:** multi-persona debate, large language models, confirmation bias, information seeking, belief change

**Relevance Score:** 8

**TL;DR:** This paper investigates the effectiveness of multi-persona debate systems powered by large language models (LLMs) in mitigating confirmation bias through an empirical study comparing them with traditional debiasing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLM-based debate systems affect user attention towards belief-challenging content and their efficacy compared to traditional methods.

**Method:** Participants were exposed to a multi-persona debate system and a two-stance retrieval-based system, with data collected through eye-tracking, belief change measures, and qualitative feedback.

**Key Contributions:**

	1. Empirical comparison of LLM-based debate systems with traditional debiasing methods
	2. Eye-tracking and qualitative feedback methodology
	3. Insights on the limits and potential of multi-persona systems in reducing confirmation bias

**Result:** The multi-persona debate system did not significantly increase attention to opposing views or shift beliefs, but it provided a buffering effect against bias from individual cognitive tendencies.

**Limitations:** The debate system did not significantly change attention or beliefs, suggesting limitations in its effectiveness for challenging established views.

**Conclusion:** The study highlights both the strengths and limitations of multi-persona debate systems in fostering balanced information engagement and offers design insights for future improvements.

**Abstract:** Multi-persona debate systems powered by large language models (LLMs) show promise in reducing confirmation bias, which can fuel echo chambers and social polarization. However, empirical evidence remains limited on whether they meaningfully shift user attention toward belief-challenging content, promote belief change, or outperform traditional debiasing strategies. To investigate this, we compare an LLM-based multi-persona debate system with a two-stance retrieval-based system, exposing participants to multiple viewpoints on controversial topics. By collecting eye-tracking data, belief change measures, and qualitative feedback, our results show that while the debate system does not significantly increase attention to opposing views, or make participants shift away from prior beliefs, it does provide a buffering effect against bias caused by individual cognitive tendency. These findings shed light on both the promise and limits of multi-persona debate systems in information seeking, and we offer design insights to guide future work toward more balanced and reflective information engagement.

</details>


### [23] [Schemex: Interactive Structural Abstraction from Examples with Contrastive Refinement](https://arxiv.org/abs/2504.11795)

*Sitong Wang, Samia Menon, Dingzeyu Li, Xiaojuan Ma, Richard Zemel, Lydia B. Chilton*

**Main category:** cs.HC

**Keywords:** schema induction, human-AI collaboration, visualization, interaction, clustering

**Relevance Score:** 6

**TL;DR:** Schemex is a visual tool designed to aid schema induction through interactive exploration and clustering, enhancing human-AI collaboration and user insight.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge in schema induction arises from the obscuring of structural patterns by surface-level variability, necessitating tools that facilitate understanding and categorizing of these schemas.

**Method:** Schemex employs interactive visual representations to guide users in clustering and refining their understanding of schemas, allowing for exploration of connections between abstract structures and concrete examples.

**Key Contributions:**

	1. Development of Schemex, an interactive visual workflow for schema induction.
	2. Empirical evidence showing enhanced user insight and confidence through the use of Schemex.
	3. Discussion of broader implications for structural abstraction in human-AI collaboration.

**Result:** User studies indicated that participants using Schemex felt more insight and confidence in their developed schemas than those using a standard AI model.

**Limitations:** The abstract does not specify limitations, but it is likely that the effectiveness may vary across different user demographics or types of tasks.

**Conclusion:** The findings suggest that structural abstraction and contrastive refinement have wider implications in enhancing understanding across various domains, advocating for tools that support these processes.

**Abstract:** Each type of creative or communicative work is underpinned by an implicit structure. People learn these structures from examples - a process known in cognitive science as schema induction. However, inducing schemas is challenging, as structural patterns are often obscured by surface-level variation. We present Schemex, an interactive visual workflow that scaffolds schema induction through clustering, abstraction, and contrastive refinement. Schemex supports users through visual representations and interactive exploration that connect abstract structures to concrete examples, promoting transparency, adaptability, and effective human-AI collaboration. In our user study, participants reported significantly greater insight and confidence in the schemas developed with Schemex compared to those created using a baseline of an AI reasoning model. We conclude by discussing the broader implications of structural abstraction and contrastive refinement across domains.

</details>


### [24] [The Adaptation Paradox: Agency vs. Mimicry in Companion Chatbots](https://arxiv.org/abs/2509.12525)

*T. James Brandt, Cecilia Xi Wang*

**Main category:** cs.HC

**Keywords:** Generative AI, Chatbots, Language Style Matching, User Rapport, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This study investigates the impact of user-generated avatars and language style matching on user rapport with chatbots, revealing an 'Adaptation Paradox' where adaptive language styles may hinder connection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how different aspects of chatbot design, such as avatar visibility and language style matching, influence user rapport and connection.

**Method:** A preregistered 3x2 experiment with 162 participants manipulating avatar generation (none, premade, user-generated) and Language Style Matching (static vs. adaptive).

**Key Contributions:**

	1. Identified the positive impact of user-generated avatars on chatbot rapport.
	2. Revealed the Adaptation Paradox where adaptive language styles may reduce perceived coherence.
	3. Provided a framework for prioritizing user-driven personalization in chatbot design.

**Result:** Generating a user-controlled avatar improved rapport, but adaptive language style matching was less effective for personalization and satisfaction and caused a perception of incoherence.

**Limitations:** Study limited to a laboratory setting with specific experimental conditions, potential lack of ecological validity.

**Conclusion:** Designers should emphasize visible user authorship in chatbot interactions to enhance user connection instead of relying on covert language mimicry.

**Abstract:** Generative AI powers a growing wave of companion chatbots, yet principles for fostering genuine connection remain unsettled. We test two routes: visible user authorship versus covert language-style mimicry. In a preregistered 3x2 experiment (N = 162), we manipulated user-controlled avatar generation (none, premade, user-generated) and Language Style Matching (LSM) (static vs. adaptive). Generating an avatar boosted rapport ($\omega^2$ = .040, p = .013), whereas adaptive LSM underperformed static style on personalization and satisfaction (d = 0.35, p = .009) and was paradoxically judged less adaptive (t = 3.07, p = .003, d = 0.48). We term this an Adaptation Paradox: synchrony erodes connection when perceived as incoherent, destabilizing persona. To explain, we propose a stability-and-legibility account: visible authorship fosters natural interaction, while covert mimicry risks incoherence. Our findings suggest designers should prioritize legible, user-driven personalization and limit stylistic shifts rather than rely on opaque mimicry.

</details>


### [25] [Textarium: Entangling Annotation, Abstraction and Argument](https://arxiv.org/abs/2509.13191)

*Philipp Proff, Marian Dörk*

**Main category:** cs.HC

**Keywords:** Annotation, Abstraction, Argumentation, Text interpretation, Visualization

**Relevance Score:** 4

**TL;DR:** Textarium is a web-based interface that integrates annotation, abstraction, and argumentation to enhance scholarly reading and writing by visualizing interpretive actions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind Textarium is to improve the process of interpreting text by facilitating both close and distant reading practices in a more visual and interactive manner.

**Method:** Textarium employs a web-based environment that allows users to highlight text, group keywords, and embed observations as anchors in essays, manifesting these actions as parameterized visualizations.

**Key Contributions:**

	1. Development of an innovative web-based environment for scholarly text interaction.
	2. Integration of annotation, abstraction, and argumentation into a single interface.
	3. Provision of parameterized visualizations that make interpretive actions transparent.

**Result:** The implementation of Textarium offers a transparent and shareable approach to the interpretive processes involved in reading and writing, enhancing scholarly communication.

**Limitations:** 

**Conclusion:** Textarium effectively bridges the gap between human analysis and computational processing, promoting a more interactive and collaborative approach to text interpretation.

**Abstract:** We present a web-based environment that connects annotation, abstraction, and argumentation during the interpretation of text. As a visual interface for scholarly reading and writing, Textarium combines human analysis with lightweight computational processing to bridge close and distant reading practices. Readers can highlight text, group keywords into concepts, and embed these observations as anchors in essays. The interface renders these interpretive actions as parameterized visualization states. Through a speculative design process of co-creative and iterative prototyping, we developed a reading-writing approach that makes interpretive processes transparent and shareable within digital narratives.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [26] [MTEB-NL and E5-NL: Embedding Benchmark and Models for Dutch](https://arxiv.org/abs/2509.12340)

*Nikolay Banar, Ehsan Lotfi, Jens Van Nooten, Cristina Arhiliuc, Marija Kliocaite, Walter Daelemans*

**Main category:** cs.CL

**Keywords:** Dutch language, embeddings, MTEB-NL, E5-NL, language models

**Relevance Score:** 4

**TL;DR:** This paper introduces resources aimed at enhancing Dutch language embeddings through the Massive Text Embedding Benchmark for Dutch (MTEB-NL) and a series of E5-NL embedding models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of the Dutch language in multilingual embedding resources and encourage their development.

**Method:** Introduction of MTEB-NL for evaluation and generation of Dutch embeddings, supplemented by a training dataset from existing Dutch retrieval datasets and synthetic data from large language models. Release of E5-NL models for embedding tasks.

**Key Contributions:**

	1. Introduction of MTEB-NL for Dutch embeddings evaluation and generation.
	2. Release of training data compiled from Dutch datasets complemented by synthetic data.
	3. Development and public release of E5-NL embedding models.

**Result:** MTEB-NL includes existing and new datasets, a training dataset enhancing task coverage, and E5-NL models that show strong performance in embedding tasks.

**Limitations:** 

**Conclusion:** The resources provided will stimulate the development and evaluation of Dutch language embeddings and are publicly available.

**Abstract:** Recently, embedding resources, including models, benchmarks, and datasets, have been widely released to support a variety of languages. However, the Dutch language remains underrepresented, typically comprising only a small fraction of the published multilingual resources. To address this gap and encourage the further development of Dutch embeddings, we introduce new resources for their evaluation and generation. First, we introduce the Massive Text Embedding Benchmark for Dutch (MTEB-NL), which includes both existing Dutch datasets and newly created ones, covering a wide range of tasks. Second, we provide a training dataset compiled from available Dutch retrieval datasets, complemented with synthetic data generated by large language models to expand task coverage beyond retrieval. Finally, we release a series of E5-NL models compact yet efficient embedding models that demonstrate strong performance across multiple tasks. We make our resources publicly available through the Hugging Face Hub and the MTEB package.

</details>


### [27] [MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables](https://arxiv.org/abs/2509.12371)

*Matteo Marcuzzo, Alessandro Zangari, Andrea Albarelli, Jose Camacho-Collados, Mohammad Taher Pilehvar*

**Main category:** cs.CL

**Keywords:** moral reasoning, large language models, adversarial testing

**Relevance Score:** 8

**TL;DR:** MORABLES is a benchmark for evaluating LLMs on moral inference using historical literature, revealing vulnerabilities in reasoning despite scale.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate LLMs' capacity for complex reasoning beyond standard benchmarks.

**Method:** A benchmark built from fables and short stories with multiple-choice questions focusing on moral inference and adversarial testing.

**Key Contributions:**

	1. Introduction of MORABLES benchmark for moral reasoning
	2. Identification of model vulnerabilities through adversarial testing
	3. Insights into the limitations of reasoning-enhanced models

**Result:** Larger models outperform smaller ones, yet remain susceptible to adversarial attacks and often fall back on superficial patterns.

**Limitations:** Models show significant self-contradictions and reliance on superficial patterns in reasoning situations.

**Conclusion:** Scale is a key driver of performance rather than true reasoning ability, with significant self-contradiction observed in responses.

**Abstract:** As LLMs excel on standard reading comprehension benchmarks, attention is shifting toward evaluating their capacity for complex abstract reasoning and inference. Literature-based benchmarks, with their rich narrative and moral depth, provide a compelling framework for evaluating such deeper comprehension skills. Here, we present MORABLES, a human-verified benchmark built from fables and short stories drawn from historical literature. The main task is structured as multiple-choice questions targeting moral inference, with carefully crafted distractors that challenge models to go beyond shallow, extractive question answering. To further stress-test model robustness, we introduce adversarial variants designed to surface LLM vulnerabilities and shortcuts due to issues such as data contamination. Our findings show that, while larger models outperform smaller ones, they remain susceptible to adversarial manipulation and often rely on superficial patterns rather than true moral reasoning. This brittleness results in significant self-contradiction, with the best models refuting their own answers in roughly 20% of cases depending on the framing of the moral choice. Interestingly, reasoning-enhanced models fail to bridge this gap, suggesting that scale - not reasoning ability - is the primary driver of performance.

</details>


### [28] [LLM-as-a-Judge: Rapid Evaluation of Legal Document Recommendation for Retrieval-Augmented Generation](https://arxiv.org/abs/2509.12382)

*Anu Pradhan, Alexandra Ortan, Apurv Verma, Madhavan Seshadri*

**Main category:** cs.CL

**Keywords:** Generative AI, LLM evaluation, Recommendation systems, Legal applications, Statistical methods

**Relevance Score:** 6

**TL;DR:** This paper explores the evaluation of Retrieval-Augmented Generation systems in legal contexts using LLMs and proposes new metrics for assessing LLM performance against human judgments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of traditional evaluation metrics for recommendation systems with the rise of Generative AI, particularly in high-stakes fields like legal research.

**Method:** The paper investigates which inter-rater reliability metrics effectively align LLM with human assessments and employs experiments to compare traditional and newer statistical methods for evaluating AI performance.

**Key Contributions:**

	1. Introduces LLM-as-a-Judge for evaluating AI systems in legal contexts.
	2. Identifies more effective metrics (Gwet's AC2) for inter-rater reliability in AI evaluations.
	3. Develops a statistically sound framework for comparing competing AI systems.

**Result:** Gwet's AC2 and rank correlation coefficients are identified as more reliable for judge selection, while the Wilcoxon Signed-Rank Test with corrections proves effective for system comparisons.

**Limitations:** 

**Conclusion:** The proposed approach aims to create a scalable evaluation framework that combines automation with the precision required for legal applications, reducing the dependency on human evaluators.

**Abstract:** The evaluation bottleneck in recommendation systems has become particularly acute with the rise of Generative AI, where traditional metrics fall short of capturing nuanced quality dimensions that matter in specialized domains like legal research. Can we trust Large Language Models to serve as reliable judges of their own kind? This paper investigates LLM-as-a-Judge as a principled approach to evaluating Retrieval-Augmented Generation systems in legal contexts, where the stakes of recommendation quality are exceptionally high.   We tackle two fundamental questions that determine practical viability: which inter-rater reliability metrics best capture the alignment between LLM and human assessments, and how do we conduct statistically sound comparisons between competing systems? Through systematic experimentation, we discover that traditional agreement metrics like Krippendorff's alpha can be misleading in the skewed distributions typical of AI system evaluations. Instead, Gwet's AC2 and rank correlation coefficients emerge as more robust indicators for judge selection, while the Wilcoxon Signed-Rank Test with Benjamini-Hochberg corrections provides the statistical rigor needed for reliable system comparisons.   Our findings suggest a path toward scalable, cost-effective evaluation that maintains the precision demanded by legal applications, transforming what was once a human-intensive bottleneck into an automated, yet statistically principled, evaluation framework.

</details>


### [29] [SENTRA: Selected-Next-Token Transformer for LLM Text Detection](https://arxiv.org/abs/2509.12385)

*Mitchell Plyler, Yilun Zhang, Alexander Tuzhilin, Saoud Khalifah, Sen Tian*

**Main category:** cs.CL

**Keywords:** LLM detection, text generation, Transformer, contrastive learning, content verification

**Relevance Score:** 9

**TL;DR:** SENTRA is a novel LLM text detector that outperforms existing baselines in identifying LLM-generated text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rising capabilities and usage of LLMs, detecting their misuse has become crucial.

**Method:** SENTRA employs a Transformer-based architecture that utilizes selected-next-token-probability sequences and incorporates contrastive pre-training on large unlabeled datasets.

**Key Contributions:**

	1. Development of SENTRA, a novel LLM text detector
	2. Demonstration of improved performance over existing classifiers
	3. Contrastive pre-training on unlabeled data for general-purpose application

**Result:** SENTRA is shown to significantly outperform popular baseline models across three datasets and 24 text domains.

**Limitations:** 

**Conclusion:** SENTRA offers a reliable method for detecting LLM-generated text, enhancing capabilities in content verification.

**Abstract:** LLMs are becoming increasingly capable and widespread. Consequently, the potential and reality of their misuse is also growing. In this work, we address the problem of detecting LLM-generated text that is not explicitly declared as such. We present a novel, general-purpose, and supervised LLM text detector, SElected-Next-Token tRAnsformer (SENTRA). SENTRA is a Transformer-based encoder leveraging selected-next-token-probability sequences and utilizing contrastive pre-training on large amounts of unlabeled data. Our experiments on three popular public datasets across 24 domains of text demonstrate SENTRA is a general-purpose classifier that significantly outperforms popular baselines in the out-of-domain setting.

</details>


### [30] [MORQA: Benchmarking Evaluation Metrics for Medical Open-Ended Question Answering](https://arxiv.org/abs/2509.12405)

*Wen-wai Yim, Asma Ben Abacha, Zixuan Yu, Robert Doerning, Fei Xia, Meliha Yetisgen*

**Main category:** cs.CL

**Keywords:** natural language generation, medical QA, LLM evaluation, benchmark, evaluation metrics

**Relevance Score:** 9

**TL;DR:** This paper introduces MORQA, a multilingual benchmark for evaluating natural language generation systems in the medical domain, demonstrating the superiority of large language models over traditional metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The accuracy and relevance of natural language generation (NLG) systems in the medical domain are crucial, necessitating better evaluation methods than traditional automatic metrics, which fail to distinguish quality in medical QA tasks.

**Method:** The study presents MORQA, a multilingual benchmark featuring 2-4+ gold-standard answers from medical professionals and expert human ratings for English and Chinese datasets. It benchmarks traditional metrics against LLM-based evaluators like GPT-4 and Gemini.

**Key Contributions:**

	1. Introduction of MORQA, a multilingual benchmark for medical NLG evaluation.
	2. Demonstration of LLM-based evaluators' superiority in correlation with expert judgments over traditional metrics.
	3. Public release of datasets and annotations to aid future research in NLG evaluation.

**Result:** LLM-based evaluators significantly outperform traditional metrics in correlating with expert judgments, illustrating their effectiveness in handling the nuances of medical QA tasks.

**Limitations:** Evaluation metrics may still have room for improvement, potential biases in human judgments, and the study is limited to English and Chinese.

**Conclusion:** The findings advocate for the use of LLM-based evaluation methods over conventional metrics, underscoring the importance of human-aligned evaluation in medical NLG systems.

**Abstract:** Evaluating natural language generation (NLG) systems in the medical domain presents unique challenges due to the critical demands for accuracy, relevance, and domain-specific expertise. Traditional automatic evaluation metrics, such as BLEU, ROUGE, and BERTScore, often fall short in distinguishing between high-quality outputs, especially given the open-ended nature of medical question answering (QA) tasks where multiple valid responses may exist. In this work, we introduce MORQA (Medical Open-Response QA), a new multilingual benchmark designed to assess the effectiveness of NLG evaluation metrics across three medical visual and text-based QA datasets in English and Chinese. Unlike prior resources, our datasets feature 2-4+ gold-standard answers authored by medical professionals, along with expert human ratings for three English and Chinese subsets. We benchmark both traditional metrics and large language model (LLM)-based evaluators, such as GPT-4 and Gemini, finding that LLM-based approaches significantly outperform traditional metrics in correlating with expert judgments. We further analyze factors driving this improvement, including LLMs' sensitivity to semantic nuances and robustness to variability among reference answers. Our results provide the first comprehensive, multilingual qualitative study of NLG evaluation in the medical domain, highlighting the need for human-aligned evaluation methods. All datasets and annotations will be publicly released to support future research.

</details>


### [31] [MedFact: Benchmarking the Fact-Checking Capabilities of Large Language Models on Chinese Medical Texts](https://arxiv.org/abs/2509.12440)

*Jiayi He, Yangmin Huang, Qianyun Du, Xiangying Zhou, Zhiyang He, Jiaxue Hu, Xiaodong Tao, Lixian Lai*

**Main category:** cs.CL

**Keywords:** Large Language Models, healthcare, fact-checking, benchmark, machine learning

**Relevance Score:** 9

**TL;DR:** Introduction of MedFact, a benchmark for evaluating the factual reliability of LLMs in healthcare, featuring 2,116 expert-annotated instances across various medical domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in existing benchmarks that fail to capture the complexity of real-world medical information, particularly in the context of LLMs.

**Method:** MedFact was constructed using a hybrid AI-human framework that includes iterative expert feedback to refine a multi-criteria filtering process, ensuring high data quality and varied difficulty.

**Key Contributions:**

	1. Introduction of the MedFact benchmark for Chinese medical fact-checking.
	2. Evaluation of LLMs reveals significant challenges in error localization.
	3. Highlights the over-criticism phenomenon in LLM outputs.

**Result:** Evaluation of 20 leading LLMs revealed models can identify errors but struggle with error localization; the 'over-criticism' phenomenon was also noted where correct information is incorrectly labeled as erroneous.

**Limitations:** Limited to Chinese medical information and focused on specific error types and specialties.

**Conclusion:** MedFact serves as a crucial resource for improving the factually reliability of LLMs in medical applications.

**Abstract:** The increasing deployment of Large Language Models (LLMs) in healthcare necessitates a rigorous evaluation of their factual reliability. However, existing benchmarks are often limited by narrow domains of data, failing to capture the complexity of real-world medical information. To address this critical gap, we introduce MedFact, a new and challenging benchmark for Chinese medical fact-checking. MedFact comprises 2,116 expert-annotated instances curated from diverse real-world texts, spanning 13 medical specialties, 8 fine-grained error types, 4 writing styles, and multiple difficulty levels. Its construction employs a hybrid AI-human framework where iterative expert feedback refines an AI-driven, multi-criteria filtering process, ensuring both high data quality and difficulty. We conduct a comprehensive evaluation of 20 leading LLMs, benchmarking their performance on veracity classification and error localization against a human expert baseline. Our results reveal that while models can often determine if a text contains an error, precisely localizing it remains a substantial challenge, with even top-performing models falling short of human performance. Furthermore, our analysis uncovers a frequent ``over-criticism'' phenomenon, a tendency for models to misidentify correct information as erroneous, which is exacerbated by advanced reasoning techniques such as multi-agent collaboration and inference-time scaling. By highlighting these critical challenges for deploying LLMs in medical applications, MedFact provides a robust resource to drive the development of more factually reliable and medically aware models.

</details>


### [32] [Topic Coverage-based Demonstration Retrieval for In-Context Learning](https://arxiv.org/abs/2509.12451)

*Wonbin Kweon, SeongKu Kang, Runchu Tian, Pengcheng Jiang, Jiawei Han, Hwanjo Yu*

**Main category:** cs.CL

**Keywords:** in-context learning, demonstration selection, topic coverage, large language models, knowledge retrieval

**Relevance Score:** 7

**TL;DR:** The paper introduces TopicK, a framework for selecting demonstrations in in-context learning based on topic coverage to improve model performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for effective selection of demonstrations in in-context learning to enhance model performance by covering fine-grained knowledge requirements.

**Method:** TopicK estimates the topic requirements of input and assesses the model's knowledge on those topics, iteratively selecting demonstrations that cover previously uncovered required topics.

**Key Contributions:**

	1. Introduces a novel framework for topic coverage-based demonstration retrieval
	2. Establishes a methodology for estimating topic requirements and model knowledge
	3. Demonstrates effectiveness through empirical validation on diverse datasets

**Result:** TopicK was validated through extensive experiments, showing improved performance across various datasets and LLMs.

**Limitations:** The study might be limited by the datasets used for validation and the specific LLMs examined.

**Conclusion:** The TopicK framework demonstrates a significant enhancement in demonstration selection by focusing on topic coverage, thereby improving in-context learning effectiveness.

**Abstract:** The effectiveness of in-context learning relies heavily on selecting demonstrations that provide all the necessary information for a given test input. To achieve this, it is crucial to identify and cover fine-grained knowledge requirements. However, prior methods often retrieve demonstrations based solely on embedding similarity or generation probability, resulting in irrelevant or redundant examples. In this paper, we propose TopicK, a topic coverage-based retrieval framework that selects demonstrations to comprehensively cover topic-level knowledge relevant to both the test input and the model. Specifically, TopicK estimates the topics required by the input and assesses the model's knowledge on those topics. TopicK then iteratively selects demonstrations that introduce previously uncovered required topics, in which the model exhibits low topical knowledge. We validate the effectiveness of TopicK through extensive experiments across various datasets and both open- and closed-source LLMs. Our source code is available at https://github.com/WonbinKweon/TopicK_EMNLP2025.

</details>


### [33] [Does Language Model Understand Language?](https://arxiv.org/abs/2509.12459)

*Suvojit Acharjee, Utathya Aich, Asfak Ali*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Language Models, Education Technologies, Cognitive Inference, Linguistic Clarity

**Relevance Score:** 9

**TL;DR:** This study evaluates state-of-the-art language models on their ability to process complex linguistic phenomena critical for human communication in educational contexts, using a new dataset and innovative evaluation metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The gap in fine-grained linguistic comprehension by language models hinders effective human communication, especially in educational technologies aligned with United Nations SDG 4 requirements.

**Method:** We introduce a new evaluation framework (Route for Evaluation of Cognitive Inference in Systematic Environments) and the LUCID dataset, which challenges language models on aspects like tense, negation, and voice. We assess models using standard metrics and a novel HCE accuracy metric.

**Key Contributions:**

	1. Development of the LUCID dataset for linguistic evaluation.
	2. Introduction of the HCE accuracy metric for assessing model predictions against human judgment.
	3. Evaluation of multiple state-of-the-art language models on complex linguistic tasks.

**Result:** Our evaluation shows Compound-Beta is the best performing model, achieving high correlations and low error rates, particularly in English and mixed-language scenarios.

**Limitations:** Focus primarily on English and Bengali, which may limit generalizability to other languages or contexts.

**Conclusion:** The findings suggest that careful assessment of language models can reveal their strengths and weaknesses in understanding nuanced linguistic elements, with implications for educational technologies.

**Abstract:** Despite advances in natural language generation and understanding, LM still struggle with fine grained linguistic phenomena such as tense, negation, voice, and modality which are the elements central to effective human communication. In the context of the United Nations SDG 4, where linguistic clarity is critical, the deployment of LMs in educational technologies demands careful scrutiny. As LMs are increasingly powering applications like tutoring systems, automated grading, and translation, their alignment with human linguistic interpretation becomes essential for effective learning. In this study, we conduct a evaluation of SOTA language models across these challenging contexts in both English and Bengali. To ensure a structured assessment, we introduce a new Route for Evaluation of Cognitive Inference in Systematic Environments guidelines. Our proposed LUCID dataset, composed of carefully crafted sentence pairs in English and Bengali, specifically challenges these models on critical aspects of language comprehension, including negation, tense, voice variations. We assess the performance of SOTA models including MISTRAL-SABA-24B, LLaMA-4-Scout-17B, LLaMA-3.3-70B, Gemma2-9B, and Compound-Beta using standard metrics like Pearson correlation, Spearman correlation, and Mean Absolute Error, as well as novel, linguistically inspired metric the HCE accuracy. The HCE accuracy measures how often model predictions fall within one standard deviation of the mean human rating, thus capturing human like tolerance for variability in language interpretation. Our findings highlight Compound-Beta as the most balanced model, consistently achieving high correlations and low MAEs across diverse language conditions. It records the highest Pearson correlation in English and demonstrates robust performance on mixed-language data, indicating a strong alignment with human judgments in cross lingual scenarios.

</details>


### [34] [Audited Reasoning Refinement: Fine-Tuning Language Models via LLM-Guided Step-Wise Evaluation and Correction](https://arxiv.org/abs/2509.12476)

*Sumanta Bhattacharyya, Sara Riaz, Pedram Rooshenas*

**Main category:** cs.CL

**Keywords:** machine learning, reasoning models, supervision, LLM adaptation, human-validated preferences

**Relevance Score:** 8

**TL;DR:** The paper presents a novel method called R2tA for enhancing task-specific reasoning models by refining reasoning traces from LLMs to create high-quality supervision signals, particularly in data-scarce domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of training effective reasoning models in situations with limited supervision or high-quality labels motivates the development of R2tA, which leverages LLM reasoning traces for model training.

**Method:** R2tA involves generating reasoning and responses from a base model, refining these traces to address errors, and employing a two-stage alignment process that includes supervised fine-tuning followed by preference optimization.

**Key Contributions:**

	1. Introduction of R2tA for improved supervision in reasoning model training
	2. Development of a dataset with induced mistakes for method evaluation
	3. Demonstration of scalability in LLM adaptation for education through empirical studies

**Result:** Empirical evaluations indicate that R2tA offers a robust and efficient approach to adapt LLMs in scarce data environments, significantly improving output quality in tasks like evaluating EERDs.

**Limitations:** The method's effectiveness may vary based on the initial quality of the LLM and the specific tasks it is applied to.

**Conclusion:** R2tA demonstrates that refined reasoning traces can effectively inform and enhance training processes for AI models in educational contexts and more.

**Abstract:** Training a task-specific small reasoning model is challenging when direct human supervision or high-quality labels are scarce. However, LLMs with reasoning capabilities produce abundant intermediate reasoning traces that can be systematically refined to create effective supervision signals. We propose Reason-Refine-then-Align (R2tA), which turns refined model rationales into supervision for training task-specific reasoning models. Our method generates initial reasoning and responses from an open-source base model on task-specific inputs, then refines these traces, fixing hallucinations and inconsistencies, to form a high-fidelity dataset. We perform a two-stage alignment, supervised fine-tuning (SFT), followed by direct preference optimization (DPO) to calibrate the model's intermediate reasoning with human-validated conceptual preferences and then condition the final output on that aligned reasoning. As a case study, we apply R2tA to evaluate extended entity relationship diagrams (EERDs) in database system design, a structurally complex task where prompt-only methods miss or hallucinate errors. We curated a dataset of 600 EERD variants (train/test split of 450/150, respectively) with induced mistakes spanning 11 categories. Empirical evaluation suggests R2tA provides a practical, cost-effective path to scalable LLM adaptation in data-scarce domains, enabling reproducible AI tools for education and beyond.

</details>


### [35] [FunAudio-ASR Technical Report](https://arxiv.org/abs/2509.12508)

*Keyu An, Yanni Chen, Chong Deng, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Wen Wang, Wupeng Wang, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, large language models, reinforcement learning

**Relevance Score:** 7

**TL;DR:** FunAudio-ASR is an advanced LLM-based ASR system designed to enhance performance in practical applications through optimization in key areas.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination issues in LLMs that degrade user experience in real-world ASR applications.

**Method:** The paper introduces FunAudio-ASR, leveraging data scaling, large model capacity, LLM integration, and reinforcement learning for ASR.

**Key Contributions:**

	1. Introduction of FunAudio-ASR combining LLMs and advanced ASR techniques.
	2. Optimizations for streaming capability and noise robustness.
	3. Demonstration of SOTA performance on real-world datasets.

**Result:** FunAudio-ASR demonstrates state-of-the-art performance in practical settings, outperforming traditional LLM-based ASR systems on industry evaluation sets.

**Limitations:** 

**Conclusion:** FunAudio-ASR's production-oriented optimizations make it highly effective for real-world ASR applications, achieving strong performance in various scenarios.

**Abstract:** In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present FunAudio-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, FunAudio-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, FunAudio-ASR achieves SOTA performance on real application datasets, demonstrating its effectiveness and robustness in practical settings.

</details>


### [36] [A comparison of pipelines for the translation of a low resource language based on transformers](https://arxiv.org/abs/2509.12514)

*Chiara Bonfanti, Michele Colombino, Giulia Coucourde, Faeze Memari, Stefano Pinardi, Rosa Meo*

**Main category:** cs.CL

**Keywords:** machine translation, transformer models, Bambara language, low-resource language, language distillation

**Relevance Score:** 3

**TL;DR:** This paper compares three transformer-based machine translation pipelines for translating French to Bambara, focusing on their effectiveness and evaluation metrics.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the need for effective translation methods for Bambara, a low-resource language, by comparing different machine translation pipelines.

**Method:** Three distinct pipelines were trained: a simple transformer model, fine-tuning LLaMA3 models, and a student-teacher model using language distillation with LaBSE.

**Key Contributions:**

	1. Comparison of three translation pipelines for a low-resource language.
	2. Introduction of a new dataset for evaluating translation accuracy.
	3. Insights into the effectiveness of instructor-based models on specific datasets.

**Result:** The first pipeline outperformed the others in translation accuracy, yielding 10% BLEU and 21% chrF on the Bayelemagaba dataset, and 33.81% BLEU and 41% chrF on the Yiri dataset.

**Limitations:** The models were only tested on specific datasets and their performance may vary with different texts or domains.

**Conclusion:** Despite the simplicity, the first pipeline achieved the best results, indicating that simpler models can outperform more complex ones in low-resource translation tasks.

**Abstract:** This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand\`e language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively.

</details>


### [37] [MAGIC-Enhanced Keyword Prompting for Zero-Shot Audio Captioning with CLIP Models](https://arxiv.org/abs/2509.12591)

*Vijay Govindarajan, Pratik Patel, Sahil Tripathi, Md Azizul Hoque, Gautam Siddharth Kashyap*

**Main category:** cs.CL

**Keywords:** Automated Audio Captioning, Zero-shot learning, Large Language Models

**Relevance Score:** 7

**TL;DR:** This paper presents a zero-shot Automated Audio Captioning (AAC) system that utilizes pre-trained models to generate captions for audio clips without extensive training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of Automated Audio Captioning due to the scarcity of datasets compared to image captioning.

**Method:** The proposed system employs a pre-trained audio CLIP model to extract auditory features and generate a structured prompt for a Large Language Model (LLM) to produce captions, refining token selection via the audio CLIP model.

**Key Contributions:**

	1. Introduction of a zero-shot AAC system
	2. Use of audio CLIP model for feature extraction
	3. Demonstrated performance improvements over traditional methods

**Result:** Experimental results indicate a 35% improvement in the NLG mean score, increasing from 4.7 to 7.3 using MAGIC search with the WavCaps model.

**Limitations:** Performance heavily depends on audio-text matching and keyword selection, with a notable drop when no keywords are used.

**Conclusion:** The effectiveness of the system is significantly affected by the audio-text matching model and keyword selection, achieving optimal results with a single keyword prompt.

**Abstract:** Automated Audio Captioning (AAC) generates captions for audio clips but faces challenges due to limited datasets compared to image captioning. To overcome this, we propose the zero-shot AAC system that leverages pre-trained models, eliminating the need for extensive training. Our approach uses a pre-trained audio CLIP model to extract auditory features and generate a structured prompt, which guides a Large Language Model (LLM) in caption generation. Unlike traditional greedy decoding, our method refines token selection through the audio CLIP model, ensuring alignment with the audio content. Experimental results demonstrate a 35% improvement in NLG mean score (from 4.7 to 7.3) using MAGIC search with the WavCaps model. The performance is heavily influenced by the audio-text matching model and keyword selection, with optimal results achieved using a single keyword prompt, and a 50% performance drop when no keyword list is used.

</details>


### [38] [EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving](https://arxiv.org/abs/2509.12603)

*Mukai Li, Linfeng Song, Zhenwen Liang, Jiahao Xu, Shansan Gong, Qi Liu, Haitao Mi, Dong Yu*

**Main category:** cs.CL

**Keywords:** Automated Theorem Proving, Large Language Models, Chain-of-Thought reasoning

**Relevance Score:** 4

**TL;DR:** The paper evaluates the efficiency of test-time scaling strategies in Automated Theorem Proving (ATP) with Large Language Models (LLMs) and proposes methods to reduce computational costs while maintaining performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiency of existing test-time scaling strategies in ATP models, which lead to high computational overhead, particularly in sampling passes and token usage.

**Method:** The authors systematically compare different test-time scaling strategies and propose two methods: a dynamic Chain-of-Thought (CoT) switching mechanism to reduce token consumption and a Diverse parallel-scaled reinforcement learning (RL) approach with trainable prefixes to enhance pass rates.

**Key Contributions:**

	1. Systematic comparison of test-time scaling strategies for ATP with LLMs
	2. Introduction of a dynamic CoT switching mechanism to reduce unnecessary token usage
	3. Development of Diverse parallel-scaled RL with trainable prefixes to improve sampling efficiency

**Result:** The proposed EconProver achieves performance comparable to baseline methods while using only 12% of the computational cost, showcasing significant efficiency improvements.

**Limitations:** 

**Conclusion:** This research provides insights for developing lightweight ATP models that maintain performance levels, thereby enabling more efficient application of LLMs in theorem proving.

**Abstract:** Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performance gains through widely adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT) reasoning and increased sampling passes. However, they both introduce significant computational overhead for inference. Moreover, existing cost analyses typically regulate only the number of sampling passes, while neglecting the substantial disparities in sampling costs introduced by different scaling strategies. In this paper, we systematically compare the efficiency of different test-time scaling strategies for ATP models and demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source approaches. We then investigate approaches to significantly reduce token usage and sample passes while maintaining the original performance. Specifically, we propose two complementary methods that can be integrated into a unified EconRL pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching mechanism designed to mitigate unnecessary token consumption, and (2) Diverse parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance pass rates under constrained sampling passes. Experiments on miniF2F and ProofNet demonstrate that our EconProver achieves comparable performance to baseline methods with only 12% of the computational cost. This work provides actionable insights for deploying lightweight ATP models without sacrificing performance.

</details>


### [39] [Positional Encoding via Token-Aware Phase Attention](https://arxiv.org/abs/2509.12635)

*Yu, Wang, Sheng Shen, Rémi Munos, Hongyuan Zhan, Yuandong Tian*

**Main category:** cs.CL

**Keywords:** Token-Aware Phase Attention, positional encoding, attention mechanism, long-context, RoPE

**Relevance Score:** 7

**TL;DR:** This paper introduces Token-Aware Phase Attention (TAPA), a novel positional encoding method that improves attention modeling in long-contexts compared to Rotary Positional Embedding (RoPE).

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of RoPE in modeling long-context due to its intrinsic distance-dependent bias in attention scores.

**Method:** The paper proposes a new approach, TAPA, which integrates a learnable phase function into the attention mechanism to enhance token interactions over long distances.

**Key Contributions:**

	1. Introduction of Token-Aware Phase Attention (TAPA) as a new positional encoding method.
	2. Demonstration of lower perplexity on long-context tasks compared to existing methods.
	3. Ability to extrapolate to unseen input lengths with minimal tuning.

**Result:** TAPA demonstrates improved performance in handling longer contexts, achieving lower perplexity compared to RoPE families and effectively extrapolating to unseen sequence lengths.

**Limitations:** 

**Conclusion:** TAPA provides a more effective method for positional encoding in attention mechanisms, particularly beneficial for long-context applications.

**Abstract:** We prove under practical assumptions that Rotary Positional Embedding (RoPE) introduces an intrinsic distance-dependent bias in attention scores that limits RoPE's ability to model long-context. RoPE extension methods may alleviate this issue, but they typically require post-hoc adjustments after pretraining, such as rescaling or hyperparameters retuning. This paper introduces Token-Aware Phase Attention (TAPA), a new positional encoding method that incorporates a learnable phase function into the attention mechanism. TAPA preserves token interactions over long range, extends to longer contexts with direct and light fine-tuning, extrapolates to unseen lengths, and attains significantly lower perplexity on long-context than RoPE families.

</details>


### [40] [PAC: Pronunciation-Aware Contextualized Large Language Model-based Automatic Speech Recognition](https://arxiv.org/abs/2509.12647)

*Li Fu, Yu Xin, Sunlu Zeng, Lu Fan, Youzheng Wu, Xiaodong He*

**Main category:** cs.CL

**Keywords:** Large Language Model, Automatic Speech Recognition, Pronunciation Modeling, Homophone Discrimination, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** This paper introduces a Pronunciation-Aware Contextualized (PAC) framework that improves pronunciation modeling and homophone discrimination in LLM-based ASR systems, achieving significant reductions in Word Error Rate.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance automatic speech recognition systems by addressing challenges in pronunciation modeling and homophone discrimination, which are critical for recognizing raw or long-tail words.

**Method:** The proposed PAC framework employs a two-stage learning paradigm: first, it utilizes a pronunciation-guided context learning method with an interleaved grapheme-phoneme context modeling strategy; second, it enhances the model's homophone discrimination through a pronunciation-discriminative reinforcement learning approach.

**Key Contributions:**

	1. Pronunciation-guided context learning method
	2. Pronunciation-discriminative reinforcement learning with perturbed label sampling
	3. Substantial reductions in Word Error Rate for long-tail words

**Result:** Experimental results demonstrate that PAC reduces Word Error Rate (WER) by 30.2% and 53.8% compared to pre-trained LLM-based ASR models, and achieves notable reductions in biased WER for long-tail words.

**Limitations:** 

**Conclusion:** The PAC framework significantly improves performance in ASR tasks, particularly for long-tail words, highlighting the importance of pronunciation in such systems.

**Abstract:** This paper presents a Pronunciation-Aware Contextualized (PAC) framework to address two key challenges in Large Language Model (LLM)-based Automatic Speech Recognition (ASR) systems: effective pronunciation modeling and robust homophone discrimination. Both are essential for raw or long-tail word recognition. The proposed approach adopts a two-stage learning paradigm. First, we introduce a pronunciation-guided context learning method. It employs an interleaved grapheme-phoneme context modeling strategy that incorporates grapheme-only distractors, encouraging the model to leverage phonemic cues for accurate recognition. Then, we propose a pronunciation-discriminative reinforcement learning method with perturbed label sampling to further enhance the model\'s ability to distinguish contextualized homophones. Experimental results on the public English Librispeech and Mandarin AISHELL-1 datasets indicate that PAC: (1) reduces relative Word Error Rate (WER) by 30.2% and 53.8% compared to pre-trained LLM-based ASR models, and (2) achieves 31.8% and 60.5% relative reductions in biased WER for long-tail words compared to strong baselines, respectively.

</details>


### [41] [Don't Change My View: Ideological Bias Auditing in Large Language Models](https://arxiv.org/abs/2509.12652)

*Paul Kröger, Emilio Barkett*

**Main category:** cs.CL

**Keywords:** large language models, ideological bias, auditing methods, statistical analysis, public discourse

**Relevance Score:** 8

**TL;DR:** This paper presents a method for detecting ideological steering in large language models by analyzing shifts in model outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the potential influence of large language models on public opinion and the need for methods to detect ideological bias.

**Method:** The authors adapt a statistical method for detecting ideological bias without accessing the internals of the model, focusing on distributional shifts in outputs across related prompts.

**Key Contributions:**

	1. Development of a model-agnostic method for ideological bias auditing in LLMs
	2. Ability to detect ideological steering without needing model internals
	3. Validation of the method through practical experiments

**Result:** The method was validated through experiments, showing that it can effectively identify steering attempts in LLM outputs.

**Limitations:** 

**Conclusion:** The proposed approach is a viable tool for auditing the behavior of LLMs and could assist in preventing ideological bias in public discourse.

**Abstract:** As large language models (LLMs) become increasingly embedded in products used by millions, their outputs may influence individual beliefs and, cumulatively, shape public opinion. If the behavior of LLMs can be intentionally steered toward specific ideological positions, such as political or religious views, then those who control these systems could gain disproportionate influence over public discourse. Although it remains an open question whether LLMs can reliably be guided toward coherent ideological stances and whether such steering can be effectively prevented, a crucial first step is to develop methods for detecting when such steering attempts occur. In this work, we adapt a previously proposed statistical method to the new context of ideological bias auditing. Our approach carries over the model-agnostic design of the original framework, which does not require access to the internals of the language model. Instead, it identifies potential ideological steering by analyzing distributional shifts in model outputs across prompts that are thematically related to a chosen topic. This design makes the method particularly suitable for auditing proprietary black-box systems. We validate our approach through a series of experiments, demonstrating its practical applicability and its potential to support independent post hoc audits of LLM behavior.

</details>


### [42] [Mitigating Strategy Preference Bias in Emotional Support Conversation via Uncertainty Estimations](https://arxiv.org/abs/2509.12661)

*Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Xingjiao Wu, Liang He*

**Main category:** cs.CL

**Keywords:** emotional support conversation, large language models, reinforcement learning, strategy planning, preference bias

**Relevance Score:** 9

**TL;DR:** The paper addresses bias in large language models during emotional support conversations by identifying knowledge boundaries and proposing a reinforcement learning method to optimize strategy planning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of emotional support conversation by addressing low accuracy in strategy planning and preference bias in large language models.

**Method:** The authors identify the knowledge boundaries of LLMs in strategy planning and propose a dual reward function for reinforcement learning to optimize strategy planning by accuracy and entropy-based confidence.

**Key Contributions:**

	1. Identified fundamental causes of bias in LLMs for emotional support dialogue.
	2. Proposed a novel reinforcement learning approach to enhance strategy planning accuracy.
	3. Demonstrated effectiveness through experiments on relevant datasets.

**Result:** Experiments show that the proposed method outperforms baseline models on ESCov and ExTES datasets, confirming its effectiveness in reducing bias.

**Limitations:** The study focuses on specific datasets and may require validation on broader applications of emotional support conversation.

**Conclusion:** The study reveals underlying causes of bias in LLMs and presents an effective way to mitigate it, potentially enhancing emotional support applications.

**Abstract:** Emotional support conversation (ESC) aims to alleviate distress through empathetic dialogue, yet large language models (LLMs) face persistent challenges in delivering effective ESC due to low accuracy in strategy planning. Moreover, there is a considerable preference bias towards specific strategies. Prior methods using fine-tuned strategy planners have shown potential in reducing such bias, while the underlying causes of the preference bias in LLMs have not well been studied. To address these issues, we first reveal the fundamental causes of the bias by identifying the knowledge boundaries of LLMs in strategy planning. Then, we propose an approach to mitigate the bias by reinforcement learning with a dual reward function, which optimizes strategy planning via both accuracy and entropy-based confidence for each region according to the knowledge boundaries. Experiments on the ESCov and ExTES datasets with multiple LLM backbones show that our approach outperforms the baselines, confirming the effectiveness of our approach.

</details>


### [43] [Chat-Driven Text Generation and Interaction for Person Retrieval](https://arxiv.org/abs/2509.12662)

*Zequn Xie, Chuxin Wang, Sihang Cai, Yeqiang Wang, Shulei Wang, Tao Jin*

**Main category:** cs.CL

**Keywords:** Text-based person search, Natural language processing, Multi-Turn Text Generation, MLLMs

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework for Text-based person search (TBPS) that uses Multi-Turn Text Generation and Interaction to improve retrieval accuracy and usability without manual annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for high-quality textual annotations in TBPS restricts scalability and practical deployment, particularly in surveillance applications.

**Method:** The proposed framework includes two modules: Multi-Turn Text Generation (MTG) which creates pseudo-labels through dialogues with MLLMs, and Multi-Turn Text Interaction (MTI) that refines user queries during inference, addressing vague descriptions.

**Key Contributions:**

	1. Introduced a method to generate pseudo-labels without manual supervision.
	2. Developed a dialogue-based query refinement process for improved user interaction.
	3. Created an annotation-free framework that enhances TBPS usability and accuracy.

**Result:** The framework significantly enhances retrieval accuracy and usability, achieving competitive results while eliminating the dependency on manual captions.

**Limitations:** 

**Conclusion:** The unified and annotation-free approach set forth demonstrates potential for scalable and practical deployment of TBPS systems.

**Abstract:** Text-based person search (TBPS) enables the retrieval of person images from large-scale databases using natural language descriptions, offering critical value in surveillance applications. However, a major challenge lies in the labor-intensive process of obtaining high-quality textual annotations, which limits scalability and practical deployment. To address this, we introduce two complementary modules: Multi-Turn Text Generation (MTG) and Multi-Turn Text Interaction (MTI). MTG generates rich pseudo-labels through simulated dialogues with MLLMs, producing fine-grained and diverse visual descriptions without manual supervision. MTI refines user queries at inference time through dynamic, dialogue-based reasoning, enabling the system to interpret and resolve vague, incomplete, or ambiguous descriptions - characteristics often seen in real-world search scenarios. Together, MTG and MTI form a unified and annotation-free framework that significantly improves retrieval accuracy, robustness, and usability. Extensive evaluations demonstrate that our method achieves competitive or superior results while eliminating the need for manual captions, paving the way for scalable and practical deployment of TBPS systems.

</details>


### [44] [Towards Inclusive Toxic Content Moderation: Addressing Vulnerabilities to Adversarial Attacks in Toxicity Classifiers Tackling LLM-generated Content](https://arxiv.org/abs/2509.12672)

*Shaz Furniturewala, Arkaitz Zubiaga*

**Main category:** cs.CL

**Keywords:** Large Language Models, Content Moderation, Adversarial Attacks, Toxicity Detection, Machine Learning

**Relevance Score:** 8

**TL;DR:** This study addresses the challenges of content moderation systems in detecting LLM-generated text by identifying and suppressing vulnerable components of toxicity classifiers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increase of machine-generated content has led to difficulties in conventional content moderation classifiers, necessitating a proactive approach to improve detection accuracy.

**Method:** We apply mechanistic interpretability techniques to fine-tuned BERT and RoBERTa classifiers, using adversarial techniques to identify and suppress vulnerable circuits responsible for misclassification.

**Key Contributions:**

	1. Identification of vulnerable components in toxicity classifiers
	2. Improvement of classifier performance against adversarial attacks
	3. Insights into demographic-specific vulnerabilities in machine learning models

**Result:** The suppression of vulnerable circuits improved performance against adversarial attacks and highlighted demographic-level insights about vulnerabilities in toxicity detection models.

**Limitations:** 

**Conclusion:** Our findings reveal distinct model heads that impact performance and vulnerability, advocating for inclusive development in toxicity detection.

**Abstract:** The volume of machine-generated content online has grown dramatically due to the widespread use of Large Language Models (LLMs), leading to new challenges for content moderation systems. Conventional content moderation classifiers, which are usually trained on text produced by humans, suffer from misclassifications due to LLM-generated text deviating from their training data and adversarial attacks that aim to avoid detection. Present-day defence tactics are reactive rather than proactive, since they rely on adversarial training or external detection models to identify attacks. In this work, we aim to identify the vulnerable components of toxicity classifiers that contribute to misclassification, proposing a novel strategy based on mechanistic interpretability techniques. Our study focuses on fine-tuned BERT and RoBERTa classifiers, testing on diverse datasets spanning a variety of minority groups. We use adversarial attacking techniques to identify vulnerable circuits. Finally, we suppress these vulnerable circuits, improving performance against adversarial attacks. We also provide demographic-level insights into these vulnerable circuits, exposing fairness and robustness gaps in model training. We find that models have distinct heads that are either crucial for performance or vulnerable to attack and suppressing the vulnerable heads improves performance on adversarial input. We also find that different heads are responsible for vulnerability across different demographic groups, which can inform more inclusive development of toxicity detection models.

</details>


### [45] [Case-Based Decision-Theoretic Decoding with Quality Memories](https://arxiv.org/abs/2509.12677)

*Hiroyuki Deguchi, Masaaki Nagata*

**Main category:** cs.CL

**Keywords:** Minimum Bayes risk, case-based decision-theoretic, text generation, machine translation, image captioning

**Relevance Score:** 9

**TL;DR:** The paper introduces case-based decision-theoretic (CBDT) decoding as a method for text generation that improves over traditional methods by utilizing domain data examples.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of text generation decodings that often rely on in-domain samples, which can limit performance when encountering out-of-domain data.

**Method:** The method proposed combines traditional MBR decoding with CBDT decoding to leverage examples from domain data for improved expected utility estimations in text generation.

**Key Contributions:**

	1. Introduction of CBDT decoding for text generation
	2. Demonstrated superior performance over MBR and MAP decoding
	3. Validation on translation and image captioning tasks with improved results

**Result:** CBDT decoding generates higher-quality texts than MAP decoding and outperforms MBR decoding in various language translation and image captioning tasks.

**Limitations:** 

**Conclusion:** The CBDT decoding method enhances text generation quality by effectively utilizing domain-specific examples, proving better than existing methods across several benchmarks.

**Abstract:** Minimum Bayes risk (MBR) decoding is a decision rule of text generation, which selects the hypothesis that maximizes the expected utility and robustly generates higher-quality texts than maximum a posteriori (MAP) decoding. However, it depends on sample texts drawn from the text generation model; thus, it is difficult to find a hypothesis that correctly captures the knowledge or information of out-of-domain. To tackle this issue, we propose case-based decision-theoretic (CBDT) decoding, another method to estimate the expected utility using examples of domain data. CBDT decoding not only generates higher-quality texts than MAP decoding, but also the combination of MBR and CBDT decoding outperformed MBR decoding in seven domain De--En and Ja$\leftrightarrow$En translation tasks and image captioning tasks on MSCOCO and nocaps datasets.

</details>


### [46] [HistoryBankQA: Multilingual Temporal Question Answering on Historical Events](https://arxiv.org/abs/2509.12720)

*Biswadip Mandal, Anant Khandelwal, Manish Gupta*

**Main category:** cs.CL

**Keywords:** temporal reasoning, historical events, multilingual database

**Relevance Score:** 8

**TL;DR:** HistoryBank is a multilingual database of over 10 million historical events aimed at improving temporal reasoning benchmarks for language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited benchmarking and dataset size for temporal reasoning in NLP, especially in the context of historical events.

**Method:** Creation of a multilingual database with 10+ million historical events and a comprehensive question answering benchmark for temporal reasoning across six tasks.

**Key Contributions:**

	1. Introduction of HistoryBank with 10M+ historical events
	2. Multilingual coverage in 10 languages
	3. Development of a comprehensive temporal QA benchmark

**Result:** Evaluation of popular language models; GPT4o shows the best performance across all tasks and languages, while Gemma-2 outperformed other small models.

**Limitations:** 

**Conclusion:** The work provides a significant resource for enhancing multilingual and temporally-aware natural language understanding and will be publicly available for further research.

**Abstract:** Temporal reasoning about historical events is a critical skill for NLP tasks like event extraction, historical entity linking, temporal question answering, timeline summarization, temporal event clustering and temporal natural language inference. Yet efforts on benchmarking temporal reasoning capabilities of large language models (LLMs) are rather limited. Existing temporal reasoning datasets are limited in scale, lack multilingual coverage and focus more on contemporary events. To address these limitations, we present HistoryBank, a multilingual database of 10M+ historical events extracted from Wikipedia timeline pages and article infoboxes. Our database provides unprecedented coverage in both historical depth and linguistic breadth with 10 languages. Additionally, we construct a comprehensive question answering benchmark for temporal reasoning across all languages. This benchmark covers a diverse set of 6 temporal QA reasoning tasks, and we evaluate a suite of popular language models (LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess their performance on these tasks. As expected GPT4o performs best across all answer types and languages; Gemma-2 outperforms the other small language models. Our work aims to provide a comprehensive resource for advancing multilingual and temporally-aware natural language understanding of historical events. To facilitate further research, we will make our code and datasets publicly available upon acceptance of this paper.

</details>


### [47] [Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision](https://arxiv.org/abs/2509.12771)

*Omri Suissa, Muhiim Ali, Shengmai Chen, Yinuo Cai, Shekhar Pradhan*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, concept abstraction, contrastive loss

**Relevance Score:** 7

**TL;DR:** This paper investigates the capacity of Vision-Language Models (VLMs) for concept abstraction and introduces a novel training methodology using a grouped contrastive loss function to enhance this capability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well VLMs can abstract general concepts beyond mere object identification and to improve these models' capacities for concept recognition.

**Method:** The authors present the CLEAR GLASS model trained with a grouped contrastive loss function on a newly created dataset (MAGIC) that includes grouped image-caption pairs and associated higher-level concepts.

**Key Contributions:**

	1. Introduction of a grouped contrastive loss function for VLMs
	2. Creation of the MAGIC dataset for training VLMs on concept abstraction
	3. Demonstration of improved abstract concept recognition in the CLEAR GLASS model

**Result:** The CLEAR GLASS model demonstrates enhanced ability to recognize abstract concepts, outperforming state-of-the-art models by effectively encoding information shared across grouped image-caption instances.

**Limitations:** 

**Conclusion:** By using a training approach that emphasizes the semantic relationships within image-caption groups, the model achieves emergent capacity in concept abstraction without prior exposure to the higher-level concepts.

**Abstract:** Humans can recognize an image as an instance of a general concept, beyond simply identifying its objects and their relationships. In this paper, we investigate 1. The extent to which VLMs have this concept abstraction capacity, and 2. Strategies for encoding the sort of higher-concept information in images that would enable the resulting VLM model (CLEAR GLASS model) to have this capability to a greater degree. To this end, we introduce a grouped image-caption dataset (MAGIC), which consists of several groups of image captions and for each group a set of associated images and higher-level conceptual labels. We use a novel contrastive loss technique to induce the model to encode in the representation of each image (caption) in a group the information that is common to all members of the image-caption group. Our main contribution is a grouped contrastive loss function based on text-image contrastive groups (outer contrastive loss) as well as an inner loss which measures the distances between image-caption instances in the group. Our training methodology results in the CLEAR GLASS model having the concept abstraction capacity as an emergent capacity because the model is not exposed to the higher-level concepts associated with each group. Instead, the training forces the model to create for each image-caption group a semantic representation that brings it closer to the semantic representation of the higher-level concepts in the latent semantic space. Our experiments show that this training methodology results in a model which shows improvement in abstract concept recognition compared to SOTA models.

</details>


### [48] [ConvergeWriter: Data-Driven Bottom-Up Article Construction](https://arxiv.org/abs/2509.12811)

*Binquan Ji, Jiaqi Wang, Ruiting Li, Xingchen Han, Yiyang Qi, Shichao Wang, Yifei Lu, Yuantao Han, Feiliang Ren*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Retrieval, Document Generation, Clustering, Factual Accuracy

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel 'bottom-up' framework for generating long-form documents using Large Language Models (LLMs), improving reliability by reversing conventional methods through iterative knowledge retrieval and unsupervised clustering.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM methods struggle with long-form generation due to disconnects between planning and available knowledge, leading to inaccuracies and fragmentation.

**Method:** The proposed approach employs a 'Retrieval-First for Knowledge, Clustering for Structure' strategy, where extensive retrieval from a knowledge base is performed first, followed by clustering of the retrieved documents into knowledge clusters to guide the generation process.

**Key Contributions:**

	1. Introduction of a 'bottom-up' generation framework for LLMs.
	2. Utilization of unsupervised clustering to guide document structure from retrieved knowledge.
	3. Demonstration of improved performance in knowledge-constrained scenarios.

**Result:** Experiments show that this method achieves performance comparable to or better than state-of-the-art baselines with unique advantages in scenarios requiring high accuracy and coherence.

**Limitations:** 

**Conclusion:** This novel paradigm enhances the generation of reliable, structured documents, fostering better applications of LLMs in knowledge-intensive fields.

**Abstract:** Large Language Models (LLMs) have shown remarkable prowess in text generation, yet producing long-form, factual documents grounded in extensive external knowledge bases remains a significant challenge. Existing "top-down" methods, which first generate a hypothesis or outline and then retrieve evidence, often suffer from a disconnect between the model's plan and the available knowledge, leading to content fragmentation and factual inaccuracies. To address these limitations, we propose a novel "bottom-up," data-driven framework that inverts the conventional generation pipeline. Our approach is predicated on a "Retrieval-First for Knowledge, Clustering for Structure" strategy, which first establishes the "knowledge boundaries" of the source corpus before any generative planning occurs. Specifically, we perform exhaustive iterative retrieval from the knowledge base and then employ an unsupervised clustering algorithm to organize the retrieved documents into distinct "knowledge clusters." These clusters form an objective, data-driven foundation that directly guides the subsequent generation of a hierarchical outline and the final document content. This bottom-up process ensures that the generated text is strictly constrained by and fully traceable to the source material, proactively adapting to the finite scope of the knowledge base and fundamentally mitigating the risk of hallucination. Experimental results on both 14B and 32B parameter models demonstrate that our method achieves performance comparable to or exceeding state-of-the-art baselines, and is expected to demonstrate unique advantages in knowledge-constrained scenarios that demand high fidelity and structural coherence. Our work presents an effective paradigm for generating reliable, structured, long-form documents, paving the way for more robust LLM applications in high-stakes, knowledge-intensive domains.

</details>


### [49] [Data Augmentation for Maltese NLP using Transliterated and Machine Translated Arabic Data](https://arxiv.org/abs/2509.12853)

*Kurt Micallef, Nizar Habash, Claudia Borg*

**Main category:** cs.CL

**Keywords:** Maltese, Arabic, Natural Language Processing, Cross-lingual Augmentation, Transliteration

**Relevance Score:** 4

**TL;DR:** The paper investigates the potential of Arabic-language resources for enhancing Maltese natural language processing (NLP) through cross-lingual augmentation techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a linguistic gap between Maltese, a Semitic language, and its linguistic relatives in Arabic, prompting the exploration of resources and techniques from Arabic to improve Maltese NLP.

**Method:** The study employs various strategies for aligning Arabic textual data with Maltese, including transliteration schemes and machine translation approaches, while introducing novel transliteration systems for Maltese orthography.

**Key Contributions:**

	1. Exploration of cross-lingual augmentation techniques for Maltese NLP using Arabic resources.
	2. Introduction of novel transliteration systems for Maltese orthography.
	3. Evaluation of the impact of Arabic-based augmentation on Maltese NLP tasks.

**Result:** The evaluation demonstrates that Arabic-based augmentation significantly benefits Maltese NLP tasks across monolingual and multilingual models.

**Limitations:** 

**Conclusion:** Using Arabic resources and innovative transliteration methods can enhance the performance of Maltese NLP, bridging linguistic and resource gaps between these languages.

**Abstract:** Maltese is a unique Semitic language that has evolved under extensive influence from Romance and Germanic languages, particularly Italian and English. Despite its Semitic roots, its orthography is based on the Latin script, creating a gap between it and its closest linguistic relatives in Arabic. In this paper, we explore whether Arabic-language resources can support Maltese natural language processing (NLP) through cross-lingual augmentation techniques. We investigate multiple strategies for aligning Arabic textual data with Maltese, including various transliteration schemes and machine translation (MT) approaches. As part of this, we also introduce novel transliteration systems that better represent Maltese orthography. We evaluate the impact of these augmentations on monolingual and mutlilingual models and demonstrate that Arabic-based augmentation can significantly benefit Maltese NLP tasks.

</details>


### [50] [Benchmarking and Improving LVLMs on Event Extraction from Multimedia Documents](https://arxiv.org/abs/2509.12876)

*Fuyu Xing, Zimu Wang, Wei Wang, Haiyang Zhang*

**Main category:** cs.CL

**Keywords:** Multimedia Event Extraction, Large Vision-Language Models, cross-modal performance

**Relevance Score:** 5

**TL;DR:** The paper systematically evaluates Large Vision-Language Models (LVLMs) on Multimedia Event Extraction (M2E2) tasks, revealing strengths in cross-modal performance and areas for improvement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of Large Vision-Language Models (LVLMs) in Multimedia Event Extraction (M2E2) tasks and identify their strengths and weaknesses.

**Method:** The authors conducted a systematic evaluation of various LVLMs, including DeepSeek-VL2 and Qwen-VL, on the M2E2 dataset, through different subtasks and settings: text-only, image-only, and cross-media, assessed via few-shot prompting and fine-tuning.

**Key Contributions:**

	1. First systematic evaluation of LVLMs on M2E2 tasks
	2. Insights into the performance of few-shot and fine-tuned LVLMs
	3. Identification of critical challenges in M2E2 capabilities.

**Result:** Few-shot LVLMs show better performance on visual tasks but struggle with textual tasks; fine-tuning with LoRA improves performance, and LVLMs demonstrate strong synergy in cross-modal tasks.

**Limitations:** 

**Conclusion:** The findings highlight specific areas for improvement in LVLMs for M2E2 tasks, particularly regarding semantic precision, localization, and cross-modal grounding.

**Abstract:** The proliferation of multimedia content necessitates the development of effective Multimedia Event Extraction (M2E2) systems. Though Large Vision-Language Models (LVLMs) have shown strong cross-modal capabilities, their utility in the M2E2 task remains underexplored. In this paper, we present the first systematic evaluation of representative LVLMs, including DeepSeek-VL2 and the Qwen-VL series, on the M2E2 dataset. Our evaluations cover text-only, image-only, and cross-media subtasks, assessed under both few-shot prompting and fine-tuning settings. Our key findings highlight the following valuable insights: (1) Few-shot LVLMs perform notably better on visual tasks but struggle significantly with textual tasks; (2) Fine-tuning LVLMs with LoRA substantially enhances model performance; and (3) LVLMs exhibit strong synergy when combining modalities, achieving superior performance in cross-modal settings. We further provide a detailed error analysis to reveal persistent challenges in areas such as semantic precision, localization, and cross-modal grounding, which remain critical obstacles for advancing M2E2 capabilities.

</details>


### [51] [The LLM Already Knows: Estimating LLM-Perceived Question Difficulty via Hidden Representations](https://arxiv.org/abs/2509.12886)

*Yubo Zhu, Dongrui Liu, Zecheng Lin, Wei Tong, Sheng Zhong, Jing Shao*

**Main category:** cs.CL

**Keywords:** difficulty estimation, large language models, adaptive reasoning, Markov chain, hidden representations

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel method for estimating the difficulty of questions for large language models using only their hidden representations, improving efficiency and accuracy over previous methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate difficulty estimation of input questions is crucial for the performance evaluation and adaptive inference of large language models, yet existing methods are computationally expensive or lack generality.

**Method:** The authors propose modeling the token-level generation process as a Markov chain, defining a value function to estimate expected output quality from the hidden states of the LLM, without generating tokens.

**Key Contributions:**

	1. Development of a Markov chain model for difficulty estimation using hidden states
	2. Improved efficiency in estimating difficulty without output generation
	3. Demonstration of enhanced adaptive reasoning strategies through effective difficulty estimates

**Result:** The proposed method consistently outperforms existing baselines in estimating difficulty across various tasks and contexts, leading to gains in inference efficiency when applied to adaptive reasoning strategies.

**Limitations:** 

**Conclusion:** The approach enhances performance in both textual and multimodal contexts, validating its utility in guiding adaptive reasoning strategies with fewer tokens generated.

**Abstract:** Estimating the difficulty of input questions as perceived by large language models (LLMs) is essential for accurate performance evaluation and adaptive inference. Existing methods typically rely on repeated response sampling, auxiliary models, or fine-tuning the target model itself, which may incur substantial computational costs or compromise generality. In this paper, we propose a novel approach for difficulty estimation that leverages only the hidden representations produced by the target LLM. We model the token-level generation process as a Markov chain and define a value function to estimate the expected output quality given any hidden state. This allows for efficient and accurate difficulty estimation based solely on the initial hidden state, without generating any output tokens. Extensive experiments across both textual and multimodal tasks demonstrate that our method consistently outperforms existing baselines in difficulty estimation. Moreover, we apply our difficulty estimates to guide adaptive reasoning strategies, including Self-Consistency, Best-of-N, and Self-Refine, achieving higher inference efficiency with fewer generated tokens.

</details>


### [52] [Conan-Embedding-v2: Training an LLM from Scratch for Text Embeddings](https://arxiv.org/abs/2509.12892)

*Shiyu Li, Yang Tang, Ruijie Liu, Shi-Zhe Chen, Xi Chen*

**Main category:** cs.CL

**Keywords:** large language models, text embeddings, cross-lingual retrieval, soft-masking mechanism, dynamic hard negative mining

**Relevance Score:** 8

**TL;DR:** Introduction of Conan-embedding-v2, a novel LLM trained to improve text embedding performance by addressing data and training gaps.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of text embeddings by bridging the data and training gaps seen in current LLMs and traditional embedding models.

**Method:** Conan-embedding-v2, a 1.4B-parameter model, was trained from scratch with additional news data and multilingual pairs. A soft-masking mechanism was introduced to transition between causal and bidirectional masks, alongside a dynamic hard negative mining method to enhance training.

**Key Contributions:**

	1. Introduction of Conan-embedding-v2 for text embedding tasks
	2. Implementation of a soft-masking mechanism for training transitions
	3. Development of a dynamic hard negative mining method to improve learning

**Result:** Conan-embedding-v2 achieves state-of-the-art performance on the Massive Text Embedding Benchmark and Chinese MTEB with only 1.4B parameters.

**Limitations:** 

**Conclusion:** The innovative training strategies implemented in Conan-embedding-v2 significantly enhance text embedding quality across languages, providing a viable alternative to traditional methods.

**Abstract:** Large language models (LLMs) have recently demonstrated excellent performance in text embedding tasks. Previous work usually use LoRA to fine-tune existing LLMs, which are limited by the data and training gap between LLMs and embedding models. In this work, we introduce Conan-embedding-v2, a new 1.4B-parameter LLM trained from scratch and fine-tuned as a text embedder. First, we add news data and multilingual pairs for LLM pretraining to bridge the data gap. Based on this, we propose a cross-lingual retrieval dataset that enables the LLM to better integrate embeddings across different languages. Second, whereas LLMs use a causal mask with token-level loss, embedding models use a bidirectional mask with sentence-level loss. This training gap makes full fine-tuning less effective than LoRA. We introduce a soft-masking mechanism to gradually transition between these two types of masks, enabling the model to learn more comprehensive representations. Based on this, we propose a dynamic hard negative mining method that exposes the model to more difficult negative examples throughout the training process. Being intuitive and effective, with only approximately 1.4B parameters, Conan-embedding-v2 achieves SOTA performance on both the Massive Text Embedding Benchmark (MTEB) and Chinese MTEB (May 19, 2025).

</details>


### [53] [All Roads Lead to Rome: Graph-Based Confidence Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2509.12908)

*Caiqi Zhang, Chang Shu, Ehsan Shareghi, Nigel Collier*

**Main category:** cs.CL

**Keywords:** confidence estimation, large language models, reasoning tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces graph-based methods for confidence estimation in reasoning tasks using large language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing confidence estimation methods for LLMs do not generalize well to reasoning tasks, highlighting the need for improved methods.

**Method:** The proposed approach uses directed graphs to model reasoning paths and estimates confidence leveraging properties like centrality and path convergence.

**Key Contributions:**

	1. Introduces graph-based confidence estimation for reasoning tasks
	2. Utilizes properties of graphs to enhance estimation methods
	3. Demonstrates improved performance on reasoning datasets

**Result:** Experiments show that the graph-based methods provide improved confidence estimation and better performance in downstream tasks compared to existing methods.

**Limitations:** 

**Conclusion:** The new training-free confidence estimation methods significantly enhance the reliability of LLMs in reasoning contexts.

**Abstract:** Confidence estimation is essential for the reliable deployment of large language models (LLMs). Existing methods are primarily designed for factual QA tasks and often fail to generalize to reasoning tasks. To address this gap, we propose a set of training-free, graph-based confidence estimation methods tailored to reasoning tasks. Our approach models reasoning paths as directed graphs and estimates confidence by exploiting graph properties such as centrality, path convergence, and path weighting. Experiments with two LLMs on three reasoning datasets demonstrate improved confidence estimation and enhanced performance on two downstream tasks.

</details>


### [54] [Automated Generation of Research Workflows from Academic Papers: A Full-text Mining Framework](https://arxiv.org/abs/2509.12955)

*Heng Zhang, Chengzhi Zhang*

**Main category:** cs.CL

**Keywords:** research workflows, Natural Language Processing, automated generation, Positive-Unlabeled Learning, workflow visualization

**Relevance Score:** 8

**TL;DR:** This paper presents a framework for generating structured research workflows by mining full-text academic papers, particularly in the NLP domain, using advanced machine learning techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for automated generation of research workflows to improve reproducibility and accelerate 'AI for Science' initiatives.

**Method:** An end-to-end framework that employs Positive-Unlabeled Learning with SciBERT for identifying workflow-descriptive paragraphs, followed by Flan-T5 for generating workflow phrases, and ChatGPT for categorizing these phrases into workflow stages.

**Key Contributions:**

	1. Development of a comprehensive framework for workflow generation from academic papers.
	2. Use of advanced NLP models like SciBERT and Flan-T5 for efficient extraction and categorization of research processes.
	3. Presentation of methodological shifts in NLP research through visual flowcharts.

**Result:** Achieved an F1-score of 0.9772 for paragraph identification and precision of 0.958 in workflow categorization, with flowcharts representing entire research workflows.

**Limitations:** 

**Conclusion:** The proposed framework not only automates workflow generation but also provides insights into methodological shifts in NLP research over two decades.

**Abstract:** The automated generation of research workflows is essential for improving the reproducibility of research and accelerating the paradigm of "AI for Science". However, existing methods typically extract merely fragmented procedural components and thus fail to capture complete research workflows. To address this gap, we propose an end-to-end framework that generates comprehensive, structured research workflows by mining full-text academic papers. As a case study in the Natural Language Processing (NLP) domain, our paragraph-centric approach first employs Positive-Unlabeled (PU) Learning with SciBERT to identify workflow-descriptive paragraphs, achieving an F1-score of 0.9772. Subsequently, we utilize Flan-T5 with prompt learning to generate workflow phrases from these paragraphs, yielding ROUGE-1, ROUGE-2, and ROUGE-L scores of 0.4543, 0.2877, and 0.4427, respectively. These phrases are then systematically categorized into data preparation, data processing, and data analysis stages using ChatGPT with few-shot learning, achieving a classification precision of 0.958. By mapping categorized phrases to their document locations in the documents, we finally generate readable visual flowcharts of the entire research workflows. This approach facilitates the analysis of workflows derived from an NLP corpus and reveals key methodological shifts over the past two decades, including the increasing emphasis on data analysis and the transition from feature engineering to ablation studies. Our work offers a validated technical framework for automated workflow generation, along with a novel, process-oriented perspective for the empirical investigation of evolving scientific paradigms. Source code and data are available at: https://github.com/ZH-heng/research_workflow.

</details>


### [55] [Investigating ReLoRA: Effects on the Learning Dynamics of Small Language Models](https://arxiv.org/abs/2509.12960)

*Yuval Weiss, David Demitri Africa, Paula Buttery, Richard Diehl Martinez*

**Main category:** cs.CL

**Keywords:** ReLoRA, small language models, pretraining, low-rank updates, learning dynamics

**Relevance Score:** 8

**TL;DR:** This paper systematically studies ReLoRA's application in small language models for pretraining, revealing its performance drawbacks compared to standard training.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of ReLoRA in pretraining small language models, especially in terms of computational efficiency and learning dynamics.

**Method:** A systematic evaluation involving ablation experiments across small language models with 11M to 66M parameters was conducted to assess performance on various metrics against standard training.

**Key Contributions:**

	1. First systematic study of ReLoRA in small language models
	2. Identification of performance issues in ReLoRA compared to standard methods
	3. Insights into the learning dynamics affected by ReLoRA in SLMs

**Result:** ReLoRA generally performed worse than standard training across multiple performance measures, with a widening gap in larger models and reinforcement of rank deficiencies.

**Limitations:** Results show that ReLoRA underperforms and reinforces deficiencies in smaller models, suggesting limitations in its transferability to pretraining.

**Conclusion:** Low-rank update strategies may not be suitable for SLM pretraining, necessitating further research in this area.

**Abstract:** Parameter-efficient methods such as LoRA have revolutionised the fine-tuning of LLMs. Still, their extension to pretraining via ReLoRA is less well understood, especially for small language models (SLMs), which offer lower computational and environmental costs. This work is the first systematic study of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and learning dynamics. Through ablation experiments, we find that ReLoRA generally performs worse than standard training on loss, Paloma perplexity and BLiMP, with the gap widening for the larger models. Further analysis of the learning dynamics of the models indicates that ReLoRA reinforces the rank deficiencies found in smaller models. These results indicate that low-rank update strategies may not transfer easily to SLM pretraining, highlighting the need for more research in the low-compute regime.

</details>


### [56] [Do LLMs Understand Wine Descriptors Across Cultures? A Benchmark for Cultural Adaptations of Wine Reviews](https://arxiv.org/abs/2509.12961)

*Chenye Zou, Xingyue Wen, Tianyi Hu, Qian Janice Wang, Daniel Hershcovich*

**Main category:** cs.CL

**Keywords:** Cultural Proximity, Wine Reviews, Neural Machine Translation

**Relevance Score:** 8

**TL;DR:** This paper addresses the challenge of adapting wine reviews between Chinese and English by incorporating culture-specific flavors and regional preferences, showcasing limitations of current translation models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the limitations of existing translation models in capturing cultural nuances in wine reviews between different languages.

**Method:** The authors compile a parallel corpus of 8k Chinese and 16k English wine reviews and benchmark translation models against cultural metrics.

**Key Contributions:**

	1. Introduction of a novel problem of culture-aware language adaptation for wine reviews
	2. Creation of the first parallel corpus for Chinese and English wine reviews
	3. Proposal of culture-oriented assessment criteria for translation quality.

**Result:** Automated and human evaluations reveal that current models inadequately address cultural aspects, struggling particularly with wine descriptions.

**Limitations:** Current models do not perform well in capturing cultural nuances in translations, particularly for flavor descriptions.

**Conclusion:** The findings underscore the necessity for improved models that can effectively translate cultural content, particularly in niche areas like wine reviews.

**Abstract:** Recent advances in large language models (LLMs) have opened the door to culture-aware language tasks. We introduce the novel problem of adapting wine reviews across Chinese and English, which goes beyond literal translation by incorporating regional taste preferences and culture-specific flavor descriptors. In a case study on cross-cultural wine review adaptation, we compile the first parallel corpus of professional reviews, containing 8k Chinese and 16k Anglophone reviews. We benchmark both neural-machine-translation baselines and state-of-the-art LLMs with automatic metrics and human evaluation. For the latter, we propose three culture-oriented criteria -- Cultural Proximity, Cultural Neutrality, and Cultural Genuineness -- to assess how naturally a translated review resonates with target-culture readers. Our analysis shows that current models struggle to capture cultural nuances, especially in translating wine descriptions across different cultures. This highlights the challenges and limitations of translation models in handling cultural content.

</details>


### [57] [SitLLM: Large Language Models for Sitting Posture Health Understanding via Pressure Sensor Data](https://arxiv.org/abs/2509.12994)

*Jian Gao, Fufangchen Zhao, Yiyang Zhang, Danfeng Yan*

**Main category:** cs.CL

**Keywords:** Sitting Posture, Multimodal Framework, Large Language Models, Health Informatics, Pressure Sensing

**Relevance Score:** 9

**TL;DR:** SitLLM addresses poor sitting posture through a multimodal framework integrating pressure sensing and large language models for personalized health feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing sitting posture monitoring systems that suffer from coarse-grained recognition and lack of semantic expressiveness.

**Method:** SitLLM combines flexible pressure sensing with LLMs, featuring three components: a Gaussian-Robust Sensor Embedding Module for robust feature extraction, a Prompt-Driven Cross-Modal Alignment Module for embedding transformation, and a Multi-Context Prompt Module for contextual information fusion.

**Key Contributions:**

	1. Lightweight multimodal framework for posture monitoring.
	2. Integration of pressure sensing with large language models for real-time feedback.
	3. Contextual information fusion for improved understanding and response generation.

**Result:** SitLLM enables fine-grained posture understanding and personalized response generation, enhancing health-oriented feedback in real-time.

**Limitations:** 

**Conclusion:** The proposed framework demonstrates potential in improving personalized health responses related to sitting posture through effective integration of pressure sensing and language models.

**Abstract:** Poor sitting posture is a critical yet often overlooked factor contributing to long-term musculoskeletal disorders and physiological dysfunctions. Existing sitting posture monitoring systems, although leveraging visual, IMU, or pressure-based modalities, often suffer from coarse-grained recognition and lack the semantic expressiveness necessary for personalized feedback. In this paper, we propose \textbf{SitLLM}, a lightweight multimodal framework that integrates flexible pressure sensing with large language models (LLMs) to enable fine-grained posture understanding and personalized health-oriented response generation. SitLLM comprises three key components: (1) a \textit{Gaussian-Robust Sensor Embedding Module} that partitions pressure maps into spatial patches and injects local noise perturbations for robust feature extraction; (2) a \textit{Prompt-Driven Cross-Modal Alignment Module} that reprograms sensor embeddings into the LLM's semantic space via multi-head cross-attention using the pre-trained vocabulary embeddings; and (3) a \textit{Multi-Context Prompt Module} that fuses feature-level, structure-level, statistical-level, and semantic-level contextual information to guide instruction comprehension.

</details>


### [58] [Multi-Model Synthetic Training for Mission-Critical Small Language Models](https://arxiv.org/abs/2509.13047)

*Nolan Platt, Pragyansmita Nayak*

**Main category:** cs.CL

**Keywords:** Large Language Models, synthetic dataset generation, maritime intelligence

**Relevance Score:** 7

**TL;DR:** This paper introduces a cost-effective method for applying LLMs to maritime intelligence by transforming AIS vessel tracking data into synthetic Q&A pairs, achieving significant accuracy with smaller models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of applying LLMs in specialized fields due to the scarcity and complexity of domain-specific training data.

**Method:** The approach utilizes LLMs as one-time teachers to convert 3.2 billion AIS records into 21,543 synthetic question and answer pairs, employing multi-model generation techniques to ensure accurate reasoning and prevent overfitting.

**Key Contributions:**

	1. Novel approach to synthetic dataset generation for LLMs in specialized fields.
	2. Demonstration of significant cost reduction in model training for maritime intelligence applications.
	3. Framework presented is highly reproducible for other domains with similar challenges.

**Result:** The fine-tuned Qwen2.5-7B model achieved 75% accuracy on maritime tasks while being 261x cheaper than using larger models for direct inference.

**Limitations:** 

**Conclusion:** Smaller, properly fine-tuned models can achieve near-equivalent accuracy to larger models at a fraction of the cost, with important implications for specialized AI applications in maritime sectors.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across many domains, yet their appli- cation to specialized fields remains constrained by the scarcity and complexity of domain-specific training data. We present a novel approach that achieves a 261x cost reduction for maritime intelligence by using LLMs as one-time teachers rather than using them directly for inference. Our method transforms 3.2 billion Automatic Identification System (AIS) vessel tracking records into 21,543 synthetic question and answer pairs through multi-model generation (GPT-4o and o3-mini), preventing over- fitting and ensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves 75% accuracy on maritime tasks, while being substantially cheaper than using a larger model for inference. We show that smaller, cheaper models - when fine tuned properly - can provide similar accuracy compared to larger models that are prohibitively expensive. Our work contributes to the growing field of synthetic dataset generation for specialized AI applications and presents a highly reproducible framework for domains where manual annotation is infeasible. Beyond expand- ing research in the growing field of specialized small language models, our approach has immediate applications in maritime safety, security operations, and vessel traffic management systems in various industries.

</details>


### [59] [Shaping Explanations: Semantic Reward Modeling with Encoder-Only Transformers for GRPO](https://arxiv.org/abs/2509.13081)

*Francesco Pappone, Ruggero Marino Lazzaroni, Federico Califano, Niccolò Gentile, Roberto Marras*

**Main category:** cs.CL

**Keywords:** Large Language Models, reward shaping, semantic similarity, Group Relative Policy Optimisation, medical education

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel approach for improving Large Language Models' (LLMs) text generation quality by using a lightweight transformer as a semantic reward model within the Group Relative Policy Optimisation (GRPO) framework, specifically applied to generating explanations for medical entrance exams.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning LLM outputs with qualitative goals like pedagogical soundness is challenging due to limitations in existing evaluation metrics.

**Method:** The authors propose a reward shaping approach using a small encoder-only transformer that provides a dense semantic reward signal based on cosine similarity between generated explanations and ground-truth references.

**Key Contributions:**

	1. Introduction of a semantic reward model within the GRPO framework.
	2. Demonstration of improved explanation clarity and faithfulness using a lightweight transformer.
	3. Application of this method to medical educational contexts.

**Result:** The approach significantly improves explanation faithfulness and clarity in generating medical-school entrance exam responses compared to a strong baseline.

**Limitations:** 

**Conclusion:** The use of lightweight encoder models for nuanced reward shaping shows promise in enhancing the quality of generated explanations in complex tasks.

**Abstract:** While Large Language Models (LLMs) excel at generating human-like text, aligning their outputs with complex, qualitative goals like pedagogical soundness remains a significant challenge. Standard reinforcement learning techniques often rely on slow and expensive LLM-as-a-judge evaluations or on brittle, keyword-based metrics like ROUGE, which fail to capture the semantic essence of a high-quality explanation. In this work, we introduce a novel approach to reward shaping within the Group Relative Policy Optimisation (GRPO) framework. Our central contribution is the use of a small, efficient encoder-only transformer as a semantic reward model. This model provides a dense, semantically rich reward signal based on the cosine similarity between a generated explanation and a ground-truth reference, guiding the policy towards explanations that are not just factually correct but also structurally and conceptually aligned with expert reasoning. We apply this method to the task of training a model for the Italian medical-school entrance examinations, following standard domain-adaptive continued pre-training (CPT) and supervised fine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic reward significantly improves explanation faithfulness and clarity over a strong SFT baseline, showcasing the power of using lightweight encoder models for nuanced reward shaping in complex generation tasks

</details>


### [60] [Empowering LLMs with Parameterized Skills for Adversarial Long-Horizon Planning](https://arxiv.org/abs/2509.13127)

*Sijia Cui, Shuai Xu, Aiyao He, Yanna Wang, Bo Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, AI agents, skill planning

**Relevance Score:** 8

**TL;DR:** Introduction of the PLAP framework for grounding LLM-based agents in long-horizon environments, demonstrating significant improvements in performance in MicroRTS.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges faced by LLM-based AI agents in grounding themselves in complex, adversarial long-horizon environments, where existing methods struggle with reliable action generation and expert reliance.

**Method:** The PLAP framework consists of a skill library of parameterized skills, a skill planner powered by LLMs, and a skill executor to convert skills into executable actions, implemented in the MicroRTS game.

**Key Contributions:**

	1. Introduction of the PLAP planning framework
	2. Demonstration of superior performance in long-horizon environments
	3. Release of an LLM leaderboard for skill planning

**Result:** PLAP, particularly driven by GPT-4o in a zero-shot setting, outperformed 80% of baseline agents; Qwen2-72B-driven PLAP excelled against top scripted agents like CoacAI.

**Limitations:** 

**Conclusion:** PLAP provides a robust method for grounding LLMs in complex environments, with an effective evaluation metric and LLM leaderboard.

**Abstract:** Recent advancements in Large Language Models(LLMs) have led to the development of LLM-based AI agents. A key challenge is the creation of agents that can effectively ground themselves in complex, adversarial long-horizon environments. Existing methods mainly focus on (1) using LLMs as policies to interact with the environment through generating low-level feasible actions, and (2) utilizing LLMs to generate high-level tasks or language guides to stimulate action generation. However, the former struggles to generate reliable actions, while the latter relies heavily on expert experience to translate high-level tasks into specific action sequences. To address these challenges, we introduce the Plan with Language, Act with Parameter (PLAP) planning framework that facilitates the grounding of LLM-based agents in long-horizon environments. The PLAP method comprises three key components: (1) a skill library containing environment-specific parameterized skills, (2) a skill planner powered by LLMs, and (3) a skill executor converting the parameterized skills into executable action sequences. We implement PLAP in MicroRTS, a long-horizon real-time strategy game that provides an unfamiliar and challenging environment for LLMs. The experimental results demonstrate the effectiveness of PLAP. In particular, GPT-4o-driven PLAP in a zero-shot setting outperforms 80% of baseline agents, and Qwen2-72B-driven PLAP, with carefully crafted few-shot examples, surpasses the top-tier scripted agent, CoacAI. Additionally, we design comprehensive evaluation metrics and test 6 closed-source and 2 open-source LLMs within the PLAP framework, ultimately releasing an LLM leaderboard ranking long-horizon skill planning ability. Our code is available at https://github.com/AI-Research-TeamX/PLAP.

</details>


### [61] [LLM Hallucination Detection: A Fast Fourier Transform Method Based on Hidden Layer Temporal Signals](https://arxiv.org/abs/2509.13154)

*Jinxin Li, Gang Tu, ShengYu Cheng, Junjie Hu, Jinting Wang, Rui Chen, Zhilong Zhou, Dongbo Shan*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, frequency-domain analysis

**Relevance Score:** 9

**TL;DR:** A new framework, HSAD, is proposed for detecting hallucinations in large language models by analyzing hidden representations and frequency-domain features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical barrier of hallucination in large language models for reliability-sensitive applications.

**Method:** HSAD uses temporal dynamics of hidden states during generation, sampling layer activations and applying Fast Fourier Transform for frequency-domain analysis.

**Key Contributions:**

	1. Introduces HSAD, a new hallucination detection framework.
	2. Uses temporal analysis of hidden representations and frequency-domain features.
	3. Achieves significant performance improvements over existing methods.

**Result:** HSAD outperforms existing detection methods by over 10 percentage points on multiple benchmarks including TruthfulQA.

**Limitations:** 

**Conclusion:** HSAD provides a robust new paradigm for hallucination detection, combining reasoning-process modeling with frequency-domain analysis.

**Abstract:** Hallucination remains a critical barrier for deploying large language models (LLMs) in reliability-sensitive applications. Existing detection methods largely fall into two categories: factuality checking, which is fundamentally constrained by external knowledge coverage, and static hidden-state analysis, that fails to capture deviations in reasoning dynamics. As a result, their effectiveness and robustness remain limited. We propose HSAD (Hidden Signal Analysis-based Detection), a novel hallucination detection framework that models the temporal dynamics of hidden representations during autoregressive generation. HSAD constructs hidden-layer signals by sampling activations across layers, applies Fast Fourier Transform (FFT) to obtain frequency-domain representations, and extracts the strongest non-DC frequency component as spectral features. Furthermore, by leveraging the autoregressive nature of LLMs, HSAD identifies optimal observation points for effective and reliable detection. Across multiple benchmarks, including TruthfulQA, HSAD achieves over 10 percentage points improvement compared to prior state-of-the-art methods. By integrating reasoning-process modeling with frequency-domain analysis, HSAD establishes a new paradigm for robust hallucination detection in LLMs.

</details>


### [62] [The Few-shot Dilemma: Over-prompting Large Language Models](https://arxiv.org/abs/2509.13196)

*Yongjian Tang, Doruk Tuncel, Christian Koerner, Thomas Runkler*

**Main category:** cs.CL

**Keywords:** over-prompting, few-shot learning, Large Language Models, TF-IDF, software requirement classification

**Relevance Score:** 9

**TL;DR:** This paper investigates the phenomenon of over-prompting in LLMs, revealing that excessive examples can degrade performance. It proposes a framework using few-shot selection methods to optimize prompt efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study is motivated by the discrepancy in previous findings on few-shot prompting for LLMs and the practical implications for LLM-assisted software engineering.

**Method:** The authors outline a prompting framework using random sampling, semantic embedding, and TF-IDF vectors for few-shot selection. They evaluate various LLMs across software requirement classification tasks.

**Key Contributions:**

	1. Identification of the over-prompting phenomenon in LLMs
	2. Development of a few-shot selection framework
	3. Demonstration of improved task performance with optimized examples

**Result:** The research shows that carefully optimising the number of domain-specific examples in prompts enhances LLM performance, surpassing the previous state-of-the-art by 1% in requirement classification.

**Limitations:** The study focuses on a limited number of LLMs and specific classification tasks, which may not generalize across all domains or applications.

**Conclusion:** The findings suggest that fewer, carefully selected examples can improve LLM performance, addressing the over-prompting issue, which has implications for LLM applications in software requirements analysis.

**Abstract:** Over-prompting, a phenomenon where excessive examples in prompts lead to diminished performance in Large Language Models (LLMs), challenges the conventional wisdom about in-context few-shot learning. To investigate this few-shot dilemma, we outline a prompting framework that leverages three standard few-shot selection methods - random sampling, semantic embedding, and TF-IDF vectors - and evaluate these methods across multiple LLMs, including GPT-4o, GPT-3.5-turbo, DeepSeek-V3, Gemma-3, LLaMA-3.1, LLaMA-3.2, and Mistral. Our experimental results reveal that incorporating excessive domain-specific examples into prompts can paradoxically degrade performance in certain LLMs, which contradicts the prior empirical conclusion that more relevant few-shot examples universally benefit LLMs. Given the trend of LLM-assisted software engineering and requirement analysis, we experiment with two real-world software requirement classification datasets. By gradually increasing the number of TF-IDF-selected and stratified few-shot examples, we identify their optimal quantity for each LLM. This combined approach achieves superior performance with fewer examples, avoiding the over-prompting problem, thus surpassing the state-of-the-art by 1% in classifying functional and non-functional requirements.

</details>


### [63] [Evaluating LLM Alignment on Personality Inference from Real-World Interview Data](https://arxiv.org/abs/2509.13244)

*Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personality Traits, Big Five, Human-Computer Interaction, AI applications

**Relevance Score:** 9

**TL;DR:** This paper evaluates the ability of Large Language Models (LLMs) to interpret human personality traits using a new benchmark of interview transcripts and validated Big Five trait scores, revealing limitations in current model alignments with psychological constructs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the unexplored ability of LLMs to interpret human personality traits in conversational settings, which is crucial for applications like emotional support and decision-making.

**Method:** A novel benchmark of semi-structured interview transcripts with validated Big Five trait scores is used to evaluate LLM performance through zero-shot prompting, fine-tuning, and regression with embeddings.

**Key Contributions:**

	1. Introduction of a novel benchmark for evaluating LLMs against continuous Big Five trait scores
	2. Systematic evaluation of LLM performance using various paradigms
	3. Identification of limitations in current LLMs' ability to align with psychological constructs

**Result:** Pearson correlations between model predictions and ground-truth personality traits are below 0.26, indicating limited alignment of LLMs with validated psychological constructs.

**Limitations:** The results indicate that current LLMs do not effectively align with validated personality assessments, suggesting a need for improved methodologies.

**Conclusion:** The findings highlight the challenges in aligning LLMs with complex human personality traits and suggest avenues for future research focusing on trait-specific prompting and improved alignment methods.

**Abstract:** Large Language Models (LLMs) are increasingly deployed in roles requiring nuanced psychological understanding, such as emotional support agents, counselors, and decision-making assistants. However, their ability to interpret human personality traits, a critical aspect of such applications, remains unexplored, particularly in ecologically valid conversational settings. While prior work has simulated LLM "personas" using discrete Big Five labels on social media data, the alignment of LLMs with continuous, ground-truth personality assessments derived from natural interactions is largely unexamined. To address this gap, we introduce a novel benchmark comprising semi-structured interview transcripts paired with validated continuous Big Five trait scores. Using this dataset, we systematically evaluate LLM performance across three paradigms: (1) zero-shot and chain-of-thought prompting with GPT-4.1 Mini, (2) LoRA-based fine-tuning applied to both RoBERTa and Meta-LLaMA architectures, and (3) regression using static embeddings from pretrained BERT and OpenAI's text-embedding-3-small. Our results reveal that all Pearson correlations between model predictions and ground-truth personality traits remain below 0.26, highlighting the limited alignment of current LLMs with validated psychological constructs. Chain-of-thought prompting offers minimal gains over zero-shot, suggesting that personality inference relies more on latent semantic representation than explicit reasoning. These findings underscore the challenges of aligning LLMs with complex human attributes and motivate future work on trait-specific prompting, context-aware modeling, and alignment-oriented fine-tuning.

</details>


### [64] [ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking Guided Attention Refinement](https://arxiv.org/abs/2509.13282)

*Ali Salamatian, Amirhossein Abaskohi, Wan-Cyuan Fan, Mir Rayat Imtiaz Hossain, Leonid Sigal, Giuseppe Carenini*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, Chart Question Answering, Eye-tracking, Gaze-guided attention, Human-computer interaction

**Relevance Score:** 8

**TL;DR:** The paper presents ChartGaze, a dataset tracking human gaze patterns during chart reasoning tasks, aiming to improve attention alignment in Large Vision-Language Models (LVLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges in chart question answering (CQA) caused by LVLMs attending to irrelevant regions, which affects both interpretability and accuracy.

**Method:** A new eye-tracking dataset, ChartGaze, is introduced, and a gaze-guided attention refinement method is proposed to align LVLM attention with human gaze patterns.

**Key Contributions:**

	1. Introduction of the ChartGaze dataset for eye-tracking during chart reasoning tasks.
	2. A novel gaze-guided attention refinement method for aligning model and human attention.
	3. Demonstrated improvements in answer accuracy and attention alignment in LVLMs.

**Result:** The proposed method results in improved answer accuracy and attention alignment, achieving up to 2.56 percentage points gain across models.

**Limitations:** 

**Conclusion:** Incorporating human gaze significantly enhances the reasoning quality and interpretability of chart-focused LVLMs.

**Abstract:** Charts are a crucial visual medium for communicating and representing information. While Large Vision-Language Models (LVLMs) have made progress on chart question answering (CQA), the task remains challenging, particularly when models attend to irrelevant regions of the chart. In this work, we present ChartGaze, a new eye-tracking dataset that captures human gaze patterns during chart reasoning tasks. Through a systematic comparison of human and model attention, we find that LVLMs often diverge from human gaze, leading to reduced interpretability and accuracy. To address this, we propose a gaze-guided attention refinement that aligns image-text attention with human fixations. Our approach improves both answer accuracy and attention alignment, yielding gains of up to 2.56 percentage points across multiple models. These results demonstrate the promise of incorporating human gaze to enhance both the reasoning quality and interpretability of chart-focused LVLMs.

</details>


### [65] [WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents](https://arxiv.org/abs/2509.13309)

*Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** AI agents, knowledge synthesis, Markov Decision Process, multi-agent systems, deep research

**Relevance Score:** 8

**TL;DR:** WebResearcher is a framework that enables AI agents to autonomously synthesize knowledge from external sources, using a Markov Decision Process and a data synthesis engine for improved research outcomes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing mono-contextual research approaches and enhance AI agents' ability to discover and synthesize knowledge.

**Method:** WebResearcher reformulates deep research as a Markov Decision Process, combining it with WebFrontier, a scalable data synthesis engine for generating high-quality training data.

**Key Contributions:**

	1. Introduction of WebResearcher framework
	2. Development of tool-augmented complexity escalation in WebFrontier
	3. Demonstration of state-of-the-art performance across benchmarks

**Result:** WebResearcher achieves state-of-the-art performance on 6 benchmarks, surpassing proprietary systems and enhancing tool-use capabilities of traditional methods.

**Limitations:** 

**Conclusion:** The proposed paradigm enables improved knowledge construction and concurrent explorations through multi-agent systems, demonstrating significant advancements in deep research methodologies.

**Abstract:** Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.

</details>


### [66] [Scaling Agents via Continual Pre-training](https://arxiv.org/abs/2509.13310)

*Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** Agentic systems, Continual pre-training, Large language models

**Relevance Score:** 8

**TL;DR:** This paper introduces Agentic Continual Pre-training (Agentic CPT) for deep research agents, leading to the development of AgentFounder, a model that excels at multi-step reasoning and tool use, achieving state-of-the-art performance on multiple benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the poor performance of general-purpose foundation models in agentic tasks, particularly in open-source implementations, by proposing a framework for robust agentic foundational models.

**Method:** The paper introduces Agentic Continual Pre-training (Agentic CPT) as a new approach in the training pipeline for deep research agents, which facilitates the concurrent learning of diverse agentic behaviors with alignment to expert demonstrations.

**Key Contributions:**

	1. Introduction of Agentic Continual Pre-training (Agentic CPT)
	2. Development of the AgentFounder model
	3. Achievement of state-of-the-art performance on multiple benchmarks

**Result:** AgentFounder-30B model achieves state-of-the-art performance on 10 benchmarks, notably scoring 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE, demonstrating improved tool-use ability and multi-step reasoning.

**Limitations:** 

**Conclusion:** The incorporation of Agentic CPT into model training significantly enhances the capabilities of foundation models in agentic tasks and presents a new direction for future research in this area.

**Abstract:** Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.

</details>


### [67] [Towards General Agentic Intelligence via Environment Scaling](https://arxiv.org/abs/2509.13311)

*Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu, Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** agentic intelligence, Large Language Models, function-calling, scalable framework, fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper proposes a scalable framework, AgentScaler, for enhancing function-calling capabilities of Large Language Models (LLMs) through diverse environment interactions and a two-phase fine-tuning strategy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The necessity for advanced agentic intelligence in LLMs for real-world applications, particularly with diverse APIs that require robust function-calling competence.

**Method:** We designed a scalable framework that automatically creates heterogeneous environments for agents to interact with and learn from, and a two-phase fine-tuning strategy is employed for training agentic capabilities.

**Key Contributions:**

	1. Development of a scalable framework for agent training in diverse environments
	2. Introduction of a two-phase fine-tuning strategy for enhancing agentic capabilities
	3. Demonstration of significant improvements in function-calling abilities on benchmark tests

**Result:** Extensive experiments show that AgentScaler significantly improves the function-calling capability of models evaluated on various agentic benchmarks.

**Limitations:** 

**Conclusion:** By scaling environments and employing a systematic training approach, we advance the development of agentic intelligence in LLMs.

**Abstract:** Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models.

</details>


### [68] [WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research](https://arxiv.org/abs/2509.13312)

*Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, Jun Zhang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** open-ended deep research, AI synthesis, human-centric research

**Relevance Score:** 8

**TL;DR:** WebWeaver is a novel dual-agent framework for open-ended deep research that improves AI's ability to synthesize web-scale information into insightful reports.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in current AI research generation methods, particularly static pipelines and hallucination issues.

**Method:** WebWeaver uses a dual-agent approach where a planner dynamically interleaves evidence acquisition with outline optimization, followed by a writer that composes reports section by section using hierarchical retrieval.

**Key Contributions:**

	1. Introduction of the WebWeaver framework for OEDR.
	2. Dynamic interleaving of planning and evidence acquisition.
	3. Targeted retrieval processes that mitigate long-context issues.

**Result:** WebWeaver achieves state-of-the-art performance on major OEDR benchmarks such as DeepResearch Bench, DeepConsult, and DeepResearchGym.

**Limitations:** 

**Conclusion:** The iterative and adaptive planning improves the production of high-quality and structured reports, emphasizing the importance of focusing on synthesis.

**Abstract:** This paper tackles open-ended deep research (OEDR), a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and one-shot generation paradigms that easily suffer from long-context failure issues like "loss in the middle" and hallucinations. To address these challenges, we introduce WebWeaver, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, source-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank for each part, it effectively mitigates long-context issues. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing high-quality, reliable, and well-structured reports.

</details>


### [69] [ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization](https://arxiv.org/abs/2509.13313)

*Xixi Wu, Kuan Li, Yida Zhao, Liwen Zhang, Litu Ou, Huifeng Yin, Zhongwang Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Minhao Cheng, Shuai Wang, Hong Cheng, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, web agents, context summarization

**Relevance Score:** 8

**TL;DR:** ReSum is a new paradigm for LLM-based web agents that improves performance on complex queries by enabling indefinite exploration through context summarization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLM-based web agents struggle with context limitations when handling complex queries, leading to incomplete solutions.

**Method:** ReSum employs periodic context summarization to convert interaction histories into compact reasoning states, allowing web agents to bypass context constraints.

**Key Contributions:**

	1. Introduction of ReSum for context summarization
	2. Integration of GRPO with segmented trajectory training
	3. Demonstrated substantial performance improvements on multiple benchmarks

**Result:** ReSum shows an average improvement of 4.5% over the ReAct paradigm, with gains of up to 8.2% using ReSum-GRPO training on web agents across three benchmarks.

**Limitations:** 

**Conclusion:** ReSum represents a significant advancement in enabling LLM-based agents to handle complex queries more effectively without being hindered by context window limitations.

**Abstract:** Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents.

</details>


### [70] [Do Natural Language Descriptions of Model Activations Convey Privileged Information?](https://arxiv.org/abs/2509.13316)

*Millicent Li, Alberto Mario Ceballos Arroyo, Giordano Rogers, Naomi Saphra, Byron C. Wallace*

**Main category:** cs.CL

**Keywords:** LLM, activation verbalization, interpretability, natural language descriptions, benchmarks

**Relevance Score:** 8

**TL;DR:** This paper critiques the effectiveness of LLM activation verbalization methods in providing insights into target model internals, revealing that they often reflect the verbalizer LLM's knowledge rather than the target model's operations.

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** To determine whether LLM activation verbalization methods provide true insights into the internal workings of LLMs or merely reflect input information.

**Method:** The authors evaluate popular verbalization methods across established datasets, analyze their performance without access to target model internals, and conduct controlled experiments to assess what the verbalizations truly reflect.

**Key Contributions:**

	1. Critique of current LLM activation verbalization methods
	2. Introduction of the concept that verbalizations reflect the verbalizer more than the target model
	3. Call for the creation of better benchmarks for evaluating verbalization techniques.

**Result:** The study finds that existing benchmarks are inadequate for assessing verbalization methods, as results often depend more on the capabilities of the verbalizer LLM than the target LLM.

**Limitations:** The findings highlight a dependence on datasets that do not effectively assess the internal workings of LLMs.

**Conclusion:** The authors advocate for the development of targeted benchmarks and experimental controls to ensure meaningful evaluation of verbalization methods in understanding LLM operations.

**Abstract:** Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they succeed at benchmarks without any access to target model internals, suggesting that these datasets are not ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the activations of the target LLM being decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.

</details>


### [71] [Do predictability factors towards signing avatars hold across cultures?](https://arxiv.org/abs/2307.02103)

*Abdelhadi Soudi, Manal El Hakkaoui, Kristof Van Laerhoven*

**Main category:** cs.CL

**Keywords:** avatar technology, sign language, Deaf-and-Hard of Hearing, attitudes, accessibility

**Relevance Score:** 7

**TL;DR:** The study explores how intrinsic and extrinsic factors influence the attitudes of Deaf-and-Hard of Hearing sign language users towards avatar technology.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve communication and accessibility for Deaf-and-Hard of Hearing individuals through avatar technology, understanding user acceptance is crucial.

**Method:** A questionnaire was designed and distributed to three participant groups (Deaf, Hearing, Hard-of-Hearing) to assess their attitudes towards avatars.

**Key Contributions:**

	1. Identification of intrinsic and extrinsic factors affecting attitudes towards avatars in sign language users.
	2. Comparison of findings with other studies on similar topics across different cultures.
	3. Introduction of a questionnaire specifically designed for Moroccan Sign Language users.

**Result:** The study found that attitudes towards avatars vary significantly based on intrinsic factors (e.g., avatar characteristics) and extrinsic factors (e.g., technology experience).

**Limitations:** The study may be limited by the small sample size and cultural specificity to Moroccan Sign Language.

**Conclusion:** Inclusive research involving Deaf researchers is important for more accurate insights; attitudes towards avatars are influenced by both user experience and cultural differences.

**Abstract:** Avatar technology can offer accessibility possibilities and improve the Deaf-and-Hard of Hearing sign language users access to communication, education and services, such as the healthcare system. However, sign language users acceptance of signing avatars as well as their attitudes towards them vary and depend on many factors. Furthermore, research on avatar technology is mostly done by researchers who are not Deaf. The study examines the extent to which intrinsic or extrinsic factors contribute to predict the attitude towards avatars across cultures. Intrinsic factors include the characteristics of the avatar, such as appearance, movements and facial expressions. Extrinsic factors include users technology experience, their hearing status, age and their sign language fluency. This work attempts to answer questions such as, if lower attitude ratings are related to poor technology experience with ASL users, for example, is that also true for Moroccan Sign Language (MSL) users? For the purposes of the study, we designed a questionnaire to understand MSL users attitude towards avatars. Three groups of participants were surveyed: Deaf (57), Hearing (20) and Hard-of-Hearing (3). The results of our study were then compared with those reported in other relevant studies.

</details>


### [72] [Context-Aware Membership Inference Attacks against Pre-trained Large Language Models](https://arxiv.org/abs/2409.13745)

*Hongyan Chang, Ali Shahin Shamsabadi, Kleomenis Katevas, Hamed Haddadi, Reza Shokri*

**Main category:** cs.CL

**Keywords:** Membership Inference Attacks, Large Language Models, Perplexity Dynamics

**Relevance Score:** 9

**TL;DR:** This paper presents a novel membership inference attack on pre-trained large language models by adapting statistical tests to the perplexity dynamics of subsequences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of prior membership inference attacks that fail on Large Language Models due to their generative nature.

**Method:** An adaptation of membership inference attack statistical tests focused on the perplexity dynamics of subsequences within a data point.

**Key Contributions:**

	1. Development of a new attack methodology for MIAs on LLMs
	2. Revealing context-dependent memorization in LLMs
	3. Significantly improved performance over previous MIA methods

**Result:** The proposed method significantly outperforms previous approaches and reveals context-dependent memorization patterns in pre-trained LLMs.

**Limitations:** 

**Conclusion:** The novel attack demonstrates the unique challenges and methodologies needed to assess membership inference in generative models like LLMs.

**Abstract:** Membership Inference Attacks (MIAs) on pre-trained Large Language Models (LLMs) aim at determining if a data point was part of the model's training set. Prior MIAs that are built for classification models fail at LLMs, due to ignoring the generative nature of LLMs across token sequences. In this paper, we present a novel attack on pre-trained LLMs that adapts MIA statistical tests to the perplexity dynamics of subsequences within a data point. Our method significantly outperforms prior approaches, revealing context-dependent memorization patterns in pre-trained LLMs.

</details>


### [73] [Responsible AI in NLP: GUS-Net Span-Level Bias Detection Dataset and Benchmark for Generalizations, Unfairness, and Stereotypes](https://arxiv.org/abs/2410.08388)

*Maximus Powers, Shaina Raza, Alex Chang, Rehana Riaz, Umang Mavani, Harshitha Reddy Jonala, Ansh Tiwari, Hua Wei*

**Main category:** cs.CL

**Keywords:** social bias, NLP, GUS dataset, token-level analysis, encoder-based models

**Relevance Score:** 8

**TL;DR:** The GUS-Net Framework facilitates token-level analysis of social bias in language technologies, providing a dataset and methods to detect representational harms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address representational harms which occur in language technologies, particularly how biases in language can affect understanding without being easily identifiable.

**Method:** Developed the GUS dataset with over 69,000 annotations and a multi-label token-level detector for analyzing phrases that convey biases such as generalizations, unfairness, and stereotypes, using BIO tagging.

**Key Contributions:**

	1. Introduced the GUS dataset for bias detection in NLP.
	2. Developed a multi-label token-level detector for span-level analysis of biases.
	3. Demonstrated that encoder-based models can outperform LLMs in detecting biases efficiently.

**Result:** Empirical evaluations indicate that encoder-based models outperform decoder-based LLMs in identifying nuanced biases while being more efficient.

**Limitations:** 

**Conclusion:** The GUS-Net Framework offers a systematic approach for the auditing and mitigation of representational harms in NLP systems, enabling fine-grained analysis and improvement.

**Abstract:** Representational harms in language technologies often occur in short spans within otherwise neutral text, where phrases may simultaneously convey generalizations, unfairness, or stereotypes. Framing bias detection as sentence-level classification obscures which words carry bias and what type is present, limiting both auditability and targeted mitigation. We introduce the GUS-Net Framework, comprising the GUS dataset and a multi-label token-level detector for span-level analysis of social bias. The GUS dataset contains 3,739 unique snippets across multiple domains, with over 69,000 token-level annotations. Each token is labeled using BIO tags (Begin, Inside, Outside) for three pathways of representational harm: Generalizations, Unfairness, and Stereotypes. To ensure reliable data annotation, we employ an automated multi-agent pipeline that proposes candidate spans which are subsequently verified and corrected by human experts. We formulate bias detection as multi-label token-level classification and benchmark both encoder-based models (e.g., BERT family variants) and decoder-based large language models (LLMs). Our evaluations cover token-level identification and span-level entity recognition on our test set, and out-of-distribution generalization. Empirical results show that encoder-based models consistently outperform decoder-based baselines on nuanced and overlapping spans while being more computationally efficient. The framework delivers interpretable, fine-grained diagnostics that enable systematic auditing and mitigation of representational harms in real-world NLP systems.

</details>


### [74] [Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs](https://arxiv.org/abs/2502.08045)

*Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou*

**Main category:** cs.CL

**Keywords:** Cultural Alignment, Large Language Models, Evaluation Frameworks

**Relevance Score:** 6

**TL;DR:** This paper critiques the use of closed-style multiple-choice surveys for evaluating cultural alignment in LLMs, advocating for more flexible evaluation methods that yield more nuanced results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of closed-style multiple-choice surveys in evaluating cultural alignment in Large Language Models (LLMs).

**Method:** The study utilizes case studies from the World Values Survey (WVS) and Hofstede Cultural Dimensions to explore the effects of evaluation constraints on LLM responses.

**Key Contributions:**

	1. Critique of closed evaluation methods for LLMs
	2. Empirical findings on cultural alignment variability based on survey constraints
	3. Proposal for more flexible evaluation frameworks

**Result:** Results indicate that LLMs demonstrate stronger cultural alignment in unconstrained settings, and minor changes in survey structure can lead to inconsistent outputs.

**Limitations:** The study primarily focuses on specific cultural dimensions and may not generalize across all evaluation contexts.

**Conclusion:** The study calls for the adoption of more robust and flexible evaluation frameworks for assessing cultural alignment in LLMs, emphasizing the importance of nuanced assessments.

**Abstract:** A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.

</details>


### [75] [How Much Do LLMs Hallucinate across Languages? On Multilingual Estimation of LLM Hallucination in the Wild](https://arxiv.org/abs/2502.12769)

*Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination, multilingual, question answering, machine translation

**Relevance Score:** 9

**TL;DR:** This paper quantifies LLM hallucination in multilingual knowledge-intensive question answering, showing similar hallucination rates across languages using both noisy and gold datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the global utility risk posed by LLM hallucinations, especially in multilingual contexts beyond English-centric studies.

**Method:** A multilingual hallucination detection model is trained, utilizing both machine translation-generated noisy data and manually annotated gold data for five high-resource languages, followed by a study across 30 languages and 6 LLM families.

**Key Contributions:**

	1. Quantitative analysis of LLM hallucination rates in 30 languages.
	2. Development of a multilingual hallucination detection model.
	3. Validation of noisy data for hallucination rate estimation across languages.

**Result:** The study reveals that longer responses with more hallucinated tokens occur in higher-resource languages, but no correlation exists between length-normalized hallucination rates and digital representation of languages. Smaller LLMs demonstrate higher hallucination rates than larger ones.

**Limitations:** The reliance on machine translation for noisy data may introduce additional biases.

**Conclusion:** The findings validate the use of noisy data for estimating hallucination rates and suggest that the problem of hallucination persists across various languages, warranting further investigation.

**Abstract:** In the age of misinformation, hallucination -- the tendency of Large Language Models (LLMs) to generate non-factual or unfaithful responses -- represents the main risk for their global utility. Despite LLMs becoming increasingly multilingual, the vast majority of research on detecting and quantifying LLM hallucination are (a) English-centric and (b) focus on machine translation (MT) and summarization, tasks that are less common ``in the wild'' than open information seeking. In contrast, we aim to quantify the extent of LLM hallucination across languages in knowledge-intensive long-form question answering. To this end, we train a multilingual hallucination detection model and conduct a large-scale study across 30 languages and 6 open-source LLM families. We start from an English hallucination detection dataset and rely on MT to generate (noisy) training data in other languages. We also manually annotate gold data for five high-resource languages; we then demonstrate, for these languages, that the estimates of hallucination rates are similar between silver (LLM-generated) and gold test sets, validating the use of silver data for estimating hallucination rates for other languages. For the final rates estimation, we build a knowledge-intensive QA dataset for 30 languages with LLM-generated prompts and Wikipedia articles as references. We find that, while LLMs generate longer responses with more hallucinated tokens for higher-resource languages, there is no correlation between length-normalized hallucination rates of languages and their digital representation. Further, we find that smaller LLMs exhibit larger hallucination rates than larger models.

</details>


### [76] [Teaching Your Models to Understand Code via Focal Preference Alignment](https://arxiv.org/abs/2503.02783)

*Jie Wu, Haoling Li, Xin Zhang, Jianwen Luo, Yangyu Huang, Ruihang Chu, Yujiu Yang, Scarlett Li*

**Main category:** cs.CL

**Keywords:** Code LLMs, Preference learning, Error correction, Iterative debugging, DPO algorithm

**Relevance Score:** 8

**TL;DR:** This paper introduces Target-DPO, a new preference learning framework that enhances Code LLMs by refining the error-correction process through human-like iterative debugging and a specialized DPO algorithm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved error-correction capabilities in Code LLMs, which currently rely on coarse evaluations that fail to capture specific error relationships.

**Method:** Target-DPO introduces a tailored preference alignment framework that identifies error regions and aligns tokens, supported by the creation of the CodeFlow dataset for iterative sample refinement.

**Key Contributions:**

	1. Introduction of Target-DPO preference alignment framework
	2. Creation of CodeFlow dataset for iterative debugging
	3. Demonstration of improved performance in Code LLMs and reduced errors

**Result:** Target-DPO significantly improves the performance of various Code LLMs, especially in code generation, and reduces the error rates in challenging tasks.

**Limitations:** 

**Conclusion:** The proposed method demonstrates substantial performance gains and offers a more nuanced understanding of error correction in code generation.

**Abstract:** Preference learning extends the performance of Code LLMs beyond traditional supervised fine-tuning by leveraging relative quality comparisons. In existing approaches, a set of n candidate solutions is evaluated based on test case success rates, with the candidate demonstrating a higher pass rate being labeled as positive and its counterpart with a lower pass rate as negative. However, because this approach aligns entire failing code blocks rather than pinpointing specific errors, it lacks the granularity necessary to capture meaningful error-correction relationships. As a result, the model is unable to learn more informative error-correction patterns. To address these issues, we propose Target-DPO, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. Target-DPO explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To facilitate it, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with Target-DPO achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that Target-DPO yields fewer errors. Code, model and datasets are in: https://github.com/JieWu02/Target-DPO.

</details>


### [77] [Dynamic Relation Inference via Verb Embeddings](https://arxiv.org/abs/2503.13021)

*Omri Suissa, Muhiim Ali, Ariana Azarbal, Hui Shen, Shekhar Pradhan*

**Main category:** cs.CL

**Keywords:** CLIP, image-text matching, relation detection, Dynamic Relation Inference, machine learning

**Relevance Score:** 8

**TL;DR:** The paper introduces DRIVE, a novel method to enhance CLIP's ability to infer relationships among objects in images through improved text and image embeddings, achieving significant improvements in zero-shot relation inference accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods struggle with CLIP's ability to infer relationships among objects in images, necessitating improved techniques for relation detection.

**Method:** The paper presents DRIVE, which fine-tunes CLIP using hard negatives from augmented COCO dataset, optimizing a novel loss function for better relationship detection.

**Key Contributions:**

	1. Introduction of Dynamic Relation Inference via Verb Embeddings (DRIVE)
	2. Fine-tuning CLIP using hard negatives and a novel loss function
	3. Significant performance improvements in zero-shot relation inference.

**Result:** DRIVE significantly enhances zero-shot relation inference accuracy compared to CLIP and other state-of-the-art models, performing well in both frozen and fine-tuned settings.

**Limitations:** 

**Conclusion:** The proposed methods demonstrate a notable improvement in relation inference tasks, paving the way for better integration of relational understanding in image-text matching systems.

**Abstract:** CLIP has demonstrated exceptional image-text matching capabilities due to its training on contrastive learning tasks. Past research has suggested that whereas CLIP effectively matches text to images when the matching can be achieved just by matching the text with the objects in the image, CLIP struggles when the matching depends on representing the relationship among the objects in the images (i.e., inferring relations). Previous attempts to address this limitation by training CLIP on relation detection datasets with only linguistic supervision have met with limited success. In this paper, we offer insights and practical methods to advance the field of relation inference from images. This paper approaches the task of creating a model that effectively detects relations among the objects in images by producing text and image embeddings that capture relationships through linguistic supervision. To this end, we propose Dynamic Relation Inference via Verb Embeddings (DRIVE), which augments the COCO dataset, fine-tunes CLIP with hard negatives subject-relation-object triples and corresponding images, and introduces a novel loss function to improve relation detection. Evaluated on multiple CLIP-based models, our method significantly improves zero-shot relation inference accuracy in both frozen and fine-tuned settings, significantly outperforming CLIP and state-of-the-art models while generalizing well on unseen data.

</details>


### [78] [Is the Top Still Spinning? Evaluating Subjectivity in Narrative Understanding](https://arxiv.org/abs/2504.01132)

*Melanie Subbiah, Akankshya Mishra, Grace Kim, Liyan Tang, Greg Durrett, Kathleen McKeown*

**Main category:** cs.CL

**Keywords:** claim faithfulness, ambiguity, narrative summarization, LLM-generated edits, evaluation metric

**Relevance Score:** 8

**TL;DR:** This paper addresses the ambiguity in evaluating the faithfulness of claims to source documents by introducing the Ambiguity Rewrite Metric (ARM), which assesses the degree of necessary edits for claim clarity instead of binary labels.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The work aims to improve the evaluation of claim faithfulness, acknowledging that binary classifications can overlook the complexity and subjectivity involved in interpreting ambiguous claims.

**Method:** The authors propose using LLM-generated edits to evaluate how much a summary needs to be altered for clarity, thus replacing binary judgments with a more nuanced metric.

**Key Contributions:**

	1. Introduction of the Ambiguity Rewrite Metric (ARM) for claim evaluation
	2. Demonstration of significant improvement in annotator agreement on claim faithfulness
	3. Focus on narrative summarization areas to highlight ambiguity issues

**Result:** The Ambiguity Rewrite Metric (ARM) indicates a 21% absolute improvement in annotator agreement regarding claim faithfulness, demonstrating reduced subjectivity in evaluations.

**Limitations:** 

**Conclusion:** By adopting ARM, the evaluation of ambiguous claims becomes more effective, leading to greater clarity and consistency in faithfulness judgments.

**Abstract:** Determining faithfulness of a claim to a source document is an important problem across many domains. This task is generally treated as a binary judgment of whether the claim is supported or unsupported in relation to the source. In many cases, though, whether a claim is supported can be ambiguous. For instance, it may depend on making inferences from given evidence, and different people can reasonably interpret the claim as either supported or unsupported based on their agreement with those inferences. Forcing binary labels upon such claims lowers the reliability of evaluation. In this work, we reframe the task to manage the subjectivity involved with factuality judgments of ambiguous claims. We introduce LLM-generated edits of summaries as a method of providing a nuanced evaluation of claims: how much does a summary need to be edited to be unambiguous? Whether a claim gets rewritten and how much it changes can be used as an automatic evaluation metric, the Ambiguity Rewrite Metric (ARM), with a much richer feedback signal than a binary judgment of faithfulness. We focus on the area of narrative summarization as it is particularly rife with ambiguity and subjective interpretation. We show that ARM produces a 21% absolute improvement in annotator agreement on claim faithfulness, indicating that subjectivity is reduced.

</details>


### [79] [Do Large Language Models Truly Grasp Addition? A Rule-Focused Diagnostic Using Two-Integer Arithmetic](https://arxiv.org/abs/2504.05262)

*Yang Yan, Yu Lu, Renjun Xu, Zhenzhong Lan*

**Main category:** cs.CL

**Keywords:** large language models, arithmetic, pattern matching, commutativity, representation invariance

**Relevance Score:** 8

**TL;DR:** This paper examines large language models' understanding of basic arithmetic, demonstrating that they primarily rely on pattern matching rather than true rule induction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs genuinely understand fundamental arithmetic rules or simply depend on pattern matching, by testing their performance on basic addition.

**Method:** The study probes LLMs' comprehension through tests of commutativity, representation invariance, and accuracy scaling across 12 leading models.

**Key Contributions:**

	1. Developed novel diagnostics for LLM arithmetic understanding
	2. Demonstrated disconnect between high accuracy and true rule understanding
	3. Provided publicly available dataset and code for further research

**Result:** Models achieved high average numeric accuracy (73.8-99.8%) but consistently failed three key diagnostics, such as accuracy dropping to <= 7.5% with symbolic inputs.

**Limitations:** Study focused solely on two-integer addition, not addressing other arithmetic operations or complex mathematical reasoning.

**Conclusion:** Current LLMs perform elementary arithmetic via pattern matching rather than actual mathematical reasoning, necessitating new benchmarks and improvements in model training.

**Abstract:** Large language models (LLMs) achieve impressive results on advanced mathematics benchmarks but sometimes fail on basic arithmetic tasks, raising the question of whether they have truly grasped fundamental arithmetic rules or are merely relying on pattern matching. To unravel this issue, we systematically probe LLMs' understanding of two-integer addition (0 to $2^64$) by testing three crucial properties: commutativity (A+B=B+A), representation invariance via symbolic remapping (e.g., $7 -> Y$), and consistent accuracy scaling with operand length. Our evaluation of 12 leading LLMs reveals a stark disconnect: while models achieve high numeric accuracy (73.8-99.8%), they systematically fail these diagnostics. Specifically, accuracy plummets to <= 7.5% with symbolic inputs, commutativity is violated in up to 20% of cases, and accuracy scaling is non-monotonic. These findings demonstrate that current LLMs address elementary addition via pattern matching, not robust rule induction, motivating new diagnostic benchmarks and innovations in model architecture and training to cultivate genuine mathematical reasoning. Our dataset and generating code are available at https://github.com/kuri-leo/llm-arithmetic-diagnostic.

</details>
