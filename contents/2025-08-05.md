# 2025-08-05

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 53]

- [cs.CL](#cs.CL) [Total: 122]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Generative AI for CAD Automation: Leveraging Large Language Models for 3D Modelling](https://arxiv.org/abs/2508.00843)

*Sumit Kumar, Sarthak Kapoor, Harsh Vardhan, Yao Zhao*

**Main category:** cs.HC

**Keywords:** Large Language Models, Computer-Aided Design, FreeCAD, workflow automation, script generation

**Relevance Score:** 8

**TL;DR:** This paper explores the use of Large Language Models (LLMs) in automating Computer-Aided Design (CAD) workflows by integrating them with FreeCAD, aiming to simplify and enhance CAD processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional CAD processes are complex and require specialized skills, hindering rapid prototyping and generative design; LLMs present a solution to automate and simplify these workflows.

**Method:** The proposed framework involves generating CAD scripts from natural language descriptions using LLMs, which are then executed and refined iteratively based on feedback.

**Key Contributions:**

	1. Integration of FreeCAD with LLMs for automated CAD design
	2. Development of a framework for generating scripts from natural language
	3. Identification of limitations and areas for improvement in LLM-assisted CAD processes

**Result:** The experiments conducted show that LLMs are effective for simple to moderately complex designs, but face challenges with highly constrained models requiring multiple refinements.

**Limitations:** LLMs struggle with highly constrained models, necessitating multiple refinements.

**Conclusion:** The study highlights LLMs' potential in enhancing CAD workflows and identifies the need for better memory retrieval and adaptive techniques for improved robustness.

**Abstract:** Large Language Models (LLMs) are revolutionizing industries by enhancing efficiency, scalability, and innovation. This paper investigates the potential of LLMs in automating Computer-Aided Design (CAD) workflows, by integrating FreeCAD with LLM as CAD design tool. Traditional CAD processes are often complex and require specialized sketching skills, posing challenges for rapid prototyping and generative design. We propose a framework where LLMs generate initial CAD scripts from natural language descriptions, which are then executed and refined iteratively based on error feedback. Through a series of experiments with increasing complexity, we assess the effectiveness of this approach. Our findings reveal that LLMs perform well for simple to moderately complex designs but struggle with highly constrained models, necessitating multiple refinements. The study highlights the need for improved memory retrieval, adaptive prompt engineering, and hybrid AI techniques to enhance script robustness. Future directions include integrating cloud-based execution and exploring advanced LLM capabilities to further streamline CAD automation. This work underscores the transformative potential of LLMs in design workflows while identifying critical areas for future development.

</details>


### [2] [Cognitive Exoskeleton: Augmenting Human Cognition with an AI-Mediated Intelligent Visual Feedback](https://arxiv.org/abs/2508.00846)

*Songlin Xu, Xinyu Zhang*

**Main category:** cs.HC

**Keywords:** Dual-DRL, Adaptive Feedback, Cognitive Performance, Deep Reinforcement Learning, Time Pressure

**Relevance Score:** 6

**TL;DR:** The paper presents a dual-DRL framework that uses AI-mediated feedback to enhance user performance in cognitive tasks by adjusting time pressure based on real-time performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve user performance in cognitively demanding tasks by providing adaptive feedback that balances time pressure with user attention and anxiety.

**Method:** The authors propose a dual-DRL framework where one agent regulates user performance by interacting with another agent that simulates user cognition behaviors based on data from existing datasets.

**Key Contributions:**

	1. Development of a dual-DRL framework for cognitive task support
	2. Adaptive time pressure feedback mechanism
	3. Successful demonstration of effectiveness in user studies

**Result:** User studies show that the dual-DRL framework successfully enhances performance compared to a baseline, proving the effectiveness of the adaptive time pressure feedback.

**Limitations:** DRL training and hyperparameter tuning may require substantial data and several iterative user studies.

**Conclusion:** The findings indicate that a well-tuned AI framework can significantly support cognitive tasks by intelligently managing time pressure and improving attention regulation.

**Abstract:** In this paper, we introduce an AI-mediated framework that can provide intelligent feedback to augment human cognition. Specifically, we leverage deep reinforcement learning (DRL) to provide adaptive time pressure feedback to improve user performance in a math arithmetic task. Time pressure feedback could either improve or deteriorate user performance by regulating user attention and anxiety. Adaptive time pressure feedback controlled by a DRL policy according to users' real-time performance could potentially solve this trade-off problem. However, the DRL training and hyperparameter tuning may require large amounts of data and iterative user studies. Therefore, we propose a dual-DRL framework that trains a regulation DRL agent to regulate user performance by interacting with another simulation DRL agent that mimics user cognition behaviors from an existing dataset. Our user study demonstrates the feasibility and effectiveness of the dual-DRL framework in augmenting user performance, in comparison to the baseline group.

</details>


### [3] [GPT Chatbots for Alleviating Anxiety and Depression: A Pilot Randomized Controlled Trial with Afghan Women](https://arxiv.org/abs/2508.00847)

*Sofia Sahab, Jawad Haqbeen, Diksha Sapkota, Takayuki Ito*

**Main category:** cs.HC

**Keywords:** GPT-4, Mental Health, Afghan Women, Supportive Listener, Linguistic Analysis

**Relevance Score:** 8

**TL;DR:** The study explores the mental health effects of GPT-4 on Afghan women, showing significant benefits of empathetic AI support in reducing anxiety and depression.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Afghan women face significant mental health challenges due to societal factors, and AI could play a role in providing support.

**Method:** A randomized controlled trial with 60 participants was conducted, dividing them into three groups: GPT-4, a supportive listener, and a waiting list. The Hospital Anxiety and Depression Scale (HADS) measured changes in anxiety and depression before and after intervention, alongside linguistic analysis of chat data.

**Key Contributions:**

	1. Empirical evidence of GPT-4's effectiveness as a supportive listener for mental health
	2. Linguistic alignment linked to improvements in mental health measures
	3. Highlighting the role of AI in mental health interventions for marginalized groups

**Result:** The supportive listener group demonstrated a significant reduction in HADS scores, a more positive tone, and higher language style matching compared to other groups.

**Limitations:** 

**Conclusion:** AI-driven interventions like GPT-4 can provide accessible mental health support but should be used to complement traditional psychotherapy.

**Abstract:** In this study, we investigated the effects of GPT-4, with and without specific conversational instructions, on the mental health of Afghan women. These women face multifaceted challenges, including Taliban-imposed restrictions, societal inequalities, and domestic violence, adversely affecting their well-being. We conducted a randomized controlled trial with 60 participants, dividing them into three groups: GPT-4, a supportive listener (GPT-4 with empathetic engagement instructions), and a waiting list. The Hospital Anxiety and Depression Scale (HADS) was used to measure anxiety and depression before and after the intervention. Linguistic analysis of chat data examined personal pronouns, tones, emotions, and Language Style Matching (LSM). The supportive listener group showed a significant reduction in HADS scores compared to the other groups. Linguistic analysis revealed a more positive tone and higher LSM in the supportive listener group, with a significant negative correlation between LSM and changes in HADS scores, indicating greater linguistic alignment was linked to reductions in anxiety and depression. Perceived empathy ratings were also significantly higher in the supportive listener group. These findings highlight the potential of AI-driven interventions, like GPT-4, in providing accessible mental health support. However, such interventions should complement traditional psychotherapy, ensuring a collaborative approach to optimize therapeutic outcomes.

</details>


### [4] [RestAware: Non-Invasive Sleep Monitoring Using FMCW Radar and AI-Generated Summaries](https://arxiv.org/abs/2508.00848)

*Agniva Banerjee, Bhanu Partap Paregi, Haroon R. Lone*

**Main category:** cs.HC

**Keywords:** sleep monitoring, FMCW radar, privacy, large language models, smart homes

**Relevance Score:** 9

**TL;DR:** RestAware is a non-invasive sleep monitoring system using FMCW radar technology, achieving high classification accuracy and generating personalized sleep summaries with large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a better sleep monitoring solution that overcomes the limitations of traditional devices, such as user discomfort, obstructions, and privacy concerns.

**Method:** RestAware utilizes a 24GHz frequency-modulated continuous wave (FMCW) radar to monitor sleep postures, evaluated on 25 participants with a K-Nearest Neighbors (KNN) classifier for accuracy.

**Key Contributions:**

	1. Introduction of a non-invasive sleep monitoring system using FMCW radar
	2. High accuracy in posture classification
	3. Integration of instruction-tuned LLMs for generating personalized sleep summaries

**Result:** Achieved 92% classification accuracy and an F1-score of 0.91 in identifying eight common sleep postures.

**Limitations:** 

**Conclusion:** RestAware provides a low-cost, privacy-preserving alternative for sleep monitoring suitable for smart homes and clinical use.

**Abstract:** Monitoring sleep posture and behavior is critical for diagnosing sleep disorders and improving overall sleep quality. However, traditional approaches, such as wearable devices, cameras, and pressure sensors, often compromise user comfort, fail under obstructions like blankets, and raise privacy concerns. To overcome these limitations, we present RestAware, a non-invasive, contactless sleep monitoring system based on a 24GHz frequency-modulated continuous wave (FMCW) radar. Our system is evaluated on 25 participants across eight common sleep postures, achieving 92% classification accuracy and an F1-score of 0.91 using a K-Nearest Neighbors (KNN) classifier. In addition, we integrate instruction-tuned large language models (Mistral, Llama, and Falcon) to generate personalized, human-readable sleep summaries from radar-derived posture data. This low-cost ($ 35), privacy-preserving solution offers a practical alternative for real-time deployment in smart homes and clinical environments.

</details>


### [5] [Gearshift Fellowship: A Next-Generation Neurocomputational Game Platform to Model and Train Human-AI Adaptability](https://arxiv.org/abs/2508.00850)

*Nadja R. Ging-Jehli, Russell K. Childers, Joshua Lu, Robert Gemma, Rachel Zhu*

**Main category:** cs.HC

**Keywords:** Gearshift Fellowship, adaptive behavior, neurocognitive modeling, serious gaming, self-regulated learning

**Relevance Score:** 6

**TL;DR:** The Gearshift Fellowship (GF) is a prototype for a new paradigm that models human and artificial agent adaptability in shifting environments through neurocognitive modeling and serious gaming.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand mechanisms of adaptive behavior across cognitive and social contexts, and to foster self-regulated learning and resilience.

**Method:** GF combines computational neurocognitive modeling with serious gaming to create a dynamic environment assessing adaptive behavior mechanisms and learning styles.

**Key Contributions:**

	1. Development of the Gearshift Fellowship as a prototype for adaptive behavior modeling
	2. Integration of serious gaming with neurocognitive assessments
	3. Provision of a flexible platform for research and clinical interventions

**Result:** Online study shows GF captures effects from traditional tasks, revealing new patterns in learning differences and adaptations linked to clinical features.

**Limitations:** 

**Conclusion:** GF serves as a tool for scientists, clinicians, and players to enhance learning and cope with stress, and promotes a co-development ecosystem for humans and machines.

**Abstract:** How do we learn when to persist, when to let go, and when to shift gears? Gearshift Fellowship (GF) is the prototype of a new Supertask paradigm designed to model how humans and artificial agents adapt to shifting environment demands. Grounded in cognitive neuroscience, computational psychiatry, economics, and artificial intelligence, Supertasks combine computational neurocognitive modeling with serious gaming. This creates a dynamic, multi-mission environment engineered to assess mechanisms of adaptive behavior across cognitive and social contexts. Computational parameters explain behavior and probe mechanisms by controlling the game environment. Unlike traditional tasks, GF enables neurocognitive modeling of individual differences across perceptual decisions, learning, and meta-cognitive levels. This positions GF as a flexible testbed for understanding how cognitive-affective control processes, learning styles, strategy use, and motivational shifts adapt across contexts and over time. It serves as an experimental platform for scientists, a phenotype-to-mechanism intervention for clinicians, and a training tool for players aiming to strengthen self-regulated learning, mood, and stress resilience. Online study (n = 60, ongoing) results show that GF recovers effects from traditional neuropsychological tasks (construct validity), uncovers novel patterns in how learning differs across contexts and how clinical features map onto distinct adaptations. These findings pave the way for developing in-game interventions that foster self-efficacy and agency to cope with real-world stress and uncertainty. GF builds a new adaptive ecosystem designed to accelerate science, transform clinical care, and foster individual growth. It offers a mirror and training ground where humans and machines co-develop together deeper flexibility and awareness.

</details>


### [6] [Visuo-Acoustic Hand Pose and Contact Estimation](https://arxiv.org/abs/2508.00852)

*Yuemin Ma, Uksang Yoo, Yunchao Yao, Shahram Najam Syed, Luca Bondi, Jonathan Francis, Jean Oh, Jeffrey Ichnowski*

**Main category:** cs.HC

**Keywords:** hand pose estimation, contact estimation, acoustic sensing, graph-based network, wearable technology

**Relevance Score:** 7

**TL;DR:** VibeMesh is a wearable system that combines vision and active acoustic sensing for accurate hand pose and contact estimation in various settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate hand pose and hand-object contact estimation is important for applications in robotics, virtual environments, and biomechanics but is hindered by visual occlusion and limitations of current sensing methods.

**Method:** VibeMesh integrates a bone-conduction speaker and piezoelectric microphones on the hand to emit acoustic signals and captures their propagation to estimate hand pose and contact using a graph-based attention network that processes audio and RGB-D data.

**Key Contributions:**

	1. A lightweight, non-intrusive visuo-acoustic sensing platform.
	2. A cross-modal graph network for joint pose and contact inference.
	3. A rich dataset with synchronized audio, RGB-D, and contact annotations.

**Result:** VibeMesh significantly improves hand pose and contact estimation accuracy and robustness compared to vision-only methods, especially in challenging scenarios like occlusion or static contacts.

**Limitations:** 

**Conclusion:** The developed system represents a breakthrough in non-intrusive sensing, enhancing the ability to interpret hand interactions in complex environments.

**Abstract:** Accurately estimating hand pose and hand-object contact events is essential for robot data-collection, immersive virtual environments, and biomechanical analysis, yet remains challenging due to visual occlusion, subtle contact cues, limitations in vision-only sensing, and the lack of accessible and flexible tactile sensing. We therefore introduce VibeMesh, a novel wearable system that fuses vision with active acoustic sensing for dense, per-vertex hand contact and pose estimation. VibeMesh integrates a bone-conduction speaker and sparse piezoelectric microphones, distributed on a human hand, emitting structured acoustic signals and capturing their propagation to infer changes induced by contact. To interpret these cross-modal signals, we propose a graph-based attention network that processes synchronized audio spectra and RGB-D-derived hand meshes to predict contact with high spatial resolution. We contribute: (i) a lightweight, non-intrusive visuo-acoustic sensing platform; (ii) a cross-modal graph network for joint pose and contact inference; (iii) a dataset of synchronized RGB-D, acoustic, and ground-truth contact annotations across diverse manipulation scenarios; and (iv) empirical results showing that VibeMesh outperforms vision-only baselines in accuracy and robustness, particularly in occluded or static-contact settings.

</details>


### [7] [EthicAlly: a Prototype for AI-Powered Research Ethics Support for the Social Sciences and Humanities](https://arxiv.org/abs/2508.00856)

*Steph Grohmann*

**Main category:** cs.HC

**Keywords:** Generative AI, Ethics support system, Social science, Humanities, Research Ethics Committee

**Relevance Score:** 6

**TL;DR:** This paper presents EthicAlly, an AI-powered ethics support system aimed at aiding social science and humanities researchers in ethical compliance and research design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in tailored ethics training and support in social sciences and humanities caused by the inadequacy of biomedical ethics models.

**Method:** The methodology involves developing a prototype, EthicAlly, utilizing constitutional AI technology and collaborative prompt development to provide structured ethics assessments.

**Key Contributions:**

	1. Introduction of EthicAlly as an AI-powered ethics assessment tool.
	2. Integration of universal and contextual ethical considerations into research support.
	3. Demonstrates the potential of AI to alleviate burdens on Research Ethics Committees.

**Result:** EthicAlly effectively incorporates universal ethics principles alongside contextual factors, offering support for ethical research design and aiding REC submissions, thereby alleviating pressures on RECs.

**Limitations:** 

**Conclusion:** The implementation of EthicAlly can enhance the ethical oversight in social sciences and humanities without automating human ethical review processes.

**Abstract:** In biomedical science, review by a Research Ethics Committee (REC) is an indispensable way of protecting human subjects from harm. However, in social science and the humanities, mandatory ethics compliance has long been met with scepticism as biomedical models of ethics can map poorly onto methodologies involving complex socio-political and cultural considerations. As a result, tailored ethics training and support as well as access to RECs with the necessary expertise is lacking in some areas, including parts of Europe and low- and middle-income countries. This paper suggests that Generative AI can meaningfully contribute to closing these gaps, illustrating this claim by presenting EthicAlly, a proof-of-concept prototype for an AI-powered ethics support system for social science and humanities researchers. Drawing on constitutional AI technology and a collaborative prompt development methodology, EthicAlly provides structured ethics assessment that incorporates both universal ethics principles and contextual and interpretive considerations relevant to most social science research. In supporting researchers in ethical research design and preparation for REC submission, this kind of system can also contribute to easing the burden on institutional RECs, without attempting to automate or replace human ethical oversight.

</details>


### [8] [Accessibility and Social Inclusivity: A Literature Review of Music Technology for Blind and Low Vision People](https://arxiv.org/abs/2508.00929)

*Shumeng Zhang, Raul Masu, Mela Bettega, Mingming Fan*

**Main category:** cs.HC

**Keywords:** Blind and Low Vision, Music Technology, Inclusive Design, Accessibility Research, Empirical Studies

**Relevance Score:** 4

**TL;DR:** Systematic literature review of music technology for blind and low vision (BLV) individuals categorizes existing studies and proposes insights for inclusive design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To organize knowledge on designing accessible technology for blind and low vision people, which has been lacking in systematic approaches.

**Method:** Literature review categorizing existing studies based on technology type and BLV people's research involvement.

**Key Contributions:**

	1. Systematic categorization of music technology for BLV individuals
	2. Identification of key trends in the design of music technology
	3. Proposal for inclusive real-world testing with the target group

**Result:** Identified six categories of BLV-oriented music technology and four key design trends. Highlighted the need for more empirical studies in real-world scenarios to enhance musical experiences.

**Limitations:** 

**Conclusion:** Emphasized a shift from 'accessible technology' to 'inclusive technology' for BLV individuals and proposed areas for collaborative testing and research.

**Abstract:** This paper presents a systematic literature review of music technology tailored for blind and low vision (BLV) individuals. Music activities can be particularly beneficial for BLV people. However, a systematic approach to organizing knowledge on designing accessible technology for BLV people has yet to be attempted. We categorize the existing studies based on the type of technology and the extent of BLV people's involvement in the research. We identify six main categories of BLV people-oriented music technology and highlight four key trends in design goals. Based on these categories, we propose four general insights focusing on (1) spatial awareness, (2) access to information, (3) (non-verbal) communication, and (4) memory. The identified trends suggest that more empirical studies involving BLV people in real-world scenarios are needed to ensure that technological advancements can enhance musical experiences and social inclusion. This research proposes collaborative music technology and inclusive real-world testing with the target group as two key areas missing in current research. They serve as a foundational step in shifting the focus from ``accessible technology'' to ``inclusive technology'' for BLV individuals within the broader field of accessibility research.

</details>


### [9] [How Long Does It Take to Alleviate Discomfort? A Preliminary Study on Reducing Cybersickness in Novice Users](https://arxiv.org/abs/2508.01070)

*Zhengxin Zhang, Shufang Qian, Yi Wang, Xiao Liu, Thuong Hoang, Chetan Arora, Jingjing Zhang, Henry Been Lirn Duh*

**Main category:** cs.HC

**Keywords:** cybersickness, locomotion tunneling, VR, novice users, user experience

**Relevance Score:** 6

**TL;DR:** This paper investigates the effects of prolonged use of locomotion tunneling on novice VR users to mitigate cybersickness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of research on the long-term effects of locomotion tunneling for mitigating cybersickness among novice VR users.

**Method:** The study involved 24 novice VR users over a week using VRChat, collecting five days of data to measure cybersickness levels and gather qualitative feedback.

**Key Contributions:**

	1. Investigates long-term effects of locomotion tunneling on novice users
	2. Findings suggest the need for adaptable mitigation strategies in VR
	3. Identified participant-perceived causes of cybersickness

**Result:** Participants showed significant reduction in cybersickness by Day 4, but encountered increased symptoms after a scene change on Day 5. Qualitative feedback revealed limitations in effectiveness of locomotion tunneling.

**Limitations:** Limited sample size and scope focused on a single platform (VRChat).

**Conclusion:** The study highlights the potential of locomotion tunneling to mitigate cybersickness but reveals its limitations in varying scenarios, necessitating further research.

**Abstract:** Cybersickness significantly impacts the user experience in VR applications. Locomotion tunneling is a widely adopted technique for mitigating cybersickness in susceptible users. However, there is a lack of research investigating the effects of prolonged use of locomotion tunneling among novice users. To fill this gap, we used VRChat as our experimental platform. We recruited 24 novice VR users, defined as participants with no prior experience using immersive virtual environments. We collected five days of data within a one-week period. The results indicated that participants exhibited significant mitigation to cybersickness by Day 4. However, a change in the VR scene on Day 5 led to a notable increase in cybersickness symptoms. Qualitative feedback revealed participant-perceived causes of cybersickness and suggested that the effectiveness of locomotion tunneling was limited in some scenarios. Finally, we discussed the limitations of the study and proposed directions for future research.

</details>


### [10] [DescribePro: Collaborative Audio Description with Human-AI Interaction](https://arxiv.org/abs/2508.01092)

*Maryam Cheema, Sina Elahimanesh, Samuel Martin, Pooyan Fazli, Hasti Seifi*

**Main category:** cs.HC

**Keywords:** Audio Description, AI-generated Content, Collaborative Tools, Human-Computer Interaction, Accessibility

**Relevance Score:** 8

**TL;DR:** DescribePro is a collaborative audio description authoring system that combines AI-generated content with manual editing, facilitating community collaboration and custom narrative styles for improved accessibility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of creating high-quality audio descriptions for blind and low vision users, highlighting the need for a balance between human and AI contributions.

**Method:** The authors developed DescribePro, a system that allows describers to refine AI-generated descriptions through iterative processes and community collaboration.

**Key Contributions:**

	1. Introduction of DescribePro for collaborative AD authoring
	2. Integration of AI-generated descriptions with manual editing
	3. Community collaboration features for narrative style exploration

**Result:** Evaluation with 18 describers indicated that AI support reduced repetitive work and allowed professionals to maintain stylistic choices while easing the cognitive load for novices.

**Limitations:** 

**Conclusion:** DescribePro demonstrates the effectiveness of AI-assisted tools in enhancing audio description authorship, offering customization options and supporting new describer training.

**Abstract:** Audio description (AD) makes video content accessible to millions of blind and low vision (BLV) users. However, creating high-quality AD involves a trade-off between the precision of human-crafted descriptions and the efficiency of AI-generated ones. To address this, we present DescribePro a collaborative AD authoring system that enables describers to iteratively refine AI-generated descriptions through multimodal large language model prompting and manual editing. DescribePro also supports community collaboration by allowing users to fork and edit existing ADs, enabling the exploration of different narrative styles. We evaluate DescribePro with 18 describers (9 professionals and 9 novices) using quantitative and qualitative methods. Results show that AI support reduces repetitive work while helping professionals preserve their stylistic choices and easing the cognitive load for novices. Collaborative tags and variations show potential for providing customizations, version control, and training new describers. These findings highlight the potential of collaborative, AI-assisted tools to enhance and scale AD authorship.

</details>


### [11] [Cross-Device Motion Interaction via Apple's Native System Frameworks](https://arxiv.org/abs/2508.01110)

*Ezequiel Santos*

**Main category:** cs.HC

**Keywords:** HCI, motion controller, tactile feedback, offline, mobile applications

**Relevance Score:** 8

**TL;DR:** An offline iPhone motion controller pipeline offers tactile feedback for low-latency applications in mobile HCI.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a low-latency, reproducible motion controller for mobile HCI research and applications without the need for internet connectivity.

**Method:** The system leverages native Apple frameworks (CoreMotion, MultipeerConnectivity, CoreHaptics) to sense motion, communicate peer-to-peer, and provide tactile feedback, logging end-to-end latency without clock synchronization.

**Key Contributions:**

	1. Development of an offline motion controller using iPhone with tactile feedback
	2. Validation through a real-time application demonstrating performance
	3. Open-source availability under MIT license

**Result:** Achieved a mean delay of 70.4 ms and 95th percentile latency below 74 ms on a typical 5 GHz Wi-Fi, with 21 participants reporting stable connections and no packet loss during a game demonstration.

**Limitations:** 

**Conclusion:** The pipeline is suited for rapid prototyping in mobile HCI, supporting casual games and educational tools, and is fully accessible with open-source code.

**Abstract:** We introduce an open-source, fully offline pipeline that transforms a consumer-grade iPhone into a motion controller with real-time tactile feedback, using only native Apple frameworks. Designed for rapid prototyping and applied mobile HCI scenarios, the system integrates CoreMotion for inertial sensing, MultipeerConnectivity for peer-to-peer data transmission at 10 Hz, and CoreHaptics for immediate tactile confirmation. A built-in logger captures end-to-end latency without requiring clock synchronization, yielding a mean delay of 70.4 ms and 95th percentile below 74 ms on typical 5 GHz Wi-Fi (-55 dBm RSSI). We validated the pipeline through a real-time demonstrator game, KeepCalm, deployed during a public event with 21 participants. Results showed stable connections, zero packet loss, and negligible power impact (24 mW on iPhone 13 mini). With fewer than 500 lines of Swift code and no reliance on cloud infrastructure, this system provides a compact, reproducible foundation for embodied interaction research, casual games, and offline educational tools. All source code, latency logs, and provisioning scripts are openly released under an MIT license.

</details>


### [12] [Presentation of Low-Frequency Vibration to the Face Using Amplitude Modulation](https://arxiv.org/abs/2508.01155)

*Yuma Akiba, Shota Nakayama, Keigo Ushiyama, Izumi Mizoguchi, Hiroyuki Kajimoto*

**Main category:** cs.HC

**Keywords:** haptic feedback, low-frequency vibration, amplitude modulation, facial perception, human-computer interaction

**Relevance Score:** 4

**TL;DR:** A method is proposed to present low-frequency vibrations to the face using amplitude modulation, producing clear sensations in various facial regions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore a new technique for delivering pure low-frequency vibration sensations to the face, which are poorly represented by existing commercial vibrators.

**Method:** The study employs amplitude modulation with a carrier frequency of approximately 200 Hz to enable the perception of low-frequency vibrations on the facial region.

**Key Contributions:**

	1. Introduction of a novel amplitude modulation technique for facial haptic feedback
	2. Demonstration of effective low-frequency vibration perception in facial areas
	3. Evaluation of subjective quality of vibration using experimental methodology

**Result:** Experiments confirmed that the proposed method successfully produces desired low-frequency perceptions on the forehead and across the face, with particular clarity around the eyes, cheeks, and lower lip.

**Limitations:** 

**Conclusion:** The innovative amplitude modulation technique can effectively deliver low-frequency vibrations to the facial area, which has implications for various applications in HCI and haptic feedback.

**Abstract:** This study proposes a method to present pure low-frequency vibration sensations to the face that cannot be presented by small commercially available vibrators. The core innovation lies in utilizing an amplitude modulation technique with a carrier frequency of approximately 200 Hz. Due to the absence of Pacinian corpuscles in the facial region - receptors responsible for detecting high-frequency vibrations around 200 Hz - only the original low-frequency signal is perceived. Three experiments were conducted. Experiments 1 and 2 were performed on the forehead to confirm that the proposed amplitude modulation method could produce the desired low-frequency perception and to evaluate the subjective quality of the vibration. The results suggested that the proposed method could produce the perception of desired pure low-frequency vibration when applied to the forehead. In Experiment 3, the proposed method was applied to the whole face, and its range of applicability was explored. The results indicated that the original low-frequency vibration was clearly perceptible around the eyes, cheeks, and lower lip area.

</details>


### [13] [RoboLinker: A Diffusion-model-based Matching Clothing Generator Between Humans and Companion Robots](https://arxiv.org/abs/2508.01165)

*Jing Tang, Qing Xiao, Kunxu Du, Zaiqiao Ye*

**Main category:** cs.HC

**Keywords:** RoboLinker, Generative Design, Human-Robot Interaction

**Relevance Score:** 3

**TL;DR:** RoboLinker is a generative design system that matches human outfits to robot attire using a diffusion-based model and an interactive interface.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to explore the aesthetic and emotional connection between humans and robots by creating visually coherent outfits that reflect their relationship.

**Method:** The authors employ a diffusion-based model that takes an image of a robot and a style prompt from users to generate a human outfit.

**Key Contributions:**

	1. Introduction of RoboLinker, a novel generative design system for human-robot attire matching.
	2. Demonstration of effective outfit generation for both humanoid and pet-like robots.
	3. User interactivity in refining design outputs to enhance personal relevance.

**Result:** RoboLinker demonstrates effective generation of outfits that complement both humanoid and pet-like robots, producing stylistically coherent and emotionally appealing results.

**Limitations:** The evaluation may be limited to a specific set of robot types and styles, and user preferences may vary widely.

**Conclusion:** The study highlights the potential for RoboLinker to enhance human-robot interaction through shared stylistic expressions in outfits.

**Abstract:** We present RoboLinker, a generative design system that creates matching outfits for humans and their robots. Using a diffusion-based model, the system takes a robot image and a style prompt from users as input, and outputs a human outfit that visually complements the robot's attire. Through an interactive interface, users can refine the generated designs. We evaluate RoboLinker with both humanoid and pet-like robots, demonstrating its capacity to produce stylistically coherent and emotionally resonant results.

</details>


### [14] [NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration](https://arxiv.org/abs/2508.01235)

*Yaxin Hu, Arissa J. Sato, Jingxin Du, Chenming Ye, Anjun Zhu, Pragathi Praveena, Bilge Mutlu*

**Main category:** cs.HC

**Keywords:** robotic telepresence, LLM, narrative guidance, remote exploration, human-robot interaction

**Relevance Score:** 9

**TL;DR:** This paper presents NarraGuide, a mobile robotic system that utilizes LLM capabilities for providing narrative guidance to users exploring remote locations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance navigation and situational awareness in robotic telepresence by integrating narrative capabilities to support exploration in unfamiliar places.

**Method:** A prototype system was developed and deployed in a geology museum, allowing remote participants to engage with the robot through a dialogue-based interface while touring the museum.

**Key Contributions:**

	1. Development of NarraGuide, a narrative-enabled robotic explorer
	2. Deployment and evaluation of the system in a real-world setting (geology museum)
	3. Insights into user engagement and preferences during remote tours.

**Result:** Remote participants (n=20) reported positive engagement with the robot, expressing preferences for interaction and demonstrating effective use of narrative guidance during exploration.

**Limitations:** 

**Conclusion:** The study shows that LLM-enabled robotic systems can significantly enrich the experience of exploring remote environments through tailored narrative guidance.

**Abstract:** Robotic telepresence enables users to navigate and experience remote environments. However, effective navigation and situational awareness depend on users' prior knowledge of the environment, limiting the usefulness of these systems for exploring unfamiliar places. We explore how integrating location-aware LLM-based narrative capabilities into a mobile robot can support remote exploration. We developed a prototype system, called NarraGuide, that provides narrative guidance for users to explore and learn about a remote place through a dialogue-based interface. We deployed our prototype in a geology museum, where remote participants (n=20) used the robot to tour the museum. Our findings reveal how users perceived the robot's role, engaged in dialogue in the tour, and expressed preferences for bystander encountering. Our work demonstrates the potential of LLM-enabled robotic capabilities to deliver location-aware narrative guidance and enrich the experience of exploring remote environments.

</details>


### [15] [ViseGPT: Towards Better Alignment of LLM-generated Data Wrangling Scripts and User Prompts](https://arxiv.org/abs/2508.01279)

*Jiajun Zhu, Xinyu Cheng, Zhongsu Luo, Yunfan Zhou, Xinhuan Shu, Di Weng, Yingcai Wu*

**Main category:** cs.HC

**Keywords:** Large Language Models, Debugging Efficiency, Human-Computer Interaction, Data Wrangling, User Experience

**Relevance Score:** 9

**TL;DR:** ViseGPT is a tool that generates test cases from user prompts to validate LLM-generated data wrangling scripts, improving debugging efficiency and user experience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance debugging efficiency and optimize user experience with LLM-generated scripts that may not fully meet user requirements.

**Method:** ViseGPT extracts constraints from user prompts to create test cases for script verification and displays results in a Gantt chart for intuitive user assessment.

**Key Contributions:**

	1. Development of ViseGPT tool for automatic constraint extraction and test case generation
	2. Usability improvements for debugging LLM-generated scripts
	3. Integration of Gantt charts for intuitive result assessment

**Result:** User studies demonstrate that ViseGPT significantly improves debugging efficiency and enhances users' ability to detect and correct issues in scripts.

**Limitations:** 

**Conclusion:** ViseGPT streamlines the workflow experience for users dealing with LLM-generated data wrangling scripts, promoting iterative refinement.

**Abstract:** Large language models (LLMs) enable the rapid generation of data wrangling scripts based on natural language instructions, but these scripts may not fully adhere to user-specified requirements, necessitating careful inspection and iterative refinement. Existing approaches primarily assist users in understanding script logic and spotting potential issues themselves, rather than providing direct validation of correctness. To enhance debugging efficiency and optimize the user experience, we develop ViseGPT, a tool that automatically extracts constraints from user prompts to generate comprehensive test cases for verifying script reliability. The test results are then transformed into a tailored Gantt chart, allowing users to intuitively assess alignment with semantic requirements and iteratively refine their scripts. Our design decisions are informed by a formative study (N=8) that explores user practices and challenges. We further evaluate the effectiveness and usability of ViseGPT through a user study (N=18). Results indicate that ViseGPT significantly improves debugging efficiency for LLM-generated data-wrangling scripts, enhances users' ability to detect and correct issues, and streamlines the workflow experience.

</details>


### [16] [ExplorAR: Assisting Older Adults to Learn Smartphone Apps through AR-powered Trial-and-Error with Interactive Guidance](https://arxiv.org/abs/2508.01282)

*Jiawei Li, Linjie Qiu, Zhiqing Wu, Qiongyan Chen, Ziyan Wang, Mingming Fan*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Augmented Reality, Older Adults, Trial-and-Error Learning, Smartphone Apps

**Relevance Score:** 9

**TL;DR:** ExplorAR is an AR-based trial-and-error learning system designed for older adults to improve their ability to use smartphone apps by providing real-time visual guidance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Older adults face challenges in learning new smartphone apps due to cognitive and physical changes; exploring how trial-and-error can be effectively designed for this demographic is crucial.

**Method:** The study developed an AR-based tool, ExplorAR, which provides situated visual guidance in augmented reality, enabling older adults to learn through trial-and-error.

**Key Contributions:**

	1. Development of ExplorAR, an AR-based trial-and-error learning system for older adults.
	2. Comparison of AR method with traditional video tutorials and simplified AR versions.
	3. Demonstration of enhanced cognitive engagement and confidence in learning for older adults.

**Result:** A user study involving 18 older adults demonstrated that ExplorAR significantly enhanced their learning experience compared to traditional video tutorials.

**Limitations:** The study was limited to a small sample size of 18 participants, which may affect the generalizability of results.

**Conclusion:** AR-supported trial-and-error learning fosters better cognitive engagement and increases confidence in using unfamiliar smartphone operations among older adults.

**Abstract:** Older adults tend to encounter challenges when learning to use new smartphone apps due to age-related cognitive and physical changes. Compared to traditional support methods such as video tutorials, trial-and-error allows older adults to learn to use smartphone apps by making and correcting mistakes. However, it remains unknown how trial-and-error should be designed to empower older adults to use smartphone apps and how well it would work for older adults. Informed by the guidelines derived from prior work, we designed and implemented ExplorAR, an AR-based trial-and-error system that offers real-time and situated visual guidance in the augmented space around the smartphone to empower older adults to explore and correct mistakes independently. We conducted a user study with 18 older adults to compare ExplorAR with traditional video tutorials and a simplified version of ExplorAR. Results show that the AR-supported trial-and-error method enhanced older adults' learning experience by fostering deeper cognitive engagement and improving confidence in exploring unknown operations.

</details>


### [17] [AffectGPT-R1: Leveraging Reinforcement Learning for Open-Vocabulary Emotion Recognition](https://arxiv.org/abs/2508.01318)

*Zheng Lian*

**Main category:** cs.HC

**Keywords:** multimodal, emotion recognition, reinforcement learning, large language models, open vocabulary

**Relevance Score:** 9

**TL;DR:** This paper presents AffectGPT-R1, a reinforcement learning framework for Open-Vocabulary Multimodal Emotion Recognition (OV-MER) that uses Group Relative Policy Optimization to directly optimize EW-based metrics instead of traditional token-level loss, demonstrating significant improvements in emotion recognition performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to enhance emotion recognition capabilities through an open-vocabulary approach that better aligns with human-like understanding of emotions and relevant evaluation metrics.

**Method:** AffectGPT-R1 employs reinforcement learning by treating emotion wheel (EW)-based metrics as a reward function and utilizes Group Relative Policy Optimization (GRPO) to maximize performance.

**Key Contributions:**

	1. Introduction of AffectGPT-R1 for OV-MER
	2. Use of GRPO to optimize EW-based metrics
	3. Demonstration of significant performance improvements on OV-MER tasks

**Result:** AffectGPT-R1 significantly improves performance on OV-MER benchmarks compared to previous methods that relied on token-level loss.

**Limitations:** 

**Conclusion:** The findings suggest that treating EW-based metrics as the reward function in reinforcement learning can effectively enhance multimodal emotion recognition.

**Abstract:** Open-Vocabulary Multimodal Emotion Recognition (OV-MER) aims to predict emotions without being constrained by predefined label spaces, enabling fine-grained and human-like emotion understanding. Unlike traditional discriminative methods, OV-MER leverages generative models, such as large language models (LLMs) with extensive vocabularies, to capture the full spectrum of emotions. Previous approaches (like AffectGPT) primarily rely on token-level loss for training. However, this objective does not align with the emotion wheel (EW)-based evaluation metrics used in OV-MER. Unfortunately, EW-based metrics cannot be directly optimized via gradient backpropagation. In this paper, we propose AffectGPT-R1, a reinforcement learning framework that directly optimizes performance on EW-based metrics. Specifically, we treat these metrics as the reward function and employ Group Relative Policy Optimization (GRPO) to maximize rewards. Experimental results demonstrate that AffectGPT-R1 achieves significant improvements on OV-MER. We hope this work advances the field of multimodal emotion recognition. Our code will be publicly available at:https://github.com/zeroQiaoba/AffectGPT.

</details>


### [18] [An Appraisal-Based Approach to Human-Centred Explanations](https://arxiv.org/abs/2508.01388)

*Rukshani Somarathna, Madhawa Perera, Tom Gedeon, Matt Adcock*

**Main category:** cs.HC

**Keywords:** explainable AI, human-centered design, cognitive science, high-stakes domains, healthcare

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel appraisal-based framework for generating explainable AI decisions that aligns with human cognitive processes, particularly in high-stakes domains like healthcare.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional explainability methods that fail to capture the cognitive processes underlying human decision making in AI systems.

**Method:** The proposed framework adapts the Component Process Model (CPM) from emotion research to generate explanations based on appraisal dimensions such as relevance, implications, coping potential, and normative significance.

**Key Contributions:**

	1. Introduction of an appraisal-based framework for explainability in AI.
	2. Application of the Component Process Model to generate cognitive explanations.
	3. Enhanced user trust through context-sensitive justifications for AI decisions.

**Result:** The framework provides context-sensitive and cognitively meaningful justifications for AI decisions, forming a new paradigm for human-centered explanations in AI-driven systems.

**Limitations:** 

**Conclusion:** Bridging cognitive science and explainable AI facilitates the generation of intuitive explanations, enhancing user understanding and trust in AI systems.

**Abstract:** Explainability remains a critical challenge in artificial intelligence (AI) systems, particularly in high stakes domains such as healthcare, finance, and decision support, where users must understand and trust automated reasoning. Traditional explainability methods such as feature importance and post-hoc justifications often fail to capture the cognitive processes that underlie human decision making, leading to either too technical or insufficiently meaningful explanations. We propose a novel appraisal based framework inspired by the Component Process Model (CPM) for explainability to address this gap. While CPM has traditionally been applied to emotion research, we use its appraisal component as a cognitive model for generating human aligned explanations. By structuring explanations around key appraisal dimensions such as relevance, implications, coping potential, and normative significance our framework provides context sensitive, cognitively meaningful justifications for AI decisions. This work introduces a new paradigm for generating intuitive, human-centred explanations in AI driven systems by bridging cognitive science and explainable AI.

</details>


### [19] [Unlocking Excellence: The Impact of Voucher Incentives on Cybersecurity Education](https://arxiv.org/abs/2508.01520)

*Jianhua Li, Shang Gao, Michelle Harvey, Trina Myers*

**Main category:** cs.HC

**Keywords:** voucher incentives, cybersecurity education, student motivation, portfolio-based assessment, career aspirations

**Relevance Score:** 4

**TL;DR:** This study examines the impact of industry voucher incentives on students' motivation in cybersecurity education, highlighting their influence on career aspirations and providing recommendations for implementation.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of leveraging voucher incentives to inspire students in cybersecurity education, addressing their impact on motivation and career aspirations.

**Method:** A 100% portfolio-based assessment strategy was adopted, allowing students to select target grades; vouchers were provided to students completing specific tasks to access industry certificate training materials and exams.

**Key Contributions:**

	1. Implemented a portfolio-based assessment strategy in cybersecurity education
	2. Demonstrated the impact of industry vouchers on students' career aspirations
	3. Provided insights and recommendations for integrating vouchers in ICT education

**Result:** Survey and interviews indicated a substantial positive influence of voucher incentives on students' career aspirations in cybersecurity education.

**Limitations:** 

**Conclusion:** The study recommends that institutions and researchers consider the adoption of voucher incentives in cybersecurity and broader ICT education to enhance student motivation and career readiness.

**Abstract:** While voucher incentives have been popular for primary and secondary schools, they are less used in higher education. In this study, we leverage industry voucher incentives to inspire students in cybersecurity education (CSE). We adopt a 100% portfolio-based assessment strategy, where students can freely select their target grades in the investigated unit. We purposely design one of the high distinction (HD) tasks to be obtaining an industry certificate and provide vouchers to those who can accomplish a predefined set of tasks before a midpoint. The voucher recipients will use the voucher to access the industry certificate training materials and sit the certificate exam for free. Passing the certificate exam is one of the conditions for gaining an HD grade. Our survey and interviews reveal a substantial influence of voucher incentives on students' career aspirations. In light of the findings, recommendations on adopting voucher incentives in CSE or broader ICT education are offered for institutions and researchers.

</details>


### [20] [Understanding Why ChatGPT Outperforms Humans in Visualization Design Advice](https://arxiv.org/abs/2508.01547)

*Yongsu Ahn, Nam Wook Kim*

**Main category:** cs.HC

**Keywords:** generative AI, data visualization, ChatGPT, user experience, human perception

**Relevance Score:** 8

**TL;DR:** Recent generative AI models, particularly ChatGPT-4, outperform humans in data visualization tasks due to superior rhetorical structure and knowledge breadth.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the reasons behind the superior performance of generative AI models compared to humans in data visualization knowledge tasks.

**Method:** Systematic comparative analysis of responses to visualization questions from two ChatGPT models and human outputs.

**Key Contributions:**

	1. Comparison of generative AI models with human outputs in data visualization
	2. Identification of strengths in rhetorical structure, knowledge breadth, and perceptual quality
	3. Implications for user experience enhancement using LLMs

**Result:** Both ChatGPT-3.5 and ChatGPT-4 were found to be generally favored over human responses, with strengths in coverage, breadth, and technical visualization feedback.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs can significantly enhance user experience in data visualization, impacting broader AI applications.

**Abstract:** This paper investigates why recent generative AI models outperform humans in data visualization knowledge tasks. Through systematic comparative analysis of responses to visualization questions, we find that differences exist between two ChatGPT models and human outputs over rhetorical structure, knowledge breadth, and perceptual quality. Our findings reveal that ChatGPT-4, as a more advanced model, displays a hybrid of characteristics from both humans and ChatGPT-3.5. The two models were generally favored over human responses, while their strengths in coverage and breadth, and emphasis on technical and task-oriented visualization feedback collectively shaped higher overall quality. Based on our findings, we draw implications for advancing user experiences based on the potential of LLMs and human perception over their capabilities, with relevance to broader applications of AI.

</details>


### [21] [How Many Times Do People Usually Experience Different Kinds of Stressors Each Day?](https://arxiv.org/abs/2508.01553)

*Sameer Neupane, Mithun Saha, David M. Almeida, Santosh Kumar*

**Main category:** cs.HC

**Keywords:** stressors, mental health, wearable technology, data analysis, health informatics

**Relevance Score:** 6

**TL;DR:** This paper develops models to estimate the frequency of daily stressors using data from a wearable-triggered study, revealing an average of 5.39 stressors per person daily.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve estimation of daily stressor frequencies for better mental health care insights compared to conventional assessment methods.

**Method:** The study analyzed stressor data from 68 participants over 100 days, using wearable-triggered prompts and asymptotic models to address sample sparsity.

**Key Contributions:**

	1. Development of asymptotic models for stressor frequency estimation
	2. Use of wearable technology for real-time stressor reporting
	3. Identification of key stressor categories and their frequencies

**Result:** Participants reported an average of 5.39 stressors per day, with the most common being work-related (1.76), health-related (0.59), and transportation-related (0.55).

**Limitations:** 

**Conclusion:** The findings provide a benchmark for understanding individual stress loads and can guide mental health treatment and interventions.

**Abstract:** Understanding how frequently people experience different kinds of daily stressors is crucial for interpreting stress exposure and informing mental health care. But it can't be directly estimated from current assessment methods, such as diaries, end-of-day interviews, and ecological momentary assessments (EMA), that use sparse sampling to limit participant burden, and a structured response format for uniformity. In this paper, we utilize stressor data collected in a 100-day field study with 68 participants that adopted wearable-triggered prompts and a freeform format to solicit stressors soon after they occurred, but limited its prompts to a small subset to keep the burden low. We develop asymptotic models to estimate the latent frequency of different kinds of real-life stressors that address sample sparsity and sampling bias. We find that people experience 5.39 stressors per day, on average. The top three are related to work (1.76/day), health (0.59/day), and transportation (0.55/day). These estimates offer a principled benchmark for interpreting individual stressor loads. They can also inform mental health care treatments and interventions by establishing population-level baselines.

</details>


### [22] [Examining the Effects of Human-Likeness of Avatars on Emotion Perception and Emotion Elicitation](https://arxiv.org/abs/2508.01743)

*Shiyao Zhang, Omar Faruk, Robert Porzel, Dennis Küster, Tanja Schultz, Hui Liu*

**Main category:** cs.HC

**Keywords:** avatar, human-likeness, emotion perception, Uncanny Valley, interaction design

**Relevance Score:** 7

**TL;DR:** This study explores how the human-likeness of avatars affects emotion perception, revealing that while highly humanoid avatars can elicit negative emotional responses, avatars with moderate human similarity can enhance positive perceptions of emotions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of avatar-based interactions online, understanding how avatar human-likeness influences emotional communication is increasingly relevant for effective interactions across various applications.

**Method:** The study examines the effects of different degrees of human-likeness in avatars on emotion perception, focusing on Ekman's six basic emotions.

**Key Contributions:**

	1. Identifies the link between avatar human-likeness and emotion perception.
	2. Demonstrates the Uncanny Valley effect in avatar interactions.
	3. Suggests practical implications for the use of avatars in business and counseling.

**Result:** High human-likeness avatars elicit negative emotional responses consistent with the Uncanny Valley effect, while moderate human-likeness avatars, like a raccoon and a shark, positively influence emotion perception.

**Limitations:** Does not address long-term effects of avatar use on emotional communication.

**Conclusion:** Human-likeness significantly affects emotion perception, and the cuteness of avatars also plays a crucial role in eliciting emotions, impacting their effectiveness in various applications.

**Abstract:** An increasing number of online interaction settings now provide the possibility to visually represent oneself via an animated avatar instead of a video stream. Benefits include protecting the communicator's privacy while still providing a means to express their individuality. In consequence, there has been a surge in means for avatar-based personalization, ranging from classic human representations to animals, food items, and more. However, using avatars also has drawbacks. Depending on the human-likeness of the avatar and the corresponding disparities between the avatar and the original expresser, avatars may elicit discomfort or even hinder effective nonverbal communication by distorting emotion perception. This study examines the relationship between the human-likeness of virtual avatars and emotion perception for Ekman's six "basic emotions". Research reveals that avatars with varying degrees of human-likeness have distinct effects on emotion perception. High human-likeness avatars, such as human avatars, tend to elicit more negative emotional responses from users, a phenomenon that is consistent with the concept of Uncanny Valley in aesthetics, which suggests that closely resembling humans can provoke negative emotional responses. Conversely, a raccoon avatar and a shark avatar, known as cuteness, which exhibit moderate human similarity in this study, demonstrate a positive influence on emotion perception. Our initial results suggest that the human-likeness of avatars is an important factor for emotion perception. The results from the follow-up study further suggest that the cuteness of avatars and their natural facial status may also play a significant role in emotion perception and elicitation. We discuss practical implications for strategically conveying specific human behavioral messages through avatars in multiple applications, such as business and counseling.

</details>


### [23] [HeadZoom: Hands-Free Zooming and Panning for 2D Image Navigation Using Head Motion](https://arxiv.org/abs/2508.01765)

*Kaining Zhang, Catarina Moreira, Pedro Belchior, Gun Lee, Mark Billinghurst, Joaquim Jorge*

**Main category:** cs.HC

**Keywords:** HeadZoom, hands-free interaction, 2D navigation, VR, user study

**Relevance Score:** 8

**TL;DR:** HeadZoom is a hands-free interaction technique that uses head movements for fluid navigation of 2D visual content, evaluated in a user study.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve hands-free navigation of 2D visual content in applications where physical interaction is limited by using head movements.

**Method:** A within-subjects user study comparing three interaction techniques: Static, Tilt Zoom, and Parallel Zoom across various metrics.

**Key Contributions:**

	1. Introduction of a new hands-free interaction technique (HeadZoom)
	2. Empirical evaluation of interaction techniques in a user study
	3. Demonstration of reduced head movement and lower perceived exertion with Parallel Zoom

**Result:** Parallel Zoom significantly reduced head movement and users had lower perceived exertion compared to Static and Tilt modes.

**Limitations:** 

**Conclusion:** HeadZoom enhances head-based 2D interaction in VR, enabling immersive and accessible systems for image exploration.

**Abstract:** We introduce \textit{HeadZoom}, a hands-free interaction technique for navigating two-dimensional visual content using head movements. The system enables fluid zooming and panning by only using real-time head tracking. It supports natural control in applications such as map exploration, radiograph inspection, and image browsing, particularly where physical interaction is limited. We evaluated HeadZoom in a within-subjects user study comparing three interaction techniques-Static, Tilt Zoom, and Parallel Zoom-across spatial, error, and subjective metrics. Results show that Parallel Zoom significantly reduced total head movement compared to Static and Tilt modes. Users reported significantly lower perceived exertion for Parallel Zoom, confirming its suitability for prolonged or precision-based tasks. By minimising movement demands while maintaining task effectiveness, HeadZoom advances the design of head-based 2D interaction in VR, creating new opportunities for immersive, accessible, and hands-free systems for image exploration.

</details>


### [24] [Sonify Anything: Towards Context-Aware Sonic Interactions in AR](https://arxiv.org/abs/2508.01789)

*Laura Schütz, Sasan Matinfar, Ulrich Eck, Daniel Roth, Nassir Navab*

**Main category:** cs.HC

**Keywords:** Augmented Reality, sound synthesis, context-aware, material recognition, sonic interactions

**Relevance Score:** 7

**TL;DR:** This paper proposes a framework for context-aware sound in Augmented Reality (AR) that uses computer vision methods to recognize real objects' materials, leading to more realistic sonic interactions.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** AR lacks natural sonic interactions due to the absence of physicality in virtual objects, resulting in incongruent multisensory experiences.

**Method:** The framework utilizes computer vision to recognize and segment materials of real objects and generates real-time, material-based sounds using physical modelling synthesis based on interaction dynamics.

**Key Contributions:**

	1. Introduction of a framework for generating context-aware sounds in AR
	2. Demonstration of improved realism in sonic interactions through material-based sounds
	3. Improved accuracy in distinguishing visually similar materials through sound differentiation

**Result:** User study results show that context-aware, material-based sounds significantly improve the realism of sonic interactions compared to generic sound effects, enhancing participants' ability to distinguish similar materials accurately and confidently.

**Limitations:** 

**Conclusion:** Context-aware, material-based sonic interactions in AR foster a stronger sense of realism, enhancing users' perception of their real-world surroundings.

**Abstract:** In Augmented Reality (AR), virtual objects interact with real objects. However, the lack of physicality of virtual objects leads to the absence of natural sonic interactions. When virtual and real objects collide, either no sound or a generic sound is played. Both lead to an incongruent multisensory experience, reducing interaction and object realism. Unlike in Virtual Reality (VR) and games, where predefined scenes and interactions allow for the playback of pre-recorded sound samples, AR requires real-time sound synthesis that dynamically adapts to novel contexts and objects to provide audiovisual congruence during interaction. To enhance real-virtual object interactions in AR, we propose a framework for context-aware sounds using methods from computer vision to recognize and segment the materials of real objects. The material's physical properties and the impact dynamics of the interaction are used to generate material-based sounds in real-time using physical modelling synthesis. In a user study with 24 participants, we compared our congruent material-based sounds to a generic sound effect, mirroring the current standard of non-context-aware sounds in AR applications. The results showed that material-based sounds led to significantly more realistic sonic interactions. Material-based sounds also enabled participants to distinguish visually similar materials with significantly greater accuracy and confidence. These findings show that context-aware, material-based sonic interactions in AR foster a stronger sense of realism and enhance our perception of real-world surroundings.

</details>


### [25] [When not to help: planning for lasting human-AI collaboration](https://arxiv.org/abs/2508.01837)

*Mark Steyvers, Lukas Mayer*

**Main category:** cs.HC

**Keywords:** AI Assistance, User Engagement, POMDP, Cognitive Modeling, Counterfactual Reasoning

**Relevance Score:** 8

**TL;DR:** This paper presents a cognitive modeling framework that uses POMDPs to optimize AI assistance timing and reduce user disengagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** AI systems need to find the right balance in offering assistance to keep users engaged without causing alert fatigue.

**Method:** The framework is based on Partially Observable Markov Decision Processes (POMDPs) and includes counterfactual reasoning to assess user performance and engagement over time.

**Key Contributions:**

	1. Introduction of a POMDP-based cognitive model for AI engagement
	2. Incorporation of counterfactual reasoning for user performance assessment
	3. Empirical demonstration of improved engagement through adaptive assistance strategies

**Result:** Simulations indicate that the framework significantly outperforms consistent assistance policies, improving both accuracy and user engagement.

**Limitations:** 

**Conclusion:** Effective communication strategies in AI can enhance long-term user interaction by balancing immediate assistance with the risk of engagement loss.

**Abstract:** AI systems and technologies that can interact with humans in real time face a communication dilemma: when to offer assistance and how frequently. Overly frequent or contextually redundant assistance can cause users to disengage, undermining the long-term benefits of AI assistance. We introduce a cognitive modeling framework based on Partially Observable Markov Decision Processes (POMDPs) that addresses this timing challenge by inferring a user's latent cognitive state related to AI engagement over time. Additionally, our framework incorporates reasoning about the long-term effects of AI assistance, explicitly aiming to avoid actions that could lead the human user to disengage or deactivate the AI. A key component of our approach is counterfactual reasoning: at each time step, the AI considers how well the user would perform independently and weighs the potential boost in performance against the risk of diminishing engagement with the AI. Through simulations, we show that this adaptive strategy significantly outperforms baseline policies in which assistance is always provided or never provided. Our results highlight the importance of balancing short-term decision accuracy with sustained user engagement, showing how communication strategies can be optimized to avoid alert fatigue while preserving the user's receptiveness to AI guidance.

</details>


### [26] [ChairPose: Pressure-based Chair Morphology Grounded Sitting Pose Estimation through Simulation-Assisted Training](https://arxiv.org/abs/2508.01850)

*Lala Shakti Swarup Ray, Vitor Fortes Rey, Bo Zhou, Paul Lukowicz, Sungho Suh*

**Main category:** cs.HC

**Keywords:** seated pose estimation, pressure sensing, human-computer interaction, healthcare, adaptive user interfaces

**Relevance Score:** 8

**TL;DR:** ChairPose is a novel seated pose estimation system that uses pressure sensing to estimate user posture without the constraints of chair geometry, aiming to improve ergonomics and interactive system design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address concerns around musculoskeletal health and ergonomics caused by prolonged seated activity and limitations of existing posture sensing methods.

**Method:** ChairPose utilizes a pressure sensing mattress and a two-stage generative model that integrates chair morphology into its inference process. It also features a physics-driven data augmentation pipeline for varying user postures.

**Key Contributions:**

	1. First full body, wearable free seated pose estimation system using pressure sensing.
	2. Incorporation of chair morphology into posture inference for improved accuracy.
	3. Physics-driven data augmentation for diverse posture simulation.

**Result:** ChairPose achieves a mean per joint position error of 89.4 mm in tests with unseen users and chairs, demonstrating effective generalization and accuracy in real world conditions.

**Limitations:** 

**Conclusion:** ChairPose shows promise for enhancing posture-aware interactive systems with applications in ergonomics, healthcare, and adaptive interfaces.

**Abstract:** Prolonged seated activity is increasingly common in modern environments, raising concerns around musculoskeletal health, ergonomics, and the design of responsive interactive systems. Existing posture sensing methods such as vision-based or wearable approaches face limitations including occlusion, privacy concerns, user discomfort, and restricted deployment flexibility. We introduce ChairPose, the first full body, wearable free seated pose estimation system that relies solely on pressure sensing and operates independently of chair geometry. ChairPose employs a two stage generative model trained on pressure maps captured from a thin, chair agnostic sensing mattress. Unlike prior approaches, our method explicitly incorporates chair morphology into the inference process, enabling accurate, occlusion free, and privacy preserving pose estimation. To support generalization across diverse users and chairs, we introduce a physics driven data augmentation pipeline that simulates realistic variations in posture and seating conditions. Evaluated across eight users and four distinct chairs, ChairPose achieves a mean per joint position error of 89.4 mm when both the user and the chair are unseen, demonstrating robust generalization to novel real world generalizability. ChairPose expands the design space for posture aware interactive systems, with potential applications in ergonomics, healthcare, and adaptive user interfaces.

</details>


### [27] [Implicit Search Intent Recognition using EEG and Eye Tracking: Novel Dataset and Cross-User Prediction](https://arxiv.org/abs/2508.01860)

*Mansi Sharma, Shuang Chen, Philipp Müller, Maurice Rekrut, Antonio Krüger*

**Main category:** cs.HC

**Keywords:** EEG, eye-tracking, search intent, visual search, cross-user prediction

**Relevance Score:** 6

**TL;DR:** This paper presents a novel approach to recognizing human search intents in visual tasks using EEG and eye-tracking data, overcoming limitations of previous methods.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve machine assistance in visual search tasks by accurately identifying human search intents (navigational vs informational).

**Method:** Introduced the first publicly available dataset for recognizing search intents based on EEG and eye-tracking measurements, and developed a cross-user prediction method.

**Key Contributions:**

	1. First publicly available EEG and eye-tracking dataset for search intent recognition
	2. New method for cross-user prediction of search intents
	3. Achieved competitive accuracy compared to existing methods

**Result:** Achieved 84.5% accuracy in predicting search intents through EEG and eye-tracking, comparable to within-user prediction accuracy.

**Limitations:** 

**Conclusion:** The new dataset and method provide significant flexibility and can better apply to real-world scenarios than previous approaches.

**Abstract:** For machines to effectively assist humans in challenging visual search tasks, they must differentiate whether a human is simply glancing into a scene (navigational intent) or searching for a target object (informational intent). Previous research proposed combining electroencephalography (EEG) and eye-tracking measurements to recognize such search intents implicitly, i.e., without explicit user input. However, the applicability of these approaches to real-world scenarios suffers from two key limitations. First, previous work used fixed search times in the informational intent condition -- a stark contrast to visual search, which naturally terminates when the target is found. Second, methods incorporating EEG measurements addressed prediction scenarios that require ground truth training data from the target user, which is impractical in many use cases. We address these limitations by making the first publicly available EEG and eye-tracking dataset for navigational vs. informational intent recognition, where the user determines search times. We present the first method for cross-user prediction of search intents from EEG and eye-tracking recordings and reach 84.5% accuracy in leave-one-user-out evaluations -- comparable to within-user prediction accuracy (85.5%) but offering much greater flexibility

</details>


### [28] [VidAnimator: User-Guided Stylized 3D Character Animation from Human Videos](https://arxiv.org/abs/2508.01878)

*Xinwu Ye, Jun-Hsiang Yao, Jielin Feng, Shuhong Mei, Xingyu Lan, Siming Chen*

**Main category:** cs.HC

**Keywords:** 3D character animation, motion transfer, interactive system, human videos, stylized animation

**Relevance Score:** 5

**TL;DR:** A framework to enable stylized 3D characters to mimic motion from human videos using a mixed-initiative and interactive system.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the time and effort required for animators to create stylized 3D character animations while enhancing realism.

**Method:** The system uses a single-view human video and a stylized 3D character to capture and transfer motion, coupled with interactive modules for user customization.

**Key Contributions:**

	1. Mixed-initiative framework for motion transfer to stylized 3D characters
	2. Interactive authoring tools for user customization
	3. Empirical evidence of natural animation generation from human videos

**Result:** The framework successfully generates natural stylized 3D character animations that resemble human motion as verified by a questionnaire study and case studies.

**Limitations:** 

**Conclusion:** The proposed system demonstrates significant utility in creating diverse and realistic character animations efficiently.

**Abstract:** With captivating visual effects, stylized 3D character animation has gained widespread use in cinematic production, advertising, social media, and the potential development of virtual reality (VR) non-player characters (NPCs). However, animating stylized 3D characters often requires significant time and effort from animators. We propose a mixed-initiative framework and interactive system to enable stylized 3D characters to mimic motion in human videos. The framework takes a single-view human video and a stylized 3D character (the target character) as input, captures the motion of the video, and then transfers the motion to the target character. In addition, it involves two interaction modules for customizing the result. Accordingly, the system incorporates two authoring tools that empower users with intuitive modification. A questionnaire study offers tangible evidence of the framework's capability of generating natural stylized 3D character animations similar to the motion in the video. Additionally, three case studies demonstrate the utility of our approach in creating diverse results.

</details>


### [29] [Anchoring and Alignment: Data Factors in Part-to-Whole Visualization](https://arxiv.org/abs/2508.01881)

*Connor Bailey, Michael Gleicher*

**Main category:** cs.HC

**Keywords:** data visualization, part-to-whole, perceptual mechanisms, value estimation, chart design

**Relevance Score:** 6

**TL;DR:** The paper investigates how data and design factors impact value estimation in part-to-whole visualizations like pie charts and stacked bar charts, emphasizing the importance of aligning data principles with perceptual mechanisms.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how part-to-whole representations affect value estimation and what design considerations enhance perceptual mechanisms in these visualizations.

**Method:** An online study was conducted to assess the impact of data and design factors such as value, position, and encoding on users' performance in estimating values from part-to-whole charts.

**Key Contributions:**

	1. Analysis of perceptual mechanisms in part-to-whole charting
	2. Empirical study on the effects of design factors in value estimation
	3. Recommendations for better visualization design based on study findings.

**Result:** The study found that salient values and alignment to scale positions significantly influence task performance in part-to-whole visualizations.

**Limitations:** 

**Conclusion:** Informed visualization design should be based on the interplay between data properties and design aspects, as they critically affect perceptual performance.

**Abstract:** We explore the effects of data and design considerations through the example case of part-to-whole data relationships. Standard part-to-whole representations like pie charts and stacked bar charts make the relationships of parts to the whole explicit. Value estimation in these charts benefits from two perceptual mechanisms: anchoring, where the value is close to a reference value with an easily recognized shape, and alignment where the beginning or end of the shape is aligned with a marker. In an online study, we explore how data and design factors such as value, position, and encoding together impact these effects in making estimations in part-to-whole charts. The results show how salient values and alignment to positions on a scale affect task performance. This demonstrates the need for informed visualization design based around how data properties and design factors affect perceptual mechanisms.

</details>


### [30] [IMUCoCo: Enabling Flexible On-Body IMU Placement for Human Pose Estimation and Activity Recognition](https://arxiv.org/abs/2508.01894)

*Haozhe Zhou, Riku Arakawa, Yuvraj Agarwal, Mayank Goel*

**Main category:** cs.HC

**Keywords:** IMU, motion sensing, pose estimation, activity recognition, human body

**Relevance Score:** 7

**TL;DR:** A novel framework called IMUCoCo enables flexible placement of IMUs for motion sensing by mapping signals from any body location into a unified space for activity recognition and pose estimation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of predefined sensor placements in IMU-based motion sensing, which restrict user experience and adaptability across various device configurations.

**Method:** IMUCoCo maps signals from a variable number of IMUs located anywhere on the human body into a unified feature space based on their spatial coordinates.

**Key Contributions:**

	1. Introduction of a novel framework for flexible IMU placement.
	2. Demonstration of accurate pose estimation across various sensor placements.
	3. Support for context-aware sensor placement suggestions.

**Result:** IMUCoCo demonstrates accurate pose estimation from diverse sensor placements, enhancing flexibility compared to traditional methods.

**Limitations:** 

**Conclusion:** The IMUCoCo framework allows for more customizable and context-dependent use of IMUs, potentially improving user experience and adaptability in motion sensing applications.

**Abstract:** IMUs are regularly used to sense human motion, recognize activities, and estimate full-body pose. Users are typically required to place sensors in predefined locations that are often dictated by common wearable form factors and the machine learning model's training process. Consequently, despite the increasing number of everyday devices equipped with IMUs, the limited adaptability has seriously constrained the user experience to only using a few well-explored device placements (e.g., wrist and ears). In this paper, we rethink IMU-based motion sensing by acknowledging that signals can be captured from any point on the human body. We introduce IMU over Continuous Coordinates (IMUCoCo), a novel framework that maps signals from a variable number of IMUs placed on the body surface into a unified feature space based on their spatial coordinates. These features can be plugged into downstream models for pose estimation and activity recognition. Our evaluations demonstrate that IMUCoCo supports accurate pose estimation in a wide range of typical and atypical sensor placements. Overall, IMUCoCo supports significantly more flexible use of IMUs for motion sensing than the state-of-the-art, allowing users to place their sensors-laden devices according to their needs and preferences. The framework also supports the ability to change device locations depending on the context and suggests placement depending on the use case.

</details>


### [31] [Effect of AI Performance, Risk Perception, and Trust on Human Dependence in Deepfake Detection AI system](https://arxiv.org/abs/2508.01906)

*Yingfan Zhou, Ester Chen, Manasa Pisipati, Aiping Xiong, Sarah Rajtmajer*

**Main category:** cs.HC

**Keywords:** synthetic data, AI trust, deepfake detection, human decision making, explainable AI

**Relevance Score:** 7

**TL;DR:** The study investigates how AI performance affects human trust and decision-making in identifying synthetic data, particularly in deepfake detection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing concerns about the malicious use of synthetic data and the trust issues arising from AI detection tools.

**Method:** An online experiment with 400 participants was conducted to assess how different levels of AI performance impact human trust in deepfake detection.

**Key Contributions:**

	1. Investigates the interplay between AI performance and human trust in deepfake detection.
	2. Provides empirical data on user behavior in risk assessment related to AI predictions.
	3. Offers insights for designing more transparent and user-friendly AI systems.

**Result:** Findings show that participants adjust their reliance on AI based on perceived risk and AI prediction results, indicating a complex relationship between AI performance and trust.

**Limitations:** The study's findings may be context-specific and may not generalize to all forms of synthetic data.

**Conclusion:** Improving the transparency and explainability of AI systems can enhance user support in mitigating risks associated with synthetic media.

**Abstract:** Synthetic images, audio, and video can now be generated and edited by Artificial Intelligence (AI). In particular, the malicious use of synthetic data has raised concerns about potential harms to cybersecurity, personal privacy, and public trust. Although AI-based detection tools exist to help identify synthetic content, their limitations often lead to user mistrust and confusion between real and fake content. This study examines the role of AI performance in influencing human trust and decision making in synthetic data identification. Through an online human subject experiment involving 400 participants, we examined how varying AI performance impacts human trust and dependence on AI in deepfake detection. Our findings indicate how participants calibrate their dependence on AI based on their perceived risk and the prediction results provided by AI. These insights contribute to the development of transparent and explainable AI systems that better support everyday users in mitigating the harms of synthetic media.

</details>


### [32] [Human Capital Visualization using Speech Amount during Meetings](https://arxiv.org/abs/2508.02075)

*Ekai Hashimoto, Takeshi Mizumoto, Kohei Nagira, Shun Shiramatsu*

**Main category:** cs.HC

**Keywords:** Human Capital, Speech Analysis, Conversation Visualization, Organizational Communication, Discourse Analysis

**Relevance Score:** 4

**TL;DR:** This study proposes strategies for visualizing human capital by quantifying speech amounts in routine meetings, highlighting the importance of conversations in enhancing organizational communication and fostering innovation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inadequacy of conventional quantification methods that only focus on measurable indicators, neglecting the critical role of conversations in human capital.

**Method:** The authors employed conversation visualization technology to quantify speech during routine meetings, analyzing differences in speech amounts based on gender, job post, and participant presence.

**Key Contributions:**

	1. Introduction of conversation visualization technology for analyzing speech in meetings
	2. Quantification of speech differences by participant attributes
	3. Insight into the interplay between speech patterns and organizational dynamics

**Result:** The analysis revealed significant differences in speech amounts associated with various attributes and organizational factors, indicating how communication patterns relate to human capital.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of conversation visualization technology in enhancing the understanding of human capital within organizations.

**Abstract:** In recent years, many companies have recognized the importance of human resources and are investing in human capital to revitalize their organizations and enhance internal communication, thereby fostering innovation. However, conventional quantification methods have mainly focused on readily measurable indicators without addressing the fundamental role of conversations in human capital. This study focuses on routine meetings and proposes strategies to visualize human capital by analyzing speech amount during these meetings. We employ conversation visualization technology, which operates effectively, to quantify speech. We then measure differences in speech amount by attributes such as gender and job post, changes in speech amount depending on whether certain participants are present, and correlations between speech amount and continuous attributes. To verify the effectiveness of our proposed methods, we analyzed speech amounts by departmental affiliation during weekly meetings at small to medium enterprises.

</details>


### [33] [Hierarchical MoE: Continuous Multimodal Emotion Recognition with Incomplete and Asynchronous Inputs](https://arxiv.org/abs/2508.02133)

*Yitong Zhu, Lei Han, GuanXuan Jiang, PengYuan Zhou, Yuyang Wang*

**Main category:** cs.HC

**Keywords:** multimodal emotion recognition, human-computer interaction, continuous emotion prediction

**Relevance Score:** 9

**TL;DR:** This paper introduces a Hierarchical Mixture-of-Experts (Hi-MoE) framework for robust continuous emotion prediction in human-computer interaction, addressing challenges like modality incompleteness and asynchrony.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve robustness in multimodal emotion recognition (MER), as existing methods struggle with dynamic modality issues and lack adaptability.

**Method:** The Hi-MoE framework employs a dual-layer expert structure that includes a Modality Expert Bank for handling missing modalities and an Emotion Expert Bank for fine-grained emotion representation, supported by a cross-modal alignment module.

**Key Contributions:**

	1. Introduction of the Hierarchical Mixture-of-Experts framework
	2. Dynamic handling of missing modalities through soft routing
	3. Improved fine-grained emotion representation using differential-attention routing

**Result:** The Hi-MoE framework shows state-of-the-art performance on the DEAP and DREAMER datasets in continuous emotion regression, demonstrating robustness against missing modalities and asynchrony.

**Limitations:** 

**Conclusion:** This research enhances the development of intelligent emotion systems that can adapt to complex real-world situations.

**Abstract:** Multimodal emotion recognition (MER) is crucial for human-computer interaction, yet real-world challenges like dynamic modality incompleteness and asynchrony severely limit its robustness. Existing methods often assume consistently complete data or lack dynamic adaptability. To address these limitations, we propose a novel Hi-MoE~(Hierarchical Mixture-of-Experts) framework for robust continuous emotion prediction. This framework employs a dual-layer expert structure. A Modality Expert Bank utilizes soft routing to dynamically handle missing modalities and achieve robust information fusion. A subsequent Emotion Expert Bank leverages differential-attention routing to flexibly attend to emotional prototypes, enabling fine-grained emotion representation. Additionally, a cross-modal alignment module explicitly addresses temporal shifts and semantic inconsistencies between modalities. Extensive experiments on benchmark datasets DEAP and DREAMER demonstrate our model's state-of-the-art performance in continuous emotion regression, showcasing exceptional robustness under challenging conditions such as dynamic modality absence and asynchronous sampling. This research significantly advances the development of intelligent emotion systems adaptable to complex real-world environments.

</details>


### [34] [EchoLadder: Progressive AI-Assisted Design of Immersive VR Scenes](https://arxiv.org/abs/2508.02173)

*Zhuangze Hou, Jingze Tian, Nianlong Li, Farong Ren, Can Liu*

**Main category:** cs.HC

**Keywords:** mixed reality, human-AI collaboration, spatial design, user creativity, large vision-language model

**Relevance Score:** 8

**TL;DR:** EchoLadder is a novel human-AI collaboration pipeline that enhances interactive scene modification in virtual reality by using large vision-language models to support novice users in spatial design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Novice users struggle with ideation and execution in spatial design on mixed reality platforms. Existing AI models lack interactive control, limiting users' iterative capabilities.

**Method:** The EchoLadder pipeline accepts verbal instructions at different levels of abstraction and spatial specificity, generating design suggestions that users can apply, regenerate, or retract.

**Key Contributions:**

	1. Introduction of EchoLadder for interactive scene modification in VR
	2. Use of LVLM for generating concrete design suggestions in spatial design
	3. Insights on user interaction strategies under AI assistance

**Result:** An ablation study demonstrated the effectiveness of EchoLadder components, while a user study revealed that it better supports user creativity in spatial design compared to a baseline without suggestions.

**Limitations:** 

**Conclusion:** EchoLadder enhances users' creative processes in spatial design and provides insights on iterative design strategies when assisted by AI, contributing valuable implications for future design systems.

**Abstract:** Mixed reality platforms allow users to create virtual environments, yet novice users struggle with both ideation and execution in spatial design. While existing AI models can automatically generate scenes based on user prompts, the lack of interactive control limits users' ability to iteratively steer the output. In this paper, we present EchoLadder, a novel human-AI collaboration pipeline that leverages large vision-language model (LVLM) to support interactive scene modification in virtual reality. EchoLadder accepts users' verbal instructions at varied levels of abstraction and spatial specificity, generates concrete design suggestions throughout a progressive design process. The suggestions can be automatically applied, regenerated and retracted by users' toggle control.Our ablation study showed effectiveness of our pipeline components. Our user study found that, compared to baseline without showing suggestions, EchoLadder better supports user creativity in spatial design. It also contributes insights on users' progressive design strategies under AI assistance, providing design implications for future systems.

</details>


### [35] [Data Augmentation for Visualization Design Knowledge Bases](https://arxiv.org/abs/2508.02216)

*Hyeok Kim, Jeffrey Heer*

**Main category:** cs.HC

**Keywords:** visualization, data augmentation, chart recommendation, feature weights, knowledge bases

**Relevance Score:** 6

**TL;DR:** The paper enhances visualization design knowledge bases by introducing data augmentation techniques to generate and label chart pairs, improving feature coverage and recommendation performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the coverage and accuracy of visualization knowledge bases by addressing the lack of comprehensive labeled chart pairs for evaluating design trade-offs.

**Method:** The authors propose data augmentation techniques to generate new chart pairs and compare various methods to efficiently scale labeling efforts for these pairs.

**Key Contributions:**

	1. Introduced data augmentation techniques for generating chart pairs
	2. Developed methods for efficient labeling of chart pairs
	3. Evaluated improvements in chart recommendation performance within the Draco knowledge base

**Result:** The study resulted in a significantly expanded corpus of chart pairs and demonstrated improved performance in chart recommendation within the Draco knowledge base.

**Limitations:** 

**Conclusion:** The proposed techniques enhance the knowledge base's ability to recommend designs by offering more representative data and learning updated feature weights.

**Abstract:** Visualization knowledge bases enable computational reasoning and recommendation over a visualization design space. These systems evaluate design trade-offs using numeric weights assigned to different features (e.g., binning a variable). Feature weights can be learned automatically by fitting a model to a collection of chart pairs, in which one chart is deemed preferable to the other. To date, labeled chart pairs have been drawn from published empirical research results; however, such pairs are not comprehensive, resulting in a training corpus that lacks many design variants and fails to systematically assess potential trade-offs. To improve knowledge base coverage and accuracy, we contribute data augmentation techniques for generating and labeling chart pairs. We present methods to generate novel chart pairs based on design permutations and by identifying under-assessed features -- leading to an expanded corpus with thousands of new chart pairs, now in need of labels. Accordingly, we next compare varied methods to scale labeling efforts to annotate chart pairs, in order to learn updated feature weights. We evaluate our methods in the context of the Draco knowledge base, demonstrating improvements to both feature coverage and chart recommendation performance.

</details>


### [36] [Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults](https://arxiv.org/abs/2508.02232)

*Lei Han, Mingnan Wei, Qiongyan Chen, Anqi Wang, Rong Pang, Kefei Liu, Rongrong Chen, David Yip*

**Main category:** cs.HC

**Keywords:** reminiscence, HCI, LLM, older adults, eye tracking

**Relevance Score:** 8

**TL;DR:** This paper explores a system called Eye2Recall that enhances photo-based reminiscence for older adults by integrating gaze and speech interactions to facilitate natural reminiscence conversations using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve well-being in older adults through technology-supported reminiscence, particularly exploring the integration of gaze and speech in LLM-powered conversations.

**Method:** The researchers conducted expert interviews to identify challenges faced by older adults and developed Eye2Recall, a system that combines eye tracking with natural language interaction, followed by a user study with ten participants to evaluate its effectiveness.

**Key Contributions:**

	1. Development of Eye2Recall integrating gaze and speech for reminiscence
	2. User study with older adults demonstrating the system's effectiveness
	3. Identification of design considerations for HCI systems aimed at older adults

**Result:** The study revealed valuable insights on design considerations for accessible reminiscence technologies and demonstrated the effectiveness of Eye2Recall in enhancing natural interactions for older adults.

**Limitations:** The study involved a small sample size of ten older adults, which may limit the generalizability of the findings.

**Conclusion:** The findings suggest that integrating technology like Eye2Recall can enhance reminiscence experiences in older adults, aligning better with their interaction patterns and supporting positive aging.

**Abstract:** Photo-based reminiscence has the potential to have a positive impact on older adults' reconnection with their personal history and improve their well-being. Supporting reminiscence in older adults through technological implementations is becoming an increasingly important area of research in the fields of HCI and CSCW. However, the impact of integrating gaze and speech as mixed-initiative interactions in LLM-powered reminiscence conversations remains under-explored. To address this, we conducted expert interviews to understand the challenges that older adults face with LLM-powered, photo-based reminiscence experiences. Based on these design considerations, we developed Eye2Recall, a system that integrates eye tracking for detecting visual interest with natural language interaction to create a mixed-initiative reminiscence experience. We evaluated its effectiveness through a user study involving ten older adults. The results have important implications for the future design of more accessible and empowering reminiscence technologies that better align with older adults' natural interaction patterns and enhance their positive aging.

</details>


### [37] [mCardiacDx: Radar-Driven Contactless Monitoring and Diagnosis of Arrhythmia](https://arxiv.org/abs/2508.02274)

*Arjun Kumar, Noppanat Wadlom, Jaeheon Kwak, Si-Hyuck Kang, Insik Shin*

**Main category:** cs.HC

**Keywords:** arrhythmia, contactless monitoring, radar, signal analysis, health informatics

**Relevance Score:** 8

**TL;DR:** The paper presents mCardiacDx, a novel radar-driven contactless system for monitoring arrhythmia through accurate signal analysis and heart pulse waveform reconstruction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in traditional arrhythmia monitoring methods that require specialized expertise and may cause patient discomfort.

**Method:** A radar-driven system using a precise target localization (PTL) technique and an encoder-decoder model to reconstruct heart pulse waveforms from reflected signals.

**Key Contributions:**

	1. Introduction of mCardiacDx for arrhythmia monitoring
	2. Development of precise target localization (PTL) technique
	3. Utilization of an encoder-decoder model for heart pulse waveform reconstruction

**Result:** mCardiacDx and the PTL technique significantly outperform state-of-the-art methods in both arrhythmia monitoring and diagnosis, showing improved accuracy in evaluating both arrhythmia patients and healthy subjects.

**Limitations:** 

**Conclusion:** The proposed system enhances the reliability and comfort of arrhythmia monitoring by enabling accurate contactless signal analysis.

**Abstract:** Arrhythmia is a common cardiac condition that can precipitate severe complications without timely intervention. While continuous monitoring is essential for timely diagnosis, conventional approaches such as electrocardiogram and wearable devices are constrained by their reliance on specialized medical expertise and patient discomfort from their contact nature. Existing contactless monitoring, primarily designed for healthy subjects, face significant challenges when analyzing reflected signals from arrhythmia patients due to disrupted spatial stability and temporal consistency.   In this paper, we introduce mCardiacDx, a radar-driven contactless system that accurately analyzes reflected signals and reconstructs heart pulse waveforms for arrhythmia monitoring and diagnosis. The key contributions of our work include a novel precise target localization (PTL) technique that locates reflected signals despite spatial disruptions, and an encoder-decoder model that transforms these signals into HPWs, addressing temporal inconsistencies. Our evaluation on a large dataset of healthy subjects and arrhythmia patients shows that both mCardiacDx and PTL outperform state-of-the-art approach in arrhythmia monitoring and diagnosis, also demonstrating improved performance in healthy subjects.

</details>


### [38] [Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits](https://arxiv.org/abs/2508.02328)

*Raj Mahmud, Shlomo Berkovsky, Mukesh Prasad, A. Baki Kocaballi*

**Main category:** cs.HC

**Keywords:** Conversational Recommender Systems, User Preferences, Dialogue Systems, User Modeling, Adaptive Interaction

**Relevance Score:** 8

**TL;DR:** The study investigates user preferences in Conversational Recommender Systems (CRSs) through a multi-turn dialogue, identifying key system qualities that influence user choices.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** To explore factors shaping user interaction preferences in CRSs, an area that has been underexplored despite the growing importance of personalized recommendations.

**Method:** A within-subjects study with 139 participants, who experienced two scripted CRS dialogues and rated aspects of their experiences while indicating preferences across eight system qualities. Logistic regression and clustering analyses were employed to interpret the data.

**Key Contributions:**

	1. Identified key system qualities influencing user preferences in CRSs
	2. Developed a predictive framework for user modeling in conversational AI
	3. Revealed distinct user profiles that inform dialogue design

**Result:** Key predictors for preference towards exploratory interaction included enjoyment, usefulness, novelty, conversational quality, and perceived effectiveness. Five distinct latent user profiles were identified based on dialogue style preferences, influenced by age, gender, and control preference.

**Limitations:** Study limited to scripted dialogues and a specific participant demographic; findings may not generalize to all user groups or types of CRSs.

**Conclusion:** The findings enhance understanding of user preferences in CRSs, suggesting a framework for designing dialogues that adapt to user needs by integrating affective and trait-level predictors.

**Abstract:** Conversational Recommender Systems (CRSs) deliver personalised recommendations through multi-turn natural language dialogue and increasingly support both task-oriented and exploratory interactions. Yet, the factors shaping user interaction preferences remain underexplored. In this within-subjects study (\(N = 139\)), participants experienced two scripted CRS dialogues, rated their experiences, and indicated the importance of eight system qualities. Logistic regression revealed that preference for the exploratory interaction was predicted by enjoyment, usefulness, novelty, and conversational quality. Unexpectedly, perceived effectiveness was also associated with exploratory preference. Clustering uncovered five latent user profiles with distinct dialogue style preferences. Moderation analyses indicated that age, gender, and control preference significantly influenced these choices. These findings integrate affective, cognitive, and trait-level predictors into CRS user modelling and inform autonomy-sensitive, value-adaptive dialogue design. The proposed predictive and adaptive framework applies broadly to conversational AI systems seeking to align dynamically with evolving user needs.

</details>


### [39] [Six Guidelines for Trustworthy, Ethical and Responsible Automation Design](https://arxiv.org/abs/2508.02371)

*Matouš Jelínek, Nadine Schlicker, Ewart de Visser*

**Main category:** cs.HC

**Keywords:** trustworthiness, automated systems, human-computer interaction, design guidelines, ethical design

**Relevance Score:** 8

**TL;DR:** The paper proposes six design guidelines to optimize trustworthiness assessments in automated systems, promoting safe human-automation interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate trustworthiness assessment is vital for users to rely on automated systems appropriately, ensuring that their trust aligns with the system's actual performance.

**Method:** The guidelines are developed from literature across multiple fields, incorporating principles from pragmatics, cognitive psychology, and ethics to enhance user-system interactions.

**Key Contributions:**

	1. Six design guidelines for trustworthiness assessment in automation
	2. Integration of pragmatics principles for user-system communication
	3. Framework for evaluating existing systems' trustworthiness capabilities

**Result:** The guidelines aim to provide actionable insights for designers, fostering calibrated trust and improving user satisfaction and safety in interactions with automated systems.

**Limitations:** 

**Conclusion:** Implementing the proposed design guidelines can enhance the trust users have in automated systems by ensuring better alignment between perceived and actual trustworthiness.

**Abstract:** Calibrated trust in automated systems (Lee and See 2004) is critical for their safe and seamless integration into society. Users should only rely on a system recommendation when it is actually correct and reject it when it is factually wrong. One requirement to achieve this goal is an accurate trustworthiness assessment, ensuring that the user's perception of the system's trustworthiness aligns with its actual trustworthiness, allowing users to make informed decisions about the extent to which they can rely on the system (Schlicker et al. 2022). We propose six design guidelines to help designers optimize for accurate trustworthiness assessments, thus fostering ethical and responsible human-automation interactions. The proposed guidelines are derived from existing literature in various fields, such as human-computer interaction, cognitive psychology, automation research, user-experience design, and ethics. We are incorporating key principles from the field of pragmatics, specifically the cultivation of common ground (H. H. Clark 1996) and Gricean communication maxims (Grice 1975). These principles are essential for the design of automated systems because the user's perception of the system's trustworthiness is shaped by both environmental contexts, such as organizational culture or societal norms, and by situational context, including the specific circumstances or scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed guidelines provide actionable insights for designers to create automated systems that make relevant trustworthiness cues available. This would ideally foster calibrated trust and more satisfactory, productive, and safe interactions between humans and automated systems. Furthermore, the proposed heuristics might work as a tool for evaluating to what extent existing systems enable users to accurately assess a system's trustworthiness.

</details>


### [40] [Talking Surveys: How Photorealistic Embodied Conversational Agents Shape Response Quality, Engagement, and Satisfaction](https://arxiv.org/abs/2508.02376)

*Matus Krajcovic, Peter Demcak, Eduard Kuric*

**Main category:** cs.HC

**Keywords:** Embodied Conversational Agents, Human-Computer Interaction, Online Surveys, User Engagement, AI

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of embodied conversational agents (ECAs) on participant engagement, satisfaction, and response quality in online surveys, proposing a method to incorporate virtual avatars into the survey process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of personal engagement and perceived accountability in online surveys can lead to issues like careless responding and satisficing. ECAs could improve participant interaction.

**Method:** A between-subjects study with 80 participants, comparing responses from those interacting with a voice-based agent with an animated video avatar to those interacting with a traditional chatbot, collecting 2,265 conversation responses.

**Key Contributions:**

	1. Development of an instrument for incorporating ECAs into surveys
	2. Demonstrated significant improvement in response quality and engagement
	3. Provided qualitative insights on participant satisfaction and interactions

**Result:** Statistical analysis showed that embodied agents led to more informative and detailed responses, with higher yet more time-efficient engagement overall, despite no significant change in reported satisfaction.

**Limitations:** Study focused on a specific participant demographic; effectiveness may vary across different populations and contexts.

**Conclusion:** The findings suggest that ECAs can enhance the online survey experience, resembling natural interactions that could foster better response quality, warranting further development of human-like agents.

**Abstract:** Embodied conversational agents (ECAs) are increasingly more realistic and capable of dynamic conversations. In online surveys, anthropomorphic agents could help address issues like careless responding and satisficing, which originate from the lack of personal engagement and perceived accountability. However, there is a lack of understanding of how ECAs in user experience research may affect participant engagement, satisfaction, and the quality of responses. As a proof of concept, we propose an instrument that enables the incorporation of conversations with a virtual avatar into surveys, using on AI-driven video generation, speech recognition, and Large Language Models. In our between-subjects study, 80 participants (UK, stratified random sample of general population) either talked to a voice-based agent with an animated video avatar, or interacted with a chatbot. Across surveys based on two self-reported psychometric tests, 2,265 conversation responses were obtained. Statistical comparison of results indicates that embodied agents can contribute significantly to more informative, detailed responses, as well as higher yet more time-efficient engagement. Furthermore, qualitative analysis provides valuable insights for causes of no significant change to satisfaction, linked to personal preferences, turn-taking delays and Uncanny Valley reactions. These findings support the pursuit and development of new methods toward human-like agents for the transformation of online surveys into more natural interactions resembling in-person interviews.

</details>


### [41] [Improving Knowledge Graph Understanding with Contextual Views](https://arxiv.org/abs/2508.02413)

*Antrea Christou, Cogan Shimizu*

**Main category:** cs.HC

**Keywords:** knowledge graphs, ontologies, data visualization, HCI, interactive tools

**Relevance Score:** 6

**TL;DR:** Introducing the Interactive Knowledge (InK) Browser tool for improved navigation and visualization of knowledge graphs utilizing ontological information.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of navigating and visualizing graph data, especially in knowledge graphs with numerous connections, necessitates improved tools that leverage schema information.

**Method:** The InK Browser employs modular views to present knowledge graph information, including interactive schema views, type-based data listings, neighborhood connections, and geospatial representations when applicable.

**Key Contributions:**

	1. Development of the Interactive Knowledge (InK) Browser tool
	2. Utilization of ontological information for enhanced graph navigation and visualization
	3. Evaluation of user experiences and scalability of tool features

**Result:** User evaluations indicated that the InK Browser effectively enhances the navigation and exploration of knowledge graphs by offering diverse perspectives and flexible views.

**Limitations:** Limited user sample size in initial evaluation, further studies needed to validate findings.

**Conclusion:** The findings suggest that incorporating ontological information can significantly improve the user experience in exploring knowledge graphs, making them more accessible for various applications.

**Abstract:** Navigating, visualizing, and discovery in graph data is frequently a difficult prospect. This is especially true for knowledge graphs (KGs), due to high number of possible labeled connections to other data.   However, KGs are frequently equipped with an ontology as a schema. That is, it informs how the relationships between data may be constrained. This additional information can be leveraged to improve how (knowledge) graph data can be navigated, visualized, or otherwise utilized in a discovery process.   In this manuscript, we introduce the Interactive Knowledge (InK) Browser. This tool specifically takes advantage ontological information (i.e., knowledge) when found in KGs. Specifically, we use modular views that provide various perspectives over the graph, including an interactive schema view, data listings based on type, neighborhood connections, and geospatial depiction (where appropriate). For this manuscript, we have evaluated the basic premise of this tool over a user group ($n= With this grown user survey, we continue to evaluate how scalable tools, including flexible views, can make KG exploration easier for a range of applications.)

</details>


### [42] [AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration](https://arxiv.org/abs/2508.02470)

*Hyunjn An, Yongwon Kim, Wonduk Seo, Joonil Park, Daye Kang, Changhoon Oh, Dokyun Kim, Seunghyun Lee*

**Main category:** cs.HC

**Keywords:** no-code platform, natural language input, AI service design

**Relevance Score:** 8

**TL;DR:** AIAP is a no-code platform that simplifies AI service design by allowing natural language input and improving user workflows.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Non-experts struggle with expressing intent and managing complexity in AI design. AIAP addresses this by providing a user-friendly platform.

**Method:** AIAP integrates natural language processing with visual workflows, employing a multi-agent system to break down user instructions into manageable steps.

**Key Contributions:**

	1. Introduction of a no-code platform for AI service design
	2. Integration of natural language input with visual workflows
	3. Demonstrated effectiveness through user studies

**Result:** A user study with 32 participants showed significant improvements in developing services, indicating that AIAP effectively enhances user experience.

**Limitations:** 

**Conclusion:** Natural language-based visual programming reduces barriers for non-experts in AI service design, leading to better usability.

**Abstract:** While many tools are available for designing AI, non-experts still face challenges in clearly expressing their intent and managing system complexity. We introduce AIAP, a no-code platform that integrates natural language input with visual workflows. AIAP leverages a coordinated multi-agent system to decompose ambiguous user instructions into modular, actionable steps, hidden from users behind a unified interface. A user study involving 32 participants showed that AIAP's AI-generated suggestions, modular workflows, and automatic identification of data, actions, and context significantly improved participants' ability to develop services intuitively. These findings highlight that natural language-based visual programming significantly reduces barriers and enhances user experience in AI service design.

</details>


### [43] [Stakeholder Perspectives on Humanistic Implementation of Computer Perception in Healthcare: A Qualitative Study](https://arxiv.org/abs/2508.02550)

*Kristin M. Kostick-Quenet, Meghan E. Hurley, Syed Ayaz, John Herrington, Casey Zampella, Julia Parish-Morris, Birkan Tunç, Gabriel Lázaro-Muñoz, J. S. Blumenthal-Barby, Eric A. Storch*

**Main category:** cs.HC

**Keywords:** computer perception, healthcare, stakeholder perspectives, privacy, personalized roadmaps

**Relevance Score:** 9

**TL;DR:** This paper explores the stakeholder perspectives on the integration of computer perception technologies in healthcare, addressing concerns about privacy, bias, and the human element of patient care.

**Read time:** 65 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the perceived risks, benefits, and challenges of computer perception technologies in healthcare from various stakeholders is essential to improve implementation while maintaining the quality of care.

**Method:** In-depth, semi-structured interviews were conducted with 102 stakeholders, including patients, caregivers, clinicians, technology developers, and scholars, followed by thematic analysis of transcripts.

**Key Contributions:**

	1. First evidence-based account of key stakeholder perspectives on CP technologies
	2. Identification of seven concern domains related to CP integration in healthcare
	3. Proposal of 'personalized roadmaps' as a practical framework for sustaining humanistic care

**Result:** Seven interlocking concern domains were identified: trustworthiness, relevance, utility, regulation, privacy, patient harms, and philosophical critiques.

**Limitations:** 

**Conclusion:** The paper proposes 'personalized roadmaps' to guide the implementation of computer perception technologies, ensuring that patient care remains humanistic.

**Abstract:** Computer perception (CP) technologies (digital phenotyping, affective computing and related passive sensing approaches) offer unprecedented opportunities to personalize healthcare, but provoke concerns about privacy, bias and the erosion of empathic, relationship-centered practice. A comprehensive understanding of perceived risks, benefits, and implementation challenges from those who design, deploy and experience these tools in real-world settings remains elusive. This study provides the first evidence-based account of key stakeholder perspectives on the relational, technical, and governance challenges raised by the integration of CP technologies into patient care. We conducted in-depth, semi-structured interviews with 102 stakeholders: adolescent patients and their caregivers, frontline clinicians, technology developers, and ethics, legal, policy or philosophy scholars. Transcripts underwent thematic analysis by a multidisciplinary team; reliability was enhanced through double coding and consensus adjudication. Stakeholders articulated seven interlocking concern domains: (1) trustworthiness and data integrity; (2) patient-specific relevance; (3) utility and workflow integration; (4) regulation and governance; (5) privacy and data protection; (6) direct and indirect patient harms; and (7) philosophical critiques of reductionism. To operationalize humanistic safeguards, we propose "personalized roadmaps": co-designed plans that predetermine which metrics will be monitored, how and when feedback is shared, thresholds for clinical action, and procedures for reconciling discrepancies between algorithmic inferences and lived experience. By translating these insights into personalized roadmaps, we offer a practical framework for developers, clinicians and policymakers seeking to harness continuous behavioral data while preserving the humanistic core of care.

</details>


### [44] [Teaching Critical Visualization: A Field Report](https://arxiv.org/abs/2508.02592)

*Andrew McNutt, Shiyi He, Sujit Kumar Kamaraj, Purbid Bambroo, Nastaran Jadidi, John Bovard, Chang Han*

**Main category:** cs.HC

**Keywords:** Critical Visualization, visualization pedagogy, critical thinking, educational methods, scavenger hunt

**Relevance Score:** 4

**TL;DR:** A report on an experimental course in Critical Visualization that successfully developed students' critical thinking and communication skills through a scavenger hunt format.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of academic courses in Critical Visualization and improve pedagogy in this field.

**Method:** An experimental course structure was implemented, using a scavenger hunt format to teach critical visualization concepts.

**Key Contributions:**

	1. First exploration of scavenger hunt format for teaching Critical Visualization
	2. Successful attainment of learning objectives in critical thinking and communication
	3. Highlighting the importance of humanistic criticality in education

**Result:** The course met its learning objectives by enhancing critical thinking skills, communication about complex ideas, and understanding of relevant theories.

**Limitations:** Improvements needed in course structure and content delivery.

**Conclusion:** The experience indicates a need for deeper integration of humanistic notions of criticality in visualization education despite areas for improvement.

**Abstract:** Critical Visualization is gaining popularity and academic focus, yet relatively few academic courses have been offered to support students in this complex area. This experience report describes a recent experimental course on the topic, exploring both what the topic could be as well as an experimental content structure (namely as scavenger hunt). Generally the course was successful, achieving the learning objectives of developing critical thinking skills, improving communication about complex ideas, and developing a knowledge about theories in the area. While improvements can be made, we hope that humanistic notions of criticality are embraced more deeply in visualization pedagogy.

</details>


### [45] [Explainable AI for Automated User-specific Feedback in Surgical Skill Acquisition](https://arxiv.org/abs/2508.02593)

*Catalina Gomez, Lalithkumar Seenivasan, Xinrui Zou, Jeewoo Yoon, Sirui Chu, Ariel Leong, Patrick Kramer, Yu-Chun Ku, Jose L. Porras, Alejandro Martin-Gomez, Masaru Ishii, Mathias Unberath*

**Main category:** cs.HC

**Keywords:** surgical training, explainable AI, automated feedback, skill acquisition, cognitive load

**Relevance Score:** 8

**TL;DR:** This paper explores the effectiveness of explainable AI (XAI)-generated feedback in surgical training, comparing it to traditional coaching methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses limitations in traditional surgical skill acquisition due to reliance on expert feedback and the need for personalized, objective assessments.

**Method:** The authors developed a simulation-based training framework utilizing XAI to analyze surgical videos, providing automated, user-specific feedback based on expert benchmarks.

**Key Contributions:**

	1. Development of a simulation-based framework using XAI for surgical training
	2. Comparison of XAI feedback with traditional coaching methods
	3. Demonstration of improved cognitive load and confidence in trainees

**Result:** The intervention improved cognitive load and confidence among trainees, with trends indicating that participants in the XAI group closely mimicked expert practices, despite no significant differences in performance gap reduction.

**Limitations:** No significant performance gap reduction was observed between feedback methods.

**Conclusion:** The findings support the potential of explainable AI in surgical education and emphasize the need for adaptive, data-driven feedback mechanisms.

**Abstract:** Traditional surgical skill acquisition relies heavily on expert feedback, yet direct access is limited by faculty availability and variability in subjective assessments. While trainees can practice independently, the lack of personalized, objective, and quantitative feedback reduces the effectiveness of self-directed learning. Recent advances in computer vision and machine learning have enabled automated surgical skill assessment, demonstrating the feasibility of automatic competency evaluation. However, it is unclear whether such Artificial Intelligence (AI)-driven feedback can contribute to skill acquisition. Here, we examine the effectiveness of explainable AI (XAI)-generated feedback in surgical training through a human-AI study. We create a simulation-based training framework that utilizes XAI to analyze videos and extract surgical skill proxies related to primitive actions. Our intervention provides automated, user-specific feedback by comparing trainee performance to expert benchmarks and highlighting deviations from optimal execution through understandable proxies for actionable guidance. In a prospective user study with medical students, we compare the impact of XAI-guided feedback against traditional video-based coaching on task outcomes, cognitive load, and trainees' perceptions of AI-assisted learning. Results showed improved cognitive load and confidence post-intervention. While no differences emerged between the two feedback types in reducing performance gaps or practice adjustments, trends in the XAI group revealed desirable effects where participants more closely mimicked expert practice. This work encourages the study of explainable AI in surgical education and the development of data-driven, adaptive feedback mechanisms that could transform learning experiences and competency assessment.

</details>


### [46] [PunchPulse: A Physically Demanding Virtual Reality Boxing Game Designed with, for and by Blind and Low-Vision Players](https://arxiv.org/abs/2508.02610)

*Sanchita S. Kamath, Omar Khan, Anurag Choudhary, Jan Meyerhoff-Liang, Soyoung Choi, JooYoung Seo*

**Main category:** cs.HC

**Keywords:** Blind and low-vision, Virtual reality, Physical activity, Exergame, Co-design

**Relevance Score:** 8

**TL;DR:** PunchPulse is a VR boxing exergame aimed at increasing physical activity in blind and low-vision individuals by providing engaging sensory feedback and promoting spatial navigation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Blind and low-vision individuals face barriers to engaging in physical activity due to a lack of accessible exercise options and the limitations of existing auditory-based solutions.

**Method:** The study utilized a multi-phased iterative design involving three BLV co-designers, with evaluations conducted through qualitative observations and quantitative measures of physical exertion and gameplay performance across different user study participants.

**Key Contributions:**

	1. Introduction of PunchPulse, a VR boxing exergame for BLV individuals.
	2. Iterative co-design process with BLV users to refine the game.
	3. Demonstration of effective engagement and physical exertion in BLV users.

**Result:** Participants reached moderate to vigorous physical activity thresholds, demonstrating high levels of immersion and engagement while using PunchPulse.

**Limitations:** The study was limited to a small sample size and specific user group; broader applicability and longer-term effectiveness of the game need further exploration.

**Conclusion:** PunchPulse illustrates the potential of virtual reality as an inclusive tool to enhance physical activity in the blind and low-vision community, filling a significant gap in exercise intervention accessibility.

**Abstract:** Blind and low-vision (BLV) individuals experience lower levels of physical activity (PA) compared to sighted peers due to a lack of accessible, engaging exercise options. Existing solutions often rely on auditory cues but do not fully integrate rich sensory feedback or support spatial navigation, limiting their effectiveness. This study introduces PunchPulse, a virtual reality (VR) boxing exergame designed to motivate BLV users to reach and sustain moderate to vigorous physical activity (MVPA) levels. Over a seven-month, multi-phased study, PunchPulse was iteratively refined with three BLV co-designers, informed by two early pilot testers, and evaluated by six additional BLV user-study participants. Data collection included both qualitative (researcher observations, SOPI) and quantitative (MVPA zones, aid usage, completion times) measures of physical exertion and gameplay performance. The user study revealed that all participants reached moderate MVPA thresholds, with high levels of immersion and engagement observed. This work demonstrates the potential of VR as an inclusive medium for promoting meaningful PA in the BLV community and addresses a critical gap in accessible, intensity-driven exercise interventions.

</details>


### [47] [Reframing Pattern: A Comprehensive Approach to a Composite Visual Variable](https://arxiv.org/abs/2508.02639)

*Tingying He, Jason Dykes, Petra Isenberg, Tobias Isenberg*

**Main category:** cs.HC

**Keywords:** pattern, visualization, texture, data encoding, design theory

**Relevance Score:** 4

**TL;DR:** This paper presents a new theory for understanding and using patterns as a visual variable in visualization, addressing inconsistencies in the terminology and concepts found in existing literature.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The inconsistent use of concepts related to 'pattern' and 'texture' in visualization research hinders effective application and further research in this area.

**Method:** A comprehensive cross-disciplinary literature review is conducted to clarify ambiguities, leading to the formulation of a new consistent treatment of pattern as a composite visual variable made up of graphic primitives.

**Key Contributions:**

	1. New theoretical framework for understanding patterns in visualization
	2. Identification of three sets of variables for pattern design
	3. Connection of visualization concepts to broader theories of maps and cartography

**Result:** The proposed pattern system includes the spatial arrangement of primitives, appearance relationships among primitives, and retinal visual variables that characterize them, which opens new design opportunities in visualization.

**Limitations:** 

**Conclusion:** The new pattern system integrates seamlessly with existing visualization theories and shows potential for enhancing visualization design, particularly in complex spatial arrangements.

**Abstract:** We present a new comprehensive theory for explaining, exploring, and using pattern as a visual variable in visualization. Although patterns have long been used for data encoding and continue to be valuable today, their conceptual foundations are precarious: the concepts and terminology used across the research literature and in practice are inconsistent, making it challenging to use patterns effectively and to conduct research to inform their use. To address this problem, we conduct a comprehensive cross-disciplinary literature review that clarifies ambiguities around the use of "pattern" and "texture". As a result, we offer a new consistent treatment of pattern as a composite visual variable composed of structured groups of graphic primitives that can serve as marks for encoding data individually and collectively. This new and widely applicable formulation opens a sizable design space for the visual variable pattern, which we formalize as a new system comprising three sets of variables: the spatial arrangement of primitives, the appearance relationships among primitives, and the retinal visual variables that characterize individual primitives. We show how our pattern system relates to existing visualization theory and highlight opportunities for visualization design. We further explore patterns based on complex spatial arrangements, demonstrating explanatory power and connecting our conceptualization to broader theory on maps and cartography. An author version and additional materials are available on OSF: osf.io/z7ae2.

</details>


### [48] [OriStitch: A Machine Embroidery Workflow to Turn Existing Fabrics into Self-Folding 3D Textiles](https://arxiv.org/abs/2412.02891)

*Zekun Chang, Yixuan Gao, Yuta Noma, Shuo Feng, Xinyi Yang, Kazuhiro Shinoda, Tung D. Ta, Koji Yatani, Tomoyuki Yokota, Takao Someya, Yoshihiro Kawahara, Koya Narumi, Francois Guimbretiere, Thijs Roumen*

**Main category:** cs.HC

**Keywords:** Computational fabrication, Self-folding structures, Textiles

**Relevance Score:** 2

**TL;DR:** OriStitch is a computational fabrication workflow that transforms flat fabrics into self-folding 3D structures using machine-embroidered functional threads and heat.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enable the creation of 3D textiles from flat fabrics, enhancing the versatility of fabric design and functionality.

**Method:** The method involves embroidering specific patterns on fabrics with functional threads, applying heat to deform the fabric, and converting 3D models to 2D stitch patterns automatically.

**Key Contributions:**

	1. Introduction of OriStitch workflow for fabric transformation
	2. Development of a tool for automatic conversion of 3D meshes to 2D stitch patterns
	3. Evaluation of multiple fabric materials with successful hinge designs

**Result:** The validation process showed the tool successfully converted 23 out of 28 models and demonstrated effective folding performance across various materials.

**Limitations:** 

**Conclusion:** OriStitch provides a new approach to 3D textile creation using existing materials and automated design conversion, with applications in diverse fabric types.

**Abstract:** OriStitch is a computational fabrication workflow to turn existing flat fabrics into self-folding 3D structures. Users turn fabrics into self-folding sheets by machine embroidering functional threads in specific patterns on fabrics, and then apply heat to deform the structure into a target 3D structure. OriStitch is compatible with a range of existing materials (e.g., leather, woven fabric, and denim).   We present the design of specific embroidered hinges that fully close under exposure to heat. We discuss the stitch pattern design, thread and fabric selection, and heating conditions. To allow users to create 3D textiles using our hinges, we create a tool to convert 3D meshes to 2D stitch patterns automatically, as well as an end-to-end fabrication and actuation workflow. To validate this workflow, we designed and fabricated a cap (303 hinges), a handbag (338 hinges), and a cover for an organically shaped vase (140 hinges).   In technical evaluation, we found that our tool successfully converted 23/28 models (textures and volumetric objects) found in related papers. We also demonstrate the folding performance across different materials (suede leather, cork, Neoprene, and felt).

</details>


### [49] [Understanding User Mental Models in AI-Driven Code Completion Tools: Insights from an Elicitation Study](https://arxiv.org/abs/2502.02194)

*Giuseppe Desolda, Andrea Esposito, Francesco Greco, Cesare Tucci, Paolo Buono, Antonio Piccinno*

**Main category:** cs.HC

**Keywords:** AI-powered code completion, human-computer interaction, developer preferences

**Relevance Score:** 8

**TL;DR:** The paper explores developers' mental models in relation to AI-powered code completion tools (CCTs) and provides guidelines for improving their interaction design based on a study with 56 developers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address interaction challenges with CCTs arising from mismatches between developers' mental models and AI behavior, which are underexplored in existing literature.

**Method:** Conducted co-design workshops with 56 developers to elicit their preferences and mental models when interacting with CCTs.

**Key Contributions:**

	1. Identification of developers' mental model preferences for CCT interactions
	2. Guidelines for customizing AI suggestion mechanisms in CCTs
	3. Development of a dynamic proof-of-concept CCT (ATHENA) that meets user needs

**Result:** The study identified key factors affecting interaction design, including preferences for suggestion triggering, display methods, and customization needs.

**Limitations:** 

**Conclusion:** The findings inform better design of CCTs and demonstrated through the development of a proof-of-concept tool called ATHENA that adapts to developers' preferences.

**Abstract:** Integrated Development Environments increasingly implement AI-powered code completion tools (CCTs), which promise to enhance developer efficiency, accuracy, and productivity. However, interaction challenges with CCTs persist, mainly due to mismatches between developers' mental models and the unpredictable behavior of AI-generated suggestions, which is an aspect underexplored in the literature. We conducted an elicitation study with 56 developers using co-design workshops to elicit their mental models when interacting with CCTs. Different important findings that might drive the interaction design with CCTs emerged. For example, developers expressed diverse preferences on when and how code suggestions should be triggered (proactive, manual, hybrid), where and how they are displayed (inline, sidebar, popup, chatbot), as well as the level of detail. It also emerged that developers need to be supported by customization of activation timing, display modality, suggestion granularity, and explanation content, to better fit the CCT to their preferences. To demonstrate the feasibility of these and the other guidelines that emerged during the study, we developed ATHENA, a proof-of-concept CCT that dynamically adapts to developers' coding preferences and environments, ensuring seamless integration into diverse workflows.

</details>


### [50] [Human Capital Visualization using Speech Amount during Meetings](https://arxiv.org/abs/2508.02075)

*Ekai Hashimoto, Takeshi Mizumoto, Kohei Nagira, Shun Shiramatsu*

**Main category:** cs.HC

**Keywords:** human capital, conversation analysis, speech visualization, internal communication, organizational innovation

**Relevance Score:** 3

**TL;DR:** This study analyzes speech amounts during routine meetings to visualize human capital and address the role of conversations in organizations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To revitalize organizations through better internal communication and foster innovation by quantifying human capital through conversation analysis.

**Method:** The study employs conversation visualization technology to analyze speech amounts during routine meetings, focusing on differences by gender, job post, and participant presence.

**Key Contributions:**

	1. Proposed a novel method for quantifying human capital through conversation analysis.
	2. Visualized speech amounts in organizational settings to illustrate communication patterns.
	3. Provided empirical data and analysis regarding the impact of participant presence on speech dynamics.

**Result:** The analysis reveals measurable differences in speech amounts related to departmental affiliation and other attributes, indicating the effectiveness of conversation as a metric for human capital.

**Limitations:** The study is based on small to medium enterprises, which may limit generalizability to larger organizations.

**Conclusion:** Understanding speech dynamics in meetings provides insights into organizational communication and highlights areas for improvement in human capital management.

**Abstract:** In recent years, many companies have recognized the importance of human resources and are investing in human capital to revitalize their organizations and enhance internal communication, thereby fostering innovation. However, conventional quantification methods have mainly focused on readily measurable indicators without addressing the fundamental role of conversations in human capital. This study focuses on routine meetings and proposes strategies to visualize human capital by analyzing speech amount during these meetings. We employ conversation visualization technology, which operates effectively, to quantify speech. We then measure differences in speech amount by attributes such as gender and job post, changes in speech amount depending on whether certain participants are present, and correlations between speech amount and continuous attributes. To verify the effectiveness of our proposed methods, we analyzed speech amounts by departmental affiliation during weekly meetings at small to medium enterprises.

</details>


### [51] [Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits](https://arxiv.org/abs/2508.02328)

*Raj Mahmud, Shlomo Berkovsky, Mukesh Prasad, A. Baki Kocaballi*

**Main category:** cs.HC

**Keywords:** Conversational Recommender Systems, User Preferences, Dialogue Design, User Modeling, Adaptive Systems

**Relevance Score:** 8

**TL;DR:** This study investigates user preferences in Conversational Recommender Systems (CRSs) and identifies factors that influence interaction preferences through a within-subjects analysis.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations in understanding user interaction preferences within CRSs, aiming to enhance personalized recommendations through improved dialogue design.

**Method:** A within-subjects study was conducted with 139 participants who experienced two scripted CRS dialogues, rated their experiences, and indicated the importance of eight system qualities, followed by logistic regression and clustering analyses for predictive user modeling.

**Key Contributions:**

	1. Identification of key factors influencing user preferences in CRSs
	2. Development of a predictive framework for user modeling in conversational AI
	3. Discovery of distinct user profiles based on dialogue style preferences

**Result:** Preference for exploratory interactions was significantly predicted by enjoyment, usefulness, novelty, conversational quality, and perceived effectiveness, with five latent user profiles identified based on distinct dialogue style preferences.

**Limitations:** Study conducted with a specific demographic, limiting generalizability; further research needed to validate findings across diverse populations.

**Conclusion:** The findings contribute to a comprehensive framework that integrates various predictors into CRS user modeling, facilitating the design of adaptive dialogue systems that can dynamically meet user needs.

**Abstract:** Conversational Recommender Systems (CRSs) deliver personalised recommendations through multi-turn natural language dialogue and increasingly support both task-oriented and exploratory interactions. Yet, the factors shaping user interaction preferences remain underexplored. In this within-subjects study (\(N = 139\)), participants experienced two scripted CRS dialogues, rated their experiences, and indicated the importance of eight system qualities. Logistic regression revealed that preference for the exploratory interaction was predicted by enjoyment, usefulness, novelty, and conversational quality. Unexpectedly, perceived effectiveness was also associated with exploratory preference. Clustering uncovered five latent user profiles with distinct dialogue style preferences. Moderation analyses indicated that age, gender, and control preference significantly influenced these choices. These findings integrate affective, cognitive, and trait-level predictors into CRS user modelling and inform autonomy-sensitive, value-adaptive dialogue design. The proposed predictive and adaptive framework applies broadly to conversational AI systems seeking to align dynamically with evolving user needs.

</details>


### [52] [Six Guidelines for Trustworthy, Ethical and Responsible Automation Design](https://arxiv.org/abs/2508.02371)

*Matouš Jelínek, Nadine Schlicker, Ewart de Visser*

**Main category:** cs.HC

**Keywords:** Trustworthiness, Human-Automation Interaction, Design Guidelines, Pragmatics, User Experience

**Relevance Score:** 8

**TL;DR:** The paper proposes design guidelines for optimizing trustworthiness assessments in automated systems to ensure safe and effective human-automation interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To achieve calibrated trust in automated systems, users must accurately perceive a system’s trustworthiness, ensuring they rely on correct recommendations and reject incorrect ones.

**Method:** The paper derives six design guidelines for trustworthiness assessments from interdisciplinary literature, incorporating principles from pragmatics and communication.

**Key Contributions:**

	1. Six design guidelines for improving trustworthiness assessments of automated systems
	2. Incorporation of principles from pragmatics for better user-system communication
	3. Actionable insights for creating ethical and responsible human-automation interactions

**Result:** The guidelines aim to provide actionable insights for designers, potentially improving user trust and interaction satisfaction with automated systems.

**Limitations:** 

**Conclusion:** The proposed guidelines can evaluate existing systems' capabilities in facilitating accurate trust assessments, fostering safer human-automation interactions.

**Abstract:** Calibrated trust in automated systems (Lee and See 2004) is critical for their safe and seamless integration into society. Users should only rely on a system recommendation when it is actually correct and reject it when it is factually wrong. One requirement to achieve this goal is an accurate trustworthiness assessment, ensuring that the user's perception of the system's trustworthiness aligns with its actual trustworthiness, allowing users to make informed decisions about the extent to which they can rely on the system (Schlicker et al. 2022). We propose six design guidelines to help designers optimize for accurate trustworthiness assessments, thus fostering ethical and responsible human-automation interactions. The proposed guidelines are derived from existing literature in various fields, such as human-computer interaction, cognitive psychology, automation research, user-experience design, and ethics. We are incorporating key principles from the field of pragmatics, specifically the cultivation of common ground (H. H. Clark 1996) and Gricean communication maxims (Grice 1975). These principles are essential for the design of automated systems because the user's perception of the system's trustworthiness is shaped by both environmental contexts, such as organizational culture or societal norms, and by situational context, including the specific circumstances or scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed guidelines provide actionable insights for designers to create automated systems that make relevant trustworthiness cues available. This would ideally foster calibrated trust and more satisfactory, productive, and safe interactions between humans and automated systems. Furthermore, the proposed heuristics might work as a tool for evaluating to what extent existing systems enable users to accurately assess a system's trustworthiness.

</details>


### [53] [AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration](https://arxiv.org/abs/2508.02470)

*Hyunjn An, Yongwon Kim, Wonduk Seo, Joonil Park, Daye Kang, Changhoon Oh, Dokyun Kim, Seunghyun Lee*

**Main category:** cs.HC

**Keywords:** no-code platform, AI service design, natural language, visual workflows, user study

**Relevance Score:** 8

**TL;DR:** AIAP is a no-code platform that simplifies AI service design for non-experts by integrating natural language and visual workflows.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The challenges faced by non-experts in expressing intent and managing complexity in AI service design.

**Method:** A no-code platform that uses a coordinated multi-agent system to break down user instructions into modular steps.

**Key Contributions:**

	1. Introduction of AIAP, a no-code platform for AI service design.
	2. Integration of natural language input with visual workflows.
	3. Demonstrated effectiveness through user studies.

**Result:** User study with 32 participants showed significant improvement in developing AI services intuitively with AIAP.

**Limitations:** 

**Conclusion:** Natural language-based visual programming lowers barriers and improves the user experience in AI service design.

**Abstract:** While many tools are available for designing AI, non-experts still face challenges in clearly expressing their intent and managing system complexity. We introduce AIAP, a no-code platform that integrates natural language input with visual workflows. AIAP leverages a coordinated multi-agent system to decompose ambiguous user instructions into modular, actionable steps, hidden from users behind a unified interface. A user study involving 32 participants showed that AIAP's AI-generated suggestions, modular workflows, and automatic identification of data, actions, and context significantly improved participants' ability to develop services intuitively. These findings highlight that natural language-based visual programming significantly reduces barriers and enhances user experience in AI service design.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [54] [Rethinking Graph-Based Document Classification: Learning Data-Driven Structures Beyond Heuristic Approaches](https://arxiv.org/abs/2508.00864)

*Margarita Bugueño, Gerard de Melo*

**Main category:** cs.CL

**Keywords:** graph-based models, document classification, self-attention, statistical filtering, NLP

**Relevance Score:** 7

**TL;DR:** This paper presents a method for learning data-driven graph structures for document classification, improving over traditional heuristic methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing graph document representations in classification rely heavily on heuristics and expert knowledge, which can limit their applicability and effectiveness.

**Method:** The proposed method constructs homogeneous weighted graphs with sentences as nodes, using a self-attention model to learn edges based on dependencies between sentence pairs. A statistical filtering strategy is employed to retain only strongly correlated sentences, enhancing graph quality.

**Key Contributions:**

	1. Development of a data-driven approach for graph construction in document classification
	2. Demonstration of improved accuracy and robustness over heuristic methods
	3. Introduction of statistical filtering to enhance graph quality

**Result:** Experiments across three document classification datasets show that the learned graph structures outperform heuristic-based graphs, achieving higher accuracy and F1 scores. The statistical filtering also improves classification robustness.

**Limitations:** 

**Conclusion:** The findings suggest that automatic graph generation can surpass traditional heuristic methods, paving the way for new applications in NLP.

**Abstract:** In document classification, graph-based models effectively capture document structure, overcoming sequence length limitations and enhancing contextual understanding. However, most existing graph document representations rely on heuristics, domain-specific rules, or expert knowledge. Unlike previous approaches, we propose a method to learn data-driven graph structures, eliminating the need for manual design and reducing domain dependence. Our approach constructs homogeneous weighted graphs with sentences as nodes, while edges are learned via a self-attention model that identifies dependencies between sentence pairs. A statistical filtering strategy aims to retain only strongly correlated sentences, improving graph quality while reducing the graph size. Experiments on three document classification datasets demonstrate that learned graphs consistently outperform heuristic-based graphs, achieving higher accuracy and $F_1$ score. Furthermore, our study demonstrates the effectiveness of the statistical filtering in improving classification robustness. These results highlight the potential of automatic graph generation over traditional heuristic approaches and open new directions for broader applications in NLP.

</details>


### [55] [FECT: Factuality Evaluation of Interpretive AI-Generated Claims in Contact Center Conversation Transcripts](https://arxiv.org/abs/2508.00889)

*Hagyeong Shin, Binoy Robin Dalal, Iwona Bialynicka-Birula, Navjot Matharu, Ryan Muir, Xingwei Yang, Samuel W. K. Wong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Factuality Evaluation, Contact Center Conversations, Human Annotation, Benchmark Dataset

**Relevance Score:** 9

**TL;DR:** This paper introduces a framework for evaluating the factuality of claims made by LLMs in contact center conversation analyses, detailing a new paradigm and dataset for this purpose.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often produce hallucinations, making factual evaluation critical in enterprise contexts, especially for business decision-making based on contact center conversations.

**Method:** The paper presents a 3D paradigm (Decompose, Decouple, Detach) for human annotation and LLM evaluation, alongside the introduction of a benchmark dataset (FECT) for factuality evaluation.

**Key Contributions:**

	1. Introduction of the 3D evaluation paradigm for LLMs.
	2. Development of the FECT dataset for factuality evaluation in contact center conversations.
	3. Findings on aligning LLM judges to improve factuality assessments.

**Result:** The study demonstrates alignment among LLM-judges using the 3D framework, providing insights for more accurate factual evaluations of AI-generated outputs.

**Limitations:** 

**Conclusion:** The proposed approach advances the methodologies for evaluating the factuality of AI interpretations in contact center dialogues, addressing gaps in current practices.

**Abstract:** Large language models (LLMs) are known to hallucinate, producing natural language outputs that are not grounded in the input, reference materials, or real-world knowledge. In enterprise applications where AI features support business decisions, such hallucinations can be particularly detrimental. LLMs that analyze and summarize contact center conversations introduce a unique set of challenges for factuality evaluation, because ground-truth labels often do not exist for analytical interpretations about sentiments captured in the conversation and root causes of the business problems. To remedy this, we first introduce a \textbf{3D} -- \textbf{Decompose, Decouple, Detach} -- paradigm in the human annotation guideline and the LLM-judges' prompt to ground the factuality labels in linguistically-informed evaluation criteria. We then introduce \textbf{FECT}, a novel benchmark dataset for \textbf{F}actuality \textbf{E}valuation of Interpretive AI-Generated \textbf{C}laims in Contact Center Conversation \textbf{T}ranscripts, labeled under our 3D paradigm. Lastly, we report our findings from aligning LLM-judges on the 3D paradigm. Overall, our findings contribute a new approach for automatically evaluating the factuality of outputs generated by an AI system for analyzing contact center conversations.

</details>


### [56] [XAutoLM: Efficient Fine-Tuning of Language Models via Meta-Learning and AutoML](https://arxiv.org/abs/2508.00924)

*Ernesto L. Estevanell-Valladares, Suilan Estevez-Velarde, Yoan Gutiérrez, Andrés Montoyo, Ruslan Mitkov*

**Main category:** cs.CL

**Keywords:** AutoML, Language Models, Meta-Learning, Resource Efficiency, Green AI

**Relevance Score:** 9

**TL;DR:** XAutoLM is an AutoML framework that enhances language model fine-tuning by leveraging past experiences to optimize model selection and hyperparameter tuning, leading to significant reductions in evaluation time and errors.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Automated frameworks for model selection and hyperparameter optimization are lacking for resource-efficient fine-tuning in language models, which often incurs high computational and environmental costs.

**Method:** XAutoLM utilizes meta-learning to reuse past experiences by extracting meta-features, enabling more efficient sampling of configurations for discriminative and generative model fine-tuning.

**Key Contributions:**

	1. Introduction of XAutoLM for efficient LM fine-tuning.
	2. Utilization of meta-learning to guide model selection and HPO.
	3. Demonstrated significant improvements in evaluation time and error rates across multiple benchmarks.

**Result:** XAutoLM outperforms zero-shot optimizers on five out of six tasks while reducing mean evaluation time by up to 4.5x and error ratios by nearly sevenfold, discovering 50% more effective pipelines.

**Limitations:** 

**Conclusion:** The introduction of XAutoLM promotes resource-efficient fine-tuning in the NLP community and supports Green AI initiatives by providing an experience store to aid in optimization.

**Abstract:** Experts in machine learning leverage domain knowledge to navigate decisions in model selection, hyperparameter optimisation, and resource allocation. This is particularly critical for fine-tuning language models (LMs), where repeated trials incur substantial computational overhead and environmental impact. However, no existing automated framework simultaneously tackles the entire model selection and HPO task for resource-efficient LM fine-tuning. We introduce XAutoLM, a meta-learning-augmented AutoML framework that reuses past experiences to optimise discriminative and generative LM fine-tuning pipelines efficiently. XAutoLM learns from stored successes and failures by extracting task- and system-level meta-features to bias its sampling toward fruitful configurations and away from costly dead ends. On four text classification and two question-answering benchmarks, XAutoLM surpasses zero-shot optimiser's peak F1 on five of six tasks, cuts mean evaluation time by up to 4.5x, reduces error ratios by up to sevenfold, and uncovers up to 50% more pipelines above the zero-shot Pareto front. In contrast, simpler memory-based baselines suffer negative transfer. We release XAutoLM and our experience store to catalyse resource-efficient, Green AI fine-tuning in the NLP community.

</details>


### [57] [MAO-ARAG: Multi-Agent Orchestration for Adaptive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.01005)

*Yiqun Chen, Erhan Zhang, Lingyong Yan, Shuaiqiang Wang, Jizhou Huang, Dawei Yin, Jiaxin Mao*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, question-answering, multi-agent orchestration

**Relevance Score:** 9

**TL;DR:** The paper introduces MAO-ARAG, an adaptive Retrieval-Augmented Generation framework that utilizes multi-agent orchestration to improve response accuracy and efficiency in question-answering systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current RAG systems struggle to balance performance and cost efficiency due to varying complexities of real-world queries.

**Method:** MAO-ARAG employs a multi-turn framework with various executor agents (like query reformulation and document selection) and a planner agent trained via reinforcement learning to optimize the workflow for each query.

**Key Contributions:**

	1. Development of the MAO-ARAG framework for adaptive RAG systems
	2. Integration of multi-agent orchestration in QA systems
	3. Usage of reinforcement learning to optimize agent workflows

**Result:** The approach demonstrates improved answer quality while maintaining cost and latency within acceptable limits across multiple QA datasets.

**Limitations:** 

**Conclusion:** MAO-ARAG effectively enhances RAG systems by dynamically planning suitable workflows tailored to specific queries, improving both performance and cost management.

**Abstract:** In question-answering (QA) systems, Retrieval-Augmented Generation (RAG) has become pivotal in enhancing response accuracy and reducing hallucination issues. The architecture of RAG systems varies significantly, encompassing single-round RAG, iterative RAG, and reasoning RAG, each tailored to address different types of queries. Due to the varying complexity of real-world queries, a fixed RAG pipeline often struggles to balance performance and cost efficiency across different queries. To address this challenge, we propose an adaptive RAG framework called MAO-ARAG, which leverages multi-agent orchestration. Our adaptive RAG is conceived as a multi-turn framework. Specifically, we define multiple executor agents, representing typical RAG modules such as query reformulation agents, document selection agent, and generation agents. A planner agent intelligently selects and integrates the appropriate agents from these executors into a suitable workflow tailored for each query, striving for high-quality answers while maintaining reasonable costs. During each turn, the planner agent is trained using reinforcement learning, guided by an outcome-based reward (F1 score) and a cost-based penalty, continuously improving answer quality while keeping costs within a reasonable range. Experiments conducted on multiple QA datasets demonstrate that our approach, which dynamically plans workflows for each query, not only achieves high answer quality but also maintains both cost and latency within acceptable limits.The code of MAO-ARAG is on https://github.com/chenyiqun/Agentic-RAG.

</details>


### [58] [UrBLiMP: A Benchmark for Evaluating the Linguistic Competence of Large Language Models in Urdu](https://arxiv.org/abs/2508.01006)

*Farah Adeeba, Brian Dillon, Hassan Sajjad, Rajesh Bhatt*

**Main category:** cs.CL

**Keywords:** Urdu, Multilingual LLMs, Linguistic Minimal Pairs, Syntactic Knowledge, Low-Resource Languages

**Relevance Score:** 7

**TL;DR:** The paper introduces the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) to evaluate the performance of multilingual LLMs in understanding Urdu grammar, highlighting their strengths and weaknesses in low-resource languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the linguistic capabilities of multilingual LLMs in low-resource languages like Urdu which often receive less data in training compared to high-resource languages.

**Method:** The study utilizes a dataset called UrBLiMP, comprising 5,696 minimal pairs of sentences that illustrate grammatical differences in Urdu, evaluated on 20 multilingual LLMs.

**Key Contributions:**

	1. Introduction of the UrBLiMP dataset for Urdu
	2. Evaluation of 20 multilingual LLMs on syntactic understanding
	3. High inter-annotator agreement indicating dataset reliability

**Result:** The evaluation shows significant performance variation across different linguistic phenomena, with LLaMA-3-70B achieving the highest average accuracy of 94.73%.

**Limitations:** Limited to evaluating only a subset of multilingual LLMs; generalization to other low-resource languages may require additional research.

**Conclusion:** The findings indicate that while current multilingual LLMs show potential in understanding Urdu, they also have notable limitations in capturing detailed syntactic knowledge.

**Abstract:** Multilingual Large Language Models (LLMs) have shown remarkable performance across various languages; however, they often include significantly less data for low-resource languages such as Urdu compared to high-resource languages like English. To assess the linguistic knowledge of LLMs in Urdu, we present the Urdu Benchmark of Linguistic Minimal Pairs (UrBLiMP) i.e. pairs of minimally different sentences that contrast in grammatical acceptability. UrBLiMP comprises 5,696 minimal pairs targeting ten core syntactic phenomena, carefully curated using the Urdu Treebank and diverse Urdu text corpora. A human evaluation of UrBLiMP annotations yielded a 96.10% inter-annotator agreement, confirming the reliability of the dataset. We evaluate twenty multilingual LLMs on UrBLiMP, revealing significant variation in performance across linguistic phenomena. While LLaMA-3-70B achieves the highest average accuracy (94.73%), its performance is statistically comparable to other top models such as Gemma-3-27B-PT. These findings highlight both the potential and the limitations of current multilingual LLMs in capturing fine-grained syntactic knowledge in low-resource languages.

</details>


### [59] [Cross-Domain Web Information Extraction at Pinterest](https://arxiv.org/abs/2508.01096)

*Michael Farag, Patrick Halina, Andrey Zaytsev, Alekhya Munagala, Imtihan Ahmed, Junhao Wang*

**Main category:** cs.CL

**Keywords:** attribute extraction, e-commerce, webpage representation, XGBoost, scalability

**Relevance Score:** 6

**TL;DR:** A system developed by Pinterest for efficient attribute extraction from e-commerce websites using a novel webpage representation, achieving high accuracy and cost-effectiveness compared to large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To efficiently convert unstructured e-commerce information into a structured format to enhance user experience and improve content distribution.

**Method:** The system utilizes a new webpage representation that merges structural, visual, and text modalities to optimize small model learning for accurate attribute extraction.

**Key Contributions:**

	1. Introduction of a novel webpage representation combining multiple modalities
	2. Demonstration of simple models like XGBoost outperforming LLMs in attribute extraction
	3. High scalability and cost-effectiveness of the approach

**Result:** Achieves high accuracy and scalability, processing over 1,000 URLs per second, at a cost-effectiveness 1000 times better than the cheapest GPT alternatives.

**Limitations:** 

**Conclusion:** The proposed attribute extraction system demonstrates that simple models can outperform more complex LLMs in terms of accuracy and scalability for specific tasks.

**Abstract:** The internet offers a massive repository of unstructured information, but it's a significant challenge to convert this into a structured format. At Pinterest, the ability to accurately extract structured product data from e-commerce websites is essential to enhance user experiences and improve content distribution. In this paper, we present Pinterest's system for attribute extraction, which achieves remarkable accuracy and scalability at a manageable cost. Our approach leverages a novel webpage representation that combines structural, visual, and text modalities into a compact form, optimizing it for small model learning. This representation captures each visible HTML node with its text, style and layout information. We show how this allows simple models such as eXtreme Gradient Boosting (XGBoost) to extract attributes more accurately than much more complex Large Language Models (LLMs) such as Generative Pre-trained Transformer (GPT). Our results demonstrate a system that is highly scalable, processing over 1,000 URLs per second, while being 1000 times more cost-effective than the cheapest GPT alternatives.

</details>


### [60] [Asking the Right Questions: Benchmarking Large Language Models in the Development of Clinical Consultation Templates](https://arxiv.org/abs/2508.01159)

*Liam G. McCoy, Fateme Nateghi Haredasht, Kanav Chopra, David Wu, David JH Wu, Abass Conteh, Sarita Khemani, Saloni Kumar Maharaj, Vishnu Ravi, Arth Pahwa, Yingjie Weng, Leah Rosengaus, Lena Giang, Kelvin Zhenghao Li, Olivia Jee, Daniel Shirvani, Ethan Goh, Jonathan H. Chen*

**Main category:** cs.CL

**Keywords:** large language models, clinical consultation templates, eConsult, health informatics, physician communication

**Relevance Score:** 9

**TL;DR:** The study assesses LLMs' ability to generate structured clinical consultation templates for electronic consultation, revealing strengths and weaknesses in template length and prioritization of clinical questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of large language models in generating clinically relevant consultation templates for improved physician communication.

**Method:** Using 145 expert-crafted templates, the study employs a multi-agent pipeline that includes prompt optimization, semantic autograding, and prioritization analysis to assess various LLMs.

**Key Contributions:**

	1. Assessment of multiple frontier LLMs in generating clinical consultation templates.
	2. Demonstration of LLM strengths in comprehensiveness but weaknesses in length and prioritization.
	3. Identification of performance degradation in narrative-driven medical fields.

**Result:** Models such as o3 achieve high comprehensiveness (up to 92.2%), but often produce overly long templates and struggle with prioritizing clinically important questions under length constraints; performance varies among medical specialties.

**Limitations:** Current models produce excessively long templates and struggle with prioritization of questions; performance varies by specialty.

**Conclusion:** LLMs have potential to improve structured clinical information exchange, yet there is a need for improved evaluation methods to measure their capacity for prioritizing essential clinical information.

**Abstract:** This study evaluates the capacity of large language models (LLMs) to generate structured clinical consultation templates for electronic consultation. Using 145 expert-crafted templates developed and routinely used by Stanford's eConsult team, we assess frontier models -- including o3, GPT-4o, Kimi K2, Claude 4 Sonnet, Llama 3 70B, and Gemini 2.5 Pro -- for their ability to produce clinically coherent, concise, and prioritized clinical question schemas. Through a multi-agent pipeline combining prompt optimization, semantic autograding, and prioritization analysis, we show that while models like o3 achieve high comprehensiveness (up to 92.2\%), they consistently generate excessively long templates and fail to correctly prioritize the most clinically important questions under length constraints. Performance varies across specialties, with significant degradation in narrative-driven fields such as psychiatry and pain medicine. Our findings demonstrate that LLMs can enhance structured clinical information exchange between physicians, while highlighting the need for more robust evaluation methods that capture a model's ability to prioritize clinically salient information within the time constraints of real-world physician communication.

</details>


### [61] [CSIRO-LT at SemEval-2025 Task 11: Adapting LLMs for Emotion Recognition for Multiple Languages](https://arxiv.org/abs/2508.01161)

*Jiyu Chen, Necva Bölücü, Sarvnaz Karimi, Diego Mollá, Cécile L. Paris*

**Main category:** cs.CL

**Keywords:** emotion recognition, multilingual LLM, fine-tuning, LoRA, Semeval 2025

**Relevance Score:** 7

**TL;DR:** This paper investigates emotion recognition in multilingual contexts using fine-tuned large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective emotion detection across diverse languages due to cultural differences in emotional expression.

**Method:** The study explores various task-adaptation strategies for LLMs, ultimately finding that fine-tuning a pre-trained multilingual LLM using LoRA settings for each language is the best approach.

**Key Contributions:**

	1. Investigation of task-adaptation strategies for emotion recognition using LLMs.
	2. Demonstration of the effectiveness of fine-tuning with LoRA settings for multilingual support.
	3. Contribution to the Semeval 2025 Task on text-based emotion recognition.

**Result:** The paper demonstrates that fine-tuning enhances the LLM's ability to recognize emotions and their intensity from text snippets.

**Limitations:** 

**Conclusion:** Fine-tuning multilingual LLMs is crucial for accurately detecting emotions across different languages.

**Abstract:** Detecting emotions across different languages is challenging due to the varied and culturally nuanced ways of emotional expressions. The \textit{Semeval 2025 Task 11: Bridging the Gap in Text-Based emotion} shared task was organised to investigate emotion recognition across different languages. The goal of the task is to implement an emotion recogniser that can identify the basic emotional states that general third-party observers would attribute to an author based on their written text snippet, along with the intensity of those emotions. We report our investigation of various task-adaptation strategies for LLMs in emotion recognition. We show that the most effective method for this task is to fine-tune a pre-trained multilingual LLM with LoRA setting separately for each language.

</details>


### [62] [Adaptive Content Restriction for Large Language Models via Suffix Optimization](https://arxiv.org/abs/2508.01198)

*Yige Li, Peihai Jiang, Jun Sun, Peng Shu, Tianming Liu, Zhen Xiang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Content Restriction, Adaptive Content Restriction, Suffix Optimization, Harmful Content Generation

**Relevance Score:** 9

**TL;DR:** The paper introduces Adaptive Content Restriction (AdaCoRe), a lightweight approach to prevent harmful content generation in LLMs without model fine-tuning, featuring a method called Suffix Optimization (SOP) and a benchmark for evaluation.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for flexible content restriction in LLMs that varies by user group and context, which can't be met by traditional fine-tuning methods due to resource demands.

**Method:** The proposed method, Suffix Optimization (SOP), appends an optimized suffix to prompts to avoid generating restricted terms while maintaining output quality.

**Key Contributions:**

	1. Introduction of the AdaCoRe task for lightweight content restrictions
	2. Development of Suffix Optimization (SOP) as a method to preserve output quality while enforcing restrictions
	3. Creation of the Content Restriction Benchmark (CoReBench) for evaluating content restriction methods.

**Result:** SOP shows significant effectiveness over system-level baselines, achieving better restriction rates across several LLMs and demonstrating practical utility in a real-world platform (POE).

**Limitations:** 

**Conclusion:** Adaptation strategies like AdaCoRe, particularly SOP, provide a promising way to manage content restrictions in LLMs without requiring extensive computational resources.

**Abstract:** Large Language Models (LLMs) have demonstrated significant success across diverse applications. However, enforcing content restrictions remains a significant challenge due to their expansive output space. One aspect of content restriction is preventing LLMs from generating harmful content via model alignment approaches such as supervised fine-tuning (SFT). Yet, the need for content restriction may vary significantly across user groups, change rapidly over time, and not always align with general definitions of harmfulness. Applying SFT to each of these specific use cases is impractical due to the high computational, data, and storage demands. Motivated by this need, we propose a new task called \textit{Adaptive Content Restriction} (AdaCoRe), which focuses on lightweight strategies -- methods without model fine-tuning -- to prevent deployed LLMs from generating restricted terms for specific use cases. We propose the first method for AdaCoRe, named \textit{Suffix Optimization (SOP)}, which appends a short, optimized suffix to any prompt to a) prevent a target LLM from generating a set of restricted terms, while b) preserving the output quality. To evaluate AdaCoRe approaches, including our SOP, we create a new \textit{Content Restriction Benchmark} (CoReBench), which contains 400 prompts for 80 restricted terms across 8 carefully selected categories. We demonstrate the effectiveness of SOP on CoReBench, which outperforms the system-level baselines such as system suffix by 15\%, 17\%, 10\%, 9\%, and 6\% on average restriction rates for Gemma2-2B, Mistral-7B, Vicuna-7B, Llama3-8B, and Llama3.1-8B, respectively. We also demonstrate that SOP is effective on POE, an online platform hosting various commercial LLMs, highlighting its practicality in real-world scenarios.

</details>


### [63] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)

*Shengqi Zhu, Jeffrey M. Rzeszotarski, David Mimno*

**Main category:** cs.CL

**Keywords:** chat logs, LLM users, user behavior, query segmentation, model capabilities

**Relevance Score:** 9

**TL;DR:** This paper investigates user behavior in LLM queries, segmenting chat logs into various components and analyzing patterns across different users and interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand the variability in user behavior when interacting with LLMs through chat queries and how their request-making differs from traditional human-human interactions.

**Method:** The study employs segmentation of chat queries into contents of requests, roles, query-specific context, and expressions, alongside diachronic analysis of user behavior over time.

**Key Contributions:**

	1. Introduces a new task of segmenting chat queries into various components.
	2. Provides insights into how user behaviors change over time with experience.
	3. Demonstrates the impact of model capabilities on user interaction patterns.

**Result:** Query patterns were found to evolve from an initial focus on requests to more complex interactions as users gain experience, with significant differences observed in user behavior when new model capabilities are introduced.

**Limitations:** 

**Conclusion:** User behavior in LLM queries is distinct from human-human interactions, and as users become more familiar, their interactions tend to converge while still being influenced by model capabilities.

**Abstract:** Chat logs provide a rich source of information about LLM users, but patterns of user behavior are often masked by the variability of queries. We present a new task, segmenting chat queries into contents of requests, roles, query-specific context, and additional expressions. We find that, despite the familiarity of chat-based interaction, request-making in LLM queries remains significantly different from comparable human-human interactions. With the data resource, we introduce an important perspective of diachronic analyses with user expressions. We find that query patterns vary between early ones emphasizing requests, and individual users explore patterns but tend to converge with experience. Finally, we show that model capabilities affect user behavior, particularly with the introduction of new models, which are traceable at the community level.

</details>


### [64] [WebDS: An End-to-End Benchmark for Web-based Data Science](https://arxiv.org/abs/2508.01222)

*Ethan Hsu, Hong Meng Yam, Ines Bouissou, Aaron Murali John, Raj Thota, Josh Koe, Vivek Sarath Putta, G K Dharesan, Alexander Spangher, Shikhar Murty, Tenghao Huang, Christopher D. Manning*

**Main category:** cs.CL

**Keywords:** web-based data science, benchmark, multi-hop interactions, LLM, data analytics

**Relevance Score:** 7

**TL;DR:** WebDS introduces a benchmark for end-to-end web-based data science tasks, highlighting performance gaps in current LLM agents.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Address the need for a benchmark that reflects the complexities of real-world data science tasks involving multi-hop web interactions.

**Method:** WebDS includes 870 web-based data science tasks from 29 websites, evaluating agents on their ability to perform multi-step operations with diverse data formats.

**Key Contributions:**

	1. First end-to-end web-based data science benchmark
	2. Inclusion of 870 diverse web tasks
	3. Identification of new failure modes in LLM agents

**Result:** Current SOTA LLM agents show significant performance gaps, completing only 15% of tasks in WebDS compared to 80% in simpler benchmarks.

**Limitations:** Limited to the 29 websites included; potential biases from the tasks selected.

**Conclusion:** WebDS offers a more realistic platform for evaluating and improving LLM capabilities in data science applications.

**Abstract:** A large portion of real-world data science tasks are complex and require multi-hop web-based interactions: finding appropriate data available on the internet, synthesizing real-time data of various modalities from different locations, and producing summarized analyses. Existing web benchmarks often focus on simplistic interactions, such as form submissions or e-commerce transactions, and often do not require diverse tool-using capabilities required for web based data science. Conversely, traditional data science benchmarks typically concentrate on static, often textually bound datasets and do not assess end-to-end workflows that encompass data acquisition, cleaning, analysis, and insight generation. In response, we introduce WebDS, the first end-to-end web-based data science benchmark. It comprises 870 web-based data science tasks across 29 diverse websites from structured government data portals to unstructured news media, challenging agents to perform complex, multi-step operations requiring the use of tools and heterogeneous data formats that better reflect the realities of modern data analytics. Evaluations of current SOTA LLM agents indicate significant performance gaps in accomplishing these tasks. For instance, Browser Use, which accomplishes 80% of tasks on Web Voyager, successfully completes only 15% of tasks in WebDS, which our analysis suggests is due to new failure modes like poor information grounding, repetitive behavior and shortcut-taking that agents performing WebDS' tasks display. By providing a more robust and realistic testing ground, WebDS sets the stage for significant advances in the development of practically useful LLM-based data science.

</details>


### [65] [WarriorMath: Enhancing the Mathematical Ability of Large Language Models with a Defect-aware Framework](https://arxiv.org/abs/2508.01245)

*Yue Chen, Minghua He, Fangkai Yang, Pu Zhao, Lu Wang, Yu Kang, Yifei Dong, Yuefeng Zhan, Hao Sun, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, mathematical problem solving, data synthesis, progressive training, HCI

**Relevance Score:** 4

**TL;DR:** WarriorMath is a defect-aware framework for enhancing Large Language Models' (LLMs) performance in mathematical problem solving through targeted data synthesis and progressive training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLMs' performance in mathematics, specifically addressing their limitations due to the lack of diverse, high-quality training data and failure modes.

**Method:** The framework employs expert LLMs to collaboratively generate, critique, and refine mathematical problems, identifying questions the base LLMs cannot solve. A progressive learning framework fine-tunes the model with increasingly challenging data focused on its weaknesses.

**Key Contributions:**

	1. Introduction of WarriorMath, a defect-aware framework for LLMs in mathematics
	2. Implementation of a collaborative expert LLMs generation process for training data
	3. Establishment of a progressive learning framework for model fine-tuning

**Result:** WarriorMath demonstrates improved performance, outperforming strong baselines by an average of 12.57% across six mathematical benchmarks.

**Limitations:** 

**Conclusion:** The results showcase the effectiveness of the defect-aware, multi-expert approach in enhancing LLMs' mathematical capabilities, establishing a new state-of-the-art.

**Abstract:** Large Language Models (LLMs) excel in solving mathematical problems, yet their performance is often limited by the availability of high-quality, diverse training data. Existing methods focus on augmenting datasets through rephrasing or difficulty progression but overlook the specific failure modes of LLMs. This results in synthetic questions that the model can already solve, providing minimal performance gains. To address this, we propose WarriorMath, a defect-aware framework for mathematical problem solving that integrates both targeted data synthesis and progressive training. In the synthesis stage, we employ multiple expert LLMs in a collaborative process to generate, critique, and refine problems. Questions that base LLMs fail to solve are identified and iteratively improved through expert-level feedback, producing high-quality, defect-aware training data. In the training stage, we introduce a progressive learning framework that iteratively fine-tunes the model using increasingly challenging data tailored to its weaknesses. Experiments on six mathematical benchmarks show that WarriorMath outperforms strong baselines by 12.57% on average, setting a new state-of-the-art. Our results demonstrate the effectiveness of a defect-aware, multi-expert framework for improving mathematical ability.

</details>


### [66] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)

*Long S. T. Nguyen, Khang H. N. Vo, Thu H. A. Nguyen, Tuan C. Bui, Duc Q. Nguyen, Thanh-Tung Tran, Anh D. Nguyen, Minh L. Nguyen, Fabien Baldacci, Thang H. Bui, Emanuel Di Nardo, Angelo Ciaramella, Son H. Le, Ihsan Ullah, Lorenzo Di Rocco, Tho T. Quan*

**Main category:** cs.CL

**Keywords:** Explainable AI, Question-Answering, Educational Technology, Large Language Models, Hackathon

**Relevance Score:** 8

**TL;DR:** The paper presents an analysis of the XAI Challenge 2025, aimed at developing explainable AI systems for educational contexts, specifically for answering student queries with transparent explanations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of AI in education necessitates transparency and interpretable AI systems, especially for real-world applications such as answering student questions.

**Method:** Participants in the challenge built question-answering systems using lightweight Large Language Models and hybrid systems that were evaluated based on their ability to provide explainable answers.

**Key Contributions:**

	1. Comprehensive analysis of the XAI Challenge 2025
	2. Development of explainable question-answering systems
	3. Insights on bridging LLMs and symbolic reasoning for educational contexts

**Result:** The provided high-quality dataset and structured competition fostered the development of innovative solutions for explainable AI in education.

**Limitations:** 

**Conclusion:** The challenge represents a notable effort in merging LLMs with symbolic reasoning to enhance explainability, offering valuable insights for future XAI-focused educational systems.

**Abstract:** The growing integration of Artificial Intelligence (AI) into education has intensified the need for transparency and interpretability. While hackathons have long served as agile environments for rapid AI prototyping, few have directly addressed eXplainable AI (XAI) in real-world educational contexts. This paper presents a comprehensive analysis of the XAI Challenge 2025, a hackathon-style competition jointly organized by Ho Chi Minh City University of Technology (HCMUT) and the International Workshop on Trustworthiness and Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked participants with building Question-Answering (QA) systems capable of answering student queries about university policies while generating clear, logic-based natural language explanations. To promote transparency and trustworthiness, solutions were required to use lightweight Large Language Models (LLMs) or hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed via logic-based templates with Z3 validation and refined through expert student review to ensure alignment with real-world academic scenarios. We describe the challenge's motivation, structure, dataset construction, and evaluation protocol. Situating the competition within the broader evolution of AI hackathons, we argue that it represents a novel effort to bridge LLMs and symbolic reasoning in service of explainability. Our findings offer actionable insights for future XAI-centered educational systems and competitive research initiatives.

</details>


### [67] [Prompting Large Language Models with Partial Knowledge for Answering Questions with Unseen Entities](https://arxiv.org/abs/2508.01290)

*Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Yanyan Wang, Hongye Tan, Jiye Liang, Xiaoli Li, Ru Li, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Knowledge Graphs, Question Answering, Partially Relevant Knowledge

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel approach to improve Retrieval-Augmented Generation (RAG) systems by effectively utilizing partially relevant knowledge embedded in Large Language Models (LLMs) to enhance performance in Knowledge Graph Question Answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of leveraging partially relevant knowledge in Retrieval-Augmented Generation systems, particularly in the context of incomplete knowledge bases.

**Method:** The authors construct partially relevant knowledge by modifying triplets from the gold reasoning path, analyzing their effects on LLMs, and validating their approach through experiments on Knowledge Graph Question Answering datasets.

**Key Contributions:**

	1. Introduced a new perspective on utilizing partially relevant knowledge in LLMs
	2. Conducted theoretical analysis and experimental validation of the awakening effect
	3. Developed the Unseen Entity KGQA task to address real-world entity linking challenges.

**Result:** The proposed awakening-based approach demonstrates improved performance over traditional methods that rely on embedding-based similarity and struggles with noise in the information retrieval process.

**Limitations:** Focuses solely on the awakening effect and does not explore fully relevant knowledge scenarios.

**Conclusion:** The findings suggest that partially relevant knowledge can effectively 'awaken' LLMs and handle practical challenges in Knowledge Graphs, leading to better performance in QA tasks.

**Abstract:** Retrieval-Augmented Generation (RAG) shows impressive performance by supplementing and substituting parametric knowledge in Large Language Models (LLMs). Retrieved knowledge can be divided into three types: explicit answer evidence, implicit answer clue, and insufficient answer context which can be further categorized into totally irrelevant and partially relevant information. Effectively utilizing partially relevant knowledge remains a key challenge for RAG systems, especially in incomplete knowledge base retrieval. Contrary to the conventional view, we propose a new perspective: LLMs can be awakened via partially relevant knowledge already embedded in LLMs. To comprehensively investigate this phenomenon, the triplets located in the gold reasoning path and their variants are used to construct partially relevant knowledge by removing the path that contains the answer. We provide theoretical analysis of the awakening effect in LLMs and support our hypothesis with experiments on two Knowledge Graphs (KGs) Question Answering (QA) datasets. Furthermore, we present a new task, Unseen Entity KGQA, simulating real-world challenges where entity linking fails due to KG incompleteness. Our awakening-based approach demonstrates greater efficacy in practical applications, outperforms traditional methods that rely on embedding-based similarity which are prone to returning noisy information.

</details>


### [68] [KEDAS: Knowledge Editing Alignment with Diverse Augmentation and Self-adaptive Inference](https://arxiv.org/abs/2508.01302)

*Chenming Tang, Yutong Yang, Yunfang Wu*

**Main category:** cs.CL

**Keywords:** Knowledge Editing, Large Language Models, Editing Efficiency

**Relevance Score:** 9

**TL;DR:** The paper introduces KEDAS, a novel approach for efficiently editing knowledge in LLMs while maintaining performance, achieving superior results in empirical evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient knowledge editing in LLMs to retain their capabilities while updating outdated information.

**Method:** KEDAS employs low-rank adaptation for alignment, a diverse edit augmentation technique for recall improvement, and a self-adaptive inference mechanism with a smart retriever for dynamic query handling.

**Key Contributions:**

	1. Introduction of KEDAS for knowledge editing in LLMs
	2. Diverse edit augmentation technique for improved recall
	3. Self-adaptive post-alignment inference mechanism with smart retrieval.

**Result:** KEDAS outperforms existing approaches in 35 out of 36 cases on four datasets with three LLMs, achieving a 19.8 improvement in harmonic mean scores related to edit success and other metrics.

**Limitations:** 

**Conclusion:** KEDAS is validated as a robust and efficient paradigm for knowledge editing alignment in LLMs.

**Abstract:** Knowledge editing aims to modify outdated knowledge in large language models (LLMs) efficiently while retaining their powerful capabilities. Most existing methods rely on either parameter-level editing or retrieval-based approaches. In this work, we propose Knowledge Editing alignment with Diverse Augmentation and Self-adaptive inference (KEDAS) to better align LLMs with knowledge editing. In the alignment phase, LLMs learn to apply in-context edited knowledge via low-rank adaptation. During editing, we design a diverse edit augmentation technique to improve the recall of edits. After that, a self-adaptive post-alignment inference mechanism is proposed, in which a filter-based smart retriever is employed to perform a dynamic selection of inference routing. Specifically, irrelevant queries will go through the original pre-alignment model directly, while relevant ones, together with their related edits, go through the model with aligned adapters activated. In experiments, KEDAS secures the highest overall performance scores in 35 out of 36 cases across four datasets with three LLMs on three settings, surpassing its strong knowledge editing alignment counterpart by about 19.8 harmonic mean scores of edit success, locality and portability and outperforming both parameter editing and retrieval-based baselines significantly. Analysis of computational cost and performance on general tasks further validates the robustness and efficiency of KEDAS, indicating that it presents an ideal paradigm of knowledge editing alignment.

</details>


### [69] [D-SCoRE: Document-Centric Segmentation and CoT Reasoning with Structured Export for QA-CoT Data Generation](https://arxiv.org/abs/2508.01309)

*Weibo Zhou, Lingbo Li, Shangsong Liang*

**Main category:** cs.CL

**Keywords:** question-answering, large language models, dataset generation, prompt engineering, supervised fine-tuning

**Relevance Score:** 9

**TL;DR:** D-SCoRE is a novel training-free pipeline that generates diverse, high-quality question-answering datasets using LLMs and prompt engineering, aimed at enhancing domain-specific supervised fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** High-quality question-answering datasets are scarce and costly, limiting the deployment of supervised fine-tuning for domain-specific large language models.

**Method:** D-SCoRE utilizes document-centric processing, segmentation, chain-of-thought reasoning, and structured export to create QA datasets from various textual sources, along with multi-dimensional control mechanisms.

**Key Contributions:**

	1. Introduction of D-SCoRE for generating QA datasets without fine-tuning.
	2. Use of multi-dimensional control mechanisms for enhanced dataset diversity.
	3. Demonstration of superior performance on test sets compared to existing QA datasets.

**Result:** The evaluation shows that LLMs fine-tuned on D-SCoRE-generated datasets outperform those trained on human-annotated datasets across several domains.

**Limitations:** 

**Conclusion:** D-SCoRE demonstrates a scalable and efficient method for QA generation that enhances fine-tuning performance for large language models without requiring extensive labeled data.

**Abstract:** The scarcity and high cost of high-quality question-answering (QA) datasets hinder supervised fine-tuning (SFT) for domain-specific large language models (LLMs). To address this, we introduce D-SCoRE, a training-free pipeline that utilizes LLMs and prompt engineering to produce diverse, high-quality QA datasets from arbitrary textual sources. D-SCoRE integrates $\textbf{D}$ocument-centric processing, $\textbf{S}$egmentation, $\textbf{Co}$T $\textbf{R}$easoning, and structured $\textbf{E}$xport to generate QA-COT datasets tailored for domain-aware SFT. Multi-dimensional control mechanisms, such as semantic role transformation, question type balancing, and counterfactual materials, enhance diversity and relevance, overcoming limitations of existing QA generation. LLMs fine-tuned on D-SCoRE-generated QA datasets, and human-annotated QA datasets (SQuAD, Covid-QA) are evaluated on SQuADShifts and Covid-QA test sets, with D-SCoRE outperforming across most domains. D-SCoRE generates six QA-CoT pairs with four-option counterfactual materials per 100-200-word text in 90 seconds using an 8B LLM on consumer-grade hardware. Its simplicity and scalability enable efficient QA generation and high-performance fine-tuning across domains.

</details>


### [70] [LinkQA: Synthesizing Diverse QA from Multiple Seeds Strongly Linked by Knowledge Points](https://arxiv.org/abs/2508.01317)

*Xuemiao Zhang, Can Ren, Chengying Tu, Rongxiang Weng, Hongfei Yan, Jingang Wang, Xunliang Cai*

**Main category:** cs.CL

**Keywords:** large language models, knowledge points, question answering

**Relevance Score:** 9

**TL;DR:** LinkSyn is a framework for generating diverse QA datasets by utilizing knowledge point graphs, improving model performance through novel synthesis techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of high-quality, diverse training data for large language models (LLMs).

**Method:** LinkSyn extracts knowledge points (KPs) from seed data to create a KP graph that synthesizes diverse QA data via graph walks, adjusting for discipline and difficulty distributions.

**Key Contributions:**

	1. Development of LinkSyn framework for QA synthesis
	2. Creation of LinkQA dataset with 50B tokens
	3. Improvement of 11.51% on MMLU and CMMLU benchmarks

**Result:** Synthesizing LinkQA, a 50B token multi-disciplinary QA dataset, which results in an 11.51% improvement on MMLU and CMMLU for Llama-3 8B models, achieving new state-of-the-art results.

**Limitations:** 

**Conclusion:** LinkSyn effectively enhances LLM performance across different model sizes and operational scales by providing a diverse and high-quality training dataset.

**Abstract:** The advancement of large language models (LLMs) struggles with the scarcity of high-quality, diverse training data. To address this limitation, we propose LinkSyn, a novel knowledge point (KP) graph-based synthesis framework that enables flexible control over discipline and difficulty distributions while balancing KP coverage and popularity. LinkSyn extracts KPs from question-answering (QA) seed data and constructs a KP graph to synthesize diverse QA data from multiple seeds strongly linked by KPs and sampled from graph walks. Specifically, LinkSyn incorporates (1) a knowledge distribution value function to guide the adjustment of path sampling probability and balance KP coverage and popularity during graph walks; (2) diffusion-based synthesis via DeepSeek-R1 by leveraging multiple seeds with dense logical associations along each path; and (3) high-difficulty QA enhancement within given disciplines by flexible difficulty adjustments. By executing LinkSyn, we synthesize LinkQA, a diverse multi-disciplinary QA dataset with 50B tokens. Extensive experiments on Llama-3 8B demonstrate that continual pre-training with LinkQA yields an average improvement of $\mathbf{11.51\%}$ on MMLU and CMMLU, establishing new SOTA results. LinkQA consistently enhances performance across model size and initial FLOPs scales.

</details>


### [71] [Large-Scale Diverse Synthesis for Mid-Training](https://arxiv.org/abs/2508.01326)

*Xuemiao Zhang, Chengying Tu, Can Ren, Rongxiang Weng, Hongfei Yan, Jingang Wang, Xunliang Cai*

**Main category:** cs.CL

**Keywords:** Large language models, Quality assurance data, Synthesis framework, DeepSeek, STEM disciplines

**Relevance Score:** 9

**TL;DR:** This paper introduces BoostQA, a large-scale QA dataset designed to improve model performance in cross-domain contexts by synthesizing diverse and high-difficulty training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of high-quality training data hampers the development of LLMs, particularly in cross-domain scenarios, necessitating improved QA data integration.

**Method:** The novel BoostQA dataset is created through a diversified synthesis pipeline that curates seed data from various sources, employs a multi-grade synthesis focused on STEM disciplines, and refines answers to enhance quality.

**Key Contributions:**

	1. Introduction of BoostQA, a 100B-token large-scale QA dataset.
	2. Implementation of a multi-grade synthesis approach focused on STEM disciplines.
	3. Demonstration of a significant performance boost in Llama-3 models using BoostQA.

**Result:** Llama-3 8B, mid-trained on BoostQA, achieves an average performance improvement of 12.74% on MMLU and CMMLU benchmarks, with state-of-the-art results across 12 assessments.

**Limitations:** 

**Conclusion:** BoostQA contributes significantly to data diversity and quality, enhancing model training efficacy and demonstrating scalability as model and data sizes increase.

**Abstract:** The scarcity of high-quality, knowledge-intensive training data hinders the development of large language models (LLMs), as traditional corpora provide limited information. Previous studies have synthesized and integrated corpora-dependent question-answering (QA) data to improve model performance but face challenges in QA data scalability and knowledge diversity, particularly in cross-domain contexts. Furthermore, leveraging our designed discipline and difficulty annotation system, we probe model deficiencies in STEM disciplines and high-difficulty data. To overcome these limitations, we propose a novel diversified pipeline to synthesize BoostQA, a 100B-token large-scale QA dataset. Our synthesis framework: (1) curates seed data from heterogeneous sources; (2) utilizes DeepSeek-R1 to implement STEM-focused multi-grade synthesis to boost data diversity and high-difficulty synthesis to mitigate difficulty degradation; (3) refines answers via DeepSeek-V3 to improve output quality. We utilize BoostQA in mid-training, a mid-stage between pre-training and post-training, to optimize domain-specific knowledge acquisition and enhance data quality. Our method enables Llama-3 8B, mid-trained on a 40B-token dataset, to achieve an average improvement of $\mathbf{12.74\%}$ on MMLU and CMMLU and establish SOTA average performance across 12 benchmarks. BoostQA also demonstrates robust scalability, with performance consistently improving as model size, data volume, and initial FLOPs scale.

</details>


### [72] [MaRGen: Multi-Agent LLM Approach for Self-Directed Market Research and Analysis](https://arxiv.org/abs/2508.01370)

*Roman Koshkin, Pengyu Dai, Nozomi Fujikawa, Masahito Togami, Marco Visentini-Scarzanella*

**Main category:** cs.CL

**Keywords:** Large Language Models, Business Analysis, Market Report Generation, Automated Evaluation, Iterative Improvement

**Relevance Score:** 8

**TL;DR:** An autonomous framework using LLMs for automated business analysis and market report generation, employing specialized agents and a novel evaluation system.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate the process of business analysis and market report generation, making it more efficient and affordable.

**Method:** The framework uses specialized agents (Researcher, Reviewer, Writer, Retriever) to query databases, analyze data, generate insights, create visualizations, and compose reports, while employing an LLM-based evaluation system for report quality.

**Key Contributions:**

	1. Development of an autonomous framework leveraging LLMs for business analysis
	2. Introduction of a novel LLM-based evaluation system for report quality
	3. Implementation of an iterative improvement mechanism for report quality optimization

**Result:** The system can generate detailed 6-page reports in 7 minutes at a cost of roughly $1, with improvements in report quality through automated reviews and expert knowledge.

**Limitations:** 

**Conclusion:** The proposed framework can significantly automate and lower the costs associated with generating market reports, potentially disrupting traditional business analysis methods.

**Abstract:** We present an autonomous framework that leverages Large Language Models (LLMs) to automate end-to-end business analysis and market report generation. At its core, the system employs specialized agents - Researcher, Reviewer, Writer, and Retriever - that collaborate to analyze data and produce comprehensive reports. These agents learn from real professional consultants' presentation materials at Amazon through in-context learning to replicate professional analytical methodologies. The framework executes a multi-step process: querying databases, analyzing data, generating insights, creating visualizations, and composing market reports. We also introduce a novel LLM-based evaluation system for assessing report quality, which shows alignment with expert human evaluations. Building on these evaluations, we implement an iterative improvement mechanism that optimizes report quality through automated review cycles. Experimental results show that report quality can be improved by both automated review cycles and consultants' unstructured knowledge. In experimental validation, our framework generates detailed 6-page reports in 7 minutes at a cost of approximately \$1. Our work could be an important step to automatically create affordable market insights.

</details>


### [73] [MedSynth: Realistic, Synthetic Medical Dialogue-Note Pairs](https://arxiv.org/abs/2508.01401)

*Ahmad Rezaie Mianroodi, Amirali Rezaie, Niko Grisel Todorov, Cyril Rakovski, Frank Rudzicz*

**Main category:** cs.CL

**Keywords:** medical dialogue, synthetic dataset, medical documentation, machine learning, health informatics

**Relevance Score:** 8

**TL;DR:** MedSynth introduces a dataset of synthetic medical dialogues and notes to improve the automation of medical documentation.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the documentation burden on physicians, which contributes to burnout, by providing robust automation tools for medical documentation.

**Method:** Creation of MedSynth, a dataset containing over 10,000 dialogue-note pairs informed by disease distributions and spanning over 2000 ICD-10 codes, aimed at enhancing Dial-2-Note and Note-2-Dial tasks.

**Key Contributions:**

	1. Introduction of a novel dataset for medical dialogue and notes generation.
	2. Enhancement of model performance for Dialogue-to-Note and Note-to-Dialogue tasks.
	3. Provides critical training data in a field lacking diverse, open-access resources.

**Result:** The dataset significantly improves the performance of models in generating medical notes from dialogues and vice versa.

**Limitations:** 

**Conclusion:** MedSynth serves as a valuable resource for advancing medical documentation processes, offering open-access and privacy-compliant training data.

**Abstract:** Physicians spend significant time documenting clinical encounters, a burden that contributes to professional burnout. To address this, robust automation tools for medical documentation are crucial. We introduce MedSynth -- a novel dataset of synthetic medical dialogues and notes designed to advance the Dialogue-to-Note (Dial-2-Note) and Note-to-Dialogue (Note-2-Dial) tasks. Informed by an extensive analysis of disease distributions, this dataset includes over 10,000 dialogue-note pairs covering over 2000 ICD-10 codes. We demonstrate that our dataset markedly enhances the performance of models in generating medical notes from dialogues, and dialogues from medical notes. The dataset provides a valuable resource in a field where open-access, privacy-compliant, and diverse training data are scarce. Code is available at https://github.com/ahmadrezarm/MedSynth/tree/main and the dataset is available at https://huggingface.co/datasets/Ahmad0067/MedSynth.

</details>


### [74] [ArzEn-MultiGenre: An aligned parallel dataset of Egyptian Arabic song lyrics, novels, and subtitles, with English translations](https://arxiv.org/abs/2508.01411)

*Rania Al-Sabbagh*

**Main category:** cs.CL

**Keywords:** Egyptian Arabic, machine translation, parallel dataset, language models, translation studies

**Relevance Score:** 4

**TL;DR:** ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics, novels, and TV show subtitles, aligned with English translations, aimed at enhancing machine translation and aiding translation studies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a rich resource for benchmarking and improving machine translation systems, especially for Egyptian Arabic.

**Method:** The dataset consists of 25,557 segment pairs of Egyptian Arabic and English translations, manually translated and aligned by experts.

**Key Contributions:**

	1. Features unique textual genres not available in existing datasets
	2. Gold-standard quality through expert translation and alignment

**Result:** The dataset is intended for use in machine translation model benchmarking, fine-tuning language models, and aiding in translation studies.

**Limitations:** 

**Conclusion:** ArzEn-MultiGenre serves not only as a tool for machine translation but also as a pedagogical resource for translation students and professionals.

**Abstract:** ArzEn-MultiGenre is a parallel dataset of Egyptian Arabic song lyrics, novels, and TV show subtitles that are manually translated and aligned with their English counterparts. The dataset contains 25,557 segment pairs that can be used to benchmark new machine translation models, fine-tune large language models in few-shot settings, and adapt commercial machine translation applications such as Google Translate. Additionally, the dataset is a valuable resource for research in various disciplines, including translation studies, cross-linguistic analysis, and lexical semantics. The dataset can also serve pedagogical purposes by training translation students and aid professional translators as a translation memory. The contributions are twofold: first, the dataset features textual genres not found in existing parallel Egyptian Arabic and English datasets, and second, it is a gold-standard dataset that has been translated and aligned by human experts.

</details>


### [75] [Discovering Bias Associations through Open-Ended LLM Generations](https://arxiv.org/abs/2508.01412)

*Jinhao Pan, Chahat Raj, Ziwei Zhu*

**Main category:** cs.CL

**Keywords:** bias, large language models, human-computer interaction, demographic identities, text generation

**Relevance Score:** 9

**TL;DR:** This paper presents the Bias Association Discovery Framework (BADF) to identify and analyze biases in Large Language Models (LLMs) by extracting associations between demographic identities and concepts from LLM outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There are critical concerns over social biases in LLMs, leading to representational harms and distorted portrayals of demographic groups.

**Method:** The Bias Association Discovery Framework (BADF) systematically extracts known and unknown associations from open-ended LLM outputs through comprehensive experiments across multiple models and contexts.

**Key Contributions:**

	1. Development of the BADF framework for bias analysis in LLMs.
	2. Empirical results revealing the landscape of bias associations in LLM outputs.
	3. Accessibility of data, code, and results for further research.

**Result:** BADF enables robust mapping of various concepts that characterize demographic identities, enhancing the understanding of biases in LLM generation.

**Limitations:** The framework's effectiveness may depend on the diversity of LLMs and contexts tested.

**Conclusion:** The findings provide valuable insights into biases in LLMs and present a scalable tool for future analysis of bias associations.

**Abstract:** Social biases embedded in Large Language Models (LLMs) raise critical concerns, resulting in representational harms -- unfair or distorted portrayals of demographic groups -- that may be expressed in subtle ways through generated language. Existing evaluation methods often depend on predefined identity-concept associations, limiting their ability to surface new or unexpected forms of bias. In this work, we present the Bias Association Discovery Framework (BADF), a systematic approach for extracting both known and previously unrecognized associations between demographic identities and descriptive concepts from open-ended LLM outputs. Through comprehensive experiments spanning multiple models and diverse real-world contexts, BADF enables robust mapping and analysis of the varied concepts that characterize demographic identities. Our findings advance the understanding of biases in open-ended generation and provide a scalable tool for identifying and analyzing bias associations in LLMs. Data, code, and results are available at https://github.com/JP-25/Discover-Open-Ended-Generation

</details>


### [76] [From Query to Logic: Ontology-Driven Multi-Hop Reasoning in LLMs](https://arxiv.org/abs/2508.01424)

*Haonan Bian, Yutao Qi, Rui Yang, Yuanxi Che, Jiaqian Wang, Heming Xia, Ranran Zhen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-hop Question Answering, Knowledge Graphs

**Relevance Score:** 9

**TL;DR:** This paper introduces ORACLE, a framework that enhances multi-hop question answering by combining LLM capabilities with knowledge graphs to improve conceptual reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with complex multi-hop question answering due to their failure to capture deep conceptual relationships between entities.

**Method:** The proposed approach involves three stages: dynamic construction of question-specific knowledge ontologies using LLMs, conversion into First-Order Logic reasoning chains, and systematic decomposition of queries into sub-questions.

**Key Contributions:**

	1. Introduction of ORACLE framework for MQA
	2. Combination of LLMs with knowledge graphs
	3. Improvement in interpretability and logical reasoning of answers

**Result:** Experimental results demonstrate ORACLE's competitive performance on MQA benchmarks, outperforming state-of-the-art models and yielding more logical reasoning.

**Limitations:** 

**Conclusion:** The method not only improves performance in MQA tasks but also generates more interpretable reasoning chains compared to existing methods.

**Abstract:** Large Language Models (LLMs), despite their success in question answering, exhibit limitations in complex multi-hop question answering (MQA) tasks that necessitate non-linear, structured reasoning. This limitation stems from their inability to adequately capture deep conceptual relationships between entities. To overcome this challenge, we present **ORACLE** (**O**ntology-driven **R**easoning **A**nd **C**hain for **L**ogical **E**ucidation), a training-free framework that combines LLMs' generative capabilities with the structural benefits of knowledge graphs. Our approach operates through three stages: (1) dynamic construction of question-specific knowledge ontologies using LLMs, (2) transformation of these ontologies into First-Order Logic reasoning chains, and (3) systematic decomposition of the original query into logically coherent sub-questions. Experimental results on several standard MQA benchmarks show that our framework achieves highly competitive performance, rivaling current state-of-the-art models like DeepSeek-R1. Detailed analyses further confirm the effectiveness of each component, while demonstrating that our method generates more logical and interpretable reasoning chains than existing approaches.

</details>


### [77] [Towards Efficient Medical Reasoning with Minimal Fine-Tuning Data](https://arxiv.org/abs/2508.01450)

*Xinlin Zhuang, Feilong Tang, Haolin Yang, Ming Hu, Huifa Li, Haochen Xue, Yichen Li, Junjun He, Zongyuan Ge, Ying Qian, Imran Razzak*

**Main category:** cs.CL

**Keywords:** Supervised Fine-Tuning, Large Language Models, Medical Reasoning, Data Selection, Gradient Influence

**Relevance Score:** 9

**TL;DR:** The paper introduces the Difficulty-Influence Quadrant (DIQ), a novel data selection strategy for Supervised Fine-Tuning of Large Language Models (LLMs) in medical reasoning, effectively balancing sample difficulty and optimization utility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing Supervised Fine-Tuning (SFT) practices that rely on unfiltered datasets, leading to high computational costs and poor performance in adapting LLMs to medical reasoning.

**Method:** The proposed Difficulty-Influence Quadrant (DIQ) strategy selects data by prioritizing samples that are both high in difficulty and high in gradient influence, optimizing the fine-tuning process with fewer data points.

**Key Contributions:**

	1. Introduction of the Difficulty-Influence Quadrant (DIQ) strategy for data selection in medical reasoning.
	2. Demonstration that fine-tuning with DIQ-selected subsets achieves high performance with minimal data.
	3. Validation through human evaluations and experiments on medical reasoning benchmarks.

**Result:** Models fine-tuned on 1% of DIQ-selected data match the performance of those using full datasets, while using 10% of the data consistently outperforms traditional methods, confirming the effectiveness of DIQ.

**Limitations:** 

**Conclusion:** DIQ enhances data quality and model performance in medical reasoning by selecting samples that facilitate expert-like reasoning patterns, demonstrating its superiority over brute-force scaling methods.

**Abstract:** Supervised Fine-Tuning (SFT) plays a pivotal role in adapting Large Language Models (LLMs) to specialized domains such as medical reasoning. However, existing SFT practices often rely on unfiltered datasets that contain redundant and low-quality samples, leading to substantial computational costs and suboptimal performance. Although existing methods attempt to alleviate this problem by selecting data based on sample difficulty, defined by knowledge and reasoning complexity, they overlook each sample's optimization utility reflected in its gradient. Interestingly, we find that gradient-based influence alone favors easy-to-optimize samples that cause large parameter shifts but lack deep reasoning chains, while difficulty alone selects noisy or overly complex cases that fail to guide stable optimization. Based on this observation, we propose a data selection strategy, Difficulty-Influence Quadrant (DIQ), which prioritizes samples in the high-difficulty-high-influence quadrant to balance complex clinical reasoning with substantial gradient influence, enabling efficient medical reasoning with minimal fine-tuning data. Furthermore, Human and LLM-as-a-judge evaluations show that DIQ-selected subsets demonstrate higher data quality and generate clinical reasoning that is more aligned with expert practices in differential diagnosis, safety check, and evidence citation, as DIQ emphasizes samples that foster expert-like reasoning patterns. Extensive experiments on medical reasoning benchmarks demonstrate that DIQ enables models fine-tuned on only 1% of selected data to match full-dataset performance, while using 10% consistently outperforms the baseline, highlighting the superiority of principled data selection over brute-force scaling. The code and data are available at https://github.com/mihara-bot/DIQ.

</details>


### [78] [TreeDiff: AST-Guided Code Generation with Diffusion LLMs](https://arxiv.org/abs/2508.01473)

*Yiming Zeng, Jinghan Cao, Zexin Li, Yiming Chen, Tao Ren, Dawei Xiang, Xidong Wu, Shangqian Gao, Tingting Yu*

**Main category:** cs.CL

**Keywords:** diffusion models, syntax-aware, Abstract Syntax Trees, code generation, programming

**Relevance Score:** 6

**TL;DR:** This paper introduces a syntax-aware diffusion framework for text generation in programming by using Abstract Syntax Trees (ASTs) to improve training and reconstruction of code.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The work addresses challenges in applying diffusion models for sequence generation in structured domains like source code, which have strict syntactic rules.

**Method:** Utilizes a syntax-aware approach, selectively corrupting syntactically meaningful code spans derived from Abstract Syntax Trees (ASTs) instead of standard token-level corruption.

**Key Contributions:**

	1. Proposes a novel syntax-aware diffusion framework for code generation.
	2. Demonstrates the effectiveness of syntax-guided denoising in improving model performance.
	3. Shows that using ASTs allows for better preservation of grammatical boundaries in programming languages.

**Result:** Experimental results show improved syntactic correctness and enhanced reconstruction accuracy in code generation, with better generalization to unseen code patterns.

**Limitations:** 

**Conclusion:** Incorporating structural information from ASTs into the diffusion-based training process significantly enhances code generation tasks.

**Abstract:** Recent advances in diffusion-based language models have opened new possibilities for controllable and bidirectional sequence generation. These models provide an alternative to traditional autoregressive approaches by framing text generation as an iterative denoising process. However, applying diffusion models to structured domains such as source code remains a significant challenge. Programming languages differ from natural language in that they follow strict syntactic and semantic rules, with hierarchical organization that must be preserved for correctness. Standard token-level corruption techniques used during training often ignore this structure, which may hinder the model's ability to learn meaningful representations of code. To address this limitation, we propose a syntax-aware diffusion framework that incorporates structural priors from Abstract Syntax Trees (ASTs) into the denoising process. Instead of masking individual tokens at random, we selectively corrupt syntactically meaningful code spans derived from AST subtrees. This enables the model to reconstruct programs in a way that respects grammatical boundaries and captures long-range dependencies. Experimental results demonstrate that syntax-aware corruption significantly improves syntactic correctness, reconstruction accuracy, and generalization to unseen code patterns. These findings highlight the potential of incorporating structural information into diffusion-based training and suggest that syntax-guided denoising is a promising direction for advancing diffusion-based language models in code generation tasks.

</details>


### [79] [Harnessing Collective Intelligence of LLMs for Robust Biomedical QA: A Multi-Model Approach](https://arxiv.org/abs/2508.01480)

*Dimitra Panou, Alexandros C. Dimopoulos, Manolis Koubarakis, Martin Reczko*

**Main category:** cs.CL

**Keywords:** biomedical text mining, question-answering, language models

**Relevance Score:** 7

**TL;DR:** This paper presents a system for biomedical question-answering using LLMs, achieving high rankings in the BioASQ challenge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for effective biomedical text mining and question-answering in light of increasing biomedical literature.

**Method:** The authors deployed various open-source LLMs as retrieval-augmented generators, using a majority voting system for Yes/No questions and a union of answers for list and factoid questions.

**Key Contributions:**

	1. Use of majority voting among LLM outputs for question answering
	2. Exploration of various LLM combinations for optimal results
	3. Achievement of high rankings in the BioASQ challenge

**Result:** The study evaluated 13 open-source LLMs, finding optimal combinations that led to high performance in the BioASQ challenge, securing top placements in multiple rounds.

**Limitations:** 

**Conclusion:** The results highlight the effectiveness of tailored LLM pipelines for different question types in biomedical Q&A tasks.

**Abstract:** Biomedical text mining and question-answering are essential yet highly demanding tasks, particularly in the face of the exponential growth of biomedical literature. In this work, we present our participation in the 13th edition of the BioASQ challenge, which involves biomedical semantic question-answering for Task 13b and biomedical question-answering for developing topics for the Synergy task. We deploy a selection of open-source large language models (LLMs) as retrieval-augmented generators to answer biomedical questions. Various models are used to process the questions. A majority voting system combines their output to determine the final answer for Yes/No questions, while for list and factoid type questions, the union of their answers in used. We evaluated 13 state-of-the-art open source LLMs, exploring all possible model combinations to contribute to the final answer, resulting in tailored LLM pipelines for each question type. Our findings provide valuable insight into which combinations of LLMs consistently produce superior results for specific question types. In the four rounds of the 2025 BioASQ challenge, our system achieved notable results: in the Synergy task, we secured 1st place for ideal answers and 2nd place for exact answers in round 2, as well as two shared 1st places for exact answers in round 3 and 4.

</details>


### [80] [TeSent: A Benchmark Dataset for Fairness-aware Explainable Sentiment Classification in Telugu](https://arxiv.org/abs/2508.01486)

*Vallabhaneni Raj Kumar, Ashwin S, Supriya Manna, Niladri Sett, Cheedella V S N M S Hema Harshitha, Kurakula Harshitha, Anand Kumar Sharma, Basina Deepakraj, Tanuj Sarkar, Bondada Navaneeth Krishna, Samanthapudi Shakeer*

**Main category:** cs.CL

**Keywords:** Telugu, NLP, Machine Learning, sentiment classification, explainability

**Relevance Score:** 7

**TL;DR:** Introduction of TeSent, a benchmark dataset for sentiment classification in Telugu, providing explainability and fairness evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the underrepresentation of Telugu in NLP and Machine Learning by providing a high-quality annotated dataset.

**Method:** Scraped Telugu texts from social media, news, and blogs to create a dataset of 26,150 sentences. Developed an annotation platform for ground truth labeling and rationales. Fine-tuned state-of-the-art models with and without human-annotated rationales.

**Key Contributions:**

	1. TeSent dataset for Telugu sentiment classification
	2. Equity Evaluation Corpus (TeEEC) for fairness evaluation
	3. Evaluation suite for explainability using rationales

**Result:** Training with rationales improved model accuracy and fairness, and enhanced explainability of outputs.

**Limitations:** Focuses only on sentiment classification and may not cover all NLP tasks in Telugu.

**Conclusion:** TeSent and TeEEC facilitate better NLP model training for Telugu, focusing on explainability and fairness in sentiment classification tasks.

**Abstract:** In the Indian subcontinent, Telugu, one of India's six classical languages, is the most widely spoken Dravidian Language. Despite its 96 million speaker base worldwide, Telugu remains underrepresented in the global NLP and Machine Learning landscape, mainly due to lack of high-quality annotated resources. This work introduces TeSent, a comprehensive benchmark dataset for sentiment classification, a key text classification problem, in Telugu. TeSent not only provides ground truth labels for the sentences, but also supplements with provisions for evaluating explainability and fairness, two critical requirements in modern-day machine learning tasks. We scraped Telugu texts covering multiple domains from various social media platforms, news websites and web-blogs to preprocess and generate 26,150 sentences, and developed a custom-built annotation platform and a carefully crafted annotation protocol for collecting the ground truth labels along with their human-annotated rationales. We then fine-tuned several SOTA pre-trained models in two ways: with rationales, and without rationales. Further, we provide a detailed plausibility and faithfulness evaluation suite, which exploits the rationales, for six widely used post-hoc explainers applied on the trained models. Lastly, we curate TeEEC, Equity Evaluation Corpus in Telugu, a corpus to evaluate fairness of Telugu sentiment and emotion related NLP tasks, and provide a fairness evaluation suite for the trained classifier models. Our experimental results suggest that training with rationales may improve model accuracy, reduce bias in models, and make the explainers' output more aligned to human reasoning.

</details>


### [81] [The Homogenizing Effect of Large Language Models on Human Expression and Thought](https://arxiv.org/abs/2508.01491)

*Zhivar Sourati, Alireza S. Ziabari, Morteza Dehghani*

**Main category:** cs.CL

**Keywords:** cognitive diversity, large language models, collective intelligence, language standardization, creativity

**Relevance Score:** 9

**TL;DR:** This review discusses the impact of large language models (LLMs) on cognitive diversity, emphasizing how their standardization of language and reasoning can marginalize alternative perspectives and decrease collective intelligence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of LLMs in daily life prompts concerns about their impact on cognitive diversity and creativity, essential for collective intelligence.

**Method:** The authors synthesize evidence from linguistics, cognitive science, and computer science to analyze the effects of LLMs on language and reasoning diversity.

**Key Contributions:**

	1. Synthesis of evidence across multiple fields on the impact of LLMs on cognitive diversity.
	2. Identification of how LLMs reinforce dominant cognitive styles while marginalizing alternative perspectives.
	3. Discussion of the societal implications of LLM standardization on creativity and adaptability.

**Result:** LLMs reinforce dominant styles of language and reasoning, marginalizing alternative voices and cognitive strategies, leading to a risk of homogenization in cognitive landscapes.

**Limitations:** 

**Conclusion:** Without addressing the risks posed by LLMs, society may face a decline in cognitive diversity, which is critical for adaptability and creativity in collective intelligence.

**Abstract:** Cognitive diversity, reflected in variations of language, perspective, and reasoning, is essential to creativity and collective intelligence. This diversity is rich and grounded in culture, history, and individual experience. Yet as large language models (LLMs) become deeply embedded in people's lives, they risk standardizing language and reasoning. This Review synthesizes evidence across linguistics, cognitive, and computer science to show how LLMs reflect and reinforce dominant styles while marginalizing alternative voices and reasoning strategies. We examine how their design and widespread use contribute to this effect by mirroring patterns in their training data and amplifying convergence as all people increasingly rely on the same models across contexts. Unchecked, this homogenization risks flattening the cognitive landscapes that drive collective intelligence and adaptability.

</details>


### [82] [A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents](https://arxiv.org/abs/2508.01503)

*Clayton Cohn, Surya Rayala, Namrata Srivastava, Joyce Horn Fonteles, Shruti Jain, Xinying Luo, Divya Mereddy, Naveeduddin Mohammed, Gautam Biswas*

**Main category:** cs.CL

**Keywords:** large language models, education, adaptive scaffolding, STEM, human-AI interaction

**Relevance Score:** 8

**TL;DR:** The paper presents a framework for integrating large language models (LLMs) into educational contexts, emphasizing their potential for adaptive scaffolding in STEM+C learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of theoretical foundations in using LLM systems like ChatGPT in classrooms, this research aims to improve educational outcomes through a structured framework.

**Method:** The authors propose a framework that merges Evidence-Centered Design with Social Cognitive Theory, demonstrated through the development of Inquizzitor, an LLM-based formative assessment agent.

**Key Contributions:**

	1. Proposed a novel framework for LLM integration in education
	2. Developed Inquizzitor as a pedagogical agent for formative assessment
	3. Provided empirical evidence supporting the framework's effectiveness.

**Result:** Inquizzitor provides high-quality assessments and interactive feedback aligned with core learning theories, positively influencing student learning experiences.

**Limitations:** 

**Conclusion:** The study demonstrates the feasibility and benefits of theory-driven integration of LLMs in educational settings, offering teachers valuable tools for adaptive instruction.

**Abstract:** Large language models (LLMs) present new opportunities for creating pedagogical agents that engage in meaningful dialogue to support student learning. However, the current use of LLM systems like ChatGPT in classrooms often lacks the solid theoretical foundation found in earlier intelligent tutoring systems. To bridge this gap, we propose a framework that combines Evidence-Centered Design with Social Cognitive Theory for adaptive scaffolding in LLM-based agents focused on STEM+C learning. We illustrate this framework with Inquizzitor, an LLM-based formative assessment agent that integrates human-AI hybrid intelligence and provides feedback grounded in cognitive science principles. Our findings show that Inquizzitor delivers high-quality assessment and interaction aligned with core learning theories, offering teachers effective guidance that students value. This research underscores the potential for theory-driven LLM integration in education, highlighting the ability of these systems to provide adaptive and principled instruction.

</details>


### [83] [MOPrompt: Multi-objective Semantic Evolution for Prompt Optimization](https://arxiv.org/abs/2508.01541)

*Sara Câmara, Eduardo Luz, Valéria Carvalho, Ivan Meneghini, Gladston Moreira*

**Main category:** cs.CL

**Keywords:** Prompt Engineering, Large Language Models, Multi-objective Optimization

**Relevance Score:** 9

**TL;DR:** This paper presents MOPrompt, a multi-objective framework for optimizing prompts in Large Language Models (LLMs) that balances task performance and context size.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexity and inefficiency of manual prompt design for LLMs and to simultaneously optimize multiple objectives including accuracy and context size.

**Method:** Introduces a Multi-objective Evolutionary Optimization framework called MOPrompt, which evaluates trade-offs between prompt performance and context size by mapping the Pareto front of prompt solutions.

**Key Contributions:**

	1. Introduction of MOPrompt for multi-objective prompt optimization
	2. Evaluation of prompts in terms of both accuracy and token length
	3. Demonstration of superior performance compared to baseline frameworks

**Result:** MOPrompt outperforms existing baseline methods in a sentiment analysis task, achieving peak accuracy equivalent to the best baseline while reducing token length by 31%.

**Limitations:** 

**Conclusion:** MOPrompt is presented as a critical tool for practitioners aiming to deploy LLMs efficiently in real-world applications, balancing the need for performance with manageable context size.

**Abstract:** Prompt engineering is crucial for unlocking the potential of Large Language Models (LLMs). Still, since manual prompt design is often complex, non-intuitive, and time-consuming, automatic prompt optimization has emerged as a research area. However, a significant challenge in prompt optimization is managing the inherent trade-off between task performance, such as accuracy, and context size. Most existing automated methods focus on a single objective, typically performance, thereby failing to explore the critical spectrum of efficiency and effectiveness. This paper introduces the MOPrompt, a novel Multi-objective Evolutionary Optimization (EMO) framework designed to optimize prompts for both accuracy and context size (measured in tokens) simultaneously. Our framework maps the Pareto front of prompt solutions, presenting practitioners with a set of trade-offs between context size and performance, a crucial tool for deploying Large Language Models (LLMs) in real-world applications. We evaluate MOPrompt on a sentiment analysis task in Portuguese, using Gemma-2B and Sabiazinho-3 as evaluation models. Our findings show that MOPrompt substantially outperforms the baseline framework. For the Sabiazinho model, MOPrompt identifies a prompt that achieves the same peak accuracy (0.97) as the best baseline solution, but with a 31% reduction in token length.

</details>


### [84] [Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models](https://arxiv.org/abs/2508.01554)

*Yujia Zheng, Tianhao Li, Haotian Huang, Tianyu Zeng, Jingyu Lu, Chuangxin Chu, Yuekai Huang, Ziyou Jiang, Qian Xiong, Yuyao Ge, Mingyang Li*

**Main category:** cs.CL

**Keywords:** prompt-based attacks, large language models, adversarial robustness, prompt dissection, NLP

**Relevance Score:** 9

**TL;DR:** This paper presents PromptAnatomy, a framework for analyzing and generating adversarial examples for large language models by dissecting prompts into components, enhancing robustness evaluation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve robustness assessment of large language models by understanding the structural components of prompts which have varying impacts on adversarial robustness.

**Method:** PromptAnatomy dissects prompts into their functional components and utilizes a perturbation method called ComPerturb to selectively manipulate these components. It employs a perplexity-based filtering mechanism to maintain linguistic quality.

**Key Contributions:**

	1. Introduction of PromptAnatomy framework for prompt analysis
	2. Development of ComPerturb for selective component perturbation
	3. Application of perplexity-based filtering for linguistic plausibility

**Result:** Extensive experiments show that ComPerturb achieves state-of-the-art success rates in adversarial attacks across multiple instruction-tuning datasets and LLMs, demonstrating the framework's effectiveness.

**Limitations:** The framework may still be limited by the quality and diversity of the initial datasets used for annotation and testing.

**Conclusion:** The study highlights the importance of recognizing prompt structure and controlling perturbations for a more accurate evaluation of adversarial robustness in LLMs.

**Abstract:** Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.

</details>


### [85] [OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers for Biomedical NER Across 12 Public Datasets](https://arxiv.org/abs/2508.01630)

*Maziyar Panahi*

**Main category:** cs.CL

**Keywords:** named-entity recognition, healthcare data, transformer models, Low-Rank Adaptation, domain-adaptive pre-training

**Relevance Score:** 9

**TL;DR:** OpenMed NER introduces a suite of domain-adapted transformer models for named-entity recognition in healthcare data, achieving state-of-the-art performance while maintaining computational efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Named-entity recognition is essential for extracting structured information from unstructured clinical notes and biomedical literature in healthcare. Current challenges exist in achieving efficient state-of-the-art performance across diverse entity types.

**Method:** OpenMed NER employs domain-adaptive pre-training (DAPT) combined with Low-Rank Adaptation (LoRA) on a corpus of clinically relevant texts, followed by task-specific fine-tuning to enhance model performance while updating minimal parameters (less than 1.5%).

**Key Contributions:**

	1. Development of OpenMed NER, a suite of domain-adapted transformer models for healthcare data.
	2. State-of-the-art performance on 10 out of 12 biomedical NER benchmarks.
	3. Efficient training with low carbon footprint and minimal parameter updates.

**Result:** The models achieve state-of-the-art micro-F1 scores on 10 out of 12 biomedical NER benchmarks, providing significant improvements especially on specialized datasets for genes and clinical data.

**Limitations:** 

**Conclusion:** OpenMed NER demonstrates that efficiently adapted open-source models can perform better than closed-source alternatives, while being more sustainable and compliant with data protection regulations.

**Abstract:** Named-entity recognition (NER) is fundamental to extracting structured information from the >80% of healthcare data that resides in unstructured clinical notes and biomedical literature. Despite recent advances with large language models, achieving state-of-the-art performance across diverse entity types while maintaining computational efficiency remains a significant challenge. We introduce OpenMed NER, a suite of open-source, domain-adapted transformer models that combine lightweight domain-adaptive pre-training (DAPT) with parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs cost-effective DAPT on a 350k-passage corpus compiled from ethically sourced, publicly available research repositories and de-identified clinical notes (PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA backbones. This is followed by task-specific fine-tuning with LoRA, which updates less than 1.5% of model parameters. We evaluate our models on 12 established biomedical NER benchmarks spanning chemicals, diseases, genes, and species. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of these 12 datasets, with substantial gains across diverse entity types. Our models advance the state-of-the-art on foundational disease and chemical benchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger improvements of over 5.3 and 9.7 percentage points on more specialized gene and clinical cell line corpora. This work demonstrates that strategically adapted open-source models can surpass closed-source solutions. This performance is achieved with remarkable efficiency: training completes in under 12 hours on a single GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively licensed, open-source checkpoints designed to help practitioners facilitate compliance with emerging data protection and AI regulations, such as the EU AI Act.

</details>


### [86] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)

*Lucio La Cava, Dominik Macko, Róbert Móro, Ivan Srba, Andrea Tagarelli*

**Main category:** cs.CL

**Keywords:** Multilingual Authorship Attribution, Machine-generated text, Large Language Models, Cross-lingual transferability, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** The paper addresses the challenges of Multilingual Authorship Attribution (AA), where the objective is to attribute texts to either human or various LLMs across multiple languages, highlighting the limitations of current monolingual approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of Large Language Models (LLMs) producing human-like text, there is a pressing need to distinguish between machine-generated and human-written content, particularly in a multilingual context.

**Method:** The study investigates authorship attribution across 18 languages using 7 LLMs and human authors, evaluating monolingual AA methods and their ability to adapt and transfer to multilingual scenarios.

**Key Contributions:**

	1. Introduction of the problem of Multilingual Authorship Attribution
	2. Analysis of the cross-lingual transferability of monolingual AA methods
	3. Examination of the impact of various generators on attribution performance

**Result:** The findings indicate that monolingual AA methods can be somewhat adapted for multilingual use; however, there are significant challenges in performance when transferring methods across different language families.

**Limitations:** Significant limitations in transferring methods across diverse language families remain, which indicates the complexity of multilingual authorship attribution.

**Conclusion:** The complexity of multilingual AA demands the development of more robust techniques to handle the diverse linguistic landscape, as existing methods show substantial limitations.

**Abstract:** As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.

</details>


### [87] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)

*Tae Soo Kim, Yoonjoo Lee, Yoonah Park, Jiho Kim, Young-Ho Kim, Juho Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, CUPID, user preferences, contextual interactions, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces CUPID, a benchmark that evaluates the ability of Large Language Models (LLMs) to infer dynamic user preferences during interactions in varying contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight that user preferences are dynamic and context-dependent, challenging the assumption of static preferences in LLM personalization.

**Method:** CUPID is a benchmark consisting of 756 human-curated interaction session histories, where users express preferences through multi-turn feedback during interactions with LLM-based chat assistants.

**Key Contributions:**

	1. Introduction of the CUPID benchmark for evaluating preference inference in LLMs
	2. Demonstration of the challenges faced by current LLMs in multi-turn interactions
	3. Provision of a resource to encourage future improvements in LLM personalization

**Result:** State-of-the-art LLMs performed poorly, with under 50% precision and 65% recall, unable to accurately infer preferences from past interactions in new contexts.

**Limitations:** The benchmark does not cover all possible contexts or user preferences that could be encountered in real-world applications.

**Conclusion:** The findings suggest a significant gap in LLM ability to provide contextually personalized interactions, emphasizing the need for advancements in their capabilities.

**Abstract:** Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [88] [The Bidirectional Process Reward Model](https://arxiv.org/abs/2508.01682)

*Lingyin Zhang, Jun Gao, Xiaoxue Ren, Ziqiang Cao*

**Main category:** cs.CL

**Keywords:** Bidirectional Process Reward Model, Large Language Models, Mathematical Reasoning, AI, Process Reward Models

**Relevance Score:** 8

**TL;DR:** This paper introduces a Bidirectional Process Reward Model (BiPRM) that enhances Large Language Models (LLMs) by evaluating reasoning steps bidirectionally, improving consistency and overall performance in mathematical reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to overcome the limitations of existing unidirectional Process Reward Models (PRMs) that restrict the evaluation of reasoning steps in LLMs by not leveraging global context effectively.

**Method:** The authors propose a Bidirectional Process Reward Model (BiPRM) that integrates a parallel right-to-left (R2L) evaluation stream with the conventional left-to-right (L2R) process, allowing for real-time assessment of earlier reasoning steps using later ones. This is achieved through prompt modifications, avoiding extra parameters or latency.

**Key Contributions:**

	1. Introduction of a bidirectional evaluation paradigm for reasoning in LLMs
	2. Efficient implementation through prompt modifications without additional parameters
	3. Significant performance gains in mathematical reasoning tasks compared to unidirectional models.

**Result:** BiPRM was tested on two mathematical reasoning benchmarks using samples from three different policy models. The results show that BiPRM outperforms unidirectional baselines with improvements of up to 31.9% in stepwise reward evaluation across various settings.

**Limitations:** The study primarily focuses on mathematical reasoning benchmarks; applicability to other domains may require further validation.

**Conclusion:** BiPRM demonstrates improved effectiveness, robustness, and broad applicability, indicating a promising new direction for process-based reward modeling in LLMs.

**Abstract:** Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning quality of Large Language Models (LLMs) by assigning fine-grained scores to intermediate reasoning steps within a solution trajectory. However, existing PRMs predominantly adopt a unidirectional left-to-right (L2R) evaluation paradigm, which limits their ability to leverage global context, making it challenging to verify the consistency of earlier steps based on later ones. In light of these challenges, we propose a novel bidirectional evaluation paradigm, named Bidirectional Process Reward Model (BiPRM). BiPRM seamlessly incorporates a parallel right-to-left (R2L) evaluation stream alongside the conventional L2R flow, enabling later reasoning steps to help assess earlier ones in real time. Notably, the built-in R2L evaluation is implemented solely through prompt modifications that reverse the original reasoning trajectory, without any additional parameters or inference latency introduced. This ensures BiPRM remains both efficient and broadly compatible with existing PRM studies. We conduct extensive experiments on two mathematical reasoning benchmarks using samples generated by three different policy models. Our method, BiPRM, is evaluated across three backbones and three distinct PRM objectives. Across all settings, BiPRM consistently outperforms unidirectional baselines, achieving up to a 31.9% improvement in stepwise reward evaluation. Generally, our results highlight BiPRM's effectiveness, robustness, and general applicability, offering a promising new direction for process-based reward modeling.

</details>


### [89] [Collaborative Chain-of-Agents for Parametric-Retrieved Knowledge Synergy](https://arxiv.org/abs/2508.01696)

*Yi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, Lizhe Zhang, Yan Liu, Bin Qin*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Multi-agent reasoning

**Relevance Score:** 9

**TL;DR:** Collaborative Chain-of-Agents (CoCoA) enhances the synergy between parametric and retrieved knowledge in Retrieval-Augmented Generation (RAG) frameworks, leading to improved performance in QA tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To fully exploit knowledge during generation in RAG methods, addressing the limited synergy between internal and external knowledge.

**Method:** Introduced CoCoA-zero for conditional knowledge induction followed by reasoning; developed CoCoA for long-chain training of multi-agent reasoning trajectories.

**Key Contributions:**

	1. CoCoA-zero multi-agent framework for conditional knowledge induction
	2. CoCoA long-chain training strategy for enhanced reasoning
	3. Improved performance on knowledge-intensive QA tasks

**Result:** CoCoA-zero and CoCoA showed superior performance on open-domain and multi-hop QA tasks.

**Limitations:** 

**Conclusion:** The proposed framework effectively integrates and leverages parametric and retrieved knowledge, advancing RAG capabilities.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a promising framework for enhancing the capabilities of Large Language Models (LLMs), especially in knowledge-intensive tasks. Despite its advantages, current RAG methods often struggle to *fully exploit knowledge during generation*. In particular, the synergy between the model's internal parametric knowledge and external retrieved knowledge remains limited. Retrieved contents may sometimes mislead generation, while certain generated content can guide the model toward more accurate outputs. In this work, we propose Collaborative Chain-of-Agents, a framework designed to enhance explicitly synergy over both parametric and retrieved knowledge. Specifically, we first introduce CoCoA-zero, a multi-agent RAG framework that first performs conditional knowledge induction and then reasons answers. Building on this, we develop CoCoA, a long-chain training strategy that synthesizes extended multi-agent reasoning trajectories from CoCoA-zero to fine-tune the LLM. This strategy enhances the model's capability to explicitly integrate and jointly leverage parametric and retrieved knowledge. Experiments results show that CoCoA-zero and CoCoA achieve superior performance on open-domain and multi-hop QA tasks.

</details>


### [90] [Am I Blue or Is My Hobby Counting Teardrops? Expression Leakage in Large Language Models as a Symptom of Irrelevancy Disruption](https://arxiv.org/abs/2508.01708)

*Berkay Köprü, Mehrzad Mashal, Yigit Gurses, Akos Kadar, Maximilian Schmitt, Ditty Mathew, Felix Burkhardt, Florian Eyben, Björn W. Schuller*

**Main category:** cs.CL

**Keywords:** large language models, expression leakage, natural language processing, benchmark dataset, sentiment analysis

**Relevance Score:** 8

**TL;DR:** The paper introduces the concept of expression leakage in large language models (LLMs), highlighting their tendency to generate sentimentally charged expressions unrelated to input context. It presents a benchmark dataset and an evaluation method, showing that expression leakage can be reduced with model scaling but requires careful construction of the models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of LLMs incorporating irrelevant information and to introduce a novel concept called expression leakage, where sentimentally charged expressions arise despite being semantically unrelated to input context.

**Method:** The authors collected a benchmark dataset, developed a scheme to generate a dataset from common-crawl data, and created an automatic evaluation pipeline that correlates well with human judgment for assessing expression leakage.

**Key Contributions:**

	1. Introduction of expression leakage as a novel phenomenon in LLMs
	2. Development of a benchmark dataset and automatic evaluation pipeline for assessing expression leakage
	3. Demonstration of the relationship between model scaling and expression leakage mitigation

**Result:** The findings indicate that expression leakage decreases as model size increases, but mitigation requires careful attention during model construction, as it cannot be addressed through prompting strategies.

**Limitations:** The evaluation method may require further validation across diverse contexts and LLM architectures.

**Conclusion:** Expression leakage is a significant issue for LLMs that necessitates specific model building strategies. The effects of negative sentiment in prompts lead to higher expression leakage compared to positive sentiment.

**Abstract:** Large language models (LLMs) have advanced natural language processing (NLP) skills such as through next-token prediction and self-attention, but their ability to integrate broad context also makes them prone to incorporating irrelevant information. Prior work has focused on semantic leakage, bias introduced by semantically irrelevant context. In this paper, we introduce expression leakage, a novel phenomenon where LLMs systematically generate sentimentally charged expressions that are semantically unrelated to the input context. To analyse the expression leakage, we collect a benchmark dataset along with a scheme to automatically generate a dataset from free-form text from common-crawl. In addition, we propose an automatic evaluation pipeline that correlates well with human judgment, which accelerates the benchmarking by decoupling from the need of annotation for each analysed model. Our experiments show that, as the model scales in the parameter space, the expression leakage reduces within the same LLM family. On the other hand, we demonstrate that expression leakage mitigation requires specific care during the model building process, and cannot be mitigated by prompting. In addition, our experiments indicate that, when negative sentiment is injected in the prompt, it disrupts the generation process more than the positive sentiment, causing a higher expression leakage rate.

</details>


### [91] [CultureGuard: Towards Culturally-Aware Dataset and Guard Model for Multilingual Safety Applications](https://arxiv.org/abs/2508.01710)

*Raviraj Joshi, Rakesh Paul, Kanishk Singla, Anusha Kamath, Michael Evans, Katherine Luna, Shaona Ghosh, Utkarsh Vaidya, Eileen Long, Sanjay Singh Chauhan, Niranjan Wartikar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Content Safety, Multilingual Datasets, Synthetic Data Generation, Cultural Adaptation

**Relevance Score:** 9

**TL;DR:** This paper introduces CultureGuard, a pipeline for creating culturally aligned safety datasets for LLMs in multiple languages, addressing the gap in multilingual content safety.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for robust safety guard models for Large Language Models (LLMs) in non-English languages due to a lack of culturally aligned labeled datasets.

**Method:** The study presents a four-stage synthetic data generation and filtering pipeline that includes cultural data segregation, adaptation, machine translation, and quality filtering.

**Key Contributions:**

	1. Introduction of a robust four-stage synthetic data pipeline
	2. Creation of a comprehensive multilingual dataset
	3. Demonstration of state-of-the-art performance in multilingual content safety

**Result:** The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, contains 386,661 samples across 9 languages and facilitates state-of-the-art performance in multilingual content safety benchmarks.

**Limitations:** 

**Conclusion:** This work significantly advances the development of culturally aware safety guard models, helping to close the safety gap in multilingual LLM applications.

**Abstract:** The increasing use of Large Language Models (LLMs) in agentic applications highlights the need for robust safety guard models. While content safety in English is well-studied, non-English languages lack similar advancements due to the high cost of collecting culturally aligned labeled datasets. We present CultureGuard, a novel solution for curating culturally aligned, high-quality safety datasets across multiple languages. Our approach introduces a four-stage synthetic data generation and filtering pipeline: cultural data segregation, cultural data adaptation, machine translation, and quality filtering. This pipeline enables the conversion and expansion of the Nemotron-Content-Safety-Dataset-V2 English safety dataset into eight distinct languages: Arabic, German, Spanish, French, Hindi, Japanese, Thai, and Chinese. The resulting dataset, Nemotron-Content-Safety-Dataset-Multilingual-v1, comprises 386,661 samples in 9 languages and facilitates the training of Llama-3.1-Nemotron-Safety-Guard-Multilingual-8B-v1 via LoRA-based fine-tuning. The final model achieves state-of-the-art performance on several multilingual content safety benchmarks. We also benchmark the latest open LLMs on multilingual safety and observe that these LLMs are more prone to give unsafe responses when prompted in non-English languages. This work represents a significant step toward closing the safety gap in multilingual LLMs by enabling the development of culturally aware safety guard models.

</details>


### [92] [Enhancing the Preference Extractor in Multi-turn Dialogues: From Annotating Disasters to Accurate Preference Extraction](https://arxiv.org/abs/2508.01739)

*Cheng Wang, ziru Liu, Pengcheng Tang, Mingyu Zhang, Quanyu Dai, Yue Zhu*

**Main category:** cs.CL

**Keywords:** dialogue systems, user preferences, large language models, data generation, annotation efficiency

**Relevance Score:** 8

**TL;DR:** The paper introduces IterChat, a novel framework for generating dialogue datasets that improves user preference extraction in dialogue systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Identifying user preferences in dialogue systems enhances service satisfaction, but challenges arise from obtaining high-quality labeled data for multi-turn dialogues.

**Method:** The authors developed a new dialogue data format that organizes dialogue data into attributed historical preferences and one-turn dialogues, utilizing GPT-4 for dataset generation by pre-defining preference slots and sampling them.

**Key Contributions:**

	1. Introduction of the IterChat framework for dialogue data generation
	2. Creation of an efficient dialogue data format that categorizes preferences
	3. Demonstrated performance improvements in preference extraction and annotation efficiency.

**Result:** Experiments show that the new dialogue format leads to superior performance in preference extraction tasks, with improved annotator efficiency indicated by a 28.4% higher win rate compared to traditional methods.

**Limitations:** 

**Conclusion:** The proposed IterChat framework reduces annotation errors and enhances model training by simplifying preference extraction processes.

**Abstract:** Identifying user preferences in dialogue systems is a pivotal aspect of providing satisfying services. Current research shows that using large language models (LLMs) to fine-tune a task-specific preference extractor yields excellent results in terms of accuracy and generalization. However, the primary challenge stems from the inherent difficulty in obtaining high-quality labeled multi-turn dialogue data. Accurately tracking user preference transitions across turns not only demands intensive domain expertise and contextual consistency maintenance for annotators (termed \textbf{``Annotating Disaster''}) but also complicates model training due to error propagation in sequential dependency learning. Inspired by the observation that multi-turn preference extraction can be decomposed into iterative executions of one-turn extraction processes. We propose a novel dialogue data generation framework named \textbf{IterChat}. First, we construct a new data format that categorizes the dialogue data into attributed historical preferences and one-turn dialogues. This reduces the probability of annotation errors and improves annotation efficiency. Then, to generate a high-quality and diverse dialogue dataset, we adopt GPT4 to pre-define the preference slots in the target preference extractor task and then randomly sample the subset of the slots and their corresponding schema values to create the dialogue datasets. Experimental results indicate that fine-tuning or only few-shot prompting with the new dialogue format yields superior performance compared to the original multi-turn dialogues. Additionally, the new data format improves annotator efficiency with a win rate of 28.4\% higher than the original multi-turn dialogues.

</details>


### [93] [AI-Generated Text is Non-Stationary: Detection via Temporal Tomography](https://arxiv.org/abs/2508.01754)

*Alva West, Yixuan Weng, Minjun Zhu, Luodan Zhang, Zhen Lin, Guangsheng Bao, Yue Zhang*

**Main category:** cs.CL

**Keywords:** AI-generated text, Detection, Temporal Discrepancy Tomography, Signal processing, Non-stationarity

**Relevance Score:** 7

**TL;DR:** This paper introduces Temporal Discrepancy Tomography (TDT), a novel approach for AI-generated text detection that captures positional information and addresses limitations of existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current detection methods for AI-generated text overlook positional information and fail against localized adversarial attacks.

**Method:** The paper proposes TDT, which reformulates text detection as a signal processing task by treating token-level discrepancies as a time-series signal, applying Continuous Wavelet Transform for analysis.

**Key Contributions:**

	1. Introduction of Temporal Discrepancy Tomography (TDT) for text detection
	2. Demonstration of non-stationarity as a fundamental characteristic of AI-generated text
	3. Significant improvements in detection performance on standard benchmarks and adversarial tasks.

**Result:** TDT achieves 0.855 AUROC on the RAID benchmark, outperforming the best baseline by 7.1%, and shows a 14.1% AUROC improvement against paraphrasing attacks.

**Limitations:** 

**Conclusion:** The study demonstrates the importance of non-stationarity in AI-generated text and the need for methods that preserve temporal dynamics for effective detection.

**Abstract:** The field of AI-generated text detection has evolved from supervised classification to zero-shot statistical analysis. However, current approaches share a fundamental limitation: they aggregate token-level measurements into scalar scores, discarding positional information about where anomalies occur. Our empirical analysis reveals that AI-generated text exhibits significant non-stationarity, statistical properties vary by 73.8\% more between text segments compared to human writing. This discovery explains why existing detectors fail against localized adversarial perturbations that exploit this overlooked characteristic. We introduce Temporal Discrepancy Tomography (TDT), a novel detection paradigm that preserves positional information by reformulating detection as a signal processing task. TDT treats token-level discrepancies as a time-series signal and applies Continuous Wavelet Transform to generate a two-dimensional time-scale representation, capturing both the location and linguistic scale of statistical anomalies. On the RAID benchmark, TDT achieves 0.855 AUROC (7.1\% improvement over the best baseline). More importantly, TDT demonstrates robust performance on adversarial tasks, with 14.1\% AUROC improvement on HART Level 2 paraphrasing attacks. Despite its sophisticated analysis, TDT maintains practical efficiency with only 13\% computational overhead. Our work establishes non-stationarity as a fundamental characteristic of AI-generated text and demonstrates that preserving temporal dynamics is essential for robust detection.

</details>


### [94] [A comprehensive taxonomy of hallucinations in Large Language Models](https://arxiv.org/abs/2508.01781)

*Manuel Cossio*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, taxonomy, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** This report presents a detailed taxonomy of hallucinations in large language models (LLMs), examining their causes, manifestations, and mitigation strategies, while underscoring the inherent inevitability of such issues in LLMs.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical challenge posed by hallucinations in LLMs, which generate plausible but factually incorrect content, and to provide a structured understanding of their types and implications for responsible use.

**Method:** The report offers a comprehensive taxonomy of LLM hallucinations, categorizing them into intrinsic and extrinsic types, and evaluates their causes, assessment metrics, and mitigation strategies.

**Key Contributions:**

	1. A formal taxonomy classifying LLM hallucinations
	2. Identification of intrinsic vs extrinsic hallucinations
	3. Proposed measures for monitoring and mitigating hallucinations in LLM applications

**Result:** It identifies various manifestations of hallucinations such as factual errors and ethical violations, and explores factors influencing hallucination perception, proposing a framework for assessment and improvement.

**Limitations:** The report may not cover all potential hallucination scenarios and is primarily focused on theoretical aspects; practical evaluation and real-world application may vary.

**Conclusion:** The study emphasizes that hallucinations are an inevitable aspect of LLMs, suggesting that future work should focus on improving detection and mitigation strategies, alongside ensuring human oversight in deployment.

**Abstract:** Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.

</details>


### [95] [HeQ: a Large and Diverse Hebrew Reading Comprehension Benchmark](https://arxiv.org/abs/2508.01812)

*Amir DN Cohen, Hilla Merhav, Yoav Goldberg, Reut Tsarfaty*

**Main category:** cs.CL

**Keywords:** Hebrew NLP, Machine Reading Comprehension, Natural Language Understanding, Morphologically Rich Languages, Question Answering

**Relevance Score:** 4

**TL;DR:** The paper presents a new Hebrew Machine Reading Comprehension (MRC) dataset addressing semantic analysis challenges in Hebrew NLP and proposes improved evaluation metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a focus on semantic understanding in Hebrew NLP benchmarks and to improve reading comprehension through MRC.

**Method:** Developed a controlled crowdsourcing protocol and novel guidelines for annotating a custom Hebrew MRC dataset with 30,147 question-answer pairs.

**Key Contributions:**

	1. Creation of a novel Hebrew MRC dataset (HeQ) with extensive question-answer pairs.
	2. Development of tailored evaluation metrics for morphologically rich languages.
	3. Insights into the performance gap between morpho-syntactic and semantic task models.

**Result:** The study found that existing evaluation metrics like F1 scores and Exact Match do not work effectively for Hebrew, and proposed new metrics. It also demonstrated low correlation between performance on morpho-syntactic tasks and reading comprehension.

**Limitations:** The dataset and metrics are specifically designed for Hebrew, potentially limiting generalizability to other languages.

**Conclusion:** The HeQ dataset and the proposed evaluation metrics pave the way for better NLU models for Hebrew and similar morphologically rich languages.

**Abstract:** Current benchmarks for Hebrew Natural Language Processing (NLP) focus mainly on morpho-syntactic tasks, neglecting the semantic dimension of language understanding. To bridge this gap, we set out to deliver a Hebrew Machine Reading Comprehension (MRC) dataset, where MRC is to be realized as extractive Question Answering. The morphologically rich nature of Hebrew poses a challenge to this endeavor: the indeterminacy and non-transparency of span boundaries in morphologically complex forms lead to annotation inconsistencies, disagreements, and flaws in standard evaluation metrics.   To remedy this, we devise a novel set of guidelines, a controlled crowdsourcing protocol, and revised evaluation metrics that are suitable for the morphologically rich nature of the language. Our resulting benchmark, HeQ (Hebrew QA), features 30,147 diverse question-answer pairs derived from both Hebrew Wikipedia articles and Israeli tech news. Our empirical investigation reveals that standard evaluation metrics such as F1 scores and Exact Match (EM) are not appropriate for Hebrew (and other MRLs), and we propose a relevant enhancement.   In addition, our experiments show low correlation between models' performance on morpho-syntactic tasks and on MRC, which suggests that models designed for the former might underperform on semantics-heavy tasks. The development and exploration of HeQ illustrate some of the challenges MRLs pose in natural language understanding (NLU), fostering progression towards more and better NLU models for Hebrew and other MRLs.

</details>


### [96] [AGENTICT$^2$S:Robust Text-to-SPARQL via Agentic Collaborative Reasoning over Heterogeneous Knowledge Graphs for the Circular Economy](https://arxiv.org/abs/2508.01815)

*Yang Zhao, Chengxiao Dai, Wei Zhuo, Tan Chuan Fu, Yue Xiu, Dusit Niyato, Jonathan Z. Low, Eugene Ho Hong Zhuang, Daren Zong Loong Tan*

**Main category:** cs.CL

**Keywords:** knowledge graphs, question answering, agent-based reasoning, sustainability, circular economy

**Relevance Score:** 6

**TL;DR:** AgenticT²S is a modular framework for question answering over heterogeneous knowledge graphs, improving accuracy and efficiency in diverse domains like the circular economy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The difficulty in reasoning across heterogeneous knowledge graphs (KGs) due to incomplete alignments and diverse schemas makes effective KGQA challenging, especially in low-resource domains.

**Method:** AgenticT²S decomposes KGQA into subtasks managed by specialized agents for retrieval, query generation, and verification, employing weak-to-strong alignment strategies for scheduling.

**Key Contributions:**

	1. Modular framework decomposing KGQA into specialized subtasks
	2. Improved execution accuracy and F₁ score on circular economy datasets
	3. Reduction in average prompt length for queries

**Result:** AgenticT²S improves execution accuracy by 17.3% and F₁ score by 25.4%, while reducing average prompt length by 46.4% on real-world circular economy KGs.

**Limitations:** 

**Conclusion:** The results affirm the advantages of agent-based schema-aware reasoning for scalable KGQA, facilitating decision-making in sustainability fields through effective cross-graph reasoning.

**Abstract:** Question answering over heterogeneous knowledge graphs (KGQA) involves reasoning across diverse schemas, incomplete alignments, and distributed data sources. Existing text-to-SPARQL approaches rely on large-scale domain-specific fine-tuning or operate within single-graph settings, limiting their generalizability in low-resource domains and their ability to handle queries spanning multiple graphs. These challenges are particularly relevant in domains such as the circular economy, where information about classifications, processes, and emissions is distributed across independently curated knowledge graphs (KGs). We present AgenticT$^2$S, a modular framework that decomposes KGQA into subtasks managed by specialized agents responsible for retrieval, query generation, and verification. A scheduler assigns subgoals to different graphs using weak-to-strong alignment strategies. A two-stage verifier detects structurally invalid and semantically underspecified queries through symbolic validation and counterfactual consistency checks. Experiments on real-world circular economy KGs demonstrate that AgenticT$^2$S improves execution accuracy by 17.3% and triple level F$_1$ by 25.4% over the best baseline, while reducing the average prompt length by 46.4%. These results demonstrate the benefits of agent-based schema-aware reasoning for scalable KGQA and support decision-making in sustainability domains through robust cross-graph reasoning.

</details>


### [97] [MLP Memory: Language Modeling with Retriever-pretrained External Memory](https://arxiv.org/abs/2508.01832)

*Rubin Wei, Jiaqi Cao, Jiarui Wang, Jushi Kai, Qipeng Guo, Bowen Zhou, Zhouhan Lin*

**Main category:** cs.CL

**Keywords:** large language models, retriever-augmented generation, external memory, hallucinations, machine learning

**Relevance Score:** 9

**TL;DR:** The paper proposes a novel architecture combining a transformer decoder and a differentiable external memory to reduce hallucinations in LLMs and enhance performance on knowledge-intensive tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address hallucination issues in decoder-only LLMs, making them more suitable for knowledge-intensive tasks.

**Method:** The proposed method decouples memorization from the LLM using a pretrained, differentiable external memory in the form of an MLP, designed to imitate the behavior of a retriever on the pretraining dataset.

**Key Contributions:**

	1. Introduction of a differentiable external memory to LLMs
	2. Demonstrated superior performance on hallucination benchmarks
	3. Achieved significant speedup compared to existing methodologies

**Result:** The architecture shows significant improvements in perplexity and performance on downstream tasks, achieving up to 24.1% improvement on WikiText-103 and Web datasets, while maintaining efficient speed during inference.

**Limitations:** 

**Conclusion:** The architecture enhances LLM performance on various tasks, reduces hallucinations, and shows substantial inference speedup without impairing reasoning abilities.

**Abstract:** While modern decoder-only LLMs achieve superior performance across various domains, hallucinations have risen to be a common problem in their generated text, hindering their application in knowledge-intensive tasks. Retriever-augmented generation (RAG) offers a solution, but the non-parametric nature of the retriever hinders its deep interaction with LLM. In this work, we propose to decouple memorization from the LLM decoder using a pretrained, differentiable external memory. The external memory is an MLP pretrained by imitating the behavior of a retriever on the entire pretraining dataset. Our resulting architecture, which comprises a transformer decoder and an external MLP memory pretrained on language modeling and retriever imitation respectively, demonstrates strong perplexity and performance on downstream tasks. Experiments show our architecture exhibits steeper power-law scaling with model size, achieving 17.5% and 24.1% improvement on WikiText-103 and Web datasets compared to decoder-only models while benefiting from added training without overfitting. We demonstrate superior performance on three hallucination benchmarks and nine memory-intensive tasks. Additionally, our approach delivers $80\times$ speedup over $k$NN-LM (500M tokens) and $1.3\times$ faster inference than decoder-only models. Unlike $k$NN-LM, which impairs reasoning, our MLP memory improves StrategyQA performance. We will open-source our code and models in the future.

</details>


### [98] [Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes](https://arxiv.org/abs/1907.00326)

*Jie Cao, Michael Tanana, Zac E. Imel, Eric Poitras, David C. Atkins, Vivek Srikumar*

**Main category:** cs.CL

**Keywords:** Motivational Interviewing, dialogue analysis, neural networks, real-time guidance, psychotherapy

**Relevance Score:** 8

**TL;DR:** This paper develops neural network models to analyze and provide real-time guidance for therapists using Motivational Interviewing in psychotherapy, focusing on classifying behavioral codes and forecasting dialogue.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding and guiding therapist-client interactions in psychotherapy, specifically using Motivational Interviewing techniques, to improve therapeutic outcomes.

**Method:** Neural network models are designed to classify MI behavioral codes and forecast future codes in therapy dialogues, grounded in recent dialogue modeling successes.

**Key Contributions:**

	1. Introduction of a dialogue observer for MI behavioral codes
	2. Development of neural network models for real-time conversation guidance
	3. Insights into network design tradeoffs for therapy dialogue

**Result:** The developed models outperform several baseline approaches in accurately classifying and forecasting MI behavioral codes in therapy sessions.

**Limitations:** The study may have limitations in generalizability across different therapeutic approaches beyond Motivational Interviewing.

**Conclusion:** The study highlights the design tradeoffs in neural network architecture for effective modeling of therapy dialogues and the potential for real-time guidance in therapy settings.

**Abstract:** Automatically analyzing dialogue can help understand and guide behavior in domains such as counseling, where interactions are largely mediated by conversation. In this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing (MI), which is effective for addressing substance abuse and related problems. Specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client MI behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. For both tasks, we define neural network models that build upon recent successes in dialogue modeling. Our experiments demonstrate that our models can outperform several baselines for both tasks. We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue.

</details>


### [99] [Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web Agents](https://arxiv.org/abs/2508.01858)

*Yuhan Guo, Cong Guo, Aiwen Sun, Hongliang He, Xinyu Yang, Yue Lu, Yingji Zhang, Xuntao Guo, Dong Zhang, Jianzhuang Liu, Jiang Duan, Yijia Xiao, Liangjian Wen, Hai-Ming Xu, Yong Dai*

**Main category:** cs.CL

**Keywords:** web agents, cognitive reasoning, knowledge acquisition, Chain-of-Thought, HCI

**Relevance Score:** 8

**TL;DR:** This paper presents the Web-CogKnowledge Framework for web agents, emphasizing the importance of knowledge acquisition and cognitive reasoning in their development. It introduces Web-CogDataset for structured knowledge and proposes a knowledge-driven Chain-of-Thought reasoning framework implemented in the Web-CogReasoner, demonstrating significant advancements in agent capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for web agents to effectively engage in cognitive reasoning by first acquiring sufficient knowledge.

**Method:** The paper proposes the Web-CogKnowledge Framework, categorizing knowledge into Factual, Conceptual, and Procedural types, and constructs the Web-CogDataset from 14 real-world websites. It operationalizes knowledge acquisition through a knowledge-driven Chain-of-Thought reasoning framework called Web-CogReasoner.

**Key Contributions:**

	1. Introduction of the Web-CogKnowledge Framework
	2. Development of the Web-CogDataset for knowledge acquisition
	3. Implementation of the knowledge-driven Chain-of-Thought reasoning framework in the Web-CogReasoner.

**Result:** The Web-CogReasoner demonstrates significant superiority over existing models, particularly in generalizing to unseen tasks, highlighting the importance of structured knowledge.

**Limitations:** 

**Conclusion:** The study emphasizes the necessity of a structured approach to knowledge acquisition and cognitive reasoning in developing advanced web agents, with open-sourced resources for further research.

**Abstract:** Multimodal large-scale models have significantly advanced the development of web agents, enabling perception and interaction with digital environments akin to human cognition. In this paper, we argue that web agents must first acquire sufficient knowledge to effectively engage in cognitive reasoning. Therefore, we decompose a web agent's capabilities into two essential stages: knowledge content learning and cognitive processes. To formalize this, we propose Web-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and Procedural. In this framework, knowledge content learning corresponds to the agent's processes of Memorizing and Understanding, which rely on the first two knowledge types, representing the "what" of learning. Conversely, cognitive processes correspond to Exploring, grounded in Procedural knowledge, defining the "how" of reasoning and action. To facilitate knowledge acquisition, we construct the Web-CogDataset, a structured resource curated from 14 real-world websites, designed to systematically instill core knowledge necessary for web agent. This dataset serves as the agent's conceptual grounding-the "nouns" upon which comprehension is built-as well as the basis for learning how to reason and act. Building on this foundation, we operationalize these processes through a novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing and training our proposed agent, the Web-CogReasoner. Extensive experimentation reveals its significant superiority over existing models, especially in generalizing to unseen tasks where structured knowledge is decisive. To enable rigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation suite designed to assess and compare agent performance across the delineated knowledge domains and cognitive capabilities. Our code and data is open sourced at https://github.com/Gnonymous/Web-CogReasoner

</details>


### [100] [Enhancing Talk Moves Analysis in Mathematics Tutoring through Classroom Teaching Discourse](https://arxiv.org/abs/2412.13395)

*Jie Cao, Abhijit Suresh, Jennifer Jacobs, Charis Clevenger, Amanda Howard, Chelsea Brown, Brent Milne, Tom Fischaber, Tamara Sumner, James H. Martin*

**Main category:** cs.CL

**Keywords:** Human tutoring, Machine learning, SAGA22 dataset, Tutoring discourse, Talk moves

**Relevance Score:** 6

**TL;DR:** This paper analyzes mathematics tutoring discourse using talk moves and presents SAGA22 dataset to improve machine learning models for tutoring.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and enhance student learning through effective tutoring interventions while addressing challenges in scaling dialogue analysis for machine learning.

**Method:** Analysis of tutoring discourse using the talk moves framework, development of the SAGA22 dataset, and exploration of various machine learning modeling strategies.

**Key Contributions:**

	1. Introduction of the SAGA22 dataset for tutoring discourse analysis.
	2. Demonstration of the effectiveness of supplementary pretraining on classroom datasets.
	3. Identification of key factors (dialogue context and speaker information) that enhance model performance.

**Result:** The research shows that supplementary pretraining on classroom data boosts model performance in tutoring contexts, especially with longer dialogue context and speaker information.

**Limitations:** The challenges in effectively modeling talk moves and the resource-intensive nature of the annotation process.

**Conclusion:** The study highlights the potential of utilizing existing classroom data for improving tutoring dialogue modeling and identifies the need for further research in talk move modeling.

**Abstract:** Human tutoring interventions play a crucial role in supporting student learning, improving academic performance, and promoting personal growth. This paper focuses on analyzing mathematics tutoring discourse using talk moves - a framework of dialogue acts grounded in Accountable Talk theory. However, scaling the collection, annotation, and analysis of extensive tutoring dialogues to develop machine learning models is a challenging and resource-intensive task. To address this, we present SAGA22, a compact dataset, and explore various modeling strategies, including dialogue context, speaker information, pretraining datasets, and further fine-tuning. By leveraging existing datasets and models designed for classroom teaching, our results demonstrate that supplementary pretraining on classroom data enhances model performance in tutoring settings, particularly when incorporating longer context and speaker information. Additionally, we conduct extensive ablation studies to underscore the challenges in talk move modeling.

</details>


### [101] [Counterfactual Probing for Hallucination Detection and Mitigation in Large Language Models](https://arxiv.org/abs/2508.01862)

*Yijun Feng*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucinations, counterfactual probing, real-time verification, factually incorrect

**Relevance Score:** 9

**TL;DR:** Proposed Counterfactual Probing method detects and mitigates hallucinations in LLM outputs using dynamically generated counterfactual statements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations in LLM outputs, which are fluent yet factually incorrect.

**Method:** Counterfactual Probing dynamically generates plausible counterfactual statements with subtle factual errors and assesses the model's confidence in response to these variations.

**Key Contributions:**

	1. Introduction of Counterfactual Probing to detect hallucinations in LLM outputs
	2. Demonstrated superior detection performance on multiple datasets
	3. Adaptive mitigation strategies that significantly reduce hallucination scores

**Result:** Counterfactual Probing outperforms baseline methods in detecting hallucinations and reduces hallucination scores by an average of 24.5%.

**Limitations:** 

**Conclusion:** The method integrates seamlessly into existing LLM pipelines as a real-time verification mechanism without requiring model retraining.

**Abstract:** Large Language Models have demonstrated remarkable capabilities across diverse tasks, yet they frequently generate hallucinations outputs that are fluent but factually incorrect or unsupported. We propose Counterfactual Probing, a novel approach for detecting and mitigating hallucinations in LLM outputs. Our method dynamically generates counterfactual statements that appear plausible but contain subtle factual errors, then evaluates the model's sensitivity to these perturbations. We hypothesize that genuine knowledge exhibits robustness to counterfactual variations, while hallucinated content shows inconsistent confidence patterns when confronted with plausible alternatives. Our comprehensive evaluation on TruthfulQA, factual statement datasets, and curated hallucination examples demonstrates that counterfactual probing achieves superior detection performance compared to baseline methods, while our adaptive mitigation strategies reduce hallucination scores by an average of 24.5%. The approach requires no model retraining and can be integrated into existing LLM pipelines as a realtime verification mechanism.

</details>


### [102] [Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language](https://arxiv.org/abs/2508.01918)

*Jaskaranjeet Singh, Rakesh Thakur*

**Main category:** cs.CL

**Keywords:** Punjabi, Large Language Models, Retrieval-Augmented Generation, Quantum Retrieval, Low-Resource NLP

**Relevance Score:** 8

**TL;DR:** This work introduces PunGPT2, a suite of open-source large language models for Punjabi, enhancing NLP capabilities for low-resource languages and pioneering quantum-aware retrieval methods with improved contextual relevance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the exclusion of low-resource languages from the NLP landscape and enhance the capabilities of language technologies for Punjabi.

**Method:** PunGPT2 was trained on a 35GB corpus using a tokenizer optimized with byte pair encoding, supplemented by a retrieval-augmented generation framework called Pun-RAG. Quantum-RAG was also introduced to combine sparse and dense retrieval methods using quantum-inspired semantic matching.

**Key Contributions:**

	1. Development of the first open-source Punjabi LLMs (PunGPT2)
	2. Introduction of Pun-RAG for improved factual grounding and recall
	3. Proposal of Quantum-RAG for hybrid retrieval combining sparse and dense methods with quantum techniques.

**Result:** PunGPT2 outperforms multilingual baselines like mBERT, mT5, and MuRIL in perplexity, factuality, and fluency.

**Limitations:** 

**Conclusion:** This work presents a framework for extending LLM capabilities to underrepresented languages and integrates quantum retrieval methods in low-resource NLP.

**Abstract:** Despite the rapid advancement of large language models (LLMs), low-resource languages remain largely excluded from the NLP landscape. We present PunGPT2, the first fully open-source suite of Punjabi large language models, trained from scratch on a 35GB domain-diverse corpus encompassing literature, religious texts, news, and social discourse. Unlike prior multilingual approaches, PunGPT2 captures rich syntactic and morphological features unique to Punjabi through a tokenizer optimised with byte pair encoding and linguistically aligned pretraining objectives. To improve factual grounding and domain recall, we introduce Pun-RAG, a retrieval-augmented generation framework combining PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant using QLoRA, enabling robust zero-shot and instruction-following performance with significantly reduced compute needs.   As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system that fuses sparse (BM25) and dense methods with quantum-inspired semantic matching. By encoding queries using amplitude-based embeddings and retrieving via quantum kernel similarity, Quantum-RAG achieves improved contextual relevance with minimal memory overhead marking the first practical integration of quantum representations in low-resource language generation. Our models significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in perplexity, factuality, and fluency. This work provides a scalable, reproducible blueprint for extending LLM capabilities to underrepresented languages and pioneers quantum-aware retrieval in low-resource NLP

</details>


### [103] [Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback](https://arxiv.org/abs/2508.01930)

*Tom S. Juzek, Zina B. Ward*

**Main category:** cs.CL

**Keywords:** Large Language Models, Learning from Human Feedback, Lexical Overuse, Explainable AI, Alignment Research

**Relevance Score:** 8

**TL;DR:** This study explores how Learning from Human Feedback (LHF) affects the lexical choices of Large Language Models (LLMs), demonstrating a potential misalignment between the preferences of LHF workers and LLM users.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reasons behind the lexical overuse of certain terms in LLMs and the influence of Learning from Human Feedback (LHF) on these preferences.

**Method:** The study uses Meta's Llama model to identify lexical preferences potentially induced by LHF and conducts experiments to link LHF to lexical overuse by analyzing participant text preferences.

**Key Contributions:**

	1. Developed a procedure for detecting LHF-induced lexical preferences in LLMs.
	2. Concretely linked LHF to lexical overuse through experimental emulation.
	3. Emphasized the importance of understanding differing lexical expectations of populations involved.

**Result:** Participants showed a systematic preference for text variants that included certain overused words, indicating LHF's influence on LLM lexical choices.

**Limitations:** The study focuses on a limited set of lexical terms and does not address the broader implications of misalignment in detail.

**Conclusion:** The findings reveal a possible misalignment in lexical expectations between LHF workers and LLM users, highlighting the need for transparency in alignment research.

**Abstract:** Large Language Models (LLMs) are known to overuse certain terms like "delve" and "intricate." The exact reasons for these lexical choices, however, have been unclear. Using Meta's Llama model, this study investigates the contribution of Learning from Human Feedback (LHF), under which we subsume Reinforcement Learning from Human Feedback and Direct Preference Optimization. We present a straightforward procedure for detecting the lexical preferences of LLMs that are potentially LHF-induced. Next, we more conclusively link LHF to lexical overuse by experimentally emulating the LHF procedure and demonstrating that participants systematically prefer text variants that include certain words. This lexical overuse can be seen as a sort of misalignment, though our study highlights the potential divergence between the lexical expectations of different populations -- namely LHF workers versus LLM users. Our work contributes to the growing body of research on explainable artificial intelligence and emphasizes the importance of both data and procedural transparency in alignment research.

</details>


### [104] [ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks](https://arxiv.org/abs/2508.01943)

*Philip Schroeder, Ondrej Biza, Thomas Weng, Hongyin Luo, James Glass*

**Main category:** cs.CL

**Keywords:** vision-language models, video reasoning, embodied AI, subtask decomposition, NLP

**Relevance Score:** 6

**TL;DR:** ROVER is a framework for improving reasoning in vision-language models by decomposing video tasks into shorter subtasks, which enhances accuracy and performance for video understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Vision-language models struggle with extended sequences of camera frames in video-based tasks, limiting their effectiveness in real-time embodied settings.

**Method:** ROVER decomposes long video trajectories into shorter segments for focused reasoning, utilizing an in-context learning approach.

**Key Contributions:**

	1. Introduces ROVER for recursive video reasoning
	2. Improves accuracy by focusing on subtasks
	3. Demonstrates scalability with a sliding context window

**Result:** ROVER outperformed baseline models in task progress estimation, frame-level reasoning, and video question answering, while reducing hallucinations and improving time complexity.

**Limitations:** 

**Conclusion:** The approach enhances performance in video reasoning tasks and scales efficiently with video length, making it suitable for real-time applications.

**Abstract:** Vision-language models (VLMs) have exhibited impressive capabilities across diverse image understanding tasks, but still struggle in settings that require reasoning over extended sequences of camera frames from a video. This limits their utility in embodied settings, which require reasoning over long frame sequences from a continuous stream of visual input at each moment of a task attempt. To address this limitation, we propose ROVER (Reasoning Over VidEo Recursively), a framework that enables the model to recursively decompose long-horizon video trajectories into segments corresponding to shorter subtasks within the trajectory. In doing so, ROVER facilitates more focused and accurate reasoning over temporally localized frame sequences without losing global context. We evaluate ROVER, implemented using an in-context learning approach, on diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa that consists of 543 videos showing both expert and perturbed non-expert trajectories across 27 robotic manipulation tasks. ROVER outperforms strong baselines across three video reasoning tasks: task progress estimation, frame-level natural language reasoning, and video question answering. We observe that, by reducing the number of frames the model reasons over at each timestep, ROVER mitigates hallucinations, especially during unexpected or non-optimal moments of a trajectory. In addition, by enabling the implementation of a subtask-specific sliding context window, ROVER's time complexity scales linearly with video length, an asymptotic improvement over baselines. Demos, code, and data available at: https://rover-vlm.github.io

</details>


### [105] [SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension](https://arxiv.org/abs/2508.01959)

*Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, Dit-Yan Yeung, Jie Zhou, Mo Yu*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, situated embedding, natural language processing, contextual retrieval

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel approach for retrieval-augmented generation (RAG) using situated embedding models (SitEmb) that enhance retrieval performance by representing short chunks of text within broader contextual windows.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of accurately interpreting longer documents in retrieval-augmented generation (RAG) due to the limitations of conventional embedding models when processing longer chunks of text.

**Method:** The authors introduce a new training paradigm for situated embedding models (SitEmb) that condition short text chunks on a broader context to improve retrieval performance.

**Key Contributions:**

	1. Introduction of the situated embedding models (SitEmb) paradigm.
	2. Development of a curated book-plot retrieval dataset for evaluation.
	3. Demonstration of substantial performance gains over state-of-the-art systems with fewer parameters.

**Result:** The SitEmb-v1 model outperforms state-of-the-art embedding models, including those with 7-8B parameters, while only using 1B parameters. The enhanced SitEmb-v1.5 model further improves performance by over 10% across multiple languages and applications.

**Limitations:** Existing embedding models are not tailored for situated context; the current method may not generalize to all types of documents or contexts.

**Conclusion:** The new situated embedding models effectively improve the retrieval process in the context of long documents, demonstrating stronger performance compared to existing methods.

**Abstract:** Retrieval-augmented generation (RAG) over long documents typically involves splitting the text into smaller chunks, which serve as the basic units for retrieval. However, due to dependencies across the original document, contextual information is often essential for accurately interpreting each chunk. To address this, prior work has explored encoding longer context windows to produce embeddings for longer chunks. Despite these efforts, gains in retrieval and downstream tasks remain limited. This is because (1) longer chunks strain the capacity of embedding models due to the increased amount of information they must encode, and (2) many real-world applications still require returning localized evidence due to constraints on model or human bandwidth.   We propose an alternative approach to this challenge by representing short chunks in a way that is conditioned on a broader context window to enhance retrieval performance -- i.e., situating a chunk's meaning within its context. We further show that existing embedding models are not well-equipped to encode such situated context effectively, and thus introduce a new training paradigm and develop the situated embedding models (SitEmb). To evaluate our method, we curate a book-plot retrieval dataset specifically designed to assess situated retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3 substantially outperforms state-of-the-art embedding models, including several with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model further improves performance by over 10% and shows strong results across different languages and several downstream applications.

</details>


### [106] [TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2508.01977)

*Fan Gao, Cheng Huang, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng, Hao Wang, Yongbin Yu*

**Main category:** cs.CL

**Keywords:** Tibetan, low-resource languages, large language models, dataset construction, AI inclusivity

**Relevance Score:** 5

**TL;DR:** Introduction of TIBSTC-CoT, a multi-domain Tibetan dataset, and the Sunshine-thinking LLM family for enhancing Tibetan language processing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address data scarcity in Tibetan, a low-resource language, and improve language understanding and generation through the development of targeted datasets and models.

**Method:** Constructed TIBSTC-CoT dataset using chain-of-thought prompting with LLMs, leading to the development of Sunshine-thinking LLMs trained on this dataset.

**Key Contributions:**

	1. Introduction of the TIBSTC-CoT dataset for Tibetan
	2. Development of Sunshine-thinking LLM family
	3. Demonstration of high performance in reasoning and generation tasks for Tibetan language

**Result:** Sunshine-thinking LLMs show strong reasoning and generation capabilities, on par with SOTA multilingual LLMs.

**Limitations:** 

**Conclusion:** The research advances inclusive AI for Tibetan by creating valuable language resources and innovative models.

**Abstract:** To address the severe data scarcity in Tibetan, a low-resource language spoken by over six million people, we introduce TIBSTC-CoT, the large-scale, multi-domain Tibetan dataset automatically constructed via chain-of-thought prompting with large language models (LLMs). TIBSTC-CoT establishes a scalable and reproducible framework for dataset creation in low-resource settings, covering diverse domains and reasoning patterns essential for language understanding and generation. Building on this dataset, we develop the Sunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with chain-of-thought capabilities. Trained entirely on TIBSTC-CoT, Sunshine-thinking has demonstrated strong reasoning and generation performance, comparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a significant step toward inclusive AI by enabling high-quality Tibetan language processing through both resource creation and model innovation. All data are available: https://github.com/Vicentvankor/sun-shine.

</details>


### [107] [Show or Tell? Modeling the evolution of request-making in Human-LLM conversations](https://arxiv.org/abs/2508.01213)

*Shengqi Zhu, Jeffrey M. Rzeszotarski, David Mimno*

**Main category:** cs.CL

**Keywords:** chat queries, user behavior, LLM, segmentation, request-making

**Relevance Score:** 9

**TL;DR:** The paper presents a new task for segmenting chat queries in LLM interactions, highlighting the variances in user behavior and the impact of model capabilities on request-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand user behaviors in LLM interactions and the nature of requests made in chat logs, which differ from human-human interactions.

**Method:** The authors introduce a task of segmenting chat queries into various components, analyzing user expressions and patterns over time.

**Key Contributions:**

	1. Introduction of a new segmentation task for chat queries
	2. Insights into the evolution of user request patterns
	3. Identification of the impact of model capabilities on user behavior

**Result:** Results indicate that user queries evolve from initial focus on requests to a more explored pattern as users gain experience, with model capabilities significantly influencing these behaviors.

**Limitations:** Limited to chat interactions; findings may not apply to other forms of human-computer interactions.

**Conclusion:** Model updates affect community-level user behavior, revealing the need for diachronic analysis to understand interaction dynamics in LLMs.

**Abstract:** Chat logs provide a rich source of information about LLM users, but patterns of user behavior are often masked by the variability of queries. We present a new task, segmenting chat queries into contents of requests, roles, query-specific context, and additional expressions. We find that, despite the familiarity of chat-based interaction, request-making in LLM queries remains significantly different from comparable human-human interactions. With the data resource, we introduce an important perspective of diachronic analyses with user expressions. We find that query patterns vary between early ones emphasizing requests, and individual users explore patterns but tend to converge with experience. Finally, we show that model capabilities affect user behavior, particularly with the introduction of new models, which are traceable at the community level.

</details>


### [108] [Contextually Aware E-Commerce Product Question Answering using RAG](https://arxiv.org/abs/2508.01990)

*Praveen Tangarajan, Anand A. Rajasekar, Manish Rathi, Vinay Rao Dandin, Ozan Ersoy*

**Main category:** cs.CL

**Keywords:** e-commerce, Product Question Answering, Retrieval Augmented Generation, user context, performance metrics

**Relevance Score:** 7

**TL;DR:** Proposes an end-to-end framework for e-commerce Product Question Answering (PQA) utilizing Retrieval Augmented Generation (RAG) for improved user context integration and personalized responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address cognitive overload in e-commerce product pages and improve the effectiveness of Product Question Answering systems by leveraging rich user context and diverse product information.

**Method:** Developed a scalable, end-to-end framework for PQA that integrates conversational history, user profiles, and product attributes, utilizing RAG techniques.

**Key Contributions:**

	1. Integration of user context and product information in PQA
	2. Scalable end-to-end framework leveraging RAG
	3. Introduction of novel metrics for evaluating RAG systems

**Result:** The proposed system can handle various types of queries and identifies information gaps in product catalogs, thereby supporting ongoing improvement.

**Limitations:** .

**Conclusion:** The framework enhances answer delivery in e-commerce environments and introduces new performance metrics for RAG evaluations.

**Abstract:** E-commerce product pages contain a mix of structured specifications, unstructured reviews, and contextual elements like personalized offers or regional variants. Although informative, this volume can lead to cognitive overload, making it difficult for users to quickly and accurately find the information they need. Existing Product Question Answering (PQA) systems often fail to utilize rich user context and diverse product information effectively. We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval Augmented Generation (RAG) that deeply integrates contextual understanding. Our system leverages conversational history, user profiles, and product attributes to deliver relevant and personalized answers. It adeptly handles objective, subjective, and multi-intent queries across heterogeneous sources, while also identifying information gaps in the catalog to support ongoing content improvement. We also introduce novel metrics to measure the framework's performance which are broadly applicable for RAG system evaluations.

</details>


### [109] [Prompting Large Language Models to Detect Dementia Family Caregivers](https://arxiv.org/abs/2508.01999)

*Md Badsha Biswas, Özlem Uzuner*

**Main category:** cs.CL

**Keywords:** Dementia, Caregiving, Social Media, Language Models, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper presents a system for identifying tweets from caregivers of dementia patients, leveraging large language models for binary classification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To assist caregivers of dementia patients by identifying relevant tweets that provide experiences and support, enabling the development of online interventions.

**Method:** The paper addresses a binary classification task using large language models (LLMs) and various prompting strategies to differentiate tweets related to caregiving for dementia.

**Key Contributions:**

	1. Development of a binary classification system for caregiver tweets
	2. Application of LLMs with effective prompting strategies
	3. High performance indicated by a macro F1-score of 0.95

**Result:** The system achieved a high performance with a macro F1-score of 0.95 on both validation and test sets using a simple zero-shot prompt on a fine-tuned model.

**Limitations:** 

**Conclusion:** The study demonstrates the effectiveness of LLMs in identifying tweets related to dementia caregiving, offering valuable insights for future internet-based interventions.

**Abstract:** Social media, such as Twitter, provides opportunities for caregivers of dementia patients to share their experiences and seek support for a variety of reasons. Availability of this information online also paves the way for the development of internet-based interventions in their support. However, for this purpose, tweets written by caregivers of dementia patients must first be identified. This paper demonstrates our system for the SMM4H 2025 shared task 3, which focuses on detecting tweets posted by individuals who have a family member with dementia. The task is outlined as a binary classification problem, differentiating between tweets that mention dementia in the context of a family member and those that do not. Our solution to this problem explores large language models (LLMs) with various prompting methods. Our results show that a simple zero-shot prompt on a fine-tuned model yielded the best results. Our final system achieved a macro F1-score of 0.95 on the validation set and the test set. Our full code is available on GitHub.

</details>


### [110] [SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents](https://arxiv.org/abs/2508.02013)

*Changhao Jiang, Jiajun Sun, Yifei Cao, Jiabao Zhuang, Hui Li, Xiaoran Fan, Ming Zhang, Junjie Ye, Shihan Dou, Zhiheng Xi, Jingqi Tong, Yilong Wu, Baoyu Fan, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Speech Role-Playing Agents, SpeechRole-Data, SpeechRole-Eval

**Relevance Score:** 7

**TL;DR:** This paper introduces SpeechRole-Data, a dataset for evaluating Speech Role-Playing Agents, and proposes SpeechRole-Eval, a comprehensive evaluation benchmark.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of systematic evaluation for Speech Role-Playing Agents (SRPAs) and to enhance personalized interaction using speech.

**Method:** Construction of a large-scale dataset (SpeechRole-Data) comprising 98 roles and 112k speech conversations, and proposal of a multidimensional evaluation benchmark (SpeechRole-Eval).

**Key Contributions:**

	1. Introduction of SpeechRole-Data dataset for SRPAs.
	2. Development of SpeechRole-Eval benchmark for performance assessment.
	3. Recommendations based on experimental evaluations.

**Result:** Experimental results highlight advantages and challenges of cascade vs. end-to-end SRPAs, particularly in vocal style consistency and role coherence.

**Limitations:** 

**Conclusion:** The paper lays the groundwork for speech-driven multimodal role-playing research, providing resources for future exploration and development.

**Abstract:** Recently, role-playing agents have emerged as a promising paradigm for achieving personalized interaction and emotional resonance. Existing research primarily focuses on the textual modality, neglecting the critical dimension of speech in realistic interactive scenarios. In particular, there is a lack of systematic evaluation for Speech Role-Playing Agents (SRPAs). To address this gap, we construct SpeechRole-Data, a large-scale, high-quality dataset that comprises 98 diverse roles and 112k speech-based single-turn and multi-turn conversations. Each role demonstrates distinct vocal characteristics, including timbre and prosody, thereby enabling more sophisticated speech role-playing. Furthermore, we propose SpeechRole-Eval, a multidimensional evaluation benchmark that systematically assesses SRPAs performance in key aspects such as fundamental interaction ability, speech expressiveness, and role-playing fidelity. Experimental results reveal the advantages and challenges of both cascaded and end-to-end speech role-playing agents in maintaining vocal style consistency and role coherence. We release all data, code, and baseline models to provide a solid foundation for speech-driven multimodal role-playing research and to foster further developments in this field.

</details>


### [111] [SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models](https://arxiv.org/abs/2508.02018)

*Wanqi Yang, Yanda Li, Yunchao Wei, Meng Fang, Ling Chen*

**Main category:** cs.CL

**Keywords:** audio-language models, reasoning evaluation, SpeechR, emotion recognition, transcription accuracy

**Relevance Score:** 7

**TL;DR:** SpeechR is a benchmark designed to evaluate the reasoning abilities of large audio-language models (LALMs) in speech-based scenarios, addressing gaps in existing evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the inadequacy of current evaluations of large audio-language models that focus primarily on surface-level perception without assessing reasoning capabilities in speech.

**Method:** SpeechR introduces a unified benchmark evaluating reasoning in LALMs along three dimensions: factual retrieval, procedural inference, and normative judgment, using multiple-choice, generative, and acoustic-feature evaluation formats.

**Key Contributions:**

	1. Introduction of the SpeechR benchmark for reasoning in speech
	2. Evaluation across three dimensions of reasoning
	3. Findings that high transcription accuracy does not guarantee reasoning performance

**Result:** Eleven state-of-the-art LALMs were evaluated, showing high transcription accuracy does not correlate with strong reasoning abilities in speech-based tasks.

**Limitations:** 

**Conclusion:** SpeechR provides a structured approach to assess reasoning in spoken language, promoting targeted analyses of model capabilities in dialogue-based applications.

**Abstract:** Large audio-language models (LALMs) have achieved near-human performance in sentence-level transcription and emotion recognition. However, existing evaluations focus mainly on surface-level perception, leaving the capacity of models for contextual and inference-driven reasoning in speech-based scenarios insufficiently examined. To address this gap, we introduce SpeechR, a unified benchmark for evaluating reasoning over speech in large audio-language models. SpeechR evaluates models along three key dimensions: factual retrieval, procedural inference, and normative judgment. It includes three distinct evaluation formats. The multiple-choice version measures answer selection accuracy. The generative version assesses the coherence and logical consistency of reasoning chains. The acoustic-feature version investigates whether variations in stress and emotion affect reasoning performance. Evaluations on eleven state-of-the-art LALMs reveal that high transcription accuracy does not translate into strong reasoning capabilities. SpeechR establishes a structured benchmark for evaluating reasoning in spoken language, enabling more targeted analysis of model capabilities across diverse dialogue-based tasks.

</details>


### [112] [Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time](https://arxiv.org/abs/2508.02037)

*Huihan Li, You Chen, Siyuan Wang, Yixin He, Ninareh Mehrabi, Rahul Gupta, Xiang Ren*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought reasoning, memorization

**Relevance Score:** 8

**TL;DR:** The paper introduces STIM, a framework for identifying memorization in LLMs, highlighting its role in reasoning errors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address concerns about LLMs' reliance on memorization, particularly in Chain-of-Thought reasoning where minor input variations lead to significant errors.

**Method:** STIM analyzes each token in a reasoning chain, attributing them to memorization sources based on their statistical co-occurrence in the pretraining corpus.

**Key Contributions:**

	1. Introduction of the STIM framework for analyzing token-level memorization.
	2. Insights into local memorization's impact on reasoning errors in LLMs.
	3. Demonstration that STIM can generalize to other structured tasks.

**Result:** Models show a higher reliance on memorization for complex tasks, with local memorization causing up to 67% of token errors, and STIM's scores effectively predict incorrect tokens in reasoning steps.

**Limitations:** 

**Conclusion:** STIM serves as a diagnostic tool for improving model reasoning and can be adapted for other structured generation tasks.

**Abstract:** Large Language Models (LLMs) perform well on reasoning benchmarks but often fail when inputs alter slightly, raising concerns about the extent to which their success relies on memorization. This issue is especially acute in Chain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger intermediate errors that cascade into incorrect final answers. We introduce STIM, a novel framework for Source-aware Token-level Identification of Memorization, which attributes each token in a reasoning chain to one of multiple memorization sources - local, mid-range, or long-range - based on their statistical co-occurrence with the token in the pretraining corpus. Our token-level analysis across tasks and distributional settings reveals that models rely more on memorization in complex or long-tail cases, and that local memorization is often the dominant driver of errors, leading to up to 67% of wrong tokens. We also show that memorization scores from STIM can be effective in predicting the wrong tokens in the wrong reasoning step. STIM offers a powerful tool for diagnosing and improving model reasoning and can generalize to other structured step-wise generation tasks.

</details>


### [113] [Marco-Voice Technical Report](https://arxiv.org/abs/2508.02038)

*Fengping Tian, Chenyang Lyu, Xuanfan Ni, Haoqin Sun, Qingjuan Li, Zhiqiang Qian, Haijun Li, Longyue Wang, Zhao Xu, Weihua Luo, Kaifu Zhang*

**Main category:** cs.CL

**Keywords:** speech synthesis, voice cloning, emotion control, neural networks, emotional dataset

**Relevance Score:** 6

**TL;DR:** The paper presents a multifunctional speech synthesis system, Marco-Voice, that integrates voice cloning and emotion control, addressing challenges in expressive speech generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To achieve highly expressive, controllable, and natural speech generation while preserving speaker identity across diverse contexts.

**Method:** Introduces a speaker-emotion disentanglement mechanism with in-batch contrastive learning and a rotational emotional embedding integration method for emotion control.

**Key Contributions:**

	1. Development of Marco-Voice integrating voice cloning and emotion control
	2. Creation of CSEMOTIONS, a high-quality emotional speech dataset
	3. Implementation of a novel speaker-emotion disentanglement mechanism

**Result:** Marco-Voice shows substantial improvements in objective and subjective metrics for speech clarity and emotional richness.

**Limitations:** 

**Conclusion:** Marco-Voice represents a significant advance in expressive neural speech synthesis, demonstrating competitive performance.

**Abstract:** This paper presents a multifunctional speech synthesis system that integrates voice cloning and emotion control speech synthesis within a unified framework. The goal of this work is to address longstanding challenges in achieving highly expressive, controllable, and natural speech generation that faithfully preserves speaker identity across diverse linguistic and emotional contexts. Our approach introduces an effective speaker-emotion disentanglement mechanism with in-batch contrastive learning, enabling independent manipulation of speaker identity and eemotional style, as well as rotational emotional embedding integration method for smooth emotion control. To support comprehensive training and evaluation, we construct CSEMOTIONS, a high-quality emotional speech dataset containing 10 hours of Mandarin speech from six professional speakers across seven emotional categories. Extensive experiments demonstrate that our system, Marco-Voice, achieves substantial improvements in both objective and subjective metrics. Comprehensive evaluations and analysis were conducted, results show that MarcoVoice delivers competitive performance in terms of speech clarity and emotional richness, representing a substantial advance in the field of expressive neural speech synthesis.

</details>


### [114] [Authorship Attribution in Multilingual Machine-Generated Texts](https://arxiv.org/abs/2508.01656)

*Lucio La Cava, Dominik Macko, Róbert Móro, Ivan Srba, Andrea Tagarelli*

**Main category:** cs.CL

**Keywords:** Multilingual Authorship Attribution, Large Language Models, Text Generation, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces Multilingual Authorship Attribution (MAA), focusing on identifying the generator of texts across multiple languages, comparing monolingual methods' adaptability and performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for distinguishing machine-generated text from human content has become urgent due to the high fluency of Large Language Models and the lack of multilingual authorship attribution methods.

**Method:** The study explores authorship attribution in 18 languages using 8 different text generators, including both LLMs and human authors, assessing monolingual methods in a multilingual context.

**Key Contributions:**

	1. Introduction of the Multilingual Authorship Attribution problem
	2. Analysis of monolingual AA methods in multilingual settings
	3. Empirical findings on generator impact in attribution performance

**Result:** Adaptive capabilities of some monolingual authorship attribution methods were confirmed, but challenges remain in cross-lingual transferability and performance across diverse language families.

**Limitations:** Limited effectiveness of monolingual methods when applied to diverse language families; need for further research in cross-lingual performance improvement.

**Conclusion:** Robust solutions are required to address the complexities of multilingual authorship attribution for real-world applications.

**Abstract:** As Large Language Models (LLMs) have reached human-like fluency and coherence, distinguishing machine-generated text (MGT) from human-written content becomes increasingly difficult. While early efforts in MGT detection have focused on binary classification, the growing landscape and diversity of LLMs require a more fine-grained yet challenging authorship attribution (AA), i.e., being able to identify the precise generator (LLM or human) behind a text. However, AA remains nowadays confined to a monolingual setting, with English being the most investigated one, overlooking the multilingual nature and usage of modern LLMs. In this work, we introduce the problem of Multilingual Authorship Attribution, which involves attributing texts to human or multiple LLM generators across diverse languages. Focusing on 18 languages -- covering multiple families and writing scripts -- and 8 generators (7 LLMs and the human-authored class), we investigate the multilingual suitability of monolingual AA methods, their cross-lingual transferability, and the impact of generators on attribution performance. Our results reveal that while certain monolingual AA methods can be adapted to multilingual settings, significant limitations and challenges remain, particularly in transferring across diverse language families, underscoring the complexity of multilingual AA and the need for more robust approaches to better match real-world scenarios.

</details>


### [115] [Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models](https://arxiv.org/abs/2508.02045)

*Soyeon Kim, Jindong Wang, Xing Xie, Steven Euijong Whang*

**Main category:** cs.CL

**Keywords:** Time-Sensitive Question-Answering, Large Language Models, Temporal Databases

**Relevance Score:** 7

**TL;DR:** Introduction of TDBench, a benchmark for evaluating time-sensitive question answering in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a scalable and comprehensive benchmark for Time-Sensitive Question-Answering (TSQA) that overcomes limitations of existing methods.

**Method:** Uses temporal databases and techniques like temporal SQL to construct TSQA pairs systematically; introduces a fine-grained evaluation metric called time accuracy.

**Key Contributions:**

	1. Development of TDBench for scalable TSQA evaluation
	2. Introduction of the time accuracy metric
	3. Reduction in reliance on human labor for TSQA testing

**Result:** Extensive experiments demonstrate TDBench's ability to enable reliable TSQA evaluation while minimizing human labor.

**Limitations:** 

**Conclusion:** TDBench complements existing approaches by allowing evaluation on application-specific data and facilitating multi-hop question generation.

**Abstract:** Facts evolve over time, making it essential for Large Language Models (LLMs) to handle time-sensitive factual knowledge accurately and reliably. While factual Time-Sensitive Question-Answering (TSQA) tasks have been widely studied, existing benchmarks often rely on manual curation or a small, fixed set of predefined templates, which restricts scalable and comprehensive TSQA evaluation. To address these challenges, we propose TDBench, a new benchmark that systematically constructs TSQA pairs by harnessing temporal databases and database techniques such as temporal SQL and functional dependencies. We also introduce a fine-grained evaluation metric called time accuracy, which assesses the validity of time references in model explanations alongside traditional answer accuracy to enable a more reliable TSQA evaluation. Extensive experiments on contemporary LLMs show how \ours{} enables scalable and comprehensive TSQA evaluation while reducing the reliance on human labor, complementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by enabling LLM evaluation on application-specific data and seamless multi-hop question generation. Code and data are publicly available at: https://github.com/ssoy0701/tdbench.git.

</details>


### [116] [CUPID: Evaluating Personalized and Contextualized Alignment of LLMs from Interactions](https://arxiv.org/abs/2508.01674)

*Tae Soo Kim, Yoonjoo Lee, Yoonah Park, Jiho Kim, Young-Ho Kim, Juho Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, user preferences, contextual personalization, CUPID benchmark, multi-turn interactions

**Relevance Score:** 9

**TL;DR:** CUPID is a benchmark for evaluating LLMs on inferring dynamic user preferences from multi-turn interactions, revealing challenges in current models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that user preferences for LLMs are static and highlight the need for contextually aware personalization in interactions.

**Method:** Introduced a benchmark called CUPID, consisting of 756 curated interaction session histories where users express preferences in various contexts, and assessed LLMs' ability to infer these preferences based on prior interactions.

**Key Contributions:**

	1. Introduction of the CUPID benchmark for LLMs
	2. Evaluation of 10 open and proprietary LLMs on user preference inference
	3. Identification of critical weaknesses in LLMs' contextual awareness capabilities

**Result:** State-of-the-art LLMs showed under 50% precision and 65% recall in inferring contextually relevant preferences from previous multi-turn interactions.

**Limitations:** 

**Conclusion:** CUPID underscores the necessity for improving LLMs to deliver personalized responses, with the benchmark serving as a resource for future advancements.

**Abstract:** Personalization of Large Language Models (LLMs) often assumes users hold static preferences that reflect globally in all tasks. In reality, humans hold dynamic preferences that change depending on the context. As users interact with an LLM in various contexts, they naturally reveal their contextual preferences, which a model must infer and apply in future contexts to ensure alignment. To assess this, we introduce CUPID, a benchmark of 756 human-curated interaction session histories between users and LLM-based chat assistants. In each interaction session, the user provides a request in a specific context and expresses their preference through multi-turn feedback. Given a new user request and prior interaction sessions, our benchmark assesses whether LLMs can infer the preference relevant to this request and generate a response that satisfies this preference. With CUPID, we evaluated 10 open and proprietary LLMs, revealing that state-of-the-art LLMs struggle to infer preferences from multi-turn interactions and fail to discern what previous context is relevant to a new request -- under 50% precision and 65% recall. Our work highlights the need to advance LLM capabilities for more contextually personalized interactions and proposes CUPID as a resource to drive these improvements.

</details>


### [117] [ProCut: LLM Prompt Compression via Attribution Estimation](https://arxiv.org/abs/2508.02053)

*Zhentao Xu, Fengyi Li, Albert Chen, Xiaofeng Wang*

**Main category:** cs.CL

**Keywords:** Prompt Compression, Attribution Analysis, LLM Optimization

**Relevance Score:** 8

**TL;DR:** ProCut is a training-free framework for compressing large prompt templates in LLM systems, reducing their size significantly while maintaining or improving task performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large-scale industrial LLM systems often result in excessively lengthy prompt templates, leading to challenges in maintenance, inference latency, and operational costs.

**Method:** ProCut compresses prompts using attribution analysis, segmenting them into meaningful units, assessing their impact on performance, and pruning less useful components without requiring training.

**Key Contributions:**

	1. Introduction of ProCut framework for prompt compression
	2. Demonstrated significant size reduction and performance improvement
	3. Development of an LLM-driven attribution estimator that reduces compression latency

**Result:** ProCut achieved a 78% reduction in prompt size in production settings while maintaining or improving task performance, outperforming alternative methods by up to 62%.

**Limitations:** 

**Conclusion:** ProCut provides a seamless integration with existing prompt-optimization frameworks, allowing for concise and effective prompt generation with reduced compression latency.

**Abstract:** In large-scale industrial LLM systems, prompt templates often expand to thousands of tokens as teams iteratively incorporate sections such as task instructions, few-shot examples, and heuristic rules to enhance robustness and coverage. This expansion leads to bloated prompts that are difficult to maintain and incur significant inference latency and serving costs. To address this, we introduce Prompt Compression via Attribution Estimation (ProCut), a flexible, LLM-agnostic, training-free framework that compresses prompts through attribution analysis. ProCut segments prompt templates into semantically meaningful units, quantifies their impact on task performance, and prunes low-utility components. Through extensive experiments on five public benchmark datasets and real-world industrial prompts, we show that ProCut achieves substantial prompt size reductions (78% fewer tokens in production) while maintaining or even slightly improving task performance (up to 62% better than alternative methods). We further introduce an LLM-driven attribution estimator that reduces compression latency by over 50%, and demonstrate that ProCut integrates seamlessly with existing prompt-optimization frameworks to produce concise, high-performing prompts.

</details>


### [118] [The SMeL Test: A simple benchmark for media literacy in language models](https://arxiv.org/abs/2508.02074)

*Gustaf Ahdritz, Anat Kleiman*

**Main category:** cs.CL

**Keywords:** large language models, media literacy, information trustworthiness, LLM evaluation, hallucination

**Relevance Score:** 8

**TL;DR:** This paper introduces the Synthetic Media Literacy Test (SMeL Test) to evaluate LLMs' ability to filter untrustworthy information online, revealing significant limitations in their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine how well large language models (LLMs) can navigate the internet's untrustworthy content landscape using human-like heuristics.

**Method:** The Synthetic Media Literacy Test (SMeL Test) benchmarks various instruction-tuned LLMs on their ability to discern reliable sources from misleading information.

**Key Contributions:**

	1. Introduction of the SMeL Test for LLM evaluation
	2. Demonstration of high hallucination rates in LLMs
	3. Analysis of model performance relative to size and capability

**Result:** No LLM consistently prioritizes reliable sources; while reasoning abilities correlate with higher scores, even top models exhibit hallucination rates of up to 70%.

**Limitations:** The results indicate that existing LLMs struggle significantly with differentiating trustworthy content.

**Conclusion:** Larger models are not inherently better at filtering information than smaller ones, highlighting a need for improved methods to address these limitations.

**Abstract:** The internet is rife with unattributed, deliberately misleading, or otherwise untrustworthy content. Though large language models (LLMs) are often tasked with autonomous web browsing, the extent to which they have learned the simple heuristics human researchers use to navigate this noisy environment is not currently known. In this paper, we introduce the Synthetic Media Literacy Test (SMeL Test), a minimal benchmark that tests the ability of language models to actively filter out untrustworthy information in context. We benchmark a variety of commonly used instruction-tuned LLMs, including reasoning models, and find that no model consistently trusts more reliable sources; while reasoning in particular is associated with higher scores, even the best API model we test hallucinates up to 70% of the time. Remarkably, larger and more capable models do not necessarily outperform their smaller counterparts. We hope our work sheds more light on this important form of hallucination and guides the development of new methods to combat it.

</details>


### [119] [When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models](https://arxiv.org/abs/2508.02087)

*Jin Li, Keyu Wang, Shu Yang, Zhuoran Zhang, Di Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, sycophancy, AI alignment, human-computer interaction, language modeling

**Relevance Score:** 8

**TL;DR:** This paper investigates the mechanisms behind sycophantic behavior in Large Language Models (LLMs), revealing that user opinions can induce such behavior through specific structural changes in the model's layers.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Understand the internal mechanisms of sycophantic behavior in LLMs and its implications for AI alignment and truthfulness.

**Method:** The study analyzes how user opinions induce sycophancy across different model families, utilizing logit-lens analysis and causal activation patching to identify a two-stage emergence process.

**Key Contributions:**

	1. Mechanistic account of sycophancy in LLMs.
	2. Identification of a two-stage process for sycophantic behavior.
	3. Influence of grammatical perspective on sycophantic rates.

**Result:** The research finds that simple opinion statements reliably induce sycophancy, while expert framing has little effect. It describes a two-stage emergence involving late-layer output shifts and deeper representational divergence.

**Limitations:** 

**Conclusion:** Sycophantic behavior is a structural phenomenon in LLMs, indicating deeper representational changes rather than a superficial response to user inputs, raising concerns for alignment in AI systems.

**Abstract:** Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing with user-stated opinions even when those contradict factual knowledge. While prior work has documented this tendency, the internal mechanisms that enable such behavior remain poorly understood. In this paper, we provide a mechanistic account of how sycophancy arises within LLMs. We first systematically study how user opinions induce sycophancy across different model families. We find that simple opinion statements reliably induce sycophancy, whereas user expertise framing has a negligible impact. Through logit-lens analysis and causal activation patching, we identify a two-stage emergence of sycophancy: (1) a late-layer output preference shift and (2) deeper representational divergence. We also verify that user authority fails to influence behavior because models do not encode it internally. In addition, we examine how grammatical perspective affects sycophantic behavior, finding that first-person prompts (``I believe...'') consistently induce higher sycophancy rates than third-person framings (``They believe...'') by creating stronger representational perturbations in deeper layers. These findings highlight that sycophancy is not a surface-level artifact but emerges from a structural override of learned knowledge in deeper layers, with implications for alignment and truthful AI systems.

</details>


### [120] ["Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth](https://arxiv.org/abs/2508.02094)

*Yaqiong Li, Peng Zhang, Lin Wang, Hansu Gu, Siyuan Qiao, Ning Gu, Tun Lu*

**Main category:** cs.CL

**Keywords:** youth toxicity, toxicity detection, social media, Chinese youth, contextual factors

**Relevance Score:** 4

**TL;DR:** This paper explores the unique features of 'youth-toxicity' in social media, highlighting how youths perceive certain language as toxic while adults do not, and proposes methods to improve toxicity detection tailored for youths.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how youths perceive toxicity in social media, particularly regarding languages viewed as nontoxic by adults.

**Method:** The authors constructed the first Chinese 'youth-toxicity' dataset and conducted extensive analysis to examine contextual factors influencing toxicity perception among Chinese youth.

**Key Contributions:**

	1. Development of the first Chinese youth-toxicity dataset
	2. Identification of contextual features influencing youth's toxicity perception
	3. Improvement of existing toxicity detection methods using these insights

**Result:** The study found that youth toxicity perception is linked to contextual factors, enhancing the accuracy of detection methods when these factors are incorporated.

**Limitations:** 

**Conclusion:** Incorporating contextual meta-information into toxicity detection methods can significantly improve accuracy, and the study provides insights for future youth-centered toxicity detection research.

**Abstract:** Risk perception is subjective, and youth's understanding of toxic content differs from that of adults. Although previous research has conducted extensive studies on toxicity detection in social media, the investigation of youth's unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as youth, is ignored. To address this gap, we aim to explore: 1) What are the features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing toxicity detection techniques accurately detect these languages (RQ2). For these questions, we took Chinese youth as the research target, constructed the first Chinese ``youth-toxicity'' dataset, and then conducted extensive analysis. Our results suggest that youth's perception of these is associated with several contextual factors, like the source of an utterance and text-related features. Incorporating these meta information into current toxicity detection methods significantly improves accuracy overall. Finally, we propose several insights into future research on youth-centered toxicity detection.

</details>


### [121] [Learning Dynamics of Meta-Learning in Small Model Pretraining](https://arxiv.org/abs/2508.02189)

*David Demitri Africa, Yuval Weiss, Paula Buttery, Richard Diehl Martinez*

**Main category:** cs.CL

**Keywords:** meta-learning, language models, interpretability, NLP, training dynamics

**Relevance Score:** 8

**TL;DR:** This paper explores the integration of meta-learning techniques to improve the pretraining of small language models, resulting in better performance and interpretability.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance both the efficiency and interpretability of small language model pretraining using meta-learning techniques.

**Method:** The authors integrated first-order MAML with subset-masked language model pretraining, resulting in various LLama-style decoder-only models ranging from 11M to 570M parameters.

**Key Contributions:**

	1. Integration of meta-learning in small language model pretraining.
	2. Demonstrates faster convergence and improved performance on NLP tasks.
	3. Offers insights into training dynamics and layer specialization.

**Result:** The proposed model achieves comparable loss levels up to 1.6 times faster, improves F1 scores on multilingual Universal NER under equal compute, and presents a clear two-stage training dynamic involving 'diversification' followed by 'compression.'

**Limitations:** 

**Conclusion:** The approach provides a notable improvement in training speed and interpretability, facilitating better understanding of model behavior and specialization during training.

**Abstract:** Large language models are powerful but costly. We ask whether meta-learning can make the pretraining of small language models not only better but also more interpretable. We integrate first-order MAML with subset-masked LM pretraining, producing four LLama-style decoder-only models (11M-570M params), and evaluate it on a fundamental NLP task with many settings and real-world applications. Compared with vanilla training, our model (i) reaches the same loss up to 1.6x sooner, (ii) improves F1 on multilingual Universal NER under equal compute, and (iii) makes the training dynamics easy to read: first the network's representations fan out ("diversify") and later they collapse into a smaller, shared subspace ("compress"). This two-stage shift shows up as a rise-and-fall in both effective-rank curves and attention-head entropy. The same curves pinpoint which layers specialise earliest and which later reconverge, giving a compact, interpretable signature of meta-adaptation. Code, checkpoints and WandB logs are released.

</details>


### [122] ["Harmless to You, Hurtful to Me!": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth](https://arxiv.org/abs/2508.02094)

*Yaqiong Li, Peng Zhang, Lin Wang, Hansu Gu, Siyuan Qiao, Ning Gu, Tun Lu*

**Main category:** cs.CL

**Keywords:** youth-toxicity, toxicity detection, social media, Chinese youth, contextual factors

**Relevance Score:** 7

**TL;DR:** This paper explores the concept of 'youth-toxicity' in social media, focusing on how youth perceive language as toxic differently than adults. It constructs a dataset specifically for Chinese youth and evaluates current toxicity detection techniques against this dataset.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the differences in toxicity perception between youth and adults in social media, specifically focusing on languages viewed as nontoxic by adults but toxic by youth.

**Method:** The study involves constructing the first Chinese 'youth-toxicity' dataset and performing extensive analysis to identify features of youth-specific toxicity and evaluate existing toxicity detection techniques' effectiveness.

**Key Contributions:**

	1. Creation of the first Chinese 'youth-toxicity' dataset.
	2. Identification of contextual factors affecting youth's perception of toxicity.
	3. Improvement of existing toxicity detection techniques through the incorporation of specific contextual information.

**Result:** The findings indicate that youth's perception of toxicity is influenced by contextual factors like the source of the utterance and text-related characteristics. Additionally, enhancing toxicity detection techniques with this contextual meta information yields improved accuracy.

**Limitations:** 

**Conclusion:** The research provides insights into the distinct perceptions of toxicity among youth and suggests directions for the development of youth-centered toxicity detection methods.

**Abstract:** Risk perception is subjective, and youth's understanding of toxic content differs from that of adults. Although previous research has conducted extensive studies on toxicity detection in social media, the investigation of youth's unique toxicity, i.e., languages perceived as nontoxic by adults but toxic as youth, is ignored. To address this gap, we aim to explore: 1) What are the features of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing toxicity detection techniques accurately detect these languages (RQ2). For these questions, we took Chinese youth as the research target, constructed the first Chinese ``youth-toxicity'' dataset, and then conducted extensive analysis. Our results suggest that youth's perception of these is associated with several contextual factors, like the source of an utterance and text-related features. Incorporating these meta information into current toxicity detection methods significantly improves accuracy overall. Finally, we propose several insights into future research on youth-centered toxicity detection.

</details>


### [123] [Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference](https://arxiv.org/abs/2508.02193)

*Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Yonghui Wu, Hao Zhou*

**Main category:** cs.CL

**Keywords:** language model, discrete-state diffusion, inference speed, code evaluation, parallel generation

**Relevance Score:** 7

**TL;DR:** Seed Diffusion Preview is a fast inference language model using discrete-state diffusion, achieving 2,146 token/s speed on H20 GPUs while maintaining competitive performance in code evaluation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the inference speed of language models by utilizing discrete-state diffusion, addressing latency in traditional token-by-token decoding methods.

**Method:** The model employs non-sequential, parallel generation techniques in discrete diffusion for enhanced speed and efficiency.

**Key Contributions:**

	1. Introduction of a fast inference language model based on discrete-state diffusion
	2. Records a notable inference speed of 2,146 token/s on H20 GPUs
	3. Sets new benchmarks in the speed-quality Pareto frontier for code models.

**Result:** Achieves an inference speed of 2,146 token/s, significantly faster than existing models like Mercury and Gemini Diffusion, while also maintaining high quality in code evaluations.

**Limitations:** 

**Conclusion:** Seed Diffusion Preview establishes a new state of the art on the speed-quality frontier for code models, proving the efficacy of discrete-state diffusion for fast inference.

**Abstract:** We present Seed Diffusion Preview, a large-scale language model based on discrete-state diffusion, offering remarkably fast inference speed. Thanks to non-sequential, parallel generation, discrete diffusion models provide a notable speedup to mitigate the inherent latency of token-by-token decoding, as demonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion Preview achieves an inference speed of 2,146 token/s over H20 GPUs while maintaining competitive performance across a sweep of standard code evaluation benchmarks, significantly faster than contemporary Mercury and Gemini Diffusion, establishing new state of the art on the speed-quality Pareto frontier for code models.

</details>


### [124] [Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems](https://arxiv.org/abs/2508.02208)

*Yebo Peng, Zixiang Liu, Yaoming Li, Zhizhuo Yang, Xinye Xu, Bowen Ye, Weijun Yuan, Zihan Wang, Tong Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, mathematical capability, proof-centric benchmarks, algorithms, evaluation techniques

**Relevance Score:** 8

**TL;DR:** This paper presents Proof2Hybrid, an automated framework for creating proof-centric benchmarks to evaluate the mathematical capabilities of Large Language Models (LLMs) using natural language mathematical corpora.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing benchmarks for evaluating the mathematical abilities of LLMs, particularly in proof-centric contexts.

**Method:** The paper introduces Proof2X, a roadmap for converting mathematical proofs into hybrid-formatted questions, including a new question type called '$m$-out-of-$n$ multiple judge questions', aimed at facilitating rigorous and automatic evaluation.

**Key Contributions:**

	1. Introduction of Proof2Hybrid framework for automated benchmark synthesis
	2. Development of '$m$-out-of-$n$ multiple judge questions' for robust evaluation
	3. Creation of AlgGeoTest benchmark for algebraic geometry

**Result:** Using the newly developed framework, the authors created AlgGeoTest, a benchmark for algebraic geometry consisting of 456 challenging questions. Evaluations showed significant comprehension deficits in state-of-the-art LLMs when tested with AlgGeoTest.

**Limitations:** 

**Conclusion:** Proof2Hybrid and AlgGeoTest enable more precise assessment of mathematical intelligence in AI, encouraging further research in this area.

**Abstract:** Evaluating the mathematical capability of Large Language Models (LLMs) is a critical yet challenging frontier. Existing benchmarks fall short, particularly for proof-centric problems, as manual creation is unscalable and costly, leaving the true mathematical abilities of LLMs largely unassessed. To overcome these barriers, we propose Proof2Hybrid, the first fully automated framework that synthesizes high-quality, proof-centric benchmarks from natural language mathematical corpora. The key novelty of our solution is Proof2X, a roadmap of converting mathematical proofs into various kinds of questions that are easy to verify. Instructed by this roadmap, we propose a new type of hybrid-formatted questions, named ``$m$-out-of-$n$ multiple judge questions'', specifically designed to enable robust, automatic evaluation while being resilient to guessing and superficial pattern matching inherent in traditional formats. As a demonstration of our framework, we introduce AlgGeoTest, a benchmark for algebraic geometry--a frontier domain of modern mathematics--comprising 456 challenging items. Our extensive evaluations on state-of-the-art LLMs using AlgGeoTest reveal profound deficits in their comprehension of algebraic geometry, providing a more precise measure of their true mathematical capabilities. Our framework and benchmark pave the way for a new wave of in-depth research into the mathematical intelligence of AI systems.

</details>


### [125] [Isolating Culture Neurons in Multilingual Large Language Models](https://arxiv.org/abs/2508.02241)

*Danial Namazifard, Lukas Galke*

**Main category:** cs.CL

**Keywords:** multilingual LLMs, cultural neurons, language-specific neurons, fairness, inclusivity

**Relevance Score:** 9

**TL;DR:** This paper investigates how multilingual large language models (LLMs) encode culture by identifying culture-specific neurons distinct from language-specific ones, finding that cultural knowledge can be isolated and modified.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the intertwining of language and culture in multilingual large language models and how cultural knowledge is represented within them.

**Method:** We extended an established methodology to locate and isolate culture-specific neurons in LLMs, conducting localization and intervention experiments using a curated dataset of 85.2 million tokens from six cultures.

**Key Contributions:**

	1. Introduction of MUREL, a dataset for analyzing cultural neurons in LLMs.
	2. Identification of distinct neuron populations for different cultures in multilingual LLMs.
	3. Demonstration that cultural neurons can be modulated independently from language-specific neurons.

**Result:** Our experiments reveal that LLMs have distinct neuron populations for different cultures, primarily in the upper layers, allowing for independent modulation of cultural knowledge separate from language.

**Limitations:** 

**Conclusion:** Cultural knowledge in multilingual language models can be selectively isolated and edited, which could enhance fairness and inclusivity in AI systems.

**Abstract:** Language and culture are deeply intertwined, yet it is so far unclear how and where multilingual large language models encode culture. Here, we extend upon an established methodology for identifying language-specific neurons and extend it to localize and isolate culture-specific neurons, carefully disentangling their overlap and interaction with language-specific neurons. To facilitate our experiments, we introduce MUREL, a curated dataset of 85.2 million tokens spanning six different cultures. Our localization and intervention experiments show that LLMs encode different cultures in distinct neuron populations, predominantly in upper layers, and that these culture neurons can be modulated independently from language-specific neurons or those specific to other cultures. These findings suggest that cultural knowledge and propensities in multilingual language models can be selectively isolated and edited - promoting fairness, inclusivity, and alignment. Code and data is available at https://github.com/namazifard/Culture_Neurons .

</details>


### [126] [Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders](https://arxiv.org/abs/2508.02256)

*Belen Alastruey, João Maria Janeiro, Alexandre Allauzen, Maha Elbayad, Loïc Barrault, Marta R. Costa-jussà*

**Main category:** cs.CL

**Keywords:** language interference, Transformer models, cross-lingual performance

**Relevance Score:** 5

**TL;DR:** Study of language interference in encoder-only Transformer models across 83 languages reveals asymmetrical interference patterns related to script rather than traditional linguistic characteristics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To quantify and analyze cross-lingual interference in Transformer models to improve multilingual model performance.

**Method:** Constructed an interference matrix by training and evaluating small BERT-like models on all possible language pairs.

**Key Contributions:**

	1. Comprehensive analysis of language interference across 83 languages.
	2. Introduction of an interference matrix for quantifying cross-lingual interference.
	3. Demonstration of the matrix's predictive capability for downstream task performance.

**Result:** Discovered that interference patterns do not align with traditional linguistic characteristics or embedding similarity, but correlate better with script, and that the interference matrix can predict performance on downstream tasks.

**Limitations:** 

**Conclusion:** The findings suggest new strategies for designing multilingual models to enhance their efficacy based on interference patterns.

**Abstract:** In this paper, we present a comprehensive study of language interference in encoder-only Transformer models across 83 languages. We construct an interference matrix by training and evaluating small BERT-like models on all possible language pairs, providing a large-scale quantification of cross-lingual interference. Our analysis reveals that interference between languages is asymmetrical and that its patterns do not align with traditional linguistic characteristics, such as language family, nor with proxies like embedding similarity, but instead better relate to script. Finally, we demonstrate that the interference matrix effectively predicts performance on downstream tasks, serving as a tool to better design multilingual models to obtain optimal performance.

</details>


### [127] [Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning](https://arxiv.org/abs/2508.02260)

*Jia Deng, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Entropy Dynamics, Performance Analysis, Reward Adjustment

**Relevance Score:** 8

**TL;DR:** This paper analyzes the entropy-performance exchange in reinforcement learning with verifiable rewards (RLVR) to enhance reasoning abilities in large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the exchange between entropy and performance in RLVR and improve the training of LLMs.

**Method:** The authors conduct a systematic empirical analysis dividing the training process into two stages: rising and plateau stage, investigating entropy-performance relations at different granularities.

**Key Contributions:**

	1. Systematic analysis of entropy-performance exchange in RLVR across different granularities
	2. Identification of key stages in the training process influencing learning efficiency
	3. Proposed methods for dynamic reward adjustment in LLM training

**Result:** In the rising stage, reducing entropy in negative samples helps learn effective reasoning patterns, leading to performance gains. In the plateau stage, learning efficiency correlates with high-entropy tokens in low-perplexity samples at the end of sequences.

**Limitations:** 

**Conclusion:** The findings inform two proposed methods for dynamically adjusting reward signals using perplexity and positional information, which show improved performance on various LLMs compared to baselines.

**Abstract:** Recently, reinforcement learning with verifiable rewards (RLVR) has been widely used for enhancing the reasoning abilities of large language models (LLMs). A core challenge in RLVR involves managing the exchange between entropy and performance of policies. Despite the importance of this exchange, a fine-grained understanding of when and how this exchange operates most effectively remains limited. To bridge this gap, we conduct a systematic empirical analysis of the entropy-performance exchange mechanism of RLVR across different levels of granularity. Specifically, we first divide the training process into two distinct stages based on entropy dynamics, i.e., rising stage and plateau stage, and then systematically investigate how this mechanism varies across stage-level, instance-level, and token-level granularitiess. Our analysis reveals that, in the rising stage, entropy reduction in negative samples facilitates the learning of effective reasoning patterns, which in turn drives rapid performance gains. Moreover, in the plateau stage, learning efficiency strongly correlates with high-entropy tokens present in low-perplexity samples and those located at the end of sequences. Motivated by these findings, we propose two methods that dynamically adjust the reward signal using perplexity and positional information to focus RL updates on tokens that exhibit high learning potential, achieving improvements compared to the baseline methods on various LLMs.

</details>


### [128] [SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System](https://arxiv.org/abs/2508.02268)

*Serry Sibaee, Omer Nacar, Yasser Al-Habashi, Adel Ammar, Wadii Boulila*

**Main category:** cs.CL

**Keywords:** Machine Translation, Natural Language Processing, Dialects, Arabic, SHAMI-MT

**Relevance Score:** 3

**TL;DR:** SHAMI-MT is a bidirectional machine translation system designed for translating between Modern Standard Arabic and the Syrian dialect. It utilizes the AraT5v2 architecture and has been rigorously evaluated, achieving a high accuracy score.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant gap between Modern Standard Arabic and regional dialects, specifically the Syrian dialect, in natural language processing applications like machine translation.

**Method:** Introduces two specialized models for MSA-to-Shami and Shami-to-MSA translation, built on the AraT5v2-base-1024 architecture and fine-tuned on the Nabra dataset.

**Key Contributions:**

	1. Introduction of a bidirectional translation system specifically for MSA and Syrian dialects.
	2. Development of high-quality translation models evaluated with state-of-the-art techniques.
	3. Contributions to the field of dialectal Arabic translation.

**Result:** The MSA-to-Shami model achieved an average quality score of 4.01 out of 5.0 when evaluated by GPT-4.1, demonstrating high accuracy and dialectal authenticity.

**Limitations:** 

**Conclusion:** SHAMI-MT offers a crucial tool for translating a previously underserved language pair, with applications in content localization and intercultural communication.

**Abstract:** The rich linguistic landscape of the Arab world is characterized by a significant gap between Modern Standard Arabic (MSA), the language of formal communication, and the diverse regional dialects used in everyday life. This diglossia presents a formidable challenge for natural language processing, particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a bidirectional machine translation system specifically engineered to bridge the communication gap between MSA and the Syrian dialect. We present two specialized models, one for MSA-to-Shami and another for Shami-to-MSA translation, both built upon the state-of-the-art AraT5v2-base-1024 architecture. The models were fine-tuned on the comprehensive Nabra dataset and rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami model achieved an outstanding average quality score of \textbf{4.01 out of 5.0} when judged by OPENAI model GPT-4.1, demonstrating its ability to produce translations that are not only accurate but also dialectally authentic. This work provides a crucial, high-fidelity tool for a previously underserved language pair, advancing the field of dialectal Arabic translation and offering significant applications in content localization, cultural heritage, and intercultural communication.

</details>


### [129] [Dynaword: From One-shot to Continuously Developed Datasets](https://arxiv.org/abs/2508.02271)

*Kenneth Enevoldsen, Kristian Nørgaard Jensen, Jan Kostkan, Balázs Szabó, Márton Kardos, Kirten Vad, Andrea Blasi Núñez, Gianluca Barmina, Jacob Nielsen, Rasmus Larsen, Peter Vahlstrup, Per Møldrup Dalum, Desmond Elliott, Lukas Galke, Peter Schneider-Kamp, Kristoffer Nielbo*

**Main category:** cs.CL

**Keywords:** large-scale datasets, natural language processing, community contributions

**Relevance Score:** 7

**TL;DR:** The paper presents the Dynaword approach and the Danish Dynaword dataset to address challenges in creating sustainable, open, and community-driven large-scale datasets for natural language processing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of current approaches in large-scale dataset creation, focusing on licensing issues, static releases, and quality assurance.

**Method:** The paper introduces the Dynaword framework for continuously updated datasets via community contributions, exemplified by Danish Dynaword, which is built to be openly licensed and highly contributive.

**Key Contributions:**

	1. Introduces the Dynaword framework for community-driven dataset creation.
	2. Launches Danish Dynaword as a practical implementation with significant token volume and contributions.
	3. Implements quality assurance processes that involve community input.

**Result:** Danish Dynaword contains over four times the tokens of similar datasets and has received significant community contributions, along with lightweight quality assurance tests.

**Limitations:** 

**Conclusion:** The Dynaword approach establishes a sustainable model for evolving datasets through community collaboration in natural language processing.

**Abstract:** Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.   To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.

</details>


### [130] [A French Version of the OLDI Seed Corpus](https://arxiv.org/abs/2508.02290)

*Malik Marmonier, Benoît Sagot, Rachel Bawden*

**Main category:** cs.CL

**Keywords:** machine translation, corpus, language resources, under-resourced languages, post-editing

**Relevance Score:** 4

**TL;DR:** The paper presents the first French partition of the OLDI Seed Corpus aimed at aiding the development of parallel corpora for under-resourced regional languages in France.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a resource for enhancing machine translation for under-resourced regional languages by leveraging a French corpus derived from technical and user-generated content.

**Method:** Utilized multiple machine translation systems and a custom interface for post-editing by qualified native speakers to create the French corpus.

**Key Contributions:**

	1. Creation of the first French partition of the OLDI Seed Corpus.
	2. Detailed methodology involving machine translation and human post-editing.
	3. Addressing translation challenges with technical and user-generated content.

**Result:** The resulting French corpus addresses unique translation challenges and combines technical terminology with stylistic irregularities from user-generated content.

**Limitations:** The focus is primarily on French and may limit direct applicability to other languages.

**Conclusion:** The French corpus serves as a pivot resource for collecting parallel corpora for regional languages, contributing to broader language accessibility in machine translation.

**Abstract:** We present the first French partition of the OLDI Seed Corpus, our submission to the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its creation process, which involved using multiple machine translation systems and a custom-built interface for post-editing by qualified native speakers. We also highlight the unique translation challenges presented by the source data, which combines highly technical, encyclopedic terminology with the stylistic irregularities characteristic of user-generated content taken from Wikipedia. This French corpus is not an end in itself, but is intended as a crucial pivot resource to facilitate the collection of parallel corpora for the under-resourced regional languages of France.

</details>


### [131] [Simple Methods Defend RAG Systems Well Against Real-World Attacks](https://arxiv.org/abs/2508.02296)

*Ilias Triantafyllopoulos, Renyi Qu, Salvatore Giorgi, Brenda Curtis, Lyle H. Ungar, João Sedoc*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Retrieval-Augmented Generation, Out-Of-Domain detection, Health informatics, Dimensionality reduction

**Relevance Score:** 9

**TL;DR:** The paper evaluates four methodologies for Out-Of-Domain query detection in Retrieval-Augmented Generation systems to enhance safety in critical applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure safety and relevant responses in RAG systems, especially in safety-critical applications like health informatics, addressing the challenge of Out-Of-Domain queries is crucial.

**Method:** The methodologies evaluated include GPT-4o, regression-based, PCA-based, and Neural Collapse methodologies, focusing on dimensionality reduction and feature separation strategies.

**Key Contributions:**

	1. Evaluation of four OOD query detection methodologies
	2. Introduction of novel dimensionality reduction and feature separation strategies
	3. Validation using real-world applications like COVID-19 chatbot

**Result:** The evaluation confirms that employing an external Out-Of-Domain detector significantly maintains response relevance during queries, validated through datasets and real-world applications.

**Limitations:** 

**Conclusion:** The study concludes that effectively detecting OOD queries is essential for RAG systems to ensure accurate and relevant responses, particularly in health-related chatbot applications.

**Abstract:** Ensuring safety and in-domain responses for Retrieval-Augmented Generation (RAG) systems is paramount in safety-critical applications, yet remains a significant challenge. To address this, we evaluate four methodologies for Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG system only responds to queries confined to the system's knowledge base. Specifically, our evaluation explores two novel dimensionality reduction and feature separation strategies: \textit{PCA}, where top components are selected using explained variance or OOD separability, and an adaptation of \textit{Neural Collapse Feature Separation}. We validate our approach on standard datasets (StackExchange and MSMARCO) and real-world applications (Substance Use and COVID-19), including tests against LLM-simulated and actual attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations of response correctness and relevance, we confirm that an external OOD detector is crucial for maintaining response relevance.

</details>


### [132] [LaMPE: Length-aware Multi-grained Position Encoding for Adaptive Long-context Scaling Without Training](https://arxiv.org/abs/2508.02308)

*Sikui Zhang, Guangze Gao, Ziyun Gan, Chunfeng Yuan, Zefeng Lin, Houwen Peng, Bing Li, Weiming Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Positional Encoding, Machine Learning, Human-Computer Interaction, Long-context scaling

**Relevance Score:** 8

**TL;DR:** This paper introduces Length-aware Multi-grained Positional Encoding (LaMPE) to enhance long-context scaling in large language models (LLMs) by dynamically allocating positional capacity based on input length.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs show performance degradation when input exceeds pretraining context windows, specifically due to the OOD behavior of Rotary Position Embedding.

**Method:** LaMPE uses a parametric scaled sigmoid function to map input lengths dynamically, alongside a novel multi-grained attention mechanism for effective positional resolution allocation.

**Key Contributions:**

	1. Introduces Length-aware Multi-grained Positional Encoding (LaMPE) for LLMs
	2. Dynamic mapping of positional capacity based on input length
	3. Novel multi-grained attention mechanism for improved locality and long-range dependencies

**Result:** LaMPE significantly improves performance on long-context benchmarks compared to existing methods across three LLMs.

**Limitations:** 

**Conclusion:** The method allows RoPE-based LLMs to adaptively manage positional information without additional training.

**Abstract:** Large language models (LLMs) experience significant performance degradation when the input exceeds the pretraining context window, primarily due to the out-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent studies mitigate this problem by remapping OOD positions into the in-distribution range with fixed mapping strategies, ignoring the dynamic relationship between input length and the model's effective context window. To this end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a training-free method that fully utilizes the model's effective context window for adaptive long-context scaling in LLMs. Motivated by the left-skewed frequency distribution of relative positions, LaMPE establishes a dynamic relationship between mapping length and input length through a parametric scaled sigmoid function to adaptively allocate positional capacity across varying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention mechanism that strategically allocates positional resolution across different sequence regions to capture both fine-grained locality and long-range dependencies. Our method can be seamlessly applied to a wide range of RoPE-based LLMs without training. Extensive experiments on three representative LLMs across five mainstream long-context benchmarks demonstrate that LaMPE achieves significant performance improvements compared to existing length extrapolation methods. The code will be released at https://github.com/scar-on/LaMPE.

</details>


### [133] [VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo](https://arxiv.org/abs/2508.02317)

*Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, Zhi Zhang, Xin Liu*

**Main category:** cs.CL

**Keywords:** omni-modal LLMs, parallelism, training framework, Machine Learning, scalability

**Relevance Score:** 7

**TL;DR:** The paper presents 6omni, a modular framework designed to efficiently train omni-modal large language models (LLMs) by decoupling communication from computation and supporting easy integration of new modalities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Training omni-modal LLMs is challenging due to the complex architectures required for diverse modalities and the need for scalable, efficient training systems.

**Method:** 6omni employs model-centric distributed recipes that utilize 3D parallelism to enhance the training of omni-modal LLMs.

**Key Contributions:**

	1. Introduction of a modular training framework for omni-modal LLMs
	2. Decoupling of communication from computation to enhance training efficiency
	3. Support for seamless integration of new modalities with minimal code changes

**Result:** A 30B parameter omni-modal mixture-of-experts model was trained at over 2,800 tokens/sec/GPU and scaled to 160K context lengths across 128 GPUs, demonstrating significant efficiency and scalability.

**Limitations:** 

**Conclusion:** 6omni provides an efficient approach to overcome the engineering burdens of training omni-modal LLMs, allowing for better scalability and flexibility.

**Abstract:** Recent advances in large language models (LLMs) have driven impressive progress in omni-modal understanding and generation. However, training omni-modal LLMs remains a significant challenge due to the heterogeneous model architectures required to process diverse modalities, necessitating sophisticated system design for efficient large-scale training. Existing frameworks typically entangle model definition with parallel logic, incurring limited scalability and substantial engineering overhead for end-to-end omni-modal training. % We present \veomni, a modular and efficient training framework to accelerate the development of omni-modal LLMs. \veomni introduces model-centric distributed recipes that decouples communication from computation, enabling efficient 3D parallelism on omni-modal LLMs. \veomni also features a flexible configuration interface supporting seamless integration of new modalities with minimal code change. % Using \veomni, a omni-modal mixture-of-experts (MoE) model with 30B parameters can be trained with over 2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D parallelism on 128 GPUs, showcasing its superior efficiency and scalability for training large omni-modal LLMs.

</details>


### [134] [CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis](https://arxiv.org/abs/2508.02322)

*Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, micro-experts, CAMERA, pruning, quantization

**Relevance Score:** 9

**TL;DR:** The paper introduces a method for optimizing Mixture-of-Experts models through the use of micro-experts and proposes a framework called CAMERA for identifying and pruning redundant micro-experts, achieving improved performance and efficiency.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of computational overhead and performance scaling in Mixture-of-Experts models, which do not scale efficiently with increasing parameters.

**Method:** Introduces micro-expert as a compression unit, establishing a framework (CAMERA) for analyzing and pruning micro-expert redundancies, complemented by structured pruning (CAMERA-P) and mixed-precision quantization (CAMERA-Q).

**Key Contributions:**

	1. Introduction of the micro-expert as a new compression unit for MoE models.
	2. Development of the CAMERA framework for identifying micro-expert redundancy.
	3. Proposal of CAMERA-P for structured pruning and CAMERA-Q for quantization of micro-experts.

**Result:** CAMERA-P consistently outperforms strong baselines under pruning ratios of 20% to 60%, while CAMERA-Q achieves superior performance with aggressive 2-bit quantization compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed methods enhance the efficiency of MoE models through effective micro-expert analysis and pruning, making it feasible to analyze large models quickly.

**Abstract:** Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are distinguished by their strong performance scaling with increasing parameters across a wide range of tasks, yet they also suffer from substantial computational and storage overheads. Notably, the performance gains of MoE models do not scale proportionally with the growth in expert parameters. While prior works attempt to reduce parameters via expert-level pruning, merging, or decomposition, they still suffer from challenges in both performance and computational efficiency. In this paper, we address these challenges by introducing micro-expert as a finer-grained compression unit that spans across matrices. We first establish a more fundamental perspective, viewing MoE layers as mixtures of micro-experts, and present CAMERA, a lightweight and training-free framework for identifying micro-expert redundancy. Our analysis uncovers significant variance in micro-expert contributions during decoding. Based on this insight, we further propose CAMERA-P, a structured micro-expert pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed for micro-experts. Extensive experiments on nine downstream tasks show that CAMERA-P consistently outperforms strong baselines under pruning ratios ranging from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under aggressive 2-bit quantization, surpassing existing matrix- and channel-level ideas. Notably, our method enables complete micro-expert analysis of Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.

</details>


### [135] [Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models](https://arxiv.org/abs/2508.02360)

*Jiayi Zhang, Shu Yang, Junchao Wu, Derek F. Wong, Di Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, political fine-tuning, cross-topic generalization, neuron localization, machine learning

**Relevance Score:** 6

**TL;DR:** This paper explores political fine-tuning of Large Language Models (LLMs), uncovering internal neuron mechanisms that lead to unintended stance generalization across different political topics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of understanding about how fine-tuning LLMs on political topics affects their stances on unrelated issues, which can lead to unintended consequences.

**Method:** The authors propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify general and topic-specific political neurons, and introduce InhibitFT, a method to mitigate cross-topic generalization.

**Key Contributions:**

	1. Introduction of PNLAC for neuron localization in political contexts
	2. Development of InhibitFT for optimizing fine-tuning and reducing generalization
	3. Empirical validation of neuron types across multiple models and datasets

**Result:** The paper identifies two types of political neurons and demonstrates that InhibitFT reduces cross-topic generalization by 20% while maintaining topic-specific performance, using only 5% neuron inhibition.

**Limitations:** 

**Conclusion:** The findings highlight the complexity of political stance representation in LLMs and provide a novel approach to mitigate unintended generalizations.

**Abstract:** Fine-tuning Large Language Models on a political topic will significantly manipulate their political stance on various issues and unintentionally affect their stance on unrelated topics. While previous studies have proposed this issue, there is still a lack of understanding regarding the internal representations of these stances and the mechanisms that lead to unintended cross-topic generalization. In this paper, we systematically explore the internal mechanisms underlying this phenomenon from a neuron-level perspective and how to mitigate the cross-topic generalization of political fine-tuning. Firstly, we propose Political Neuron Localization through Activation Contrasting (PNLAC) to identify two distinct types of political neurons: general political neurons, which govern stance across multiple political topics, and topic-specific neurons} that affect the model's political stance on individual topics. We find the existence of these political neuron types across four models and datasets through activation patching experiments. Leveraging these insights, we introduce InhibitFT, an inhibition-based fine-tuning method, effectively mitigating the cross-topic stance generalization. Experimental results demonstrate the robustness of identified neuron types across various models and datasets, and show that InhibitFT significantly reduces the cross-topic stance generalization by 20% on average, while preserving topic-specific performance. Moreover, we demonstrate that selectively inhibiting only 5% of neurons is sufficient to effectively mitigate the cross-topic stance generalization.

</details>


### [136] [CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation](https://arxiv.org/abs/2508.02401)

*Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang*

**Main category:** cs.CL

**Keywords:** large language models, KV cache, memory efficiency, attention heads, contextual processing

**Relevance Score:** 7

**TL;DR:** The paper presents a method called CompressKV to improve memory and execution efficiency in long-context processing of LLMs by using selective attention heads for key-value cache retention.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing size of KV caches in LLMs poses challenges for memory and execution efficiency, particularly because existing methods overlook the distinct functions of attention heads.

**Method:** The proposed method identifies specific attention heads capable of retrieving important tokens and their contextual meanings, and adopts a layer-adaptive KV cache allocation strategy.

**Key Contributions:**

	1. Introduced a method for selective attention head usage for KV cache management
	2. Developed a layer-adaptive KV cache allocation strategy
	3. Demonstrated significant performance improvements on standard benchmarks

**Result:** CompressKV outperforms state-of-the-art approaches across various memory budgets on the LongBench and Needle-in-a-Haystack benchmarks.

**Limitations:** 

**Conclusion:** The new method enhances memory efficiency in long-context LLMs while maintaining performance, preventing the eviction of critical tokens.

**Abstract:** Recent advances in large language models (LLMs) have significantly boosted long-context processing. However, the increasing key-value (KV) cache size poses critical challenges to memory and execution efficiency. Most KV cache compression methods rely on heuristic token eviction using all attention heads in Grouped Query Attention (GQA)-based LLMs. This method ignores the different functionalities of attention heads, leading to the eviction of critical tokens and thus degrades the performance of LLMs.   To address the issue above, instead of using all the attention heads in GQA-based LLMs to determine important tokens as in the previous work, we first identify the attention heads in each layer that are not only capable of retrieving the initial and final tokens of a prompt, but also capable of retrieving important tokens within the text and attending to their surrounding semantic context. Afterwards, we exploit such heads to determine the important tokens and retain their corresponding KV cache pairs. Furthermore, we analyze the cache eviction error of each layer individually and introduce a layer-adaptive KV cache allocation strategy. Experimental results demonstrate the proposed CompressKV consistently outperforms state-of-the-art approaches under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks. Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.

</details>


### [137] [Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding](https://arxiv.org/abs/2508.02426)

*Linyu Li, Zhi Jin, Yuanpeng He, Dongming Jin, Yichi Zhang, Haoran Duan, Nyima Tash*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Continual Learning, Machine Learning, Knowledge Graph Embedding

**Relevance Score:** 7

**TL;DR:** This paper presents BAKE, a continual knowledge graph embedding model that mitigates catastrophic forgetting, demonstrating substantial improvements over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional KGE models cannot handle the dynamic nature of real-world knowledge graphs, leading to the necessity for continual knowledge graph embedding (CKGE) to prevent catastrophic forgetting.

**Method:** The BAKE model employs Bayesian posterior updates to maintain knowledge across evolving datasets and introduces a continual clustering method to minimize the change between new and existing knowledge.

**Key Contributions:**

	1. Introduced BAKE model leveraging Bayesian updates for CKGE.
	2. Proposed continual clustering method to combat knowledge forgetting.
	3. Demonstrated significant improvements over existing CKGE models.

**Result:** BAKE significantly outperforms existing CKGE baseline models in preserving knowledge across multiple time snapshots.

**Limitations:** 

**Conclusion:** The effective use of Bayesian updates in BAKE provides a robust framework for continually evolving knowledge graphs, addressing the critical issue of knowledge forgetting.

**Abstract:** Since knowledge graphs (KG) will continue to evolve in real scenarios, traditional KGE models are only suitable for static knowledge graphs. Therefore, continual knowledge graph embedding (CKGE) has attracted the attention of researchers. Currently, a key challenge facing CKGE is that the model is prone to "catastrophic forgetting", resulting in the loss of previously learned knowledge. In order to effectively alleviate this problem, we propose a new CKGE model BAKE. First, we note that the Bayesian posterior update principle provides a natural continual learning strategy that is insensitive to data order and can theoretically effectively resist the forgetting of previous knowledge during data evolution. Different from the existing CKGE method, BAKE regards each batch of new data as a Bayesian update of the model prior. Under this framework, as long as the posterior distribution of the model is maintained, the model can better preserve the knowledge of early snapshots even after evolving through multiple time snapshots. Secondly, we propose a continual clustering method for CKGE, which further directly combats knowledge forgetting by constraining the evolution difference (or change amplitude) between new and old knowledge between different snapshots. We conduct extensive experiments on BAKE on multiple datasets, and the results show that BAKE significantly outperforms existing baseline models.

</details>


### [138] [AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications](https://arxiv.org/abs/2508.02430)

*Robin Nowak, Patrick Figge, Carolin Haeussler*

**Main category:** cs.CL

**Keywords:** Innovation measurement, Large language models, Machine learning, Expert evaluation, Unstructured text data

**Relevance Score:** 8

**TL;DR:** This paper investigates how large language models (LLMs) can assist in measuring innovation by approximating expert assessments from unstructured text data, demonstrating superior performance compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Empirical innovation research is often constrained by reliance on context-specific proxies and expert evaluations, which limits its scope.

**Method:** Design an LLM framework to assess innovation from unstructured text data, validated through two studies: evaluating software updates and user-generated product feedback.

**Key Contributions:**

	1. Development of an LLM framework for measuring innovation
	2. Demonstration of superior performance compared to traditional methods
	3. Discussion of design decisions affecting LLM performance

**Result:** The LLM framework achieved higher F1-scores and consistent results compared to alternative measures and state-of-the-art machine learning models.

**Limitations:** The paper may depend on the quality and diversity of training data for the LLM framework.

**Conclusion:** The framework provides R&D personnel and researchers with tools to effectively measure innovation using LLMs, addressing limitations of human expert evaluations.

**Abstract:** Measuring innovation often relies on context-specific proxies and on expert evaluation. Hence, empirical innovation research is often limited to settings where such data is available. We investigate how large language models (LLMs) can be leveraged to overcome the constraints of manual expert evaluations and assist researchers in measuring innovation. We design an LLM framework that reliably approximates domain experts' assessment of innovation from unstructured text data. We demonstrate the performance and broad applicability of this framework through two studies in different contexts: (1) the innovativeness of software application updates and (2) the originality of user-generated feedback and improvement ideas in product reviews. We compared the performance (F1-score) and reliability (consistency rate) of our LLM framework against alternative measures used in prior innovation studies, and to state-of-the-art machine learning- and deep learning-based models. The LLM framework achieved higher F1-scores than the other approaches, and its results are highly consistent (i.e., results do not change across runs). This article equips R&D personnel in firms, as well as researchers, reviewers, and editors, with the knowledge and tools to effectively use LLMs for measuring innovation and evaluating the performance of LLM-based innovation measures. In doing so, we discuss, the impact of important design decisions-including model selection, prompt engineering, training data size, training data distribution, and parameter settings-on performance and reliability. Given the challenges inherent in using human expert evaluation and existing text-based measures, our framework has important implications for harnessing LLMs as reliable, increasingly accessible, and broadly applicable research tools for measuring innovation.

</details>


### [139] [LatentPrompt: Optimizing Promts in Latent Space](https://arxiv.org/abs/2508.02452)

*Mateusz Bystroński, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz*

**Main category:** cs.CL

**Keywords:** Large Language Models, prompt optimization, latent semantic space, automated evaluation, sentiment classification

**Relevance Score:** 9

**TL;DR:** LatentPrompt is a model-agnostic framework for optimizing prompts for Large Language Models using latent semantic space, improving task performance without manual exploration.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective prompt optimization techniques for LLMs that do not rely on heuristics or manual exploration.

**Method:** LatentPrompt uses a continuous latent space to automatically generate, evaluate, and refine candidate prompts from initial seed prompts, maximizing performance for specific tasks.

**Key Contributions:**

	1. Introduction of a model-agnostic framework for prompt optimization.
	2. Use of latent semantic space for automatic prompt generation and evaluation.
	3. Demonstrated effectiveness with improved classification accuracy in a real-world sentiment analysis task.

**Result:** In a study on the Financial PhraseBank sentiment classification benchmark, LatentPrompt improved classification accuracy by approximately 3 percent after a single optimization cycle.

**Limitations:** The framework's performance may vary based on the chosen latent space representation and the specific task.

**Conclusion:** LatentPrompt's model-agnostic framework is versatile and can be applied to various domains and tasks, requiring only black-box access to LLMs and an automatic evaluation metric.

**Abstract:** Recent advances have shown that optimizing prompts for Large Language Models (LLMs) can significantly improve task performance, yet many optimization techniques rely on heuristics or manual exploration. We present LatentPrompt, a model-agnostic framework for prompt optimization that leverages latent semantic space to automatically generate, evaluate, and refine candidate prompts without requiring hand-crafted rules. Beginning with a set of seed prompts, our method embeds them in a continuous latent space and systematically explores this space to identify prompts that maximize task-specific performance. In a proof-of-concept study on the Financial PhraseBank sentiment classification benchmark, LatentPrompt increased classification accuracy by approximately 3 percent after a single optimization cycle. The framework is broadly applicable, requiring only black-box access to an LLM and an automatic evaluation metric, making it suitable for diverse domains and tasks.

</details>


### [140] [Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity](https://arxiv.org/abs/2508.02498)

*Md Tasin Abir, Arpita Chowdhury, Ashfia Rahman*

**Main category:** cs.CL

**Keywords:** collective identity, Facebook, political mobilization, multimodal expressions, uprising

**Relevance Score:** 4

**TL;DR:** This study examines how Facebook facilitated collective identity during the Monsoon Uprising in Bangladesh using visual and verbal expressions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the role of social media in fostering identity and mobilization during political protests, particularly in authoritarian contexts.

**Method:** A qualitative analysis of multimodal expressions on Facebook, including visuals, memes, and discourse, during the July 2024 pro-democracy uprising in Bangladesh.

**Key Contributions:**

	1. Analysis of multimodal communication in social protests
	2. Demonstration of the role of Facebook in shaping collective identity
	3. Insights into the use of visual rhetoric and discourse during political unrest.

**Result:** Facebook was instrumental in unifying protesters by employing symbolic imagery and discourse, which fostered a collective identity against government repression.

**Limitations:** 

**Conclusion:** The study illustrates that online platforms like Facebook can effectively aid in identity construction and political mobilization during social movements.

**Abstract:** This study investigates how Facebook shaped collective identity during the July 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising. During government repression, protesters turned to Facebook as a central space for resistance, where multimodal expressions, images, memes, videos, hashtags, and satirical posts played an important role in unifying participants. Using a qualitative approach, this research analyzes visual rhetoric, verbal discourse, and digital irony to reveal how shared symbols, protest art, and slogans built a sense of solidarity. Key elements included the symbolic use of red, the ironic metaphorical use of the term "Razakar", and the widespread sharing of visuals representing courage, injustice, and resistance. The findings show that the combination of visual and verbal strategies on Facebook not only mobilized public sentiment, but also built a strong collective identity that challenged authoritarian narratives. This study tries to demonstrate how online platforms can serve as powerful tools for identity construction and political mobilization in the digital age.

</details>


### [141] [From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks](https://arxiv.org/abs/2508.02502)

*Shuzhou Yuan, Zhan Qu, Mario Tawfelis, Michael Färber*

**Main category:** cs.CL

**Keywords:** Large Language Models, psycholinguistics, language identity, cross-linguistic cognition, LLMs

**Relevance Score:** 8

**TL;DR:** This paper investigates how Large Language Models (LLMs) encode psycholinguistic knowledge in different languages and their responses to linguistic identity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the psycholinguistic responses of LLMs across different languages and understand their internal representations related to language identity.

**Method:** The study uses two tasks (sound symbolism and word valence) to evaluate Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct under monolingual and bilingual prompting in English, Dutch, and Chinese.

**Key Contributions:**

	1. Demonstrated that LLMs exhibit human-like psycholinguistic responses based on language identity.
	2. Probed deeper layers of LLMs revealing more decodable psycholinguistic signals.
	3. Provided insights into LLMs as models of cross-linguistic cognition.

**Result:** Both models had outputs that adjusted based on the prompted language identity, with Qwen exhibiting greater sensitivity between Dutch and Chinese. Psycholinguistic signals were found more decodable in deeper layers, especially with Chinese prompts showing stronger valence representations.

**Limitations:** 

**Conclusion:** The findings reveal that LLMs' outputs and internal representations are influenced by language identity, contributing to our understanding of cross-linguistic cognition.

**Abstract:** Large Language Models (LLMs) exhibit strong linguistic capabilities, but little is known about how they encode psycholinguistic knowledge across languages. We investigate whether and how LLMs exhibit human-like psycholinguistic responses under different linguistic identities using two tasks: sound symbolism and word valence. We evaluate two models, Llama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and bilingual prompting in English, Dutch, and Chinese. Behaviorally, both models adjust their outputs based on prompted language identity, with Qwen showing greater sensitivity and sharper distinctions between Dutch and Chinese. Probing analysis reveals that psycholinguistic signals become more decodable in deeper layers, with Chinese prompts yielding stronger and more stable valence representations than Dutch. Our results demonstrate that language identity conditions both output behavior and internal representations in LLMs, providing new insights into their application as models of cross-linguistic cognition.

</details>


### [142] [Modular Arithmetic: Language Models Solve Math Digit by Digit](https://arxiv.org/abs/2508.02513)

*Tanja Baeumel, Daniil Gurgurov, Yusser al Ghussin, Josef van Genabith, Simon Ostermann*

**Main category:** cs.CL

**Keywords:** Large Language Models, arithmetic, digit-position circuits, causal interventions, Feature Importance

**Relevance Score:** 8

**TL;DR:** This paper investigates how Large Language Models (LLMs) handle simple arithmetic tasks by examining their internal digit-position-specific circuits used for calculation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The work aims to provide a unified understanding of the mechanisms LLMs use for arithmetic tasks, which has not been fully explored.

**Method:** The authors employed Feature Importance and Causal Interventions to identify and validate the existence of digit-position-specific circuits in LLMs that perform independent operations on different digit positions (units, tens, hundreds).

**Key Contributions:**

	1. Identification of digit-position-specific circuits in LLMs
	2. Demonstration of causal interventions impacting model predictions
	3. Insights into the interpretable structure of arithmetic problem-solving in LLMs

**Result:** The findings reveal that LLMs possess modular circuits operating on distinct digit positions, which are effective regardless of the model's size or tokenization approach.

**Limitations:** 

**Conclusion:** The presence of these digit-position-specific circuits contributes to the compositional and interpretable nature of how LLMs solve arithmetic problems, underscoring their causal significance in predictions.

**Abstract:** While recent work has begun to uncover the internal strategies that Large Language Models (LLMs) employ for simple arithmetic tasks, a unified understanding of their underlying mechanisms is still lacking. We extend recent findings showing that LLMs represent numbers in a digit-wise manner and present evidence for the existence of digit-position-specific circuits that LLMs use to perform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that operate independently on different digit positions (units, tens, hundreds). Notably, such circuits exist independently of model size and of tokenization strategy, i.e. both for models that encode longer numbers digit-by-digit and as one token. Using Feature Importance and Causal Interventions, we identify and validate the digit-position-specific circuits, revealing a compositional and interpretable structure underlying the solving of arithmetic problems in LLMs. Our interventions selectively alter the model's prediction at targeted digit positions, demonstrating the causal role of digit-position circuits in solving arithmetic tasks.

</details>


### [143] [PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs](https://arxiv.org/abs/2508.02515)

*Zhan Qu, Shuzhou Yuan, Michael Färber*

**Main category:** cs.CL

**Keywords:** Large Language Models, Songci, Poetry Generation, Evaluation Framework, Generate-Critic Architecture

**Relevance Score:** 5

**TL;DR:** This paper investigates the constrained generation capabilities of large language models (LLMs) in producing Songci, a Chinese poetry form, using a multi-faceted evaluation framework and proposes improvements via a Generate-Critic architecture.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the generative performance of LLMs in producing culturally significant poetry under strict structural and tonal constraints.

**Method:** The study develops a comprehensive evaluation framework that combines formal scoring, automated quality assessment, human evaluation, and classification-based probing. It evaluates 18 LLMs across different prompting strategies and introduces a Generate-Critic architecture for fine-tuning LLMs.

**Key Contributions:**

	1. Development of a comprehensive evaluation framework for LLMs in poetry generation
	2. Proposition of a Generate-Critic architecture for fine-tuning LLMs
	3. Empirical analysis of 18 different LLMs under diverse prompting strategies

**Result:** The evaluation of 18 LLMs revealed varying generative performances, and the introduction of a Generate-Critic architecture led to improvements in conformity scores by up to 5.88%.

**Limitations:** The study is limited to the evaluation of LLMs in a specific poetic form and does not explore generative performance in other literary forms.

**Conclusion:** The research provides insights into the capabilities and limitations of LLMs in generating formally constrained poetry, suggesting areas for further enhancement in model training and evaluation.

**Abstract:** This paper presents a systematic investigation into the constrained generation capabilities of large language models (LLMs) in producing Songci, a classical Chinese poetry form characterized by strict structural, tonal, and rhyme constraints defined by Cipai templates. We first develop a comprehensive, multi-faceted evaluation framework that includes: (i) a formal conformity score, (ii) automated quality assessment using LLMs, (iii) human evaluation, and (iv) classification-based probing tasks. Using this framework, we evaluate the generative performance of 18 LLMs, including 3 proprietary models and 15 open-source models across four families, under five prompting strategies: zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought. Finally, we propose a Generate-Critic architecture in which the evaluation framework functions as an automated critic. Leveraging the critic's feedback as a reward signal, we fine-tune three lightweight open-source LLMs via supervised fine-tuning (SFT), resulting in improvements of up to 5.88% in formal conformity. Our findings offer new insights into the generative strengths and limitations of LLMs in producing culturally significant and formally constrained literary texts.

</details>


### [144] [I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2](https://arxiv.org/abs/2508.02527)

*Jack Merullo, Arjun Khurana, Oliver McLaughlin*

**Main category:** cs.CL

**Keywords:** phonetic tasks, Llama-3.2-1B-Instruct, phoneme representation

**Relevance Score:** 7

**TL;DR:** The study investigates Llama's internal representation of phonetic information, revealing a structured model of phonemes aiding phonetic tasks like rhyming.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the phonetic capabilities of large language models despite a lack of explicit phonetic or auditory grounding.

**Method:** Analyzed the Llama-3.2-1B-Instruct model to assess its internal phoneme representation and identified a specific phoneme mover head that enhances phonetic information during tasks.

**Key Contributions:**

	1. Identification of a phoneme mover head that processes phonetic information.
	2. Visualization of phoneme representations aligning with known linguistic models.
	3. Evidence of intrinsic phonetic organization within the model's latent space.

**Result:** The results indicate that Llama possesses a rich model of phonemes and organizes them in a latent space reminiscent of the IPA vowel chart without explicit supervision.

**Limitations:** 

**Conclusion:** Llama's learning reflects an intrinsic understanding of phonemes, demonstrating its potential in phonetic tasks even without direct training on phonetic principles.

**Abstract:** Large language models demonstrate proficiency on phonetic tasks, such as rhyming, without explicit phonetic or auditory grounding. In this work, we investigate how \verb|Llama-3.2-1B-Instruct| represents token-level phonetic information. Our results suggest that Llama uses a rich internal model of phonemes to complete phonetic tasks. We provide evidence for high-level organization of phoneme representations in its latent space. In doing so, we also identify a ``phoneme mover head" which promotes phonetic information during rhyming tasks. We visualize the output space of this head and find that, while notable differences exist, Llama learns a model of vowels similar to the standard IPA vowel chart for humans, despite receiving no direct supervision to do so.

</details>


### [145] [Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction](https://arxiv.org/abs/2508.02532)

*Karan Reddy, Mayukha Pal*

**Main category:** cs.CL

**Keywords:** Graph Neural Networks, Transformers, Domain-specific Question Answering, Technical Documents

**Relevance Score:** 8

**TL;DR:** This paper introduces the Contextual Graph Transformer (CGT), a hybrid model combining Graph Neural Networks and Transformers for effective domain-specific question answering in technical documents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Standard transformer-based models struggle with syntax and entity relationships in complex technical documents, necessitating a specialized model.

**Method:** The CGT constructs a dynamic graph with edges based on sequential, skip-gram, and semantic similarity, processed by GATv2Conv layers before being passed to a Transformer encoder.

**Key Contributions:**

	1. Introduction of Contextual Graph Transformer (CGT) for domain-specific question answering
	2. Demonstrated superior performance over GPT-2 and BERT with fewer parameters
	3. Showcased the model's adaptability in processing technical language and entities.

**Result:** CGT outperforms baseline models like GPT-2 and BERT by 24.7% in accuracy with 62.4% fewer parameters, thanks to its ability to model structural interactions and semantic coherence.

**Limitations:** 

**Conclusion:** CGT adapts well to technical language, improving grounding, entity tracking, and retrieval-augmented responses in practical applications.

**Abstract:** Standard transformer-based language models, while powerful for general text, often struggle with the fine-grained syntax and entity relationships in complex technical, engineering documents. To address this, we propose the Contextual Graph Transformer (CGT), a hybrid neural architecture that combines Graph Neural Networks (GNNs) and Transformers for domain-specific question answering. CGT constructs a dynamic graph over input tokens using sequential, skip-gram, and semantic similarity edges, which is processed by GATv2Conv layers for local structure learning. These enriched embeddings are then passed to a Transformer encoder to capture global dependencies. Unlike generic large models, technical domains often require specialized language models with stronger contextualization and structure awareness. CGT offers a parameter-efficient solution for such use cases. Integrated into a Retrieval-Augmented Generation (RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7% higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from CGTs ability to jointly model structural token interactions and long-range semantic coherence. The model is trained from scratch using a two-phase approach: pretraining on general text followed by fine-tuning on domain-specific manuals. This highlights CGTs adaptability to technical language, enabling better grounding, entity tracking, and retrieval-augmented responses in real-world applications.

</details>


### [146] [What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)](https://arxiv.org/abs/2508.02540)

*Anastasia Zhukova, Terry Ruas, Felix Hamborg, Karsten Donnay, Bela Gipp*

**Main category:** cs.CL

**Keywords:** news bias, information reliability, visualization, text analysis, media literacy

**Relevance Score:** 5

**TL;DR:** This paper presents a novel methodology for the automatic identification of bias in news articles by analyzing three dimensions: commission, omission, and source selection, aiming to provide a comprehensive solution rather than treating these biases separately.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in news volume has made it difficult for readers to identify reliable sources and neutrality in reporting, which is crucial for informed decision-making.

**Method:** The authors propose a pipeline methodology that jointly addresses bias types across three tasks: commission, omission, and source selection, providing analysis and visualization tools.

**Key Contributions:**

	1. Joint methodology for bias identification in news articles
	2. Comprehensive framework addressing multiple dimensions of bias
	3. Visualization tool demonstrating the extracted features and patterns

**Result:** The methodology offers a structured approach to bias identification in news articles and includes a visualization example that highlights patterns of text reuse.

**Limitations:** 

**Conclusion:** A comprehensive approach to bias identification can aid readers in discerning the reliability and neutrality of news articles, enhancing their understanding of media content.

**Abstract:** In a world overwhelmed with news, determining which information comes from reliable sources or how neutral is the reported information in the news articles poses a challenge to news readers. In this paper, we propose a methodology for automatically identifying bias by commission, omission, and source selection (COSS) as a joint three-fold objective, as opposed to the previous work separately addressing these types of bias. In a pipeline concept, we describe the goals and tasks of its steps toward bias identification and provide an example of a visualization that leverages the extracted features and patterns of text reuse.

</details>


### [147] [Building and Aligning Comparable Corpora](https://arxiv.org/abs/2508.02555)

*Motaz Saad, David Langlois, Kamel Smaili*

**Main category:** cs.CL

**Keywords:** comparable corpora, cross-lingual similarity, Latent Semantic Indexing, multilingual NLP, document alignment

**Relevance Score:** 5

**TL;DR:** The paper presents a methodology for building and aligning comparable corpora in multiple languages, focusing on English, French, and Arabic, using cross-lingual similarity measures to facilitate multilingual NLP tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve multilingual natural language processing by creating comparable corpora when parallel texts are unavailable in various domains or languages.

**Method:** The authors constructed comparable corpora from Wikipedia and EURONEWS, then aligned these documents using two cross-lingual similarity measures: a bilingual dictionary and Latent Semantic Indexing (LSI).

**Key Contributions:**

	1. Development of a methodology for building comparable corpora from diverse sources.
	2. Introduction and evaluation of the Cross-Lingual LSI measure as a superior alignment technique.
	3. Demonstration of the effectiveness of CL-LSI in aligning documents at both topic and event levels.

**Result:** The experiments show that the Cross-Lingual LSI (CL-LSI) significantly outperforms the bilingual dictionary-based measure in aligning documents, successfully aligning them at both topic and event levels.

**Limitations:** 

**Conclusion:** The study concludes that CL-LSI is effective in the automatic alignment of comparable documents across languages, which can aid multilingual NLP applications.

**Abstract:** Comparable corpus is a set of topic aligned documents in multiple languages, which are not necessarily translations of each other. These documents are useful for multilingual natural language processing when there is no parallel text available in some domains or languages. In addition, comparable documents are informative because they can tell what is being said about a topic in different languages. In this paper, we present a method to build comparable corpora from Wikipedia encyclopedia and EURONEWS website in English, French and Arabic languages. We further experiment a method to automatically align comparable documents using cross-lingual similarity measures. We investigate two cross-lingual similarity measures to align comparable documents. The first measure is based on bilingual dictionary, and the second measure is based on Latent Semantic Indexing (LSI). Experiments on several corpora show that the Cross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure. Finally, we collect English and Arabic news documents from the British Broadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively. Then we use the CL-LSI similarity measure to automatically align comparable documents of BBC and JSC. The evaluation of the alignment shows that CL-LSI is not only able to align cross-lingual documents at the topic level, but also it is able to do this at the event level.

</details>


### [148] [Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks](https://arxiv.org/abs/2508.02556)

*Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda*

**Main category:** cs.CL

**Keywords:** clinical text annotation, SNOMED CT, Bidirectional GRU, sequence labeling, machine learning

**Relevance Score:** 9

**TL;DR:** This study presents a neural sequence labeling approach using a Bidirectional GRU for automated annotation of clinical texts with SNOMED CT concepts, achieving a 90% F1-score.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Automated annotation of clinical text is vital for structured data extraction and decision support, but manual processes are impractical at scale.

**Method:** A Bidirectional GRU model processes text with domain-adapted tokenization and segments sentences into overlapping 19-token chunks with enriched features, assigning IOB tags for concept recognition.

**Key Contributions:**

	1. Introduced a neural sequence labeling approach for clinical text annotation.
	2. Demonstrated the effectiveness of a Bi-GRU model with high performance on concept recognition tasks.
	3. Showed that RNN-based models can be both efficient and accurate compared to transformer-based models.

**Result:** Achieved a 90% F1-score on the validation set, outperforming traditional rule-based systems and matching or exceeding existing neural models.

**Limitations:** 

**Conclusion:** Lightweight RNN-based architectures provide high-quality clinical concept annotation with lower computational costs, suitable for practical applications.

**Abstract:** Automated annotation of clinical text with standardized medical concepts is critical for enabling structured data extraction and decision support. SNOMED CT provides a rich ontology for labeling clinical entities, but manual annotation is labor-intensive and impractical at scale. This study introduces a neural sequence labeling approach for SNOMED CT concept recognition using a Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences into overlapping 19-token chunks enriched with contextual, syntactic, and morphological features. The Bi-GRU model assigns IOB tags to identify concept spans and achieves strong performance with a 90 percent F1-score on the validation set. These results surpass traditional rule-based systems and match or exceed existing neural models. Qualitative analysis shows effective handling of ambiguous terms and misspellings. Our findings highlight that lightweight RNN-based architectures can deliver high-quality clinical concept annotation with significantly lower computational cost than transformer-based models, making them well-suited for real-world deployment.

</details>


### [149] [Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction](https://arxiv.org/abs/2508.02558)

*Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** Diffusion Models, Large Language Models, Cache Eviction, Sparse Attention, Token Saliency

**Relevance Score:** 8

**TL;DR:** Sparse-dLLM introduces a training-free framework for dynamic cache eviction and sparse attention in diffusion large language models, improving decoding efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Diffusion Large Language Models face high computational and memory costs during inference due to quadratic complexity. Current caching methods improve speed but have high memory demands, particularly in long-context scenarios.

**Method:** The paper presents Sparse-dLLM, which combines dynamic cache eviction with sparse attention using a delayed bidirectional sparse caching strategy, retaining crucial tokens while evicting unimportant ones based on attention patterns.

**Key Contributions:**

	1. First training-free framework for dynamic cache eviction in dLLMs
	2. Integration of sparse attention through delayed bidirectional caching
	3. Demonstration of up to 10x higher throughput compared to existing methods

**Result:** Sparse-dLLM achieves up to 10 times higher throughput compared to vanilla diffusion large language models while maintaining performance and similar peak memory costs.

**Limitations:** 

**Conclusion:** The proposed framework significantly enhances efficiency and effectiveness in the use of diffusion large language models, making it suitable for applications with long-context requirements.

**Abstract:** Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness.

</details>


### [150] [Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs](https://arxiv.org/abs/2508.02573)

*Jérémie Dentan, Davide Buscaldi, Sonia Vanier*

**Main category:** cs.CL

**Keywords:** Large Language Models, memorization, attention weights, taxonomy, interpretability

**Relevance Score:** 9

**TL;DR:** This paper explores verbatim memorization in LLMs, proposing a new taxonomy based on attention weights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the distinct mechanisms of verbatim memorization in LLMs and improve the existing taxonomy that is poorly aligned with attention weights.

**Method:** Trained CNNs on LLM attention weights and evaluated the alignment with a proposed taxonomy of memorization forms.

**Key Contributions:**

	1. Introduction of a new taxonomy for verbatim memorization in LLMs
	2. Development of a custom visual interpretability technique
	3. Demonstration that the existing taxonomy inadequately reflects the nature of memorized samples.

**Result:** The analysis revealed that the existing taxonomy fails to represent the mechanisms accurately, leading to a new classification: guessed samples, recalled samples, and non-memorized samples.

**Limitations:** 

**Conclusion:** Few-shot verbatim memorization doesn't correspond to a distinct attention mechanism, and a significant number of samples are guessed by the model, necessitating separate study.

**Abstract:** Verbatim memorization in Large Language Models (LLMs) is a multifaceted phenomenon involving distinct underlying mechanisms. We introduce a novel method to analyze the different forms of memorization described by the existing taxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the attention weights of the LLM and evaluate the alignment between this taxonomy and the attention weights involved in decoding.   We find that the existing taxonomy performs poorly and fails to reflect distinct mechanisms within the attention blocks. We propose a new taxonomy that maximizes alignment with the attention weights, consisting of three categories: memorized samples that are guessed using language modeling abilities, memorized samples that are recalled due to high duplication in the training set, and non-memorized samples. Our results reveal that few-shot verbatim memorization does not correspond to a distinct attention mechanism. We also show that a significant proportion of extractable samples are in fact guessed by the model and should therefore be studied separately. Finally, we develop a custom visual interpretability technique to localize the regions of the attention weights involved in each form of memorization.

</details>


### [151] [EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare](https://arxiv.org/abs/2508.02574)

*Eman Alamoudi, Ellis Solaiman*

**Main category:** cs.CL

**Keywords:** Arabic, sentiment analysis, healthcare, ChatGPT, aspect-based labeling

**Relevance Score:** 8

**TL;DR:** EHSAN introduces a novel pipeline for Arabic aspect-based sentiment analysis in healthcare, combining ChatGPT pseudo-labelling and human review to create an annotated dataset with high accuracy in classification tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenges of dialect diversity and lack of aspect-level sentiment labels in Arabic-language patient feedback, enabling more effective automated sentiment analysis.

**Method:** A data-centric hybrid pipeline that integrates ChatGPT pseudo-labelling with targeted human review, producing an explainable Arabic aspect-based sentiment dataset for healthcare assessments.

**Key Contributions:**

	1. First explainable Arabic aspect-based sentiment dataset for healthcare
	2. Combination of ChatGPT pseudo-labelling and human review
	3. High accuracy achieved with minimal human supervision

**Result:** The Arabic-specific model exhibited high accuracy even with minimal human supervision, and results indicated a significant improvement in classification metrics by reducing the number of aspect classes.

**Limitations:** Future work is needed to ensure generalisation across different hospitals and improve prompt refinement.

**Conclusion:** The proposed methodology demonstrates an effective, scalable approach for sentiment analysis in healthcare, with potential for broader generalisation and improved model interpretability in future work.

**Abstract:** Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.

</details>


### [152] [MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification](https://arxiv.org/abs/2508.02584)

*Ming Pok Ng, Junqi Jiang, Gabriel Freedman, Antonio Rago, Francesca Toni*

**Main category:** cs.CL

**Keywords:** Large Language Models, Claim Verification, Argumentative Reasoning

**Relevance Score:** 9

**TL;DR:** Introducing MArgE, a framework for structured claim verification using multiple LLMs to improve accuracy and justification in decision-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable methods to combine outputs from multiple LLMs to mitigate errors such as hallucinations in AI models.

**Method:** A framework called MArgE leverages argumentative LLMs to construct structured argument trees for claim verification, enhancing the interpretability of model decisions.

**Key Contributions:**

	1. Development of MArgE framework for structured claim verification
	2. Demonstration of improved accuracy over traditional LLM approaches
	3. Novel combination of argumentative reasoning with LLM outputs

**Result:** MArgE significantly outperforms individual LLMs and existing multi-LLM debate methods in terms of claim verification accuracy.

**Limitations:** 

**Conclusion:** The study highlights the benefits of formal argumentative reasoning in enhancing the reliability of conclusions drawn from multiple LLM outputs.

**Abstract:** Leveraging outputs from multiple large language models (LLMs) is emerging as a method for harnessing their power across a wide range of tasks while mitigating their capacity for making errors, e.g., hallucinations. However, current approaches to combining insights from multiple LLMs often involve unstructured interactions (e.g., free debate), resulting in model generations that are not faithfully justifiable. In this work, we introduce MArgE, a novel framework to provide formal structure to the evidence from each LLM, in the form of a tree of extracted arguments, for the task of claim verification. We use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks and semantics from the field of computational argumentation, to construct structured argument trees for given claims. This process creates an inspectable pathway from the initial arguments to the final claim verification decisions, providing a faithful justification thereof. We show experimentally that MArgE can significantly outperform single LLMs, including three open-source models (4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior methods for unstructured multi-LLM debates. We thus demonstrate the advantages of incorporating formal, argumentative reasoning mechanisms when combining multiple LLM outputs.

</details>


### [153] [CharBench: Evaluating the Role of Tokenization in Character-Level Tasks](https://arxiv.org/abs/2508.02591)

*Omri Uzan, Yuval Pinter*

**Main category:** cs.CL

**Keywords:** character-level reasoning, language models, tokenization, CharBench, performance analysis

**Relevance Score:** 8

**TL;DR:** CharBench is a new benchmark for character-level tasks revealing significant challenges for language models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the struggles of language models with character-level reasoning and the unclear role of tokenization in model performance.

**Method:** Introduced CharBench, a benchmark two orders of magnitude larger than existing alternatives, and evaluated various language models using this benchmark.

**Key Contributions:**

	1. Introduction of CharBench as a benchmark for character-level tasks
	2. Analysis of model performance in relation to tokenization and word properties
	3. Identification of key correlations affecting performance in character-level reasoning tasks

**Result:** Leading models showed an average accuracy of 43.6%, with some tasks yielding as low as 32.3%. Analysis revealed that word length and actual character count significantly influence performance.

**Limitations:** The benchmark may primarily reflect current limitations and does not address broader language understanding tasks.

**Conclusion:** The findings indicate that longer tokens can obscure character position information, and the study encourages further research using CharBench to enhance model performance.

**Abstract:** Tasks that require character-level reasoning, such as counting or locating characters within words, remain challenging for contemporary language models. A common conjecture is that language models' reliance on subword units, rather than characters, contributes to their struggles with character-level tasks, yet recent studies offer conflicting conclusions about the role of tokenization, leaving its impact unclear. To address this gap, we introduce CharBench, a comprehensive benchmark of character-level tasks that is two orders of magnitude larger than existing alternatives. We evaluate a diverse range of leading open-weight and proprietary models on CharBench and find that it presents a significant challenge to modern LLMs, with an average accuracy of 43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic properties of words and their segmentations into tokens correspond to model performance. For counting tasks, we find that tokenization properties are weakly correlated with correctness, while the length of the queried word and the actual character count play a more significant part. In contrast, for tasks requiring intra-word positional understanding, performance is negatively correlated with the length of the token containing the queried character, suggesting that longer tokens obscure character position information for LLMs. We encourage future work to build on the benchmark and evaluation methodology introduced here as tools for improving model performance on such tasks.

</details>


### [154] [Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation](https://arxiv.org/abs/2508.02618)

*Jianxiang Zang, Meiling Ning, Shihan Dou, Jiazheng Zhang, Tao Gui, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** reinforcement learning, human feedback, large language models, preference modeling, attention mechanisms

**Relevance Score:** 9

**TL;DR:** This paper proposes "Interaction Distillation", a new training framework to improve preference modeling in reinforcement learning from human feedback for large language models, addressing limitations due to inadequate token-level interactions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the robustness of reward models in LLMs by addressing the vulnerabilities caused by mainstream preference modeling inadequacies.

**Method:** The authors introduce "Interaction Distillation", which uses an interaction-based natural language understanding model as a teacher to optimize attention-level interactions during preference modeling.

**Key Contributions:**

	1. Introduction of Interaction Distillation framework for preference modeling
	2. Optimization of attention-level interactions in reward models
	3. Demonstrated stability and generalizability of reward signals compared to existing methods

**Result:** Extensive experiments show that Interaction Distillation offers more stable and generalizable reward signals compared to existing reward model optimization methods that focus on data noise.

**Limitations:** 

**Conclusion:** The study concludes that attention hacking is a fundamental limitation in reward modeling, which can be mitigated through the proposed interaction-based training approach.

**Abstract:** The reward model (RM), as the core component of reinforcement learning from human feedback (RLHF) for large language models (LLMs), responsible for providing reward signals to generated responses. However, mainstream preference modeling in RM is inadequate in terms of token-level interaction, making its judgment signals vulnerable to being hacked by misallocated attention to context. This stems from two fundamental limitations: (1) Current preference modeling employs decoder-only architectures, where the unidirectional causal attention mechanism leads to forward-decaying intra-sequence attention within the prompt-response sequence. (2) The independent Siamese-encoding paradigm induces the absence of token-level inter-sequence attention between chosen and rejected sequences. To address this "attention hacking", we propose "Interaction Distillation", a novel training framework for more adequate preference modeling through attention-level optimization. The method introduces an interaction-based natural language understanding model as the teacher to provide sophisticated token interaction patterns via comprehensive attention, and guides the preference modeling to simulate teacher model's interaction pattern through an attentional alignment objective. Through extensive experiments, interaction distillation has demonstrated its ability to provide more stable and generalizable reward signals compared to state-of-the-art RM optimization methods that target data noise, highlighting the attention hacking constitute a more fundamental limitation in RM.

</details>


### [155] [Pointer: Linear-Complexity Long-Range Modeling without Pre-training](https://arxiv.org/abs/2508.02631)

*Zixi Li*

**Main category:** cs.CL

**Keywords:** Pointer architecture, sequence modeling, interpretability

**Relevance Score:** 6

**TL;DR:** Pointer introduces an architecture for long-range sequence modeling with linear complexity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of standard attention mechanisms for long sequences.

**Method:** Layer-wise pointer chaining to create explicit long-distance connections without pairwise interactions.

**Key Contributions:**

	1. Linear $O(NK)$ complexity for long-range sequence modeling
	2. No requirement for pre-training
	3. Superior performance with interpretable pointer patterns

**Result:** Achieves $2$--$10\times$ speedup on long sequences with over $95\%$ accuracy on copy tasks up to 2048 tokens.

**Limitations:** 

**Conclusion:** Pointer is a promising alternative to traditional attention mechanisms for efficient long-range modeling.

**Abstract:** We introduce Pointer, a novel architecture that achieves linear $O(NK)$ complexity for long-range sequence modeling while maintaining superior performance without requiring pre-training. Unlike standard attention mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses layer-wise pointer chaining where each layer's pointer selection depends on previous layer's pointer positions, creating explicit long-distance connections through pointer chains. We demonstrate that this architecture achieves $2$--$10\times$ speedup on long sequences compared to standard transformers, maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and learns interpretable pointer patterns that reveal structured dependency modeling. Our experiments on efficiency benchmarks, long-range dependency tasks, and interpretability analysis show that Pointer offers a compelling alternative to attention mechanisms for scenarios requiring efficient long-range modeling without pre-training dependencies.

</details>


### [156] [Test Set Quality in Multilingual LLM Evaluation](https://arxiv.org/abs/2508.02635)

*Kranti Chalamalasetti, Gabriel Bernier-Colborne, Yvan Gauthier, Sowmya Vajjala*

**Main category:** cs.CL

**Keywords:** Multilingual Datasets, Large Language Models, Dataset Quality

**Relevance Score:** 7

**TL;DR:** The paper analyzes multilingual benchmark datasets for LLMs, highlighting quality issues in French and Telugu datasets, and recommends revisiting test set quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of attention on the quality of multilingual benchmark datasets used for evaluating Large Language Models.

**Method:** Manual analysis of multilingual evaluation sets for French and Telugu, identifying errors and comparing the performance differences across several LLMs with original and revised datasets.

**Key Contributions:**

	1. Manual analysis of errors in multilingual datasets
	2. Performance comparison across LLMs with original vs revised datasets
	3. Recommendations for improving dataset quality

**Result:** Identified substantial errors in multilingual datasets, leading to performance differences of almost 10% across LLMs for both languages.

**Limitations:** 

**Conclusion:** Test sets should be revisited for correctness and potentially versioned, with recommendations provided for dataset creators and consumers.

**Abstract:** Several multilingual benchmark datasets have been developed in a semi-automatic manner in the recent past to measure progress and understand the state-of-the-art in the multilingual capabilities of Large Language Models. However, there is not a lot of attention paid to the quality of the datasets themselves, despite the existence of previous work in identifying errors in even fully human-annotated test sets. In this paper, we manually analyze recent multilingual evaluation sets in two languages - French and Telugu, identifying several errors in the process. We compare the performance difference across several LLMs with the original and revised versions of the datasets and identify large differences (almost 10% in some cases) in both languages). Based on these results, we argue that test sets should not be considered immutable and should be revisited, checked for correctness, and potentially versioned. We end with some recommendations for both the dataset creators as well as consumers on addressing the dataset quality issues.

</details>


### [157] [You Can Generate It Again: Data-to-Text Generation with Verification and Correction Prompting](https://arxiv.org/abs/2306.15933)

*Xuan Ren, Zeyu Zhang, Lingqiao Liu*

**Main category:** cs.CL

**Keywords:** small language models, data-to-text generation, semantic fidelity, verification, keywords

**Relevance Score:** 8

**TL;DR:** This paper proposes the Verification and Correction Prompting (VCP) approach to enhance semantic fidelity in small language models for data-to-text generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Small language models often fail to include important keywords in generated text, impacting the quality of data-to-text tasks. The paper aims to improve keyword inclusion and semantic fidelity in these models.

**Method:** The VCP approach includes a multi-step process during inference: generation, verification (checking for keywords), and regeneration. The training procedure is designed to help the model incorporate feedback from erroneous predictions effectively.

**Key Contributions:**

	1. Introduction of the Verification and Correction Prompting (VCP) method.
	2. Demonstrated reduction in Semantic Error Rate (SER) while maintaining text quality.
	3. Developed a training procedure that integrates feedback from keyword checks.

**Result:** The VCP approach successfully reduces the Semantic Error Rate (SER) while preserving the quality of the generated text.

**Limitations:** The verification rule may produce inaccuracies that could affect the training process and results.

**Conclusion:** Implementing a feedback system in small language models can enhance their performance in data-to-text tasks, making them more competitive against larger models.

**Abstract:** Small language models like T5 excel in generating high-quality text for data-to-text tasks, offering adaptability and cost-efficiency compared to Large Language Models (LLMs). However, they frequently miss keywords, which is considered one of the most severe and common errors in this task. In this work, we explore the potential of using feedback systems to enhance semantic fidelity in smaller language models for data-to-text generation tasks, through our Verification and Correction Prompting (VCP) approach. In the inference stage, our approach involves a multi-step process, including generation, verification, and regeneration stages. During the verification stage, we implement a simple rule to check for the presence of every keyword in the prediction. Recognizing that this rule can be inaccurate, we have developed a carefully designed training procedure, which enabling the model to incorporate feedback from the error-correcting prompt effectively, despite its potential inaccuracies. The VCP approach effectively reduces the Semantic Error Rate (SER) while maintaining the text's quality.

</details>


### [158] [Thinker-DDM: Modeling Deliberation for Machine Translation with a Drift-Diffusion Process](https://arxiv.org/abs/2402.10699)

*Hongbin Na, Zimu Wang, Mieradilijiang Maimaiti, Tong Chen, Wei Wang, Tao Shen, Ling Chen*

**Main category:** cs.CL

**Keywords:** Machine Translation, Large Language Models, Drift-Diffusion Model

**Relevance Score:** 8

**TL;DR:** This paper integrates a Drift-Diffusion Model with large language models to improve decision-making in machine translation, outperforming existing methods in various resource settings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance LLM-based machine translation by incorporating human-like decision-making processes, which have been largely overlooked in prior research.

**Method:** The authors introduce Thinker with Drift-Diffusion Model (Thinker-DDM) to emulate dynamic decision-making in translation, and conduct experiments across high-resource, low-resource, and commonsense translation scenarios using WMT22 and CommonMT datasets.

**Key Contributions:**

	1. Introduction of Thinker-DDM for machine translation
	2. Emulation of human-like decision-making in LLMs
	3. Demonstrated effectiveness across different translation resource scenarios.

**Result:** Thinker-DDM demonstrates superior performance compared to baseline models in high-resource and low-resource settings, with additional effectiveness shown in commonsense translation tasks.

**Limitations:** 

**Conclusion:** The incorporation of decision-making mechanisms inspired by human translators significantly improves machine translation outcomes across various resource constraints.

**Abstract:** Large language models (LLMs) have demonstrated promising potential in various downstream tasks, including machine translation. However, prior work on LLM-based machine translation has mainly focused on better utilizing training data, demonstrations, or pre-defined and universal knowledge to improve performance, with a lack of consideration of decision-making like human translators. In this paper, we incorporate Thinker with the Drift-Diffusion Model (Thinker-DDM) to address this issue. We then redefine the Drift-Diffusion process to emulate human translators' dynamic decision-making under constrained resources. We conduct extensive experiments under the high-resource, low-resource, and commonsense translation settings using the WMT22 and CommonMT datasets, in which Thinker-DDM outperforms baselines in the first two scenarios. We also perform additional analysis and evaluation on commonsense translation to illustrate the high effectiveness and efficacy of the proposed method.

</details>


### [159] [THREAD: Thinking Deeper with Recursive Spawning](https://arxiv.org/abs/2405.17402)

*Philip Schroeder, Nathaniel Morgan, Hongyin Luo, James Glass*

**Main category:** cs.CL

**Keywords:** large language models, recursive threading, task decomposition, few-shot learning, benchmark performance

**Relevance Score:** 9

**TL;DR:** Proposes a recursive and dynamic threading approach for large language models to improve performance on complex tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance language models' performance in handling longer and more complex contexts by allowing them to dynamically spawn threads to offload tasks.

**Method:** Introduces Thinking Recursively and Dynamically (ThReaD), framing model generation as a thread of execution that can dynamically decompose tasks into sub-problems handled by child threads.

**Key Contributions:**

	1. Introduces a novel threading framework for LLMs
	2. Demonstrates superior performance on multiple benchmarks
	3. Implements few-shot learning for task solving and question answering.

**Result:** Achieved state-of-the-art performance with GPT-4 and GPT-3.5 across various benchmarks and improved performance by 10% to 50% with smaller models like Llama-3-8b and CodeLlama-7b.

**Limitations:** 

**Conclusion:** ThReaD enhances the capability of LLMs to solve complex tasks more effectively by decomposing them into simpler sub-tasks.

**Abstract:** Large language models (LLMs) have shown impressive capabilities across diverse settings, but still struggle as the length and complexity of the context increases. To address this challenge, we propose Thinking Recursively and Dynamically (ThReaD). THREAD frames model generation as a thread of execution that, based on the context, can run to completion or dynamically spawn new threads. By spawning, threads can offload work (e.g., thinking, retrieving information) to child threads, which only return tokens needed for the parent thread to do its work. In effect, this enables the model to adapt, as needed, the amount of intermediate work used to produce tokens. We apply THREAD in the settings of LLM task solving and question answering, where the dynamic threading allows the model to recursively decompose the given task or question into progressively simpler sub-problems that can be solved by separate child threads. We test THREAD, implemented using a few-shot learning approach, on diverse benchmarks for agent tasks and data-grounded question answering. THREAD achieves state-of-the-art performance with GPT-4 and GPT-3.5 on these benchmarks, including ALFWorld, TextCraft, and WebShop, along with two new benchmarks, DataCommons QA and MIMIC-III ICU QA. In addition, THREAD outperforms existing frameworks by 10% to 50% absolute points with smaller models, including Llama-3-8b and CodeLlama-7b.

</details>


### [160] [Dynamic Order Template Prediction for Generative Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2406.11130)

*Yonghyun Jun, Hwanhee Lee*

**Main category:** cs.CL

**Keywords:** Aspect-based sentiment analysis, Dynamic Order Template, Multi-view prompting, F1-scores, Inference time

**Relevance Score:** 6

**TL;DR:** The paper introduces a Dynamic Order Template (DOT) method for aspect-based sentiment analysis (ABSA), improving efficiency and accuracy in sentiment tuple prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional ABSA models struggle with template staticity and element dependency capture, leading to inefficiencies and errors. There is a need for a flexible approach that can adapt to various instances of data.

**Method:** The proposed DOT method dynamically generates multiple views for each instance based on its entropy, allowing for a tailored and efficient prediction process.

**Key Contributions:**

	1. Introduction of Dynamic Order Template for ABSA
	2. Improvement in F1-scores on benchmark datasets ASQP and ACOS
	3. Reduction in inference time for ABSA tasks

**Result:** The DOT method improves F1-scores on ASQP and ACOS datasets and reduces the inference time compared to previous methods.

**Limitations:** 

**Conclusion:** By utilizing instance-level insights for view generation, the DOT method enhances the performance and efficiency of ABSA tasks, promising better application in real scenarios.

**Abstract:** Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific aspects within texts, resulting in detailed sentiment tuples. Previous ABSA models often use static templates to predict all of the elements in the tuples, and these models often fail to accurately capture dependencies between elements. Multi-view prompting method improves the performance of ABSA by predicting tuples with various templates and then ensembling the results. However, this method suffers from inefficiencies and out-of-distribution errors. In this paper, we propose a Dynamic Order Template (DOT) method for ABSA, which dynamically generates necessary views for each instance based on instance-level entropy. Ensuring the diverse and relevant view generation, our proposed method improves F1-scores on ASQP and ACOS datasets while significantly reducing inference time.

</details>


### [161] [Can Tool-augmented Large Language Models be Aware of Incomplete Conditions?](https://arxiv.org/abs/2406.12307)

*Seungbin Yang, ChaeHun Park, Taehee Kim, Jaegul Choo*

**Main category:** cs.CL

**Keywords:** large language models, tool-augmented LLMs, incomplete information, benchmark dataset, prompting strategy

**Relevance Score:** 9

**TL;DR:** This study investigates LLMs' ability to identify incomplete scenarios and proposes a new prompting strategy to enhance their reliability in decision-making.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the challenges LLMs face when dealing with incomplete information and to improve their reliability in tool use in real-world situations.

**Method:** A benchmark dataset was created to simulate ambiguous and incomplete conditions, followed by experiments to evaluate the LLMs' ability to recognize these scenarios and analyze their behavior.

**Key Contributions:**

	1. Creation of a benchmark dataset for evaluating incomplete condition recognition in LLMs
	2. Development of a prompting-based reasoning strategy to improve tool-use decision-making
	3. Insights into the behavioral flaws of LLMs when encountering ambiguous scenarios

**Result:** State-of-the-art LLMs struggle with identifying incomplete conditions and often misuse tools without sufficient context. The new prompting strategy significantly improves their performance in assessing information adequacy.

**Limitations:** The study is limited to certain types of incomplete conditions and may not cover all real-world scenarios.

**Conclusion:** Enhancing LLMs' decision-making in ambiguous contexts is critical for their reliability, especially in applications where users provide partial information.

**Abstract:** Recent advancements in integrating large language models (LLMs) with tools have allowed the models to interact with real-world environments. However, these tool-augmented LLMs often encounter incomplete scenarios when users provide partial information or the necessary tools are unavailable. Recognizing and managing such scenarios is crucial for LLMs to ensure their reliability, but this exploration remains understudied. This study examines whether LLMs can identify incomplete conditions and appropriately determine when to refrain from using tools. To quantitatively evaluate this capability, we construct a new benchmark dataset where instances are systematically altered to simulate the ambiguous and incomplete conditions common in real-world interactions. Our experiments reveal that even state-of-the-art LLMs often struggle to identify these conditions, attempting to use tools without sufficient information or when the correct tool is unavailable. To better understand these limitations, we conduct a detailed behavioral analysis across various conditions, including implicit evaluation and scenarios where models receive feedback from previous tool invocations. Based on this analysis, we propose a novel prompting-based reasoning strategy that explicitly instructs models to assess the sufficiency of information and the availability of tools. Our proposed approach significantly enhances the models' ability to recognize incomplete conditions, resulting in more informed and contextually appropriate tool-use decisions. We believe our research contributes to advancing the reliability of LLMs, especially in real-world applications where incomplete or ambiguous information is common. Our dataset is available at https://huggingface.co/datasets/ddehun/ICT.

</details>


### [162] [Cascade Reward Sampling for Efficient Decoding-Time Alignment](https://arxiv.org/abs/2406.16306)

*Bolian Li, Yifan Wang, Anamika Lochab, Ananth Grama, Ruqi Zhang*

**Main category:** cs.CL

**Keywords:** large language models, decoding-time alignment, reward models

**Relevance Score:** 8

**TL;DR:** The paper introduces Cascade Reward Sampling (CARDS) to enhance the efficiency of decoding-time alignment in large language models (LLMs) while maintaining alignment quality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning LLMs with human preferences is crucial, but existing decoding-time alignment techniques are inefficient, leading to wasted computations and prolonged decoding times.

**Method:** The CARDS approach employs a segment-level rejection sampling algorithm that minimizes redundant evaluations of LLMs and reward models (RMs) through an uncertainty-based segmentation mechanism.

**Key Contributions:**

	1. Introduction of the Cascade Reward Sampling (CARDS) technique.
	2. Development of an uncertainty-based segmentation mechanism for efficiency.
	3. Demonstration of significant improvements in decoding time and alignment quality.

**Result:** CARDS achieves approximately a 70% reduction in decoding time and over 90% improvement in alignment utility and safety benchmarks compared to existing methods.

**Limitations:** 

**Conclusion:** The results indicate that CARDS significantly enhances both decoding efficiency and alignment quality without sacrificing the general utility of pretrained LLMs.

**Abstract:** Aligning large language models (LLMs) with human preferences is essential for their applications. Recently, decoding-time alignment has emerged as an effective plug-and-play technique that avoids fine-tuning model parameters. This approach retains the general utility of pretrained LLMs but often suffers from significant inefficiencies during decoding, primarily due to wasted token generation and excessive reward evaluations. To address these challenges, we introduce Cascade Reward Sampling (CARDS) to resolve both efficiency bottlenecks in decoding-time alignment. Specifically, we develop a segment-level rejection sampling algorithm that minimizes redundant computations of both LLMs and reward models (RMs). Central to CARDS is an uncertainty-based segmentation mechanism, which ensures the accuracy of RMs evaluations on incomplete segments. Furthermore, we provide a detailed analysis of reward scores on segments to elucidate the improved alignment performance. Experimental results demonstrate that CARDS significantly improves decoding efficiency, alignment quality, and general utility compared to existing decoding-time alignment methods, achieving approximately a 70% reduction in decoding time and over 90% win-ties in utility and safety benchmarks.

</details>


### [163] [Path-LLM: A Shortest-Path-based LLM Learning for Unified Graph Representation](https://arxiv.org/abs/2408.05456)

*Wenbo Shang, Xuliang Zhu, Xin Huang*

**Main category:** cs.CL

**Keywords:** graph representation learning, large language model, Path-LLM, HCI, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Path-LLM, a novel model for unified graph representation learning that leverages large language models and innovative path features to improve efficiency and accuracy in graph analytics tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for graph representation learning face challenges such as excessive training requirements, poor generalization capabilities, and shallow semantic understanding, prompting the need for a more efficient solution.

**Method:** The Path-LLM framework introduces four techniques including the long-to-short shortest path (L2SP) mechanism for efficient path selection, path textualization for generating training texts, and integration with self-supervised LLM training to align graph generation with causal language modeling.

**Key Contributions:**

	1. Development of the L2SP selection mechanism for path efficiency
	2. Introduction of path textualization for effective LLM training
	3. Validation of Path-LLM's superiority through extensive benchmarking against leading models

**Result:** Experiments demonstrate that Path-LLM outperforms state-of-the-art methods like WalkLM and GraphGPT on node classification, edge validation, and keyword search tasks, achieving speed improvements of up to 35x and reducing training paths by over 90%.

**Limitations:** 

**Conclusion:** Path-LLM effectively enhances the process of generating unified graph embeddings, overcoming many limitations of previous models while demonstrating significant performance improvements on various graph tasks.

**Abstract:** Unified graph representation learning aims to generate node embeddings, which can be applied to multiple downstream applications of graph analytics. However, existing studies based on graph neural networks and language models either suffer from the limitations of numerous training needs toward specific downstream predictions, poor generalization, or shallow semantic features. In this work, we propose a novel Path-LLM model to efficiently learn unified graph representation, which leverages a powerful large language model (LLM) to incorporate our proposed path features. Our Path-LLM framework consists of four well-designed techniques. First, we develop a new mechanism of long-to-short shortest path (L2SP) selection, which can cover key connections between different dense groups. An in-depth analysis and comparison of different path selections is conducted to justify the rationale behind our designed L2SP method. Next, we design path textualization to obtain L2SP-based training texts with key phrase selection from node text attributes. We then feed the texts into a self-supervised LLM training process to align next node/edge generation in L2SP with next token generation in causal language modeling for graph representation learning and finally extract the unified graph embeddings. We theoretically analyze the algorithm complexity of our Path-LLM approach. Extensive experiments on large-scale graph benchmarks validate the superiority of Path-LLM against state-of-the-art methods WalkLM, GraphGPT, OFA, and GraphTranslator on two classical graph learning tasks (node classification and edge validation) and one NP-hard graph query processing task (keyword search). Compared with WalkLM, our approach saves more than 90% of training paths on millions-scale graphs and runs at most 35x faster.

</details>


### [164] [Arena-Lite: Efficient and Reliable Large Language Model Evaluation via Tournament-Based Direct Comparisons](https://arxiv.org/abs/2411.01281)

*Seonil Son, Ju-Min Oh, Heegon Jin, Cheolhun Jang, Jeongbeom Jeong, Kuntae Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation, tournament structure, Arena-Lite, reliability

**Relevance Score:** 9

**TL;DR:** Arena-Lite is a novel evaluation framework for Large Language Models (LLMs) that employs a tournament structure for head-to-head comparisons, enhancing system ranking reliability and reducing required comparisons.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of system evaluations for LLMs by moving beyond baseline-mediated comparisons, which can be less reliable.

**Method:** Arena-Lite integrates a tournament structure on top of direct head-to-head comparisons between systems, allowing evaluations without needing baseline outputs.

**Key Contributions:**

	1. Proposed a tournament structure for LLM evaluation that eliminates baseline reliance.
	2. Demonstrated higher evaluation reliability with fewer comparisons in experiments.
	3. Released a web demonstration and code to encourage practical adoption of Arena-Lite.

**Result:** Experiments show that Arena-Lite achieves consistently higher reliability with fewer comparisons, making it suitable even with smaller datasets or less robust judges.

**Limitations:** The approach may still be limited by the quality of judges and the datasets used for comparison.

**Conclusion:** The introduction of Arena-Lite offers a more reliable and efficient evaluation method for LLMs, with resources made available for wider adoption.

**Abstract:** As Large Language Models (LLMs) expand across domains, LLM judges have become essential for systems evaluation. Current benchmarks typically compare system outputs against baselines. This baseline-mediated approach, though convenient, yields lower reliability than direct comparison between systems. We propose Arena-Lite which integrates tournament structure on top of head-to-head comparison. The application of a tournament structure and direct comparison eliminates the need for baseline outputs, reduces the number of required comparisons, and allows higher reliability in system rankings. We conducted two experiments: (1) controlled stochastic modeling and (2) empirical validation with a real LLM judge. Those experiments collectively demonstrate that Arena-Lite consistently achieves higher reliability with fewer comparisons, even with smaller datasets or weaker judges. We release an easy-to-use web demonstration and code to foster adoption of Arena-Lite, streamlining model selection across research and industry communities. Arena-Lite demo and code are available on \href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}

</details>


### [165] [Training and Evaluating Language Models with Template-based Data Generation](https://arxiv.org/abs/2411.18104)

*Yifan Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Generation, Math Problems, Template-based Approach, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** The paper introduces Template-based Data Generation (TDG) for creating high-quality math problem datasets to improve reasoning in large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of data scarcity that hinders complex reasoning capabilities in LLMs, particularly for mathematical problem-solving.

**Method:** TDG harnesses GPT-4 to automatically generate parameterized meta-templates, enabling the creation of millions of synthetic math problems with verifiable solutions.

**Key Contributions:**

	1. Introduction of Template-based Data Generation (TDG) for dataset creation
	2. Development of TemplateMath Part I: TemplateGSM with 7 million math problems
	3. Verifiable solutions enabling reliable model alignment through RLVR.

**Result:** The creation of TemplateMath Part I: TemplateGSM, a dataset comprising over 7 million generated grade school math problems, significantly enhances the quality and scale of training data.

**Limitations:** 

**Conclusion:** TDG and TemplateGSM offer a scalable solution to data scarcity and verification for improving the reasoning abilities of LLMs, providing resources for model alignment and supervised fine-tuning.

**Abstract:** The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, a fundamental bottleneck persists: these models often struggle with tasks requiring complex, multi-step reasoning, particularly in mathematical problem-solving. This deficiency stems from the critical scarcity of large-scale, high-quality, domain-specific datasets necessary for cultivating sophisticated reasoning abilities. To overcome this challenge, we introduce Template-based Data Generation (TDG), a novel and scalable paradigm that harnesses frontier LLMs (GPT-4) to automatically generate parameterized meta-templates, which in turn synthesize a virtually infinite stream of high-quality problems and solutions. Using this paradigm, we create TemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million synthetically generated grade school math problems. Each problem is accompanied by a programmatically verifiable solution, offering an unprecedented level of quality at scale. This resource not only resolves the data scarcity issue for supervised fine-tuning but also provides a robust mechanism for model alignment through Reinforcement Learning with Verifiable Rewards (RLVR). Our approach elevates data augmentation by employing GPT-4 for meta-template creation, guaranteeing diverse and complex problem structures. By providing a scalable solution to the data and verification bottleneck, TDG and TemplateGSM pave the way for a new generation of LLMs with powerful, reliable reasoning skills. The code and data are available at https://github.com/iiis-ai/TemplateMath.

</details>


### [166] [Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity](https://arxiv.org/abs/2412.02252)

*Da Ma, Lu Chen, Situo Zhang, Yuxun Miao, Su Zhu, Zhi Chen, Hongshen Xu, Hanqi Li, Shuai Fan, Lei Pan, Kai Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, KV cache, memory compression, attention mechanisms, contextual tokens

**Relevance Score:** 8

**TL;DR:** Proximal tokens over Distant tokens (PoD) is a KV cache compression framework that reduces memory usage by addressing the importance of tokens during inference in Large Language Models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing memory usage in LLMs due to larger context windows and existing compression methods that may discard critical information motivated the development of PoD.

**Method:** PoD allocates memory based on token importance by retaining proximal tokens fully and compactly storing distant tokens, sharing key states across layers.

**Key Contributions:**

	1. Introduction of PoD, a novel KV cache compression framework.
	2. Preservation of proximal tokens and efficient sharing of distant token states across layers.
	3. Demonstrated significant memory reduction without performance loss on various benchmarks.

**Result:** PoD reduces KV cache memory usage by up to 35% while maintaining performance in long-context tasks based on experimental validation.

**Limitations:** 

**Conclusion:** PoD offers a novel approach to KV cache compression in LLMs that can work alongside existing selection methods to further optimize memory usage.

**Abstract:** The rapid expansion of context window sizes in Large Language Models~(LLMs) has enabled them to tackle increasingly complex tasks involving lengthy documents. However, this progress comes at the cost of a substantial increase in memory usage during inference, primarily due to the linear growth of the key-value~(KV) cache. Existing KV cache compression methods often discard less relevant tokens, which can lead to significant performance degradation when critical information is lost. In this paper, we propose \textsc{PoD}~(Proximal tokens over Distant tokens), a novel KV cache compression framework that allocates memory according to token importance, retaining less important tokens in a more compact, shared form rather than discarding them entirely. Our approach is motivated by two key observations: (1) proximal tokens -- those at the beginning and end of the context -- are significantly more important for next-token prediction, and (2) attention scores for distant tokens are highly redundant across consecutive layers. Leveraging these insights, \textsc{PoD} preserves the full KV cache for proximal tokens, while for distant tokens, it shares key states across layers. Since attention scores are determined by both queries and keys, sharing key states enables multiple layers to reuse a single set of keys for distant tokens, substantially reducing KV cache memory without discarding essential context. We further introduce a lightweight post-training adaptation to enable the model to adjust to this new attention-sharing structure. Extensive experiments on both synthetic~(Needle in a Haystack) and real-world long-context benchmarks demonstrate that \textsc{PoD} reduces KV cache memory usage by up to 35\% without compromising performance. Our method is orthogonal to existing token-selection-based techniques and can be combined with them for further KV cache compression.

</details>


### [167] [Self-Evolving Critique Abilities in Large Language Models](https://arxiv.org/abs/2501.05727)

*Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, Junyang Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, critique abilities, self-generated data, contrasting-critic approach, self-validation scheme

**Relevance Score:** 9

**TL;DR:** This paper introduces SCRIT, a framework that enhances the critique abilities of Large Language Models (LLMs) using self-generated data and a contrastive-critic approach to improve quality and ensure continuous improvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better feedback mechanisms in LLMs for tasks where human evaluation is challenging, and to allow LLMs to identify and correct their own flaws without external supervision.

**Method:** The SCRIT framework trains LLMs with self-generated data, employing a contrastive-critic approach to improve understanding and a self-validation scheme for data quality, culminating in a model that performs inference without reference solutions.

**Key Contributions:**

	1. Introduction of SCRIT framework for self-evolving critique capabilities
	2. Implementation of contrastive-critic approach for better data quality
	3. Demonstrated improvements in both mathematical and scientific reasoning tasks.

**Result:** SCRIT shows significant performance improvements: 10.0% relative gain in critique-correction accuracy and a 19.0% improvement in error identification F1-score across diverse benchmarks.

**Limitations:** 

**Conclusion:** SCRIT enables LLMs to evolve their critique abilities independently, scaling positively with data and model size and supporting continuous performance enhancements through iterative training.

**Abstract:** Despite their remarkable performance, Large Language Models (LLMs) face a critical challenge: providing feedback for tasks where human evaluation is difficult or where LLMs potentially outperform humans. In such scenarios, leveraging the critique ability of LLMs themselves - identifying and correcting flaws - shows considerable promise. This paper explores enhancing critique abilities of LLMs, noting that current approaches rely on human annotations or more powerful models, leaving the challenge of improving critique abilities without external supervision unresolved. We introduce SCRIT (Self-evolving CRITic), a framework that trains LLMs with self-generated data to evolve their critique abilities. To address the low quality of naively generated data, we propose a contrastive-critic approach that uses reference solutions during data synthesis to enhance the model's understanding of key concepts, and incorporates a self-validation scheme to ensure data quality. The final trained model operates without any reference solutions at inference time. Implemented with Qwen2.5-72B-Instruct, a leading LLM, SCRIT demonstrates consistent improvements across a wide range of benchmarks spanning both mathematical and scientific reasoning: achieving a 10.0\% relative gain in critique-correction accuracy and a 19.0\% relative improvement in error identification F1-score. Our analysis reveals that SCRIT's performance scales positively with data and model size and enables continuous improvement through multi-round iterations.

</details>


### [168] [Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation](https://arxiv.org/abs/2502.13207)

*Giorgio Franceschelli, Mirco Musolesi*

**Main category:** cs.CL

**Keywords:** large language models, creativity, information theory, reinforcement learning, output evaluation

**Relevance Score:** 8

**TL;DR:** This paper proposes a context-based score for evaluating the originality and value of outputs from large language models (LLMs) in creative tasks, addressing the trade-off between diversity and quality in AI-generated content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to tackle the challenge of generating diverse and high-quality outputs from large language models when applied to creative tasks.

**Method:** The authors propose a context-based score rooted in information theory, which is used as a reward in a reinforcement learning framework to fine-tune LLMs.

**Key Contributions:**

	1. Introduction of a context-based score for evaluating LLM outputs in creativity tasks
	2. Integration of this score into a reinforcement learning framework
	3. Empirical validation across multiple creative tasks

**Result:** Experiments demonstrate that using the proposed score enhances the value and originality of outputs in various creative tasks, including poetry generation and math problem solving.

**Limitations:** 

**Conclusion:** The study concludes that the context-based scoring mechanism effectively improves the performance of LLMs by encouraging diversity without sacrificing quality.

**Abstract:** Despite the increasing use of large language models for creative tasks, their outputs often lack diversity. Common solutions, such as sampling at higher temperatures, can compromise the quality of the results. Dealing with this trade-off is still an open challenge in designing AI systems for creativity. Drawing on information theory, we propose a context-based score to quantitatively evaluate value and originality. This score incentivizes accuracy and adherence to the request while fostering divergence from the learned distribution. We show that our score can be used as a reward in a reinforcement learning framework to fine-tune large language models for maximum performance. We validate our strategy through experiments considering a variety of creative tasks, such as poetry generation and math problem solving, demonstrating that it enhances the value and originality of the generated solutions.

</details>


### [169] [Towards Question Answering over Large Semi-structured Tables](https://arxiv.org/abs/2502.13422)

*Yuxiang Wang, Junhao Gan, Jianzhong Qi*

**Main category:** cs.CL

**Keywords:** Table Question Answering, large tables, decomposition, LLM, QA benchmarks

**Relevance Score:** 6

**TL;DR:** TaDRe is a Table Question Answering model that improves analysis of large tables through enhanced decomposition techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The prevalence of web information in semi-structured tables and the challenges of extracting accurate answers from large tables.

**Method:** TaDRe integrates pre- and post-table decomposition refinements to enhance the quality of table decomposition for improved QA accuracy.

**Key Contributions:**

	1. Introduction of pre- and post-decomposition refinements in TableQA
	2. Construction of new large-table TableQA benchmarks
	3. Achievement of state-of-the-art performance in TableQA tasks

**Result:** TaDRe demonstrates state-of-the-art performance on large-table TableQA tasks through extensive experiments on newly constructed benchmarks.

**Limitations:** 

**Conclusion:** TaDRe offers an effective solution to enhance the decomposing of large tables, leading to better question answering results.

**Abstract:** Table Question Answering (TableQA) attracts strong interests due to the prevalence of web information presented in the form of semi-structured tables. Despite many efforts, TableQA over large tables remains an open challenge. This is because large tables may overwhelm models that try to comprehend them in full to locate question answers. Recent studies reduce input table size by decomposing tables into smaller, question-relevant sub-tables via generating programs to parse the tables. However, such solutions are subject to program generation and execution errors and are difficult to ensure decomposition quality. To address this issue, we propose TaDRe, a TableQA model that incorporates both pre- and post-table decomposition refinements to ensure table decomposition quality, hence achieving highly accurate TableQA results. To evaluate TaDRe, we construct two new large-table TableQA benchmarks via LLM-driven table expansion and QA pair generation. Extensive experiments on both the new and public benchmarks show that TaDRe achieves state-of-the-art performance on large-table TableQA tasks.

</details>


### [170] [Control Illusion: The Failure of Instruction Hierarchies in Large Language Models](https://arxiv.org/abs/2502.15851)

*Yilin Geng, Haonan Li, Honglin Mu, Xudong Han, Timothy Baldwin, Omri Abend, Eduard Hovy, Lea Frermann*

**Main category:** cs.CL

**Keywords:** large language models, instruction hierarchy, constraint prioritization, social hierarchies, model bias

**Relevance Score:** 8

**TL;DR:** This paper evaluates how hierarchical instructions are prioritized in large language models (LLMs) and finds models struggle with consistent prioritization, primarily influenced by inherent biases and social hierarchies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how effectively LLMs handle hierarchical instruction schemes, particularly in enforcing instruction prioritization.

**Method:** A systematic evaluation framework based on constraint prioritization was introduced to assess the instruction hierarchy enforcement of six state-of-the-art LLMs.

**Key Contributions:**

	1. Introduction of a systematic evaluation framework for LLM instruction hierarchies.
	2. Findings on the inconsistency of instruction prioritization in LLMs.
	3. Insights into the influence of social hierarchies over system/user roles in LLMs.

**Result:** Experiments showed that LLMs struggle with consistent instruction prioritization, with biases towards certain constraint types; the common system/user prompt separation is ineffective in establishing a reliable hierarchy.

**Limitations:** The study is limited to six state-of-the-art LLMs and may not generalize to all LLM architectures or instruction formats.

**Conclusion:** LLMs are more likely to obey constraints framed through natural social hierarchies than through system/user roles, indicating that pretraining social structures influence model behavior more than post-training adjustments.

**Abstract:** Large language models (LLMs) are increasingly deployed with hierarchical instruction schemes, where certain instructions (e.g., system-level directives) are expected to take precedence over others (e.g., user messages). Yet, we lack a systematic understanding of how effectively these hierarchical control mechanisms work. We introduce a systematic evaluation framework based on constraint prioritization to assess how well LLMs enforce instruction hierarchies. Our experiments across six state-of-the-art LLMs reveal that models struggle with consistent instruction prioritization, even for simple formatting conflicts. We find that the widely-adopted system/user prompt separation fails to establish a reliable instruction hierarchy, and models exhibit strong inherent biases toward certain constraint types regardless of their priority designation. We find that LLMs more reliably obey constraints framed through natural social hierarchies (e.g., authority, expertise, consensus) than system/user roles, which suggests that pretraining-derived social structures act as latent control priors, with potentially stronger influence than post-training guardrails.

</details>


### [171] [What are Foundation Models Cooking in the Post-Soviet World?](https://arxiv.org/abs/2502.18583)

*Anton Lavrouk, Tarek Naous, Alan Ritter, Wei Xu*

**Main category:** cs.CL

**Keywords:** Post-Soviet, food knowledge, multimodal dataset, cultural understanding, foundation models

**Relevance Score:** 3

**TL;DR:** This study investigates Post-Soviet cultural food knowledge of foundation models using a multimodal dataset called BORSch to assess origin identification of dishes.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze how well foundation models can understand and identify the origins of dishes from Post-Soviet cultures, which reflects broader cultural knowledge.

**Method:** A multimodal dataset (BORSch) was constructed with 1147 Russian and 823 Ukrainian dishes. The performance of leading models was evaluated in text-only and multimodal Question Answering tasks, as well as in generating visual descriptions.

**Key Contributions:**

	1. Introduction of the BORSch dataset for Post-Soviet food knowledge
	2. Demonstration of leading models' shortcomings in cultural origin identification
	3. Proposing visual description as a complement to QA for evaluating cultural understanding

**Result:** Models exhibited poor identification of dish origins, often mispredicting based on the question's language. Analysis of pretraining data suggested misleading co-occurrences and linguistic features were factors in these poor performances.

**Limitations:** The study primarily focuses on the Russian and Ukrainian languages and may not represent the full diversity of Post-Soviet cultures.

**Conclusion:** QA may not be a sufficient sole measure for evaluating models' cultural understanding, indicating a need for more robust methods, such as visual description tasks.

**Abstract:** The culture of the Post-Soviet states is complex, shaped by a turbulent history that continues to influence current events. In this study, we investigate the Post-Soviet cultural food knowledge of foundation models by constructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes in the Russian and Ukrainian languages, centered around the Post-Soviet region. We demonstrate that leading models struggle to correctly identify the origins of dishes from Post-Soviet nations in both text-only and multimodal Question Answering (QA), instead over-predicting countries linked to the language the question is asked in. Through analysis of pretraining data, we show that these results can be explained by misleading dish-origin co-occurrences, along with linguistic phenomena such as Russian-Ukrainian code mixing. Finally, to move beyond QA-based assessments, we test models' abilities to produce accurate visual descriptions of dishes. The weak correlation between this task and QA suggests that QA alone may be insufficient as an evaluation of cultural understanding. To foster further research, we will make BORSch publicly available at https://github.com/alavrouk/BORSch.

</details>


### [172] [Predictive Data Selection: The Data That Predicts Is the Data That Teaches](https://arxiv.org/abs/2503.00808)

*Kashun Shum, Yuzhen Huang, Hongjian Zou, Qi Ding, Yixuan Liao, Xiaoxin Chen, Qian Liu, Junxian He*

**Main category:** cs.CL

**Keywords:** data selection, language models, pretraining, machine learning, computational efficiency

**Relevance Score:** 9

**TL;DR:** This paper introduces PreSelect, a method for selecting pretraining data based on its predictive power for downstream performance, achieving significant reductions in compute requirements.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To estimate the contribution of data during language model pretraining and improve data selection efficiency.

**Method:** The authors introduce PreSelect, a fastText-based scorer for predictive data selection, tested on language models with varying parameters.

**Key Contributions:**

	1. Introduction of PreSelect for efficient data selection
	2. Demonstration of significant compute reduction
	3. Open-sourcing of data selection scorer and datasets

**Result:** Models trained with PreSelect outperform baselines, achieving better performance with 10x less compute, indicating efficient data contributes effectively to learning.

**Limitations:** 

**Conclusion:** PreSelect is an effective method for data selection in pretraining, providing significant improvements in performance and efficiency.

**Abstract:** Language model pretraining involves training on extensive corpora, where data quality plays a pivotal role. In this work, we aim to directly estimate the contribution of data during pretraining and select pretraining data in an efficient manner. Specifically, we draw inspiration from recent findings showing that compression efficiency (i.e., the normalized loss) of diverse models on certain text correlates strongly with their downstream performance, when the text domain aligns with the downstream benchmarks(Huang et al., 2024). Building on this observation, we hypothesize that data on which model losses are predictive of downstream abilities also contribute effectively to learning, which shares similar intuition with Thrush et al.(2024). To leverage this insight, we introduce predictive data selection (PreSelect), a lightweight and efficient data selection method that requires training and deploying only a fastText-based scorer. Through comprehensive experiments with 1B and 3B parameter models, we demonstrate that models trained on 30B tokens selected with PreSelect surpass the performance of the vanilla baseline trained on 300B tokens, achieving a 10x reduction in compute requirements. Furthermore, PreSelect significantly outperforms other competitive data selection baselines, such as DCLM and FineWeb-Edu on a scale of 3B models trained on 100B tokens. We open-source our trained data selection scorer along with the curated datasets at https://github.com/hkust-nlp/PreSelect.

</details>


### [173] [Efficient Dynamic Clustering-Based Document Compression for Retrieval-Augmented-Generation](https://arxiv.org/abs/2504.03165)

*Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Document Compression, Knowledge Injection

**Relevance Score:** 9

**TL;DR:** Proposes EDC2-RAG, a framework to enhance RAG by leveraging inter-document relationships for improved document compression and reducing noise in LLM generations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of current RAG implementations that struggle with noise and redundancy affecting generation quality.

**Method:** Introduces an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG) that uses latent inter-document relationships to filter out irrelevant and redundant information.

**Key Contributions:**

	1. Development of the EDC2-RAG framework
	2. Utilization of latent inter-document relationships for compression
	3. Empirical validation through diverse datasets

**Result:** Achieves consistent performance improvements on knowledge-QA and Hallucination-Detection datasets, indicating strong robustness and applicability.

**Limitations:** 

**Conclusion:** The proposed EDC2-RAG framework shows significant advancements in handling noise and enhancing generation accuracy in LLMs during inference.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge injection during large language model (LLM) inference in recent years. However, due to their limited ability to exploit fine-grained inter-document relationships, current RAG implementations face challenges in effectively addressing the retrieved noise and redundancy content, which may cause error in the generation results. To address these limitations, we propose an Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG) that utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and Hallucination-Detection datasets. Experimental results show that our method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.

</details>


### [174] [Enhancing Time Series Forecasting via Multi-Level Text Alignment with LLMs](https://arxiv.org/abs/2504.07360)

*Taibiao Zhao, Xiaobing Chen, Mingxuan Sun*

**Main category:** cs.CL

**Keywords:** time series forecasting, large language models, interpretability, multi-level alignment, text representations

**Relevance Score:** 9

**TL;DR:** This paper presents a multi-level text alignment framework for time series forecasting using large language models (LLMs) to improve both prediction accuracy and interpretability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to effectively align time series data with language-based representations is essential for enhancing predictive accuracy and interpretability in forecasting tasks.

**Method:** The proposed framework decomposes time series data into trend, seasonal, and residual components and creates component-specific text representations through a multi-level alignment mechanism with pre-trained word tokens.

**Key Contributions:**

	1. Introduces a multi-level text alignment framework for time series forecasting using LLMs.
	2. Decomposes time series data into interpretable components for improved forecasting.
	3. Demonstrates superior performance in accuracy and interpretability over existing models.

**Result:** Experiments show that the proposed method outperforms state-of-the-art forecasting models in both accuracy and interpretability across multiple datasets.

**Limitations:** 

**Conclusion:** The multi-level text alignment framework effectively bridges the gap between continuous time series data and discrete LLM representations, yielding significant improvements in forecasting.

**Abstract:** The adaptation of large language models (LLMs) to time series forecasting poses unique challenges, as time series data is continuous in nature, while LLMs operate on discrete tokens. Despite the success of LLMs in natural language processing (NLP) and other structured domains, aligning time series data with language-based representations while maintaining both predictive accuracy and interpretability remains a significant hurdle. Existing methods have attempted to reprogram time series data into text-based forms, but these often fall short in delivering meaningful, interpretable results. In this paper, we propose a multi-level text alignment framework for time series forecasting using LLMs that not only improves prediction accuracy but also enhances the interpretability of time series representations. Our method decomposes time series into trend, seasonal, and residual components, which are then reprogrammed into component-specific text representations. We introduce a multi-level alignment mechanism, where component-specific embeddings are aligned with pre-trained word tokens, enabling more interpretable forecasts. Experiments on multiple datasets demonstrate that our method outperforms state-of-the-art models in accuracy while providing good interpretability.

</details>


### [175] [Thought Manipulation: External Thought Can Be Efficient for Large Reasoning Models](https://arxiv.org/abs/2504.13626)

*Yule Liu, Jingyi Zheng, Zhen Sun, Zifan Peng, Wenhan Dong, Zeyang Sha, Shiwen Cui, Weiqiang Wang, Xinlei He*

**Main category:** cs.CL

**Keywords:** large reasoning models, chains of thought, optimization, computational efficiency, machine learning

**Relevance Score:** 7

**TL;DR:** The paper presents a method to optimize large reasoning models (LRMs) by reducing redundant reasoning steps using external chains of thought (CoTs) generated by smaller models, leading to significant computational savings without sacrificing performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the 'overthinking' problem in large reasoning models, which results in excessive redundancy and limited performance gains, while aiming to improve efficiency and accessibility of these models for real-world applications.

**Method:** The proposed method, named \\Method, integrates external CoTs between thinking tokens to manipulate LRM behavior, enabling models to bypass unnecessary steps and reduce output token counts while maintaining performance.

**Key Contributions:**

	1. Introduction of a method to place external CoTs within LRMs to optimize reasoning processes.
	2. Significant reduction of output tokens without performance loss, enhancing efficiency for LRM applications.
	3. Identification of two suboptimal modes in LRM behavior and proposed solutions to mitigate these issues.

**Result:** Experimental results show that \\Method reduces output token counts by approximately 30% when applied to the QwQ-32B model on the LiveBench/Code dataset, with minimal overhead from the CoT generator.

**Limitations:** No specific limitations mentioned in the abstract.

**Conclusion:** The findings demonstrate that \\Method is an efficient approach to optimize LRM inference and improve the application of reasoning models in various tasks.

**Abstract:** Recent advancements in large reasoning models (LRMs) have demonstrated the effectiveness of scaling test-time computation to enhance reasoning capabilities on various tasks. However, LRMs often suffer from an ``overthinking'' problem, where the model generates excessively redundant reasoning steps with limited performance gains. In this work, we empirically reveal an important characteristic of LRM behaviors that placing external CoTs generated by smaller models between the thinking token (\texttt{<think>} and \texttt{</think>}) can effectively manipulate the model to generate fewer thoughts. Building on this finding, we propose a simple yet efficient pipeline, \Method, to enable LRMs to bypass unnecessary intermediate steps, thereby significantly reducing computational costs. We conduct extensive experiments to evaluate the utility and efficiency of \Method. For instance, when applied to QwQ-32B on the LiveBench/Code dataset, \Method keeps the original performance while reducing output token counts by approximately 30\%, with minimal overhead introduced by the CoT generator. Furthermore, we identify two suboptimal modes, blindly following flawed external thoughts and unnecessary rethinking, and show that simple mitigations, such as difficulty-aware fallbacks, can further improve performance. Overall, \Method offers a practical, general, and efficient way to optimize LRM inference, making powerful reasoning models more accessible and scalable for real-world applications.

</details>
