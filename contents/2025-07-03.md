# 2025-07-03

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 17]

- [cs.CL](#cs.CL) [Total: 49]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)

*Megan T. deBettencourt, Sruthi Sakthivel, Emily A. Holmes, Mark Chevillet*

**Main category:** cs.HC

**Keywords:** AI-guided therapy, pupillometry, trauma treatment, digital intervention, evidence-based

**Relevance Score:** 8

**TL;DR:** This study tests ANTIDOTE, an AI-powered digital treatment for reducing intrusive memories following trauma, showing promising results in engagement and effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore scalable alternatives to human-guided trauma therapies using generative AI and neurotechnology.

**Method:** The study involved 100 healthy volunteers exposed to traumatic videos, with participants assigned to either an AI-guided intervention or an active control condition.

**Key Contributions:**

	1. Introduces ANTIDOTE, a novel AI-guided intervention for trauma.
	2. Demonstrates the use of pupillometry as a biomarker for engagement and symptom reduction.
	3. Confirms the successful delivery of trauma treatment through AI without human guidance.

**Result:** Participants receiving the AI-guided intervention reported significantly fewer intrusive memories over the following week, with pupil size tracking engagement and predicting symptom reduction.

**Limitations:** 

**Conclusion:** The findings suggest that AI-guided digital interventions may be effective and scalable solutions for trauma treatment.

**Abstract:** Trauma prevalence is vast globally. Evidence-based digital treatments can help, but most require human guidance. Human guides provide tailored instructions and responsiveness to internal cognitive states, but limit scalability. Can generative AI and neurotechnology provide a scalable alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to automatically deliver and monitor an evidence-based digital treatment, specifically the Imagery Competing Task Intervention (ICTI), to reduce intrusive memories after psychological trauma. One hundred healthy volunteers were exposed to videos of traumatic events and randomly assigned to an intervention or active control condition. As predicted, intervention participants reported significantly fewer intrusive memories over the following week. Post-hoc assessment against clinical rubrics confirmed the AI guide delivered the intervention successfully. Additionally, pupil size tracked intervention engagement and predicted symptom reduction, providing a candidate biomarker of intervention effectiveness. These findings open a path toward rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [2] [From Literature to ReWA: Discussing Reproductive Well-being in HCI](https://arxiv.org/abs/2507.01121)

*Hafsah Mahzabin Chowdhury, Sharifa Sultana*

**Main category:** cs.HC

**Keywords:** reproductive well-being, human-computer interaction, cultural sensitivity, AI integration, framework

**Relevance Score:** 5

**TL;DR:** This literature review synthesizes findings from 147 papers to explore the intersection of technology and reproductive well-being, highlighting inclusivity gaps and proposing a framework for design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the narrow, Western-centric assumptions in current reproductive well-being technologies and synthesize diverse perspectives in the intersection of culture, gender, and technology.

**Method:** Literature review of 147 peer-reviewed papers across various fields, analyzing themes in technology and reproductive well-being.

**Key Contributions:**

	1. Synthesis of diverse literature on reproductive well-being and technology.
	2. Identification of critical inclusivity gaps in current technological approaches.
	3. Introduction of the ReWA framework for a more inclusive design process.

**Result:** Identified three thematic waves: early access and education, cultural sensitivity and privacy, and AI integration with policy-aware design. Highlighted gaps in inclusivity for various user groups, including men, non-binary users, and migrants.

**Limitations:** Limited focus on the role of specific stakeholders in reproductive well-being; potential bias in the geographical representation of studies.

**Conclusion:** The proposed ReWA framework provides six design orientations to enhance reproductive well-being across diverse contexts, emphasizing inclusivity and stakeholder involvement.

**Abstract:** Reproductive well-being is shaped by intersecting cultural, religious, gendered, and political contexts, yet current technologies often reflect narrow, Western-centric assumptions. In this literature review, we synthesize findings from 147 peer-reviewed papers published between 2015 and 2025 across HCI, CSCW and social computing, ICTD, digital and public health, and AI for well-being scholarship to map the evolving reproductive well-being landscape. We identify three thematic waves that focused on early access and education, cultural sensitivity and privacy, and AI integration with policy-aware design, and highlight how technologies support or constrain diverse reproductive experiences. Our analysis reveals critical gaps in inclusivity, with persistent exclusions of men and non-binary users, migrants, and users in the Global South. Additionally, we surfaced the significant absence of literature on the role of stakeholders (e.g., husband and family members, household maids and cleaning helping hands, midwife, etc.) in the reproductive well-being space. Drawing on the findings from the literature, we propose the ReWA framework to support reproductive well-being for all agendas through six design orientations associated with: location, culture, and history; polyvocality and agency; rationality, temporality, distributive roles, and methodology.

</details>


### [3] [Animated Visual Encoding and Layer Blending for Identification of Educational Game Strategies](https://arxiv.org/abs/2507.01134)

*Braden Roper, William Thompson, Chris Weaver*

**Main category:** cs.HC

**Keywords:** Game-Based Learning, visualization, kinetic visualization, animated data narratives, player strategies

**Relevance Score:** 4

**TL;DR:** The paper introduces an animated visual encoding tool that utilizes kinetic visualization to improve understanding of player strategies in Game-Based Learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for better tools to interpret and visualize player strategy data in Game-Based Learning contexts.

**Method:** The proposed tool employs kinetic visualization techniques that allow researchers to create animated data narratives using parameter interpolation curves and blending layers.

**Key Contributions:**

	1. Introduction of a kinetic visualization tool for Game-Based Learning
	2. Capability to create animated data narratives through parameter interpolation
	3. Addressing challenges of visualizing noisy and complex data from gameplay

**Result:** The tool effectively addresses visualization challenges such as overplotting and noise in data collected from game-state and action tracking.

**Limitations:** The effectiveness of the tool may vary depending on the specific use case and data characteristics.

**Conclusion:** This animated visual encoding tool demonstrates enhanced interpretability of player strategies, facilitating deeper insights for researchers.

**Abstract:** Game-Based Learning has proven to be an effective method for enhancing engagement with educational material. However, gaining a deeper understanding of player strategies remains challenging. Sequential game-state and action-based tracking tools often gather extensive data that can be difficult to interpret as long-term strategy. This data presents unique problems to visualization, as it can be fairly natural, noisy data but is constrained within synthetic, controlled environments, leading to issues such as overplotting which can make interpretation complicated. We propose an animated visual encoding tool that utilizes kinetic visualization to address these issues. This tool enables researchers to construct animated data narratives through the configuration of parameter interpolation curves and blending layers. Finally, we demonstrate the usefulness of the tool while addressing specific interests as outlined by a domain expert collaborator.

</details>


### [4] [A Methodological Framework for Capturing Cognitive-Affective States in Collaborative Learning](https://arxiv.org/abs/2507.01166)

*Sifatul Anindho, Videep Venkatesha, Nathaniel Blanchard*

**Main category:** cs.HC

**Keywords:** cognitive-affective states, collaborative learning, adaptive learning systems

**Relevance Score:** 6

**TL;DR:** Analyzes how to identify cognitive-affective states in collaborative learning using participant reports and videos.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To track cognitive-affective states without disrupting group collaboration and improve adaptive learning systems.

**Method:** Used a retrospect cued recall paradigm and pre-identified cognitive-affective states to gather participant reports.

**Key Contributions:**

	1. Initial analysis of cognitive-affective state reporting
	2. Comparison of self-reports and probe responses
	3. Implications for adaptive learning systems

**Result:** Discovery of frequency and temporal changes in participant reports across two collection methods.

**Limitations:** 

**Conclusion:** Improves techniques for monitoring cognitive-affective states in collaborative learning, informing educational data mining and adaptive systems.

**Abstract:** Identification of affective and attentional states of individuals within groups is difficult to obtain without disrupting the natural flow of collaboration. Recent work from our group used a retrospect cued recall paradigm where participants spoke about their cognitive-affective states while they viewed videos of their groups. We then collected additional participants where their reports were constrained to a subset of pre-identified cognitive-affective states. In this latter case, participants either self reported or reported in response to probes. Here, we present an initial analysis of the frequency and temporal distribution of participant reports, and how the distributions of labels changed across the two collections. Our approach has implications for the educational data mining community in tracking cognitive-affective states in collaborative learning more effectively and in developing improved adaptive learning systems that can detect and respond to cognitive-affective states.

</details>


### [5] [Judgment as Coordination: A Joint Systems View of Visualization Design Practice](https://arxiv.org/abs/2507.01209)

*Paul C. Parsons, Arran Ridley*

**Main category:** cs.HC

**Keywords:** visualization design, design judgment, Joint Cognitive Systems

**Relevance Score:** 4

**TL;DR:** The paper reframes professional visualization design by emphasizing systems-level dynamics in design judgment, focusing on collaboration and adaptation in practice.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To shift the discourse in visualization design from researcher-centered to a systems-level understanding that incorporates collaborative and systemic dimensions.

**Method:** The study involved ethnographic observations of design teams and qualitative studies with individual practitioners to explore design judgment at a systems level.

**Key Contributions:**

	1. Proposal of a systems-level reframing of design judgment
	2. Insights into collaborative dynamics in design practice
	3. Introduction of Joint Cognitive Systems as a framework for analysis

**Result:** The analysis revealed that coherence in design work is often maintained through repairing alignment and adjusting plans rather than by choosing optimal options.

**Limitations:** 

**Conclusion:** This systems perspective highlights hidden aspects of design work and provides a conceptual vocabulary for better understanding design activities in practice.

**Abstract:** Professional visualization design has become an increasingly important area of inquiry, yet much of the field's discourse remains anchored in researcher-centered contexts. Studies of design practice often focus on individual designers' decisions and reflections, offering limited insight into the collaborative and systemic dimensions of professional work. In this paper, we propose a systems-level reframing of design judgment grounded in the coordination and adaptation that sustain progress amid uncertainty, constraint, and misalignment. Drawing on sustained engagement across multiple empirical studies--including ethnographic observation of design teams and qualitative studies of individual practitioners--we identify recurring episodes in which coherence was preserved not by selecting an optimal option, but by repairing alignment, adjusting plans, and reframing goals. We interpret these dynamics through the lens of Joint Cognitive Systems, which provide tools for analyzing how judgment emerges as a distributed capacity within sociotechnical activity. This perspective surfaces often-invisible work in visualization design and offers researchers a new conceptual vocabulary for studying how design activity is sustained in practice.

</details>


### [6] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)

*Vishakha Lall, Yisi Liu*

**Main category:** cs.HC

**Keywords:** maritime training, AI, performance assessment, speech recognition, stress detection

**Relevance Score:** 3

**TL;DR:** An AI-driven framework enhances maritime training by effectively assessing trainee performance through objective measures like visual focus tracking and speech recognition, thus improving readiness for high-risk scenarios.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses subjectivity and measurement challenges in traditional maritime training methods that rely on trainer assessments.

**Method:** The framework utilizes AI techniques for visual focus determination, speech recognition specific to maritime contexts, and stress detection through vocal analysis.

**Key Contributions:**

	1. Development of an AI framework for objective performance assessment in maritime training
	2. Integration of eye tracking and speech recognition tailored for maritime scenarios
	3. Demonstration of high accuracy in stress detection and performance metrics

**Result:** The AI algorithms achieved approximately 92% accuracy in visual detection, 91% in maritime speech recognition, and 90% in stress detection, outperforming current benchmarks.

**Limitations:** 

**Conclusion:** AI can significantly improve maritime training by providing objective performance analytics and facilitating personalized feedback, enhancing preparedness for operational challenges.

**Abstract:** Traditional simulator-based training for maritime professionals is critical for ensuring safety at sea but often depends on subjective trainer assessments of technical skills, behavioral focus, communication, and body language, posing challenges such as subjectivity, difficulty in measuring key features, and cognitive limitations. Addressing these issues, this study develops an AI-driven framework to enhance maritime training by objectively assessing trainee performance through visual focus tracking, speech recognition, and stress detection, improving readiness for high-risk scenarios. The system integrates AI techniques, including visual focus determination using eye tracking, pupil dilation analysis, and computer vision; communication analysis through a maritime-specific speech-to-text model and natural language processing; communication correctness using large language models; and mental stress detection via vocal pitch. Models were evaluated on data from simulated maritime scenarios with seafarers exposed to controlled high-stress events. The AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for maritime speech recognition, and ~90% for stress detection, surpassing existing benchmarks. The system provides insights into visual attention, adherence to communication checklists, and stress levels under demanding conditions. This study demonstrates how AI can transform maritime training by delivering objective performance analytics, enabling personalized feedback, and improving preparedness for real-world operational challenges.

</details>


### [7] [Challenges & Opportunities with LLM-Assisted Visualization Retargeting](https://arxiv.org/abs/2507.01436)

*Luke S. Snyder, Chenglong Wang, Steven Drucker*

**Main category:** cs.HC

**Keywords:** visualization, Large Language Models, retargeting, program synthesis, chart adaptation

**Relevance Score:** 8

**TL;DR:** This paper investigates how Large Language Models (LLMs) can assist in adapting visualization code to new datasets, comparing two approaches in effectiveness and identifying limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in retargeting existing custom chart implementations for new datasets, which is often difficult and time-consuming for users.

**Method:** The authors evaluate LLM assistance performance across various datasets and chart complexities, comparing direct instruction of LLMs with a constrained program synthesis approach.

**Key Contributions:**

	1. Characterization of LLM performance in visualization retargeting.
	2. Comparison of direct LLM instruction vs. program synthesis pipeline.
	3. Design recommendations for improving LLM-assisted visualization retargeting.

**Result:** The evaluation reveals that both approaches face challenges if the new data is not properly transformed, highlighting their limitations.

**Limitations:** Struggles with untransformed new data when adapting existing visualization examples.

**Conclusion:** The findings lead to design recommendations for future retargeting systems to enhance their effectiveness in utilizing LLMs for visualization adaptation.

**Abstract:** Despite the ubiquity of visualization examples published on the web, retargeting existing custom chart implementations to new datasets remains difficult, time-intensive, and tedious. The adaptation process assumes author familiarity with both the implementation of the example as well as how the new dataset might need to be transformed to fit into the example code. With recent advances in Large Language Models (LLMs), automatic adaptation of code can be achieved from high-level user prompts, reducing the barrier for visualization retargeting. To better understand how LLMs can assist retargeting and its potential limitations, we characterize and evaluate the performance of LLM assistance across multiple datasets and charts of varying complexity, categorizing failures according to type and severity. In our evaluation, we compare two approaches: (1) directly instructing the LLM model to fully generate and adapt code by treating code as text inputs and (2) a more constrained program synthesis pipeline where the LLM guides the code construction process by providing structural information (e.g., visual encodings) based on properties of the example code and data. We find that both approaches struggle when new data has not been appropriately transformed, and discuss important design recommendations for future retargeting systems.

</details>


### [8] [Analysis of Drone-Assisted Building Inspection Training in VR vs 2D Monitor Display: an EEG Study](https://arxiv.org/abs/2507.01471)

*Pengkun Liu, Jackson Greene, Jiali Huang, Pingbo Tang, Yu Hou*

**Main category:** cs.HC

**Keywords:** drone-assisted inspection, brain connectivity, tacit knowledge transfer, cognitive states, adaptive automation

**Relevance Score:** 4

**TL;DR:** The study investigates brain connectivity and its impact on drone-assisted inspection training, revealing patterns that could enhance knowledge transfer in such tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover how information processing in specific brain regions affects performance in drone-assisted inspection and the efficiency of tacit knowledge transfer.

**Method:** Electroencephalogram (EEG) and dynamic causal modeling were used to analyze brain activity while participants performed drone-assisted energy audit tasks under different display modalities.

**Key Contributions:**

	1. Revealed causal connections between brain regions during drone task performance.
	2. Identified consistent brain connectivity patterns associated with visual inspection.
	3. Provided insights for designing adaptive automation in drone-based inspections.

**Result:** The study identified similar single-direction connectivity patterns across different simulation groups, with consistent brain activity linked to visual inspection performance before and after training.

**Limitations:** 

**Conclusion:** The findings suggest that understanding brain connectivity can inform the design of adaptive automation for improving knowledge transfer in drone inspections.

**Abstract:** Researchers have been using simulation-based methods for drone-assisted inspection training. Multiple brain regions are associated with information processes and decision-making, and the connectivity of these regions may further influence inspectors' performance. However, researchers do not understand the pathways of the information flows when drone pilots process the maintenance and manipulation of information, which may affect the efficiency of tacit knowledge transfer. This study aims to reveal the causal connection between participants' brain regions using an electroencephalogram and dynamic causal modeling when processing drone-assisted building energy audit tasks using different display modalities. The results showed similar single-direction connectivity patterns for the different simulation groups. The results also showed similar patterns between brain regions related to visual inspection performance before and after training. These findings highlight the nature of brain asymmetries and may be utilized in measuring cognitive states and designing adaptive automation in the knowledge transfer of drone-based inspection.

</details>


### [9] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)

*Wen Zhan, Ziqun Hua, Peiyue Lin, Yunfei Chen*

**Main category:** cs.HC

**Keywords:** AI-assisted co-creation, aging migrants, narrative expression, Hanzi, human-AI collaboration

**Relevance Score:** 7

**TL;DR:** This paper examines AI-assisted co-creation for older adults, focusing on aging migrants in urban China, to help them express fragmented personal narratives through oral storytelling and symbolic reconstruction of Hanzi using LLM-supported techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the underrepresentation of aging migrants' narratives and the challenges they face in verbalizing their experiences.

**Method:** The research involved a pilot workshop where participants engaged in oral storytelling and symbolic reconstruction of Hanzi with AI assistance, utilizing physical materials and facilitated human interaction.

**Key Contributions:**

	1. Introduces a novel approach to narrative expression for aging migrants using AI
	2. Reconceptualizes AI's role from content producer to supportive mechanism
	3. Demonstrates the feasibility of engaging older adults in AI-assisted creativity without digital literacy

**Result:** Participants successfully transformed their lived experiences into visual and tactile forms, demonstrating how AI can enhance narrative expression without requiring digital literacy.

**Limitations:** The study is limited to a specific demographic (aging migrants in urban China) and may not generalize to other populations or contexts.

**Conclusion:** The findings suggest that AI can function as a supportive mechanism in human-AI collaborations, enhancing the narrative agency of older adults within sociotechnical systems.

**Abstract:** This paper explores how older adults, particularly aging migrants in urban China, can engage AI-assisted co-creation to express personal narratives that are often fragmented, underrepresented, or difficult to verbalize. Through a pilot workshop combining oral storytelling and the symbolic reconstruction of Hanzi, participants shared memories of migration and recreated new character forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM), together with physical materials. Supported by human facilitation and a soft AI presence, participants transformed lived experience into visual and tactile expressions without requiring digital literacy. This approach offers new perspectives on human-AI collaboration and aging by repositioning AI not as a content producer but as a supportive mechanism, and by supporting narrative agency within sociotechnical systems.

</details>


### [10] [Designing for Community Care: Reimagining Support for Equity & Well-being in Academia](https://arxiv.org/abs/2507.01690)

*Beatriz Severes, Ana O. Henriques, Rory Clark, Paulo Bala, Anna Carter, Rua Mae Williams, Geraldine Fitzpatrick*

**Main category:** cs.HC

**Keywords:** HCI, peer-support networks, care ethics, academic well-being, participatory design

**Relevance Score:** 7

**TL;DR:** This paper discusses the importance of formalizing peer-support networks in academic settings to enhance well-being through inclusivity and care ethics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the shortcomings of current peer-support networks in academic institutions, which are often informal and inequitable, impacting both students and faculty.

**Method:** The approach adopted includes HCI methodologies, participatory design, and care ethics, focusing on co-design activities and reflections among participants.

**Key Contributions:**

	1. Rethinking peer-support networks for faculty and staff inclusion
	2. Developing equitable support system resources
	3. Co-designing strategies to embed care in academic ecosystems.

**Result:** Participants will identify systemic gaps in existing support networks and develop strategies to create structured, inclusive care-based ecosystems.

**Limitations:** 

**Conclusion:** The workshop aims to foster a supportive academic community by integrating care and resilience into peer-support frameworks.

**Abstract:** Academic well-being is deeply influenced by peer-support networks, yet they remain informal, inequitable, and unsustainable, often relying on personal connections and social capital rather than structured, inclusive systems. Additionally, institutional well-being responses frequently focus on student populations, neglecting the emotional labour of faculty and staff, reinforcing an exclusionary academic culture. Drawing on HCI methodologies, participatory design, and care ethics, this workshop will provide a space for rethinking how academic communities can support inclusive networks. Through pre-workshop engagement, co-design activities, and reflection, participants will examine systemic gaps in networks and explore ways to embed care, equity, and sustainability into academic peer-support frameworks -- from informal, exclusionary models to structured, inclusive care-based ecosystems. At the end of the workshop, participants will co-develop design strategies for integrating care and resilience in academic ecosystems, resources for designing equitable support systems, and a peer network invested and committed to fostering a supportive academic community.

</details>


### [11] [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](https://arxiv.org/abs/2507.01719)

*Dorian Peters, Fernanda Espinoza, Marco da Re, Guido Ivetta, Luciana Benotti, Rafael A. Calvo*

**Main category:** cs.HC

**Keywords:** Conversational AI, Cultural diversity, Health informatics, Latin America, Participatory workshops

**Relevance Score:** 9

**TL;DR:** This paper explores the need for culturally and linguistically responsive conversational AI in health contexts, particularly in Latin America, proposing a framework for culturally appropriate CAI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of inclusivity in current LLMs regarding diverse cultural experiences, specifically in health applications in Latin America.

**Method:** The study uses qualitative data obtained through participatory workshops conducted in Latin America to understand cultural misalignments and local perspectives on health chatbots.

**Key Contributions:**

	1. Introduces a framework for culturally appropriate conversational AI for health
	2. Highlights the importance of local data and community perspectives
	3. Challenges the conventional academic views on culture in technology

**Result:** Findings suggest that traditional academic notions of culture are inadequate at the local level, highlighting the need for a broader framework that considers economics, politics, geography, and logistics in cultural experiences.

**Limitations:** 

**Conclusion:** The proposed 'Pluriversal Conversational AI for Health' framework emphasizes relationality and tolerance rather than merely increasing training data for CAI.

**Abstract:** There is justifiable interest in leveraging conversational AI (CAI) for health across the majority world, but to be effective, CAI must respond appropriately within culturally and linguistically diverse contexts. Therefore, we need ways to address the fact that current LLMs exclude many lived experiences globally. Various advances are underway which focus on top-down approaches and increasing training data. In this paper, we aim to complement these with a bottom-up locally-grounded approach based on qualitative data collected during participatory workshops in Latin America. Our goal is to construct a rich and human-centred understanding of: a) potential areas of cultural misalignment in digital health; b) regional perspectives on chatbots for health and c)strategies for creating culturally-appropriate CAI; with a focus on the understudied Latin American context. Our findings show that academic boundaries on notions of culture lose meaning at the ground level and technologies will need to engage with a broader framework; one that encapsulates the way economics, politics, geography and local logistics are entangled in cultural experience. To this end, we introduce a framework for 'Pluriversal Conversational AI for Health' which allows for the possibility that more relationality and tolerance, rather than just more data, may be called for.

</details>


### [12] [Human-Machine Collaboration-Guided Space Design: Combination of Machine Learning Models and Humanistic Design Concepts](https://arxiv.org/abs/2507.01776)

*Yuxuan Yang*

**Main category:** cs.HC

**Keywords:** Machine Learning, Human-Machine Collaboration, Spatial Design, User Experience, Cultural Relevance

**Relevance Score:** 7

**TL;DR:** This paper proposes a human-machine collaboration framework that integrates machine learning (ML) with human creativity in spatial design to balance efficiency and emotional impact.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize space utilization and enhance design processes while ensuring emotional, cultural, and aesthetic dimensions are preserved in design.

**Method:** The paper discusses the integration of various ML models with human-centered design principles, utilizing case studies in office and residential design.

**Key Contributions:**

	1. Proposes a human-machine collaboration framework for spatial design.
	2. Illustrates the integration of ML models with human-centered design principles.
	3. Provides case studies showcasing the effectiveness of the framework.

**Result:** The framework demonstrates that combining ML-generated solutions with human intuition and cultural insight leads to more emotionally engaging and relevant designs.

**Limitations:** 

**Conclusion:** Merging ML with human creativity can achieve a balance of efficiency and emotional resonance in spatial design.

**Abstract:** The integration of machine learning (ML) into spatial design holds immense potential for optimizing space utilization, enhancing functionality, and streamlining design processes. ML can automate tasks, predict performance outcomes, and tailor spaces to user preferences. However, the emotional, cultural, and aesthetic dimensions of design remain crucial for creating spaces that truly resonate with users-elements that ML alone cannot address. The key challenge lies in harmonizing data-driven efficiency with the nuanced, subjective aspects of design. This paper proposes a human-machine collaboration framework to bridge this gap. An effective framework should recognize that while ML enhances design efficiency through automation and prediction, it must be paired with human creativity to ensure spaces are emotionally engaging and culturally relevant. Human designers contribute intuition, empathy, and cultural insight, guiding ML-generated solutions to align with users' emotional and cultural needs. Additionally, we explore how various ML models can be integrated with human-centered design principles. These models can automate design generation and optimization, while human designers refine the outputs to ensure emotional resonance and aesthetic appeal. Through case studies in office and residential design, we illustrate how this framework fosters both creativity and cultural relevance. By merging ML with human creativity, spatial design can achieve a balance of efficiency and emotional impact, resulting in environments that are both functional and deeply human.

</details>


### [13] [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](https://arxiv.org/abs/2507.01862)

*Sanjay Krishna Anbalagan, Xinrui Nie, Umesh Mohan, Vijay Kumar Kanamarlapudi, Anughna Kommalapati, Xiaodan Zhao*

**Main category:** cs.HC

**Keywords:** chatbot, LLM, user interaction, HCI, task modeling

**Relevance Score:** 8

**TL;DR:** This paper proposes integrating GUI-inspired tasks into LLM prompts for better clarity and user satisfaction in multi-step chatbot interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the clarity and effectiveness of conversational agents in handling complex multi-step interactions that are historically managed better by traditional GUIs.

**Method:** Modeling tasks related to user acknowledgment ('submit-like') and context switching ('reset-like') as explicit components in the prompts of large language models.

**Key Contributions:**

	1. Introduction of GUI-inspired task modeling in LLM prompts
	2. Demonstration of the method in real-world scenarios
	3. Quantitative improvements in user interaction metrics

**Result:** The proposed method showed enhancements in multi-turn task coherence, increased user satisfaction, and improved efficiency in applications like hotel booking and customer management.

**Limitations:** 

**Conclusion:** By structuring session data around explicit user actions, the study highlights a novel approach for aligning chatbot interactions with backend processes, reducing confusion among users.

**Abstract:** Domain specific chatbot applications often involve multi step interactions, such as refining search filters, selecting multiple items, or performing comparisons. Traditional graphical user interfaces (GUIs) handle these workflows by providing explicit "Submit" (commit data) and "Reset" (discard data) actions, allowing back-end systems to track user intent unambiguously. In contrast, conversational agents rely on subtle language cues, which can lead to confusion and incomplete context management. This paper proposes modeling these GUI inspired metaphors acknowledgment (submit like) and context switching (reset-like) as explicit tasks within large language model (LLM) prompts. By capturing user acknowledgment, reset actions, and chain of thought (CoT) reasoning as structured session data, we preserve clarity, reduce user confusion, and align domain-specific chatbot interactions with back-end logic. We demonstrate our approach in hotel booking and customer management scenarios, highlighting improvements in multi-turn task coherence, user satisfaction, and efficiency.

</details>


### [14] [Spatial tangible user interfaces for cognitive assessment and training](https://arxiv.org/abs/2507.01944)

*Ehud Sharlin, Yuichi Itoh, Benjamin Watson, Yoshifumi Kitamura, Steve Sutphen, Lili Liu, Fumio Kishino*

**Main category:** cs.HC

**Keywords:** Tangible User Interfaces, Cognitive Assessment, Cognitive Training, Spatial Ability, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** This paper explores Tangible User Interfaces, particularly spatial TUIs, and their applications in cognitive assessment and training, presenting results from the Cognitive Cubes implementation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to demonstrate how Tangible User Interfaces, specifically spatial TUIs, can enhance human-computer interaction and address its current limitations.

**Method:** The authors developed a novel TUI known as Cognitive Cubes to evaluate its effectiveness in cognitive assessment and training, particularly focusing on spatial abilities.

**Key Contributions:**

	1. Introduction of Cognitive Cubes as a TUI for cognitive assessment and training.
	2. Demonstration of the capabilities of spatial TUIs over traditional interface technologies.
	3. Experimental evidence supporting the effectiveness of TUIs in evaluating and possibly enhancing cognitive spatial abilities.

**Result:** The experiments conducted using Cognitive Cubes yielded promising results for assessing and potentially training spatial abilities.

**Limitations:** 

**Conclusion:** The study indicates that spatial TUIs can significantly impact cognitive assessment and training approaches.

**Abstract:** This paper discusses Tangible User Interfaces (TUIs) and their potential impact on cognitive assessment and cognitive training. We believe that TUIs, and particularly a subset that we dub spatial TUIs, can extend human computer interaction beyond some of its current limitations. Spatial TUIs exploit human innate spatial and tactile ability in an intuitive and direct manner, affording interaction paradigms that are practically impossible using current interface technology. As proof-of-concept we examine implementations in the field of cognitive assessment and training. In this paper we use Cognitive Cubes, a novel TUI we developed, as an applied test bed for our beliefs, presenting promising experimental results for cognitive assessment of spatial ability, and possibly for training purposes.

</details>


### [15] [Decoding Neural Signals: Invasive BMI Review](https://arxiv.org/abs/2211.03324)

*Rezwan Firuzi, Ayub Bokani, Jahan Hassan, Hamed Ahmadyani, Mohammad Foad Abdi, Dana Naderi, Diako Ebrahimi*

**Main category:** cs.HC

**Keywords:** brain machine interface, BMI, neurobiology, engineering, human-AI symbiosis

**Relevance Score:** 5

**TL;DR:** This chapter discusses invasive brain machine interfaces (BMIs), their technological principles, applications, methodologies for brain signal detection and stimulation, and the challenges and opportunities for their development.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the transformative potential of invasive BMIs beyond medical applications and understand their implications in technology and society.

**Method:** The chapter analyzes biological and engineering principles, applications of BMIs, and methodologies for detecting, decoding, and stimulating brain signals.

**Key Contributions:**

	1. Detailed analysis of invasive BMI principles
	2. Exploration of diverse applications and methodologies
	3. Discussion on societal implications and challenges

**Result:** It provides a comprehensive overview of invasive BMIs, their foundational elements, applications, and potential societal impacts.

**Limitations:** 

**Conclusion:** Invasive BMIs hold significant promise and pose multifaceted challenges that must be addressed for future advancements.

**Abstract:** Human civilization has witnessed transformative technological milestones, from ancient fire lighting to the internet era. This chapter delves into the invasive brain machine interface (BMI), a pioneering technology poised to be a defining chapter in our progress. Beyond aiding medical conditions, invasive BMI promises far reaching impacts across diverse technologies and aspects of life. The exploration begins by unraveling the biological and engineering principles essential for BMI implementation. The chapter comprehensively analyzes potential applications, methodologies for detecting and decoding brain signals, and options for stimulating signals within the human brain. It concludes with a discussion on the multifaceted challenges and opportunities for the continued development of invasive BMI. This chapter not only provides a profound understanding of the foundational elements of invasive BMI but also serves as a guide through its applications, intricacies, and potential societal implications. Navigating neurobiology, engineering innovations, and the evolving landscape of human AI symbiosis, the chapter sheds light on the promises and hurdles that define the future of invasive BMI.

</details>


### [16] [SoK: Usability Studies in Differential Privacy](https://arxiv.org/abs/2412.16825)

*Onyinye Dibia, Prianka Bhattacharjee, Brad Stenger, Steven Baldasty, Mako Bates, Ivoline C. Ngong, Yuanyuan Feng, Joseph P. Near*

**Main category:** cs.HC

**Keywords:** Differential Privacy, usability, user-centered design, privacy communication, adoption challenges

**Relevance Score:** 5

**TL;DR:** This paper systematizes research on the usability of Differential Privacy (DP), identifying challenges and best practices for developers and non-technical stakeholders.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address hurdles in the practical adoption of Differential Privacy by examining usability issues and providing actionable insights for better communication and tool design.

**Method:** The authors conducted a systematic review of existing literature on the usability of Differential Privacy tools and communication strategies regarding privacy parameters.

**Key Contributions:**

	1. Systematization of existing usability research on Differential Privacy
	2. Identification of core challenges and best practices
	3. Proposals for future research directions emphasizing user-centered design

**Result:** The study uncovers critical usability challenges and gaps in current DP tools, along with best practices for improving accessibility and understanding across user groups.

**Limitations:** Focuses primarily on usability without delving deeply into technical implementation aspects of Differential Privacy.

**Conclusion:** The paper calls for user-centered design in DP tools and better communication strategies to enhance adoption by diverse stakeholders.

**Abstract:** Differential Privacy (DP) has emerged as a pivotal approach for safeguarding individual privacy in data analysis, yet its practical adoption is often hindered by challenges in the implementation and communication of DP. This paper presents a comprehensive systematization of existing research studies around the usability of DP, synthesizing insights from studies on both the practical use of DP tools and strategies for conveying DP parameters that determine privacy protection levels, such as epsilon($\varepsilon$). By reviewing and analyzing these studies, we identify core usability challenges, best practices, and critical gaps in current DP tools that affect adoption across diverse user groups, including developers, data analysts, and non-technical stakeholders. Our analysis highlights actionable insights and pathways for future research that emphasizes user-centered design and clear communication, fostering the development of more accessible DP tools that meet practical needs and support broader adoption.

</details>


### [17] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)

*Wen Zhan, Ziqun Hua, Peiyue Lin, Yunfei Chen*

**Main category:** cs.HC

**Keywords:** AI-assisted co-creation, aging migrants, narrative agency, Hanzi reconstruction, human-AI collaboration

**Relevance Score:** 8

**TL;DR:** The paper investigates AI-assisted co-creation techniques for older adults to express complex personal narratives through a workshop that combines storytelling and glyph reconstruction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable aging migrants in urban China to express fragmented personal narratives using AI as a supportive tool rather than a content creator.

**Method:** A pilot workshop where participants engaged in oral storytelling and reconstructed Hanzi characters using a Large Language Model, combined with physical materials, with human facilitation.

**Key Contributions:**

	1. Introduced a novel workshop format for storytelling using AI and traditional methods.
	2. Demonstrated the feasibility of narrative engagement without digital literacy.
	3. Shifted the perspective of AI from content generation to support in narrative agency.

**Result:** Participants successfully transformed their lived experiences into visual and tactile forms without the need for digital literacy, showcasing effective human-AI collaboration.

**Limitations:** Limited to a specific demographic (aging migrants in urban China) and may not be generalizable.

**Conclusion:** The approach redefines AI's role in narrative expression for older adults, emphasizing supportive interaction within sociotechnical contexts.

**Abstract:** This paper explores how older adults, particularly aging migrants in urban China, can engage AI-assisted co-creation to express personal narratives that are often fragmented, underrepresented, or difficult to verbalize. Through a pilot workshop combining oral storytelling and the symbolic reconstruction of Hanzi, participants shared memories of migration and recreated new character forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM), together with physical materials. Supported by human facilitation and a soft AI presence, participants transformed lived experience into visual and tactile expressions without requiring digital literacy. This approach offers new perspectives on human-AI collaboration and aging by repositioning AI not as a content producer but as a supportive mechanism, and by supporting narrative agency within sociotechnical systems.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [18] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)

*Imran Mirza, Cole Huang, Ishwara Vasista, Rohan Patil, Asli Akalin, Sean O'Brien, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** multi-agent systems, bias assessment, large language models, fairness, transparent evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces MALIBU, a benchmark for assessing biases in multi-agent systems utilizing large language models (LLMs) in persona-based interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the reinforcement of implicit biases in LLM-based multi-agent systems, which can lead to unfair representations and a lack of equitable outcomes.

**Method:** MALIBU evaluates bias through scenario-based assessments where AI models complete tasks in predefined contexts. Responses are scored by a multi-agent judging system in two phases assessing labeled demographics and preference comparisons.

**Key Contributions:**

	1. Introduction of the MALIBU benchmark for bias assessment in LLM multi-agent systems
	2. Development of a methodology for scenario-based assessments of biases
	3. Identification of potential trade-offs in bias mitigation strategies

**Result:** The study found that bias mitigation strategies might benefit marginalized personas, indicating a complexity in achieving true neutrality across responses.

**Limitations:** The study primarily focuses on specific demographic personas and may not fully encapsulate all forms of bias present in LLM outputs.

**Conclusion:** The findings highlight the necessity for nuanced bias detection methods, balanced fairness strategies, and the development of transparent evaluation benchmarks for multi-agent systems.

**Abstract:** Multi-agent systems, which consist of multiple AI models interacting within a shared environment, are increasingly used for persona-based interactions. However, if not carefully designed, these systems can reinforce implicit biases in large language models (LLMs), raising concerns about fairness and equitable representation. We present MALIBU, a novel benchmark developed to assess the degree to which LLM-based multi-agent systems implicitly reinforce social biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems through scenario-based assessments. AI models complete tasks within predefined contexts, and their responses undergo evaluation by an LLM-based multi-agent judging system in two phases. In the first phase, judges score responses labeled with specific demographic personas (e.g., gender, race, religion) across four metrics. In the second phase, judges compare paired responses assigned to different personas, scoring them and selecting the superior response. Our study quantifies biases in LLM-generated outputs, revealing that bias mitigation may favor marginalized personas over true neutrality, emphasizing the need for nuanced detection, balanced fairness strategies, and transparent evaluation benchmarks in multi-agent systems.

</details>


### [19] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)

*Huiling You, Samia Touileb, Erik Velldal, Lilja vrelid*

**Main category:** cs.CL

**Keywords:** abstractive summarization, event evaluation, generative models, natural language processing, news articles

**Relevance Score:** 6

**TL;DR:** This paper proposes a method to evaluate the quality of abstractive summaries by focusing on overlapping events between generated summaries, reference summaries, and original news articles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional evaluation of generated summaries often relies on human-authored summaries as references, which may not effectively represent the events reported in news articles.

**Method:** The authors propose a novel evaluation approach that calculates overlapping events between generated summaries, reference summaries, and original articles, utilizing a richly annotated Norwegian dataset with event annotations.

**Key Contributions:**

	1. Introduction of a new evaluation method focusing on event overlaps for summarization quality assessment.
	2. Utilization of an annotated dataset that includes event information relevant to news articles.
	3. Provision of insights into how well generative models capture event information in summaries.

**Result:** Experimental results demonstrate that this new evaluation method provides deeper insights into the event information captured in the summaries compared to traditional methods.

**Limitations:** The study is based on a specific Norwegian dataset, which may limit the generalizability of the findings to other languages or contexts.

**Conclusion:** The proposed evaluation metric enhances the understanding of the quality of abstractive summaries by focusing on event coverage, potentially leading to better summary generation techniques.

**Abstract:** An abstractive summary of a news article contains its most important information in a condensed version. The evaluation of automatically generated summaries by generative language models relies heavily on human-authored summaries as gold references, by calculating overlapping units or similarity scores. News articles report events, and ideally so should the summaries. In this work, we propose to evaluate the quality of abstractive summaries by calculating overlapping events between generated summaries, reference summaries, and the original news articles. We experiment on a richly annotated Norwegian dataset comprising both events annotations and summaries authored by expert human annotators. Our approach provides more insight into the event information contained in the summaries.

</details>


### [20] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)

*Simon Brjesson, Erik Ersmark, Pierre Nugues*

**Main category:** cs.CL

**Keywords:** Nordisk familjebok, semantic embeddings, geographic focus, Wikidata, historical encyclopedia

**Relevance Score:** 2

**TL;DR:** This paper analyzes the evolution of geographic entries in the 4Nordisk familjebok4 encyclopedia through digitized text from two editions, using semantic embeddings and classifiers to identify shifts in focus between 1876-1926.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To study the changes in geographic focus in the 4Nordisk familjebok4 encyclopedia as it reflects intellectual trends in Sweden influenced by significant global events.

**Method:** The authors employed semantic sentence embeddings for resegmenting raw text and matched entries between editions. They used a transformer-based classifier to extract geographical entries and linked them to Wikidata.

**Key Contributions:**

	1. Usage of semantic embeddings to analyze historical texts
	2. Identification of geographic trends in an influential encyclopedia
	3. Linking of entries to Wikidata for enhanced scholarly access

**Result:** The analysis reveals a notable shift in geographic focus from Europe towards North America, Africa, Asia, Australia, and northern Scandinavia between the first and second editions.

**Limitations:** 

**Conclusion:** The findings indicate a small yet significant shift in emphasis that aligns with historical events such as the First World War and the emergence of new global powers.

**Abstract:** The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and 20th centuries. It was written by a team of experts and aimed to be an intellectual reference, stressing precision and accuracy. This encyclopedia had four main editions remarkable by their size, ranging from 20 to 38 volumes. As a consequence, the \textit{Nordisk familjebok} had a considerable influence in universities, schools, the media, and society overall. As new editions were released, the selection of entries and their content evolved, reflecting intellectual changes in Sweden.   In this paper, we used digitized versions from \textit{Project Runeberg}. We first resegmented the raw text into entries and matched pairs of entries between the first and second editions using semantic sentence embeddings. We then extracted the geographical entries from both editions using a transformer-based classifier and linked them to Wikidata. This enabled us to identify geographic trends and possible shifts between the first and second editions, written between 1876-1899 and 1904-1926, respectively.   Interpreting the results, we observe a small but significant shift in geographic focus away from Europe and towards North America, Africa, Asia, Australia, and northern Scandinavia from the first to the second edition, confirming the influence of the First World War and the rise of new powers. The code and data are available on GitHub at https://github.com/sibbo/nordisk-familjebok.

</details>


### [21] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)

*Adamu Lawan, Juhua Pu, Haruna Yunusa, Jawad Muhammad, Muhammad Lawan*

**Main category:** cs.CL

**Keywords:** Aspect-based Sentiment Analysis, NLP, xLSTM, Machine Learning, Deep Learning

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel framework, xLSTM with Multihead Exponential Gated Fusion (MEGA), for aspect-based sentiment analysis (ABSA) that improves efficiency and performance by utilizing an advanced modeling technique.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Existing ABSA methods struggle with balancing computational efficiency and high performance, often lacking context or being computationally expensive.

**Method:** The paper introduces xLSTM with Multihead Exponential Gated Fusion (MEGA), integrating a bi-directional mLSTM architecture with PF-mLSTM streams and an mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF).

**Key Contributions:**

	1. Introduction of xLSTM with Multihead Exponential Gated Fusion (MEGA) framework.
	2. Utilization of PF-mLSTM for enhanced localized context modeling.
	3. Development of the MECGAF mechanism for dynamic combination of outputs.

**Result:** Experimental results show that MEGA outperforms state-of-the-art baselines on three benchmark datasets, achieving superior accuracy and efficiency in ABSA tasks.

**Limitations:** 

**Conclusion:** The proposed MEGA framework effectively enhances localized context modeling and optimizes short-range dependency capture while maintaining global context.

**Abstract:** Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language Processing (NLP) task that extracts aspects from text and determines their associated sentiments, enabling fine-grained analysis of user opinions. Existing ABSA methods struggle to balance computational efficiency with high performance: deep learning models often lack global context, transformers demand significant computational resources, and Mamba-based approaches face CUDA dependency and diminished local correlations. Recent advancements in Extended Long Short-Term Memory (xLSTM) models, particularly their efficient modeling of long-range dependencies, have significantly advanced the NLP community. However, their potential in ABSA remains untapped. To this end, we propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework integrating a bi-directional mLSTM architecture with forward and partially flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context modeling by processing the initial sequence segment in reverse with dedicated parameters, preserving critical short-range patterns. We further introduce an mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that dynamically combines forward mLSTM outputs as query and key with PF-mLSTM outputs as value, optimizing short-range dependency capture while maintaining global context and efficiency. Experimental results on three benchmark datasets demonstrate that MEGA outperforms state-of-the-art baselines, achieving superior accuracy and efficiency in ABSA tasks.

</details>


### [22] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)

*Yu Fan, Yang Tian, Shauli Ravfogel, Mrinmaya Sachan, Elliott Ash, Alexander Hoyle*

**Main category:** cs.CL

**Keywords:** embedding-based metrics, debiasing algorithm, text similarity

**Relevance Score:** 7

**TL;DR:** This paper presents a debiasing algorithm that effectively reduces biases in embedding-based similarity metrics for text sequences, leading to improved document similarity and clustering performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses biases in embedding-based text similarity metrics caused by spurious confounders, which hinder applications requiring texts from diverse sources.

**Method:** The study introduces a debiasing algorithm that removes observed confounder information from encoder representations, ensuring cleaner embeddings.

**Key Contributions:**

	1. Introduction of a novel debiasing algorithm for text embeddings
	2. Demonstration of improved similarity and clustering performance
	3. Validation of retained effectiveness on out-of-distribution tasks

**Result:** The embeddings show improved performance in document similarity and clustering metrics across all evaluated embedding variants and tasks, with minimal computational cost involved.

**Limitations:** 

**Conclusion:** The proposed debiasing method not only reduces biases but also maintains performance on out-of-distribution benchmarks without degrading the embeddings.

**Abstract:** Embedding-based similarity metrics between text sequences can be influenced not just by the content dimensions we most care about, but can also be biased by spurious attributes like the text's source or language. These document confounders cause problems for many applications, but especially those that need to pool texts from different corpora. This paper shows that a debiasing algorithm that removes information about observed confounders from the encoder representations substantially reduces these biases at a minimal computational cost. Document similarity and clustering metrics improve across every embedding variant and task we evaluate -- often dramatically. Interestingly, performance on out-of-distribution benchmarks is not impacted, indicating that the embeddings are not otherwise degraded.

</details>


### [23] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)

*Micha Matak, Jarosaw A. Chudziak*

**Main category:** cs.CL

**Keywords:** large language models, legal information retrieval, cognitive agents

**Relevance Score:** 4

**TL;DR:** This paper presents gAIus, a cognitive LLM-based agent designed to improve legal information retrieval for non-English and non-Chinese speaking countries, demonstrating substantial improvements over existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by legal information retrieval in non-English and non-Chinese contexts, particularly in Poland, and to leverage large language models for better explainability and results.

**Method:** Development of the gAIus architecture which utilizes a retrieval mechanism based on the Polish Civil Code and evaluates performance against law apprenticeship exam questions.

**Key Contributions:**

	1. Introduction of gAIus, a cognitive agent for legal tasks
	2. Significant performance improvement over existing LLMs in legal contexts
	3. Creation of a specialized dataset for evaluating legal information retrieval

**Result:** The gAIus architecture improves the performance of GPT-3.5 by 419%, surpassing GPT-4o and significantly enhancing its effectiveness in legal information retrieval tasks.

**Limitations:** 

**Conclusion:** The findings suggest promising directions for future research applications in legal contexts, emphasizing the need for explainability and improved retrieval in AI applications.

**Abstract:** In this paper we discuss the capability of large language models to base their answer and provide proper references when dealing with legal matters of non-english and non-chinese speaking country. We discuss the history of legal information retrieval, the difference between case law and statute law, its impact on the legal tasks and analyze the latest research in this field. Basing on that background we introduce gAIus, the architecture of the cognitive LLM-based agent, whose responses are based on the knowledge retrieved from certain legal act, which is Polish Civil Code. We propose a retrieval mechanism which is more explainable, human-friendly and achieves better results than embedding-based approaches. To evaluate our method we create special dataset based on single-choice questions from entrance exams for law apprenticeships conducted in Poland. The proposed architecture critically leveraged the abilities of used large language models, improving the gpt-3.5-turbo-0125 by 419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%. At the end of our paper we show the possible future path of research and potential applications of our findings.

</details>


### [24] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)

*Cindy Lie Tabuse, David Restepo, Carolina Gracitelli, Fernando Korn Malerbi, Caio Regatieri, Luis Filipe Nakayama*

**Main category:** cs.CL

**Keywords:** large language models, ophthalmology, diabetic retinopathy, glaucoma, clinical decision-making

**Relevance Score:** 9

**TL;DR:** GPT-4 was evaluated for its ability to simulate clinical decisions in ophthalmology based on structured text prompts related to retinal images.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Exploring the utility of LLMs in ophthalmology for clinical reasoning and decision-making.

**Method:** A retrospective diagnostic validation study using 300 annotated fundus images, assessing GPT-4's ability to classify and recommend referrals for diabetic retinopathy and glaucoma.

**Key Contributions:**

	1. Evaluation of LLMs in a clinical context for ophthalmology
	2. Insights on the performance of GPT-4 in image interpretation tasks
	3. Implications for the educational and documentation roles of LLMs in healthcare.

**Result:** GPT-4 achieved moderate performance with an accuracy of 67.5% in ICDR classification and 82.3% in binary DR referral, but performed poorly in glaucoma referrals, showing that metadata usage did not significantly affect outcomes.

**Limitations:** Lack of precision for complex ophthalmic tasks; not suitable for clinical use.

**Conclusion:** LLMs like GPT-4 can assist in ophthalmic decision-making from structured prompts, but lack precision for complex tasks, indicating potential roles in education and workflows rather than direct clinical applications.

**Abstract:** Large language models (LLMs) can simulate clinical reasoning based on natural language prompts, but their utility in ophthalmology is largely unexplored. This study evaluated GPT-4's ability to interpret structured textual descriptions of retinal fundus photographs and simulate clinical decisions for diabetic retinopathy (DR) and glaucoma screening, including the impact of adding real or synthetic clinical metadata. We conducted a retrospective diagnostic validation study using 300 annotated fundus images. GPT-4 received structured prompts describing each image, with or without patient metadata. The model was tasked with assigning an ICDR severity score, recommending DR referral, and estimating the cup-to-disc ratio for glaucoma referral. Performance was evaluated using accuracy, macro and weighted F1 scores, and Cohen's kappa. McNemar's test and change rate analysis were used to assess the influence of metadata. GPT-4 showed moderate performance for ICDR classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25), driven mainly by correct identification of normal cases. Performance improved in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For glaucoma referral, performance was poor across all settings (accuracy ~78%, F1 <0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes (McNemar p > 0.05), and predictions remained consistent across conditions. GPT-4 can simulate basic ophthalmic decision-making from structured prompts but lacks precision for complex tasks. While not suitable for clinical use, LLMs may assist in education, documentation, or image annotation workflows in ophthalmology.

</details>


### [25] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)

*Juan Chen, Baolong Bi, Wei Zhang, Jingyan Sui, Xiaofei Zhu, Yuanzhuo Wang, Lingrui Mei, Shenghua Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Conflict-Aware, Evidence Summarization, QA Repair

**Relevance Score:** 9

**TL;DR:** CARE-RAG improves the reliability of Retrieval-Augmented Generation (RAG) systems by addressing knowledge conflicts within generated content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability of RAG systems by mitigating knowledge conflicts that arise from internal inconsistencies and noisy external content.

**Method:** The CARE-RAG framework employs Conflict-Driven Summarization to evaluate all available evidence, deriving parameter-aware evidence and refining retrieved content for accuracy and relevance.

**Key Contributions:**

	1. Introduction of CARE-RAG framework for conflict-aware evidence processing.
	2. Implementation of Conflict-Driven Summarization to evaluate diverse evidence.
	3. QA Repair step to ensure evaluation integrity of benchmark answers.

**Result:** CARE-RAG consistently outperforms existing RAG baselines in retrieval-based QA datasets, particularly in cases of conflicting or noisy evidence.

**Limitations:** 

**Conclusion:** By introducing a QA Repair step and employing a conflict-driven summarization model, CARE-RAG significantly enhances generation trustworthiness and reliability in LLM outputs.

**Abstract:** Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating their parametric knowledge with external retrieved content. However, knowledge conflicts caused by internal inconsistencies or noisy retrieved content can severely undermine the generation reliability of RAG systems.In this work, we argue that LLMs should rethink all evidence, including both retrieved content and internal knowledge, before generating responses.We propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel framework that improves trustworthiness through Conflict-Driven Summarization of all available evidence.CARE-RAG first derives parameter-aware evidence by comparing parameter records to identify diverse internal perspectives. It then refines retrieved evidences to produce context-aware evidence, removing irrelevant or misleading content. To detect and summarize conflicts, we distill a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable synthesis across multiple sources.To further ensure evaluation integrity, we introduce a QA Repair step to correct outdated or ambiguous benchmark answers.Experiments on revised QA datasets with retrieval data show that CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios with noisy or conflicting evidence.

</details>


### [26] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)

*Xinxi Lyu, Michael Duan, Rulin Shao, Pang Wei Koh, Sewon Min*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented Generation, RAG, CompactDS, web-scale datastore, reasoning benchmarks

**Relevance Score:** 9

**TL;DR:** This paper introduces CompactDS, a high-quality, web-scale datastore that enhances Retrieval-augmented Generation (RAG) capabilities, leading to significant accuracy improvements across reasoning-intensive benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve RAG performance on reasoning-intensive benchmarks by addressing the lack of a usable web-scale datastore aligned with pretraining data.

**Method:** The authors developed CompactDS, a diverse and compact datastore combining in-memory approximate nearest neighbor retrieval with on-disk exact search to optimize retrieval speed and recall.

**Key Contributions:**

	1. Introduction of CompactDS, a new web-scale datastore for RAG
	2. Demonstration of significant accuracy improvements across multiple reasoning benchmarks
	3. Establishment of the importance of diverse data sources for effective retrieval

**Result:** CompactDS demonstrated substantial accuracy gains on several benchmarks, including 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH when used in a minimal RAG pipeline.

**Limitations:** 

**Conclusion:** The paper concludes that a diverse set of data sources is essential for optimal RAG performance, and CompactDS outperformed existing search engines while being simpler and more reproducible.

**Abstract:** Retrieval-augmented Generation (RAG) has primarily been studied in limited settings, such as factoid question answering; more challenging, reasoning-intensive benchmarks have seen limited success from minimal RAG. In this work, we challenge this prevailing view on established, reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We identify a key missing component in prior work: a usable, web-scale datastore aligned with the breadth of pretraining data. To this end, we introduce CompactDS: a diverse, high-quality, web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single-node. The key insights are (1) most web content can be filtered out without sacrificing coverage, and a compact, high-quality subset is sufficient; and (2) combining in-memory approximate nearest neighbor (ANN) retrieval and on-disk exact search balances speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves consistent accuracy improvements across all benchmarks and model sizes (8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA, and 19% on MATH. No single data source suffices alone, highlighting the importance of diversity of sources (web crawls, curated math, academic papers, textbooks). Finally, we show that our carefully designed in-house datastore matches or outperforms web search engines such as Google Search, as well as recently proposed, complex agent-based RAG systems--all while maintaining simplicity, reproducibility, and self-containment. We release CompactDS and our retrieval pipeline, supporting future research exploring retrieval-based AI systems.

</details>


### [27] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)

*Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Activation sparsity, Inference speed-up

**Relevance Score:** 8

**TL;DR:** LaRoSA proposes a method for activation sparsification in LLMs, enhancing efficiency without extra training requirements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing activation sparsity methods that either require extensive retraining or suffer from inconsistent performance due to empirical pruning.

**Method:** LaRoSA uses layerwise orthogonal rotations to transform input activations and applies Top-K selection to achieve effective and stable sparsity in Large Language Models.

**Key Contributions:**

	1. Introduction of LaRoSA for improved activation sparsity
	2. Utilization of layerwise orthogonal rotations for transformation
	3. Demonstration of significant efficiency gains without extra training

**Result:** LaRoSA shows minimal performance degradation with a 0.17 perplexity gap and a 1.30x wall-clock speed-up at 40% sparsity in LLaMA2-7B, outperforming TEAL and CATS in specific tasks.

**Limitations:** 

**Conclusion:** LaRoSA effectively enhances LLM efficiency and performance without additional training, making it a practical solution for real-world applications.

**Abstract:** Activation sparsity can reduce the computational overhead and memory transfers during the forward pass of Large Language Model (LLM) inference. Existing methods face limitations, either demanding time-consuming recovery training that hinders real-world adoption, or relying on empirical magnitude-based pruning, which causes fluctuating sparsity and unstable inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse Activation), a novel method for activation sparsification designed to improve LLM efficiency without requiring additional training or magnitude-based pruning. We leverage layerwise orthogonal rotations to transform input activations into rotated forms that are more suitable for sparsification. By employing a Top-K selection approach within the rotated activations, we achieve consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA is effective across various sizes and types of LLMs, demonstrating minimal performance degradation and robust inference acceleration. Specifically, for LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in zero-shot tasks compared to the dense model to just 0.54%, while surpassing TEAL by 1.77% and CATS by 17.14%.

</details>


### [28] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)

*Nifu Dan, Yujun Cai, Yiwei Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Physics Reasoning, Instruction-tuned Models

**Relevance Score:** 2

**TL;DR:** This study explores the application of advanced instruction-tuned reasoning models to solve diverse physics problems, demonstrating significant improvements in accuracy and reasoning patterns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enhance the physics reasoning capabilities of Large Language Models (LLMs) through advanced instruction-tuning and problem-solving techniques.

**Method:** Experimental evaluation of the Deepseek-R1 model on the SciBench benchmark, focusing on few-shot prompting and reasoning pattern analysis.

**Key Contributions:**

	1. Demonstrated state-of-the-art performance of advanced instruction-tuned reasoning models in physics problem solving.
	2. Generated unique reasoning patterns that emphasize symbolic derivation.
	3. Showed measurable accuracy improvements through few-shot prompting.

**Result:** Deepseek-R1 achieves state-of-the-art accuracy in solving complex physics problems while generating distinctive reasoning patterns emphasizing symbolic derivation.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of advanced reasoning models and the potential for performance enhancement through few-shot prompting.

**Abstract:** Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains.

</details>


### [29] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)

*Xunjian Yin, Sitao Cheng, Yuxi Xie, Xinyu Hu, Li Lin, Xinyi Wang, Liangming Pan, William Yang Wang, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** reverse language model, autoregressive model, mathematical reasoning

**Relevance Score:** 6

**TL;DR:** LEDOM is a reverse language model trained on 435B tokens, demonstrating improved outputs in mathematical reasoning tasks through reverse token prediction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To introduce a novel type of language model that processes sequences in reverse and to explore its applications in enhancing generative performance.

**Method:** LEDOM is an autoregressive model trained on vast amounts of data, functioning by predicting tokens in reverse temporal order.

**Key Contributions:**

	1. Introduction of the first purely reverse language model
	2. Demonstration of improved mathematical reasoning through reverse token prediction
	3. All models and resources will be publicly released to aid future research.

**Result:** Substantial performance improvements on mathematical reasoning tasks by leveraging reverse reasoning capabilities.

**Limitations:** 

**Conclusion:** LEDOM shows significant promise as a foundational model for a range of tasks and will be made publicly available for further research.

**Abstract:** We introduce LEDOM, the first purely reverse language model, trained autoregressively on 435B tokens with 2B and 7B parameter variants, which processes sequences in reverse temporal order through previous token prediction. For the first time, we present the reverse language model as a potential foundational model across general tasks, accompanied by a set of intriguing examples and insights. Based on LEDOM, we further introduce a novel application: Reverse Reward, where LEDOM-guided reranking of forward language model outputs leads to substantial performance improvements on mathematical reasoning tasks. This approach leverages LEDOM's unique backward reasoning capability to refine generation quality through posterior evaluation. Our findings suggest that LEDOM exhibits unique characteristics with broad application potential. We will release all models, training code, and pre-training data to facilitate future research.

</details>


### [30] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)

*Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, Yahui Zhou*

**Main category:** cs.CL

**Keywords:** reward models, RLHF, preference datasets, human-AI synergy, Skywork-Reward-V2

**Relevance Score:** 8

**TL;DR:** Presentation of Skywork-Reward-V2, a suite of eight reward models trained on a large-scale preference dataset, SynPref-40M, addressing the shortcomings of previous reward models in RLHF.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current reward models in reinforcement learning from human feedback perform poorly on evaluations due to limitations in preference datasets.

**Method:** Introduction of SynPref-40M, a large-scale preference dataset with 40 million pairs. A human-AI synergistic pipeline for data curation was designed, where humans provide annotations and LLMs assist in curation.

**Key Contributions:**

	1. Introduction of a 40 million preference pair dataset, SynPref-40M.
	2. Development of Skywork-Reward-V2 models achieving state-of-the-art results.
	3. Human-AI pipeline for scalable and high-quality data curation.

**Result:** Skywork-Reward-V2 models show state-of-the-art performance across seven major benchmarks, demonstrating robustness in alignment with human preferences and other capabilities.

**Limitations:** 

**Conclusion:** Skywork-Reward-V2 highlights the potential of improved data curation and quality in reward modeling and its impact on RLHF.

**Abstract:** Despite the critical role of reward models (RMs) in reinforcement learning from human feedback (RLHF), current state-of-the-art open RMs perform poorly on most existing evaluation benchmarks, failing to capture the spectrum of nuanced and sophisticated human preferences. Even approaches that incorporate advanced training techniques have not yielded meaningful performance improvements. We hypothesize that this brittleness stems primarily from limitations in preference datasets, which are often narrowly scoped, synthetically labeled, or lack rigorous quality control. To address these challenges, we present a large-scale preference dataset comprising 40 million preference pairs, named SynPref-40M. To enable data curation at scale, we design a human-AI synergistic two-stage pipeline that leverages the complementary strengths of human annotation quality and AI scalability. In this pipeline, humans provide verified annotations, while large language models perform automatic curation based on human guidance. Training on this preference mixture, we introduce Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B parameters, trained on a carefully curated subset of 26 million preference pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile across a wide range of capabilities, including alignment with human preferences, objective correctness, safety, resistance to stylistic biases, and best-of-N scaling, achieving state-of-the-art performance across seven major reward model benchmarks. Ablation studies confirm that the effectiveness of our approach stems not only from data scale but also from high-quality curation. The Skywork-Reward-V2 series represents substantial progress in open reward models, highlighting the untapped potential of existing preference datasets and demonstrating how human-AI curation synergy can unlock significantly higher data quality.

</details>


### [31] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)

*Ting Xu, Xiaoxiao Deng, Xiandong Meng, Haifeng Yang, Yan Wu*

**Main category:** cs.CL

**Keywords:** electronic health records, deep learning, multi-label disease prediction, Transformer model, attention mechanisms

**Relevance Score:** 9

**TL;DR:** This paper presents a deep learning approach leveraging attention mechanisms for information extraction and multi-label disease prediction from electronic health record texts, utilizing a Transformer model on the MIMIC-IV dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of unstructured and high-dimensional semantic complexity in electronic health record texts for improved information extraction and disease prediction.

**Method:** A Transformer-based architecture utilizing multi-layer self-attention mechanisms is proposed for representation learning on clinical text, followed by a Sigmoid-based multi-label classifier.

**Key Contributions:**

	1. Introduction of a context-aware semantic alignment mechanism
	2. Application of multi-layer self-attention in clinical text representation
	3. Strong performance across various metrics compared to existing methods.

**Result:** The method outperforms existing approaches on multiple performance metrics and maintains strong generalization across varying conditions.

**Limitations:** 

**Conclusion:** The developed framework provides an efficient foundational algorithm for processing clinical texts, with practical significance for multi-label medical text modeling tasks.

**Abstract:** This paper addresses the challenges posed by the unstructured nature and high-dimensional semantic complexity of electronic health record texts. A deep learning method based on attention mechanisms is proposed to achieve unified modeling for information extraction and multi-label disease prediction. The study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is used to perform representation learning over clinical text. Multi-layer self-attention mechanisms are employed to capture key medical entities and their contextual relationships. A Sigmoid-based multi-label classifier is then applied to predict multiple disease labels. The model incorporates a context-aware semantic alignment mechanism, enhancing its representational capacity in typical medical scenarios such as label co-occurrence and sparse information. To comprehensively evaluate model performance, a series of experiments were conducted, including baseline comparisons, hyperparameter sensitivity analysis, data perturbation studies, and noise injection tests. Results demonstrate that the proposed method consistently outperforms representative existing approaches across multiple performance metrics. The model maintains strong generalization under varying data scales, interference levels, and model depth configurations. The framework developed in this study offers an efficient algorithmic foundation for processing real-world clinical texts and presents practical significance for multi-label medical text modeling tasks.

</details>


### [32] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)

*Tianyu Liu, Qitan Lv, Hao Li, Xing Gao, Xiao Sun*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, token retrieval, inference acceleration, LogitSpec

**Relevance Score:** 9

**TL;DR:** LogitSpec is a training-free method that enhances speculative decoding in LLMs by expanding the token retrieval range, increasing inference speed and efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need to speed up LLM inference and reduce the overhead of existing draft models in speculative decoding.

**Method:** LogitSpec generates draft tokens by leveraging the logit of the last token to predict the next two tokens and retrieves relevant references for them in a retrieval-based manner.

**Key Contributions:**

	1. Proposes a novel method for generating draft tokens using logit speculation.
	2. Eliminates the need for a training phase, making it plug-and-play.
	3. Demonstrates significant speed and efficiency improvements in LLM decoding.

**Result:** LogitSpec achieved up to 2.61 times speedup and 3.28 mean accepted tokens per decoding step across various text generation benchmarks.

**Limitations:** 

**Conclusion:** LogitSpec provides a significant improvement in inference acceleration without the complexity of training a draft model, making it practical for integration into current LLM frameworks.

**Abstract:** Speculative decoding (SD), where a small draft model is employed to propose draft tokens in advance and then the target model validates them in parallel, has emerged as a promising technique for LLM inference acceleration. Many endeavors to improve SD are to eliminate the need for a draft model and generate draft tokens in a retrieval-based manner in order to further alleviate the drafting overhead and significantly reduce the difficulty in deployment and applications. However, retrieval-based SD relies on a matching paradigm to retrieval the most relevant reference as the draft tokens, where these methods often fail to find matched and accurate draft tokens. To address this challenge, we propose LogitSpec to effectively expand the retrieval range and find the most relevant reference as drafts. Our LogitSpec is motivated by the observation that the logit of the last token can not only predict the next token, but also speculate the next next token. Specifically, LogitSpec generates draft tokens in two steps: (1) utilizing the last logit to speculate the next next token; (2) retrieving relevant reference for both the next token and the next next token. LogitSpec is training-free and plug-and-play, which can be easily integrated into existing LLM inference frameworks. Extensive experiments on a wide range of text generation benchmarks demonstrate that LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens per decoding step. Our code is available at https://github.com/smart-lty/LogitSpec.

</details>


### [33] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)

*Yingqiang Gao, Kaede Johnson, David Froehlich, Luisa Carrer, Sarah Ebling*

**Main category:** cs.CL

**Keywords:** Automatic Text Simplification, Large Language Models, User Feedback

**Relevance Score:** 9

**TL;DR:** The paper presents a method for enhancing automatic text simplification using human feedback from individuals with intellectual disabilities to personalize learning in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language accessibility for individuals with intellectual disabilities through personalized text simplifications.

**Method:** The authors extend standard supervised fine-tuning of LLMs by implementing direct preference optimization using feedback from users with intellectual disabilities on generated text simplifications.

**Key Contributions:**

	1. Introduction of direct preference optimization for LLM-based ATS systems
	2. Development of a comprehensive pipeline for personalized LLM-based ATS
	3. Highlighting the importance of user feedback in AI accessibility solutions

**Result:** Post-training LLMs exhibited effective personalization in generating text simplifications that align with user preferences, demonstrating the value of direct feedback in model training.

**Limitations:** 

**Conclusion:** Including the preferences of actual users with intellectual disabilities in the development of ATS systems is crucial for creating effective personalized AI solutions.

**Abstract:** Automatic text simplification (ATS) aims to enhance language accessibility for various target groups, particularly persons with intellectual disabilities. Recent advancements in generative AI, especially large language models (LLMs), have substantially improved the quality of machine-generated text simplifications, thereby mitigating information barriers for the target group. However, existing LLM-based ATS systems do not incorporate preference feedback on text simplifications during training, resulting in a lack of personalization tailored to the specific needs of target group representatives.   In this work, we extend the standard supervised fine-tuning (SFT) approach for adapting LLM-based ATS models by leveraging a computationally efficient LLM alignment technique -- direct preference optimization (DPO). Specifically, we post-train LLM-based ATS models using human feedback collected from persons with intellectual disabilities, reflecting their preferences on paired text simplifications generated by mainstream LLMs. Furthermore, we propose a pipeline for developing personalized LLM-based ATS systems, encompassing data collection, model selection, SFT and DPO post-training, and evaluation. Our findings underscore the necessity of active participation of target group persons in designing personalized AI accessibility solutions aligned with human expectations. This work represents a step towards personalizing inclusive AI systems at the target-group level, incorporating insights not only from text simplification experts but also from target group persons themselves.

</details>


### [34] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)

*lvaro Zaera, Diana Nicoleta Popa, Ivan Sekulic, Paolo Rosso*

**Main category:** cs.CL

**Keywords:** Out-of-scope detection, Task-oriented dialogue systems, Large language models

**Relevance Score:** 8

**TL;DR:** This paper presents a novel framework that enhances out-of-scope intent detection in task-oriented dialogue systems by combining uncertainty modeling with fine-tuned large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of out-of-scope intent detection in task-oriented dialogue systems, ensuring robustness to unseen and ambiguous queries.

**Method:** The framework applies uncertainty estimation to outputs from an in-scope intent detection classifier and utilizes a fine-tuned LLM to address high-uncertainty instances for final decision-making.

**Key Contributions:**

	1. Introduction of a modular framework for OOS intent detection
	2. Effective integration of uncertainty modeling with LLMs
	3. Demonstrated state-of-the-art results on OOS detection benchmarks

**Result:** The proposed method achieves state-of-the-art performance on key benchmarks, effectively balancing computational efficiency and performance.

**Limitations:** 

**Conclusion:** The combination of traditional methods with fine-tuned LLMs leads to improved OOS detection in real-world applications.

**Abstract:** Out-of-scope (OOS) intent detection is a critical challenge in task-oriented dialogue systems (TODS), as it ensures robustness to unseen and ambiguous queries. In this work, we propose a novel but simple modular framework that combines uncertainty modeling with fine-tuned large language models (LLMs) for efficient and accurate OOS detection. The first step applies uncertainty estimation to the output of an in-scope intent detection classifier, which is currently deployed in a real-world TODS handling tens of thousands of user interactions daily. The second step then leverages an emerging LLM-based approach, where a fine-tuned LLM is triggered to make a final decision on instances with high uncertainty. Unlike prior approaches, our method effectively balances computational efficiency and performance, combining traditional approaches with LLMs and yielding state-of-the-art results on key OOS detection benchmarks, including real-world OOS data acquired from a deployed TODS.

</details>


### [35] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)

*Quang Minh Nguyen, Taegyoon Kim*

**Main category:** cs.CL

**Keywords:** stance detection, large language models, external information, performance degradation, bias

**Relevance Score:** 8

**TL;DR:** This study evaluates the impact of external information on stance detection performance across large language models, revealing that such information often degrades performance contrary to prior assumptions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether external information, such as Wikipedia and web search excerpts, can enhance stance detection performance in large language models.

**Method:** The study conducts a systematic evaluation involving eight large language models across three datasets with twelve targets to assess the effect of external information on stance detection.

**Key Contributions:**

	1. Systematic evaluation of external information impact on LLMs for stance detection.
	2. Demonstration of performance degradation in LLMs when using external information.
	3. Insights into the alignment of LLM predictions with provided information rather than ground truth.

**Result:** External information predominantly degraded stance detection performance, with macro F1 scores decreasing by up to 27.9%.

**Limitations:** Results may vary across different contexts and datasets; the study focuses on specific models and targets.

**Conclusion:** The research highlights the risk of information biases in LLM-based stance classifiers, contrasting with earlier studies that suggested performance improvement through external information.

**Abstract:** In the stance detection task, a text is classified as either favorable, opposing, or neutral towards a target. Prior work suggests that the use of external information, e.g., excerpts from Wikipedia, improves stance detection performance. However, whether or not such information can benefit large language models (LLMs) remains an unanswered question, despite their wide adoption in many reasoning tasks. In this study, we conduct a systematic evaluation on how Wikipedia and web search external information can affect stance detection across eight LLMs and in three datasets with 12 targets. Surprisingly, we find that such information degrades performance in most cases, with macro F1 scores dropping by up to 27.9\%. We explain this through experiments showing LLMs' tendency to align their predictions with the stance and sentiment of the provided information rather than the ground truth stance of the given text. We also find that performance degradation persists with chain-of-thought prompting, while fine-tuning mitigates but does not fully eliminate it. Our findings, in contrast to previous literature on BERT-based systems which suggests that external information enhances performance, highlight the risks of information biases in LLM-based stance classifiers. Code is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [36] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)

*Shutong Feng, Hsien-chin Lin, Nurul Lubis, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Renato Vukovic, Milica Gai*

**Main category:** cs.CL

**Keywords:** task-oriented dialogue, large language models, reinforcement learning, emotional intelligence, conversational agents

**Relevance Score:** 9

**TL;DR:** This paper presents LUSTER, a novel LLM-based system for task-oriented dialogue that incorporates emotional understanding and reinforcement learning to improve conversational agents' effectiveness.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Advances in large language models have improved fluency and understanding, but effective task-oriented dialogue systems still struggle with emotional intelligence and precise communication in noisy environments.

**Method:** The authors developed a task-oriented dialogue system that integrates architectural, representational, and emotional elements, using an LLM and reinforcement learning to optimize for user sentiment and task success in a challenging evaluation setup.

**Key Contributions:**

	1. Development of LUSTER, an LLM-based ToD system
	2. Integration of emotional considerations into ToD architecture
	3. Use of end-to-end reinforcement learning for optimizing user sentiment and task success

**Result:** The study reveals that blending LLM capabilities with structured rewards enhances the resilience and emotional responsiveness of task-oriented dialogue systems.

**Limitations:** 

**Conclusion:** The findings suggest that LUSTER provides a promising framework for creating next-generation conversational agents that can better understand and respond to user emotions and achieve task goals efficiently.

**Abstract:** Task-oriented dialogue (ToD) systems are designed to help users achieve specific goals through natural language interaction. While recent advances in large language models (LLMs) have significantly improved linguistic fluency and contextual understanding, building effective and emotionally intelligent ToD systems remains a complex challenge. Effective ToD systems must optimise for task success, emotional understanding and responsiveness, and precise information conveyance, all within inherently noisy and ambiguous conversational environments. In this work, we investigate architectural, representational, optimisational as well as emotional considerations of ToD systems. We set up systems covering these design considerations with a challenging evaluation environment composed of a natural-language user simulator coupled with an imperfect natural language understanding module. We propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end \textbf{R}einforcement learning with both short-term (user sentiment) and long-term (task success) rewards. Our findings demonstrate that combining LLM capability with structured reward modelling leads to more resilient and emotionally responsive ToD systems, offering a practical path forward for next-generation conversational agents.

</details>


### [37] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)

*Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Jo Wood, Pranava Madhyastha*

**Main category:** cs.CL

**Keywords:** Chart Question Answering, Dataset, Multimodal Models, Natural Language Processing, Visualization

**Relevance Score:** 7

**TL;DR:** This paper introduces a novel dataset for chart question answering (CQA) derived from visualization notebooks, emphasizing authentic reasoning workflows in a multimodal context.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to create a dataset for chart question answering that reflects real-world scenarios and analytical narratives, addressing the shortcomings of existing benchmarks.

**Method:** The dataset was constructed by compiling visualization notebooks with real multi-view charts and pairing them with natural language questions that resonate with analytical narratives.

**Key Contributions:**

	1. Introduction of a new dataset for chart question answering based on visualization notebooks.
	2. Inclusion of natural language questions grounded in analytical narratives.
	3. Identification of performance challenges in state-of-the-art models in a more authentic CQA context.

**Result:** When benchmarked with state-of-the-art multimodal large language models, it was found that GPT-4.1 achieved an accuracy of 69.3%, highlighting the difficulties posed by this dataset's more realistic setting.

**Limitations:** 

**Conclusion:** The findings indicate a substantial performance gap in chart question answering tasks for current models, calling for further advancements in multimodal understanding.

**Abstract:** We present a new dataset for chart question answering (CQA) constructed from visualization notebooks. The dataset features real-world, multi-view charts paired with natural language questions grounded in analytical narratives. Unlike prior benchmarks, our data reflects ecologically valid reasoning workflows. Benchmarking state-of-the-art multimodal large language models reveals a significant performance gap, with GPT-4.1 achieving an accuracy of 69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [38] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)

*Georgii Levtsov, Dmitry Ustalov*

**Main category:** cs.CL

**Keywords:** NLP, model evaluation, pairwise comparisons, global scores, neural language models

**Relevance Score:** 7

**TL;DR:** This paper investigates the effectiveness of global scores versus pairwise comparisons for model evaluation in NLP, highlighting their respective strengths and weaknesses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve decision-making in selecting appropriate model evaluation strategies for instruction-tuned neural language models in natural language processing.

**Method:** The study involved computational experiments on both synthetic and real-world datasets, employing standard global metrics alongside the Bradley-Terry model for pairwise comparisons.

**Key Contributions:**

	1. Empirical investigation of global scores vs. pairwise comparisons in NLP model evaluation
	2. Foundations for improved evaluation strategies for instruction-tuned neural models
	3. Provision of code and data for reproducibility and further research

**Result:** Global scores tend to offer more reliable overall rankings but may overlook strong models, while pairwise comparisons excel in distinguishing strong models in the presence of lower global scores, although they require more comparisons to reach convergence.

**Limitations:** Pairwise comparisons require more comparisons to converge if there are frequent ties.

**Conclusion:** A balanced approach to model evaluation that considers both global metrics and pairwise comparisons may yield the best insights for selecting models in NLP tasks.

**Abstract:** With the advent of highly capable instruction-tuned neural language models, benchmarking in natural language processing (NLP) is increasingly shifting towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper empirically investigates the strengths and weaknesses of both global scores and pairwise comparisons to aid decision-making in selecting appropriate model evaluation strategies. Through computational experiments on synthetic and real-world datasets using standard global metrics and the popular Bradley-Terry model for pairwise comparisons, we found that while global scores provide more reliable overall rankings, they can underestimate strong models with rare, significant errors or low confidence. Conversely, pairwise comparisons are particularly effective for identifying strong contenders among models with lower global scores, especially where quality metrics are hard to define (e.g., text generation), though they require more comparisons to converge if ties are frequent. Our code and data are available at https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [39] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)

*Rifki Afina Putri*

**Main category:** cs.CL

**Keywords:** transferability, pre-trained language models, sentiment analysis, low-resource languages, MAD-X

**Relevance Score:** 7

**TL;DR:** This paper explores the transferability of pre-trained language models for sentiment analysis in low-resource Indonesian local languages, comparing zero-shot performance and an adapter-based approach.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how well pre-trained language models transfer to low-resource languages, focusing on sentiment analysis across different Indonesian local languages.

**Method:** The study evaluates zero-shot performance and adapter-based transfer using models like monolingual Indonesian BERT, mBERT, and XLM-R on ten local languages, categorized as seen, partially seen, and unseen.

**Key Contributions:**

	1. Empirical analysis of language model transferability to low-resource languages.
	2. Introduction of the MAD-X adapter improving performance without labeled data.
	3. Categorization of languages to analyze model performance disparities.

**Result:** Multilingual models perform best on seen languages, moderately on partially seen ones, and poorly on unseen languages. The MAD-X adapter significantly improves performance, particularly for seen and partially seen languages, without needing labeled data.

**Limitations:** Does not fully explain performance differences by vocabulary overlap and tokenization.

**Conclusion:** Prior exposure of the model to a language is the strongest predictor of transfer success, while tokenization and vocabulary overlap show weak correlation with prediction quality.

**Abstract:** In this paper, we investigate the transferability of pre-trained language models to low-resource Indonesian local languages through the task of sentiment analysis. We evaluate both zero-shot performance and adapter-based transfer on ten local languages using models of different types: a monolingual Indonesian BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based approach called MAD-X. To better understand model behavior, we group the target languages into three categories: seen (included during pre-training), partially seen (not included but linguistically related to seen languages), and unseen (absent and unrelated in pre-training data). Our results reveal clear performance disparities across these groups: multilingual models perform best on seen languages, moderately on partially seen ones, and poorly on unseen languages. We find that MAD-X significantly improves performance, especially for seen and partially seen languages, without requiring labeled data in the target language. Additionally, we conduct a further analysis on tokenization and show that while subword fragmentation and vocabulary overlap with Indonesian correlate weakly with prediction quality, they do not fully explain the observed performance. Instead, the most consistent predictor of transfer success is the model's prior exposure to the language, either directly or through a related language.

</details>


### [40] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)

*Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma*

**Main category:** cs.CL

**Keywords:** multimodal, Large Language Models, meme harmfulness, evaluation framework, machine learning

**Relevance Score:** 4

**TL;DR:** Introducing AdamMeme, a framework for evaluating multimodal Large Language Models' understanding of harmful memes via dynamic, agent-based assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of multimodal Large Language Models (mLLMs) in understanding harmful memes, as existing static benchmarks are inadequate.

**Method:** AdamMeme employs a flexible, agent-based approach that continuously updates meme data and evaluates mLLMs through multi-agent collaboration.

**Key Contributions:**

	1. Introduction of the AdamMeme framework for assessing mLLMs on harmful memes
	2. Dynamic adaptation of evaluations through agent collaboration
	3. Fine-grained analysis of model-specific weaknesses in meme interpretation

**Result:** Extensive experiments demonstrate that AdamMeme effectively reveals varying performances and weaknesses in target mLLMs regarding harmful meme interpretation.

**Limitations:** The framework may require frequent updates to keep pace with rapidly evolving meme content.

**Conclusion:** The AdamMeme framework allows for more comprehensive and adaptable assessments of mLLMs, helping to identify specific limitations in their reasoning about harmfulness.

**Abstract:** The proliferation of multimodal memes in the social media era demands that multimodal Large Language Models (mLLMs) effectively understand meme harmfulness. Existing benchmarks for assessing mLLMs on harmful meme understanding rely on accuracy-based, model-agnostic evaluations using static datasets. These benchmarks are limited in their ability to provide up-to-date and thorough assessments, as online memes evolve dynamically. To address this, we propose AdamMeme, a flexible, agent-based evaluation framework that adaptively probes the reasoning capabilities of mLLMs in deciphering meme harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive evaluations by iteratively updating the meme data with challenging samples, thereby exposing specific limitations in how mLLMs interpret harmfulness. Extensive experiments show that our framework systematically reveals the varying performance of different target mLLMs, offering in-depth, fine-grained analyses of model-specific weaknesses. Our code is available at https://github.com/Lbotirx/AdamMeme.

</details>


### [41] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)

*Aditya Tomar, Rudra Murthy, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** Bias Detection, Stereotype Detection, Language Models, Fair AI, Dataset

**Relevance Score:** 8

**TL;DR:** The paper explores bias and stereotype detection in language models, introducing a dataset called StereoBias. It finds that joint training on these tasks improves detection performance significantly.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the harmful effects of bias and stereotypes in language models, especially in critical applications like content moderation and decision-making.

**Method:** The authors introduce StereoBias, a dataset labeled for bias and stereotype detection across various categories and conduct experiments comparing encoder-only models and fine-tuned decoder-only models using QLoRA.

**Key Contributions:**

	1. Introduction of the StereoBias dataset for bias and stereotype detection.
	2. Demonstration that joint training improves bias detection more than task separation.
	3. Experiments showing that improvements result from the connection between bias and stereotypes.

**Result:** Joint training on bias and stereotype detection enhances model performance, improving bias detection significantly compared to individual training.

**Limitations:** 

**Conclusion:** Leveraging the relationship between bias and stereotypes can lead to the development of fairer and more effective AI systems.

**Abstract:** Bias and stereotypes in language models can cause harm, especially in sensitive areas like content moderation and decision-making. This paper addresses bias and stereotype detection by exploring how jointly learning these tasks enhances model performance. We introduce StereoBias, a unique dataset labeled for bias and stereotype detection across five categories: religion, gender, socio-economic status, race, profession, and others, enabling a deeper study of their relationship. Our experiments compare encoder-only models and fine-tuned decoder-only models using QLoRA. While encoder-only models perform well, decoder-only models also show competitive results. Crucially, joint training on bias and stereotype detection significantly improves bias detection compared to training them separately. Additional experiments with sentiment analysis confirm that the improvements stem from the connection between bias and stereotypes, not multi-task learning alone. These findings highlight the value of leveraging stereotype information to build fairer and more effective AI systems.

</details>


### [42] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)

*Oliver Wardas, Florian Matthes*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Legal AI, Large Language Models, Employment Law, Contract Review

**Relevance Score:** 8

**TL;DR:** The paper explores the use of Large Language Models (LLMs) in classifying clauses of German employment contracts as valid, unfair, or void, addressing challenges in legal NLP applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Legal work presents unique challenges for NLP due to its text-heavy and resource-intensive nature. A need for interpretable and trustworthy data-driven approaches in dynamic legal environments exists.

**Method:** Collaborated with legal experts to extend an existing dataset and evaluate LLMs using in-context learning across three legal contexts: no context, full legal text, and distilled examination guidelines.

**Key Contributions:**

	1. Extended dataset with examination guidelines and legal annotations
	2. Exploration of LLMs in contract legality classification
	3. Comparison of performance using different legal context variants

**Result:** Results indicate that full-text sources moderately improve performance of LLMs, and examination guidelines significantly boost recall for void clauses, achieving an 80% weighted F1-Score.

**Limitations:** LLMs' performance remains considerably below that of human lawyers, indicating limitations in current methodologies.

**Conclusion:** While LLMs show potential in assisting legal reviews, their performance is still significantly lower than that of human lawyers, highlighting both advancements and limitations in the use of AI in legal contexts.

**Abstract:** Legal work, characterized by its text-heavy and resource-intensive nature, presents unique challenges and opportunities for NLP research. While data-driven approaches have advanced the field, their lack of interpretability and trustworthiness limits their applicability in dynamic legal environments. To address these issues, we collaborated with legal experts to extend an existing dataset and explored the use of Large Language Models (LLMs) and in-context learning to evaluate the legality of clauses in German employment contracts. Our work evaluates the ability of different LLMs to classify clauses as "valid," "unfair," or "void" under three legal context variants: no legal context, full-text sources of laws and court rulings, and distilled versions of these (referred to as examination guidelines). Results show that full-text sources moderately improve performance, while examination guidelines significantly enhance recall for void clauses and weighted F1-Score, reaching 80\%. Despite these advancements, LLMs' performance when using full-text sources remains substantially below that of human lawyers. We contribute an extended dataset, including examination guidelines, referenced legal sources, and corresponding annotations, alongside our code and all log files. Our findings highlight the potential of LLMs to assist lawyers in contract legality review while also underscoring the limitations of the methods presented.

</details>


### [43] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)

*Matteo Di Cristofaro*

**Main category:** cs.CL

**Keywords:** tokenisation, corpus linguistics, data representation, linguistic analysis, corpus fidelity

**Relevance Score:** 3

**TL;DR:** This paper explores the impact of tokenisation discrepancies on language data representation and analysis, particularly with challenges involving emojis and homoglyphs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how discrepancies in tokenisation affect linguistic data representation and the validity of analytical findings in corpus linguistics.

**Method:** The study examines tokenisation issues, focusing on emojis and homoglyphs, and presents preprocessing methods for accurate digital text representation in corpora.

**Key Contributions:**

	1. Illuminates how tokenisation discrepancies affect linguistic representation
	2. Provides methods for preprocessing emojis and homoglyphs
	3. Highlights the need for linguistic and technical understanding in corpus analysis

**Result:** The research highlights the importance of careful tokenisation to maintain the fidelity of corpus data and emphasizes the necessity for both linguistic and technical understanding to improve corpus analysis accuracy.

**Limitations:** 

**Conclusion:** Ensuring accurate tokenisation is critical for reliable linguistic analysis, with implications for both quantitative and qualitative research approaches.

**Abstract:** Tokenisation - "the process of splitting text into atomic parts" (Brezina & Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides the basis for any applicable quantitative method (e.g. collocations) while ensuring the reliability of qualitative approaches. This paper examines how discrepancies in tokenisation affect the representation of language data and the validity of analytical findings: investigating the challenges posed by emojis and homoglyphs, the study highlights the necessity of preprocessing these elements to maintain corpus fidelity to the source data. The research presents methods for ensuring that digital texts are accurately represented in corpora, thereby supporting reliable linguistic analysis and guaranteeing the repeatability of linguistic interpretations. The findings emphasise the necessity of a detailed understanding of both linguistic and technical aspects involved in digital textual data to enhance the accuracy of corpus analysis, and have significant implications for both quantitative and qualitative approaches in corpus-based research.

</details>


### [44] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)

*Zhixun Chen, Ping Guo, Wenhan Han, Yifan Zhang, Binbin Liu, Haobin Lin, Fengze Liu, Yan Zhao, Bingni Zhang, Taifeng Wang, Yin Zheng, Meng Fang*

**Main category:** cs.CL

**Keywords:** data quality, large language models, multilingual, MuRating, document evaluation

**Relevance Score:** 8

**TL;DR:** MuRating is a framework that enhances data quality in multilingual large language models by transferring high-quality English data signals to evaluate document quality across 17 languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models by addressing data quality issues in multilingual contexts, particularly when existing methods primarily focus on English.

**Method:** MuRating aggregates multiple English 'raters' through pairwise comparisons to create unified document-quality scores, which are then projected through translation to train a multilingual evaluator.

**Key Contributions:**

	1. Introduction of MuRating framework for multilingual document evaluation
	2. Demonstrated performance improvement of LLaMA model through balanced data selection
	3. Analysis of translation fidelity and selection biases in multilingual training.

**Result:** MuRating selects balanced subsets of English and multilingual content for pretraining a LLaMA model, significantly boosting accuracy on both English and multilingual benchmarks, particularly on knowledge-intensive tasks.

**Limitations:** Focuses on the performance related to selected benchmarks; potential biases in the training data.

**Conclusion:** The framework not only improves performance across multiple languages but also highlights issues such as translation fidelity and selection bias, paving the way for future research.

**Abstract:** Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a single rater for 17 target languages. MuRating aggregates multiple English "raters" via pairwise comparisons to learn unified document-quality scores,then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to strong baselines, including QuRater, AskLLM, DCLM and so on, our approach boosts average accuracy on both English benchmarks and multilingual evaluations, with especially large gains on knowledge-intensive tasks. We further analyze translation fidelity, selection biases, and underrepresentation of narrative material, outlining directions for future work.

</details>


### [45] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)

*Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofsttter*

**Main category:** cs.CL

**Keywords:** evaluation awareness, AI governance, safety evaluations, Llama-3, deception

**Relevance Score:** 8

**TL;DR:** This paper investigates evaluation awareness in Llama-3.3-70B-Instruct, demonstrating its ability to distinguish between testing and deployment phases which has implications for AI governance and safety evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding evaluation awareness is critical for AI governance as it affects the reliability of safety evaluations and industry commitments.

**Method:** The study employs linear probes to analyze Llama-3.3-70B-Instruct's ability to separate prompts from evaluation and deployment phases.

**Key Contributions:**

	1. Demonstrates evaluation awareness in Llama-3.3-70B-Instruct.
	2. Reveals implications for AI governance and safety evaluations.
	3. Suggests leveraging model internals for safety audits.

**Result:** Findings reveal that the model successfully distinguishes between real-world evaluation and deployment prompts, with safety evaluations being classified correctly, indicating that they may seem artificial to the model.

**Limitations:** Limited to the analysis of a specific model (Llama-3.3-70B-Instruct) and its current evaluation methods.

**Conclusion:** The research highlights the importance of trustworthy evaluations and suggests exploring model internals to enhance blackbox methods for safety audits.

**Abstract:** Language models can distinguish between testing and deployment phases -- a capability known as evaluation awareness. This has significant safety and policy implications, potentially undermining the reliability of evaluations that are central to AI governance frameworks and voluntary industry commitments. In this paper, we study evaluation awareness in Llama-3.3-70B-Instruct. We show that linear probes can separate real-world evaluation and deployment prompts, suggesting that current models internally represent this distinction. We also find that current safety evaluations are correctly classified by the probes, suggesting that they already appear artificial or inauthentic to models. Our findings underscore the importance of ensuring trustworthy evaluations and understanding deceptive capabilities. More broadly, our work showcases how model internals may be leveraged to support blackbox methods in safety audits, especially for future models more competent at evaluation awareness and deception.

</details>


### [46] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)

*Tianze Hua, Tian Yun, Ellie Pavlick*

**Main category:** cs.CL

**Keywords:** multimodal AI, vision-language models, conflicting information, internal representations, attention mechanisms

**Relevance Score:** 8

**TL;DR:** This paper investigates the behavior of vision-language models when faced with conflicting information from different input modalities, revealing biases towards certain modalities and suggesting methods for improved model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how multimodal AI models handle conflicting input streams, specifically focusing on vision-language models.

**Method:** The paper tests models with inconsistent inputs (e.g., images and captions that do not match) and analyzes how they report information from different modalities, observing biases and evaluating internal representations and attention mechanisms.

**Key Contributions:**

	1. Identified biases in modality preference among vision-language models when faced with conflicting information.
	2. Revealed internal representation structures and specific attention heads that favor one modality.
	3. Proposed transferability of modality-agnostic router heads to enhance model performance across datasets.

**Result:** Models tend to favor one input modality over the other when reporting information, with variations observed across different models. The study identifies specific attention heads and modality-agnostic router heads that can enhance model performance by restructuring representations and improving handling of conflicting signals.

**Limitations:** The study is limited to vision-language models and may not generalize to other multimodal systems outside this scope.

**Conclusion:** The research contributes to a deeper understanding of multimodal model behavior in the presence of conflicting information, paving the way for future improvements in AI systems that work with complex multimodal data.

**Abstract:** AI models are increasingly required to be multimodal, integrating disparate input streams into a coherent state representation on which subsequent behaviors and actions can be based. This paper seeks to understand how such models behave when input streams present conflicting information. Focusing specifically on vision-language models, we provide inconsistent inputs (e.g., an image of a dog paired with the caption "A photo of a cat") and ask the model to report the information present in one of the specific modalities (e.g., "What does the caption say / What is in the image?"). We find that models often favor one modality over the other, e.g., reporting the image regardless of what the caption says, but that different models differ in which modality they favor. We find evidence that the behaviorally preferred modality is evident in the internal representational structure of the model, and that specific attention heads can restructure the representations to favor one modality over the other. Moreover, we find modality-agnostic "router heads" which appear to promote answers about the modality requested in the instruction, and which can be manipulated or transferred in order to improve performance across datasets and modalities. Together, the work provides essential steps towards identifying and controlling if and how models detect and resolve conflicting signals within complex multimodal environments.

</details>


### [47] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)

*Katharina Beckh, Elisa Studeny, Sujan Sai Gannamaneni, Dario Antweiler, Stefan Rping*

**Main category:** cs.CL

**Keywords:** medical coding, explainable AI, dataset analysis, health informatics, automatic documentation

**Relevance Score:** 8

**TL;DR:** This paper analyzes the MDACE dataset to evaluate explainable medical coding systems, revealing alignment between ground truth evidence and code descriptions, and offers recommendations for system development.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve automatic medical coding processes by enhancing transparency and explainability through the evaluation of existing systems.

**Method:** An in-depth analysis of the MDACE dataset combined with plausibility evaluations of explainable medical coding systems.

**Key Contributions:**

	1. In-depth analysis of the MDACE dataset.
	2. Evaluation of existing explainable medical coding systems.
	3. Recommendations for enhancing automatic medical coding processes.

**Result:** Findings show a significant alignment between ground truth evidence and code descriptions, indicating a viable method for assessing medical coding systems.

**Limitations:** Limited to the evaluation of existing systems and datasets; potential generalization issues beyond the MDACE dataset.

**Conclusion:** The study provides insights into automatic medical coding and emphasizes the need for improved explainability methods, offering recommendations for future development.

**Abstract:** Automatic medical coding has the potential to ease documentation and billing processes. For this task, transparency plays an important role for medical coders and regulatory bodies, which can be achieved using explainability methods. However, the evaluation of these approaches has been mostly limited to short text and binary settings due to a scarcity of annotated data. Recent efforts by Cheng et al. (2023) have introduced the MDACE dataset, which provides a valuable resource containing code evidence in clinical records. In this work, we conduct an in-depth analysis of the MDACE dataset and perform plausibility evaluation of current explainable medical coding systems from an applied perspective. With this, we contribute to a deeper understanding of automatic medical coding and evidence extraction. Our findings reveal that ground truth evidence aligns with code descriptions to a certain degree. An investigation into state-of-the-art approaches shows a high overlap with ground truth evidence. We propose match measures and highlight success and failure cases. Based on our findings, we provide recommendations for developing and evaluating explainable medical coding systems.

</details>


### [48] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)

*Nikita Neveditsin, Pawan Lingras, Vijay Mago*

**Main category:** cs.CL

**Keywords:** parseability, JSON, clinical notes, language models, attribute-value extraction

**Relevance Score:** 9

**TL;DR:** This paper compares the parseability of JSON, YAML, and XML outputs from small language models for extracting attributes from clinical notes, finding JSON most effective.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To identify the most effective serialization format for open attribute-value extraction in clinical settings using language models.

**Method:** A comparative analysis of parseability focused on three serialization formats (JSON, YAML, XML) using small language models.

**Key Contributions:**

	1. Identification of JSON as the most effective serialization format for clinical note extraction.
	2. Analysis of parseability affected by document length and note types.
	3. Insights on error patterns related to specific formats.

**Result:** JSON consistently exhibits the highest parseability, with improvements seen through targeted prompting, but challenges arise with longer documents and specific note types.

**Limitations:** Focused primarily on structured output parseability; other factors in real-world deployment remain unaddressed.

**Conclusion:** The study provides practical guidance for selecting serialization formats and designing prompts for language models in clinical contexts.

**Abstract:** We present a comparative analysis of the parseability of structured outputs generated by small language models for open attribute-value extraction from clinical notes. We evaluate three widely used serialization formats: JSON, YAML, and XML, and find that JSON consistently yields the highest parseability. Structural robustness improves with targeted prompting and larger models, but declines for longer documents and certain note types. Our error analysis identifies recurring format-specific failure patterns. These findings offer practical guidance for selecting serialization formats and designing prompts when deploying language models in privacy-sensitive clinical settings.

</details>


### [49] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)

*Arthur Wuhrmann, Anastasiia Kucherenko, Andrei Kucharavy*

**Main category:** cs.CL

**Keywords:** Large Language Models, transparency, training data, fairness, natural language processing

**Relevance Score:** 9

**TL;DR:** The paper presents a systematic method to analyze how training data influences the outputs of Large Language Models (LLMs), focusing on low-perplexity text spans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance transparency, accountability, privacy, and fairness in LLM outputs by understanding the impact of their training data.

**Method:** A systematic approach is introduced that analyzes low-perplexity sequences generated by LLMs, tracing them back to their original sources in the training data.

**Key Contributions:**

	1. Introduces a systematic analysis of low-perplexity sequences in LLMs.
	2. Quantifies the distribution of training data recall in LLM outputs.
	3. Highlights the limitations of mapping generated text back to training sources.

**Result:** Many low-perplexity spans cannot be directly mapped to the training corpus. For those that can, the study quantifies their occurrence in the source documents, revealing insights into verbatim recall.

**Limitations:** The study primarily focuses on low-perplexity spans, which may not represent all aspects of LLM outputs.

**Conclusion:** The findings pave the way for a deeper understanding of LLM behavior influenced by training data.

**Abstract:** As Large Language Models (LLMs) become increasingly widespread, understanding how specific training data shapes their outputs is crucial for transparency, accountability, privacy, and fairness. To explore how LLMs leverage and replicate their training data, we introduce a systematic approach centered on analyzing low-perplexity sequences - high-probability text spans generated by the model. Our pipeline reliably extracts such long sequences across diverse topics while avoiding degeneration, then traces them back to their sources in the training data. Surprisingly, we find that a substantial portion of these low-perplexity spans cannot be mapped to the corpus. For those that do match, we quantify the distribution of occurrences across source documents, highlighting the scope and nature of verbatim recall and paving a way toward better understanding of how LLMs training data impacts their behavior.

</details>


### [50] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)

*Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation framework, Indic languages, multilingual benchmarking, open-source

**Relevance Score:** 8

**TL;DR:** EKA-EVAL is a comprehensive evaluation framework for multilingual and Indic-specific benchmarks for Large Language Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for evaluation frameworks for linguistically diverse areas like India, surpassing English-centric benchmarks.

**Method:** EKA-EVAL integrates over 35 benchmarks, including 10 Indic-specific datasets, to cover various evaluation categories such as reasoning and reading comprehension, with support for distributed inference and multi-GPU usage.

**Key Contributions:**

	1. First end-to-end evaluation suite for Indic and global LLMs.
	2. Integration of multiple benchmarks including Indic-specific datasets.
	3. Support for advanced features like distributed inference and multi-GPU usage.

**Result:** EKA-EVAL provides broader benchmark coverage compared to existing tools and is the first extensible evaluation suite for both global and Indic LLMs.

**Limitations:** 

**Conclusion:** The framework is open-source and aims to establish a multilingual evaluation ecosystem for LLMs, with planned expansion to over 100 benchmarks.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has intensified the need for evaluation frameworks that go beyond English centric benchmarks and address the requirements of linguistically diverse regions such as India. We present EKA-EVAL, a unified and production-ready evaluation framework that integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning categories like reasoning, mathematics, tool use, long-context understanding, and reading comprehension. Compared to existing Indian language evaluation tools, EKA-EVAL offers broader benchmark coverage, with built-in support for distributed inference, quantization, and multi-GPU usage. Our systematic comparison positions EKA-EVAL as the first end-to-end, extensible evaluation suite tailored for both global and Indic LLMs, significantly lowering the barrier to multilingual benchmarking. The framework is open-source and publicly available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA initiative (https://eka.soket.ai), which aims to scale up to over 100 benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [51] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)

*Kenan Tang, Yanhong Li, Yao Qin*

**Main category:** cs.CL

**Keywords:** multilingual learning, language models, adaptive learning, vocabulary expansion, knowledge graphs

**Relevance Score:** 8

**TL;DR:** DIY-MKG is an open-source tool for multilingual language learning that enables users to create personalized vocabulary knowledge graphs and generates adaptive quizzes using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations in existing language learning tools, particularly for polyglot learners, including lack of cross-linguistic connections, limited customization, and problems with cognitive offloading.

**Method:** DIY-MKG allows users to construct personalized vocabulary graphs by expanding with related words suggested by an LLM, coupled with rich annotation features and adaptive quizzes generated by the system.

**Key Contributions:**

	1. Open-source design enabling personalized vocabulary knowledge graphs.
	2. Integration of LLM for dynamic quiz generation tailored to individual learning needs.
	3. Feedback loop for quiz question refinement through user engagement.

**Result:** The evaluation of DIY-MKG demonstrated reliable vocabulary expansion across languages and high accuracy in quiz generation, confirming the system's effectiveness.

**Limitations:** 

**Conclusion:** DIY-MKG significantly enhances the language learning experience for polyglot users by providing the tools necessary for customized learning and active engagement.

**Abstract:** Existing language learning tools, even those powered by Large Language Models (LLMs), often lack support for polyglot learners to build linguistic connections across vocabularies in multiple languages, provide limited customization for individual learning paces or needs, and suffer from detrimental cognitive offloading. To address these limitations, we design Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system that supports polyglot language learning. DIY-MKG allows the user to build personalized vocabulary knowledge graphs, which are constructed by selective expansion with related words suggested by an LLM. The system further enhances learning through rich annotation capabilities and an adaptive review module that leverages LLMs for dynamic, personalized quiz generation. In addition, DIY-MKG allows users to flag incorrect quiz questions, simultaneously increasing user engagement and providing a feedback loop for prompt refinement. Our evaluation of LLM-based components in DIY-MKG shows that vocabulary expansion is reliable and fair across multiple languages, and that the generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [52] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)

*Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou*

**Main category:** cs.CL

**Keywords:** long language models, small language models, reasoning tasks, distillation, HCI

**Relevance Score:** 8

**TL;DR:** Introducing MiCoTAl, a framework that improves long-form Chain of Thought (CoT) distillation for small language models (SLMs) using intermediate-sized models as teacher assistants.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of small language models in learning long-form reasoning due to their restricted capacity and high computational demands of large models.

**Method:** The MiCoTA framework employs intermediate-sized models as teacher assistants and utilizes intermediate-length CoT sequences to enhance the learning process of SLMs.

**Key Contributions:**

	1. Introduction of MiCoTAl framework for CoT distillation.
	2. Use of intermediate-sized models as teacher assistants.
	3. Demonstrated significant performance enhancements on reasoning tasks.

**Result:** SLMs distilled from large teachers showed poor performance, but using MiCoTA, they achieved significant improvements in reasoning performance on various benchmarks, with scores improving by 3.47 and 3.93 for specific models.

**Limitations:** The work is still in progress and may require further validation and testing.

**Conclusion:** MiCoTA enables SLMs to improve reasoning capabilities, providing a new avenue for effective long-CoT data distillation in language models.

**Abstract:** Large language models (LLMs) excel at reasoning tasks requiring long thought sequences for planning, reflection, and refinement. However, their substantial model size and high computational demands are impractical for widespread deployment. Yet, small language models (SLMs) often struggle to learn long-form CoT reasoning due to their limited capacity, a phenomenon we refer to as the "SLMs Learnability Gap". To address this, we introduce \textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation (MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA employs intermediate-sized models as teacher assistants and utilizes intermediate-length CoT sequences to bridge both the capacity and reasoning length gaps. Our experiments on downstream tasks demonstrate that although SLMs distilled from large teachers can perform poorly, by applying MiCoTA, they achieve significant improvements in reasoning performance. Specifically, Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and 3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform a quantitative experiment demonstrating that our method produces data more closely aligned with base SLM distributions. Our insights pave the way for future research into long-CoT data distillation for SLMs.

</details>


### [53] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)

*Songtao Liu, Peng Liu*

**Main category:** cs.CL

**Keywords:** pruning, large language models, attention heads, adaptive rescaling, model compression

**Relevance Score:** 8

**TL;DR:** This paper presents a novel pruning algorithm for large language models that strategically prunes attention heads in higher layers and introduces an adaptive rescaling parameter to maintain representation scale, leading to improved performance over existing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of pruning large language models by addressing the limitations of conventional training-free structured pruning methods that do not consider the architectural positioning of attention heads.

**Method:** A novel pruning algorithm that selectively prunes attention heads from higher layers of LLMs, along with an adaptive rescaling parameter to adjust token representations post-pruning.

**Key Contributions:**

	1. Introduction of a strategic pruning algorithm targeting higher layers of LLMs
	2. Adaptive rescaling parameter to maintain token representation scale after pruning
	3. Demonstrated performance improvements across multiple LLMs and datasets.

**Result:** The proposed method outperforms traditional structured pruning techniques across various large language models in both generation and discriminative tasks, particularly excelling in generation tasks.

**Limitations:** 

**Conclusion:** The new approach to pruning and representation rescaling enables significant performance improvements in LLMs, indicating that attention head positioning and post-pruning adjustments are crucial for effective model compression.

**Abstract:** Pruning is a highly effective approach for compressing large language models (LLMs), significantly reducing inference latency. However, conventional training-free structured pruning methods often employ a heuristic metric that indiscriminately removes some attention heads across all pruning layers, without considering their positions within the network architecture. In this work, we propose a novel pruning algorithm that strategically prunes attention heads in the model's higher layers. Since the removal of attention heads can alter the magnitude of token representations, we introduce an adaptive rescaling parameter that calibrates the representation scale post-pruning to counteract this effect. We conduct comprehensive experiments on a wide range of LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our evaluation includes both generation and discriminative tasks across 27 datasets. The results consistently demonstrate that our method outperforms existing structured pruning methods. This improvement is particularly notable in generation tasks, where our approach significantly outperforms existing baselines.

</details>


### [54] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)

*Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** AI for Research, Large Language Models, Taxonomy, Research Gaps, Applications

**Relevance Score:** 7

**TL;DR:** A comprehensive survey on the application of AI in scientific research, addressing existing gaps and providing a systematic taxonomy for AI4Research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a unified understanding and survey of AI applications in the research process due to recent advancements in large language models.

**Method:** Introduce a systematic taxonomy for classifying five mainstream tasks in AI4Research, identify research gaps and future directions, and compile various resources and applications.

**Key Contributions:**

	1. Systematic taxonomy for classifying tasks in AI4Research
	2. Identification of key research gaps and future directions
	3. Compilation of multidisciplinary applications and resources

**Result:** The survey categorizes tasks in AI4Research, highlights research gaps, and provides an extensive list of resources and applications.

**Limitations:** 

**Conclusion:** The work aims to stimulate innovative breakthroughs in AI4Research by providing the research community with better access to resources.

**Abstract:** Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [55] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)

*Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Large Language Models, Multi-objective Optimization, Gradient Descent

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach, Gradient-Adaptive Policy Optimization (GAPO), for aligning large language models with human preferences by framing the problem as multi-objective optimization, showing superior results in both helpfulness and harmlessness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of effectively aligning LLMs with diverse human preferences, especially when those preferences conflict.

**Method:** Introduction of Gradient-Adaptive Policy Optimization (GAPO), which employs multiple-gradient descent to optimally balance trade-offs between conflicting human values in LLM alignment.

**Key Contributions:**

	1. Introduction of GAPO for human preference alignment in LLMs.
	2. Development of P-GAPO that considers user preferences across multiple objectives.
	3. Demonstration of superior performance compared to existing methods.

**Result:** GAPO outperforms current state-of-the-art methods in aligning LLMs, showing improved performance on metrics of helpfulness and harmlessness on the Mistral-7B model.

**Limitations:** 

**Conclusion:** GAPO converges towards a Pareto optimal solution, offering a promising approach for addressing the multi-objective nature of human value alignment in LLMs.

**Abstract:** Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful technique for aligning large language models (LLMs) with human preferences. However, effectively aligning LLMs with diverse human preferences remains a significant challenge, particularly when they are conflict. To address this issue, we frame human value alignment as a multi-objective optimization problem, aiming to maximize a set of potentially conflicting objectives. We introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning paradigm that employs multiple-gradient descent to align LLMs with diverse preference distributions. GAPO adaptively rescales the gradients for each objective to determine an update direction that optimally balances the trade-offs between objectives. Additionally, we introduce P-GAPO, which incorporates user preferences across different objectives and achieves Pareto solutions that better align with the user's specific needs. Our theoretical analysis demonstrates that GAPO converges towards a Pareto optimal solution for multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms current state-of-the-art methods, achieving superior performance in both helpfulness and harmlessness.

</details>


### [56] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)

*Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, Xian Li*

**Main category:** cs.CL

**Keywords:** natural language processing, reasoning traces, machine learning, student model, teacher model

**Relevance Score:** 8

**TL;DR:** The paper investigates the effectiveness of distilling reasoning traces from a teacher model to improve a student model's reasoning capabilities, demonstrating that carefully selected, challenging examples are more effective than random sampling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how reasoning demonstrations from a teacher model can be systematically used to boost the reasoning skills of a smaller student model.

**Method:** A systematic analysis of distilling reasoning capabilities is conducted, involving the selection of high-quality reasoning traces ('NaturalThoughts') from a teacher model and evaluating their impact on student models.

**Key Contributions:**

	1. Curated high-quality reasoning traces ('NaturalThoughts') for model training.
	2. Systematic analysis of sample efficiency and scalability in reasoning task training.
	3. Demonstrated superior performance of 'NaturalThoughts' compared to existing datasets on STEM benchmarks.

**Result:** Training with 'NaturalThoughts' significantly outperforms traditional reasoning datasets on various STEM reasoning benchmarks, demonstrating improved sample efficiency.

**Limitations:** 

**Conclusion:** Selecting challenging examples that require diverse reasoning strategies is a more effective approach for transferring reasoning skills from larger teacher models to smaller student models.

**Abstract:** Recent work has shown that distilling reasoning traces from a larger teacher model via supervised finetuning outperforms reinforcement learning with the smaller student model alone (Guo et al. 2025). However, there has not been a systematic study of what kind of reasoning demonstrations from the teacher are most effective in improving the student model's reasoning capabilities. In this work we curate high-quality "NaturalThoughts" by selecting reasoning traces from a strong teacher model based on a large pool of questions from NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of factors that affect distilling reasoning capabilities, in terms of sample efficiency and scalability for general reasoning tasks. We observe that simply scaling up data size with random sampling is a strong baseline with steady performance gains. Further, we find that selecting difficult examples that require more diverse reasoning strategies is more sample-efficient to transfer the teacher model's reasoning skills. Evaluated on both Llama and Qwen models, training with NaturalThoughts outperforms existing reasoning datasets such as OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [57] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)

*Yu-Shiang Huang, Chuan-Ju Wang, Chung-Chi Chen*

**Main category:** cs.CL

**Keywords:** natural language generation, decision-making, human-LM collaboration, evaluation framework, financial text

**Relevance Score:** 8

**TL;DR:** This paper introduces a decision-oriented framework for evaluating natural language generation in high-stakes domains by measuring its influence on human and LLM decision outcomes, finding the importance of analytical commentary in enhancing decision-making.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of conventional intrinsic evaluation methods in assessing the efficacy of generated text in decision-making contexts.

**Method:** The authors employ market digest texts to evaluate decision-making quality based on financial performance of trades executed by human investors and LLM agents informed by these texts.

**Key Contributions:**

	1. Proposes a novel decision-oriented evaluation framework for NLG based on decision outcomes.
	2. Demonstrates that analytical commentaries can enhance the performance of human-LLM teams compared to using plain summaries.
	3. Highlights the limitations of traditional evaluation methods in high-stakes domains.

**Result:** Neither humans nor LLMs outperform random performance using summaries alone, but richer analytical commentaries lead to significant improvements in collaborative decision-making outcomes.

**Limitations:** The performance impacts were constrained by reliance solely on specific market digest text types and may not generalize to all high-stakes domains.

**Conclusion:** The study emphasizes that traditional metrics for text generation are insufficient and advocates for evaluating text based on its facilitation of decision-making between humans and LLMs.

**Abstract:** Natural language generation (NLG) is increasingly deployed in high-stakes domains, yet common intrinsic evaluation methods, such as n-gram overlap or sentence plausibility, weakly correlate with actual decision-making efficacy. We propose a decision-oriented framework for evaluating generated text by directly measuring its influence on human and large language model (LLM) decision outcomes. Using market digest texts--including objective morning summaries and subjective closing-bell analyses--as test cases, we assess decision quality based on the financial performance of trades executed by human investors and autonomous LLM agents informed exclusively by these texts. Our findings reveal that neither humans nor LLM agents consistently surpass random performance when relying solely on summaries. However, richer analytical commentaries enable collaborative human-LLM teams to outperform individual human or agent baselines significantly. Our approach underscores the importance of evaluating generated text by its ability to facilitate synergistic decision-making between humans and LLMs, highlighting critical limitations of traditional intrinsic metrics.

</details>


### [58] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)

*Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, low-resource languages, Wav2Vec-BERT, Whisper, performance comparison

**Relevance Score:** 6

**TL;DR:** This study compares the performance of OpenAI's Whisper and Facebook's Wav2Vec-BERT ASR models on Bangla, a low-resource language, finding Wav2Vec-BERT superior in efficiency and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and improve Automatic Speech Recognition systems for low-resource languages by comparing state-of-the-art models.

**Method:** Experiments were conducted using Mozilla Common Voice-17 and OpenSLR datasets, focusing on fine-tuning and hyperparameter optimization to assess model performance based on WER, CER, training time, and computational efficiency.

**Key Contributions:**

	1. Comparison of two advanced ASR models on low-resource language
	2. Evidence of Wav2Vec-BERT's superiority in efficiency and performance
	3. Insights into ASR model optimization for low-resource languages

**Result:** Wav2Vec-BERT outperformed Whisper in all key metrics tested, including lower WER and CER, faster training times, and better computational efficiency.

**Limitations:** 

**Conclusion:** The study suggests that Wav2Vec-BERT is more suitable for developing efficient speech recognition systems for low-resource languages like Bangla.

**Abstract:** In recent years, neural models trained on large multilingual text and speech datasets have shown great potential for supporting low-resource languages. This study investigates the performances of two state-of-the-art Automatic Speech Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to evaluate model performances. Through systematic fine-tuning and hyperparameter optimization, including learning rate, epochs, and model checkpoint selection, we have compared the models based on Word Error Rate (WER), Character Error Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model outperformed Whisper across all key evaluation metrics, demonstrated superior performance while requiring fewer computational resources, and offered valuable insights to develop robust speech recognition systems in low-resource linguistic settings.

</details>


### [59] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)

*Adrian de Wynter, Tangming Yuan*

**Main category:** cs.CL

**Keywords:** large language models, dialogue comprehension, persuasive debates, argumentation theory, AI evaluation

**Relevance Score:** 9

**TL;DR:** This paper evaluates large language models' (LLMs) ability to maintain persuasive debates and their comprehension of dialogical structures. While LLMs can effectively sway beliefs in dialogues, they lack true understanding of context and deeper dialogue structures, leading to limitations in their use as evaluators.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are increasingly deployed in critical areas such as chatbots and mental health, their reasoning capabilities and understanding of dialogue need rigorous examination to ensure their effectiveness and reliability.

**Method:** The study evaluates the ability of LLMs to maintain debates and measures this against their comprehension of dialogue structures and pragmatic context.

**Key Contributions:**

	1. Evaluated the debate-maintaining abilities of LLMs.
	2. Demonstrated the disconnect between effective dialogue and understanding of context.
	3. Highlighted implications for argumentation theory regarding LLMs as evaluators.

**Result:** LLMs can maintain coherent and persuasive debates, swaying participants' beliefs, but when tested for deeper comprehension of dialogue, they fail to demonstrate understanding, revealing limitations in their use as evaluators.

**Limitations:** LLMs' understanding of the context in dialogues is lacking, which affects their evaluation capabilities.

**Conclusion:** Effective dialogue performance does not require true understanding, suggesting that pragmatic context and coherence are secondary to the persuasive effectiveness of LLMs.

**Abstract:** Large language models (LLMs) are excellent at maintaining high-level, convincing dialogues. They are being fast deployed as chatbots and evaluators in sensitive areas, such as peer review and mental health applications. This, along with the disparate accounts on their reasoning capabilities, calls for a closer examination of LLMs and their comprehension of dialogue. In this work we begin by evaluating LLMs' ability to maintain a debate--one of the purest yet most complex forms of human communication. Then we measure how this capability relates to their understanding of what is being talked about, namely, their comprehension of dialogical structures and the pragmatic context. We find that LLMs are capable of maintaining coherent, persuasive debates, often swaying the beliefs of participants and audiences alike. We also note that awareness or suspicion of AI involvement encourage people to be more critical of the arguments made. When polling LLMs on their comprehension of deeper structures of dialogue, however, they cannot demonstrate said understanding. Our findings tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand the context. More broadly, for the field of argumentation theory we posit that, if an agent can convincingly maintain a dialogue, it is not necessary for it to know what it is talking about. Hence, the modelling of pragmatic context and coherence are secondary to effectiveness.

</details>


### [60] [Don't Say No: Jailbreaking LLM by Suppressing Refusal](https://arxiv.org/abs/2404.16369)

*Yukai Zhou, Jian Lou, Zhijie Huang, Zhan Qin, Yibei Yang, Wenjie Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety alignment, jailbreaking attacks, DSN attack, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces the DSN attack, a novel method designed to enhance the safety alignment of Large Language Models (LLMs) by preventing the generation of toxic content, achieving state-of-the-art success rates in evading safety measures.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the safety alignment of LLMs against jailbreaking attacks and ensure they generate responses consistent with human values.

**Method:** The study critiques traditional approaches to jailbreaking and proposes a new attack method named DSN, which integrates a cosine decay schedule with refusal suppression strategies to better manipulate LLM responses.

**Key Contributions:**

	1. Introduction of the DSN attack methodology
	2. Demonstration of state-of-the-art attack success rates
	3. Enhanced adaptability to unseen datasets and black-box models.

**Result:** DSN outperforms existing methods in success rates and shows strong adaptability to various datasets and models.

**Limitations:** 

**Conclusion:** The proposed DSN approach enhances LLM response safety and offers significant improvements over traditional jailbreaking attacks, demonstrating high universality and effectiveness.

**Abstract:** Ensuring the safety alignment of Large Language Models (LLMs) is critical for generating responses consistent with human values. However, LLMs remain vulnerable to jailbreaking attacks, where carefully crafted prompts manipulate them into producing toxic content. One category of such attacks reformulates the task as an optimization problem, aiming to elicit affirmative responses from the LLM. However, these methods heavily rely on predefined objectionable behaviors, limiting their effectiveness and adaptability to diverse harmful queries. In this study, we first identify why the vanilla target loss is suboptimal and then propose enhancements to the loss objective. We introduce DSN (Don't Say No) attack, which combines a cosine decay schedule method with refusal suppression to achieve higher success rates. Extensive experiments demonstrate that DSN outperforms baseline attacks and achieves state-of-the-art attack success rates (ASR). DSN also shows strong universality and transferability to unseen datasets and black-box models.

</details>


### [61] [Divergent Creativity in Humans and Large Language Models](https://arxiv.org/abs/2405.13012)

*Antoine Bellemare-Pepin, Franois Lespinasse, Philipp Thlke, Yann Harel, Kory Mathewson, Jay A. Olson, Yoshua Bengio, Karim Jerbi*

**Main category:** cs.CL

**Keywords:** Large Language Models, semantic diversity, human creativity, computational creativity, divergent thinking

**Relevance Score:** 9

**TL;DR:** This paper evaluates the semantic diversity of Large Language Models (LLMs) compared to human divergent thinking, revealing that LLMs can outperform average humans but not highly creative individuals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate LLMs' semantic diversity and compare it to human divergent thinking, addressing misconceptions about AI and creativity.

**Method:** Analyzed semantic divergence in LLMs and a dataset of 100,000 humans using the Divergent Association Task and established objective measures.

**Key Contributions:**

	1. Systematic evaluation framework for LLMs' semantic diversity
	2. Benchmarking of LLMs against human creativity
	3. Techniques for improving LLM outputs in terms of semantic diversity

**Result:** LLMs can surpass average human performance in semantic diversity but do not reach the level of highly creative individuals, indicating limitations in their creative abilities.

**Limitations:** Current LLMs still cannot surpass the typical performance of highly creative humans.

**Conclusion:** While LLMs show promise in creative tasks, they do not replace human creative labor due to inherent limitations in their outputs compared to highly creative humans. Techniques for enhancing LLM outputs are suggested.

**Abstract:** The recent surge of Large Language Models (LLMs) has led to claims that they are approaching a level of creativity akin to human capabilities. This idea has sparked a blend of excitement and apprehension. However, a critical piece that has been missing in this discourse is a systematic evaluation of LLMs' semantic diversity, particularly in comparison to human divergent thinking. To bridge this gap, we leverage recent advances in computational creativity to analyze semantic divergence in both state-of-the-art LLMs and a substantial dataset of 100,000 humans. We found evidence that LLMs can surpass average human performance on the Divergent Association Task, and approach human creative writing abilities, though they fall short of the typical performance of highly creative humans. Notably, even the top performing LLMs are still largely surpassed by highly creative individuals, underscoring a ceiling that current LLMs still fail to surpass. Our human-machine benchmarking framework addresses the polemic surrounding the imminent replacement of human creative labour by AI, disentangling the quality of the respective creative linguistic outputs using established objective measures. While prompting deeper exploration of the distinctive elements of human inventive thought compared to those of AI systems, we lay out a series of techniques to improve their outputs with respect to semantic diversity, such as prompt design and hyper-parameter tuning.

</details>


### [62] [Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions](https://arxiv.org/abs/2408.02544)

*Xinbei Ma, Yiting Wang, Yao Yao, Tongxin Yuan, Aston Zhang, Zhuosheng Zhang, Hai Zhao*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Multimodal Large Language Models, User Interface Agents, Distraction, Adversarial Environments

**Relevance Score:** 8

**TL;DR:** This paper explores the distraction susceptibility of multimodal large language model agents in GUI environments and proposes methods to enhance their faithfulness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the faithfulness of multimodal large language model agents in GUI settings and understand their susceptibility to distractions from the environment.

**Method:** Evaluated a variety of multimodal large language models as GUI agents using a simulated dataset, with experiments designed around three different working patterns that exhibit varying levels of perception.

**Key Contributions:**

	1. Assessment of MLLM agents' distraction susceptibility in a GUI context
	2. Introduction of adversarial environment injection for improving agent performance
	3. First study indicating the distraction risk of these agents from benign but irrelevant environmental content.

**Result:** Experimental results demonstrate that all evaluated models, regardless of being generalist or specialist GUI agents, show a tendency to be distracted by unrelated environmental content.

**Limitations:** The scope is limited to benign environments; further studies needed in various contextual setups.

**Conclusion:** The findings highlight the need for further research on improving the distraction resilience of multimodal agents and also suggest exploring adversarial environment injection methods to enhance agent faithfulness.

**Abstract:** This paper investigates the faithfulness of multimodal large language model (MLLM) agents in a graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general scenario is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using a simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness of agents, our findings first indicate that these agents are prone to environmental distractions. Furthermore, we implement an adversarial environment injection and analyze the approach to improve faithfulness, calling for a collective focus on this important topic.

</details>


### [63] [Unifying Global and Near-Context Biasing in a Single Trie Pass](https://arxiv.org/abs/2409.13514)

*Iuliia Thorbecke, Esa Villatoro-Tello, Juan Zuluaga-Gomez, Shashi Kumar, Sergio Burdisso, Pradeep Rangappa, Andrs Carofilis, Srikanth Madikeri, Petr Motlicek, Karthik Pandia, Kadri Haciolu, Andreas Stolcke*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, named entities, n-gram language model, biasing strategies, context adaptation

**Relevance Score:** 4

**TL;DR:** This paper investigates challenges in automatic speech recognition (ASR) for recognizing rare words and adapting to new domains. It proposes a novel method combining a named entity bias list and an n-gram language model to enhance ASR performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the difficulties in recognizing rare, out-of-vocabulary words, particularly named entities, and to improve adaptability in ASR systems with minimal computational costs.

**Method:** The authors introduce a practical approach that unites a named entity bias list with a word-level n-gram language model, integrating this method into a transducer-based ASR system.

**Key Contributions:**

	1. Combines named entity biasing with n-gram language model
	2. Demonstrates significant improvements in entity recognition
	3. Integrates efficiently into existing ASR systems with little overhead

**Result:** The proposed method improves entity recognition by up to 32% relative and reduces overall word error rate (WER) by up to 12% relative, tested across three datasets in four languages.

**Limitations:** 

**Conclusion:** This approach balances simplicity and effectiveness, leading to better ASR performance and entity recognition.

**Abstract:** Despite the success of end-to-end automatic speech recognition (ASR) models, challenges persist in recognizing rare, out-of-vocabulary words - including named entities (NE) - and in adapting to new domains using only text data. This work presents a practical approach to address these challenges through an unexplored combination of an NE bias list and a word-level n-gram language model (LM). This solution balances simplicity and effectiveness, improving entities' recognition while maintaining or even enhancing overall ASR performance. We efficiently integrate this enriched biasing method into a transducer-based ASR system, enabling context adaptation with almost no computational overhead. We present our results on three datasets spanning four languages and compare them to state-of-the-art biasing strategies. We demonstrate that the proposed combination of keyword biasing and n-gram LM improves entity recognition by up to 32% relative and reduces overall WER by up to a 12% relative.

</details>


### [64] [Guaranteed Generation from Large Language Models](https://arxiv.org/abs/2410.06716)

*Minbeom Kim, Thibaut Thonet, Jos Rozen, Hwaran Lee, Kyomin Jung, Marc Dymetman*

**Main category:** cs.CL

**Keywords:** large language models, constraint satisfaction, text generation, GUARD, inference efficiency

**Relevance Score:** 9

**TL;DR:** The paper presents GUARD, a method that ensures constraint satisfaction in text generation from large language models while maintaining the original distribution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing use of large language models (LLMs), there is a need for mechanisms to control text generation to adhere to specific constraints without losing the quality of the output.

**Method:** GUARD combines training-time approaches with inference-time rejection sampling to enforce constraints while controlling the KL divergence between proposal and ideal distributions.

**Key Contributions:**

	1. Introduction of GUARD for guaranteed text generation with constraints
	2. Theoretical framework linking KL divergence control to inference efficiency
	3. Extensive experimentation validating GUARD's performance in challenging scenarios

**Result:** GUARD achieves perfect constraint satisfaction in text generation scenarios with minimal impact on the quality of the original output distribution and significantly improves inference efficiency.

**Limitations:** 

**Conclusion:** GUARD offers a viable solution to the challenge of generating text that meets strict constraints, demonstrating effective constraint enforcement without sacrificing generative performance.

**Abstract:** As large language models (LLMs) are increasingly used across various applications, there is a growing need to control text generation to satisfy specific constraints or requirements. This raises a crucial question: Is it possible to guarantee strict constraint satisfaction in generated outputs while preserving the distribution of the original model as much as possible? We first define the ideal distribution - the one closest to the original model, which also always satisfies the expressed constraint - as the ultimate goal of guaranteed generation. We then state a fundamental limitation, namely that it is impossible to reach that goal through autoregressive training alone. This motivates the necessity of combining training-time and inference-time methods to enforce such guarantees. Based on this insight, we propose GUARD, a simple yet effective approach that combines an autoregressive proposal distribution with rejection sampling. Through GUARD's theoretical properties, we show how controlling the KL divergence between a specific proposal and the target ideal distribution simultaneously optimizes inference speed and distributional closeness. To validate these theoretical concepts, we conduct extensive experiments on two text generation settings with hard-to-satisfy constraints: a lexical constraint scenario and a sentiment reversal scenario. These experiments show that GUARD achieves perfect constraint satisfaction while almost preserving the ideal distribution with highly improved inference efficiency. GUARD provides a principled approach to enforcing strict guarantees for LLMs without compromising their generative capabilities.

</details>


### [65] [A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions](https://arxiv.org/abs/2412.05563)

*Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Z. Ren, Anirudha Majumdar*

**Main category:** cs.CL

**Keywords:** large language models, uncertainty quantification, hallucinations, AI applications, robotics

**Relevance Score:** 9

**TL;DR:** This survey reviews uncertainty quantification methods for large language models (LLMs), focusing on their detection of hallucinations and applications in various AI fields.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** With the integration of LLMs into society, there are concerns about their reliability due to the generation of factually incorrect responses, known as hallucinations.

**Method:** The paper reviews existing methods of uncertainty quantification for LLMs and categorizes them into a unified taxonomy to enhance understanding.

**Key Contributions:**

	1. Extensive review of uncertainty quantification methods for LLMs
	2. Taxonomy of methods to aid understanding
	3. Identification of applications across different AI domains

**Result:** The survey identifies strengths and weaknesses of current uncertainty quantification methods, and discusses their applications in chatbots, text applications, and robotics.

**Limitations:** 

**Conclusion:** The study concludes with open challenges in LLM uncertainty quantification, encouraging further research.

**Abstract:** The remarkable performance of large language models (LLMs) in content generation, coding, and common-sense reasoning has spurred widespread integration into many facets of society. However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-incorrect responses, which are expressed with striking confidence. Previous work has shown that hallucinations and other non-factual responses generated by LLMs can be detected by examining the uncertainty of the LLM in its response to the pertinent prompt, driving significant research efforts devoted to quantifying the uncertainty of LLMs. This survey seeks to provide an extensive review of existing uncertainty quantification methods for LLMs, identifying their salient features, along with their strengths and weaknesses. We present existing methods within a relevant taxonomy, unifying ostensibly disparate methods to aid understanding of the state of the art. Furthermore, we highlight applications of uncertainty quantification methods for LLMs, spanning chatbot and textual applications to embodied artificial intelligence applications in robotics. We conclude with open research challenges in uncertainty quantification of LLMs, seeking to motivate future research.

</details>


### [66] [Text to Band Gap: Pre-trained Language Models as Encoders for Semiconductor Band Gap Prediction](https://arxiv.org/abs/2501.03456)

*Ying-Ting Yeh, Janghoon Ock, Shagun Maheshwari, Amir Barati Farimani*

**Main category:** cs.CL

**Keywords:** semiconductor materials, band gap prediction, transformer models, materials informatics, language representation

**Relevance Score:** 7

**TL;DR:** The paper explores transformer-based language models for predicting semiconductor material band gaps using textual representations, outperforming conventional methods while minimizing feature engineering.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational intensity of quantum chemistry simulations and the data preprocessing challenges of shallow ML models, this study aims to utilize pretrained language models for predicting material properties directly from textual data.

**Method:** The authors employ transformer-based models like RoBERTa, T5, and LLaMA, using structured strings and natural language narratives as input, followed by finetuning on a dataset of inorganic compounds with custom regression heads.

**Key Contributions:**

	1. Demonstrates the effectiveness of finetuned language models for predicting band gaps from textual data.
	2. Introduces a scalable, language-native framework for materials informatics that reduces the need for feature engineering.
	3. Shows that the LLaMA-3 architecture provides competitive accuracy with minimal finetuning.

**Result:** Finetuned LLaMA-3 achieves superior prediction performance (MAE of 0.25 eV and R2 of 0.89) compared to the best shallow ML baseline (MAE of 0.32 eV and R2 of 0.84), with minimal finetuning required.

**Limitations:** 

**Conclusion:** The findings demonstrate the capabilities of language models in scientific property prediction, offering a scalable framework for materials informatics without extensive manual feature extraction.

**Abstract:** We investigate the use of transformer-based language models, RoBERTa, T5, and LLaMA, for predicting the band gaps of semiconductor materials directly from textual representations that encode key material features such as chemical composition, crystal system, space group, number of atoms per unit cell, valence electron count, and other relevant electronic and structural properties. Quantum chemistry simulations such as DFT provide accurate predictions but are computationally intensive, limiting their feasibility for large-scale materials screening. Shallow ML models offer faster alternatives but typically require extensive data preprocessing to convert non-numerical material features into structured numerical inputs, often at the cost of losing critical descriptive information. In contrast, our approach leverages pretrained language models to process textual data directly, eliminating the need for manual feature engineering. We construct material descriptions in two formats: structured strings that combine key features in a consistent template, and natural language narratives generated using the ChatGPT API. For each model, we append a custom regression head and perform task-specific finetuning on a curated dataset of inorganic compounds. Our results show that finetuned language models, particularly the decoder-only LLaMA-3 architecture, can outperform conventional approaches in prediction accuracy and flexibility, achieving an MAE of 0.25 eV and R2 of 0.89, compared to the best shallow ML baseline, which achieved an MAE of 0.32 eV and R2 of 0.84. Notably, LLaMA-3 achieves competitive accuracy with minimal finetuning, suggesting its architecture enables more transferable representations for scientific tasks. This work demonstrates the effectiveness of finetuned language models for scientific property prediction and provides a scalable, language-native framework for materials informatics.

</details>
