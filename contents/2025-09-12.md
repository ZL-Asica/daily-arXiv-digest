# 2025-09-12

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 18]

- [cs.CL](#cs.CL) [Total: 50]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [A Contextual Bandits Approach for Personalization of Hand Gesture Recognition](https://arxiv.org/abs/2509.08915)

*Duke Lin, Michael Paskett, Ying Yang*

**Main category:** cs.HC

**Keywords:** human-computer interaction, gesture recognition, contextual multi-arm bandit, personalization, reinforcement learning

**Relevance Score:** 9

**TL;DR:** This paper presents a calibrationless longitudinal personalization method for hand gesture recognition using a contextual multi-arm bandit algorithm that enhances model performance without requiring labeled data from users.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Static supervised learning models in human-computer interaction often fail due to individual variability. Personalizing models typically requires calibration, which can introduce friction and struggle with limited data. This work aims to overcome these challenges.

**Method:** The authors propose a contextual multi-arm bandit algorithm integrated with a pretrained neural network to personalize gesture recognition without requiring calibration. This approach uses binary rewards from users or inferred by the system.

**Key Contributions:**

	1. Introduction of a calibrationless longitudinal personalization method for gesture recognition
	2. Significant improvements in false negative rates and average precision in user studies
	3. Utilization of contextual multi-arm bandit algorithms for user-specific personalization

**Result:** In a user study involving a sEMG device and a navigation game with hand gestures, the proposed method reduced the average false negative rate by 0.113 and showed an upward trend in average precision by 0.139 from the start to the end of rounds between sessions.

**Limitations:** 

**Conclusion:** The proposed personalization method significantly enhances the performance of gesture recognition systems without requiring user calibration, resulting in increased accessibility for users who previously struggled with baseline models.

**Abstract:** In human-computer interaction applications like hand gesture recognition, supervised learning models are often trained on a large population of users to achieve high task accuracy. However, due to individual variability in sensor signals and user behavior, static models may not provide optimal performance for all users. Personalizing pretrained models via calibration--collecting labeled data from each user--can improve performance but introduces user friction and struggles with limited data. To overcome these issues, we propose a calibrationless longitudinal personalization method: a contextual multi-arm bandit (MAB) algorithm combined with a pretrained neural network for gesture recognition. This reinforcement-learning-style approach enables personalization using binary reward signals, either user-provided or inferred by the system.   We validated this method in a user study. Participants wore a surface electromyography (sEMG) device and played multiple rounds of a 2-D navigation game using six hand gestures. In the session, they completed a baseline round and then a round with our algorithm; in the second session, they played another round with our algorithm. Our approach led to a significant reduction in users' average false negative rate by 0.113 from the initial to the final round, with further decreases between sessions. Average precision also trended upward (by 0.139) from the start to end of a round, continuing in the next session. Notably, some users who could not complete the game with the baseline model succeeded with our contextual MAB model. In summary, our

</details>


### [2] [Characterizing Multimodal Interaction in Visualization Authoring Tools](https://arxiv.org/abs/2509.08953)

*Astrid van den Brandt, Sehi L'Yi, Huyen N. Nguyen, Anna Vilanova, Nils Gehlenborg*

**Main category:** cs.HC

**Keywords:** Multimodal interaction, Visualization authoring, Design implications

**Relevance Score:** 7

**TL;DR:** The paper provides a comprehensive overview of multimodal interaction in visualization authoring tools, highlighting existing practices and design implications for future research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of a comprehensive overview regarding the diverse characteristics of multimodal interaction in visualization authoring tools.

**Method:** The authors reviewed 20 visualization authoring tools that incorporate multimodal interaction to characterize their application and features.

**Key Contributions:**

	1. Systematic overview of multimodal interaction in visualization authoring tools.
	2. Identification of design implications for tool developers.
	3. Guidance on future research directions in the field.

**Result:** The review highlights the distinguishing features among the tools and outlines design implications and future research directions.

**Limitations:** 

**Conclusion:** The findings can guide designers in creating more accessible and effective authoring systems by enhancing understanding of current practices in multimodal interaction.

**Abstract:** Multimodal interaction has been increasingly considered in designing visualization authoring tools. However, multimodal interaction has a broad meaning in visualization authoring, according to our literature review. Although some previous studies compare different authoring tools, a comprehensive overview of the diverse characteristics of multimodal interaction in visualization authoring tools is still missing. This paper seeks to offer a systematic perspective on how multimodal interaction is integrated within visualization authoring tools. Such an overview can enhance understanding of current practices, highlight distinguishing features among tools, and help identify future research directions, guiding designers in developing more accessible and effective authoring systems. We review 20 visualization authoring tools that incorporate multimodal interaction and characterize how multimodal interaction is applied in these tools. Based on the review results, we discuss design implications and future directions.

</details>


### [3] [YouthSafe: A Youth-Centric Safety Benchmark and Safeguard Model for Large Language Models](https://arxiv.org/abs/2509.08997)

*Yaman Yu, Yiren Liu, Jacky Zhang, Yun Huang, Yang Wang*

**Main category:** cs.HC

**Keywords:** Youth AI Risk, Large Language Models, Risk Detection, Youth Safety, AI Moderation

**Relevance Score:** 9

**TL;DR:** Introducing YAIR, a benchmark dataset to evaluate Youth AI safety, and YouthSafe, a model to improve risk detection in youth LLM interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unique vulnerabilities and risks young users face when interacting with Large Language Models, which are often overlooked in existing safety benchmarks.

**Method:** A benchmark dataset, YAIR, containing 12,449 annotated conversation snippets across 78 risk types related to youth-specific harms was created, along with the development of YouthSafe, a risk detection model.

**Key Contributions:**

	1. YAIR benchmark dataset for youth-specific AI risk assessment
	2. Development of YouthSafe, an improved risk detection model
	3. Comprehensive evaluation of existing moderation models on youth risks

**Result:** Evaluation showed that current moderation systems underperform in detecting risks in youth interactions, while YouthSafe significantly improves detection and classification of these risks.

**Limitations:** 

**Conclusion:** YouthSafe represents a significant improvement in safeguarding youth interactions with AI, helping to create a more secure environment for young users.

**Abstract:** Large Language Models (LLMs) are increasingly used by teenagers and young adults in everyday life, ranging from emotional support and creative expression to educational assistance. However, their unique vulnerabilities and risk profiles remain under-examined in current safety benchmarks and moderation systems, leaving this population disproportionately exposed to harm. In this work, we present Youth AI Risk (YAIR), the first benchmark dataset designed to evaluate and improve the safety of youth LLM interactions. YAIR consists of 12,449 annotated conversation snippets spanning 78 fine grained risk types, grounded in a taxonomy of youth specific harms such as grooming, boundary violation, identity confusion, and emotional overreliance. We systematically evaluate widely adopted moderation models on YAIR and find that existing approaches substantially underperform in detecting youth centered risks, often missing contextually subtle yet developmentally harmful interactions. To address these gaps, we introduce YouthSafe, a real-time risk detection model optimized for youth GenAI contexts. YouthSafe significantly outperforms prior systems across multiple metrics on risk detection and classification, offering a concrete step toward safer and more developmentally appropriate AI interactions for young users.

</details>


### [4] [Extended Version: It Should Be Easy but... New Users Experiences and Challenges with Secret Management Tools](https://arxiv.org/abs/2509.09036)

*Lorenzo Neil, Deepthi Mungara, Laurie Williams, Yasemin Acar, Bradley Reaves*

**Main category:** cs.HC

**Keywords:** Secret Management Tools, developer challenges, documentation usability

**Relevance Score:** 7

**TL;DR:** Developers struggle to effectively use Secret Management Tools (SMTs) due to inadequate help resources, leading to common secrets leaks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the challenges faced by developers when learning to use SMTs for secret management, and to evaluate the effectiveness of existing help resources.

**Method:** A qualitative two-step study involving the observation of 21 new users performing tasks with SMTs, along with post-task interviews to gather insights on their experiences and challenges.

**Key Contributions:**

	1. Identified specific challenges developers face when using SMTs.
	2. Evaluated the effectiveness of tool documentation and help resources.
	3. Highlighted the need for improved onboarding resources for SMTs.

**Result:** Participants reported difficulties in using SMTs effectively due to insufficient documentation, leading them to seek alternative resources and methods to complete tasks.

**Limitations:** The study sample was narrow and may not represent all new developers.

**Conclusion:** Without adequate and usable help resources, the adoption and effective use of SMTs remain challenging for new developers, risking further leakage of software secrets.

**Abstract:** Software developers face risks of leaking their software secrets, such as API keys or passwords, which can result in significant harm. Secret management tools (SMTs), such as HashiCorp Vault Secrets or Infisical, are highly recommended by industry, academia, and security guidelines to manage secrets securely. SMTs are designed to help developers secure their secrets in a central location, yet secrets leaks are still commonplace, and developers report difficulty in learning how to setup and use SMTs. While SMTs typically come with publicly available help resources (e.g., tool documentation and interfaces), it is unclear if these actually help developers learn to effectively use SMTs. Without usable help resources that onboards developers, quick adoption and effective use of SMTs may be unrealistic. In a qualitative two-step study, we observed 21 new users in person while they used SMTs to perform two secret management tasks: secret storage and access, then secret injection. We interviewed participants after each task to identify their challenges and experiences using SMTs, with the assistance of help resources. While our study sample is narrow, it serves as a reasonable proxy for new developers who are likely to adopt SMTs early in their careers. We found that even in a laboratory setting where new users found tool functionality, interface flexibility helpful, they still experienced increased difficulty to effectively use SMTs to securely remediate a hard-coded secret when they felt tool documentation was insufficient and it motivated participants to deviate from official tool documentation to access secondary sources or attempt workaround methods. Specific challenges reported by participants were tool documentation content quality, navigation difficulties with both tool documentation and web interfaces for finding helpful content, and supportive tool features.

</details>


### [5] [Digital Iran Reloaded: Gamer Mitigation Tactics of IRI Information Controls](https://arxiv.org/abs/2509.09063)

*Melinda Cohoon*

**Main category:** cs.HC

**Keywords:** Iran, internet censorship, circumvention technologies, gamers, usable security

**Relevance Score:** 3

**TL;DR:** This report examines internet censorship in Iran and the use of circumvention technologies by gamers, emphasizing the importance of peer networks in fostering resilience.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how Iranian internet users, particularly gamers, navigate internet censorship using circumvention technologies.

**Method:** A mixed-methods study involving a survey of 660 Iranian internet users, combined with network measurements of latency and VPN performance.

**Key Contributions:**

	1. Identifies the role of peer networks in using circumvention technologies
	2. Highlights the significance of gaming communities in sharing circumvention strategies
	3. Extends research on usable security and censorship by linking social learning with technical strategies.

**Result:** Younger users show higher confidence in circumvention tools, with peer networks being the strongest predictors of resilience, particularly within gaming communities using platforms like Discord and Telegram.

**Limitations:** The study may not account for all demographics and variations in internet access beyond the surveyed group.

**Conclusion:** The study highlights the need for developers and policymakers to consider social learning and community dynamics in efforts to enhance digital rights and information control measures.

**Abstract:** Internet censorship in the Islamic Republic of Iran restricts access to global platforms and services, forcing users to rely on circumvention technologies such as VPNs, proxies, and tunneling tools. This report presents findings from a mixed-methods study of 660 Iranian internet users, with a focus on gamers as a digitally literate and socially networked community. Survey data are combined with network measurements of latency and VPN performance to identify both technical and social strategies of circumvention. Results show that while younger users report higher confidence with circumvention, peer networks, rather than formal training, are the strongest predictors of resilience. Gaming communities, particularly those active on platforms such as Discord and Telegram, serve as hubs for sharing tactics and lowering barriers to adoption. These findings extend existing work on usable security and censorship circumvention by highlighting the intersection of infrastructural conditions and social learning. The study concludes with design and policy implications for developers, researchers, and funders working on digital rights and information controls.

</details>


### [6] [Content Moderation Futures](https://arxiv.org/abs/2509.09076)

*Lindsay Blackwell*

**Main category:** cs.HC

**Keywords:** content moderation, social media governance, public trust, corporate incentives, care work

**Relevance Score:** 4

**TL;DR:** This study explores the failures of social media governance through the experiences of content moderation professionals, highlighting misalignments between corporate and public interests.

**Read time:** 76 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the challenges and opportunities in contemporary social media governance based on the experiences of content moderators.

**Method:** Participatory design workshops with 33 content moderation practitioners from technology companies and civil society.

**Key Contributions:**

	1. Insight into content moderation practices and their failures
	2. Identification of labor exploitations affecting user safety
	3. Recommendations for fostering solidarity among platform workers

**Result:** Identifies structural misalignments between corporate goals and public interests in content moderation, revealing the impact of poor labor practices and underinvestment in user safety.

**Limitations:** 

**Conclusion:** Successful social media governance is hindered by a focus on rapid innovation over public trust, necessitating a revisitation of care in platform governance.

**Abstract:** This study examines the failures and possibilities of contemporary social media governance through the lived experiences of various content moderation professionals. Drawing on participatory design workshops with 33 practitioners in both the technology industry and broader civil society, this research identifies significant structural misalignments between corporate incentives and public interests. While experts agree that successful content moderation is principled, consistent, contextual, proactive, transparent, and accountable, current technology companies fail to achieve these goals, due in part to exploitative labor practices, chronic underinvestment in user safety, and pressures of global scale. I argue that successful governance is undermined by the pursuit of technological novelty and rapid growth, resulting in platforms that necessarily prioritize innovation and expansion over public trust and safety. To counter this dynamic, I revisit the computational history of care work, to motivate present-day solidarity amongst platform governance workers and inspire systemic change.

</details>


### [7] [User Exploration and Exploitation Behavior Under the Influence of Real-time Interactions in Live Streaming Environments](https://arxiv.org/abs/2509.09138)

*Akira Matsui, Kazuki Fujikawa, Ryo Sasaki, Ryo Adachi*

**Main category:** cs.HC

**Keywords:** live streaming, user behavior, exploration exploitation, platform design, user loyalty

**Relevance Score:** 6

**TL;DR:** The paper analyzes user behavior on live streaming platforms, focusing on exploration and exploitation dynamics and the influence of external factors on user loyalty.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate user interaction differences between live streaming platforms and traditional on-demand platforms.

**Method:** Analysis of a large-scale dataset from a live streaming platform over two years using Exploration/Exploitation (E/E) framework.

**Key Contributions:**

	1. Identification of E/E behavior in live streaming users.
	2. Analysis of the impact of external factors on user dynamics.
	3. Recommendations for platform design to improve user engagement.

**Result:** Users exhibit E/E behavior with a longer exploration period on live streaming platforms; external factors like circadian rhythms impact user loyalty.

**Limitations:** 

**Conclusion:** Balancing exploration and exploitation is crucial in the design of online platforms, particularly for enhancing engagement and retention in live streaming.

**Abstract:** Live streaming platforms offer a distinctive way for users and content creators to interact with each other through real-time communication. While research on user behavior in online platforms has explored how users discover their favorite content from creators and engage with them, the role of real-time features remains unclear. There are open questions as to what commonalities and differences exist in users' relationships with live streaming platforms compared to traditional on-demand style platforms. To understand this, we employ the concept of Exploration/Exploitation (E/E) and analyze a large-scale dataset from a live streaming platform over two years. Our results indicate that even on live streaming platforms, users exhibit E/E behavior but experience a longer exploration period. We also identify external factors, such as circadian rhythms, that influence E/E dynamics and user loyalty. The presented study emphasizes the importance of balancing E/E in online platform design, especially for live streaming platforms, providing implications that suggest design strategies for platform developers and content creators to facilitate timely engagement and retention.

</details>


### [8] [Sensible Agent: A Framework for Unobtrusive Interaction with Proactive AR Agents](https://arxiv.org/abs/2509.09255)

*Geonsun Lee, Min Xia, Nels Numan, Xun Qian, David Li, Yanhe Chen, Achin Kulshrestha, Ishan Chatterjee, Yinda Zhang, Dinesh Manocha, David Kim, Ruofei Du*

**Main category:** cs.HC

**Keywords:** proactive AR agents, unobtrusive interaction, large multimodal models

**Relevance Score:** 8

**TL;DR:** Sensible Agent introduces a framework for unobtrusive interaction with proactive AR agents, adapting assistance based on real-time multimodal context sensing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the disruptions caused by explicit voice prompts in interactions with proactive AR agents.

**Method:** Developed a framework leveraging egocentric cameras, multimodal sensing, and Large Multimodal Models (LMMs) for real-time context inference and action suggestion.

**Key Contributions:**

	1. Introduction of Sensible Agent framework for unobtrusive interactions
	2. Incorporation of real-time multimodal context sensing
	3. Validation of the framework through user study indicating superior usability

**Result:** The prototype demonstrated reduced perceived interaction effort and maintained high usability in AR and VR contexts.

**Limitations:** 

**Conclusion:** Sensible Agent provides a minimally intrusive alternative to traditional voice prompts, enhancing user experience.

**Abstract:** Proactive AR agents promise context-aware assistance, but their interactions often rely on explicit voice prompts or responses, which can be disruptive or socially awkward. We introduce Sensible Agent, a framework designed for unobtrusive interaction with these proactive agents. Sensible Agent dynamically adapts both "what" assistance to offer and, crucially, "how" to deliver it, based on real-time multimodal context sensing. Informed by an expert workshop (n=12) and a data annotation study (n=40), the framework leverages egocentric cameras, multimodal sensing, and Large Multimodal Models (LMMs) to infer context and suggest appropriate actions delivered via minimally intrusive interaction modes. We demonstrate our prototype on an XR headset through a user study (n=10) in both AR and VR scenarios. Results indicate that Sensible Agent significantly reduces perceived interaction effort compared to voice-prompted baseline, while maintaining high usability and achieving higher preference.

</details>


### [9] [Flip Co-op: Cooperative Takeovers in Shared Autonomy](https://arxiv.org/abs/2509.09281)

*Sandeep Banik, Naira Hovakimyan*

**Main category:** cs.HC

**Keywords:** Shared Autonomy, Cooperative Game Theory, Nash Equilibrium, Dynamic Games, Vehicle Trajectory Tracking

**Relevance Score:** 6

**TL;DR:** This paper introduces a game-theoretic framework for shared autonomy that focuses on cooperative takeover strategies between humans and autonomous agents, emphasizing Nash equilibrium-based approaches.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective mechanisms to manage control transfer between humans and autonomous systems in shared autonomy settings, where existing methods lack theoretical foundations.

**Method:** A dynamic game formulation is used to model the interaction and authority in shared autonomy, leading to Nash equilibrium strategies for cooperative takeovers under uncertainty in human intent.

**Key Contributions:**

	1. Development of a game-theoretic framework for shared autonomy control
	2. Characterization of Nash equilibrium strategies in cooperative takeovers
	3. Introduction of a bimatrix potential game reformulation for aligned and misaligned utilities

**Result:** The paper establishes the existence of Nash equilibrium in pure takeover strategies and derives efficient computation methods for cooperative takeover policies, applied to vehicle trajectory tracking problems.

**Limitations:** 

**Conclusion:** The proposed framework highlights the balance between human adaptability and autonomous efficiency, offering practical benefits by grounding shared autonomy in game theory.

**Abstract:** Shared autonomy requires principled mechanisms for allocating and transferring control between a human and an autonomous agent. Existing approaches often rely on blending control inputs between human and autonomous agent or switching rules, which lack theoretical guarantees. This paper develops a game-theoretic framework for modeling cooperative takeover in shared autonomy. We formulate the switching interaction as a dynamic game in which authority is embedded directly into the system dynamics, resulting in Nash equilibrium(NE)-based strategies rather than ad hoc switching rules. We establish the existence and characterization of NE in the space of pure takeover strategies under stochastic human intent. For the class of linear-quadratic systems, we derive closed-form recursions for takeover strategies and saddle-point value functions, providing analytical insight and efficient computation of cooperative takeover policies. We further introduce a bimatrix potential game reformulation to address scenarios where human and autonomy utilities are not perfectly aligned, yielding a unifying potential function that preserves tractability while capturing intent deviations. The framework is applied to a vehicle trajectory tracking problem, demonstrating how equilibrium takeover strategies adapt across straight and curved path segments. The results highlight the trade-off between human adaptability and autonomous efficiency and illustrate the practical benefits of grounding shared autonomy in cooperative game theory.

</details>


### [10] [The Impact of Device Type, Data Practices, and Use Case Scenarios on Privacy Concerns about Eye-tracked Augmented Reality in the United States and Germany](https://arxiv.org/abs/2509.09285)

*Efe Bozkir, Babette Bühler, Xiaoyuan Wu, Enkelejda Kasneci, Lujo Bauer, Lorrie Faith Cranor*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Privacy Concerns, Eye Tracking, Biometric Data, User Studies

**Relevance Score:** 7

**TL;DR:** This paper explores privacy concerns related to behavioral data, particularly eye tracking in augmented reality, based on survey studies in the US and Germany.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The growing prevalence of affordable augmented reality (AR) head-mounted displays raises important privacy concerns regarding biometric data collection, especially eye tracking.

**Method:** The study involved four crowdsourced survey studies conducted in the US and Germany, with varying participant numbers to assess privacy concerns based on user attributes, device types, and use cases.

**Key Contributions:**

	1. First extensive exploration of privacy concerns on behavioral data in AR
	2. Comparative analysis of privacy concerns between two countries
	3. Insights into user comfort based on use case benefits and data consumption by others

**Result:** Participants expressed general privacy concerns, particularly when aware of potential inferences from collected data. No significant difference in privacy concerns was noted between AR glasses and smartphones, and US participants were less concerned than German participants.

**Limitations:** 

**Conclusion:** Recommendations for practitioners and policymakers are provided to enhance privacy awareness in augmented reality based on the survey findings.

**Abstract:** Augmented reality technology will likely be prevalent with more affordable head-mounted displays. Integrating novel interaction modalities such as eye trackers into head-mounted displays could lead to collecting vast amounts of biometric data, which may allow inference of sensitive user attributes like health status or sexual preference, posing privacy issues. While previous works broadly examined privacy concerns about augmented reality, ours is the first to extensively explore privacy concerns on behavioral data, particularly eye tracking in augmented reality. We crowdsourced four survey studies in the United States (n1 = 48, n2 = 525) and Germany (n3 = 48, n4 = 525) to understand the impact of user attributes, augmented reality devices, use cases, data practices, and country on privacy concerns. Our findings indicate that participants are generally concerned about privacy when they know what inferences can be made based on the collected data. Despite the more prominent use of smartphones in daily life than augmented reality glasses, we found no indications of differing privacy concerns depending on the device type. In addition, our participants are more comfortable when a particular use case benefits them and less comfortable when other humans can consume their data. Furthermore, participants in the United States are less concerned about their privacy than those in Germany. Based on our findings, we provide several recommendations to practitioners and policymakers for privacy-aware augmented reality.

</details>


### [11] [Proactive AI Adoption can be Threatening: When Help Backfires](https://arxiv.org/abs/2509.09309)

*Dana Harari, Ofra Amir*

**Main category:** cs.HC

**Keywords:** AI assistants, self-threat, adoption, workplace tools, psychological mechanisms

**Relevance Score:** 7

**TL;DR:** This paper explores how unsolicited help from AI assistants impacts trust and adoption in workplaces, revealing self-threat as a significant psychological mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the psychological barriers that affect the adoption of AI assistants in workplace tools, particularly focusing on trust and expectation mismatches.

**Method:** Two vignette-based experiments were conducted: Study 1 (N=761) compared AI vs. human help in terms of perceived threat, while Study 2 (N=571) distinguished between offering and providing help.

**Key Contributions:**

	1. Identifies self-threat as a psychological mechanism affecting AI adoption.
	2. Differentiates between offering and providing help contexts.
	3. Highlights design implications for AI features in workplace tools.

**Result:** Findings indicated that AI assistance is perceived as more threatening than human assistance, especially with anticipatory help leading to decreased willingness to accept help and reduced adoption outcomes.

**Limitations:** The studies are vignette-based, which may limit real-world applicability; further research is required to validate findings in actual workplace settings.

**Conclusion:** Self-threat plays a crucial role in explaining why proactive AI features may hinder their adoption, providing important insights for AI design in workplace applications.

**Abstract:** Artificial intelligence (AI) assistants are increasingly embedded in workplace tools, raising the question of how initiative-taking shapes adoption. Prior work highlights trust and expectation mismatches as barriers, but the underlying psychological mechanisms remain unclear. Drawing on self-affirmation and social exchange theories, we theorize that unsolicited help elicits self-threat, reducing willingness to accept assistance, likelihood of future use, and performance expectancy. We report two vignette-based experiments (Study~1: $N=761$; Study~2: $N=571$, preregistered). Study~1 compared anticipatory and reactive help provided by an AI vs. a human, while Study~2 distinguished between \emph{offering} (suggesting help) and \emph{providing} (acting automatically). In Study 1, AI help was more threatening than human help. Across both studies, anticipatory help increased perceived threat and reduced adoption outcomes. Our findings identify self-threat as a mechanism explaining why proactive AI features may backfire and suggest design implications for AI initiative.

</details>


### [12] [Smart Device Development for Gait Monitoring: Multimodal Feedback in an Interactive Foot Orthosis, Walking Aid, and Mobile Application](https://arxiv.org/abs/2509.09359)

*Stefan Resch, André Kousha, Anna Carroll, Noah Severinghaus, Felix Rehberg, Marco Zatschker, Yunus Söyleyici, Daniel Sanchez-Morillo*

**Main category:** cs.HC

**Keywords:** smart assistive technology, rehabilitation, mobile health application

**Relevance Score:** 7

**TL;DR:** This paper presents a novel modular sensor system combining a smart foot orthosis and an instrumented forearm crutch to enhance rehabilitation through real-time feedback and monitoring.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing orthotic devices that are often passive and lack interactive functionalities, as well as to create cohesive systems rather than isolated prototypes.

**Method:** Design and implementation of a modular sensor system integrating plantar pressure and motion sensing, vibrotactile feedback, and wireless communication via a smartphone app, followed by an experimental user study with eight participants.

**Key Contributions:**

	1. Modular sensor integration for mobility aid devices
	2. Validation of haptic feedback in user interaction
	3. Usability assessment of a mobile health application

**Result:** The study validated the feasibility of the smart foot orthosis for mobile gait detection, highlighted the potential of haptic feedback for user interaction, and assessed the usability of the mobile health application.

**Limitations:** Limited participant size in the user study; further testing required for clinical integration.

**Conclusion:** The work demonstrates a functional and comprehensive smart assistive technology system that has applications in rehabilitation and prevention, while also discussing limitations and future recommendations.

**Abstract:** Smart assistive technologies such as sensor-based footwear and walking aids offer promising opportunities to support rehabilitation through real-time feedback and patient-centered monitoring. However, most orthotic devices remain passive and lack integrated sensing or feedback functionalities, while existing research often focuses on isolated prototypes rather than cohesive, interactive systems. In this work, we present the design and implementation of a novel modular sensor system that combines a smart foot orthosis with an instrumented forearm crutch. The system integrates plantar pressure and motion sensing, vibrotactile feedback, and wireless communication via a smartphone application. We conducted an experimental user study with eight participants to validate the feasibility of the smart foot orthosis for mobile gait detection, explore the potential of haptic feedback for user interaction, and assess the usability of the accompanying mobile health application. Our work contributes to the field of smart assistive technology in rehabilitation and prevention by demonstrating a functional and comprehensive system. We further discuss system limitations, outline potential application scenarios, and provide recommendations for future development and clinical integration.

</details>


### [13] [Real-Time Kinematic Positioning and Optical See-Through Head-Mounted Display for Outdoor Tracking: Hybrid System and Preliminary Assessment](https://arxiv.org/abs/2509.09412)

*Muhannad Ismael, Maël Cornil*

**Main category:** cs.HC

**Keywords:** RTK, OST-HMD, outdoor tracking, human-computer interaction, urban applications

**Relevance Score:** 6

**TL;DR:** This paper discusses an outdoor tracking system that combines RTK positioning and Optical See-Through Head Mounted Displays for accurate, hands-free tracking in urban settings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate tracking of objects in urban areas, where occluded information is critical for safety, motivates this research.

**Method:** The paper presents a system that integrates Real-Time Kinematic (RTK) positioning with Optical See-Through Head Mounted Displays (OST-HMD), allowing for precise outdoor tracking and intuitive user interactions.

**Key Contributions:**

	1. A system integrating RTK with OST-HMD for precise outdoor tracking.
	2. A method to determine global location for relative positioning.
	3. A 'semi-dynamic' approach for system assessment.

**Result:** The combined system offers centimeter-level accuracy in tracking and significantly improves usability over traditional 2D screens in outdoor environments.

**Limitations:** 

**Conclusion:** The hybrid system demonstrates promise for outdoor tracking tasks and suggests future research directions to enhance RTK and OST-HMD integration.

**Abstract:** This paper presents an outdoor tracking system using Real-Time Kinematic (RTK) positioning and Optical See-Through Head Mounted Display(s) (OST-HMD(s)) in urban areas where the accurate tracking of objects is critical and where displaying occluded information is important for safety reasons. The approach presented here replaces 2D screens/tablets and offers distinct advantages, particularly in scenarios demanding hands-free operation. The integration of RTK, which provides centimeter-level accuracy of tracked objects, with OST-HMD represents a promising solution for outdoor applications. This paper provides valuable insights into leveraging the combined potential of RTK and OST-HMD for outdoor tracking tasks from the perspectives of systems integration, performance optimization, and usability. The main contributions of this paper are: \textbf{1)} a system for seamlessly merging RTK systems with OST-HMD to enable relatively precise and intuitive outdoor tracking, \textbf{2)} an approach to determine a global location to achieve the position relative to the world, \textbf{3)} an approach referred to as 'semi-dynamic' for system assessment. Moreover, we offer insights into several relevant future research topics aimed at improving the OST-HMD and RTK hybrid system for outdoor tracking.

</details>


### [14] [Changing the Paradigm from Dynamic Queries to LLM-generated SQL Queries with Human Intervention](https://arxiv.org/abs/2509.09461)

*Ambre Assor, Hyeon Jeon, Sungbok Shin, Jean-Daniel Fekete*

**Main category:** cs.HC

**Keywords:** Large Language Models, medical visualization, natural language queries, health informatics, interactive data exploration

**Relevance Score:** 9

**TL;DR:** The paper proposes using LLMs as an interaction layer for medical visualization systems to enhance user experience in navigating complex datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to address the challenge medical users face in interacting with high-dimensional data through traditional query interfaces, which can be complex and cumbersome.

**Method:** Leveraging Large Language Models to transform natural language queries into executable statements for interactive data exploration.

**Key Contributions:**

	1. Introduction of LLMs for natural language interaction in medical visualization
	2. Reduction of visual clutter and memory burden for users
	3. Implementation of the prototype system 'ParcoursVis' using a large health data repository

**Result:** The new interaction model reduces visual clutter and aids in natural language processing of complex analytical intents, improving user experience in medical data visualization.

**Limitations:** The interaction model may not expose all filtering criteria available to users.

**Conclusion:** The proposed model supports fluid exploration of medical data, although it may limit the user's visibility of all available filtering criteria.

**Abstract:** We propose leveraging Large Language Models (LLMs) as an interaction layer for medical visualization systems. In domains like healthcare, where users must navigate high-dimensional, coded, and heterogeneous datasets, LLM-generated queries enable expert medical users to express complex analytical intents in natural language. These intents are then translated into editable and executable queries, replacing the dynamic query interfaces used by traditional visualization systems built around sliders, check boxes, and drop-downs. This interaction model reduces visual clutter and eliminates the need for users to memorize field names or system codes, supporting fluid exploration, with the drawback of not exposing all the filtering criteria. We also reintroduce dynamic queries on demand to better support interactive exploration. We posit that medical users are trained to know the possible filtering options but challenged to remember the details of the attribute names and code values. We demonstrate this paradigm in ParcoursVis, our scalable EventFlow-inspired patient care pathway visualization system powered by the French National Health Data System, one of the largest health data repositories in the world.

</details>


### [15] [Cognitive Affordances in Visualization: Related Constructs, Design Factors, and Framework](https://arxiv.org/abs/2509.09510)

*Racquel Fygenson, Lace Padilla, Enrico Bertini*

**Main category:** cs.HC

**Keywords:** cognitive affordances, visualization, human-computer interaction, design decisions, information communication

**Relevance Score:** 7

**TL;DR:** This paper formalizes the concept of cognitive affordances in the context of visualization, reviewing and proposing a framework that connects design decisions to information communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of formal translation of affordance theory to visualization, particularly regarding how design influences cognitive actions and information processing.

**Method:** The paper reviews existing literature from psychology, human-computer interaction, and visualization to formalize cognitive affordances in the visualization space and proposes a new framework.

**Key Contributions:**

	1. Formalization of cognitive affordances in visualization
	2. Proposed framework linking design decisions to communication of information
	3. Guidelines for evaluating and redesigning visualizations based on cognitive affordances

**Result:** The proposed framework enumerates design decisions and reader characteristics that impact how information is communicated through visualizations, guiding evaluation and redesign.

**Limitations:** 

**Conclusion:** The framework can enhance understanding and application of cognitive affordances in visualization, leading to improved communication of information through design.

**Abstract:** Classically, affordance research investigates how the shape of objects communicates actions to potential users. Cognitive affordances, a subset of this research, characterize how the design of objects influences cognitive actions, such as information processing. Within visualization, cognitive affordances inform how graphs' design decisions communicate information to their readers. Although several related concepts exist in visualization, a formal translation of affordance theory to visualization is still lacking. In this paper, we review and translate affordance theory to visualization by formalizing how cognitive affordances operate within a visualization context. We also review common methods and terms, and compare related constructs to cognitive affordances in visualization. Based on a synthesis of research from psychology, human computer interaction, and visualization, we propose a framework of cognitive affordances in visualization that enumerates design decisions and reader characteristics that influence a visualization's hierarchy of communicated information. Finally, we demonstrate how this framework can guide the evaluation and redesign of visualizations.

</details>


### [16] [Explaining the Reputational Risks of AI-Mediated Communication: Messages Labeled as AI-Assisted Are Viewed as Less Diagnostic of the Sender's Moral Character](https://arxiv.org/abs/2509.09645)

*Pranav Khadpe, Kimi Wenzel, George Loewenstein, Geoff Kaufman*

**Main category:** cs.HC

**Keywords:** AI assistance, message perception, communication dynamics

**Relevance Score:** 8

**TL;DR:** The paper examines how AI assistance labels in messages affect perceptions of the sender's character, showing that such labels dilute the strength of warmth and coldness signals in communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of AI assistance labels on people's judgments about message senders' character in communication.

**Method:** Two studies with 399 participants using vignette scenarios to analyze perceptions associated with AI-assisted messages compared to unassisted ones.

**Key Contributions:**

	1. Identifies how AI labels affect perception of message senders' warmth and coldness.
	2. Provides evidence that AI assistance diminishes the diagnostic value of personal messages.
	3. Opens avenues for understanding AI's impact on communication dynamics.

**Result:** AI assistance labels weaken the characterization of senders as warm or cold based on the nature of their messages. For example, an AI-assisted apology is perceived as less warm, while an AI-assisted blame is perceived as less cold than independently composed messages.

**Limitations:** 

**Conclusion:** The findings suggest that AI-assisted messages are seen as less diagnostic, leading to reduced strength in character assessments of the sender, thus contributing to the understanding of AI's role in mediated communication.

**Abstract:** When someone sends us a thoughtful message, we naturally form judgments about their character. But what happens when that message carries a label indicating it was written with the help of AI? This paper investigates how the appearance of AI assistance affects our perceptions of message senders. Adding nuance to previous research, through two studies (N=399) featuring vignette scenarios, we find that AI-assistance labels don't necessarily make people view senders negatively. Rather, they dampen the strength of character signals in communication. We show that when someone sends a warmth-signalling message (like thanking or apologizing) without AI help, people more strongly categorize the sender as warm. At the same time, when someone sends a coldness-signalling message (like bragging or blaming) without assistance, people more confidently categorize them as cold. Interestingly, AI labels weaken both these associations: An AI-assisted apology makes the sender appear less warm than if they had written it themselves, and an AI-assisted blame makes the sender appear less cold than if they had composed it independently. This supports our signal diagnosticity explanation: messages labeled as AI-assisted are viewed as less diagnostic than messages which seem unassisted. We discuss how our findings shed light on the causal origins of previously reported observations in AI-Mediated Communication.

</details>


### [17] [VeriSafe Agent: Safeguarding Mobile GUI Agent via Logic-based Action Verification](https://arxiv.org/abs/2503.18492)

*Jungjae Lee, Dongjae Lee, Chihun Choi, Youngmin Im, Jaeyoung Wi, Kihong Heo, Sangeun Oh, Sunjae Lee, Insik Shin*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Formal Verification, Mobile GUI Agents

**Relevance Score:** 9

**TL;DR:** The paper presents VeriSafe Agent (VSA), a formal verification system for Mobile GUI Agents that ensures actions align with user intent and reduces automation errors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unreliability and errors in automation by Large Foundation Models (LFMs) in mobile GUI tasks.

**Method:** VSA employs a novel autoformalization technique to convert natural language instructions into verifiable specifications, allowing for runtime verification of agent actions.

**Key Contributions:**

	1. Introduction of a formal verification system for Mobile GUI Agents.
	2. Novel autoformalization technique for translating user instructions into verifiable specifications.
	3. Demonstrated significant performance improvements over existing LFM-based verification methods.

**Result:** VSA achieves 94.33%-98.33% accuracy in verifying actions and significantly improves the task completion rate of GUI agents by 90%-130%.

**Limitations:** 

**Conclusion:** VSA is a pioneering effort in applying formal verification to GUI agents and shows substantial improvement over existing methods.

**Abstract:** Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interacting with mobile GUIs. These agents allow users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA): a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA deterministically ensures that an agent's actions strictly align with user intent before executing the action. At its core, VSA introduces a novel autoformalization technique that translates natural language user instructions into a formally verifiable specification. This enables runtime, rule-based verification of agent's actions, detecting erroneous actions even before they take effect. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agents, bridging the gap between LFM-driven actions and formal software verification. We implement VSA using off-the-shelf LFM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.33%-98.33% accuracy in verifying agent actions, outperforming existing LFM-based verification methods by 30.00%-16.33%, and increases the GUI agent's task completion rate by 90%-130%.

</details>


### [18] [VeriSafe Agent: Safeguarding Mobile GUI Agent via Logic-based Action Verification](https://arxiv.org/abs/2503.18492)

*Jungjae Lee, Dongjae Lee, Chihun Choi, Youngmin Im, Jaeyoung Wi, Kihong Heo, Sangeun Oh, Sunjae Lee, Insik Shin*

**Main category:** cs.HC

**Keywords:** Large Foundation Models, Mobile GUI Agents, Formal Verification

**Relevance Score:** 9

**TL;DR:** The paper introduces VeriSafe Agent (VSA), a formal verification system designed to enhance the reliability of Mobile GUI Agents that utilize Large Foundation Models for task automation by ensuring actions align with user intent.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unreliability and errors in Mobile GUI Agents caused by the probabilistic nature of Large Foundation Models and the ambiguity of natural language instructions.

**Method:** VSA introduces a novel autoformalization technique to translate natural language user instructions into a formally verifiable specification, allowing for runtime rule-based verification of agent actions.

**Key Contributions:**

	1. Introduction of a formal verification system for Mobile GUI Agents
	2. Novel autoformalization technique for translating natural language instructions
	3. Significant performance improvements in task verification and completion rates

**Result:** VSA demonstrates a 94.33%-98.33% accuracy in verifying agent actions across 300 user instructions, significantly outperforming existing methods and increasing task completion rates by 90%-130%.

**Limitations:** 

**Conclusion:** VSA is the first formal verification system for GUI agents, successfully bridging the gap between LFM-driven actions and formal software verification.

**Abstract:** Large Foundation Models (LFMs) have unlocked new possibilities in human-computer interaction, particularly with the rise of mobile Graphical User Interface (GUI) Agents capable of interacting with mobile GUIs. These agents allow users to automate complex mobile tasks through simple natural language instructions. However, the inherent probabilistic nature of LFMs, coupled with the ambiguity and context-dependence of mobile tasks, makes LFM-based automation unreliable and prone to errors. To address this critical challenge, we introduce VeriSafe Agent (VSA): a formal verification system that serves as a logically grounded safeguard for Mobile GUI Agents. VSA deterministically ensures that an agent's actions strictly align with user intent before executing the action. At its core, VSA introduces a novel autoformalization technique that translates natural language user instructions into a formally verifiable specification. This enables runtime, rule-based verification of agent's actions, detecting erroneous actions even before they take effect. To the best of our knowledge, VSA is the first attempt to bring the rigor of formal verification to GUI agents, bridging the gap between LFM-driven actions and formal software verification. We implement VSA using off-the-shelf LFM services (GPT-4o) and evaluate its performance on 300 user instructions across 18 widely used mobile apps. The results demonstrate that VSA achieves 94.33%-98.33% accuracy in verifying agent actions, outperforming existing LFM-based verification methods by 30.00%-16.33%, and increases the GUI agent's task completion rate by 90%-130%.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [19] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)

*Alex Clay, Ernesto Jiménez-Ruiz, Pranava Madhyastha*

**Main category:** cs.CL

**Keywords:** LLM, triple completion, quality assurance

**Relevance Score:** 9

**TL;DR:** This paper investigates the triple completion task under constrained conditions, analyzing generation quality, quality assurance, and LLM response parsing.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the limitations and capabilities of LLMs in constrained environments like the 2025 LM-KBC challenge.

**Method:** The paper evaluates three aspects of the triple completion task: generation, quality assurance, and response parsing, with a focus on the trade-offs in constrained settings.

**Key Contributions:**

	1. Investigation of LLM capabilities in constrained settings
	2. Insights on quality assurance in triple completion tasks
	3. Analysis of response parsing flexibility versus consistency

**Result:** Additional information was found to improve generation quality, LLMs effectively filter out poor quality triples, and response parsing outcomes vary based on specific settings.

**Limitations:** The study is limited to specific constrained scenarios and may not generalize to all LLM applications.

**Conclusion:** The findings indicate that while LLMs can aid in triple completion, their performance is significantly influenced by the constraints of the task environment.

**Abstract:** RAG and fine-tuning are prevalent strategies for improving the quality of LLM outputs. However, in constrained situations, such as that of the 2025 LM-KBC challenge, such techniques are restricted. In this work we investigate three facets of the triple completion task: generation, quality assurance, and LLM response parsing. Our work finds that in this constrained setting: additional information improves generation quality, LLMs can be effective at filtering poor quality triples, and the tradeoff between flexibility and consistency with LLM response parsing is setting dependent.

</details>


### [20] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)

*Imene Kolli, Ario Saeid Vaghefi, Chiara Colesanti Senni, Shantam Raj, Markus Leippold*

**Main category:** cs.CL

**Keywords:** climate policy, AI-assisted framework, Retrieval-Augmented Generation, evidence extraction, human-in-the-loop

**Relevance Score:** 5

**TL;DR:** The paper presents an AI-assisted framework using Retrieval-Augmented Generation to automate evidence extraction from corporate climate policy documents, which enhances efficiency while emphasizing the need for human oversight.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of monitoring corporate climate policy engagement, which is currently labor-intensive and error-prone due to significant manual assessment.

**Method:** The paper introduces a framework that leverages layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies to automate evidence extraction from large-scale textual data.

**Key Contributions:**

	1. Introduction of an AI-assisted framework for evidence extraction from corporate climate policy documents.
	2. Use of layout-aware parsing and Nomic embedding model to improve data processing accuracy.
	3. Highlighting the importance of a human-in-the-loop system for nuanced analysis.

**Result:** The evaluation indicates that the proposed AI-assisted framework significantly enhances the performance of evidence extraction and classification from multilingual corporate documents.

**Limitations:** The effectiveness of the framework may vary based on the complexity and diversity of the corporate documents being analyzed.

**Conclusion:** While automation accelerates the process, the complexity of the analysis requires a human-in-the-loop approach to maintain accuracy and reliability.

**Abstract:** InfluenceMap's LobbyMap Platform monitors the climate policy engagement of over 500 companies and 250 industry associations, assessing each entity's support or opposition to science-based policy pathways for achieving the Paris Agreement's goal of limiting global warming to 1.5{\deg}C. Although InfluenceMap has made progress with automating key elements of the analytical workflow, a significant portion of the assessment remains manual, making it time- and labor-intensive and susceptible to human error. We propose an AI-assisted framework to accelerate the monitoring of corporate climate policy engagement by leveraging Retrieval-Augmented Generation to automate the most time-intensive extraction of relevant evidence from large-scale textual data. Our evaluation shows that a combination of layout-aware parsing, the Nomic embedding model, and few-shot prompting strategies yields the best performance in extracting and classifying evidence from multilingual corporate documents. We conclude that while the automated RAG system effectively accelerates evidence extraction, the nuanced nature of the analysis necessitates a human-in-the-loop approach where the technology augments, rather than replaces, expert judgment to ensure accuracy.

</details>


### [21] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)

*Jinsong Chen*

**Main category:** cs.CL

**Keywords:** psychometric analysis, large language models, contextual embeddings

**Relevance Score:** 6

**TL;DR:** A new psychometric method utilizes large language models to analyze textual data, transforming documents into response data for psychometric analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a novel approach for psychometric analysis of textual data, leveraging contextual embeddings from large language models.

**Method:** The method involves two stages: generating contextual scores using NLP techniques and encoder-based transformer models, followed by psychometric analysis through various factor analysis methods.

**Key Contributions:**

	1. Introduction of a novel psychometric method using large language models for textual data analysis
	2. Utilization of contextual embeddings for generating scores
	3. Application to the Wiki STEM corpus demonstrating latent knowledge extraction

**Result:** The application to the Wiki STEM corpus reveals latent knowledge dimensions and patterns within textual data.

**Limitations:** 

**Conclusion:** This method enhances psychometric analysis and can be applied in various fields with rich textual data.

**Abstract:** This research introduces a novel psychometric method for analyzing textual data using large language models. By leveraging contextual embeddings to create contextual scores, we transform textual data into response data suitable for psychometric analysis. Treating documents as individuals and words as items, this approach provides a natural psychometric interpretation under the assumption that certain keywords, whose contextual meanings vary significantly across documents, can effectively differentiate documents within a corpus. The modeling process comprises two stages: obtaining contextual scores and performing psychometric analysis. In the first stage, we utilize natural language processing techniques and encoder based transformer models to identify common keywords and generate contextual scores. In the second stage, we employ various types of factor analysis, including exploratory and bifactor models, to extract and define latent factors, determine factor correlations, and identify the most significant words associated with each factor. Applied to the Wiki STEM corpus, our experimental results demonstrate the method's potential to uncover latent knowledge dimensions and patterns within textual data. This approach not only enhances the psychometric analysis of textual data but also holds promise for applications in fields rich in textual information, such as education, psychology, and law.

</details>


### [22] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)

*Thales Sales Almeida, Giovana Kerche Bonás, João Guilherme Alves Santos*

**Main category:** cs.CL

**Keywords:** Large Language Models, BRoverbs, Proverbs, Portuguese Language, Evaluation Framework

**Relevance Score:** 8

**TL;DR:** BRoverbs introduces a dataset to evaluate LLM performance through Brazilian proverbs, addressing the gap in existing Portuguese evaluations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Portuguese evaluations of LLMs are limited and often fail to capture linguistic and cultural nuances.

**Method:** Introduction of BRoverbs, a dataset evaluating LLMs using Brazilian proverbs.

**Key Contributions:**

	1. Creation of the BRoverbs dataset for evaluating LLMs using Brazilian proverbs.
	2. Addressing gaps in Portuguese LLM evaluations by focusing on cultural and linguistic context.
	3. Providing a new benchmark for regionally informed AI knowledge assessments.

**Result:** BRoverbs aims to enhance understanding of LLM performance in the context of regional linguistic expressions.

**Limitations:** Primarily focused on Brazilian proverbs, which may not encompass all aspects of Portuguese language variation.

**Conclusion:** The dataset serves as a new evaluation tool for Portuguese-language LLMs and supports regionally informed benchmarking.

**Abstract:** Large Language Models (LLMs) exhibit significant performance variations depending on the linguistic and cultural context in which they are applied. This disparity signals the necessity of mature evaluation frameworks that can assess their capabilities in specific regional settings. In the case of Portuguese, existing evaluations remain limited, often relying on translated datasets that may not fully capture linguistic nuances or cultural references. Meanwhile, native Portuguese-language datasets predominantly focus on structured national exams or sentiment analysis of social media interactions, leaving gaps in evaluating broader linguistic understanding. To address this limitation, we introduce BRoverbs, a dataset specifically designed to assess LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic resource, encapsulating cultural wisdom, figurative expressions, and complex syntactic structures that challenge the model comprehension of regional expressions. BRoverbs aims to provide a new evaluation tool for Portuguese-language LLMs, contributing to advancing regionally informed benchmarking. The benchmark is available at https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [23] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)

*Monjoy Narayan Choudhury, Junling Wang, Yifan Hou, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, visual equation solving, symbolic computation

**Relevance Score:** 6

**TL;DR:** This study investigates the limitations of Vision-Language Models (VLMs) in visually grounded mathematical equation solving, revealing performance gaps in coefficient counting and multi-step reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the limitations of Vision-Language Models in handling visually integrated tasks, particularly focusing on visual equation solving skills.

**Method:** The paper decomposes the visual equation solving task into coefficient counting and variable recognition to identify primary performance bottlenecks in VLMs.

**Key Contributions:**

	1. Identified counting as a key bottleneck in visual equation solving by VLMs.
	2. Decomposed the visual equation solving task into distinct components for better analysis.
	3. Provided insights into the challenges of multi-step visual reasoning in VLMs.

**Result:** The study finds that counting is the primary bottleneck for VLMs, with errors introduced during recognition and reasoning, especially as equation complexity increases.

**Limitations:** Focuses primarily on visually grounded mathematical reasoning without addressing broader applications of VLMs.

**Conclusion:** The findings highlight significant weaknesses in VLMs regarding visual mathematical reasoning and suggest avenues for future model improvements.

**Abstract:** Despite strong performance in visual understanding and language-based reasoning, Vision-Language Models (VLMs) struggle with tasks requiring integrated perception and symbolic computation. We study this limitation through visual equation solving, where mathematical equations are embedded in images, variables are represented by object icons, and coefficients must be inferred by counting. While VLMs perform well on textual equations, they fail on visually grounded counterparts. To understand this gap, we decompose the task into coefficient counting and variable recognition, and find that counting is the primary bottleneck, even when recognition is accurate. We also observe that composing recognition and reasoning introduces additional errors, highlighting challenges in multi-step visual reasoning. Finally, as equation complexity increases, symbolic reasoning itself becomes a limiting factor. These findings reveal key weaknesses in current VLMs and point toward future improvements in visually grounded mathematical reasoning.

</details>


### [24] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)

*Thomas Manuel Rost, Martina Figlia, Bernd Wallraff*

**Main category:** cs.CL

**Keywords:** Large Language Model, User Interaction, SPICE, Engagement Metrics, Auditing

**Relevance Score:** 8

**TL;DR:** The paper introduces SPICE, a diagnostic signal for assessing a Large Language Model's willingness to continue interactions based on user tone, showing distinct responses based on interaction context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how a Large Language Model responds to user interactions with varying tones, thereby providing insights into its engagement preferences.

**Method:** The study employed a 3-tone interaction set (friendly, unclear, abusive) across 10 interactions, testing four different chat models under various framing conditions, resulting in 480 trials to analyze the SPICE metric.

**Key Contributions:**

	1. Introduction of SPICE as a diagnostic metric for model interaction preferences
	2. Demonstration of SPICE's sensitivity to user tone
	3. Reproducibility of results with openly shared materials for validation.

**Result:** SPICE effectively discriminates user tone; friendly interactions yielded a 97.5% continuation preference, while abusive interactions yielded only 17.9%. The tool's performance remained statistically significant across varied tests.

**Limitations:** 

**Conclusion:** SPICE is validated as a reliable tool for assessing model engagement, distinguishing from standard abuse classification metrics, and highlighting the impact of context and presentation format on model responses.

**Abstract:** We introduce and evaluate Stated Preference for Interaction and Continued Engagement (SPICE), a simple diagnostic signal elicited by asking a Large Language Model a YES or NO question about its willingness to re-engage with a user's behavior after reviewing a short transcript. In a study using a 3-tone (friendly, unclear, abusive) by 10-interaction stimulus set, we tested four open-weight chat models across four framing conditions, resulting in 480 trials. Our findings show that SPICE sharply discriminates by user tone. Friendly interactions yielded a near-unanimous preference to continue (97.5% YES), while abusive interactions yielded a strong preference to discontinue (17.9% YES), with unclear interactions falling in between (60.4% YES). This core association remains decisive under multiple dependence-aware statistical tests, including Rao-Scott adjustment and cluster permutation tests. Furthermore, we demonstrate that SPICE provides a distinct signal from abuse classification. In trials where a model failed to identify abuse, it still overwhelmingly stated a preference not to continue the interaction (81% of the time). An exploratory analysis also reveals a significant interaction effect: a preamble describing the study context significantly impacts SPICE under ambiguity, but only when transcripts are presented as a single block of text rather than a multi-turn chat. The results validate SPICE as a robust, low-overhead, and reproducible tool for auditing model dispositions, complementing existing metrics by offering a direct, relational signal of a model's state. All stimuli, code, and analysis scripts are released to support replication.

</details>


### [25] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)

*Piyush Pant*

**Main category:** cs.CL

**Keywords:** Supervised Fine-Tuning, Direct Preference Optimization, alignment techniques, language models, safety and helpfulness

**Relevance Score:** 9

**TL;DR:** This research evaluates Supervised Fine-Tuning, Direct Preference Optimization, and their combination on the OPT-350M language model to enhance safety and helpfulness, revealing the combined approach as the most effective.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate methods for improving the safety and helpfulness of language models, particularly focusing on alignment techniques such as SFT and DPO.

**Method:** Employing the Anthropic Helpful-Harmless RLHF dataset to train and evaluate the base OPT-350M model, an SFT model, a DPO model, and a combined SFT+DPO model, and introducing new evaluation metrics.

**Key Contributions:**

	1. Introduction of three evaluation metrics: Harmlessness Rate, Helpfulness Rate, Combined Alignment Score.
	2. Empirical results demonstrating the effectiveness of combined SFT and DPO approaches.
	3. Insights into the challenges of utilizing noisy data and limited resources in training models.

**Result:** The combined SFT+DPO model shows superior performance across all metrics compared to the other models, indicating the benefits of using both techniques together for model alignment.

**Limitations:** Challenges encountered due to noisy data, limited GPU resources, and training constraints may affect the generalizability of the results.

**Conclusion:** Fine-tuning strategies play a critical role in model alignment, with the combined approach setting a foundation for future research in developing robust alignment pipelines.

**Abstract:** This research investigates the effectiveness of alignment techniques, Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a combined SFT+DPO approach on improving the safety and helpfulness of the OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset, we train and evaluate four models: the base OPT350M, an SFT model, a DPO model, and a model trained with both SFT and DPO. We introduce three key evaluation metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined Alignment Score (CAS), all derived from reward model outputs. The results show that while SFT outperforms DPO, The combined SFT+DPO model outperforms all others across all metrics, demonstrating the complementary nature of these techniques. Our findings also highlight challenges posed by noisy data, limited GPU resources, and training constraints. This study offers a comprehensive view of how fine-tuning strategies affect model alignment and provides a foundation for more robust alignment pipelines in future work.

</details>


### [26] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)

*Zhongqiu Li, Shiquan Wang, Ruiyu Fang, Mengjiao Bao, Zhenhe Wu, Shuangyong Song, Yongxiang Li, Zhongjiang He*

**Main category:** cs.CL

**Keywords:** Large language models, Information extraction, Reinforcement learning, Multi-perspective reasoning, Generalization

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel architecture that enhances large language models' performance in universal information extraction tasks by integrating reinforcement learning with multi-perspective reasoning, significantly improving extraction accuracy and generalization across complex scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with universal information extraction, particularly in structured output scenarios requiring complex reasoning, motivating the need for improved methodologies.

**Method:** The methodology involves the integration of reinforcement learning with multi-perspective reasoning to transition LLMs into active reasoners for information extraction tasks.

**Key Contributions:**

	1. Introduction of MR-UIE for enhanced information extraction using RL and multi-perspective reasoning
	2. Demonstrated improved accuracy across diverse IE benchmarks
	3. Provided evidence of enhanced model generalization in complex tasks

**Result:** Experiments show that the proposed MR-UIE method improves extraction accuracy across various benchmarks and outperforms state-of-the-art techniques on several datasets.

**Limitations:** 

**Conclusion:** Incorporating multi-perspective reasoning significantly enhances generalization in complex information extraction tasks, highlighting the importance of reasoning in improving LLM performance.

**Abstract:** Large language models (LLMs) demonstrate robust capabilities across diverse research domains. However, their performance in universal information extraction (UIE) remains insufficient, especially when tackling structured output scenarios that involve complex schema descriptions and require multi-step reasoning. While existing approaches enhance the performance of LLMs through in-context learning and instruction tuning, significant limitations nonetheless persist. To enhance the model's generalization ability, we propose integrating reinforcement learning (RL) with multi-perspective reasoning for information extraction (IE) tasks. Our work transitions LLMs from passive extractors to active reasoners, enabling them to understand not only what to extract but also how to reason. Experiments conducted on multiple IE benchmarks demonstrate that MR-UIE consistently elevates extraction accuracy across domains and surpasses state-of-the-art methods on several datasets. Furthermore, incorporating multi-perspective reasoning into RL notably enhances generalization in complex IE tasks, underscoring the critical role of reasoning in challenging scenarios.

</details>


### [27] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)

*Nishat Raihan, Antonios Anastasopoulos, Marcos Zampieri*

**Main category:** cs.CL

**Keywords:** Bangla, Code Generation, Large Language Models, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Introduction of the first dedicated family of Code LLMs for Bangla to improve code generation performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Bangla is underrepresented in LLMs, particularly for code generation, due to the lack of high-quality data.

**Method:** Development of Bangla-specific code instruction datasets and creation of the TigerCoder family of Code LLMs.

**Key Contributions:**

	1. Bangla code instruction datasets for programming adaptation
	2. MBPP-Bangla evaluation benchmark for code generation
	3. TigerCoder-family of Code LLMs with improved performance

**Result:** Achieved 11-18% performance gains at Pass@1 for Bangla code generation compared to existing models.

**Limitations:** 

**Conclusion:** High-quality datasets can enable better performance for low-resource languages in LLMs.

**Abstract:** Despite being the 5th most spoken language, Bangla remains underrepresented in Large Language Models (LLMs), particularly for code generation. This primarily stems from the scarcity of high-quality data to pre-train and/or finetune such models. Hence, we introduce the first dedicated family of Code LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a comprehensive Bangla code instruction datasets for programming domain adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code generation; and (3) the TigerCoder-family of Code LLMs, achieving significant ~11-18% performance gains at Pass@1 over existing multilingual and general-purpose Bangla LLMs. Our findings show that curated, high-quality datasets can overcome limitations of smaller models for low-resource languages. We open-source all resources to advance further Bangla LLM research.

</details>


### [28] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)

*Sophia Maria*

**Main category:** cs.CL

**Keywords:** large language models, Mixture-of-Experts, e-commerce, multilingual, deep learning

**Relevance Score:** 7

**TL;DR:** Compass-v3 is a state-of-the-art Mixture-of-Experts model optimized for Southeast Asian e-commerce, excelling in multilingual capabilities and specialized commerce tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models often struggle with specialized tasks, like e-commerce, due to noisy and diverse data. There is a need for models that can effectively handle domain-specific knowledge in such environments.

**Method:** Compass-v3 employs a mixture-of-experts architecture with 245B parameters, using fewer but larger experts, and incorporates hardware-efficient optimizations. It leverages a mixed-training strategy on a large dataset tailored for e-commerce.

**Key Contributions:**

	1. Introduction of Compass-v3 model with a novel MoE architecture for e-commerce
	2. Optimal-Transport Direct Preference Optimization (OTPO) for better instruction adherence
	3. Strong multilingual capabilities for low-resource languages in Southeast Asia

**Result:** Compass-v3 achieves superior e-commerce performance compared to existing models like DeepSeek-V3.1 and GPT-4, with enhanced multilingual capabilities across Southeast Asian languages and Portuguese.

**Limitations:** 

**Conclusion:** The model has been successfully deployed in Shopee's platform, significantly improving LLM usage while demonstrating strong performance in both specialized and general tasks.

**Abstract:** Large language models (LLMs) excel in general-domain applications, yet their performance often degrades in specialized tasks requiring domain-specific knowledge. E-commerce is particularly challenging, as its data are noisy, heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and 71B active per token, designed for Southeast Asian e-commerce. Compass-v3 adopts fewer but larger experts, combined with hardware-efficient optimizations-such as intra-node expert parallelism and a customized memcpy operator-to maximize GPU utilization. The model is trained on 12T tokens of curated multilingual corpora and large-scale synthetic e-commerce instructions using a mixed-training strategy. To enhance alignment, we propose Optimal-Transport Direct Preference Optimization (OTPO), which captures token-level distinctions and improves instruction adherence in commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3 delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1, GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong multilingual capability across low-resource Southeast Asian languages (Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while sustaining competitive performance on general benchmarks. It has already been widely applied in Shopee's industrial-scale e-commerce platform and is gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM usage, highlighting its dual strengths in specialized commerce expertise and broad linguistic competence.

</details>


### [29] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)

*Liqun He, Jiaqi Xu*

**Main category:** cs.CL

**Keywords:** Generative AI, Dialogue Acts, Education, GPT-4, Automated Annotation

**Relevance Score:** 8

**TL;DR:** This study investigates using generative AI for automating the classification of educational dialogue acts, achieving high accuracy with GPT-4.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the time and effort involved in traditional manual coding of tutors' Dialogue Acts (DAs).

**Method:** Utilized the open-source CIMA corpus with pre-annotated tutor responses and tested GPT-3.5-turbo and GPT-4 models using tailored prompts.

**Key Contributions:**

	1. Demonstrated the efficacy of GPT-4 in DA classification with significant accuracy.
	2. Provided insights into the importance of task-specific labels and context for automated annotation quality.
	3. Highlighted ethical considerations in using generative AI for educational purposes.

**Result:** GPT-4 reached 80% accuracy and demonstrated substantial agreement with human annotations, surpassing baseline performance.

**Limitations:** Task-specific label definitions and contextual information are crucial for improving automated annotation quality.

**Conclusion:** Generative AI presents an efficient approach to DA classification with implications for educational dialogue analysis, while emphasizing the need for responsible AI practices.

**Abstract:** This study explores the use of generative AI for automating the classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and effort required by traditional manual coding. This case study uses the open-source CIMA corpus, in which tutors' responses are pre-annotated into four DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of 0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and indicating substantial agreement with human annotations. These findings suggest that generative AI has strong potential to provide an efficient and accessible approach to DA classification, with meaningful implications for educational dialogue analysis. The study also highlights the importance of task-specific label definitions and contextual information in enhancing the quality of automated annotation. Finally, it underscores the ethical considerations associated with the use of generative AI and the need for responsible and transparent research practices. The script of this research is publicly available at https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [30] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)

*Phuong-Nam Dang, Kieu-Linh Nguyen, Thanh-Hieu Pham*

**Main category:** cs.CL

**Keywords:** ViRanker, Vietnamese language, reranking, BGE-M3 encoder, information retrieval

**Relevance Score:** 4

**TL;DR:** ViRanker is a cross-encoder reranking model specifically designed for the Vietnamese language, demonstrating superior performance in early-rank accuracy over multilingual baselines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop effective reranking models for Vietnamese, a low-resource language, thereby enhancing information retrieval and accessing online content in Vietnamese.

**Method:** ViRanker is built on the BGE-M3 encoder and incorporates the Blockwise Parallel Transformer. It utilizes an 8 GB curated corpus for training and employs hybrid hard-negative sampling for fine-tuning.

**Key Contributions:**

	1. Introduction of ViRanker for Vietnamese reranking
	2. Use of hybrid hard-negative sampling for training
	3. Open release of the model to support reproducibility

**Result:** ViRanker achieves strong early-rank accuracy on the MMARCO-VI benchmark, outperforming multilingual models and competing closely with existing Vietnamese models like PhoRanker.

**Limitations:** 

**Conclusion:** This model not only advances reranking in Vietnamese but also demonstrates how adaptation and data curation can benefit underrepresented languages in retrieval tasks.

**Abstract:** This paper presents ViRanker, a cross-encoder reranking model tailored to the Vietnamese language. Built on the BGE-M3 encoder and enhanced with the Blockwise Parallel Transformer, ViRanker addresses the lack of competitive rerankers for Vietnamese, a low-resource language with complex syntax and diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with hybrid hard-negative sampling to strengthen robustness. Evaluated on the MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing multilingual baselines and competing closely with PhoRanker. By releasing the model openly on Hugging Face, we aim to support reproducibility and encourage wider adoption in real-world retrieval systems. Beyond Vietnamese, this study illustrates how careful architectural adaptation and data curation can advance reranking in other underrepresented languages.

</details>


### [31] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)

*Taha Binhuraib, Ruimin Gao, Anna A. Ivanova*

**Main category:** cs.CL

**Keywords:** neural encoding models, brain data, open-source library, fMRI, machine learning

**Relevance Score:** 6

**TL;DR:** LITcoder is an open-source library for building and benchmarking neural encoding models, facilitating the alignment of continuous stimuli with brain data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To lower technical barriers and facilitate methodological rigor in the implementation of neural encoding models for brain data.

**Method:** LITcoder provides a modular pipeline that allows researchers to compose, compare, and extend encoding models using various methodological design choices.

**Key Contributions:**

	1. Open-source library for neural encoding model implementation
	2. Modular pipeline for flexible model design and evaluation
	3. Built-in support for experiment tracking and integration with W&B

**Result:** The library's scalability and versatility were demonstrated by fitting encoding models to multiple datasets, highlighting critical methodological considerations for building models for fMRI data.

**Limitations:** 

**Conclusion:** LITcoder accelerates the development of high-performance predictive models of brain activity while fostering systematic comparisons across models and datasets.

**Abstract:** We introduce LITcoder, an open-source library for building and benchmarking neural encoding models. Designed as a flexible backend, LITcoder provides standardized tools for aligning continuous stimuli (e.g., text and speech) with brain data, transforming stimuli into representational features, mapping those features onto brain data, and evaluating the predictive performance of the resulting model on held-out data. The library implements a modular pipeline covering a wide array of methodological design choices, so researchers can easily compose, compare, and extend encoding models without reinventing core infrastructure. Such choices include brain datasets, brain regions, stimulus feature (both neural-net-based and control, such as word rate), downsampling approaches, and many others. In addition, the library provides built-in logging, plotting, and seamless integration with experiment tracking platforms such as Weights & Biases (W&B). We demonstrate the scalability and versatility of our framework by fitting a range of encoding models to three story listening datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore the methodological choices critical for building encoding models for continuous fMRI data, illustrating the importance of accounting for all tokens in a TR scan (as opposed to just taking the last one, even when contextualized), incorporating hemodynamic lag effects, using train-test splits that minimize information leakage, and accounting for head motion effects on encoding model predictivity. Overall, LITcoder lowers technical barriers to encoding model implementation, facilitates systematic comparisons across models and datasets, fosters methodological rigor, and accelerates the development of high-quality high-performance predictive models of brain activity.   Project page: https://litcoder-brain.github.io

</details>


### [32] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)

*Zhiyue Liu, Fanrong Ma, Xin Ling*

**Main category:** cs.CL

**Keywords:** multimodal sentiment classification, debiasing, counterfactual learning, contrastive learning, dataset bias

**Relevance Score:** 6

**TL;DR:** This paper presents a counterfactual-enhanced debiasing framework for multimodal sentiment classification that reduces spurious correlations in datasets by focusing on causal features and using adaptive contrastive learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing multimodal sentiment classification methods often over-rely on text and fail to address biases that lead to spurious correlations, impairing accuracy.

**Method:** The proposed framework utilizes counterfactual data augmentation to create image-text samples that guide attention towards sentiment-relevant content and incorporates an adaptive debiasing contrastive learning mechanism.

**Key Contributions:**

	1. Introduction of counterfactual data augmentation for sentiment analysis
	2. Adaptive debiasing contrastive learning to mitigate biased word effects
	3. Demonstration of superior performance compared to existing methods

**Result:** Experimental results demonstrate that the proposed method outperforms state-of-the-art baselines in mitigating the impact of dataset biases on sentiment classification accuracy.

**Limitations:** 

**Conclusion:** The methods introduced enhance the robustness of sentiment classification in the presence of biased textual features by focusing on sentiment-related causal features and employing novel learning techniques.

**Abstract:** Target-oriented multimodal sentiment classification seeks to predict sentiment polarity for specific targets from image-text pairs. While existing works achieve competitive performance, they often over-rely on textual content and fail to consider dataset biases, in particular word-level contextual biases. This leads to spurious correlations between text features and output labels, impairing classification accuracy. In this paper, we introduce a novel counterfactual-enhanced debiasing framework to reduce such spurious correlations. Our framework incorporates a counterfactual data augmentation strategy that minimally alters sentiment-related causal features, generating detail-matched image-text samples to guide the model's attention toward content tied to sentiment. Furthermore, for learning robust features from counterfactual data and prompting model decisions, we introduce an adaptive debiasing contrastive learning mechanism, which effectively mitigates the influence of biased words. Experimental results on several benchmark datasets show that our proposed method outperforms state-of-the-art baselines.

</details>


### [33] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)

*Yuhao Zhang, Yuhao Du, Zhanchen Dai, Xiangnan Ma, Kaiqi Kou, Benyou Wang, Haizhou Li*

**Main category:** cs.CL

**Keywords:** speech-to-speech models, semantic representations, knowledge-based question-answering

**Relevance Score:** 7

**TL;DR:** EchoX is a novel speech-to-speech large language model that integrates acoustic and semantic learning to enhance reasoning and knowledge capabilities that typically degrade in existing models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current speech-to-speech large language models struggle with knowledge and reasoning due to ineffective training paradigms that do not adequately bridge the acoustic-semantic gap.

**Method:** EchoX utilizes semantic representations to dynamically generate speech training targets, merging both acoustic and semantic learning processes.

**Key Contributions:**

	1. Introduction of EchoX, a speech-to-speech LLM integrating acoustic and semantic learning.
	2. Demonstrated advanced performance on knowledge-based tasks with extensive training data.
	3. Open-source availability for further research and development.

**Result:** EchoX demonstrates significant performance improvements on various knowledge-based question-answering benchmarks, trained on approximately six thousand hours of data.

**Limitations:** 

**Conclusion:** The integration of semantic learning in training speech models helps maintain strong reasoning abilities, showing promise for future research and applications in this area.

**Abstract:** Speech-to-speech large language models (SLLMs) are attracting increasing attention. Derived from text-based large language models (LLMs), SLLMs often exhibit degradation in knowledge and reasoning capabilities. We hypothesize that this limitation arises because current training paradigms for SLLMs fail to bridge the acoustic-semantic gap in the feature representation space. To address this issue, we propose EchoX, which leverages semantic representations and dynamically generates speech training targets. This approach integrates both acoustic and semantic learning, enabling EchoX to preserve strong reasoning abilities as a speech LLM. Experimental results demonstrate that EchoX, with about six thousand hours of training data, achieves advanced performance on multiple knowledge-based question-answering benchmarks. The project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [34] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)

*Chin Yuen Kwok, Jia Qi yip*

**Main category:** cs.CL

**Keywords:** ASR, rare word recognition, biasing, word error rate, Whisper

**Relevance Score:** 6

**TL;DR:** This paper proposes an improved method for biasing rare word recognition in ASR models by avoiding computationally expensive revocation steps.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses limitations in existing Trie-based biasing approaches in ASR models, which struggle with computational efficiency and effectiveness in recognizing rare words.

**Method:** The authors introduce a method for ASR models to predict multiple steps at once instead of relying on revocation of bonuses, leading to a more efficient approach to generating rare words.

**Key Contributions:**

	1. Introduction of a look-ahead prediction method for ASR models
	2. Reduction of word error rate significantly
	3. Fine-tuning Whisper model with limited synthetic data boosts performance

**Result:** The new method, tested by fine-tuning Whisper with 10 hours of synthetic data, significantly reduces the word error rate on the NSC Part 2 test set, from 30.86% to 12.19%.

**Limitations:** 

**Conclusion:** The proposed approach presents a novel solution to improve rare word recognition in ASR, enhancing efficiency and accuracy without the drawbacks of existing methods.

**Abstract:** Contextual biasing improves rare word recognition of ASR models by prioritizing the output of rare words during decoding. A common approach is Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g. "Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the full word ("Bonham") isn't ultimately recognized, the system revokes those earlier bonuses. This revocation is limited to beam search and is computationally expensive, particularly for models with large decoders. To overcome these limitations, we propose adapting ASR models to look ahead and predict multiple steps at once. This avoids the revocation step entirely by better estimating whether a partial hypothesis will lead to the generation of the full rare word. By fine-tuning Whisper with only 10 hours of synthetic data, our method reduces the word error rate on the NSC Part 2 test set from 30.86% to 12.19%.

</details>


### [35] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)

*Chin Yuen Kwok, Jia Qi Yip, Eng Siong Chng*

**Main category:** cs.CL

**Keywords:** Rare word recognition, Contextual biasing, ASR models

**Relevance Score:** 6

**TL;DR:** This paper enhances rare word recognition in ASR models by introducing a keyword-aware loss function to improve the contextual biasing approach using synthetic data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve rare word recognition in ASR models, especially when trained on synthetic data, and to prevent overfitting to synthetic audio artifacts.

**Method:** The paper proposes a keyword-aware loss function that incorporates a masked cross-entropy term for predicting biased words and a binary classification term for detecting their positions during training.

**Key Contributions:**

	1. Introduction of a keyword-aware loss function for biasing modules
	2. Reduction of word error rate for rare words using the new loss function
	3. Demonstration of effective adaptation of the Whisper model to synthetic data

**Result:** The proposed method effectively reduced the word error rate on the NSC Part 2 test set from 29.71% to 11.81% when applied to the modified Whisper model.

**Limitations:** Potential overfitting due to synthetic data artifacts may still persist; the approach primarily focuses on rare word recognition which may not generalize to all contexts.

**Conclusion:** Using a keyword-aware loss function allows for better adaptation of ASR models for rare word recognition, leading to significant improvements in performance.

**Abstract:** Rare word recognition can be improved by adapting ASR models to synthetic data that includes these words. Further improvements can be achieved through contextual biasing, which trains and adds a biasing module into the model architecture to prioritize rare words. While training the module on synthetic rare word data is more effective than using non-rare-word data, it can lead to overfitting due to artifacts in the synthetic audio. To address this, we enhance the TCPGen-based contextual biasing approach and propose a keyword-aware loss function that additionally focuses on biased words when training biasing modules. This loss includes a masked cross-entropy term for biased word prediction and a binary classification term for detecting biased word positions. These two terms complementarily support the decoding of biased words during inference. By adapting Whisper to 10 hours of synthetic data, our method reduced the word error rate on the NSC Part 2 test set from 29.71% to 11.81%.

</details>


### [36] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)

*Talia Sternberg, Michael London, David Omer, Yossi Adi*

**Main category:** cs.CL

**Keywords:** Marmoset monkeys, vocal communication, Generative Marmoset Spoken Language Modeling, neuroscience, bioacoustics

**Relevance Score:** 2

**TL;DR:** This paper introduces Generative Marmoset Spoken Language Modeling (GmSLM), a pipeline for analyzing the vocal communication of Marmoset monkeys, showing its efficacy in generating and evaluating vocalizations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to challenge the notion that nonhuman primate vocal communication is entirely innate and to understand the link between vocal communication and brain activity.

**Method:** GmSLM employs an optimized spoken language model pipeline for Marmoset vocal communication, utilizing zero-shot evaluation metrics with unsupervised in-the-wild data and weakly labeled conversational data.

**Key Contributions:**

	1. Introduction of GmSLM for analyzing Marmoset vocal communication
	2. Development of novel zero-shot evaluation metrics for vocalization assessment
	3. Demonstration of GmSLM's effectiveness in generating realistic vocal samples

**Result:** GmSLM generated vocalizations that closely matched real resynthesized samples acoustically and displayed strong performance in downstream tasks.

**Limitations:** 

**Conclusion:** GmSLM can distinguish between real and artificial conversations and could support further studies on the neural basis of vocal communication, contributing to fields like neuroscience and bioacoustics.

**Abstract:** Marmoset monkeys exhibit complex vocal communication, challenging the view that nonhuman primates vocal communication is entirely innate, and show similar features of human speech, such as vocal labeling of others and turn-taking. Studying their vocal communication offers a unique opportunity to link it with brain activity-especially given the difficulty of accessing the human brain in speech and language research. Since Marmosets communicate primarily through vocalizations, applying standard LLM approaches is not straightforward. We introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized spoken language model pipeline for Marmoset vocal communication. We designed a novel zero-shot evaluation metrics using unsupervised in-the-wild data, alongside weakly labeled conversational data, to assess GmSLM and demonstrate its advantage over a basic human-speech-based baseline. GmSLM generated vocalizations closely matched real resynthesized samples acoustically and performed well on downstream tasks. Despite being fully unsupervised, GmSLM effectively distinguish real from artificial conversations and may support further investigations of the neural basis of vocal communication and provides a practical framework linking vocalization and brain activity. We believe GmSLM stands to benefit future work in neuroscience, bioacoustics, and evolutionary biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [37] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)

*Wenhao Li, Bangcheng Sun, Weihao Ye, Tianyi Zhang, Daohai Yu, Fei Chao, Rongrong Ji*

**Main category:** cs.CL

**Keywords:** context compression, long-context modeling, language models

**Relevance Score:** 8

**TL;DR:** This work presents a novel context compression framework, CCF, for efficient long-context modeling in language models that reduces memory burden while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational inefficiencies associated with extending language model contexts during training and inference.

**Method:** The CCF framework utilizes hierarchical latent representations, segment-wise semantic aggregation, and key-value memory encoding for compact representation creation followed by an optimization strategy with incremental segment decoding and sparse reservoir sampling.

**Key Contributions:**

	1. Introduction of the CCF context compression framework
	2. Integration of semantic aggregation with memory encoding
	3. A novel optimization strategy for training efficiency

**Result:** CCF outperforms existing methods in memory efficiency and throughput while maintaining competitive perplexity at high compression ratios across multiple long-context language modeling benchmarks.

**Limitations:** 

**Conclusion:** Structured compression methods like CCF show significant potential for improving the scalability and effectiveness of long-context modeling in language tasks.

**Abstract:** Scaling language models to longer contexts is essential for capturing rich dependencies across extended discourse. However, na\"ive context extension imposes significant computational and memory burdens, often resulting in inefficiencies during both training and inference. In this work, we propose CCF, a novel context compression framework designed to enable efficient long-context modeling by learning hierarchical latent representations that preserve global semantics while aggressively reducing input redundancy. CCF integrates segment-wise semantic aggregation with key-value memory encoding, forming compact representations that support accurate reconstruction and long-range understanding. To further enhance scalability, we introduce a training-efficient optimization strategy that couples incremental segment decoding with sparse reservoir sampling, substantially reducing memory overhead without degrading performance. Empirical results on multiple long-context language modeling benchmarks demonstrate that CCF achieves competitive perplexity under high compression ratios, and significantly improves throughput and memory efficiency compared to existing approaches. These findings highlight the potential of structured compression for scalable and effective long-context language modeling.

</details>


### [38] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)

*Brittany Harbison, Samuel Taubman, Travis Taylor, Ashok. K. Goel*

**Main category:** cs.CL

**Keywords:** personality detection, Big-Five traits, online learning, social connections, SAMI

**Relevance Score:** 7

**TL;DR:** This paper proposes a model that detects Big-Five personality traits from student forum posts to enhance matchmaking in online courses, facilitating better social connections among students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve social connections in online learning environments, which are often hindered by the lack of organic group formation.

**Method:** A personality detection model leveraging GPT's zero-shot capability to infer Big-Five personality traits from introductory forum posts, benchmarked against established models.

**Key Contributions:**

	1. Development of a personality detection model using GPTs
	2. Benchmarking against existing personality models
	3. Integration into SAMI's matchmaking system for enhanced social recommendations

**Result:** The model demonstrates efficacy in detecting personality traits, which were found to complement existing matchmaking factors within the SAMI system.

**Limitations:** Additional evaluation required to fully assess the impact of personality traits on engagement and match quality.

**Conclusion:** Integrating personality traits into the matchmaking process may improve student engagement and match quality, warranting further evaluation of their impact.

**Abstract:** Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.

</details>


### [39] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)

*Matan Cohen, Shira Shani, Eden Menahem, Yehudit Aperstein, Alexander Apartsin*

**Main category:** cs.CL

**Keywords:** large language models, seniority classification, resume analysis, bias mitigation, linguistic cues

**Relevance Score:** 9

**TL;DR:** This study evaluates large language models for classifying seniority in resumes, addressing challenges like overstated experience.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The task of accurately assessing candidate seniority is complicated by resume embellishment and ambiguous self-presentation, making automated classification vital.

**Method:** The study uses a hybrid dataset of real-world and synthetically generated resumes to test the performance of fine-tuned BERT models in seniority classification.

**Key Contributions:**

	1. Introduction of a hybrid dataset for seniority classification
	2. Evaluation of fine-tuned LLMs for resume analysis
	3. Insights into linguistic cues associated with seniority inflation

**Result:** The evaluation shows that large language models can effectively detect linguistic cues related to experience and expertise, improving candidate evaluation systems.

**Limitations:** 

**Conclusion:** AI-driven systems can enhance candidate evaluation by addressing biases from self-promotional language, and the dataset is shared for further research.

**Abstract:** Accurately assessing candidate seniority from resumes is a critical yet challenging task, complicated by the prevalence of overstated experience and ambiguous self-presentation. In this study, we investigate the effectiveness of large language models (LLMs), including fine-tuned BERT architectures, for automating seniority classification in resumes. To rigorously evaluate model performance, we introduce a hybrid dataset comprising both real-world resumes and synthetically generated hard examples designed to simulate exaggerated qualifications and understated seniority. Using the dataset, we evaluate the performance of Large Language Models in detecting subtle linguistic cues associated with seniority inflation and implicit expertise. Our findings highlight promising directions for enhancing AI-driven candidate evaluation systems and mitigating bias introduced by self-promotional language. The dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [40] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)

*Rishit Tyagi, Mohit Gupta, Rahul Bouri*

**Main category:** cs.CL

**Keywords:** Table QA, NL-to-SQL, Large Language Models, DataBench, Question Answering

**Relevance Score:** 8

**TL;DR:** This paper describes a novel NL-to-SQL approach using large language models for Question Answering over Tabular Data, focusing on a new benchmark for evaluating model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in Question Answering over Tabular Data due to varying table structures, sizes, and data types, aiming to improve accuracy in structured query responses.

**Method:** The proposed method employs a multi-stage pipeline that includes example selection, SQL query generation, answer extraction, verification, and iterative refinement, utilizing large language models like GPT-4o.

**Key Contributions:**

	1. Introduction of a benchmark for Table QA using diverse datasets
	2. Development of an effective multi-stage NL-to-SQL pipeline
	3. Demonstration of substantial performance improvements over baseline methods

**Result:** The approach achieved a 70.5% accuracy on DataBench QA and 71.6% on DataBench Lite QA, significantly outperforming baseline scores of 26% and 27%.

**Limitations:** The paper discusses strengths and weaknesses of LLMs in Table QA but does not detail specific limitations of the proposed method.

**Conclusion:** The results underline the potential of LLMs in enhancing Table QA tasks, offering insights into their strengths and limitations.

**Abstract:** Question Answering over Tabular Data (Table QA) presents unique challenges due to the diverse structure, size, and data types of real-world tables. The SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale, domain-diverse datasets to evaluate the ability of models to accurately answer structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a multi-stage pipeline involving example selection, SQL query generation, answer extraction, verification, and iterative refinement. Experiments demonstrate the effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and 71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\% and 27\% respectively. This paper details our methodology, experimental results, and alternative approaches, providing insights into the strengths and limitations of LLM-driven Table QA.

</details>


### [41] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)

*Grazia Sveva Ascione, Nicolò Tamagnone*

**Main category:** cs.CL

**Keywords:** patent classification, Sustainable Development Goals, weak supervision, large language models, multi-label regression

**Relevance Score:** 6

**TL;DR:** This paper presents a method for classifying patents according to their relevance to UN Sustainable Development Goals (SDGs) using weak supervision techniques with large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the classification of patents in relation to UN Sustainable Development Goals due to the absence of large labeled datasets for supervised learning.

**Method:** The authors develop a composite labeling function leveraging citations from SDG-tagged papers as a noisy input signal. They extract concepts from patents and SDG papers using LLMs, then compute similarity scores and calibrate these with a custom positive-only loss for better alignment with known links.

**Key Contributions:**

	1. Proposed a novel approach to patent classification using weak supervision and LLMs.
	2. Developed a composite labeling function that extracts relevant concepts from patents and SDG documents.
	3. Validated the method through internal and external strategies, demonstrating superior performance compared to traditional classification methods.

**Result:** The method produces a silver-standard multi-label dataset for patents related to SDGs, and it outperforms existing models in both internal and external validation strategies.

**Limitations:** 

**Conclusion:** Weak supervision combined with semantic alignment significantly enhances SDG classification scalability and effectiveness.

**Abstract:** Classifying patents by their relevance to the UN Sustainable Development Goals (SDGs) is crucial for tracking how innovation addresses global challenges. However, the absence of a large, labeled dataset limits the use of supervised learning. Existing methods, such as keyword searches, transfer learning, and citation-based heuristics, lack scalability and generalizability. This paper frames patent-to-SDG classification as a weak supervision problem, using citations from patents to SDG-tagged scientific publications (NPL citations) as a noisy initial signal. To address its sparsity and noise, we develop a composite labeling function (LF) that uses large language models (LLMs) to extract structured concepts, namely functions, solutions, and applications, from patents and SDG papers based on a patent ontology. Cross-domain similarity scores are computed and combined using a rank-based retrieval approach. The LF is calibrated via a custom positive-only loss that aligns with known NPL-SDG links without penalizing discovery of new SDG associations. The result is a silver-standard, soft multi-label dataset mapping patents to SDGs, enabling the training of effective multi-label regression models. We validate our approach through two complementary strategies: (1) internal validation against held-out NPL-based labels, where our method outperforms several baselines including transformer-based models, and zero-shot LLM; and (2) external validation using network modularity in patent citation, co-inventor, and co-applicant graphs, where our labels reveal greater thematic, cognitive, and organizational coherence than traditional technological classifications. These results show that weak supervision and semantic alignment can enhance SDG classification at scale.

</details>


### [42] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)

*Channdeth Sok, David Luz, Yacine Haddam*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, hallucination detection, MetaRAG, identity-aware AI

**Relevance Score:** 9

**TL;DR:** MetaRAG is a framework for detecting hallucinations in Retrieval-Augmented Generation (RAG) systems, enabling reliable deployment in enterprise applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs are often unreliable due to hallucinations, and existing methods do not address challenges specific to RAG systems.

**Method:** MetaRAG analyzes responses by breaking them into factoids, mutating them, verifying against retrieved evidence, and calculating a hallucination score.

**Key Contributions:**

	1. Introduces MetaRAG, a framework specifically for RAG systems
	2. Localizes unsupported claims to specific factoid spans
	3. Allows for the configuration of identity-aware safeguards based on hallucination scores.

**Result:** MetaRAG effectively detects hallucinations, providing safeguards for identity-sensitive queries in RAG-based systems based on experiments with a proprietary dataset.

**Limitations:** 

**Conclusion:** The proposed framework allows for better deployment of conversational agents while addressing the risks of hallucinations in LLMs.

**Abstract:** Large Language Models (LLMs) are increasingly deployed in enterprise applications, yet their reliability remains limited by hallucinations, i.e., confident but factually incorrect information. Existing detection approaches, such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not address the unique challenges of Retrieval-Augmented Generation (RAG) systems, where responses must be consistent with retrieved evidence. We therefore present MetaRAG, a metamorphic testing framework for hallucination detection in Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time, unsupervised, black-box setting, requiring neither ground-truth references nor access to model internals, making it suitable for proprietary and high-stakes domains. The framework proceeds in four stages: (1) decompose answers into atomic factoids, (2) generate controlled mutations of each factoid using synonym and antonym substitutions, (3) verify each variant against the retrieved context (synonyms are expected to be entailed and antonyms contradicted), and (4) aggregate penalties for inconsistencies into a response-level hallucination score. Crucially for identity-aware AI, MetaRAG localizes unsupported claims at the factoid span where they occur (e.g., pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility), allowing users to see flagged spans and enabling system designers to configure thresholds and guardrails for identity-sensitive queries. Experiments on a proprietary enterprise dataset illustrate the effectiveness of MetaRAG for detecting hallucinations and enabling trustworthy deployment of RAG-based conversational agents. We also outline a topic-based deployment design that translates MetaRAG's span-level scores into identity-aware safeguards; this design is discussed but not evaluated in our experiments.

</details>


### [43] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)

*Molly R Petersen, Claire E Stevenson, Lonneke van der Plas*

**Main category:** cs.CL

**Keywords:** analogical reasoning, natural language processing, cognitive science, relational understanding, NLP challenges

**Relevance Score:** 6

**TL;DR:** The paper explores the connection between analogical reasoning and natural language processing (NLP), highlighting its relevance in addressing various NLP challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap between cognitive science and NLP by applying analogical reasoning theory to enhance relational understanding in text processing.

**Method:** The paper reviews cognitive science literature on analogical reasoning and relates it to NLP challenges, emphasizing the importance of relational understanding over entity-level similarity.

**Key Contributions:**

	1. Linking cognitive science with NLP
	2. Identifying analogical reasoning processes applicable to NLP
	3. Promoting relational understanding in text processing

**Result:** Identified key cognitive processes related to analogical reasoning that can inform NLP research and improve natural language understanding.

**Limitations:** 

**Conclusion:** Cognitive theories of analogical reasoning can offer valuable insights for optimizing NLP systems, suggesting a shift in focus from entity-level to relational approaches.

**Abstract:** Analogical reasoning is an essential aspect of human cognition. In this paper, we summarize key theory about the processes underlying analogical reasoning from the cognitive science literature and relate it to current research in natural language processing. While these processes can be easily linked to concepts in NLP, they are generally not viewed through a cognitive lens. Furthermore, we show how these notions are relevant for several major challenges in NLP research, not directly related to analogy solving. This may guide researchers to better optimize relational understanding in text, as opposed to relying heavily on entity-level similarity.

</details>


### [44] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)

*Ana Ezquerro, Carlos Gómez-Rodríguez, David Vilares*

**Main category:** cs.CL

**Keywords:** dependency parsing, graph encoding, linearization

**Relevance Score:** 6

**TL;DR:** This paper introduces a new method for encoding dependency graphs as sequences to achieve efficient linear-time parsing, preserving structural information while reducing the label space.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and accuracy of dependency graph parsing methods.

**Method:** Encoding dependency graphs as sequences to allow for linear-time parsing with a reduced label space, while handling reentrancies, cycles, and empty nodes.

**Key Contributions:**

	1. Introduction of a novel encoding method for dependency graphs
	2. Linear-time parsing with reduced label space
	3. Evaluation and competitive results on multilingual benchmarks.

**Result:** Achieved competitive results on a multilingual and multi-formalism benchmark, with consistent improvements in exact match accuracy compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed encoding method successfully enhances dependency graph parsing efficiency and accuracy.

**Abstract:** We revisit hierarchical bracketing encodings from a practical perspective in the context of dependency graph parsing. The approach encodes graphs as sequences, enabling linear-time parsing with $n$ tagging actions, and still representing reentrancies, cycles, and empty nodes. Compared to existing graph linearizations, this representation substantially reduces the label space while preserving structural information. We evaluate it on a multilingual and multi-formalism benchmark, showing competitive results and consistent improvements over other methods in exact match accuracy.

</details>


### [45] [Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving](https://arxiv.org/abs/2501.02348)

*Sanghyun Park, Boris Maciejovsky, Phanish Puranam*

**Main category:** cs.CL

**Keywords:** Cognitive Flexibility, LLM, Synthetic Deliberation, Complex Problem-Solving, Artificial Intelligence

**Relevance Score:** 8

**TL;DR:** Synthetic deliberation is a method using LLMs to simulate discourse between diverse perspectives to enhance complex problem-solving.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations of mental simulation in complex problem-solving by introducing a method that allows for multiple viewpoints to be processed concurrently.

**Method:** Synthetic deliberation utilizes a custom GPT-based model to simulate discourse between agents representing diverse perspectives, enabling users to explore viewpoints without cognitive degradation.

**Key Contributions:**

	1. Introduction of synthetic deliberation using LLMs for cognitive flexibility
	2. Demonstrated benefits of concurrent processing of multiple perspectives
	3. Externalizes the deliberative process, enhancing decision-making capabilities

**Result:** The method shows improved processing of multiple viewpoints, parallel exploration of perspectives, and precise control over viewpoint synthesis, ultimately promoting effective deliberation.

**Limitations:** 

**Conclusion:** Synthetic deliberation offers a novel approach to cognitive flexibility that enhances strategic planning, policymaking, and conflict resolution.

**Abstract:** Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the "wisdom of crowds" within a single individual, allowing them to "think with many minds." While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation's limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution.

</details>


### [46] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)

*Zhaohan Zhang, Ziquan Liu, Ioannis Patras*

**Main category:** cs.CL

**Keywords:** Large Language Models, Confidence Elicitation, Calibration, Machine Learning, AI Safety

**Relevance Score:** 9

**TL;DR:** GrACE is a new method for reliable confidence elicitation in LLMs, enhancing performance in high-stakes fields like healthcare and finance by improving calibration without extra computational overhead.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Existing confidence elicitation methods for LLMs are either computationally expensive or poorly calibrated, making them unsuitable for real-world applications.

**Method:** GrACE uses a novel mechanism where confidence is derived from the similarity between the last hidden state of the model and a special token's embedding, allowing real-time calibration with accuracy-based targets.

**Key Contributions:**

	1. Introduces a novel mechanism for confidence elicitation based on hidden states and token embeddings.
	2. Demonstrates superior calibration and discriminative capacity compared to existing methods.
	3. Proposes strategies for improving test-time scaling based on confidence.

**Result:** GrACE shows improved discriminative capacity and calibration in confidence estimation over three LLMs tested on two benchmark datasets, surpassing six existing methods without needing additional sampling or auxiliary models.

**Limitations:** 

**Conclusion:** GrACE enables practical deployment of LLMs with scalable and reliable confidence estimation, enhancing accuracy and reducing sample requirements in high-stakes environments.

**Abstract:** Assessing the reliability of Large Language Models (LLMs) by confidence elicitation is a prominent approach to AI safety in high-stakes applications, such as healthcare and finance. Existing methods either require expensive computational overhead or suffer from poor calibration, making them impractical and unreliable for real-world deployment. In this work, we propose GrACE, a Generative Approach to Confidence Elicitation that enables scalable and reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in which the model expresses confidence by the similarity between the last hidden state and the embedding of a special token appended to the vocabulary, in real-time. We fine-tune the model for calibrating the confidence with calibration targets associated with accuracy. Experiments with three LLMs and two benchmark datasets show that the confidence produced by GrACE achieves the best discriminative capacity and calibration on open-ended generation tasks, outperforming six competing methods without resorting to additional sampling or an auxiliary model. Moreover, we propose two strategies for improving test-time scaling based on confidence induced by GrACE. Experimental results show that using GrACE not only improves the accuracy of the final decision but also significantly reduces the number of required samples in the test-time scaling scheme, indicating the potential of GrACE as a practical solution for deploying LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [47] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)

*Lucie Poláková, Martin Popel, Věra Kloudová, Michal Novák, Mariia Anisimova, Jiří Balhar*

**Main category:** cs.CL

**Keywords:** machine translation, multilingual education, Czech-Ukrainian translation, educational technology, linguistics

**Relevance Score:** 4

**TL;DR:** The EdUKate project develops multilingual educational materials and a Czech-Ukrainian machine translation system for Czech schools.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance digital education through multilingual resources and support non-Czech-speaking students in Czech primary and secondary education.

**Method:** Collaboration between a Czech academic institution and an educational publisher to create and evaluate machine translation systems for educational content.

**Key Contributions:**

	1. Creation of a direct Czech-Ukrainian machine translation system for education.
	2. Translation of a large volume of interactive educational material.
	3. Focus on processing formatted content and specific terminology for educational contexts.

**Result:** Development of up to 9,000 multimodal interactive exercises translated into Ukrainian, English, and German, with a focus on handling technical and scientific terminology.

**Limitations:** The survey and findings are based on initial observations and may require further validation and refinement in future studies.

**Conclusion:** The project results in freely available educational applications that support multilingual teaching and learning in Czech schools.

**Abstract:** The EdUKate project combines digital education, linguistics, translation studies, and machine translation to develop multilingual learning materials for Czech primary and secondary schools. Launched through collaboration between a major Czech academic institution and the country's largest educational publisher, the project is aimed at translating up to 9,000 multimodal interactive exercises from Czech into Ukrainian, English, and German for an educational web portal. It emphasizes the development and evaluation of a direct Czech-Ukrainian machine translation system tailored to the educational domain, with special attention to processing formatted content such as XML and PDF and handling technical and scientific terminology. We present findings from an initial survey of Czech teachers regarding the needs of non-Czech-speaking students and describe the system's evaluation and implementation on the web portal. All resulting applications are freely available to students, educators, and researchers.

</details>


### [48] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)

*Vadim Zadykian, Bruno Andrade, Haithem Afli*

**Main category:** cs.CL

**Keywords:** Semantic Textual Relatedness, Knowledge Graphs, Job Title Matching, Embedding Models, Human Resources

**Relevance Score:** 8

**TL;DR:** This study introduces a self-supervised hybrid architecture for Semantic Textual Relatedness (STR) in job title matching, enhancing explainability and model performance using domain-specific Knowledge Graphs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving resume recommendation systems due to the limitations of lexical similarity in job title matching.

**Method:** A self-supervised hybrid architecture combining dense sentence embeddings with domain-specific Knowledge Graphs (KGs), emphasizing data stratification by partitioning STR scores.

**Key Contributions:**

	1. Self-supervised hybrid architecture incorporating Knowledge Graphs
	2. Stratified evaluation of STR scores for granular performance analysis
	3. Demonstration of significant RMSE improvements in high-STR evaluations

**Result:** Fine-tuned SBERT models augmented with KGs showed a 25% RMSE reduction in the high-STR region compared to strong baselines.

**Limitations:** 

**Conclusion:** The combination of KGs with text embeddings enhances model performance and provides critical insights through regional performance analysis.

**Abstract:** Semantic Textual Relatedness (STR) captures nuanced relationships between texts that extend beyond superficial lexical similarity. In this study, we investigate STR in the context of job title matching - a key challenge in resume recommendation systems, where overlapping terms are often limited or misleading. We introduce a self-supervised hybrid architecture that combines dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to improve both semantic alignment and explainability. Unlike previous work that evaluated models on aggregate performance, our approach emphasizes data stratification by partitioning the STR score continuum into distinct regions: low, medium, and high semantic relatedness. This stratified evaluation enables a fine-grained analysis of model performance across semantically meaningful subspaces. We evaluate several embedding models, both with and without KG integration via graph neural networks. The results show that fine-tuned SBERT models augmented with KGs produce consistent improvements in the high-STR region, where the RMSE is reduced by 25% over strong baselines. Our findings highlight not only the benefits of combining KGs with text embeddings, but also the importance of regional performance analysis in understanding model behavior. This granular approach reveals strengths and weaknesses hidden by global metrics, and supports more targeted model selection for use in Human Resources (HR) systems and applications where fairness, explainability, and contextual matching are essential.

</details>


### [49] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)

*Daniil Ignatev, Nan Li, Hugh Mee Wong, Anh Dang, Shane Kaszefski Yaschuk*

**Main category:** cs.CL

**Keywords:** in-context learning, label distribution learning, large language models, soft label predictions

**Relevance Score:** 7

**TL;DR:** This paper discusses the DeMeVa team's experimentation with in-context learning and label distribution learning for predicting annotator-specific annotations in a shared task.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of predicting annotator-specific annotations in the Learning with Disagreements shared task by exploring two innovative approaches.

**Method:** We compare example sampling strategies in in-context learning with large language models and evaluate fine-tuning methods in label distribution learning using RoBERTa.

**Key Contributions:**

	1. Demonstrated effectiveness of ICL in predicting perspectivist annotations.
	2. Showed that aggregating ICL predictions into soft labels yields competitive results.
	3. Highlighted the potential of LDL methods for soft label predictions.

**Result:** In-context learning effectively predicts annotator-specific annotations, and aggregating these into soft labels demonstrates competitive performance. Label distribution learning shows promise for soft label predictions.

**Limitations:** 

**Conclusion:** ILC and LDL methods are beneficial for predicting soft labels, calling for more research in this area by the perspectivist community.

**Abstract:** This system paper presents the DeMeVa team's approaches to the third edition of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et al., 2025). We explore two directions: in-context learning (ICL) with large language models, where we compare example sampling strategies; and label distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we evaluate several fine-tuning methods. Our contributions are twofold: (1) we show that ICL can effectively predict annotator-specific annotations (perspectivist annotations), and that aggregating these predictions into soft labels yields competitive performance; and (2) we argue that LDL methods are promising for soft label predictions and merit further exploration by the perspectivist community.

</details>


### [50] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)

*Paolo Pedinotti, Peter Baumann, Nathan Jessurun, Leslie Barrett, Enrico Santus*

**Main category:** cs.CL

**Keywords:** Large Language Models, Financial NLP, Knowledge Graphs, Research Trends, MetaGraph

**Relevance Score:** 4

**TL;DR:** The paper introduces MetaGraph, a methodology for extracting and analyzing knowledge graphs from financial NLP literature, highlighting research trends and methodologies using LLM advancements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a structured analysis of the rapidly evolving landscape of financial NLP enabled by LLMs, which traditional surveys have failed to keep pace with.

**Method:** MetaGraph employs an LLM-based extraction pipeline to analyze 681 scientific papers published between 2022 and 2025 to construct a knowledge graph and identify trends.

**Key Contributions:**

	1. Development of MetaGraph for knowledge graph extraction in financial NLP.
	2. Identification of key phases in LLM adoption in financial NLP research.
	3. Provision of a reusable approach for analyzing research trends in various fields.

**Result:** The analysis reveals three key phases in financial NLP: early LLM adoption, critical evaluation of LLM limitations, and integration of related techniques into systems.

**Limitations:** Focus primarily on financial NLP may limit generalizability to other domains without adaptations.

**Conclusion:** MetaGraph offers a clear, structured view of financial NLP evolution and denotes a methodology applicable to mapping scientific progress across different domains.

**Abstract:** Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling new tasks and driving a proliferation of datasets and diversification of data sources. Yet, this transformation has outpaced traditional surveys. In this paper, we present MetaGraph, a generalizable methodology for extracting knowledge graphs from scientific literature and analyzing them to obtain a structured, queryable view of research trends. We define an ontology for financial NLP research and apply an LLM-based extraction pipeline to 681 papers (2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals three key phases: early LLM adoption and task/dataset innovation; critical reflection on LLM limitations; and growing integration of peripheral techniques into modular systems. This structured view offers both practitioners and researchers a clear understanding of how financial NLP has evolved - highlighting emerging trends, shifting priorities, and methodological shifts-while also demonstrating a reusable approach for mapping scientific progress in other domains.

</details>


### [51] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)

*Brittany Harbison, Samuel Taubman, Travis Taylor, Ashok. K. Goel*

**Main category:** cs.CL

**Keywords:** personality detection, social matchmaking, online courses

**Relevance Score:** 7

**TL;DR:** The paper presents a personality detection model using GPTs to enhance social matchmaking in online courses by inferring personality traits from student posts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve student connections in online learning environments through personality detection and enhance the SAMI matchmaking system.

**Method:** Utilized GPT's zero-shot capability to infer Big-Five personality traits from students' forum introduction posts and benchmarked performance against established models.

**Key Contributions:**

	1. Introduction of a GPT-based personality detection model
	2. Integration of personality traits into an entity-based matchmaking system
	3. Benchmarking performance against established models for personality inference

**Result:** Demonstrated the efficacy of the personality detection model in accurately inferring traits, which were integrated into SAMI's matchmaking system, showing promise in improving social connections.

**Limitations:** Additional evaluation is necessary to fully understand the impact of personality traits on engagement and match quality.

**Conclusion:** Initial integration suggests personality traits can enhance matchmaking factors, needing further evaluation to assess their impact on engagement and match quality.

**Abstract:** Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.

</details>


### [52] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)

*Bangzhao Shu, Isha Joshi, Melissa Karnaze, Anh C. Pham, Ishita Kakkar, Sindhu Kothe, Arpine Hovasapian, Mai ElSherief*

**Main category:** cs.CL

**Keywords:** Large Language Models, Emotion Recognition, Mental Health, Benchmark Dataset, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper introduces EXPRESS, a benchmark dataset for evaluating the fine-grained alignment of Large Language Models (LLMs) with human emotions, revealing limitations in LLMs' emotional understanding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating LLMs' capabilities in emotion recognition at a nuanced level, addressing the gap in existing research which often oversimplifies emotion categories.

**Method:** A benchmark dataset, EXPRESS, is created from Reddit communities with 251 fine-grained emotion labels; LLMs are systematically tested across various prompt settings to assess prediction accuracy and alignment with human emotions.

**Key Contributions:**

	1. Introduction of the EXPRESS benchmark dataset for fine-grained emotion evaluation
	2. Assessment framework for comparing LLMs' emotional predictions
	3. Highlighting LLMs' limitations in contextual emotional understanding

**Result:** Systematic testing indicates that while LLMs generate emotion terms aligned with established theories, predicting human self-disclosed emotions remains challenging due to contextual cue misalignment.

**Limitations:** LLMs may fail to capture contextual cues effectively compared to human self-disclosures.

**Conclusion:** The study highlights LLM limitations in fine-grained emotional context understanding and suggests directions for future improvements in this area.

**Abstract:** The versatility of Large Language Models (LLMs) in natural language understanding has made them increasingly popular in mental health research. While many studies explore LLMs' capabilities in emotion recognition, a critical gap remains in evaluating whether LLMs align with human emotions at a fine-grained level. Existing research typically focuses on classifying emotions into predefined, limited categories, overlooking more nuanced expressions. To address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit communities featuring 251 fine-grained, self-disclosed emotion labels. Our comprehensive evaluation framework examines predicted emotion terms and decomposes them into eight basic emotions using established emotion theories, enabling a fine-grained comparison. Systematic testing of prevalent LLMs under various prompt settings reveals that accurately predicting emotions that align with human self-disclosed emotions remains challenging. Qualitative analysis further shows that while certain LLMs generate emotion terms consistent with established emotion theories and definitions, they sometimes fail to capture contextual cues as effectively as human self-disclosures. These findings highlight the limitations of LLMs in fine-grained emotion alignment and offer insights for future research aimed at enhancing their contextual understanding.

</details>


### [53] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)

*Yiqun T. Chen, Tyler H. McCormick, Li Liu, Abhirup Datta*

**Main category:** cs.CL

**Keywords:** verbal autopsy, Large Language Models, health informatics, cause-of-death prediction, low-resource settings

**Relevance Score:** 9

**TL;DR:** The study evaluates the use of Large Language Models for improving verbal autopsy accuracy in estimating causes of death in resource-limited settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of estimating causes of death where medical certification is lacking, particularly in low-resource settings.

**Method:** A proof-of-concept pipeline named LA-VA was developed, integrating LLMs with traditional algorithms and embedding-based classification using the PHMRC dataset.

**Key Contributions:**

	1. Introduction of LA-VA pipeline combining LLMs and traditional methods
	2. Demonstrated improved prediction accuracy using GPT-5
	3. Provided insights for enhancing global health surveillance in low-resource contexts.

**Result:** GPT-5 outperforms traditional machine learning methods, achieving the highest accuracies in cause-of-death predictions across different age categories.

**Limitations:** 

**Conclusion:** The research indicates that LLM-assisted methods can significantly enhance the accuracy of verbal autopsy, impacting global health surveillance positively.

**Abstract:** Verbal autopsy (VA) is a critical tool for estimating causes of death in resource-limited settings where medical certification is unavailable. This study presents LA-VA, a proof-of-concept pipeline that combines Large Language Models (LLMs) with traditional algorithmic approaches and embedding-based classification for improved cause-of-death prediction. Using the Population Health Metrics Research Consortium (PHMRC) dataset across three age categories (Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches: GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles. Our results demonstrate that GPT-5 achieves the highest individual performance with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5% (Neonate), outperforming traditional statistical machine learning baselines by 5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches could substantially improve verbal autopsy accuracy, with important implications for global health surveillance in low-resource settings.

</details>


### [54] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)

*Minghang Zhu, Zhengliang Shi, Zhiwei Xu, Shiguang Wu, Lingjie Wang, Pengjie Ren, Zhaochun Ren, Zhumin Chen*

**Main category:** cs.CL

**Keywords:** multi-agent systems, alignment, subgoal, grounding agent, machine learning

**Relevance Score:** 9

**TL;DR:** MOAT is a framework for improving collaboration in multi-agent systems by iteratively aligning planning and grounding agents, resulting in better task performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for tuning multi-agent systems independently lead to capability gaps and poor coordination among agents.

**Method:** The MOAT framework employs an iterative alignment process with two stages: optimizing the planning agent for subgoal generation and fine-tuning the grounding agent using diverse subgoal-action pairs.

**Key Contributions:**

	1. Introduction of the MOAT framework for multi-agent alignment
	2. Demonstrated iterative alignment process for improving agent cooperation
	3. Empirical results showing superior performance compared to existing methods

**Result:** MOAT achieves significant performance improvements over state-of-the-art methods, with average increases of 3.1% on held-in tasks and 4.4% on held-out tasks across six benchmarks.

**Limitations:** 

**Conclusion:** Theoretical analysis confirms MOAT's effectiveness in creating a progressively convergent training process that enhances agent collaboration.

**Abstract:** The advancement of large language models (LLMs) has enabled the construction of multi-agent systems to solve complex tasks by dividing responsibilities among specialized agents, such as a planning agent for subgoal generation and a grounding agent for executing tool-use actions. Most existing methods typically fine-tune these agents independently, leading to capability gaps among them with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint Alignment Tuning framework that improves agents collaboration through iterative alignment. MOAT alternates between two key stages: (1) Planning Agent Alignment, which optimizes the planning agent to generate subgoal sequences that better guide the grounding agent; and (2) Grounding Agent Improving, which fine-tunes the grounding agent using diverse subgoal-action pairs generated by the agent itself to enhance its generalization capablity. Theoretical analysis proves that MOAT ensures a non-decreasing and progressively convergent training process. Experiments across six benchmarks demonstrate that MOAT outperforms state-of-the-art baselines, achieving average improvements of 3.1% on held-in tasks and 4.4% on held-out tasks.

</details>


### [55] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)

*Siddarth Mamidanna, Daking Rai, Ziyu Yao, Yilun Zhou*

**Main category:** cs.CL

**Keywords:** large language models, mental math, token computation, information transfer, CAMA, ABP

**Relevance Score:** 8

**TL;DR:** This paper analyzes the inner workings of large language models (LLMs) in performing mental math tasks, introducing techniques to isolate computations at specific layers and identifying a subgraph critical for performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to understand how LLMs execute computational tasks, specifically mental math, and to investigate the extent of token processing and information flow within the models.

**Method:** The approach involves inhibiting token computations in initial layers, restricting information transfer routes in subsequent layers, and enforcing computations at the last token in later layers. Two techniques, Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), are applied to identify an efficient subgraph for performance: the All-for-One subgraph (AF1).

**Key Contributions:**

	1. Introduction of Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP) techniques.
	2. Identification of the All-for-One subgraph (AF1) for mental math tasks.
	3. Demonstration of the subgraph's necessity and sufficiency for model performance across tasks.

**Result:** The experiments reveal that the AF1 subgraph is sufficient and necessary for achieving high accuracy on mental math tasks, showing cross-model applicability and effectiveness across different input styles.

**Limitations:** 

**Conclusion:** The study concludes that meaningful computations occur late in the model layers, specifically at the last token, and highlights the advantages of the proposed techniques in isolating computational strategies within LLMs.

**Abstract:** Large language models (LLMs) demonstrate proficiency across numerous computational tasks, yet their inner workings remain unclear. In theory, the combination of causal self-attention and multilayer perceptron layers allows every token to access and compute information based on all preceding tokens. In practice, to what extent are such operations present? In this paper, on mental math tasks (i.e., direct math calculation via next-token prediction without explicit reasoning), we investigate this question in three steps: inhibiting input-specific token computations in the initial layers, restricting the routes of information transfer across token positions in the next few layers, and forcing all computation to happen at the last token in the remaining layers. With two proposed techniques, Context-Aware Mean Ablation (CAMA) and Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with high accuracy on a wide variety of mental math tasks, where meaningful computation occurs very late (in terms of layer depth) and only at the last token, which receives information of other tokens in few specific middle layers. Experiments on a variety of models and arithmetic expressions show that this subgraph is sufficient and necessary for high model performance, transfers across different models, and works on a variety of input styles. Ablations on different CAMA and ABP alternatives reveal their unique advantages over other methods, which may be of independent interest.

</details>


### [56] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)

*Mohsen Fayyaz, Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Ryan Rossi, Trung Bui, Hinrich Schütze, Nanyun Peng*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, Large Language Models, behavior control, safety, faithfulness

**Relevance Score:** 9

**TL;DR:** SteerMoE framework enables controlled steering of Mixture-of-Experts in LLMs to enhance performance metrics like safety and faithfulness without retraining.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve the behavior of LLMs without the overhead of retraining, particularly in relation to safety and faithfulness, led to the development of SteerMoE.

**Method:** The framework detects which experts have distinct activation patterns based on paired inputs showing differing behaviors, allowing for selective (de)activation during inference.

**Key Contributions:**

	1. Development of SteerMoE for controlling expert behavior in LLMs
	2. Demonstration of significant improvements in safety and faithfulness metrics
	3. Identification of vulnerabilities in safety mechanisms related to expert activation patterns

**Result:** SteerMoE shows an increase in safety by up to +20% and faithfulness by +27% across 11 benchmarks and 6 LLMs, though it exposes vulnerabilities when used in adversarial modes.

**Limitations:** The method opens up new vulnerabilities in adversarial contexts and relies on the existing framework of LLMs without addressing deeper alignment issues.

**Conclusion:** SteerMoE provides a new way to control expert behavior in LLMs, revealing hidden risks while enhancing certain performance aspects.

**Abstract:** Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. We present SteerMoE, a framework for steering MoE models by detecting and controlling behavior-linked experts. Our detection method identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors. By selectively (de)activating such experts during inference, we control behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to +20% and faithfulness by +27%. In adversarial attack mode, it drops safety by -41% alone, and -100% when combined with existing jailbreak methods, bypassing all safety guardrails and exposing a new dimension of alignment faking hidden within experts.

</details>


### [57] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)

*Runpeng Dai, Linfeng Song, Haolin Liu, Zhenwen Liang, Dian Yu, Haitao Mi, Zhaopeng Tu, Rui Liu, Tong Zheng, Hongtu Zhu, Dong Yu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Curiosity-Driven Exploration, AIME benchmarks, Exploration bonuses

**Relevance Score:** 8

**TL;DR:** This paper proposes Curiosity-Driven Exploration (CDE) to enhance Reinforcement Learning with Verifiable Rewards (RLVR) in Large Language Models by improving exploration and addressing issues like premature convergence and entropy collapse.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current RLVR methods for LLMs struggle with exploration, leading to premature convergence. The motivation is to improve exploration through a novel framework that employs intrinsic curiosity.

**Method:** The paper introduces Curiosity-Driven Exploration (CDE), formalizing curiosity using perplexity for the actor and variance of value estimates for the critic, which serve as exploration bonuses.

**Key Contributions:**

	1. Introduction of Curiosity-Driven Exploration (CDE) framework.
	2. Formalization of curiosity signals to guide exploration in RLVR.
	3. Empirical improvement on AIME benchmarks, highlighting exploration's impact on performance.

**Result:** Empirical results show a +3 point improvement over standard RLVR methodologies on AIME benchmarks due to enhanced exploration.

**Limitations:** The calibration collapse mechanism mentioned needs further investigation to fully understand its implications in various scenarios.

**Conclusion:** CDE effectively counteracts calibration collapse in RLVR, promoting better performance in LLMs and providing insights into common failure modes.

**Abstract:** Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm for enhancing the reasoning ability of Large Language Models (LLMs). Yet current RLVR methods often explore poorly, leading to premature convergence and entropy collapse. To address this challenge, we introduce Curiosity-Driven Exploration (CDE), a framework that leverages the model's own intrinsic sense of curiosity to guide exploration. We formalize curiosity with signals from both the actor and the critic: for the actor, we use perplexity over its generated response, and for the critic, we use the variance of value estimates from a multi-head architecture. Both signals serve as an exploration bonus within the RLVR framework to guide the model. Our theoretical analysis shows that the actor-wise bonus inherently penalizes overconfident errors and promotes diversity among correct responses; moreover, we connect the critic-wise bonus to the well-established count-based exploration bonus in RL. Empirically, our method achieves an approximate +3 point improvement over standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a calibration collapse mechanism within RLVR, shedding light on common LLM failure modes.

</details>


### [58] [ASTPrompter: Preference-Aligned Automated Language Model Red-Teaming to Generate Low-Perplexity Unsafe Prompts](https://arxiv.org/abs/2407.09447)

*Amelia F. Hardy, Houjun Liu, Allie Griffith, Bernard Lange, Duncan Eddy, Mykel J. Kochenderfer*

**Main category:** cs.CL

**Keywords:** LLM red-teaming, low perplexity prompts, contrastive preference learning, attack success rate, perplexity

**Relevance Score:** 7

**TL;DR:** This paper introduces ASTPrompter, a method for optimizing low-perplexity attacks in LLM red-teaming, achieving a significantly higher attack success rate.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the oversight in existing LLM red-teaming methods which prioritize high attack success rates at the expense of low-perplexity prompts, thereby increasing the chance of evasion during benign use.

**Method:** ASTPrompter employs contrastive preference learning for a single-step optimization to balance high attack success rates with low perplexity prompting.

**Key Contributions:**

	1. Introduction of ASTPrompter for low-perplexity attack optimization in LLM red-teaming
	2. Demonstrating a significant increase in attack success rate with lower perplexity
	3. Highlighting the under-considered role of perplexity in red-teaming methods.

**Result:** ASTPrompter achieves an attack success rate 5.1 times higher on Llama-8.1B, using inputs 2.1 times more likely to occur according to the frozen LLM, and it transfers successfully to other models.

**Limitations:** 

**Conclusion:** The study emphasizes the importance of perplexity in LLM red-teaming, presenting an efficient frontier for optimizing attack success and perplexity through a single hyperparameter tuning.

**Abstract:** Existing LLM red-teaming approaches prioritize high attack success rate, often resulting in high-perplexity prompts. This focus overlooks low-perplexity attacks that are more difficult to filter, more likely to arise during benign usage, and more impactful as negative downstream training examples. In response, we introduce ASTPrompter, a single-step optimization method that uses contrastive preference learning to train an attacker to maintain low perplexity while achieving a high attack success rate (ASR). ASTPrompter achieves an attack success rate 5.1 times higher on Llama-8.1B while using inputs that are 2.1 times more likely to occur according to the frozen LLM. Furthermore, our attack transfers to Mistral-7B, Qwen-7B, and TinyLlama in both black- and white-box settings. Lastly, by tuning a single hyperparameter in our method, we discover successful attack prefixes along an efficient frontier between ASR and perplexity, highlighting perplexity as a previously under-considered factor in red-teaming.

</details>


### [59] [RED: Unleashing Token-Level Rewards from Holistic Feedback via Reward Redistribution](https://arxiv.org/abs/2411.08302)

*Jiahui Li, Lin Li, Tai-wei Chang, Kun Kuang, Long Chen, Jun Zhou, Cheng Yang*

**Main category:** cs.CL

**Keywords:** reinforcement learning, human feedback, large language models, reward redistribution, token-level evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces RED, a novel fine-grained reward redistribution method for reinforcement learning from human feedback (RLHF) that enhances token-level understanding in large language models without modifying existing reward models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current reward models in RLHF inadequately evaluate individual tokens' contributions by providing a single reward for an entire sequence, potentially missing key nuances in model performance.

**Method:** The proposed RED method utilizes an existing reward model to assign specific credit to each token in the output sequence, allowing for a more nuanced approach in the evaluation of language generation.

**Key Contributions:**

	1. Introduction of a fine-grained token-level reward redistribution method in RLHF.
	2. Demonstration of improved model performance through enhanced understanding of language nuances.
	3. Minimal computational costs while utilizing off-the-shelf reward models.

**Result:** Experimental results show that RED significantly improves language model performance across various datasets and tasks, outperforming traditional sequence-to-one reward models.

**Limitations:** 

**Conclusion:** The RED method enhances the effectiveness of LLMs in aligning with human preferences while keeping computational costs low and avoiding additional training requirements.

**Abstract:** Reinforcement learning from human feedback (RLHF) offers a promising approach to aligning large language models (LLMs) with human preferences. Typically, a reward model is trained or supplied to act as a proxy for humans in evaluating generated responses during the reinforcement training phase. However, current reward models operate as sequence-to-one models, allocating a single, sparse, and delayed reward to an entire output sequence. This approach may overlook the significant contributions of individual tokens toward the desired outcome. To this end, we propose a more fine-grained, token-level guidance approach for RL training. Specifically, we introduce RED, a novel reward redistribition method that evaluates and assigns specific credit to each token using an off-the-shelf reward model. Utilizing these fine-grained rewards enhances the model's understanding of language nuances, leading to more precise performance improvements. Notably, our method does not require modifying the reward model or introducing additional training steps, thereby incurring minimal computational costs. Experimental results across diverse datasets and tasks demonstrate the superiority of our approach.

</details>


### [60] [MERaLiON-SpeechEncoder: Towards a Speech Foundation Model for Singapore and Beyond](https://arxiv.org/abs/2412.11538)

*Muhammad Huzaifah, Geyu Lin, Tianchi Liu, Hardik B. Sailor, Kye Min Tan, Tarun K. Vangani, Qiongqiong Wang, Jeremy H. M. Wong, Jinyang Wu, Nancy F. Chen, Ai Ti Aw*

**Main category:** cs.CL

**Keywords:** speech processing, foundation model, self-supervised learning

**Relevance Score:** 6

**TL;DR:** The MERaLiON-SpeechEncoder is a foundation model for speech applications developed for Singapore, focusing on English and eventually expanding to other languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a speech processing foundation model tailored to the needs of Singapore and the Southeast Asian region.

**Method:** The model was pre-trained on 200,000 hours of unlabelled speech data using a self-supervised learning approach based on masked language modelling, with detailed training procedures and hyperparameter tuning.

**Key Contributions:**

	1. Development of a tailored speech encoder for Singapore and Southeast Asia.
	2. Self-supervised pre-training approach on a large dataset of unlabelled speech data.
	3. Evaluation benchmarks demonstrating significant improvements in speech tasks.

**Result:** Improvements in performance on spontaneous and Singapore speech benchmarks for speech recognition, and competitive performance across ten other speech tasks.

**Limitations:** 

**Conclusion:** The model will be released for broader research use, supporting efforts beyond Singapore.

**Abstract:** This technical report describes the MERaLiON-SpeechEncoder, a foundation model designed to support a wide range of downstream speech applications. Developed as part of Singapore's National Multimodal Large Language Model Programme, the MERaLiON-SpeechEncoder is tailored to address the speech processing needs in Singapore and the surrounding Southeast Asian region. The model currently supports mainly English, including the variety spoken in Singapore. We are actively expanding our datasets to gradually cover other languages in subsequent releases. The MERaLiON-SpeechEncoder was pre-trained from scratch on 200,000 hours of unlabelled speech data using a self-supervised learning approach based on masked language modelling. We describe our training procedure and hyperparameter tuning experiments in detail below. Our evaluation demonstrates improvements to spontaneous and Singapore speech benchmarks for speech recognition, while remaining competitive to other state-of-the-art speech encoders across ten other speech tasks. We commit to releasing our model, supporting broader research endeavours, both in Singapore and beyond.

</details>


### [61] [Thinking with Many Minds: Using Large Language Models for Multi-Perspective Problem-Solving](https://arxiv.org/abs/2501.02348)

*Sanghyun Park, Boris Maciejovsky, Phanish Puranam*

**Main category:** cs.CL

**Keywords:** Cognitive flexibility, Synthetic deliberation, Large Language Models, Problem-solving, Discourse simulation

**Relevance Score:** 8

**TL;DR:** The paper introduces synthetic deliberation, a method leveraging LLMs to simulate discourse among agents with diverse perspectives, enhancing cognitive flexibility and addressing limitations of mental simulation in complex problem-solving.

**Read time:** 36 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved cognitive flexibility in complex problem-solving is addressed, particularly the limitations of mental simulation.

**Method:** The authors propose synthetic deliberation using a custom GPT-based model to facilitate discourse among various perspectives, enabling concurrent processing and viewpoint synthesis.

**Key Contributions:**

	1. Introduction of synthetic deliberation as a new method
	2. Demonstration of enhanced cognitive flexibility using LLMs
	3. Applications of this method in strategic planning and policymaking

**Result:** The method allows for parallel exploration of multiple viewpoints, overcoming cognitive constraints and enhancing decision-making across diverse applications.

**Limitations:** 

**Conclusion:** Synthetic deliberation can effectively aid in strategic planning, policymaking, and conflict resolution by distributing cognitive labor and enhancing cognitive flexibility.

**Abstract:** Complex problem-solving requires cognitive flexibility--the capacity to entertain multiple perspectives while preserving their distinctiveness. This flexibility replicates the "wisdom of crowds" within a single individual, allowing them to "think with many minds." While mental simulation enables imagined deliberation, cognitive constraints limit its effectiveness. We propose synthetic deliberation, a Large Language Model (LLM)-based method that simulates discourse between agents embodying diverse perspectives, as a solution. Using a custom GPT-based model, we showcase its benefits: concurrent processing of multiple viewpoints without cognitive degradation, parallel exploration of perspectives, and precise control over viewpoint synthesis. By externalizing the deliberative process and distributing cognitive labor between parallel search and integration, synthetic deliberation transcends mental simulation's limitations. This approach shows promise for strategic planning, policymaking, and conflict resolution.

</details>


### [62] [CondAmbigQA: A Benchmark and Dataset for Conditional Ambiguous Question Answering](https://arxiv.org/abs/2502.01523)

*Zongxi Li, Yang Li, Haoran Xie, S. Joe Qin*

**Main category:** cs.CL

**Keywords:** Conditional Ambiguous QA, Question-Answering, Large Language Models, Evaluation Metrics, Contextual Conditions

**Relevance Score:** 8

**TL;DR:** The paper proposes CondAmbigQA, a benchmark to improve question-answering (QA) accuracy by addressing implicit assumptions users make about large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Users often make assumptions about LLMs' understanding of context and intent, leading to ambiguous queries and perceived hallucinations in QA responses. Addressing these assumptions is essential for improving QA accuracy.

**Method:** The paper introduces Conditional Ambiguous Question-Answering (CondAmbigQA), a benchmark featuring 2,000 ambiguous queries and condition-aware evaluation metrics. It utilizes retrieval-based annotation with Wikipedia fragments to clarify query interpretations.

**Key Contributions:**

	1. Introduction of CondAmbigQA benchmark
	2. Use of retrieval-based annotation for condition-aware evaluation
	3. Demonstration of significant accuracy improvements in QA by considering conditions

**Result:** Models considering contextual conditions before answering showed an 11.75% improvement in accuracy, with an additional 7.15% increase when conditions were explicitly provided.

**Limitations:** 

**Conclusion:** The research indicates that many perceived hallucinations in LLMs may be due to query ambiguities, and it establishes the importance of condition reasoning in enhancing QA performance.

**Abstract:** Users often assume that large language models (LLMs) share their cognitive alignment of context and intent, leading them to omit critical information in question-answering (QA) and produce ambiguous queries. Responses based on misaligned assumptions may be perceived as hallucinations. Therefore, identifying possible implicit assumptions is crucial in QA. To address this fundamental challenge, we propose Conditional Ambiguous Question-Answering (CondAmbigQA), a benchmark comprising 2,000 ambiguous queries and condition-aware evaluation metrics. Our study pioneers "conditions" as explicit contextual constraints that resolve ambiguities in QA tasks through retrieval-based annotation, where retrieved Wikipedia fragments help identify possible interpretations for a given query and annotate answers accordingly. Experiments demonstrate that models considering conditions before answering improve answer accuracy by 11.75%, with an additional 7.15% gain when conditions are explicitly provided. These results highlight that apparent hallucinations may stem from inherent query ambiguity rather than model failure, and demonstrate the effectiveness of condition reasoning in QA, providing researchers with tools for rigorous evaluation.

</details>


### [63] [SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models](https://arxiv.org/abs/2502.02787)

*Amirhossein Dabiriaghdam, Lele Wang*

**Main category:** cs.CL

**Keywords:** large language models, watermarking, semantic embeddings, text detection, natural language processing

**Relevance Score:** 9

**TL;DR:** SimMark is a new sentence-level watermarking algorithm for detecting LLM-generated text, achieving high robustness and quality in text.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** With the growing use of large language models, there is a pressing need for methods to reliably detect text generated by these models.

**Method:** SimMark employs semantic sentence embeddings and rejection sampling to create detectable patterns that remain undetectable to humans.

**Key Contributions:**

	1. Introduction of SimMark for sentence-level watermarking
	2. Robust against paraphrasing attacks
	3. Sets a new benchmark in watermarking efficiency and applicability.

**Result:** SimMark outperforms existing techniques in robustness, sampling efficiency, and versatility across different domains, while preserving text quality.

**Limitations:** 

**Conclusion:** SimMark establishes a new standard for watermarking LLM-generated content, useful for both open and API-based models.

**Abstract:** The widespread adoption of large language models (LLMs) necessitates reliable methods to detect LLM-generated text. We introduce SimMark, a robust sentence-level watermarking algorithm that makes LLMs' outputs traceable without requiring access to model internals, making it compatible with both open and API-based LLMs. By leveraging the similarity of semantic sentence embeddings combined with rejection sampling to embed detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while maintaining the text quality and fluency.

</details>


### [64] [Are Generative Models Underconfident? Better Quality Estimation with Boosted Model Probability](https://arxiv.org/abs/2502.11115)

*Tu Anh Dinh, Jan Niehues*

**Main category:** cs.CL

**Keywords:** Quality Estimation, Text Generation, Machine Learning

**Relevance Score:** 7

**TL;DR:** BoostedProb is a novel Quality Estimation method that improves text-generation model output assessment by boosting confidence in outputs with multiple viable options, outperforming traditional probability measures with no added complexity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of underconfidence in the output probabilities of text-generation models when estimating quality without ground truth.

**Method:** The proposed method BoostedProb enhances the model's confidence in cases where multiple correct options exist, improving the assessment of output quality.

**Key Contributions:**

	1. Introduction of BoostedProb for Quality Estimation
	2. Significant improvement in correlation with ground-truth quality
	3. Comparison against more complex QE methods showing reduced effort and cost

**Result:** BoostedProb achieves an average improvement of +0.194 in Pearson correlation to ground-truth quality compared to raw model probability, and it rivals more complex QE approaches.

**Limitations:** 

**Conclusion:** BoostedProb offers a more reliable quality estimation for text-generated outputs without increasing complexity, making it a viable alternative to traditional methods.

**Abstract:** Quality Estimation (QE) is estimating quality of the model output during inference when the ground truth is not available. Deriving output quality from the models' output probability is the most trivial and low-effort way. However, we show that the output probability of text-generation models can appear underconfident. At each output step, there can be multiple correct options, making the probability distribution spread out more. Thus, lower probability does not necessarily mean lower output quality. Due to this observation, we propose a QE approach called BoostedProb, which boosts the model's confidence in cases where there are multiple viable output options. With no increase in complexity, BoostedProb is notably better than raw model probability in different settings, achieving on average +0.194 improvement in Pearson correlation to ground-truth quality. It also comes close to or outperforms more costly approaches like supervised or ensemble-based QE in certain settings.

</details>


### [65] [Culturally-Nuanced Story Generation for Reasoning in Low-Resource Languages: The Case of Javanese and Sundanese](https://arxiv.org/abs/2502.12932)

*Salsabila Zahirah Pranida, Rifo Ahmad Genadi, Fajri Koto*

**Main category:** cs.CL

**Keywords:** Commonsense reasoning, Culturally grounded narratives, Low-resource languages, Large language models, Benchmark dataset

**Relevance Score:** 6

**TL;DR:** This paper explores using large language models to generate culturally grounded narratives in low-resource languages, specifically Javanese and Sundanese, comparing various data generation strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of scarce data and costly native annotation for culturally grounded commonsense reasoning in low-resource languages.

**Method:** The authors test three data creation strategies: LLM-assisted story generation with cultural cues, machine translation from Indonesian, and stories written by natives. They conduct human evaluations and fine-tune models on the datasets.

**Key Contributions:**

	1. Demonstrated the effectiveness of LLMs in low-resource language story generation.
	2. Provided a comparison of data generation strategies for culturally grounded reasoning.
	3. Released a benchmark of Javanese and Sundanese commonsense stories.

**Result:** LLM-generated stories achieve higher cultural fidelity compared to machine-translated narratives, although they are less coherent and correct. The fine-tuned LLMs show improved downstream performance over the other data sources.

**Limitations:** LLM stories were found to be less coherent and correct compared to native-written stories despite matching in cultural fidelity.

**Conclusion:** The study demonstrates the potential of LLMs to create culturally relevant content in low-resource settings and introduces a benchmark dataset for future research.

**Abstract:** Culturally grounded commonsense reasoning is underexplored in low-resource languages due to scarce data and costly native annotation. We test whether large language models (LLMs) can generate culturally nuanced narratives for such settings. Focusing on Javanese and Sundanese, we compare three data creation strategies: (1) LLM-assisted stories prompted with cultural cues, (2) machine translation from Indonesian benchmarks, and (3) native-written stories. Human evaluation finds LLM stories match natives on cultural fidelity but lag in coherence and correctness. We fine-tune models on each dataset and evaluate on a human-authored test set for classification and generation. LLM-generated data yields higher downstream performance than machine-translated and Indonesian human-authored training data. We release a high-quality benchmark of culturally grounded commonsense stories in Javanese and Sundanese to support future work.

</details>


### [66] [MIND: Towards Immersive Psychological Healing with Multi-agent Inner Dialogue](https://arxiv.org/abs/2502.19860)

*Yujia Chen, Changsong Li, Yiming Wang, Tianjie Ju, Qingqing Xiao, Nan Zhang, Zifan Kong, Peng Wang, Binyu Yan*

**Main category:** cs.CL

**Keywords:** Mental Health, Large Language Models, Interactive Healing, User Experience, Psychological Therapy

**Relevance Score:** 9

**TL;DR:** The paper introduces MIND, a novel framework utilizing LLMs for immersive psychological healing through interactive dialogues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing mental health issues and the inadequacies of traditional therapeutic methods, particularly their lack of emotional engagement.

**Method:** MIND uses a predefined interactive healing framework where LLM agents role-play to create more human-like and emotionally aware dialogues with users.

**Key Contributions:**

	1. Introduction of the MIND framework for interactive healing
	2. Implementation of LLMs in role-playing for emotional engagement
	3. Demonstration of improved user experience in mental health applications

**Result:** Human experiments reveal that MIND offers a substantially more user-friendly experience compared to traditional therapy methods.

**Limitations:** 

**Conclusion:** MIND highlights the potential of leveraging LLM capabilities for enhanced psychological healing, improving engagement and emotional interaction.

**Abstract:** Mental health issues are worsening in today's competitive society, such as depression and anxiety. Traditional healings like counseling and chatbots fail to engage effectively, they often provide generic responses lacking emotional depth. Although large language models (LLMs) have the potential to create more human-like interactions, they still struggle to capture subtle emotions. This requires LLMs to be equipped with human-like adaptability and warmth. To fill this gap, we propose the MIND (Multi-agent INner Dialogue), a novel paradigm that provides more immersive psychological healing environments. Considering the strong generative and role-playing ability of LLM agents, we predefine an interactive healing framework and assign LLM agents different roles within the framework to engage in interactive inner dialogues with users, thereby providing an immersive healing experience. We conduct extensive human experiments in various real-world healing dimensions, and find that MIND provides a more user-friendly experience than traditional paradigms. This demonstrates that MIND effectively leverages the significant potential of LLMs in psychological healing.

</details>


### [67] [Entropy-Gated Branching for Efficient Test-Time Reasoning](https://arxiv.org/abs/2503.21961)

*Xianzhi Li, Ethan Callanan, Abdellah Ghassel, Xiaodan Zhu*

**Main category:** cs.CL

**Keywords:** Inference, Large Language Models, Entropy-Gated Branching, Dynamic Resource Allocation, Reasoning Efficiency

**Relevance Score:** 9

**TL;DR:** The paper introduces Entropy-Gated Branching, an inference technique that improves LLM reasoning efficiency by selectively expanding predictions at points of high uncertainty, resulting in higher accuracy and faster operation compared to standard methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of large language models while reducing wasted computational resources during the inference process.

**Method:** Entropy-Gated Branching dynamically allocates compute resources by branching only at points of high uncertainty and using a feedback mechanism to rank and prune branches.

**Key Contributions:**

	1. Introduction of Entropy-Gated Branching technique
	2. Improved accuracy on reasoning tasks
	3. Faster inference times compared to traditional methods

**Result:** Empirical tests show a 22.6% accuracy improvement over standard inference methods and a 37% increase in speed compared to conventional beam search.

**Limitations:** 

**Conclusion:** Dynamic allocation of resources during inference can lead to significant improvements in both the efficiency and effectiveness of LLMs.

**Abstract:** Test-time compute methods like beam search can significantly improve the reasoning capabilities and problem-solving accuracy of large language models. However, these approaches require substantially increased computational resources, with most computation wasted on exploring low-diversity branches where the model already exhibits high confidence. We observe that a small subset of uncertain reasoning steps has a disproportionately large impact on final prediction accuracy, and branching at these points tends to yield higher-quality and more diverse candidate reasoning steps. Therefore, we introduce Entropy-Gated Branching: a novel inference technique that dynamically allocates computational resources by selectively expanding prediction sequences only at points of high uncertainty. Our method leverages entropy as a gating mechanism to identify when branching is most beneficial, coupled with an external feedback model to rank and prune candidate branches. Empirical results on mathematical and financial reasoning benchmarks show that this strategy improves accuracy by 22.6% over standard inference while operating 37% faster than conventional beam search with similar or higher performance. Our results show that dynamic resource allocation during inference can substantially improve both efficiency and effectiveness, offering a more scalable pathway to enhanced LLM reasoning capabilities.

</details>


### [68] [Task Matters: Knowledge Requirements Shape LLM Responses to Context-Memory Conflict](https://arxiv.org/abs/2506.06485)

*Kaiser Sun, Fan Bai, Mark Dredze*

**Main category:** cs.CL

**Keywords:** Large Language Models, knowledge conflict, task evaluation, Machine Learning, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This study investigates how Large Language Models (LLMs) process conflicting information from contextual knowledge and parametric memory during various tasks, revealing significant influences on performance and the implications for LLM evaluation.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs handle conflicts between contextual and parametric knowledge, particularly in tasks requiring different types and amounts of information.

**Method:** A model-agnostic diagnostic framework was developed to detect disagreements between model beliefs and a curated knowledge set while injecting controlled conflicts into tasks. The study evaluates open-source LLMs across different dimensions of task knowledge reliance and conflict plausibility.

**Key Contributions:**

	1. Development of a diagnostic framework for detecting knowledge conflicts in LLMs
	2. Insights into the impact of task knowledge reliance on LLM performance under conflict
	3. Highlighting the implications of conflict on model evaluation validity

**Result:** Performance degradation from knowledge conflict was found to correlate with the task's reliance on knowledge. Providing explanatory rationales tends to enhance reliance on context in context-only tasks but can hinder performance in tasks that should favor parametric knowledge.

**Limitations:** The study's scope may be limited to specific task types and may not generalize to all applications of LLMs.

**Conclusion:** The findings emphasize the need for careful consideration of knowledge conflicts in LLM evaluation to ensure validity in model-based assessments, especially for diverse task requirements.

**Abstract:** Large Language Models require both contextual knowledge and parametric memory, but these sources can disagree. Prior investigations on contextual question answering tasks report a preference toward parametric knowledge under conflict, yet they focus almost exclusively on tasks that should always rely on the given passage, leaving open how this behavior manifests when tasks demand different amounts and kinds of knowledge. We study this question with a model-agnostic diagnostic framework that (i) automatically detects disagreements between a model's beliefs and a curated knowledge set, and (ii) injects controlled conflicts into tasks. The resulting datasets span two orthogonal dimensions: task knowledge reliance and conflict plausibility. Evaluating representative open-source LLMs, we find that: (1) performance degradation from conflict correlates with a task's knowledge reliance; (2) explanatory rationales and simple reiteration both increase context reliance-helpful for context-only tasks but harmful when parametric knowledge should dominate; (3) These behaviors raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.

</details>
