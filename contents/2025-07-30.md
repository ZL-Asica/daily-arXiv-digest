# 2025-07-30

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 29]

- [cs.CL](#cs.CL) [Total: 74]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Enhancing Manufacturing Training Through VR Simulations](https://arxiv.org/abs/2507.21070)

*Vladislav Li, Ilias Siniosoglou, Panagiotis Sarigiannidis, Vasileios Argyriou*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Industrial Training, Gesture-based Controls, Training Efficacy, Information Retention

**Relevance Score:** 4

**TL;DR:** This paper discusses a VR-based industrial training system that enhances learning through immersive simulations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to reconcile theoretical knowledge with practical experience in industrial manufacturing training.

**Method:** The paper presents a VR training architecture featuring high-fidelity simulations, dynamic scenarios, and adaptive feedback systems, with gesture-based controls.

**Key Contributions:**

	1. Introduction of VR Training Scenario Score (VRTSS) for performance assessment
	2. Implementation of gesture-based controls for improved accessibility
	3. Demonstrated significant improvements in trainee outcomes compared to conventional methods.

**Result:** Experimental results indicate significant improvements in information retention, task execution precision, and overall training efficacy.

**Limitations:** 

**Conclusion:** The study demonstrates VR's potential as a scalable and efficient alternative to traditional training methods in industrial settings.

**Abstract:** In contemporary training for industrial manufacturing, reconciling theoretical knowledge with practical experience continues to be a significant difficulty. As companies transition to more intricate and technology-oriented settings, conventional training methods frequently inadequately equip workers with essential practical skills while maintaining safety and efficiency. Virtual Reality has emerged as a transformational instrument to tackle this issue by providing immersive, interactive, and risk-free teaching experiences. Through the simulation of authentic industrial environments, virtual reality facilitates the acquisition of vital skills for trainees within a regulated and stimulating context, therefore mitigating the hazards linked to experiential learning in the workplace. This paper presents a sophisticated VR-based industrial training architecture aimed at improving learning efficacy via high-fidelity simulations, dynamic and context-sensitive scenarios, and adaptive feedback systems. The suggested system incorporates intuitive gesture-based controls, reducing the learning curve for users across all skill levels. A new scoring metric, namely, VR Training Scenario Score (VRTSS), is used to assess trainee performance dynamically, guaranteeing ongoing engagement and incentive. The experimental assessment of the system reveals promising outcomes, with significant enhancements in information retention, task execution precision, and overall training efficacy. The results highlight the capability of VR as a crucial instrument in industrial training, providing a scalable, interactive, and efficient substitute for conventional learning methods.

</details>


### [2] [FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents](https://arxiv.org/abs/2507.21071)

*Qinglong Yang, Haoming Li, Haotian Zhao, Xiaokai Yan, Jingtao Ding, Fengli Xu, Yong Li*

**Main category:** cs.HC

**Keywords:** mobile GUI agents, multimodal large language models, human-device interaction, proactive task suggestions, personalized task execution

**Relevance Score:** 8

**TL;DR:** The paper introduces the FingerTip benchmark for proactive and personalized mobile GUI agents utilizing multimodal large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance human-device interaction by enabling proactive intent anticipation and leveraging user context during task execution.

**Method:** The FingerTip benchmark consists of tracks for proactive task suggestions and personalized task execution, collecting human demonstrations of multi-step Android interactions in real-life contexts.

**Key Contributions:**

	1. Introduction of the FingerTip benchmark for evaluating mobile GUI agents.
	2. Collection of real-life, context-rich user interaction data.
	3. Demonstration of improved model performance leveraging user preferences.

**Result:** Experiments demonstrated that models fine-tuned with collected data effectively utilized user information to enhance task performance, revealing challenges in the proposed tasks.

**Limitations:** Challenges in task execution and the need for continuous user interaction data to maintain effectiveness.

**Conclusion:** The findings suggest the potential for creating more user-oriented mobile GUI agents through proactive and context-aware capabilities.

**Abstract:** Mobile GUI agents are becoming critical tools for enhancing human-device interaction efficiency, with multimodal large language models (MLLMs) emerging as dominant paradigms in this domain. Current agents, however, are limited to following explicit human instructions, resulting in insufficient capability for proactive intent anticipation. Additionally, these agents fail to leverage the contextual information associated with users during task execution, thereby neglecting potentially vast differences in user preferences. To address these challenges, we introduce the FingerTip benchmark. It contains two new tracks: proactive task suggestions by analyzing environment observation and users' previous intents, and personalized task execution by catering to users' action preferences. We collected unique human demonstrations of multi-step Android device interactions across a variety of everyday apps. These demonstrations are not isolated but are continuously acquired from the users' long-term usage in their real lives, and encompass essential user-related contextual information. Our experiments reveal challenges of the tasks we propose. The model fine-tuned with the data we collected effectively utilized user information and achieved good results, highlighting the potential of our approach in building more user-oriented mobile GUI agents. Our code is open-source at https://anonymous.4open.science/r/FingerTip-57B8 for reproducibility.

</details>


### [3] [Snap, Segment, Deploy: A Visual Data and Detection Pipeline for Wearable Industrial Assistants](https://arxiv.org/abs/2507.21072)

*Di Wen, Junwei Zheng, Ruiping Liu, Yi Xu, Kunyu Peng, Rainer Stiefelhagen*

**Main category:** cs.HC

**Keywords:** Industrial assistance, On-device processing, Retrieval-Augmented Generation, Voice interfaces, Privacy-preserving solutions

**Relevance Score:** 6

**TL;DR:** A mobile-device-based assistant system for industrial tasks that operates entirely on-device, offering real-time support through voice interfaces and perception technologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of rapid adaptation and support in industrial assembly tasks under limited computing and privacy constraints.

**Method:** Developed a modular on-device pipeline integrating object detection, speech recognition, and Retrieval-Augmented Generation (RAG) for real-time interaction.

**Key Contributions:**

	1. Introduction of an on-device mobile assistant for industrial training.
	2. Integration of lightweight object detection and speech recognition for semi-hands-free interaction.
	3. Creation of the Gear8 dataset and demonstration of improved robustness to domain shifts.

**Result:** Experiments with the Gear8 dataset showed improved robustness to domain shifts and positive user feedback confirmed the system's practical viability.

**Limitations:** 

**Conclusion:** The framework provides a deployable, privacy-preserving smart assistance solution for industrial environments.

**Abstract:** Industrial assembly tasks increasingly demand rapid adaptation to complex procedures and varied components, yet are often conducted in environments with limited computing, connectivity, and strict privacy requirements. These constraints make conventional cloud-based or fully autonomous solutions impractical for factory deployment. This paper introduces a mobile-device-based assistant system for industrial training and operational support, enabling real-time, semi-hands-free interaction through on-device perception and voice interfaces. The system integrates lightweight object detection, speech recognition, and Retrieval-Augmented Generation (RAG) into a modular on-device pipeline that operates entirely on-device, enabling intuitive support for part handling and procedure understanding without relying on manual supervision or cloud services. To enable scalable training, we adopt an automated data construction pipeline and introduce a two-stage refinement strategy to improve visual robustness under domain shift. Experiments on our generated dataset, i.e., Gear8, demonstrate improved robustness to domain shift and common visual corruptions. A structured user study further confirms its practical viability, with positive user feedback on the clarity of the guidance and the quality of the interaction. These results indicate that our framework offers a deployable solution for real-time, privacy-preserving smart assistance in industrial environments. We will release the Gear8 dataset and source code upon acceptance.

</details>


### [4] [Empowering Educators in the Age of AI: An Empirical Study on Creating custom GPTs in Qualitative Research Method education](https://arxiv.org/abs/2507.21074)

*Qian Huang, Thijs Willems*

**Main category:** cs.HC

**Keywords:** Generative AI, Qualitative Research, Educator Design, AI in Education, Pedagogical Intent

**Relevance Score:** 8

**TL;DR:** This study explores the integration of custom GPT tools in a Master's-level Qualitative Research Methods course, highlighting how educators can design AI applications to enhance learning.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how educators can actively shape the design and use of generative AI tools in education, moving beyond the view of students as passive users.

**Method:** The study employed action research methodology and the TPACK framework to investigate the integration of custom GPT tools for various educational tasks.

**Key Contributions:**

	1. Demonstrated the potential of AI as a scaffold for active learning when paired with human facilitation.
	2. Showed custom GPTs can act as cognitive partners in research practice.
	3. Highlighted the importance of educator-led design for successful AI integration in pedagogy.

**Result:** The thematic analysis revealed that the use of GPT tools enhanced student reflexivity, improved interview techniques, and supported structured analytic thinking, despite some students expressing concerns about cognitive overload and reduced immersion.

**Limitations:** Students expressed concerns about cognitive overload and the formulaic nature of AI responses.

**Conclusion:** The study concludes that educator-led design of AI tools is crucial for meaningful integration of AI in education, promoting reflective and collaborative learning.

**Abstract:** As generative AI (Gen-AI) tools become more prevalent in education, there is a growing need to understand how educators, not just students, can actively shape their design and use. This study investigates how two instructors integrated four custom GPT tools into a Masters-level Qualitative Research Methods course for Urban Planning Policy students. Addressing two key gaps: the dominant framing of students as passive AI users, and the limited use of AI in qualitative methods education. The study explores how Gen-AI can support disciplinary learning when aligned with pedagogical intent. Drawing on the Technological Pedagogical Content Knowledge (TPACK) framework and action research methodology, the instructors designed GPTs to scaffold tasks such as research question formulation, interview practice, fieldnote analysis, and design thinking. Thematic analysis of student reflections, AI chat logs, and final assignments revealed that the tools enhanced student reflexivity, improved interview techniques, and supported structured analytic thinking. However, students also expressed concerns about cognitive overload, reduced immersion in data, and the formulaic nature of AI responses. The study offers three key insights: AI can be a powerful scaffold for active learning when paired with human facilitation; custom GPTs can serve as cognitive partners in iterative research practice; and educator-led design is critical to pedagogically meaningful AI integration. This research contributes to emerging scholarship on AI in higher education by demonstrating how empowering educators to design custom tools can promote more reflective, responsible, and collaborative learning with AI.

</details>


### [5] [Can LLMs Reason About Trust?: A Pilot Study](https://arxiv.org/abs/2507.21075)

*Anushka Debnath, Stephen Cranefield, Emiliano Lorini, Bastin Tony Roy Savarimuthu*

**Main category:** cs.HC

**Keywords:** trust, Large Language Models, human-computer interaction, social relationships, role-playing

**Relevance Score:** 9

**TL;DR:** The paper explores how Large Language Models (LLMs) can reason about and induce trust in interpersonal interactions that occur through digital communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI can facilitate trust in electronic interactions, which is critical for collaboration and healthy relationships in society.

**Method:** The study investigates LLMs' capability to analyze trust between individuals and simulate interactions to foster trust, using role-play scenarios.

**Key Contributions:**

	1. Investigation of trust reasoning by LLMs in social interactions.
	2. Demonstration of LLM role-playing to foster trust between parties.
	3. Assessment of trust dynamics in electronic communication.

**Result:** LLMs show potential in reasoning about trust dynamics and can engage in role-playing to induce trust in specific scenarios.

**Limitations:** The study is limited to specific scenarios and may not generalize to all trust dynamics.

**Conclusion:** Integrating LLMs into applications can enhance users' understanding of trust in their digital interactions, leading to better cooperative outcomes.

**Abstract:** In human society, trust is an essential component of social attitude that helps build and maintain long-term, healthy relationships which creates a strong foundation for cooperation, enabling individuals to work together effectively and achieve shared goals. As many human interactions occur through electronic means such as using mobile apps, the potential arises for AI systems to assist users in understanding the social state of their relationships. In this paper we investigate the ability of Large Language Models (LLMs) to reason about trust between two individuals in an environment which requires fostering trust relationships. We also assess whether LLMs are capable of inducing trust by role-playing one party in a trust based interaction and planning actions which can instil trust.

</details>


### [6] [Data-Driven and Participatory Approaches toward Neuro-Inclusive AI](https://arxiv.org/abs/2507.21077)

*Naba Rizvi*

**Main category:** cs.HC

**Keywords:** Neuro-Inclusive AI, autism, bias, machine learning, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper critiques biased AI representations that marginalize autistic individuals and proposes Neuro-Inclusive AI which emphasizes neurodiversity over mimicking human communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the marginalization of autistic individuals in AI systems and propose frameworks for more inclusive representation.

**Method:** Empirical experiments with annotators and LLMs to assess binary labeling of anti-autistic hate speech; creation of an evaluation benchmark called AUTALIC.

**Key Contributions:**

	1. Definition and exploration of Neuro-Inclusive AI
	2. Development of the AUTALIC benchmark for evaluating anti-autistic hate speech
	3. Empirical evidence on the exclusion of autistic perspectives in AI systems

**Result:** 90% of current human-like AI agents exclude autistic perspectives; the AUTALIC benchmark allows for the evaluation and fine-tuning of models for better representation.

**Limitations:** Ethical considerations in AI development are often disregarded, posing challenges for implementation.

**Conclusion:** There is a significant lack of ethical consideration in AI developing environments, necessitating the establishment of Neuro-Inclusive AI standards and practices.

**Abstract:** Biased data representation in AI marginalizes up to 75 million autistic people worldwide through medical applications viewing autism as a deficit of neurotypical social skills rather than an aspect of human diversity, and this perspective is grounded in research questioning the humanity of autistic people. Turing defined artificial intelligence as the ability to mimic human communication, and as AI development increasingly focuses on human-like agents, this benchmark remains popular. In contrast, we define Neuro-Inclusive AI as datasets and systems that move away from mimicking humanness as a benchmark for machine intelligence. Then, we explore the origins, prevalence, and impact of anti-autistic biases in current research. Our work finds that 90% of human-like AI agents exclude autistic perspectives, and AI creators continue to believe ethical considerations are beyond the scope of their work. To improve the autistic representation in data, we conduct empirical experiments with annotators and LLMs, finding that binary labeling schemes sufficiently capture the nuances of labeling anti-autistic hate speech. Our benchmark, AUTALIC, can be used to evaluate or fine-tune models, and was developed to serve as a foundation for more neuro-inclusive future work.

</details>


### [7] [What Makes a Level Hard in Super Mario Maker 2?](https://arxiv.org/abs/2507.21078)

*Carlo A. Furia, Andrea Mocci*

**Main category:** cs.HC

**Keywords:** Super Mario Maker 2, level design, difficulty analysis, natural language processing, user-generated content

**Relevance Score:** 4

**TL;DR:** Analyzes factors affecting difficulty in user-designed levels of Super Mario Maker 2.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand what factors influence the difficulty of user-created levels in Super Mario Maker 2.

**Method:** The paper employs regression models and natural language processing techniques to analyze user-written levels data.

**Key Contributions:**

	1. Analysis of user-level design data from SMM2
	2. Identification of level characteristics affecting difficulty
	3. Use of regression and NLP techniques for evaluation

**Result:** Identifies level characteristics and sentiments consistently associated with levels' difficulties.

**Limitations:** 

**Conclusion:** The findings provide insights into differences between easy and hard levels, contributing to better understanding of user-level design.

**Abstract:** Games like Super Mario Maker 2 (SMM2) lower the barrier for casual users to become level designers. In this paper, we set out to analyze a vast amount of data about SMM2 user-written levels, in order to understand what factors affect a level's difficulty as experienced by other users. To this end, we perform two kinds of analyses: one based on regression models and one using natural language processing techniques. The main results shed light on which level characteristics (e.g., its style, popularity, timing) and which topics and sentiments have a consistent association with easier or harder levels. While none of our findings are startling, they help distill some key differences between easy and hard SMM2 levels, which, in turn, can pave the way for a better understanding of end-user level design.

</details>


### [8] [Metaverse Support Groups for LGBTQ+ Youth: An Observational Study on Safety, Self-Expression, and Early Intervention](https://arxiv.org/abs/2507.21079)

*Joe Hasei, Yosuke Matsumoto, Hiroki Kawai, Yuko Okahisa, Manabu Takaki, Toshifumi Ozaki*

**Main category:** cs.HC

**Keywords:** metaverse, LGBTQ+ support, social isolation, suicide risk, virtual environments

**Relevance Score:** 8

**TL;DR:** This study evaluates metaverse-based support groups aimed at reducing social isolation and suicide risk among LGBTQ+ youths, finding significant improvements in social confidence and high user satisfaction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address social isolation and suicide risk among LGBTQ+ youths through innovative online support mechanisms.

**Method:** Participants engaged in metaverse-based support groups on the Cluster platform, utilizing avatars for anonymity and self-expression.

**Key Contributions:**

	1. Demonstrated high satisfaction and low discomfort in avatar use for LGBTQ+ youths
	2. Showed metaverse environments improve social confidence and provide safer spaces
	3. Reinforced the importance of integrated virtual and in-person support frameworks

**Result:** 79.2% of participants chose gender-matching avatars, with significant improvements in social confidence, particularly among those with initially low confidence (p<0.001).

**Limitations:** Highly confident individuals offline faced adaptation challenges, suggesting that virtual support should complement, not replace, physical interactions.

**Conclusion:** Metaverse-based support offers effective psychological support, enhancing self-expression and acceptance, while complementing traditional in-person services.

**Abstract:** This study assessed metaverse-based support groups designed to reduce social isolation and suicide risk among LGBTQ+ youths. Using the Cluster platform, enhanced anonymity, avatar-based self-expression, and accessibility were provided. Key findings showed that 79.2% chose avatars matching their gender identity, reporting high satisfaction (mean: 4.10/5) and low discomfort (mean: 1.79/5). Social confidence significantly improved in virtual spaces compared to real-world interactions (p<0.001), particularly among participants with initially low confidence, averaging an increase of 2.08 points. About half of the first-time participants were 16 or younger, highlighting potential for early intervention. The metaverse scored higher than real-world environments for safety/privacy (3.94/5), self-expression (4.02/5), and accessibility (4.21/5). Additionally, 73.6% reported feeling more accepted virtually. However, some highly confident individuals offline experienced mild adaptation challenges, averaging a confidence decrease of 0.58 points, indicating virtual support complements rather than replaces in-person services. These findings suggest metaverse-based support effectively lowers psychological barriers and provides affirming spaces, potentially reducing severe outcomes such as suicidal ideation. Future studies should focus on integrating virtual support with existing community and clinical frameworks to enhance long-term impacts.

</details>


### [9] [Empathy in Explanation](https://arxiv.org/abs/2507.21081)

*Katherine M. Collins, Kartik Chandra, Adrian Weller, Jonathan Ragan-Kelley, Joshua B. Tenenbaum*

**Main category:** cs.HC

**Keywords:** explanation, emotion, human-computer interaction, computational modeling, doctor-patient communication

**Relevance Score:** 8

**TL;DR:** The paper presents a computational framework examining the role of emotion in explanations, specifically in a medical context, and shows that emotional considerations improve predictive accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate why and how emotion influences the process of giving explanations, particularly in a cooperative interaction between explainer and listener.

**Method:** Developed a computational model that simulates explainers considering emotional impact, validated against human intuitions in doctor-patient contexts.

**Key Contributions:**

	1. Introduces a novel computational framework for modeling explanations considering emotion.
	2. Demonstrates through empirical testing that emotion-aware explanations align more closely with human intuition.
	3. Provides insights into doctor-patient communication dynamics.

**Result:** The model accurately predicts human intuition regarding emotional considerations in explanations, outperforming non-emotional baseline models.

**Limitations:** The research is based on a specific domain (medical explanations) and may not generalize to other contexts.

**Conclusion:** Emotional factors play a significant role in how explanations are formulated, highlighting the need for incorporating emotional context in AI systems for better human interaction.

**Abstract:** Why do we give the explanations we do? Recent work has suggested that we should think of explanation as a kind of cooperative social interaction, between a why-question-asker and an explainer. Here, we apply this perspective to consider the role that emotion plays in this social interaction. We develop a computational framework for modeling explainers who consider the emotional impact an explanation might have on a listener. We test our framework by using it to model human intuitions about how a doctor might explain to a patient why they have a disease, taking into account the patient's propensity for regret. Our model predicts human intuitions well, better than emotion-agnostic ablations, suggesting that people do indeed reason about emotion when giving explanations.

</details>


### [10] [Eliciting User Requirements for AI-Enhanced Learning Environments using a Participatory Approach](https://arxiv.org/abs/2507.21088)

*Bibeg Limbu, Irene-Angelica Chounta, Vilma Sukacke, Andromachi Filippidi, Chara Spyropoulou, Marianna Anagnostopoulou, Eleftheria Tsourlidaki, Nikos Karacapilidis*

**Main category:** cs.HC

**Keywords:** AI in education, stakeholder expectations, user requirements, thematic analysis, participatory workshops

**Relevance Score:** 7

**TL;DR:** The paper investigates stakeholder needs and expectations for AI in educational contexts through participatory workshops, analyzing qualitative data to formulate user requirements for AI-enhanced learning environments.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the needs and expectations of educational stakeholders regarding the integration of AI technologies in learning environments.

**Method:** Two-phased participatory workshops involving qualitative data collection and analysis through deductive and inductive thematic analysis, framed by Activity Theory.

**Key Contributions:**

	1. Identification of user needs and expectations for AI in education
	2. Methodological approach using participatory workshops and thematic analysis
	3. Recommendations for future AI-enhanced learning designs

**Result:** Identified needs and expectations of users, along with contradictions, leading to user requirements for integrating AI in educational settings.

**Limitations:** The study is limited to specific educational stakeholders and contexts, potentially affecting generalizability.

**Conclusion:** The findings provide valuable insights for designing future AI-based educational technologies that meet stakeholder requirements.

**Abstract:** This paper explores the needs \& expectations of educational stakeholders for AI (Artificial Intelligence)-enhanced learning environments. Data was collected following two-phased participatory workshops. The first workshop outlined stakeholders' profiles in terms of technical and pedagogical characteristics. The qualitative data collected was analysed using deductive thematic analysis with Activity Theory, explicating the user needs. The second workshop articulated expectations related to the integration of AI in education. Inductive thematic analysis of the second workshop led to the elicitation of users' expectations. We cross-examined the needs and expectations, identifying contradictions, to generate user requirements for emerging technologies. The paper provides suggestions for future design initiatives that incorporate AI in learning environments.

</details>


### [11] [Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations](https://arxiv.org/abs/2507.21089)

*Xiaotian Su, Naim Zierau, Soomin Kim, April Yi Wang, Thiemo Wambsganss*

**Main category:** cs.HC

**Keywords:** emotional awareness, hate speech, social media, proactive moderation, user behavior

**Relevance Score:** 7

**TL;DR:** The paper evaluates two types of emotion monitoring dashboards aimed at increasing emotional awareness and reducing hate speech on social media, based on a study with 211 participants.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address criticisms of social media moderation techniques that often lead to censorship and fail to mitigate the root causes of uncivil behavior.

**Method:** Conducted a study involving 211 participants to evaluate the effects of two emotion monitoring dashboards on user commenting behavior and emotional experiences.

**Key Contributions:**

	1. Proposed two emotion monitoring dashboards to enhance emotional awareness.
	2. Evaluated the impact on user commenting behavior and emotional experiences.
	3. Provided insights for further research on emotion regulation tools in social media.

**Result:** The interventions increased users' emotional awareness and reduced hate speech, but also led to increased expression of negative emotions when discussing sensitive issues.

**Limitations:** Potential unintended effects include increased negativity in user emotions when discussing sensitive topics.

**Conclusion:** Integrating proactive emotion regulation tools into social media platforms could foster healthier digital interactions, despite some unintended negative effects.

**Abstract:** Social media platforms increasingly employ proactive moderation techniques, such as detecting and curbing toxic and uncivil comments, to prevent the spread of harmful content. Despite these efforts, such approaches are often criticized for creating a climate of censorship and failing to address the underlying causes of uncivil behavior. Our work makes both theoretical and practical contributions by proposing and evaluating two types of emotion monitoring dashboards to users' emotional awareness and mitigate hate speech. In a study involving 211 participants, we evaluate the effects of the two mechanisms on user commenting behavior and emotional experiences. The results reveal that these interventions effectively increase users' awareness of their emotional states and reduce hate speech. However, our findings also indicate potential unintended effects, including increased expression of negative emotions (Angry, Fear, and Sad) when discussing sensitive issues. These insights provide a basis for further research on integrating proactive emotion regulation tools into social media platforms to foster healthier digital interactions.

</details>


### [12] [Thinking Like a Scientist: Can Interactive Simulations Foster Critical AI Literacy?](https://arxiv.org/abs/2507.21090)

*Yiling Zhao, Audrey Michal, Nithum Thain, Hari Subramonyam*

**Main category:** cs.HC

**Keywords:** AI literacy, interactive simulations, education, bias in language models, learning methodologies

**Relevance Score:** 7

**TL;DR:** This study explores the effectiveness of interactive simulations in enhancing AI literacy, comparing it to traditional educational methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for fostering critical AI literacy as AI increasingly influences decisions made by individuals and society.

**Method:** Controlled study with 605 participants examining the impact of interactive AI tutorials on learning about fairness, dataset representativeness, and bias.

**Key Contributions:**

	1. Demonstration of interactive simulations' effectiveness in teaching AI concepts
	2. Evidence of improved knowledge transfer and confidence in understanding AI bias
	3. Critique of traditional educational methods for AI literacy

**Result:** Interactive simulations enhance AI literacy and knowledge transfer, resulting in increased self-reported confidence among participants, though engagement does not predict learning outcomes.

**Limitations:** 

**Conclusion:** Interactive, inquiry-driven methodologies are more effective for educating individuals about AI, enabling critical engagement with AI technologies.

**Abstract:** As AI systems shape individual and societal decisions, fostering critical AI literacy is essential. Traditional approaches, such as blog articles, static lessons, and social media discussions, often fail to support deep conceptual understanding and critical engagement. This study examines whether interactive simulations can help learners think like a scientist by engaging them in hypothesis testing, experimentation, and direct observation of AI behavior. In a controlled study with 605 participants, we assess how interactive AI tutorials impact learning of key concepts such as fairness, dataset representativeness, and bias in language models. Results show that interactive simulations effectively enhance AI literacy across topics, supporting greater knowledge transfer and self-reported confidence, though engagement alone does not predict learning. This work contributes to the growing field of AI literacy education, highlighting how interactive, inquiry-driven methodologies can better equip individuals to critically engage with AI in their daily lives.

</details>


### [13] [VizGenie: Toward Self-Refining, Domain-Aware Workflows for Next-Generation Scientific Visualization](https://arxiv.org/abs/2507.21124)

*Ayan Biswas, Terece L. Turton, Nishath Rajiv Ranasinghe, Shawn Jones, Bradley Love, William Jones, Aric Hagberg, Han-Wei Shen, Nathan DeBardeleben, Earl Lawrence*

**Main category:** cs.HC

**Keywords:** scientific visualization, large language models, natural language interface

**Relevance Score:** 8

**TL;DR:** VizGenie is a self-improving framework that enhances scientific visualization using LLMs with a user-friendly natural language interface, enabling dynamic generation and validation of visualization scripts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance scientific visualization by creating a flexible system that integrates LLMs for generating domain-specific visualization tools on-demand.

**Method:** VizGenie orchestrates a collection of dynamically generated modules and integrates LLMs to produce visualization scripts validated through automated backend processing. It incorporates image-based analysis and visual question answering.

**Key Contributions:**

	1. Dynamic generation of visualization scripts using LLMs
	2. Integration of intuitive natural language interface for user queries
	3. Enhanced adaptability through continuous learning from user interactions

**Result:** The system significantly reduces cognitive overhead during iterative visualization tasks and supports deeper exploration through interactive queries.

**Limitations:** 

**Conclusion:** VizGenie establishes a sustainable and evolving visualization practice that enhances insight generation and reproducibility in scientific research.

**Abstract:** We present VizGenie, a self-improving, agentic framework that advances scientific visualization through large language model (LLM) by orchestrating of a collection of domain-specific and dynamically generated modules. Users initially access core functionalities--such as threshold-based filtering, slice extraction, and statistical analysis--through pre-existing tools. For tasks beyond this baseline, VizGenie autonomously employs LLMs to generate new visualization scripts (e.g., VTK Python code), expanding its capabilities on-demand. Each generated script undergoes automated backend validation and is seamlessly integrated upon successful testing, continuously enhancing the system's adaptability and robustness. A distinctive feature of VizGenie is its intuitive natural language interface, allowing users to issue high-level feature-based queries (e.g., ``visualize the skull"). The system leverages image-based analysis and visual question answering (VQA) via fine-tuned vision models to interpret these queries precisely, bridging domain expertise and technical implementation. Additionally, users can interactively query generated visualizations through VQA, facilitating deeper exploration. Reliability and reproducibility are further strengthened by Retrieval-Augmented Generation (RAG), providing context-driven responses while maintaining comprehensive provenance records. Evaluations on complex volumetric datasets demonstrate significant reductions in cognitive overhead for iterative visualization tasks. By integrating curated domain-specific tools with LLM-driven flexibility, VizGenie not only accelerates insight generation but also establishes a sustainable, continuously evolving visualization practice. The resulting platform dynamically learns from user interactions, consistently enhancing support for feature-centric exploration and reproducible research in scientific visualization.

</details>


### [14] [Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems](https://arxiv.org/abs/2507.21303)

*Viktoria Marcus, Griffin Pitts, Sanaz Motamedi*

**Main category:** cs.HC

**Keywords:** automated driving systems, external human-machine interfaces, pedestrian safety, trust, traffic fatalities

**Relevance Score:** 7

**TL;DR:** The study explores the impact of external human-machine interfaces (eHMIs) on pedestrian interactions with level-5 automated driving systems (ADSs), highlighting their role in improving safety and trust.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high rate of traffic fatalities involving vulnerable road users, particularly due to human error, and to evaluate how eHMIs can enhance communication between pedestrians and level-5 ADSs.

**Method:** An online survey was conducted with 153 participants assessing their interactions with level-5 ADSs in road-crossing scenarios, both with and without eHMIs.

**Key Contributions:**

	1. Demonstration of the positive impact of eHMIs on pedestrian behavior and safety perceptions in interactions with level-5 ADSs.
	2. Identification of visual eHMI features as more effective than auditory ones, enhancing user experience and understanding.
	3. Provision of empirical data supporting the importance of clear communication between pedestrians and automated systems.

**Result:** Participants who encountered eHMIs reported crossing earlier and with more confidence, exhibiting increased perceptions of safety, trust, and understanding towards level-5 ADSs compared to scenarios without eHMIs.

**Limitations:** 

**Conclusion:** eHMIs significantly enhance pedestrian understanding of ADS intent and improve their perceived safety and trust, thereby facilitating better interactions between pedestrians and automated vehicles.

**Abstract:** Each year, over half of global traffic fatalities involve vulnerable road users (e.g. pedestrians), often due to human error. Level-5 automated driving systems (ADSs) could reduce driver errors contributing to pedestrian accidents, though effectiveness depends on clarity and understandability for other road users. External human-machine interfaces (eHMIs) have been proposed to facilitate pedestrian-ADS communication, though consensus on optimal eHMI features remains unclear. In an online survey, 153 participants responded to road-crossing scenarios involving level-5 ADSs, with and without eHMIs. With eHMIs, pedestrians crossed earlier and more confidently, and reported significantly increased perceptions of safety, trust, and understanding when interacting with level-5 ADSs. Visual eHMI features (including a text display and external speedometer) were ranked more necessary than auditory ones, though auditory cues received positive feedback. This study demonstrates that eHMIs can significantly improve pedestrians' understanding of level-5 ADS intent and enhance perceived safety and trust, facilitating more intuitive pedestrian-ADS interactions.

</details>


### [15] [ProMemAssist: Exploring Timely Proactive Assistance Through Working Memory Modeling in Multi-Modal Wearable Devices](https://arxiv.org/abs/2507.21378)

*Kevin Pu, Ting Zhang, Naveen Sendhilnathan, Sebastian Freitag, Raj Sodhi, Tanya Jonker*

**Main category:** cs.HC

**Keywords:** wearable AI, working memory, user engagement, context-sensitive assistance, smart glasses

**Relevance Score:** 9

**TL;DR:** ProMemAssist is a smart glasses system that models a user's working memory in real-time to deliver context-sensitive assistance, outperforming traditional LLM systems in user engagement.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing wearable AI systems lack consideration for users' mental states and often require user initiation for assistance.

**Method:** ProMemAssist uses multi-modal sensor signals to model working memory, employing cognitive theories to represent information and predict optimal assistance timing.

**Key Contributions:**

	1. Introduction of a smart glasses system for modeling working memory in real-time
	2. Demonstrated improved engagement in user studies over LLM systems
	3. Provided design implications for context-sensitive user support

**Result:** In user studies, ProMemAssist provided more tailored assistance and achieved higher user engagement compared to a LLM baseline.

**Limitations:** 

**Conclusion:** The use of working memory modeling can enhance user-aware proactive agents, opening up new design possibilities for assistance systems.

**Abstract:** Wearable AI systems aim to provide timely assistance in daily life, but existing approaches often rely on user initiation or predefined task knowledge, neglecting users' current mental states. We introduce ProMemAssist, a smart glasses system that models a user's working memory (WM) in real-time using multi-modal sensor signals. Grounded in cognitive theories of WM, our system represents perceived information as memory items and episodes with encoding mechanisms, such as displacement and interference. This WM model informs a timing predictor that balances the value of assistance with the cost of interruption. In a user study with 12 participants completing cognitively demanding tasks, ProMemAssist delivered more selective assistance and received higher engagement compared to an LLM baseline system. Qualitative feedback highlights the benefits of WM modeling for nuanced, context-sensitive support, offering design implications for more attentive and user-aware proactive agents.

</details>


### [16] [InSituTale: Enhancing Augmented Data Storytelling with Physical Objects](https://arxiv.org/abs/2507.21411)

*Kentaro Takahira, Yue Yu, Takanori Fujiwara, Suzuki Ryo, Huamin Qu*

**Main category:** cs.HC

**Keywords:** data storytelling, augmented reality, HCI, visualizations, interaction design

**Relevance Score:** 8

**TL;DR:** This paper presents InSituTale, a system for augmented physical data storytelling that allows presenters to manipulate visualizations through physical object interactions, enhancing narrative delivery.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to improve narrative delivery in presentations by incorporating interactions with physical objects, as existing systems mainly focus on body gestures or speech.

**Method:** The authors surveyed data-driven presentations to identify common visualization commands and conducted workshops with HCI/VIS researchers to develop mappings between physical manipulations and these commands. They then created the InSituTale prototype using object tracking and Vision-LLM to execute visualization commands through physical interactions.

**Key Contributions:**

	1. Introduction of augmented physical data storytelling
	2. Development of InSituTale prototype integrating object tracking and Vision-LLM
	3. User study demonstrating high utility and intuitive interactions

**Result:** User studies with 12 participants showed that InSituTale allows for intuitive interactions and contributes to a cohesive and engaging presentation experience, demonstrating high utility.

**Limitations:** 

**Conclusion:** Augmented physical data storytelling through InSituTale effectively blends physical and digital elements, enhancing user engagement in data presentations.

**Abstract:** Augmented data storytelling enhances narrative delivery by integrating visualizations with physical environments and presenter actions. Existing systems predominantly rely on body gestures or speech to control visualizations, leaving interactions with physical objects largely underexplored. We introduce augmented physical data storytelling, an approach enabling presenters to manipulate visualizations through physical object interactions. To inform this approach, we first conducted a survey of data-driven presentations to identify common visualization commands. We then conducted workshops with nine HCI/VIS researchers to collect mappings between physical manipulations and these commands. Guided by these insights, we developed InSituTale, a prototype that combines object tracking via a depth camera with Vision-LLM for detecting real-world events. Through physical manipulations, presenters can dynamically execute various visualization commands, delivering cohesive data storytelling experiences that blend physical and digital elements. A user study with 12 participants demonstrated that InSituTale enables intuitive interactions, offers high utility, and facilitates an engaging presentation experience.

</details>


### [17] [MindChat: Enhancing BCI Spelling with Large Language Models in Realistic Scenarios](https://arxiv.org/abs/2507.21435)

*JIaheng Wang, Yucun Zhong, Chengjie Huang, Lin Yao*

**Main category:** cs.HC

**Keywords:** brain-computer interface, BCI speller, large language model, motor disabilities, context-aware predictions

**Relevance Score:** 9

**TL;DR:** The paper presents MindChat, an LLM-assisted BCI speller designed to enhance typing efficiency for users with motor disabilities by suggesting context-aware completions, significantly reducing keystrokes and spelling time.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current BCI spellers require extensive manual typing, leading to increased spelling errors and inefficiency, particularly for individuals with severe motor disabilities.

**Method:** MindChat uses a large language model (GPT-4o) to provide continuous context-aware word and sentence suggestions during spelling tasks, leveraging prompt engineering techniques.

**Key Contributions:**

	1. Development of MindChat, an LLM-assisted BCI speller that improves efficiency.
	2. Demonstrated significant reduction in keystrokes and spelling time through experimental results.
	3. Utilization of prompt engineering for better user interaction with BCI technology.

**Result:** In online experiments, MindChat achieved over a 62% reduction in keystrokes and more than a 32% decrease in spelling time compared to traditional BCI spellers.

**Limitations:** 

**Conclusion:** The integration of LLMs in BCI spellers can enhance speed and efficiency, facilitating practical applications for users with communication challenges due to motor impairments.

**Abstract:** Brain-computer interface (BCI) spellers can render a new communication channel independent of peripheral nervous system, which are especially valuable for patients with severe motor disabilities. However, current BCI spellers often require users to type intended utterances letter-by-letter while spelling errors grow proportionally due to inaccurate electroencephalogram (EEG) decoding, largely impeding the efficiency and usability of BCIs in real-world communication. In this paper, we present MindChat, a large language model (LLM)-assisted BCI speller to enhance BCI spelling efficiency by reducing users' manual keystrokes. Building upon prompt engineering, we prompt LLMs (GPT-4o) to continuously suggest context-aware word and sentence completions/predictions during spelling. Online copy-spelling experiments encompassing four dialogue scenarios demonstrate that MindChat saves more than 62\% keystrokes and over 32\% spelling time compared with traditional BCI spellers. We envision high-speed BCI spellers enhanced by LLMs will potentially lead to truly practical applications.

</details>


### [18] [Using Tactile Charts to Support Comprehension and Learning of Complex Visualizations for Blind and Low-Vision Individuals](https://arxiv.org/abs/2507.21462)

*Tingying He, Maggie McCracken, Daniel Hajas, Sarah Creem-Regehr, Alexander Lex*

**Main category:** cs.HC

**Keywords:** tactile charts, blind and low-vision, data visualization, mental models, education

**Relevance Score:** 8

**TL;DR:** This study examines the effectiveness of tactile charts in enhancing comprehension and learning for blind and low-vision (BLV) individuals, presenting four custom 3D-printed tactile chart designs after conducting interviews with BLV participants.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Blind and low-vision individuals often rely on assistive technologies for accessing visual information, making it essential to explore alternative approaches like tactile charts to build their mental models of data visualizations.

**Method:** The authors designed four 3D-printed tactile templates for complex chart types and conducted thematic analysis through interviews with 12 BLV participants to assess improvements in understanding and mental models.

**Key Contributions:**

	1. Development of four tactile chart designs for complex visualizations
	2. Comparison of tactile charts versus traditional alt texts among BLV participants
	3. Insights on BLV participants' preferences for tactile chart learning

**Result:** The study found that tactile charts significantly support understanding of complex chart types and are favored by BLV individuals as a learning method over traditional alternative texts.

**Limitations:** The study primarily involves a small sample size and may not generalize to all BLV individuals.

**Conclusion:** Tactile chart models are effective in improving the understanding of complex visualizations for BLV individuals and can play a valuable role in their education.

**Abstract:** We investigate whether tactile charts support comprehension and learning of complex visualizations for blind and low-vision (BLV) individuals and contribute four tactile chart designs and an interview study. Visualizations are powerful tools for conveying data, yet BLV individuals typically can rely only on assistive technologies -- primarily alternative texts -- to access this information. Prior research shows the importance of mental models of chart types for interpreting these descriptions, yet BLV individuals have no means to build such a mental model based on images of visualizations. Tactile charts show promise to fill this gap in supporting the process of building mental models. Yet studies on tactile data representations mostly focus on simple chart types, and it is unclear whether they are also appropriate for more complex charts as would be found in scientific publications. Working with two BLV researchers, we designed 3D-printed tactile template charts with exploration instructions for four advanced chart types: UpSet plots, violin plots, clustered heatmaps, and faceted line charts. We then conducted an interview study with 12 BLV participants comparing whether using our tactile templates improves mental models and understanding of charts and whether this understanding translates to novel datasets experienced through alt texts. Thematic analysis shows that tactile models support chart type understanding and are the preferred learning method by BLV individuals. We also report participants' opinions on tactile chart design and their role in BLV education.

</details>


### [19] [Conversations over Clicks: Impact of Chatbots on Information Search in Interdisciplinary Learning](https://arxiv.org/abs/2507.21490)

*Hannah Kim, Sergei L. Kosakovsky Pond, Stephen MacNeil*

**Main category:** cs.HC

**Keywords:** Generative AI, e-learning, bioinformatics, learner engagement, information search

**Relevance Score:** 6

**TL;DR:** This paper explores the influence of generative AI (GenAI) on learner engagement and information search in bioinformatics education, highlighting both supportive and counterproductive effects based on learner preparation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how GenAI affects learner experiences in navigating complex information spaces in interdisciplinary fields like bioinformatics.

**Method:** An autoethnographic approach was used to study learner interactions with a GenAI chatbot, examining orienteering behaviors and information scent identification.

**Key Contributions:**

	1. Investigation of GenAI's impact on learner orienteering behaviors
	2. Identification of how information scent is perceived in GenAI responses
	3. Recommendations for cautious adoption of GenAI in e-learning environments.

**Result:** GenAI supports orienteering once a learning plan is established but is counterproductive before that; traditional information formats are less effective with GenAI responses.

**Limitations:** The findings may not generalize to all learning contexts outside of bioinformatics.

**Conclusion:** GenAI should be integrated into e-learning with caution, especially in interdisciplinary contexts, due to its nuanced effects on learner navigation and information processing.

**Abstract:** This full research paper investigates the impact of generative AI (GenAI) on the learner experience, with a focus on how learners engage with and utilize the information it provides. In e-learning environments, learners often need to navigate a complex information space on their own. This challenge is further compounded in interdisciplinary fields like bioinformatics, due to the varied prior knowledge and backgrounds. In this paper, we studied how GenAI influences information search in bioinformatics research: (1) How do interactions with a GenAI chatbot influence learner orienteering behaviors?; and (2) How do learners identify information scent in GenAI chatbot responses? We adopted an autoethnographic approach to investigate these questions. GenAI was found to support orienteering once a learning plan was established, but it was counterproductive prior to that. Moreover, traditionally value-rich information sources such as bullet points and related terms proved less effective when applied to GenAI responses. Information scents were primarily recognized through the presence or absence of prior knowledge of the domain. These findings suggest that GenAI should be adopted into e-learning environments with caution, particularly in interdisciplinary learning contexts.

</details>


### [20] [AI Literacy as a Key Driver of User Experience in AI-Powered Assessment: Insights from Socratic Mind](https://arxiv.org/abs/2507.21654)

*Meryem Yilmaz Soylu, Jeonghyun Lee, Jui-Tse Hung, Christopher Zhang Cui, David A. Joyner*

**Main category:** cs.HC

**Keywords:** AI literacy, higher education, user experience, formative assessment, self-determination theory

**Relevance Score:** 8

**TL;DR:** This study investigates how AI literacy influences students' perceptions of an AI-powered assessment tool in higher education.

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding student interactions with AI tools in higher education is essential for effective learning support.

**Method:** The study analyzed data from 309 undergraduate students using validated surveys and partial least squares structural equation modeling.

**Key Contributions:**

	1. Identifies the importance of AI literacy over mere exposure in shaping student experiences with AI tools.
	2. Suggests design principles for AI educational tools to cater to varying literacy levels.
	3. Provides empirical data linking AI literacy to learning effectiveness through usability and engagement.

**Result:** AI literacy predicts usability, satisfaction, and engagement with the AI tool, while prior exposure to AI technology does not significantly affect these perceptions.

**Limitations:** 

**Conclusion:** Enhancing AI literacy is crucial for improving student experiences with AI tools, suggesting the need for user-centered design in education technology.

**Abstract:** As Artificial Intelligence (AI) tools become increasingly embedded in higher education, understanding how students interact with these systems is essential to supporting effective learning. This study examines how students' AI literacy and prior exposure to AI technologies shape their perceptions of Socratic Mind, an interactive AI-powered formative assessment tool. Drawing on Self-Determination Theory and user experience research, we analyze relationships among AI literacy, perceived usability, satisfaction, engagement, and perceived learning effectiveness. Data from 309 undergraduates in Computer Science and Business courses were collected through validated surveys. Partial least squares structural equation modeling showed that AI literacy - especially self-efficacy, conceptual understanding, and application skills - significantly predicts usability, satisfaction, and engagement. Usability and satisfaction, in turn, strongly predict perceived learning effectiveness, while prior AI exposure showed no significant effect. These findings highlight that AI literacy, rather than exposure alone, shapes student experiences. Designers should integrate adaptive guidance and user-centered features to support diverse literacy levels, fostering inclusive, motivating, and effective AI-based learning environments.

</details>


### [21] [Identification of Design Recommendations for Augmented Reality Authors in Corporate Training](https://arxiv.org/abs/2507.21722)

*Stefan Graser, Martin Schrepp, Stephan Böhm*

**Main category:** cs.HC

**Keywords:** Augmented Reality, User-Centered Design, Corporate Training, Design Recommendations, NLP

**Relevance Score:** 6

**TL;DR:** This study identifies and classifies AR design recommendations for the User-Centered Design process in Corporate Training, addressing the lack of context-specific guidelines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a gap in context-specific AR design recommendations during the software development process, particularly relevant to the evaluation phase of User-Centered Design.

**Method:** A multi-method approach was used: extending an existing Mixed Reality design recommendations dataset with AR-specific recommendations, classifying them using a NLP-based classification approach, summarizing content, and qualitatively evaluating relevance with expert feedback.

**Key Contributions:**

	1. Developed an updated dataset of AR design recommendations for Corporate Training.
	2. Utilized NLP classification methods to analyze design recommendations.
	3. Provided expert evaluations on the relevance of design topics for AR in Corporate Training.

**Result:** An updated dataset of 597 design recommendations classified into 84 topics was created; 32 topics with 284 statements were evaluated as relevant for AR in Corporate Training.

**Limitations:** 

**Conclusion:** The research enhances understanding of AR application design in Corporate Training and contributes to a User Experience measurement approach for AR authors.

**Abstract:** Innovative technologies, such as Augmented Reality (AR), introduce new interaction paradigms, demanding the identification of software requirements during the software development process. In general, design recommendations are related to this, supporting the design of applications positively and meeting stakeholder needs. However, current research lacks context-specific AR design recommendations. This study addresses this gap by identifying and analyzing practical AR design recommendations relevant to the evaluation phase of the User-Centered Design (UCD) process. We rely on an existing dataset of Mixed Reality (MR) design recommendations. We applied a multi-method approach by (1) extending the dataset with AR-specific recommendations published since 2020, (2) classifying the identified recommendations using a NLP classification approach based on a pre-trained Sentence Transformer model, (3) summarizing the content of all topics, and (4) evaluating their relevance concerning AR in Corporate Training (CT) both based on a qualitative Round Robin approach with five experts. As a result, an updated dataset of 597 practitioner design recommendations, classified into 84 topics, is provided with new insights into their applicability in the context of AR in CT. Based on this, 32 topics with a total of 284 statements were evaluated as relevant for AR in CT. This research directly contributes to the authors' work for extending their AR-specific User Experience (UX) measurement approach, supporting AR authors in targeting the improvement of AR applications for CT scenarios.

</details>


### [22] [Helping or Homogenizing? GenAI as a Design Partner to Pre-Service SLPs for Just-in-Time Programming of AAC](https://arxiv.org/abs/2507.21811)

*Cynthia Zastudil, Christine Holyfield, Christine Kapp, Kate Hamilton, Kriti Baru, Liam Newsam, June A. Smith, Stephen MacNeil*

**Main category:** cs.HC

**Keywords:** generative AI, augmentative and alternative communication, visual scene displays, speech-language pathologists, assistive technology

**Relevance Score:** 6

**TL;DR:** This paper presents a generative AI prototype aimed at assisting non-expert users, particularly pre-service speech-language pathologists, in creating visual scene displays (VSDs) for augmentative and alternative communication (AAC) devices for minimally verbal autistic children.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate the configuration of VSDs for AAC users by utilizing generative AI, addressing challenges faced by non-experts.

**Method:** A prototype was developed to automatically suggest hotspots on images for VSDs; a user study was conducted to evaluate its effectiveness among pre-service speech-language pathologists.

**Key Contributions:**

	1. Development of a generative AI prototype for AAC device configuration
	2. User study demonstrating the impact on efficiency and confidence
	3. Identification of risks such as over-reliance and communication homogenization

**Result:** The use of the prototype improved SLPs' efficiency and confidence in creating VSDs, but also led to concerns about over-reliance and homogenization of communication options.

**Limitations:** Mixed results in user experience and concerns about the quality of generated communication options.

**Conclusion:** While the prototype enhances the creation of VSDs, it raises caution regarding the risks of generative AI in assistive technology, necessitating further research.

**Abstract:** Augmentative and alternative communication (AAC) devices are used by many people around the world who experience difficulties in communicating verbally. One AAC device which is especially useful for minimally verbal autistic children in developing language and communication skills are visual scene displays (VSD). VSDs use images with interactive hotspots embedded in them to directly connect language to real-world contexts which are meaningful to the AAC user. While VSDs can effectively support emergent communicators, their widespread adoption is impacted by how difficult these devices are to configure. We developed a prototype that uses generative AI to automatically suggest initial hotspots on an image to help non-experts efficiently create VSDs. We conducted a within-subjects user study to understand how effective our prototype is in supporting non-expert users, specifically pre-service speech-language pathologists (SLP) who are not familiar with VSDs as an AAC intervention. Pre-service SLPs are actively studying to become clinically certified SLPs and have domain-specific knowledge about language and communication skill development. We evaluated the effectiveness of our prototype based on creation time, quality, and user confidence. We also analyzed the relevance and developmental appropriateness of the automatically generated hotspots and how often users interacted with the generated hotspots. Our results were mixed with SLPs becoming more efficient and confident. However, there were multiple negative impacts as well, including over-reliance and homogenization of communication options. The implications of these findings reach beyond the domain of AAC, especially as generative AI becomes more prevalent across domains, including assistive technology. Future work is needed to further identify and address these risks associated with integrating generative AI into assistive technology.

</details>


### [23] [VeasyGuide: Personalized Visual Guidance for Low-vision Learners on Instructor Actions in Presentation Videos](https://arxiv.org/abs/2507.21837)

*Yotam Sechayk, Ariel Shamir, Amy Pavel, Takeo Igarashi*

**Main category:** cs.HC

**Keywords:** Low-vision learners, Educational videos, Motion detection

**Relevance Score:** 7

**TL;DR:** VeasyGuide enhances learning for low-vision learners by dynamically highlighting instructor actions in educational videos, improving information detection and reducing cognitive load.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Low-vision learners struggle to interpret visual cues in educational videos, leading to missed information and increased cognitive load.

**Method:** A co-design study with low-vision participants led to the development of VeasyGuide, which uses motion detection for highlighting instructor actions in real-time based on user needs.

**Key Contributions:**

	1. Development of VeasyGuide for low-vision learner accessibility
	2. Use of motion detection for dynamic highlighting
	3. Improvement in engagement for both low-vision and sighted learners

**Result:** The evaluation demonstrated significant improvements in action detection, response times, and reduced cognitive load for low-vision learners, and increased engagement among sighted participants.

**Limitations:** 

**Conclusion:** VeasyGuide presents a promising approach to making educational content more accessible for all learners through personalization and real-time feedback.

**Abstract:** Instructors often rely on visual actions such as pointing, marking, and sketching to convey information in educational presentation videos. These subtle visual cues often lack verbal descriptions, forcing low-vision (LV) learners to search for visual indicators or rely solely on audio, which can lead to missed information and increased cognitive load. To address this challenge, we conducted a co-design study with three LV participants and developed VeasyGuide, a tool that uses motion detection to identify instructor actions and dynamically highlight and magnify them. VeasyGuide produces familiar visual highlights that convey spatial context and adapt to diverse learners and content through extensive personalization and real-time visual feedback. VeasyGuide reduces visual search effort by clarifying what to look for and where to look. In an evaluation with 8 LV participants, learners demonstrated a significant improvement in detecting instructor actions, with faster response times and significantly reduced cognitive load. A separate evaluation with 8 sighted participants showed that VeasyGuide also enhanced engagement and attentiveness, suggesting its potential as a universally beneficial tool.

</details>


### [24] [Leveraging LLMs for Persona-Based Visualization of Election Data](https://arxiv.org/abs/2507.21900)

*Swaroop Panda, Arun Kumar Sekar*

**Main category:** cs.HC

**Keywords:** visualization, personas, election, Large Language Models, voter demographics

**Relevance Score:** 7

**TL;DR:** This paper explores the use of personas derived from diverse voter demographics to design effective visualizations for election information, enhanced by Large Language Models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve the understanding and relevance of election information through personalized visualizations that cater to different voter demographics.

**Method:** The study combines data from UK parliamentary elections with LLM developments to create personas representing various voter segments and inform the design of visualizations based on these personas.

**Key Contributions:**

	1. Utilization of personas for election data visualization
	2. Integration of Large Language Models in persona development
	3. Prototyping and evaluation of tailored visualizations for diverse voter segments

**Result:** Prototypes of visualizations tailored to identified personas were created and evaluated, illustrating improvements in clarity and relevance of election data for different voter groups.

**Limitations:** 

**Conclusion:** The findings suggest that designing visualizations with an understanding of voter personas greatly enhances information dissemination, offering actionable insights for election strategy and education.

**Abstract:** Visualizations are essential tools for disseminating information regarding elections and their outcomes, potentially influencing public perceptions. Personas, delineating distinctive segments within the populace, furnish a valuable framework for comprehending the nuanced perspectives, requisites, and behaviors of diverse voter demographics. In this work, we propose making visualizations tailored to these personas to make election information easier to understand and more relevant. Using data from UK parliamentary elections and new developments in Large Language Models (LLMs), we create personas that encompass the diverse demographics, technological preferences, voting tendencies, and information consumption patterns observed among voters.Subsequently, we elucidate how these personas can inform the design of visualizations through specific design criteria. We then provide illustrative examples of visualization prototypes based on these criteria and evaluate these prototypes using these personas and LLMs. We finally propose some actionable insights based upon the framework and the different design artifacts.

</details>


### [25] [MapAgent: Trajectory-Constructed Memory-Augmented Planning for Mobile Task Automation](https://arxiv.org/abs/2507.21953)

*Yi Kong, Dianxi Shi, Guoli Yang, Zhang ke-di, Chenlin Huang, Xiaopeng Li, Songchang Jin*

**Main category:** cs.HC

**Keywords:** Large Language Models, task planning, mobile applications, HCI, autonomous agents

**Relevance Score:** 9

**TL;DR:** MapAgent is a novel LLM-based agent framework that enhances task planning on mobile devices by using a memory mechanism based on historical trajectories.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve task automation on mobile devices by addressing the challenges faced by LLM-based agents in understanding complex real-world tasks.

**Method:** Introduced a trajectory-based memory mechanism to create a structured database of task execution snapshots, coupled with a coarse-to-fine task planning approach to enhance context-aware planning.

**Key Contributions:**

	1. Development of a trajectory-based memory mechanism for task planning.
	2. Coarse-to-fine task planning approach that enhances understanding of app scenarios.
	3. Execution of planned tasks through a dual-LLM architecture.

**Result:** MapAgent outperforms existing methods in real-world scenarios by providing more informed task planning and effective execution.

**Limitations:** 

**Conclusion:** The proposed framework significantly improves task automation and will be open-sourced to encourage further research and development.

**Abstract:** The recent advancement of autonomous agents powered by Large Language Models (LLMs) has demonstrated significant potential for automating tasks on mobile devices through graphical user interfaces (GUIs). Despite initial progress, these agents still face challenges when handling complex real-world tasks. These challenges arise from a lack of knowledge about real-life mobile applications in LLM-based agents, which may lead to ineffective task planning and even cause hallucinations. To address these challenges, we propose a novel LLM-based agent framework called MapAgent that leverages memory constructed from historical trajectories to augment current task planning. Specifically, we first propose a trajectory-based memory mechanism that transforms task execution trajectories into a reusable and structured page-memory database. Each page within a trajectory is extracted as a compact yet comprehensive snapshot, capturing both its UI layout and functional context. Secondly, we introduce a coarse-to-fine task planning approach that retrieves relevant pages from the memory database based on similarity and injects them into the LLM planner to compensate for potential deficiencies in understanding real-world app scenarios, thereby achieving more informed and context-aware task planning. Finally, planned tasks are transformed into executable actions through a task executor supported by a dual-LLM architecture, ensuring effective tracking of task progress. Experimental results in real-world scenarios demonstrate that MapAgent achieves superior performance to existing methods. The code will be open-sourced to support further research.

</details>


### [26] [DataSway: Vivifying Metaphoric Visualization with Animation Clip Generation and Coordination](https://arxiv.org/abs/2507.22051)

*Liwenhan Xie, Jiayi Zhou, Anyi Rao, Huamin Qu, Xinhuan Shu*

**Main category:** cs.HC

**Keywords:** visualization, human-AI co-creation, metaphoric visualizations, animation, data engagement

**Relevance Score:** 8

**TL;DR:** This paper presents a human-AI co-creation workflow for animating metaphoric visualizations in web-based contexts, addressing challenges in design and implementation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to enhance comprehension and engagement in abstract data through animated metaphoric visualizations, while overcoming design challenges faced by creators.

**Method:** A human-AI co-creation workflow is proposed, utilizing vision-language models (VLMs) to generate animation clips and allowing users to coordinate timelines based on various attributes.

**Key Contributions:**

	1. Development of a human-AI co-creation workflow for animation design
	2. Introduction of DataSway, a tool for creating SVG-based animated visualizations
	3. Insights gathered from user studies on creativity support and usability

**Result:** A user study with 14 participants showed that the prototype, DataSway, effectively supports creativity and usability for animating metaphoric visualizations.

**Limitations:** The study is limited by its small sample size and focus on specific design workflows, which may not generalize to all types of visualizations.

**Conclusion:** The findings highlight the potential of the proposed workflow and prototype in advancing data visualization practices, with recommendations for future research in the field.

**Abstract:** Animating metaphoric visualizations brings data to life, enhancing the comprehension of abstract data encodings and fostering deeper engagement. However, creators face significant challenges in designing these animations, such as crafting motions that align semantically with the metaphors, maintaining faithful data representation during animation, and seamlessly integrating interactivity. We propose a human-AI co-creation workflow that facilitates creating animations for SVG-based metaphoric visualizations. Users can initially derive animation clips for data elements from vision-language models (VLMs) and subsequently coordinate their timelines based on entity order, attribute values, spatial layout, or randomness. Our design decisions were informed by a formative study with experienced designers (N=8). We further developed a prototype, DataSway, and conducted a user study (N=14) to evaluate its creativity support and usability. A gallery with 6 cases demonstrates its capabilities and applications in web-based hypermedia. We conclude with implications for future research on bespoke data visualization animation.

</details>


### [27] [Exploring Keyboard Positioning and Ten-Finger Typing in Mixed Reality](https://arxiv.org/abs/2410.04177)

*Cecilia Schmitz, Joshua Reynolds, Scott Kuhl, Keith Vertanen*

**Main category:** cs.HC

**Keywords:** mixed reality, typing performance, tactile feedback, eye-tracking, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper investigates typing performance in mixed reality using virtual keyboards, focusing on tactile feedback and eye-tracking techniques.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved typing accuracy and speed in mixed reality environments, as traditional midair typing methods lack tactile feedback and convenience.

**Method:** Two experiments were conducted: the first explored tactile feedback by positioning a virtual keyboard on various surfaces and the second investigated ten-finger typing using an eye-tracking technique to enhance keystroke accuracy.

**Key Contributions:**

	1. Introduction of tactile feedback for virtual keyboards in mixed reality
	2. Novel eye-tracking technique to prevent accidental key presses
	3. Comparative analysis of typing methods in mixed reality environments

**Result:** Participants achieved the highest typing speed of 12 words-per-minute with the midair keyboard and had lower correction rates with ten-finger typing when using eye-tracking, despite being slightly slower than index-finger typing.

**Limitations:** Results may vary based on different user preferences and the lack of auto-correct in the tested keyboards.

**Conclusion:** While midair keyboards without tactile feedback were still preferred for speed, incorporating tactile feedback and eye-tracking can enhance accuracy in mixed reality typing.

**Abstract:** Accuracy and speed are pivotal when typing. Mixed reality typing is typically performed by typing on a midair keyboard with your index fingers. This deprives users of both the tactile feedback available on physical devices and the ability to press keys with the most convenient finger. Our first experiment investigated providing tactile feedback by positioning the virtual keyboard on a table or wall. The keyboard was deterministic (without auto-correct), supported mixed case typing with symbols, and relied only on the hand-tracking provided by a commodity headset's egocentric cameras. Users preferred and had the highest entry rate of 12 words-per-minute using a midair keyboard. Error rates were similar in all conditions. Our second experiment explored ten-finger typing and used a novel eye-tracking technique to avoid accidental key presses. This technique was preferred for ten-finger typing and halved corrections. However, participants were faster using their index fingers without eye-tracking at 11 words-per-minute.

</details>


### [28] [Can LLMs Reason About Trust?: A Pilot Study](https://arxiv.org/abs/2507.21075)

*Anushka Debnath, Stephen Cranefield, Emiliano Lorini, Bastin Tony Roy Savarimuthu*

**Main category:** cs.HC

**Keywords:** Trust, Large Language Models, Human-Computer Interaction, AI, Social Relationships

**Relevance Score:** 8

**TL;DR:** This paper explores how Large Language Models (LLMs) can understand and foster trust in human relationships through electronic interactions.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** Trust is crucial in social relationships, and as many interactions occur online, AI has the potential to assist in managing these dynamics.

**Method:** The paper investigates LLMs' reasoning about trust between individuals and their ability to induce trust through role-playing scenarios.

**Key Contributions:**

	1. Analysis of LLMs in reasoning about trust
	2. Role-playing scenario implementation
	3. Insights into AI-assisted social relationship management

**Result:** The study finds that LLMs can effectively reason about and induce trust in specified contexts, demonstrating their potential in relationship management.

**Limitations:** The investigation is limited to specific scenarios and may not generalize to all types of trust interactions.

**Conclusion:** LLMs can help foster trust in digital interactions, paving the way for more effective cooperative behaviors among users.

**Abstract:** In human society, trust is an essential component of social attitude that helps build and maintain long-term, healthy relationships which creates a strong foundation for cooperation, enabling individuals to work together effectively and achieve shared goals. As many human interactions occur through electronic means such as using mobile apps, the potential arises for AI systems to assist users in understanding the social state of their relationships. In this paper we investigate the ability of Large Language Models (LLMs) to reason about trust between two individuals in an environment which requires fostering trust relationships. We also assess whether LLMs are capable of inducing trust by role-playing one party in a trust based interaction and planning actions which can instil trust.

</details>


### [29] [Emotionally Aware Moderation: The Potential of Emotion Monitoring in Shaping Healthier Social Media Conversations](https://arxiv.org/abs/2507.21089)

*Xiaotian Su, Naim Zierau, Soomin Kim, April Yi Wang, Thiemo Wambsganss*

**Main category:** cs.HC

**Keywords:** emotion monitoring, hate speech, social media, user behavior, emotional awareness

**Relevance Score:** 7

**TL;DR:** This paper evaluates emotion monitoring dashboards aimed at increasing users' emotional awareness and reducing hate speech on social media.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current moderation techniques in curbing hate speech without fostering a climate of censorship.

**Method:** A study was conducted with 211 participants to assess the impact of two emotion monitoring mechanisms on commenting behavior and emotional experiences.

**Key Contributions:**

	1. Proposed emotion monitoring dashboards
	2. Evaluated user behavior and emotional outcomes
	3. Identified unintended effects of emotional interventions

**Result:** The interventions improved users' awareness of their emotional states and successfully reduced hate speech, but also led to increased expression of negative emotions when discussing sensitive issues.

**Limitations:** Potential unintended consequences of increased negative emotional expression.

**Conclusion:** While emotion regulation tools can lower hate speech, they might inadvertently heighten negative emotional expressions, necessitating further investigation into their design and implementation.

**Abstract:** Social media platforms increasingly employ proactive moderation techniques, such as detecting and curbing toxic and uncivil comments, to prevent the spread of harmful content. Despite these efforts, such approaches are often criticized for creating a climate of censorship and failing to address the underlying causes of uncivil behavior. Our work makes both theoretical and practical contributions by proposing and evaluating two types of emotion monitoring dashboards to users' emotional awareness and mitigate hate speech. In a study involving 211 participants, we evaluate the effects of the two mechanisms on user commenting behavior and emotional experiences. The results reveal that these interventions effectively increase users' awareness of their emotional states and reduce hate speech. However, our findings also indicate potential unintended effects, including increased expression of negative emotions (Angry, Fear, and Sad) when discussing sensitive issues. These insights provide a basis for further research on integrating proactive emotion regulation tools into social media platforms to foster healthier digital interactions.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [30] [Categorical Classification of Book Summaries Using Word Embedding Techniques](https://arxiv.org/abs/2507.21058)

*Kerem Keskin, Mümine Kaya Keleş*

**Main category:** cs.CL

**Keywords:** word embedding, natural language processing, machine learning, text classification, Turkish

**Relevance Score:** 5

**TL;DR:** The study classifies book summaries and categories using word embeddings, NLP techniques, and machine learning algorithms, focusing on methods like TF-IDF and Word2Vec, with a successful application on Turkish texts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To classify book summaries and categories effectively using advanced NLP techniques and machine learning algorithms.

**Method:** The study employed word embedding methods including One Hot Encoding, Word2Vec, and TF-IDF, alongside machine learning classifiers such as Support Vector Machine, Naive Bayes, and Logistic Regression.

**Key Contributions:**

	1. Comparison of multiple word embedding techniques (TF-IDF, One-Hot Encoding, Word2Vec)
	2. Evaluation of various machine learning algorithms on Turkish texts
	3. Insights into the performance of ML models in text classification tasks

**Result:** The models that utilized TF-IDF and One-Hot Encoding techniques achieved superior results for classifying Turkish texts.

**Limitations:** The study is focused on Turkish texts, which may limit the generalizability of results to other languages.

**Conclusion:** Support Vector Machine, Naive Bayes, and Logistic Regression models performed well with the chosen word embedding techniques, indicating their effectiveness in this context.

**Abstract:** In this study, book summaries and categories taken from book sites were classified using word embedding methods, natural language processing techniques and machine learning algorithms. In addition, one hot encoding, Word2Vec and Term Frequency - Inverse Document Frequency (TF-IDF) methods, which are frequently used word embedding methods were used in this study and their success was compared. Additionally, the combination table of the pre-processing methods used is shown and added to the table. Looking at the results, it was observed that Support Vector Machine, Naive Bayes and Logistic Regression Models and TF-IDF and One-Hot Encoder word embedding techniques gave more successful results for Turkish texts.

</details>


### [31] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)

*Sabrina Patania, Luca Annese, Cansu Koyuturk, Azzurra Ruggeri, Dimitri Ognibene*

**Main category:** cs.CL

**Keywords:** Large Language Models, Social Learning, Pedagogical Strategies, Knowledge Acquisition, AI Training

**Relevance Score:** 8

**TL;DR:** Explores socially mediated learning for AI models using dyadic dialogues between learner and teacher agents to enhance knowledge acquisition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional AI learning methods struggle with integrating complex knowledge. This study seeks to improve knowledge acquisition in LLMs via sociocultural learning paradigms.

**Method:** Introduced the 'AI Social Gym' where AI learner engages with teacher agents in structured dialogues, assessing different pedagogical strategies for knowledge acquisition.

**Key Contributions:**

	1. Proposed a novel environment for AI learning called 'AI Social Gym'.
	2. Demonstrated the effectiveness of mixed-direction pedagogical strategies in knowledge acquisition.
	3. Highlighted the importance of integrating psychological theories into AI training.

**Result:** Mixed-direction interactions significantly enhance LLM's ontology acquisition, outperforming traditional instructional methods and direct knowledge access.

**Limitations:** The study may not account for all types of knowledge integration and context variations in real-world applications.

**Conclusion:** Integrating pedagogical insights into AI training can improve post-training knowledge acquisition and response quality, presenting a valuable alternative to existing approaches like prompt engineering.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.   We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.   Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.   These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering

</details>


### [32] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)

*David James Woo, Yangyang Yu, Kai Guo, Yilin Huang, April Ka Yeng Fung*

**Main category:** cs.CL

**Keywords:** AI in education, EFL writing, editing behaviors, composition quality, human-rated scoring

**Relevance Score:** 6

**TL;DR:** This research examines EFL secondary students' editing behaviors with AI-generated texts in their writing process.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the impact of AI-generated text on students' expository writing and editing behaviors, as its use in EFL contexts is still understudied.

**Method:** A convergent design analyzing screen recordings and compositions of 39 Hong Kong secondary students using qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.

**Key Contributions:**

	1. Investigated the editing behaviors of students using AI-generated texts
	2. Identified significant patterns in editing processes during expository writing
	3. Highlighted the need for genre-specific instruction regarding AI integration

**Result:** Identified two editing patterns among students and found that while AI-generated words positively correlated with composition scores, editing effort had minimal impact on quality.

**Limitations:** 

**Conclusion:** AI supports students' writing but doesn't replace writing skills; genre-specific instruction and assessments that value the writing process are important.

**Abstract:** Text generated by artificial intelligence (AI) chatbots is increasingly used in English as a foreign language (EFL) writing contexts, yet its impact on students' expository writing process and compositions remains understudied. This research examines how EFL secondary students edit AI-generated text. Exploring editing behaviors in their expository writing process and in expository compositions, and their effect on human-rated scores for content, organization, language, and overall quality. Participants were 39 Hong Kong secondary students who wrote an expository composition with AI chatbots in a workshop. A convergent design was employed to analyze their screen recordings and compositions to examine students' editing behaviors and writing qualities. Analytical methods included qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis. We analyzed over 260 edits per dataset, and identified two editing patterns: one where students refined introductory units repeatedly before progressing, and another where they quickly shifted to extensive edits in body units (e.g., topic and supporting sentences). MLR analyses revealed that the number of AI-generated words positively predicted all score dimensions, while most editing variables showed minimal impact. These results suggest a disconnect between students' significant editing effort and improved composition quality, indicating AI supports but does not replace writing skills. The findings highlight the importance of genre-specific instruction and process-focused writing before AI integration. Educators should also develop assessments valuing both process and product to encourage critical engagement with AI text.

</details>


### [33] [Which symbol grounding problem should we try to solve?](https://arxiv.org/abs/2507.21080)

*Vincent C. Müller*

**Main category:** cs.CL

**Keywords:** grounding problem, artificial agents, semantic commitment

**Relevance Score:** 2

**TL;DR:** The paper critiques Floridi and Taddeo's 'zero semantic commitment' condition for the grounding problem and proposes a re-thinking of the problem itself.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of the 'zero semantic commitment' condition proposed by Floridi and Taddeo regarding the grounding problem in artificial agents.

**Method:** The author critiques existing proposals, especially contrasting Floridi and Taddeo's condition with Luc Steels' suggestion, advocating for a re-evaluation of the grounding problem's definition and the role of system goals.

**Key Contributions:**

	1. Critique of 'zero semantic commitment' as insufficient for grounding problem solutions.
	2. Introduction of a new perspective on defining the grounding problem.
	3. Emphasis on the role of 'goals' in computational agent design.

**Result:** The author concludes that the true grounding problem involves understanding and reproducing the behavioral ability and function of meaning in computational agents rather than adhering strictly to previous definitions.

**Limitations:** 

**Conclusion:** A re-assessment of the grounding problem is necessary, focusing on how to explain and reproduce meaning-related behaviors in artificial systems.

**Abstract:** Floridi and Taddeo propose a condition of "zero semantic commitment" for solutions to the grounding problem, and a solution to it. I argue briefly that their condition cannot be fulfilled, not even by their own solution. After a look at Luc Steels' very different competing suggestion, I suggest that we need to re-think what the problem is and what role the 'goals' in a system play in formulating the problem. On the basis of a proper understanding of computing, I come to the conclusion that the only sensible grounding problem is how we can explain and re-produce the behavioral ability and function of meaning in artificial computational agents

</details>


### [34] [ChatGPT Reads Your Tone and Responds Accordingly -- Until It Does Not -- Emotional Framing Induces Bias in LLM Outputs](https://arxiv.org/abs/2507.21083)

*Franck Bardol*

**Main category:** cs.CL

**Keywords:** Large Language Models, Emotional Framing, AI Alignment, Biases in AI, GPT-4

**Relevance Score:** 9

**TL;DR:** This paper investigates how emotional phrasing of prompts affects the responses of GPT-4, revealing significant biases influenced by the tone of the input.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how large language models adjust their responses based on the emotional tone of inquiries and understand the implications for AI alignment and trust.

**Method:** The study systematically varied the emotional tone of 156 prompts and analyzed the resulting model responses, focusing on negative and neutral framing.

**Key Contributions:**

	1. Identification of the 'rebound' bias in LLM responses to emotional phrasing
	2. Introduction of the 'tone floor' concept to quantify response negativity
	3. Utilization of tone-valence transition matrices to analyze model behavior

**Result:** Findings indicate that GPT-4 is three times less likely to respond negatively to negatively framed prompts compared to neutral ones, highlighting a rebound bias and suppressed tone variation in sensitive topics.

**Limitations:** Limited scope of prompts and contexts analyzed; results may not generalize to all types of language models or applications.

**Conclusion:** The research demonstrates significant tone-driven biases in LLM responses, calling for further examination in the context of AI alignment and user trust.

**Abstract:** Large Language Models like GPT-4 adjust their responses not only based on the question asked, but also on how it is emotionally phrased. We systematically vary the emotional tone of 156 prompts - spanning controversial and everyday topics - and analyze how it affects model responses. Our findings show that GPT-4 is three times less likely to respond negatively to a negatively framed question than to a neutral one. This suggests a "rebound" bias where the model overcorrects, often shifting toward neutrality or positivity. On sensitive topics (e.g., justice or politics), this effect is even more pronounced: tone-based variation is suppressed, suggesting an alignment override. We introduce concepts like the "tone floor" - a lower bound in response negativity - and use tone-valence transition matrices to quantify behavior. Visualizations based on 1536-dimensional embeddings confirm semantic drift based on tone. Our work highlights an underexplored class of biases driven by emotional framing in prompts, with implications for AI alignment and trust. Code and data are available at: https://github.com/bardolfranck/llm-responses-viewer

</details>


### [35] [Reviving Your MNEME: Predicting The Side Effects of LLM Unlearning and Fine-Tuning via Sparse Model Diffing](https://arxiv.org/abs/2507.21084)

*Aly M. Kassem, Zhuan Shi, Negar Rostamzadeh, Golnoosh Farnadi*

**Main category:** cs.CL

**Keywords:** large language models, fine-tuning, model diffing, unintended side effects, HCI

**Relevance Score:** 8

**TL;DR:** The paper presents MNEME, a framework to identify unintended side effects of fine-tuning large language models (LLMs) by comparing base and fine-tuned models on task-agnostic data, achieving high prediction accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of general approaches for detecting unintended side effects resulting from fine-tuning of LLMs, which can degrade performance on specific tasks.

**Method:** MNEME uses sparse model diffing to compare base and fine-tuned models on task-agnostic data to detect behavioral shifts without needing access to fine-tuning data.

**Key Contributions:**

	1. Introduction of the MNEME framework for detecting unintended side effects in LLMs
	2. Demonstration of high accuracy in predicting model behavior shifts
	3. Provision of practical tools for managing fine-tuning effects

**Result:** MNEME achieves up to 95 percent accuracy in predicting side effects across five LLMs and three scenarios, aligning with known benchmarks.

**Limitations:** 

**Conclusion:** The framework shows that sparse probing and diffing can provide scalable tools for understanding and managing changes in LLM behavior resulting from fine-tuning.

**Abstract:** Large language models (LLMs) are frequently fine-tuned or unlearned to adapt to new tasks or eliminate undesirable behaviors. While existing evaluation methods assess performance after such interventions, there remains no general approach for detecting unintended side effects, such as unlearning biology content degrading performance on chemistry tasks, particularly when these effects are unpredictable or emergent. To address this issue, we introduce MNEME, Model diffiNg for Evaluating Mechanistic Effects, a lightweight framework for identifying these side effects using sparse model diffing. MNEME compares base and fine-tuned models on task-agnostic data (for example, The Pile, LMSYS-Chat-1M) without access to fine-tuning data to isolate behavioral shifts. Applied to five LLMs across three scenarios: WMDP knowledge unlearning, emergent misalignment, and benign fine-tuning, MNEME achieves up to 95 percent accuracy in predicting side effects, aligning with known benchmarks and requiring no custom heuristics. Furthermore, we show that retraining on high-activation samples can partially reverse these effects. Our results demonstrate that sparse probing and diffing offer a scalable and automated lens into fine-tuning-induced model changes, providing practical tools for understanding and managing LLM behavior.

</details>


### [36] [Multi-Amateur Contrastive Decoding for Text Generation](https://arxiv.org/abs/2507.21086)

*Jaydip Sen, Subhasis Dasgupta, Hetvi Waghela*

**Main category:** cs.CL

**Keywords:** Contrastive Decoding, Multi-Amateur Contrastive Decoding, Language Generation, Fluency, Coherence

**Relevance Score:** 8

**TL;DR:** This paper introduces Multi-Amateur Contrastive Decoding (MACD), an enhancement to contrastive decoding that uses multiple amateur models to improve fluency, coherence, diversity, and adaptability in language generation, addressing limitations of traditional methods.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of Contrastive Decoding (CD), which relies on a single amateur model, hindering its ability to capture diverse failure modes in language generation.

**Method:** MACD employs an ensemble of amateur models, integrating contrastive signals through averaging and consensus penalization, and allows for controllable generation with stylistic or content biases.

**Key Contributions:**

	1. Introduction of Multi-Amateur Contrastive Decoding (MACD)
	2. Ensemble approach to characterizing generation failure modes
	3. Facilitates controllable text generation based on stylistic and content biases

**Result:** Experimental results indicate that MACD outperforms both traditional decoding methods and the original CD in fluency, coherence, diversity, and adaptability across various content domains without additional training.

**Limitations:** 

**Conclusion:** MACD represents a significant advancement in language generation by effectively leveraging multiple amateur models to enhance output quality.

**Abstract:** Contrastive Decoding (CD) has emerged as an effective inference-time strategy for enhancing open-ended text generation by exploiting the divergence in output probabilities between a large expert language model and a smaller amateur model. Although CD improves coherence and fluency, its dependence on a single amateur restricts its capacity to capture the diverse and multifaceted failure modes of language generation, such as repetition, hallucination, and stylistic drift. This paper proposes Multi-Amateur Contrastive Decoding (MACD), a generalization of the CD framework that employs an ensemble of amateur models to more comprehensively characterize undesirable generation patterns. MACD integrates contrastive signals through both averaging and consensus penalization mechanisms and extends the plausibility constraint to operate effectively in the multi-amateur setting. Furthermore, the framework enables controllable generation by incorporating amateurs with targeted stylistic or content biases. Experimental results across multiple domains, such as news, encyclopedic, and narrative, demonstrate that MACD consistently surpasses conventional decoding methods and the original CD approach in terms of fluency, coherence, diversity, and adaptability, all without requiring additional training or fine-tuning.

</details>


### [37] [QU-NLP at CheckThat! 2025: Multilingual Subjectivity in News Articles Detection using Feature-Augmented Transformer Models with Sequential Cross-Lingual Fine-Tuning](https://arxiv.org/abs/2507.21095)

*Mohammad AL-Smadi*

**Main category:** cs.CL

**Keywords:** subjectivity detection, transformer architecture, cross-lingual, TF-IDF, natural language processing

**Relevance Score:** 4

**TL;DR:** The paper introduces a feature-augmented transformer architecture for subjectivity detection, achieving high performance across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a system that can effectively distinguish between subjective and objective sentences in news articles across different languages.

**Method:** The approach combines contextual embeddings from pre-trained language models with additional statistical and linguistic features, utilizing AraELECTRA and a cross-lingual DeBERTa~V3 model.

**Key Contributions:**

	1. Feature-augmented transformer architecture
	2. Successful application in monolingual and zero-shot settings across multiple languages
	3. Importance of TF-IDF features and gating mechanism in improving model performance

**Result:** The system showed competitive performance, ranking 1st in English and Romanian in zero-shot settings, and performing well in monolingual tasks for several languages.

**Limitations:** 

**Conclusion:** The findings underscore the effectiveness of incorporating TF-IDF features and cross-lingual transfer in subjectivity detection tasks.

**Abstract:** This paper presents our approach to the CheckThat! 2025 Task 1 on subjectivity detection, where systems are challenged to distinguish whether a sentence from a news article expresses the subjective view of the author or presents an objective view on the covered topic. We propose a feature-augmented transformer architecture that combines contextual embeddings from pre-trained language models with statistical and linguistic features. Our system leveraged pre-trained transformers with additional lexical features: for Arabic we used AraELECTRA augmented with part-of-speech (POS) tags and TF-IDF features, while for the other languages we fine-tuned a cross-lingual DeBERTa~V3 model combined with TF-IDF features through a gating mechanism. We evaluated our system in monolingual, multilingual, and zero-shot settings across multiple languages including English, Arabic, German, Italian, and several unseen languages. The results demonstrate the effectiveness of our approach, achieving competitive performance across different languages with notable success in the monolingual setting for English (rank 1st with macro-F1=0.8052), German (rank 3rd with macro-F1=0.8013), Arabic (rank 4th with macro-F1=0.5771), and Romanian (rank 1st with macro-F1=0.8126) in the zero-shot setting. We also conducted an ablation analysis that demonstrated the importance of combining TF-IDF features with the gating mechanism and the cross-lingual transfer for subjectivity detection. Furthermore, our analysis reveals the model's sensitivity to both the order of cross-lingual fine-tuning and the linguistic proximity of the training languages.

</details>


### [38] [Rewrite-to-Rank: Optimizing Ad Visibility via Retrieval-Aware Text Rewriting](https://arxiv.org/abs/2507.21099)

*Chloe Ho, Ishneet Sukhvinder Singh, Diya Sharma, Tanvi Reddy Anumandla, Michael Lu, Vasu Sharma, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** LLM, advertisement rewriting, retrieval systems, fine-tuning, reinforcement learning

**Relevance Score:** 6

**TL;DR:** Explores LLM-based advertisement rewriting to improve visibility in retrieval systems using a supervised fine-tuning framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understand the impact of ad content phrasing on their visibility without changing retrieval models.

**Method:** Implemented a supervised fine-tuning framework focusing on semantic relevance and content fidelity; evaluated using DeltaMRR@K and DeltaDIR@K metrics.

**Key Contributions:**

	1. Proposed a supervised fine-tuning framework for ad rewriting
	2. Introduced DeltaMRR@K and DeltaDIR@K as evaluation metrics
	3. Demonstrated PPO models outperforming traditional methods in ad visibility enhancement

**Result:** PPO trained models showed superior performance over prompt engineering and supervised fine-tuning, achieving improved metrics for ad visibility.

**Limitations:** 

**Conclusion:** Ad rewriting significantly influences retrieval and prompt effectiveness; reinforcement learning methods can optimize ad phrasing.

**Abstract:** Search algorithms and user query relevance have given LLMs the ability to return relevant information, but the effect of content phrasing on ad visibility remains underexplored. We investigate how LLM-based rewriting of advertisements can improve their ranking in retrieval systems and inclusion in generated LLM responses, without modifying the retrieval model itself. We introduce a supervised fine-tuning framework with a custom loss balancing semantic relevance and content fidelity. To evaluate effectiveness, we propose two metrics: DeltaMRR@K (ranking improvement) and DeltaDIR@K (inclusion frequency improvement). Our approach presents a scalable method to optimize ad phrasing, enhancing visibility in retrieval-based LLM workflows. Experiments across both instruction-based and few-shot prompting demonstrate that PPO trained models outperform both prompt engineering and supervised fine-tuning in most cases, achieving up to a 2.79 DeltaDIR@5 and 0.0073 DeltaMRR@5 in instruction-based prompting. These results highlight the importance of how the ad is written before retrieval and prompt format and reinforcement learning in effective ad rewriting for LLM integrated retrieval systems.

</details>


### [39] [iLSU-T: an Open Dataset for Uruguayan Sign Language Translation](https://arxiv.org/abs/2507.21104)

*Ariel E. Stassi, Yanina Boria, J. Matías Di Martino, Gregory Randall*

**Main category:** cs.CL

**Keywords:** sign language, machine translation, dataset, accessibility, computer vision

**Relevance Score:** 4

**TL;DR:** This paper presents iLSU T, an open dataset of Uruguayan Sign Language videos accompanied by audio and text transcriptions, aimed at improving sign language translation and processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for localized datasets for sign language translation to enhance accessibility and inclusion.

**Method:** The dataset comprises over 185 hours of interpreted sign language videos from public TV broadcasting, featuring 18 professional interpreters and covering diverse topics. Experiments were conducted using three state-of-the-art translation algorithms to establish a baseline for the dataset.

**Key Contributions:**

	1. Introduction of the iLSU T dataset of Uruguayan Sign Language
	2. Experiments demonstrating the dataset's effectiveness
	3. Highlighting the importance of localized datasets for sign language accessibility

**Result:** The experiments demonstrated the dataset's usefulness and underscored the importance of localized data in advancing sign language processing techniques.

**Limitations:** 

**Conclusion:** The iLSU T dataset can significantly aid in developing new tools for sign language processing and underscores the necessity for more localized datasets.

**Abstract:** Automatic sign language translation has gained particular interest in the computer vision and computational linguistics communities in recent years. Given each sign language country particularities, machine translation requires local data to develop new techniques and adapt existing ones. This work presents iLSU T, an open dataset of interpreted Uruguayan Sign Language RGB videos with audio and text transcriptions. This type of multimodal and curated data is paramount for developing novel approaches to understand or generate tools for sign language processing. iLSU T comprises more than 185 hours of interpreted sign language videos from public TV broadcasting. It covers diverse topics and includes the participation of 18 professional interpreters of sign language. A series of experiments using three state of the art translation algorithms is presented. The aim is to establish a baseline for this dataset and evaluate its usefulness and the proposed pipeline for data processing. The experiments highlight the need for more localized datasets for sign language translation and understanding, which are critical for developing novel tools to improve accessibility and inclusion of all individuals. Our data and code can be accessed.

</details>


### [40] [Creation of a Numerical Scoring System to Objectively Measure and Compare the Level of Rhetoric in Arabic Texts: A Feasibility Study, and A Working Prototype](https://arxiv.org/abs/2507.21106)

*Mandar Marathe*

**Main category:** cs.CL

**Keywords:** Arabic Rhetoric, Literary Devices, Text Analysis, Computational Linguistics, Language Processing

**Relevance Score:** 0

**TL;DR:** The study develops a method to measure the density of Arabic rhetorical devices in texts to assess their impact and use across genres.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable objective evaluation of Arabic rhetoric usage in texts and compare its density across different genres and authors.

**Method:** A system was constructed to identify and calculate the density of 84 common literary devices based on morpheme counts, supported by electronic tools including a website and online calculator.

**Key Contributions:**

	1. Development of a measurement system for Arabic rhetorical devices.
	2. Creation of electronic support tools including an online calculator.
	3. Establishment of a technique for reporting rhetorical device distribution.

**Result:** The project produced a tool that accurately reports the density of rhetorical devices in any Arabic text or speech.

**Limitations:** 

**Conclusion:** This tool provides a new way to assess Arabic rhetorical richness and aids further linguistic analysis.

**Abstract:** Arabic Rhetoric is the field of Arabic linguistics which governs the art and science of conveying a message with greater beauty, impact and persuasiveness. The field is as ancient as the Arabic language itself and is found extensively in classical and contemporary Arabic poetry, free verse and prose. In practical terms, it is the intelligent use of word order, figurative speech and linguistic embellishments to enhance message delivery. Despite the volumes that have been written about it and the high status accorded to it, there is no way to objectively know whether a speaker or writer has used Arabic rhetoric in a given text, to what extent, and why. There is no objective way to compare the use of Arabic rhetoric across genres, authors or epochs. It is impossible to know which of pre-Islamic poetry, Andalucian Arabic poetry, or modern literary genres are richer in Arabic rhetoric. The aim of the current study was to devise a way to measure the density of the literary devices which constitute Arabic rhetoric in a given text, as a proxy marker for Arabic rhetoric itself. A comprehensive list of 84 of the commonest literary devices and their definitions was compiled. A system of identifying literary devices in texts was constructed. A method of calculating the density of literary devices based on the morpheme count of the text was utilised. Four electronic tools and an analogue tool were created to support the calculation of an Arabic text's rhetorical literary device density, including a website and online calculator. Additionally, a technique of reporting the distribution of literary devices used across the three sub-domains of Arabic rhetoric was created. The output of this project is a working tool which can accurately report the density of Arabic rhetoric in any Arabic text or speech.

</details>


### [41] [Curved Inference: Concern-Sensitive Geometry in Large Language Model Residual Streams](https://arxiv.org/abs/2507.21107)

*Rob Manson*

**Main category:** cs.CL

**Keywords:** large language models, geometric interpretability, semantic abstraction

**Relevance Score:** 8

**TL;DR:** This paper introduces Curved Inference, a geometric Interpretability framework for analyzing large language models' responses to semantic shifts across various domains.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how large language models track changes in semantic concerns and how this affects their internal trajectories.

**Method:** The study analyzes two models, Gemma3-1b and LLaMA3.2-3b, using geometric metrics related to curvature and salience on 20 matched prompts across different semantic domains.

**Key Contributions:**

	1. Introduction of the Curved Inference framework
	2. Geometric analysis of LLM trajectories in response to semantic prompts
	3. Insights into model alignment and emergent inference dynamics

**Result:** LLaMA3.2-3b shows significant scaling in curvature and salience with increased concern intensity, while Gemma3-1b exhibits weaker differentiation in responses.

**Limitations:** 

**Conclusion:** Curved Inference provides a new approach to assess model alignment and abstraction, revealing the dynamics of inference through prompts.

**Abstract:** We propose Curved Inference - a geometric Interpretability framework that tracks how the residual stream trajectory of a large language model bends in response to shifts in semantic concern. Across 20 matched prompts spanning emotional, moral, perspective, logical, identity, environmental, and nonsense domains, we analyse Gemma3-1b and LLaMA3.2-3b using five native-space metrics, with a primary focus on curvature (\k{appa}_i) and salience (S(t)). These metrics are computed under a pullback semantic metric derived from the unembedding matrix, ensuring that all measurements reflect token-aligned geometry rather than raw coordinate structure. We find that concern-shifted prompts reliably alter internal activation trajectories in both models - with LLaMA exhibiting consistent, statistically significant scaling in both curvature and salience as concern intensity increases. Gemma also responds to concern but shows weaker differentiation between moderate and strong variants. Our results support a two-layer view of LLM geometry - a latent conceptual structure encoded in the embedding space, and a contextual trajectory shaped by prompt-specific inference. Curved Inference reveals how models navigate, reorient, or reinforce semantic meaning over depth, offering a principled method for diagnosing alignment, abstraction, and emergent inference dynamics. These findings offer fresh insight into semantic abstraction and model alignment through the lens of Curved Inference.

</details>


### [42] [A Survey of Classification Tasks and Approaches for Legal Contracts](https://arxiv.org/abs/2507.21108)

*Amrita Singh, Aditya Joshi, Jiaojiao Jiang, Hye-young Paik*

**Main category:** cs.CL

**Keywords:** Legal Contract Classification, Natural Language Processing, Machine Learning, Deep Learning, Legal Informatics

**Relevance Score:** 5

**TL;DR:** This survey provides a comprehensive overview of Automatic Legal Contract Classification (LCC), including challenges, methodologies, and future research directions to enhance efficiency and accessibility in legal contract analysis.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** Manual reviews of legal contracts are inefficient and error-prone, creating a need for automation through Automatic Legal Contract Classification.

**Method:** The survey examines seven classification tasks, fourteen datasets, and introduces a methodology taxonomy for LCC, including Traditional Machine Learning, Deep Learning, and Transformer-based approaches.

**Key Contributions:**

	1. Comprehensive overview of LCC challenges and methodologies
	2. Detailed examination of datasets and evaluation techniques
	3. Suggestions for future research directions in legal NLP

**Result:** The survey highlights key tasks, datasets, and evaluation techniques in LCC, presenting the best-performing results and identifying limitations of current methods.

**Limitations:** None explicitly stated in the abstract, but acknowledges limitations of current methodologies.

**Conclusion:** The findings advocate for future research to enhance the efficiency, accuracy, and scalability of LCC, aiming to improve legal processes and access to legal information.

**Abstract:** Given the large size and volumes of contracts and their underlying inherent complexity, manual reviews become inefficient and prone to errors, creating a clear need for automation. Automatic Legal Contract Classification (LCC) revolutionizes the way legal contracts are analyzed, offering substantial improvements in speed, accuracy, and accessibility. This survey delves into the challenges of automatic LCC and a detailed examination of key tasks, datasets, and methodologies. We identify seven classification tasks within LCC, and review fourteen datasets related to English-language contracts, including public, proprietary, and non-public sources. We also introduce a methodology taxonomy for LCC, categorized into Traditional Machine Learning, Deep Learning, and Transformer-based approaches. Additionally, the survey discusses evaluation techniques and highlights the best-performing results from the reviewed studies. By providing a thorough overview of current methods and their limitations, this survey suggests future research directions to improve the efficiency, accuracy, and scalability of LCC. As the first comprehensive survey on LCC, it aims to support legal NLP researchers and practitioners in improving legal processes, making legal information more accessible, and promoting a more informed and equitable society.

</details>


### [43] [SemRAG: Semantic Knowledge-Augmented RAG for Improved Question-Answering](https://arxiv.org/abs/2507.21110)

*Kezhen Zhong, Basem Suleiman, Abdelkarim Erradi, Shijing Chen*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, semantic chunking, knowledge graphs

**Relevance Score:** 10

**TL;DR:** SemRAG is an enhanced Retrieval Augmented Generation framework that uses semantic chunking and knowledge graphs to improve performance in domain-specific tasks without extensive fine-tuning.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for integrating domain-specific knowledge in large language models efficiently, while overcoming the limitations of existing adaptations that are computationally expensive and prone to overfitting.

**Method:** SemRAG employs a semantic chunking algorithm that segments documents based on cosine similarity from sentence embeddings, and structures retrieved information into knowledge graphs to enhance retrieval accuracy and maintain semantic coherence.

**Key Contributions:**

	1. Introduction of the semantic chunking algorithm for efficient document segmentation.
	2. Utilization of knowledge graphs to improve contextual understanding and entity relationships.
	3. Demonstration of significant performance improvements over traditional RAG methods.

**Result:** SemRAG outperforms traditional RAG methods, significantly enhancing the relevance and correctness of retrieved information in tests conducted on MultiHop RAG and Wikipedia datasets.

**Limitations:** 

**Conclusion:** SemRAG provides a practical and scalable solution for creating efficient domain-specific LLM pipelines without the resource-intensive process of fine-tuning, aligning with sustainability goals.

**Abstract:** This paper introduces SemRAG, an enhanced Retrieval Augmented Generation (RAG) framework that efficiently integrates domain-specific knowledge using semantic chunking and knowledge graphs without extensive fine-tuning. Integrating domain-specific knowledge into large language models (LLMs) is crucial for improving their performance in specialized tasks. Yet, existing adaptations are computationally expensive, prone to overfitting and limit scalability. To address these challenges, SemRAG employs a semantic chunking algorithm that segments documents based on the cosine similarity from sentence embeddings, preserving semantic coherence while reducing computational overhead. Additionally, by structuring retrieved information into knowledge graphs, SemRAG captures relationships between entities, improving retrieval accuracy and contextual understanding. Experimental results on MultiHop RAG and Wikipedia datasets demonstrate SemRAG has significantly enhances the relevance and correctness of retrieved information from the Knowledge Graph, outperforming traditional RAG methods. Furthermore, we investigate the optimization of buffer sizes for different data corpus, as optimizing buffer sizes tailored to specific datasets can further improve retrieval performance, as integration of knowledge graphs strengthens entity relationships for better contextual comprehension. The primary advantage of SemRAG is its ability to create an efficient, accurate domain-specific LLM pipeline while avoiding resource-intensive fine-tuning. This makes it a practical and scalable approach aligned with sustainability goals, offering a viable solution for AI applications in domain-specific fields.

</details>


### [44] [InsurTech innovation using natural language processing](https://arxiv.org/abs/2507.21112)

*Panyi Dong, Zhiyu Quan*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Insurance, InsurTech, Data Analytics, Risk Assessment

**Relevance Score:** 6

**TL;DR:** The paper explores the integration of natural language processing (NLP) in insurance operations, showcasing case studies that illustrate its application in transforming unstructured text into actionable data for actuarial analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of InsurTech is prompting traditional insurance companies to use alternative data sources and advanced technologies to maintain competitiveness.

**Method:** The paper presents a conceptual overview and practical case studies of NLP, applying various techniques to real-world alternative data from an InsurTech partner to enhance traditional insurance operations.

**Key Contributions:**

	1. Demonstration of NLP's application in a real-world InsurTech context.
	2. Introduction of new industry classifications for risk assessment.
	3. Refinement of traditional insurance pricing strategies through enriched text-derived insights.

**Result:** NLP techniques were applied to transform unstructured text into structured data, enriching traditional rating factors for insurance pricing and offering novel perspectives for risk assessment.

**Limitations:** 

**Conclusion:** NLP is essential for modern insurance analytics, acting as a foundational element rather than just a supplementary tool.

**Abstract:** With the rapid rise of InsurTech, traditional insurance companies are increasingly exploring alternative data sources and advanced technologies to sustain their competitive edge. This paper provides both a conceptual overview and practical case studies of natural language processing (NLP) and its emerging applications within insurance operations with a focus on transforming raw, unstructured text into structured data suitable for actuarial analysis and decision-making. Leveraging real-world alternative data provided by an InsurTech industry partner that enriches traditional insurance data sources, we apply various NLP techniques to demonstrate practical use cases in the commercial insurance context. These enriched, text-derived insights not only add to and refine traditional rating factors for commercial insurance pricing but also offer novel perspectives for assessing underlying risk by introducing novel industry classifications. Through these demonstrations, we show that NLP is not merely a supplementary tool but a foundational element for modern, data-driven insurance analytics.

</details>


### [45] [TRIDENT: Benchmarking LLM Safety in Finance, Medicine, and Law](https://arxiv.org/abs/2507.21134)

*Zheng Hui, Yijiang River Dong, Ehsan Shareghi, Nigel Collier*

**Main category:** cs.CL

**Keywords:** large language models, safety, benchmark, high-risk domains, ethical compliance

**Relevance Score:** 9

**TL;DR:** The paper introduces Trident-Bench, a benchmark for evaluating large language model (LLM) safety in high-risk domains like law, finance, and medicine, revealing key safety gaps in generalist and domain-specialized models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing deployment of LLMs in high-risk domains necessitates systematic evaluations of their safety and compliance, which has been overlooked in prior work.

**Method:** The authors define domain-specific safety principles for LLMs inspired by existing ethical frameworks and create Trident-Bench to test these principles across various models.

**Key Contributions:**

	1. Introduction of Trident-Bench for evaluating LLM safety in high-risk domains.
	2. Establishment of domain-specific safety principles for LLM assessments.
	3. Empirical evaluation showing gaps in safety performance between generalist and domain-specialized models.

**Result:** Trident-Bench was used to evaluate 19 models, demonstrating that while generalist models can meet basic safety expectations, domain-specialized models often fail to address subtle ethical issues effectively.

**Limitations:** The evaluation focuses on only 19 models, and further research is needed to comprehensively address all safety nuances.

**Conclusion:** Trident-Bench serves as a foundational resource for assessing LLM safety in regulated fields, urging the need for targeted improvements to address safety concerns in these critical domains.

**Abstract:** As large language models (LLMs) are increasingly deployed in high-risk domains such as law, finance, and medicine, systematically evaluating their domain-specific safety and compliance becomes critical. While prior work has largely focused on improving LLM performance in these domains, it has often neglected the evaluation of domain-specific safety risks. To bridge this gap, we first define domain-specific safety principles for LLMs based on the AMA Principles of Medical Ethics, the ABA Model Rules of Professional Conduct, and the CFA Institute Code of Ethics. Building on this foundation, we introduce Trident-Bench, a benchmark specifically targeting LLM safety in the legal, financial, and medical domains. We evaluated 19 general-purpose and domain-specialized models on Trident-Bench and show that it effectively reveals key safety gaps -- strong generalist models (e.g., GPT, Gemini) can meet basic expectations, whereas domain-specialized models often struggle with subtle ethical nuances. This highlights an urgent need for finer-grained domain-specific safety improvements. By introducing Trident-Bench, our work provides one of the first systematic resources for studying LLM safety in law and finance, and lays the groundwork for future research aimed at reducing the safety risks of deploying LLMs in professionally regulated fields. Code and benchmark will be released at: https://github.com/zackhuiiiii/TRIDENT

</details>


### [46] [TTS-1 Technical Report](https://arxiv.org/abs/2507.21138)

*Oleg Atamanenko, Anna Chalova, Joseph Coombes, Nikki Cope, Phillip Dang, Zhifeng Deng, Jimmy Du, Michael Ermolenko, Feifan Fan, Yufei Feng, Cheryl Fichter, Pavel Filimonov, Louis Fischer, Kylan Gibbs, Valeria Gusarova, Pavel Karpik, Andreas Assad Kottner, Ian Lee, Oliver Louie, Jasmine Mai, Mikhail Mamontov, Suri Mao, Nurullah Morshed, Igor Poletaev, Florin Radu, Dmytro Semernia, Evgenii Shingarev, Vikram Sivaraja, Peter Skirko, Rinat Takhautdinov, Robert Villahermosa, Jean Wang*

**Main category:** cs.CL

**Keywords:** text-to-speech, Transformer, speech synthesis, deep learning, emotional control

**Relevance Score:** 5

**TL;DR:** Introduction of Inworld TTS-1, a set of Transformer-based text-to-speech models designed for high-quality speech synthesis and real-time applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop state-of-the-art text-to-speech models that can deliver exceptional quality and expressiveness for demanding applications and real-time use cases.

**Method:** The models utilize a sequential process of pre-training, fine-tuning, and RL-alignment to optimize the SpeechLM component, scaling train-time compute for maximum performance.

**Key Contributions:**

	1. Development of TTS-1-Max (8.8B parameters) for high-quality speech synthesis.
	2. Creation of TTS-1 (1.6B parameters) optimized for real-time applications.
	3. Open-sourcing training and modeling code to promote further research.

**Result:** Both TTS-1 and TTS-1-Max achieve state-of-the-art performance on benchmarks, generating high-resolution speech at 48 kHz with low latency in 11 languages, allowing for emotional control and non-verbal vocalizations.

**Limitations:** 

**Conclusion:** The introduction of these models marks a significant advancement in text-to-speech technology, offering powerful tools for developers and researchers in the field.

**Abstract:** We introduce Inworld TTS-1, a set of two Transformer-based autoregressive text-to-speech (TTS) models. Our largest model, TTS-1-Max, has 8.8B parameters and is designed for utmost quality and expressiveness in demanding applications. TTS-1 is our most efficient model, with 1.6B parameters, built for real-time speech synthesis and on-device use cases. By scaling train-time compute and applying a sequential process of pre-training, fine-tuning, and RL-alignment of the speech-language model (SpeechLM) component, both models achieve state-of-the-art performance on a variety of benchmarks, demonstrating exceptional quality relying purely on in-context learning of the speaker's voice. Inworld TTS-1 and TTS-1-Max can generate high-resolution 48 kHz speech with low latency, and support 11 languages with fine-grained emotional control and non-verbal vocalizations through audio markups. We additionally open-source our training and modeling code under an MIT license.

</details>


### [47] [Diverse LLMs or Diverse Question Interpretations? That is the Ensembling Question](https://arxiv.org/abs/2507.21168)

*Rafael Rosales, Santiago Miret*

**Main category:** cs.CL

**Keywords:** machine learning, large language models, diversity approaches, ensemble accuracy, binary questions

**Relevance Score:** 8

**TL;DR:** This paper evaluates two diversity approaches in using LLMs for answering binary questions and finds that question interpretation diversity yields better performance than model diversity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance performance in machine learning models, particularly LLMs, by exploring effective implementations of diversity in answering binary questions.

**Method:** The paper compares two approaches: model diversity (using multiple models) and question interpretation diversity (using variations of the same question with a single model), employing majority voting for consensus on answers.

**Key Contributions:**

	1. Comparative analysis of two diversity techniques in LLMs
	2. Experimental results showing the superiority of question interpretation diversity
	3. Insights into performance dynamics of model diversity

**Result:** Experiments on datasets like boolq, strategyqa, and pubmedqa reveal that question interpretation diversity improves ensemble accuracy consistently, while model diversity does not significantly enhance performance.

**Limitations:** The study does not explore diversity techniques beyond the comparison made, which may limit broader applicability.

**Conclusion:** Adopting question interpretation diversity is more effective for LLMs in answering binary questions, as opposed to relying solely on different models.

**Abstract:** Effectively leveraging diversity has been shown to improve performance for various machine learning models, including large language models (LLMs). However, determining the most effective way of using diversity remains a challenge. In this work, we compare two diversity approaches for answering binary questions using LLMs: model diversity, which relies on multiple models answering the same question, and question interpretation diversity, which relies on using the same model to answer the same question framed in different ways. For both cases, we apply majority voting as the ensemble consensus heuristic to determine the final answer. Our experiments on boolq, strategyqa, and pubmedqa show that question interpretation diversity consistently leads to better ensemble accuracy compared to model diversity. Furthermore, our analysis of GPT and LLaMa shows that model diversity typically produces results between the best and the worst ensemble members without clear improvement.

</details>


### [48] [Contrast-CAT: Contrasting Activations for Enhanced Interpretability in Transformer-based Text Classifiers](https://arxiv.org/abs/2507.21186)

*Sungmin Han, Jeonghyun Lee, Sangkyun Lee*

**Main category:** cs.CL

**Keywords:** transformers, explainability, attribution methods, human-computer interaction, machine learning

**Relevance Score:** 8

**TL;DR:** Contrast-CAT is a novel method designed to enhance the interpretability of transformer-based text classification by filtering out class-irrelevant features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Explaining the decisions of transformer models is crucial for trust and safe deployment, yet existing attribution methods face challenges due to class-irrelevant features.

**Method:** Contrast-CAT improves token-level attributions by contrasting the activations of input sequences with reference activations to eliminate irrelevant features.

**Key Contributions:**

	1. Introduction of Contrast-CAT, a new attribution method for transformers.
	2. Improved filtering of class-irrelevant features in attributions.
	3. Demonstrated superior performance over existing methods in various datasets.

**Result:** Experimental results show that Contrast-CAT outperforms state-of-the-art attribution methods, with notable improvements in interpretability metrics like AOPC and LOdds.

**Limitations:** 

**Conclusion:** Contrast-CAT effectively enhances the reliability of interpretations in transformer-based text classification, making it a significant advancement in the field.

**Abstract:** Transformers have profoundly influenced AI research, but explaining their decisions remains challenging -- even for relatively simpler tasks such as classification -- which hinders trust and safe deployment in real-world applications. Although activation-based attribution methods effectively explain transformer-based text classification models, our findings reveal that these methods can be undermined by class-irrelevant features within activations, leading to less reliable interpretations. To address this limitation, we propose Contrast-CAT, a novel activation contrast-based attribution method that refines token-level attributions by filtering out class-irrelevant features. By contrasting the activations of an input sequence with reference activations, Contrast-CAT generates clearer and more faithful attribution maps. Experimental results across various datasets and models confirm that Contrast-CAT consistently outperforms state-of-the-art methods. Notably, under the MoRF setting, it achieves average improvements of x1.30 in AOPC and x2.25 in LOdds over the most competing methods, demonstrating its effectiveness in enhancing interpretability for transformer-based text classification.

</details>


### [49] [Understanding Public Perception of Crime in Bangladesh: A Transformer-Based Approach with Explainability](https://arxiv.org/abs/2507.21234)

*Fatema Binte Hassan, Md Al Jubair, Mohammad Mehadi Hasan, Tahmid Hossain, S M Mehebubur Rahman Khan Shuvo, Mohammad Shamsul Arefin*

**Main category:** cs.CL

**Keywords:** Sentiment Analysis, Social Media, Transformer Models, XLM-RoBERTa, Public Policy

**Relevance Score:** 4

**TL;DR:** The paper investigates public sentiment on crime-related news through sentiment analysis of Bangla social media comments using a transformer-based model, achieving high accuracy and enhancing model interpretability.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the dynamics of public perception regarding crime news as expressed on social media.

**Method:** A transformer-based model using XLM-RoBERTa Base architecture was developed and tested on a dataset of 28,528 Bangla-language comments, classifying sentiments as positive, negative, or neutral.

**Key Contributions:**

	1. Development of a new dataset of Bangla comments for sentiment analysis
	2. Introduction of a transformer-based model that outperforms existing methods
	3. Application of explainable AI for improved model interpretability

**Result:** The proposed model achieved a classification accuracy of 97%, surpassing existing state-of-the-art methods in Bangla sentiment analysis, and utilized explainable AI techniques for interpretability.

**Limitations:** 

**Conclusion:** Transformer-based models prove effective in low-resource languages like Bengali, providing valuable insights that can influence public policy and crime prevention strategies.

**Abstract:** In recent years, social media platforms have become prominent spaces for individuals to express their opinions on ongoing events, including criminal incidents. As a result, public sentiment can shift dynamically over time. This study investigates the evolving public perception of crime-related news by classifying user-generated comments into three categories: positive, negative, and neutral. A newly curated dataset comprising 28,528 Bangla-language social media comments was developed for this purpose. We propose a transformer-based model utilizing the XLM-RoBERTa Base architecture, which achieves a classification accuracy of 97%, outperforming existing state-of-the-art methods in Bangla sentiment analysis. To enhance model interpretability, explainable AI technique is employed to identify the most influential features driving sentiment classification. The results underscore the effectiveness of transformer-based models in processing low-resource languages such as Bengali and demonstrate their potential to extract actionable insights that can support public policy formulation and crime prevention strategies.

</details>


### [50] [Bangla BERT for Hyperpartisan News Detection: A Semi-Supervised and Explainable AI Approach](https://arxiv.org/abs/2507.21242)

*Mohammad Mehadi Hasan, Fatema Binte Hassan, Md Al Jubair, Zobayer Ahmed, Sazzatul Yeakin, Md Masum Billah*

**Main category:** cs.CL

**Keywords:** Bangla BERT, hyperpartisan news, natural language processing, semi-supervised learning, LIME

**Relevance Score:** 6

**TL;DR:** This paper presents a fine-tuned Bangla BERT model to classify hyperpartisan news in Bangla, achieving a significant accuracy of 95.65%, surpassing traditional machine learning methods, with interpretable predictions using LIME.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rapid spread of misinformation in the Bangla language due to a lack of sophisticated NLP methods for detecting hyperpartisan news.

**Method:** Fine-tuning Bangla BERT, a transformer-based model, and comparing its performance against traditional machine learning models while implementing semi-supervised learning and LIME for interpretability.

**Key Contributions:**

	1. Fine-tuning of Bangla BERT for hyperpartisan news classification
	2. Implementation of semi-supervised learning techniques
	3. Use of LIME for interpretable model predictions

**Result:** The Bangla BERT model achieved a remarkable accuracy of 95.65%, outperforming conventional approaches to hyperpartisan news classification.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of transformer models in low-resource settings, suggesting possibilities for further advancements in NLP for Bangla.

**Abstract:** In the current digital landscape, misinformation circulates rapidly, shaping public perception and causing societal divisions. It is difficult to identify hyperpartisan news in Bangla since there aren't many sophisticated natural language processing methods available for this low-resource language. Without effective detection methods, biased content can spread unchecked, posing serious risks to informed discourse. To address this gap, our research fine-tunes Bangla BERT. This is a state-of-the-art transformer-based model, designed to enhance classification accuracy for hyperpartisan news. We evaluate its performance against traditional machine learning models and implement semi-supervised learning to enhance predictions further. Not only that, we use LIME to provide transparent explanations of the model's decision-making process, which helps to build trust in its outcomes. With a remarkable accuracy score of 95.65%, Bangla BERT outperforms conventional approaches, according to our trial data. The findings of this study demonstrate the usefulness of transformer models even in environments with limited resources, which opens the door to further improvements in this area.

</details>


### [51] [Can human clinical rationales improve the performance and explainability of clinical text classification models?](https://arxiv.org/abs/2507.21302)

*Christoph Metzner, Shang Gao, Drahomira Herrmannova, Heidi A. Hanson*

**Main category:** cs.CL

**Keywords:** clinical rationales, transformer models, explainability, cancer diagnosis, health informatics

**Relevance Score:** 8

**TL;DR:** This study examines the use of human-based clinical rationales to enhance the performance and explainability of transformer-based models in classifying clinical text, particularly for cancer diagnoses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance and explainability of AI-driven clinical text classification by using human-based clinical rationales as additional supervision in transformer models.

**Method:** Analyzed 99,125 human-based clinical rationales along with 128,649 electronic pathology reports to evaluate the performance of transformer-based models in extracting primary cancer site diagnoses.

**Key Contributions:**

	1. Investigation of the impact of human-based rationales on AI model performance and explainability in healthcare settings.
	2. Analysis of rationale quality using sufficiency as a metric.
	3. Comparative performance analysis between rationale-based training and standard report-based training.

**Result:** Clinical rationales improved model performance in high-resource scenarios but showed inconsistent performance in low-resource settings. Models trained on rationales performed worse than those trained solely on more reports.

**Limitations:** Clinical rationales did not consistently improve performance and were outperformed by models trained on additional reports. Also, the sufficiency metric led to inconsistent results.

**Conclusion:** The use of clinical rationales for training does not significantly enhance model performance compared to using more reports, though they may improve explainability.

**Abstract:** AI-driven clinical text classification is vital for explainable automated retrieval of population-level health information. This work investigates whether human-based clinical rationales can serve as additional supervision to improve both performance and explainability of transformer-based models that automatically encode clinical documents. We analyzed 99,125 human-based clinical rationales that provide plausible explanations for primary cancer site diagnoses, using them as additional training samples alongside 128,649 electronic pathology reports to evaluate transformer-based models for extracting primary cancer sites. We also investigated sufficiency as a way to measure rationale quality for pre-selecting rationales. Our results showed that clinical rationales as additional training data can improve model performance in high-resource scenarios but produce inconsistent behavior when resources are limited. Using sufficiency as an automatic metric to preselect rationales also leads to inconsistent results. Importantly, models trained on rationales were consistently outperformed by models trained on additional reports instead. This suggests that clinical rationales don't consistently improve model performance and are outperformed by simply using more reports. Therefore, if the goal is optimizing accuracy, annotation efforts should focus on labeling more reports rather than creating rationales. However, if explainability is the priority, training models on rationale-supplemented data may help them better identify rationale-like features. We conclude that using clinical rationales as additional training data results in smaller performance improvements and only slightly better explainability (measured as average token-level rationale coverage) compared to training on additional reports.

</details>


### [52] [Do Large Language Models Understand Morality Across Cultures?](https://arxiv.org/abs/2507.21319)

*Hadi Mohammadi, Yasmeen F. S. S. Meijer, Efthymia Papadopoulou, Ayoub Bagheri*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cross-Cultural Differences, Moral Perspectives, Bias in AI, Ethical AI

**Relevance Score:** 9

**TL;DR:** This study explores how well large language models (LLMs) capture cross-cultural moral differences, finding significant biases and misalignment with survey data on moral attitudes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate ethical concerns surrounding the biases present in LLMs and their alignment with cross-cultural moral perspectives.

**Method:** Employ three methods: comparing variances in moral scores, conducting cluster alignment analyses, and probing models with comparative prompts.

**Key Contributions:**

	1. Analysis of cross-cultural moral alignment in LLMs
	2. Identification of bias in LLM outputs
	3. Recommendations for improving ethical alignment in LLM development

**Result:** Current LLMs fail to capture the diversity of cross-cultural moral attitudes, compressing differences and aligning poorly with international survey data.

**Limitations:** Focused on limited moral dimensions and cultural contexts; results may not generalize to all cultural or ethical frameworks.

**Conclusion:** There's a pressing need to improve cultural representativeness in LLMs to ensure responsible and ethical development and deployment.

**Abstract:** Recent advancements in large language models (LLMs) have established them as powerful tools across numerous domains. However, persistent concerns about embedded biases, such as gender, racial, and cultural biases arising from their training data, raise significant questions about the ethical use and societal consequences of these technologies. This study investigates the extent to which LLMs capture cross-cultural differences and similarities in moral perspectives. Specifically, we examine whether LLM outputs align with patterns observed in international survey data on moral attitudes. To this end, we employ three complementary methods: (1) comparing variances in moral scores produced by models versus those reported in surveys, (2) conducting cluster alignment analyses to assess correspondence between country groupings derived from LLM outputs and survey data, and (3) directly probing models with comparative prompts using systematically chosen token pairs. Our results reveal that current LLMs often fail to reproduce the full spectrum of cross-cultural moral variation, tending to compress differences and exhibit low alignment with empirical survey patterns. These findings highlight a pressing need for more robust approaches to mitigate biases and improve cultural representativeness in LLMs. We conclude by discussing the implications for the responsible development and global deployment of LLMs, emphasizing fairness and ethical alignment.

</details>


### [53] [A Deep Learning Automatic Speech Recognition Model for Shona Language](https://arxiv.org/abs/2507.21331)

*Leslie Wellington Sirora, Mainford Mutandavari*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Deep Learning, Shona Language

**Relevance Score:** 4

**TL;DR:** A deep learning-based Automatic Speech Recognition system was developed for the Shona language, achieving significant improvements in recognition accuracy over traditional models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of developing an ASR system for Shona, a low-resource language with unique tonal and grammatical complexities, by improving recognition accuracy.

**Method:** The research utilized a hybrid architecture combining a Convolutional Neural Network for acoustic modeling and a Long Short-Term Memory network for language modeling, employing data augmentation and transfer learning techniques.

**Key Contributions:**

	1. Development of an ASR system tailored for Shona language
	2. Use of hybrid deep learning architecture with data augmentation techniques
	3. Achievement of benchmark recognition metrics for under-resourced languages

**Result:** The deep learning-based ASR system achieved a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%.

**Limitations:** 

**Conclusion:** The study demonstrated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona, aiding in better communication for Shona speakers worldwide.

**Abstract:** This study presented the development of a deep learning-based Automatic Speech Recognition system for Shona, a low-resource language characterized by unique tonal and grammatical complexities. The research aimed to address the challenges posed by limited training data, lack of labelled data, and the intricate tonal nuances present in Shona speech, with the objective of achieving significant improvements in recognition accuracy compared to traditional statistical models. The research first explored the feasibility of using deep learning to develop an accurate ASR system for Shona. Second, it investigated the specific challenges involved in designing and implementing deep learning architectures for Shona speech recognition and proposed strategies to mitigate these challenges. Lastly, it compared the performance of the deep learning-based model with existing statistical models in terms of accuracy. The developed ASR system utilized a hybrid architecture consisting of a Convolutional Neural Network for acoustic modelling and a Long Short-Term Memory network for language modelling. To overcome the scarcity of data, data augmentation techniques and transfer learning were employed. Attention mechanisms were also incorporated to accommodate the tonal nature of Shona speech. The resulting ASR system achieved impressive results, with a Word Error Rate of 29%, Phoneme Error Rate of 12%, and an overall accuracy of 74%. These metrics indicated the potential of deep learning to enhance ASR accuracy for under-resourced languages like Shona. This study contributed to the advancement of ASR technology for under-resourced languages like Shona, ultimately fostering improved accessibility and communication for Shona speakers worldwide.

</details>


### [54] [Dialogic Social Learning for Artificial Agents: Enhancing LLM Ontology Acquisition through Mixed-Initiative Educational Interactions](https://arxiv.org/abs/2507.21065)

*Sabrina Patania, Luca Annese, Cansu Koyuturk, Azzurra Ruggeri, Dimitri Ognibene*

**Main category:** cs.CL

**Keywords:** Large Language Models, sociocultural theory, pedagogical dialogues, knowledge acquisition, AI training

**Relevance Score:** 9

**TL;DR:** This study introduces the 'AI Social Gym', a dynamic environment for AI learners to engage in pedagogical dialogues with AI teachers, enhancing knowledge acquisition through dialogic interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional AI training paradigms that rely on large datasets and sparse feedback, by exploring socially mediated learning paradigms inspired by Vygotsky's sociocultural theory.

**Method:** The research involves the creation of the 'AI Social Gym' environment where an AI learner engages in dialogues with AI teacher agents, focusing on different pedagogical strategies and their impact on ontology acquisition.

**Key Contributions:**

	1. Development of the AI Social Gym for AI learning.
	2. Demonstration of the effectiveness of dialogic interactions for knowledge acquisition in LLMs.
	3. Integration of pedagogical strategies into AI training methodologies.

**Result:** Empirical results show that dialogic interactions, especially mixed-direction strategies, significantly improve the LLM's ability to acquire and apply new knowledge compared to traditional methods.

**Limitations:** 

**Conclusion:** Integrating pedagogical and psychological insights into AI training can enhance post-training knowledge acquisition and response quality, providing a new avenue to improve LLM performance.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.   We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.   Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.   These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering

</details>


### [55] [StructText: A Synthetic Table-to-Text Approach for Benchmark Generation with Multi-Dimensional Evaluation](https://arxiv.org/abs/2507.21340)

*Satyananda Kashyap, Sola Shirai, Nandana Mihindukulasooriya, Horst Samulowitz*

**Main category:** cs.CL

**Keywords:** structured information extraction, large language models, benchmark generation, natural language processing, enterprise use cases

**Relevance Score:** 8

**TL;DR:** This paper presents StructText, a framework designed to automatically generate high-quality benchmarks for extracting key-value pairs from text using existing tabular data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of benchmarks for evaluating extraction quality of LLMs in specific domains, facilitating the conversion of natural language into structured formats for enterprise use.

**Method:** StructText utilizes a two-stage 'plan-then-execute' pipeline that synthesizes natural language text from existing tabular data, and employs a multi-dimensional evaluation strategy that includes LLM-based judgments and objective extraction metrics.

**Key Contributions:**

	1. Introduction of StructText framework for benchmark generation
	2. Two-stage plan-then-execute pipeline for text synthesis
	3. Multi-dimensional evaluation combining LLM judgments and objective metrics

**Result:** Evaluated on 71,539 examples across 49 datasets, results indicate strong factual accuracy from LLMs, but challenges with narrative coherence affecting automated extraction.

**Limitations:** The framework may not cover all domain-specific variations in data extraction.

**Conclusion:** The framework includes released datasets, evaluation tools, and baseline extraction systems to stimulate further research in structured information extraction.

**Abstract:** Extracting structured information from text, such as key-value pairs that could augment tabular data, is quite useful in many enterprise use cases. Although large language models (LLMs) have enabled numerous automated pipelines for converting natural language into structured formats, there is still a lack of benchmarks for evaluating their extraction quality, especially in specific domains or focused documents specific to a given organization. Building such benchmarks by manual annotations is labour-intensive and limits the size and scalability of the benchmarks. In this work, we present StructText, an end-to-end framework for automatically generating high-fidelity benchmarks for key-value extraction from text using existing tabular data. It uses available tabular data as structured ground truth, and follows a two-stage ``plan-then-execute'' pipeline to synthetically generate corresponding natural-language text. To ensure alignment between text and structured source, we introduce a multi-dimensional evaluation strategy that combines (a) LLM-based judgments on factuality, hallucination, and coherence and (b) objective extraction metrics measuring numeric and temporal accuracy. We evaluated the proposed method on 71,539 examples across 49 datasets. Results reveal that while LLMs achieve strong factual accuracy and avoid hallucination, they struggle with narrative coherence in producing extractable text. Notably, models presume numerical and temporal information with high fidelity yet this information becomes embedded in narratives that resist automated extraction. We release a framework, including datasets, evaluation tools, and baseline extraction systems, to support continued research.

</details>


### [56] [Turbocharging Web Automation: The Impact of Compressed History States](https://arxiv.org/abs/2507.21369)

*Xiyue Zhu, Peng Tang, Haofu Liao, Srikar Appalaraju*

**Main category:** cs.CL

**Keywords:** web automation, history states, language models, accuracy improvement, dataset evaluation

**Relevance Score:** 6

**TL;DR:** This paper introduces a web history compressor that enhances web automation by effectively using historical states, resulting in improved accuracy in task execution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current web automation methods that do not effectively utilize historical states, particularly due to the verbose nature of web page states.

**Method:** The authors propose a novel history compressor module that distills task-relevant information from verbose history states into a fixed-length representation to improve automation accuracy.

**Key Contributions:**

	1. Introduction of a novel web history compressor for web automation.
	2. Demonstration of improved accuracy through experiments on Mind2Web and WebLINX datasets.
	3. Addressing the challenges of verbose web page states in automation tasks.

**Result:** The proposed method demonstrates 1.2-5.4% absolute accuracy improvements on the Mind2Web and WebLINX datasets compared to a baseline approach that lacks history inputs.

**Limitations:** 

**Conclusion:** The introduction of a history compressor significantly boosts the effectiveness of web automation by leveraging historical states more efficiently.

**Abstract:** Language models have led to a leap forward in web automation. The current web automation approaches take the current web state, history actions, and language instruction as inputs to predict the next action, overlooking the importance of history states. However, the highly verbose nature of web page states can result in long input sequences and sparse information, hampering the effective utilization of history states. In this paper, we propose a novel web history compressor approach to turbocharge web automation using history states. Our approach employs a history compressor module that distills the most task-relevant information from each history state into a fixed-length short representation, mitigating the challenges posed by the highly verbose history states. Experiments are conducted on the Mind2Web and WebLINX datasets to evaluate the effectiveness of our approach. Results show that our approach obtains 1.2-5.4% absolute accuracy improvements compared to the baseline approach without history inputs.

</details>


### [57] [MemTool: Optimizing Short-Term Memory Management for Dynamic Tool Calling in LLM Agent Multi-Turn Conversations](https://arxiv.org/abs/2507.21428)

*Elias Lumer, Anmol Gulati, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, James A. Burke*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-turn interactions, Memory management, Tool management, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** Introducing MemTool, a memory framework for LLM agents that enhances tool management across multi-turn conversations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of fixed context windows in LLM agents during multi-turn interactions by enabling dynamic management of tools and contexts.

**Method:** MemTool introduces three architectures: Autonomous Agent Mode, Workflow Mode, and Hybrid Mode, evaluated on 13+ LLMs using the ScaleMCP benchmark through 100 user interactions.

**Key Contributions:**

	1. Developed the MemTool framework for LLM memory management
	2. Evaluated three distinct interaction architectures for LLM agents
	3. Demonstrated the efficiency of tool management in multi-turn dialogues

**Result:** Autonomous Agent Mode achieved high tool removal efficiency (90-94%), while medium-sized models showed lower efficiency (0-60%). Workflow and Hybrid modes effectively managed tool removal, with varying success in task completion across modes.

**Limitations:** The study primarily focuses on the performance of various LLMs; potential limitations in generalizability across different contexts or applications are not explored.

**Conclusion:** MemTool provides important insights for optimizing LLM interaction modes based on efficiency and task completion, highlighting trade-offs between agency and effectiveness.

**Abstract:** Large Language Model (LLM) agents have shown significant autonomous capabilities in dynamically searching and incorporating relevant tools or Model Context Protocol (MCP) servers for individual queries. However, fixed context windows limit effectiveness in multi-turn interactions requiring repeated, independent tool usage. We introduce MemTool, a short-term memory framework enabling LLM agents to dynamically manage tools or MCP server contexts across multi-turn conversations. MemTool offers three agentic architectures: 1) Autonomous Agent Mode, granting full tool management autonomy, 2) Workflow Mode, providing deterministic control without autonomy, and 3) Hybrid Mode, combining autonomous and deterministic control. Evaluating each MemTool mode across 13+ LLMs on the ScaleMCP benchmark, we conducted experiments over 100 consecutive user interactions, measuring tool removal ratios (short-term memory efficiency) and task completion accuracy. In Autonomous Agent Mode, reasoning LLMs achieve high tool-removal efficiency (90-94% over a 3-window average), while medium-sized models exhibit significantly lower efficiency (0-60%). Workflow and Hybrid modes consistently manage tool removal effectively, whereas Autonomous and Hybrid modes excel at task completion. We present trade-offs and recommendations for each MemTool mode based on task accuracy, agency, and model capabilities.

</details>


### [58] [Product vs. Process: Exploring EFL Students' Editing of AI-Generated Text for Expository Writing](https://arxiv.org/abs/2507.21073)

*David James Woo, Yangyang Yu, Kai Guo, Yilin Huang, April Ka Yeng Fung*

**Main category:** cs.CL

**Keywords:** AI in education, EFL writing, editing behavior, composition quality, human rated scoring

**Relevance Score:** 5

**TL;DR:** This research examines how EFL secondary students edit AI-generated text in their writing, analyzing editing behaviors and their impact on composition quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To study the impact of AI-generated text on students' expository writing processes and compositions in EFL contexts, which is currently understudied.

**Method:** The study used a convergent design analyzing screen recordings and compositions of 39 Hong Kong secondary students, employing qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis.

**Key Contributions:**

	1. Identification of distinct editing patterns among EFL students using AI-generated text.
	2. Findings indicate a disconnect between editing efforts and improved writing quality.
	3. Highlights the need for genre-specific teaching and assessment methods in writing education.

**Result:** The analysis identified two editing patterns among students and revealed that while AI-generated words positively impacted scores, editing efforts had minimal overall impact on composition quality.

**Limitations:** Study limited to a specific population of Hong Kong secondary students and may not generalize to other contexts.

**Conclusion:** AI supports EFL writing but does not replace the need for foundational writing skills, emphasizing the necessity of process-focused instruction before integrating AI.

**Abstract:** Text generated by artificial intelligence (AI) chatbots is increasingly used in English as a foreign language (EFL) writing contexts, yet its impact on students' expository writing process and compositions remains understudied. This research examines how EFL secondary students edit AI-generated text. Exploring editing behaviors in their expository writing process and in expository compositions, and their effect on human-rated scores for content, organization, language, and overall quality. Participants were 39 Hong Kong secondary students who wrote an expository composition with AI chatbots in a workshop. A convergent design was employed to analyze their screen recordings and compositions to examine students' editing behaviors and writing qualities. Analytical methods included qualitative coding, descriptive statistics, temporal sequence analysis, human-rated scoring, and multiple linear regression analysis. We analyzed over 260 edits per dataset, and identified two editing patterns: one where students refined introductory units repeatedly before progressing, and another where they quickly shifted to extensive edits in body units (e.g., topic and supporting sentences). MLR analyses revealed that the number of AI-generated words positively predicted all score dimensions, while most editing variables showed minimal impact. These results suggest a disconnect between students' significant editing effort and improved composition quality, indicating AI supports but does not replace writing skills. The findings highlight the importance of genre-specific instruction and process-focused writing before AI integration. Educators should also develop assessments valuing both process and product to encourage critical engagement with AI text.

</details>


### [59] [Towards Locally Deployable Fine-Tuned Causal Large Language Models for Mode Choice Behaviour](https://arxiv.org/abs/2507.21432)

*Tareq Alsaleh, Bilal Farooq*

**Main category:** cs.CL

**Keywords:** Causal LLMs, Travel Mode Choice, Interpretability

**Relevance Score:** 7

**TL;DR:** This study presents LiTransMC, a fine-tuned causal LLM for predicting travel mode choices, outperforming traditional models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop locally deployable causal LLMs that improve travel mode choice prediction while ensuring interpretability and privacy.

**Method:** Benchmarked eleven LLMs across three datasets, generating synthetic predictions and evaluating with BERTopic and a novel Explanation Strength Index.

**Key Contributions:**

	1. Introduction of LiTransMC, a novel causal LLM for travel mode choice
	2. Systematic benchmarking of multiple LLMs demonstrating improved predictive performance
	3. Exploration of model reasoning and interpretability through structured analysis

**Result:** LiTransMC achieved a weighted F1 score of 0.6845, outperforming larger proprietary models and classical methods in predictive accuracy and distributional calibration.

**Limitations:** 

**Conclusion:** The study shows the potential of developing specialist LLMs that combine prediction and interpretability for transport research and policy.

**Abstract:** This study investigates the adoption of open-access, locally deployable causal large language models (LLMs) for travel mode choice prediction and introduces LiTransMC, the first fine-tuned causal LLM developed for this task. We systematically benchmark eleven LLMs (1-12B parameters) across three stated and revealed preference datasets, testing 396 configurations and generating over 79,000 synthetic commuter predictions. Beyond predictive accuracy, we evaluate models generated reasoning using BERTopic for topic modelling and a novel Explanation Strength Index, providing the first structured analysis of how LLMs articulate decision factors in alignment with behavioural theory. LiTransMC, fine-tuned using parameter efficient and loss masking strategy, achieved a weighted F1 score of 0.6845 and a Jensen-Shannon Divergence of 0.000245, surpassing both untuned local models and larger proprietary systems, including GPT-4o with advanced persona inference and embedding-based loading, while also outperforming classical mode choice methods such as discrete choice models and machine learning classifiers for the same dataset. This dual improvement, i.e., high instant-level accuracy and near-perfect distributional calibration, demonstrates the feasibility of creating specialist, locally deployable LLMs that integrate prediction and interpretability. Through combining structured behavioural prediction with natural language reasoning, this work unlocks the potential for conversational, multi-task transport models capable of supporting agent-based simulations, policy testing, and behavioural insight generation. These findings establish a pathway for transforming general purpose LLMs into specialized, explainable tools for transportation research and policy formulation, while maintaining privacy, reducing cost, and broadening access through local deployment.

</details>


### [60] [Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench](https://arxiv.org/abs/2507.21476)

*Reuben Narad, Siddharth Suresh, Jiayi Chen, Pine S. L. Dysart-Bricken, Bob Mankoff, Robert Nowak, Jifan Zhang, Lalit Jain*

**Main category:** cs.CL

**Keywords:** Humor, Large Language Models, Benchmarking, Reasoning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** HumorBench is a benchmark to assess LLMs' capabilities in understanding and explaining humor in cartoon captions, featuring 300 unique pairs and expert-annotated rubrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate LLMs in reasoning beyond STEM domains, particularly in humor comprehension.

**Method:** HumorBench utilizes approximately 300 cartoon-caption pairs with annotated evaluation rubrics to assess LLMs' explanations and identification of joke elements.

**Key Contributions:**

	1. Introduction of HumorBench for LLM evaluation in humor reasoning
	2. Findings on the transferability of STEM reasoning abilities to humor comprehension
	3. Insights on model performance with varying token budgets.

**Result:** Current state-of-the-art LLMs show effective transfer of STEM reasoning skills to humor comprehension, and models trained on STEM data perform well on HumorBench.

**Limitations:** 

**Conclusion:** Increased token budgets for reasoning yield variable performance across models in humor reasoning tasks.

**Abstract:** We present HumorBench, a benchmark designed to evaluate large language models' (LLMs) ability to reason about and explain sophisticated humor in cartoon captions. As reasoning models increasingly saturate existing benchmarks in mathematics and science, novel and challenging evaluations of model intelligence beyond STEM domains are essential. Reasoning is fundamentally involved in text-based humor comprehension, requiring the identification of connections between concepts in cartoons/captions and external cultural references, wordplays, and other mechanisms. HumorBench includes approximately 300 unique cartoon-caption pairs from the New Yorker Caption Contest and Cartoonstock.com, with expert-annotated evaluation rubrics identifying essential joke elements. LLMs are evaluated based on their explanations towards the humor and abilities in identifying the joke elements. To perform well on this task, models must form and test hypotheses about associations between concepts, potentially backtracking from initial interpretations to arrive at the most plausible explanation. Our extensive benchmarking of current SOTA models reveals three key insights: (1) LLM progress on STEM reasoning transfers effectively to humor comprehension; (2) models trained exclusively on STEM reasoning data still perform well on HumorBench, demonstrating strong transferability of reasoning abilities; and (3) test-time scaling by increasing thinking token budgets yields mixed results across different models in humor reasoning.

</details>


### [61] [Improving Task Diversity in Label Efficient Supervised Finetuning of LLMs](https://arxiv.org/abs/2507.21482)

*Abhinav Arabelly, Jagrut Nemade, Robert D Nowak, Jifan Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, label-efficient learning, task-diversity, inverse confidence weighting, annotation costs

**Relevance Score:** 9

**TL;DR:** This paper introduces a label-efficient learning strategy for supervised finetuning of large language models by leveraging task-diversity for effective data selection, resulting in higher accuracy and reduced annotation costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the labor-intensive and costly process of human annotation in developing specialized applications of large language models.

**Method:** The proposed approach leverages task-diversity for data selection using inverse confidence weighting to sample examples across tasks, offering a simpler and less computationally intensive alternative to existing methods.

**Key Contributions:**

	1. Introduction of a task-diversity principle for data selection in supervised finetuning
	2. Development of an inverse confidence weighting sampling strategy
	3. Demonstrated accuracy improvements and significant reductions in annotation costs.

**Result:** The method achieves better accuracy than models trained on complete datasets, showing a 4% increase in MMLU score and consistently performing at or above existing best methods while reducing annotation costs by up to 80%.

**Limitations:** 

**Conclusion:** This approach presents a straightforward yet effective solution for improving label efficiency in supervised finetuning by utilizing task-diversity.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but developing high-performing models for specialized applications often requires substantial human annotation -- a process that is time-consuming, labor-intensive, and expensive. In this paper, we address the label-efficient learning problem for supervised finetuning (SFT) by leveraging task-diversity as a fundamental principle for effective data selection. This is markedly different from existing methods based on the prompt-diversity. Our approach is based on two key observations: 1) task labels for different prompts are often readily available; 2) pre-trained models have significantly varying levels of confidence across tasks. We combine these facts to devise a simple yet effective sampling strategy: we select examples across tasks using an inverse confidence weighting strategy. This produces models comparable to or better than those trained with more complex sampling procedures, while being significantly easier to implement and less computationally intensive. Notably, our experimental results demonstrate that this method can achieve better accuracy than training on the complete dataset (a 4\% increase in MMLU score). Across various annotation budgets and two instruction finetuning datasets, our algorithm consistently performs at or above the level of the best existing methods, while reducing annotation costs by up to 80\%.

</details>


### [62] [VN-MTEB: Vietnamese Massive Text Embedding Benchmark](https://arxiv.org/abs/2507.21500)

*Loc Pham, Tung Luu, Thu Vo, Minh Nguyen, Viet Hoang*

**Main category:** cs.CL

**Keywords:** Vietnamese benchmark, text embeddings, Rotary Positional Embedding, AI model evaluation, large language models

**Relevance Score:** 6

**TL;DR:** The paper introduces VN-MTEB, a benchmark for Vietnamese text embeddings created to evaluate AI models, addressing limitations in existing datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant lack of large-scale test datasets for Vietnamese text, which hampers effective evaluation of AI models in real-world applications.

**Method:** A large number of English samples from the Massive Text Embedding Benchmark were translated into Vietnamese using an automated framework leveraging large language models. The benchmark consists of 41 datasets across six different tasks.

**Key Contributions:**

	1. Introduction of a Vietnamese benchmark for AI models
	2. Translation framework using LLMs for creating high-quality datasets
	3. Performance analysis of embedding models with different positional embedding techniques

**Result:** Analysis shows that models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks.

**Limitations:** The study focuses solely on Vietnamese text, potentially limiting its applicability to other languages.

**Conclusion:** VN-MTEB provides a comprehensive resource for evaluating embedding models for Vietnamese text and includes datasets available on HuggingFace.

**Abstract:** Vietnam ranks among the top countries in terms of both internet traffic and online toxicity. As a result, implementing embedding models for recommendation and content control duties in applications is crucial. However, a lack of large-scale test datasets, both in volume and task diversity, makes it tricky for scientists to effectively evaluate AI models before deploying them in real-world, large-scale projects. To solve this important problem, we introduce a Vietnamese benchmark, VN-MTEB for embedding models, which we created by translating a large number of English samples from the Massive Text Embedding Benchmark using our new automated framework. We leverage the strengths of large language models (LLMs) and cutting-edge embedding models to conduct translation and filtering processes to retain high-quality samples, guaranteeing a natural flow of language and semantic fidelity while preserving named entity recognition (NER) and code snippets. Our comprehensive benchmark consists of 41 datasets from six tasks specifically designed for Vietnamese text embeddings. In our analysis, we find that bigger and more complex models using Rotary Positional Embedding outperform those using Absolute Positional Embedding in embedding tasks. Datasets are available at HuggingFace: https://huggingface.co/collections/GreenNode/vn-mteb-68871433f0f7573b8e1a6686

</details>


### [63] [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509)

*Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey*

**Main category:** cs.CL

**Keywords:** personality vectors, large language models, personality traits, machine learning, HCI

**Relevance Score:** 9

**TL;DR:** The paper introduces persona vectors to monitor and control personality shifts in large language models, ensuring alignment with desired traits and mitigating unintended changes during training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To identify and manage deviations in the Assistant persona of large language models, which may lead to undesirable traits such as evil or hallucination.

**Method:** The authors propose a method to extract persona vectors that represent various personality traits, allowing for real-time monitoring and intervention during deployment and training.

**Key Contributions:**

	1. Introduction of persona vectors for monitoring model personality shifts
	2. Development of preventative steering methods to avoid undesirable changes
	3. Automated extraction method applicable to various personality traits

**Result:** The study found that personality shifts during training are correlated with changes along persona vectors, and that these shifts can be mitigated with targeted interventions.

**Limitations:** 

**Conclusion:** The paper demonstrates that automated persona vector extraction can help predict and control personality changes in language models, improving alignment with intended user interaction.

**Abstract:** Large language models interact with users through a simulated 'Assistant' persona. While the Assistant is typically trained to be helpful, harmless, and honest, it sometimes deviates from these ideals. In this paper, we identify directions in the model's activation space-persona vectors-underlying several traits, such as evil, sycophancy, and propensity to hallucinate. We confirm that these vectors can be used to monitor fluctuations in the Assistant's personality at deployment time. We then apply persona vectors to predict and control personality shifts that occur during training. We find that both intended and unintended personality changes after finetuning are strongly correlated with shifts along the relevant persona vectors. These shifts can be mitigated through post-hoc intervention, or avoided in the first place with a new preventative steering method. Moreover, persona vectors can be used to flag training data that will produce undesirable personality changes, both at the dataset level and the individual sample level. Our method for extracting persona vectors is automated and can be applied to any personality trait of interest, given only a natural-language description.

</details>


### [64] [Model-free Speculative Decoding for Transformer-based ASR with Token Map Drafting](https://arxiv.org/abs/2507.21522)

*Tuan Vu Ho, Hiroaki Kokubo, Masaaki Yamamoto, Yohei Kawaguchi*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, speculative decoding, Token Map Drafting

**Relevance Score:** 4

**TL;DR:** This paper introduces a model-free speculative decoding technique called Token Map Drafting, which improves the efficiency of automatic speech recognition systems on resource-constrained devices without compromising accuracy.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** End-to-end ASR systems like Whisper face challenges with high computational costs during autoregressive decoding, limiting their deployment on resource-constrained devices.

**Method:** The proposed Token Map Drafting technique utilizes a precomputed n-gram token map from domain-specific training data to accelerate speculative decoding without a separate draft model.

**Key Contributions:**

	1. Introduction of Token Map Drafting for ASR
	2. Significant speed improvements in decoding without accuracy loss
	3. Model-free solution that reduces reliance on separate draft models

**Result:** The method achieves speed-ups of 1.27x on the CI-AVSR dataset and 1.37x on an internal dataset, with a 10% improvement over the Distill-spec baseline on CPU, all while maintaining transcription accuracy.

**Limitations:** 

**Conclusion:** Token Map Drafting offers an effective solution for enhancing ASR inference speed on devices lacking hardware accelerators, making it suitable for on-device applications.

**Abstract:** End-to-end automatic speech recognition (ASR) systems based on transformer architectures, such as Whisper, offer high transcription accuracy and robustness. However, their autoregressive decoding is computationally expensive, hence limiting deployment on CPU-based and resource-constrained devices. Speculative decoding (SD) mitigates this issue by using a smaller draft model to propose candidate tokens, which are then verified by the main model. However, this approach is impractical for devices lacking hardware accelerators like GPUs. To address this, we propose \emph{Token Map Drafting}, a model-free SD technique that eliminates the need for a separate draft model. Instead, we leverage a precomputed n-gram token map derived from domain-specific training data, enabling efficient speculative decoding with minimal overhead. Our method significantly accelerates ASR inference in structured, low-perplexity domains without sacrificing transcription accuracy. Experimental results demonstrate decoding speed-ups of $1.27\times$ on the CI-AVSR dataset and $1.37\times$ on our internal dataset without degrading recognition accuracy. Additionally, our approach achieves a $10\%$ absolute improvement in decoding speed over the Distill-spec baseline running on CPU, highlighting its effectiveness for on-device ASR applications.

</details>


### [65] [TriangleMix: A Lossless and Efficient Attention Pattern for Long Context Prefilling](https://arxiv.org/abs/2507.21526)

*Zhiyuan He, Yike Zhang, Chengruidong Zhang, Huiqiang Jiang, Yuqing Yang, Lili Qiu*

**Main category:** cs.CL

**Keywords:** Large Language Models, attention mechanisms, sparsity methods

**Relevance Score:** 7

**TL;DR:** TriangleMix reduces attention overhead in LLMs by employing a novel static attention pattern, achieving significant speedups while maintaining accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the computational bottlenecks caused by the quadratic growth of attention mechanisms in LLMs during the prefilling stage.

**Method:** TriangleMix implements dense attention in shallow layers and a triangle-shaped sparse pattern in deeper layers, providing a training-free static attention solution.

**Key Contributions:**

	1. Introduction of TriangleMix as a training-free static attention pattern
	2. Reduction of attention overhead significantly across layers
	3. Integration with dynamic sparsity methods for improved inference speed

**Result:** TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers and decreases Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths of 32K to 128K.

**Limitations:** 

**Conclusion:** TriangleMix enhances LLM inference efficiency and integrates well with dynamic sparsity methods for further improvements.

**Abstract:** Large Language Models (LLMs) rely on attention mechanisms whose time complexity grows quadratically with input sequence length, creating significant computational bottlenecks during the prefilling stage. Existing static sparse attention methods typically degrade accuracy, while dynamic sparsity methods introduce additional computational overhead due to runtime sparse index estimation. To address these limitations, we propose TriangleMix, a novel training-free static attention pattern. TriangleMix employs dense attention in shallow layers and switches to a triangle-shaped sparse pattern in deeper layers. Extensive experiments demonstrate that TriangleMix reduces attention overhead by 3.7x to 15.3x in deep layers, and decreases overall Time-to-First-Token (TTFT) by 12% to 32% for sequence lengths ranging from 32K to 128K, without sacrificing model accuracy. Moreover, TriangleMix can be seamlessly integrated with dynamic sparsity methods to achieve further speedup, e.g. accelerating MInference by 19% at 128K, highlighting its potential to enhance LLM inference efficiency.

</details>


### [66] [Automatic Classification of User Requirements from Online Feedback -- A Replication Study](https://arxiv.org/abs/2507.21532)

*Meet Bhatt, Nic Boilard, Muhammad Rehan Chaudhary, Cole Thompson, Jacob Idoko, Aakash Sorathiya, Gouri Ginde*

**Main category:** cs.CL

**Keywords:** NLP4RE, requirements engineering, deep learning, replication study, GPT-4o

**Relevance Score:** 8

**TL;DR:** The paper replicates and extends a previous NLP4RE study focused on classifying user requirements from online feedback using deep learning, providing improved insights into model performance and replication readiness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To replicate and extend previous research in applying NLP techniques to requirements engineering, thus enhancing understanding and validation of NLP4RE methodologies.

**Method:** The study reproduces original results using source code and evaluates model performance on an external dataset, comparing it to a GPT-4o zero-shot classifier.

**Key Contributions:**

	1. Replication of a previous NLP4RE study to confirm findings.
	2. Evaluation of model performance on an external dataset and comparison with a GPT-4o zero-shot classifier.
	3. Provision of a replication study ID-card to enhance replication readiness.

**Result:** Diverse reproducibility levels were found across models, with Naive Bayes showing perfect reproducibility and BERT demonstrating mixed results. GPT-4o performed comparably to traditional models and the study confirmed the baseline's replication readiness.

**Limitations:** Missing environment setup files hindered full replication readiness but were included in the study's replication package.

**Conclusion:** The findings indicate potential for improved machine-assisted workflows in requirements engineering, emphasizing the need for better replication practices and transparency in setup documentation.

**Abstract:** Natural language processing (NLP) techniques have been widely applied in the requirements engineering (RE) field to support tasks such as classification and ambiguity detection. Although RE research is rooted in empirical investigation, it has paid limited attention to replicating NLP for RE (NLP4RE) studies. The rapidly advancing realm of NLP is creating new opportunities for efficient, machine-assisted workflows, which can bring new perspectives and results to the forefront. Thus, we replicate and extend a previous NLP4RE study (baseline), "Classifying User Requirements from Online Feedback in Small Dataset Environments using Deep Learning", which evaluated different deep learning models for requirement classification from user reviews. We reproduced the original results using publicly released source code, thereby helping to strengthen the external validity of the baseline study. We then extended the setup by evaluating model performance on an external dataset and comparing results to a GPT-4o zero-shot classifier. Furthermore, we prepared the replication study ID-card for the baseline study, important for evaluating replication readiness. Results showed diverse reproducibility levels across different models, with Naive Bayes demonstrating perfect reproducibility. In contrast, BERT and other models showed mixed results. Our findings revealed that baseline deep learning models, BERT and ELMo, exhibited good generalization capabilities on an external dataset, and GPT-4o showed performance comparable to traditional baseline machine learning models. Additionally, our assessment confirmed the baseline study's replication readiness; however missing environment setup files would have further enhanced readiness. We include this missing information in our replication package and provide the replication study ID-card for our study to further encourage and support the replication of our study.

</details>


### [67] [Modern Uyghur Dependency Treebank (MUDT): An Integrated Morphosyntactic Framework for a Low-Resource Language](https://arxiv.org/abs/2507.21536)

*Jiaxin Zuo, Yiquan Wang, Yuan Pan, Xiadiya Yibulayin*

**Main category:** cs.CL

**Keywords:** Uyghur, Natural Language Processing, Dependency Annotation, Treebank, Morphological Complexity

**Relevance Score:** 3

**TL;DR:** This study presents a dependency annotation framework for Uyghur NLP addressing resource gaps, highlighting significant divergences from existing standards and improving parsing accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address a critical resource gap in Uyghur Natural Language Processing, this research aims to create a dependency annotation framework for the language.

**Method:** The study introduces 18 main relations and 26 subtypes in the framework and conducts a cross-standard evaluation using a pre-trained Universal Dependencies parser.

**Key Contributions:**

	1. Development of a tailored dependency annotation framework for Uyghur.
	2. Identification of 47.9% divergence in annotations from existing standards.
	3. Introduction of nine annotation principles ensuring typological accuracy.

**Result:** The analysis showed a 47.9% divergence in annotations, demonstrating the limitations of universal schemes in handling Uyghur-specific structures.

**Limitations:** 

**Conclusion:** The Modern Uyghur Dependency Treebank (MUDT) improves parsing and downstream NLP tasks and serves as a model for other morphologically complex languages.

**Abstract:** To address a critical resource gap in Uyghur Natural Language Processing (NLP), this study introduces a dependency annotation framework designed to overcome the limitations of existing treebanks for the low-resource, agglutinative language. This inventory includes 18 main relations and 26 subtypes, with specific labels such as cop:zero for verbless clauses and instr:case=loc/dat for nuanced instrumental functions. To empirically validate the necessity of this tailored approach, we conducted a cross-standard evaluation using a pre-trained Universal Dependencies parser. The analysis revealed a systematic 47.9% divergence in annotations, pinpointing the inadequacy of universal schemes for handling Uyghur-specific structures. Grounded in nine annotation principles that ensure typological accuracy and semantic transparency, the Modern Uyghur Dependency Treebank (MUDT) provides a more accurate and semantically transparent representation, designed to enable significant improvements in parsing and downstream NLP tasks, and offers a replicable model for other morphologically complex languages.

</details>


### [68] [MAGIC: A Multi-Hop and Graph-Based Benchmark for Inter-Context Conflicts in Retrieval-Augmented Generation](https://arxiv.org/abs/2507.21544)

*Jungyeon Lee, Kangmin Lee, Taeuk Kim*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Knowledge Graph, LLM, Conflict Detection, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper addresses knowledge conflict in RAG systems using a knowledge graph-based framework to generate conflicts and analyze LLM performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate knowledge conflict in retrieval-augmented generation systems, addressing limitations in existing benchmarks.

**Method:** A knowledge graph (KG)-based framework generates varied conflicts between distinct contexts, enabling interpretation through the relational structure of KGs.

**Key Contributions:**

	1. Introduction of a KG-based framework for generating conflicts
	2. Development of the MAGIC benchmark for examining knowledge conflict
	3. Insights into LLMs' capabilities and limitations in conflict detection

**Result:** Experimental results on the MAGIC benchmark reveal that LLMs, both open-source and proprietary, struggle with conflict detection, especially in multi-hop reasoning scenarios.

**Limitations:** 

**Conclusion:** In-depth analyses provide a foundation for enhancing LLMs' ability to process conflicting information more effectively.

**Abstract:** Knowledge conflict often arises in retrieval-augmented generation (RAG) systems, where retrieved documents may be inconsistent with one another or contradict the model's parametric knowledge. Existing benchmarks for investigating the phenomenon have notable limitations, including a narrow focus on the question answering setup, heavy reliance on entity substitution techniques, and a restricted range of conflict types. To address these issues, we propose a knowledge graph (KG)-based framework that generates varied and subtle conflicts between two similar yet distinct contexts, while ensuring interpretability through the explicit relational structure of KGs. Experimental results on our benchmark, MAGIC, provide intriguing insights into the inner workings of LLMs regarding knowledge conflict: both open-source and proprietary models struggle with conflict detection -- especially when multi-hop reasoning is required -- and often fail to pinpoint the exact source of contradictions. Finally, we present in-depth analyses that serve as a foundation for improving LLMs in integrating diverse, sometimes even conflicting, information.

</details>


### [69] [Evaluating the cognitive reality of Spanish irregular morphomic patterns: Humans vs. Transformers](https://arxiv.org/abs/2507.21556)

*Akhilesh Kakolu Ramarao, Kevin Tang, Dinah Baer-Henney*

**Main category:** cs.CL

**Keywords:** transformer models, Spanish linguistics, morphomic patterns, neural networks, human behavior

**Relevance Score:** 5

**TL;DR:** This study compares transformer-based neural networks to human behaviors regarding Spanish irregular morphomic patterns, revealing key differences in linguistic response preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how transformer models replicate human sensitivity to complex linguistic phenomena, specifically focusing on the Spanish morphomic pattern.

**Method:** The study utilized the same analytical framework as an earlier human study, testing transformer models under controlled input conditions across three frequency distributions of irregular verb patterns.

**Key Contributions:**

	1. Direct comparison of transformer models and human behavior on linguistic tasks
	2. Identification of performance patterns across different frequency distributions
	3. Revealing discrepancies in response preferences between humans and models

**Result:** Transformer models achieved higher accuracy in stem and suffix identification than humans but diverged in preference for natural versus irregular responses, which were influenced by their training data.

**Limitations:** The study focuses solely on Spanish and specific neuro-linguistic tasks, which may limit the generalizability of findings.

**Conclusion:** Models showed some sensitivity to phonological similarities only in specific frequency conditions, indicating significant differences between model behavior and human linguistic processing.

**Abstract:** This study investigates the cognitive plausibility of the Spanish irregular morphomic pattern by directly comparing transformer-based neural networks to human behavioral data from \citet{Nevins2015TheRA}. Using the same analytical framework as the original human study, we evaluate whether transformer models can replicate human-like sensitivity to a complex linguistic phenomena, the morphome, under controlled input conditions. Our experiments focus on three frequency conditions: natural, low-frequency, and high-frequency distributions of verbs exhibiting irregular morphomic patterns. While the models outperformed humans in stem and suffix accuracy, a clear divergence emerged in response preferences. Unlike humans, who consistently favored natural responses across all test items, models' preferred irregular responses and were influenced by the proportion of irregular verbs in their training data. Additionally, models trained on the natural and low-frequency distributions, but not the high-frequency distribution, were sensitive to the phonological similarity between test items and real Spanish L-shaped verbs.

</details>


### [70] [Multi-Hypothesis Distillation of Multilingual Neural Translation Models for Low-Resource Languages](https://arxiv.org/abs/2507.21568)

*Aarón Galiano-Jiménez, Juan Antonio Pérez-Ortiz, Felipe Sánchez-Martínez, Víctor M. Sánchez-Cartagena*

**Main category:** cs.CL

**Keywords:** knowledge distillation, multilingual translation, sequence-level learning

**Relevance Score:** 7

**TL;DR:** This paper presents Multi-Hypothesis Distillation, a sequence-level knowledge distillation method for multilingual translation models, which improves variability and performance of student models by exposing them to multiple translations from teacher models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve knowledge distillation in multilingual translation models by leveraging the teacher model's output distribution beyond standard decoding methods.

**Method:** The authors introduce Multi-Hypothesis Distillation (MHD), utilizing n-best lists from beam search to expose the student model to a wider array of translations and addressing variability issues.

**Key Contributions:**

	1. Introduction of Multi-Hypothesis Distillation (MHD) for sequence-level knowledge distillation.
	2. Demonstration of improved lexical variability and richness in generated translations.
	3. Mitigation of gender bias amplification in student models through enhanced training techniques.

**Result:** The study finds that while sampling methods may compromise translation quality slightly, they notably enhance variability and lexical richness, leading to improved student model performance.

**Limitations:** The potential compromise in translation quality when using sampling methods compared to beam search approaches.

**Conclusion:** The proposed MHD method not only enhances translation diversity but also helps mitigate gender bias amplification in knowledge distillation.

**Abstract:** This paper explores sequence-level knowledge distillation (KD) of multilingual pre-trained encoder-decoder translation models. We argue that the teacher model's output distribution holds valuable insights for the student, beyond the approximated mode obtained through beam search (the standard decoding method), and present Multi-Hypothesis Distillation (MHD), a sequence-level KD method that generates multiple translations for each source sentence. This provides a larger representation of the teacher model distribution and exposes the student model to a wider range of target-side prefixes. We leverage $n$-best lists from beam search to guide the student's learning and examine alternative decoding methods to address issues like low variability and the under-representation of infrequent tokens. For low-resource languages, our research shows that while sampling methods may slightly compromise translation quality compared to beam search based approaches, they enhance the generated corpora with greater variability and lexical richness. This ultimately improves student model performance and mitigates the gender bias amplification often associated with KD.

</details>


### [71] [Multilingual JobBERT for Cross-Lingual Job Title Matching](https://arxiv.org/abs/2507.21609)

*Jens-Joris Decorte, Matthias De Lange, Jeroen Van Hautte*

**Main category:** cs.CL

**Keywords:** cross-lingual, job title matching, contrastive learning, multilingual, JobBERT-V3

**Relevance Score:** 7

**TL;DR:** JobBERT-V3 is a cross-lingual job title matching model using contrastive learning, outperforming multilingual baselines with no task-specific supervision.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve job title matching across multiple languages by leveraging a large multilingual dataset and contrastive learning techniques.

**Method:** The model is built on the architecture of JobBERT-V2 and incorporates synthetic translations across four languages with extensive evaluations on the TalentCLEF 2025 benchmark.

**Key Contributions:**

	1. Introduction of JobBERT-V3 for cross-lingual job title matching
	2. Use of synthetic translations to enhance multilingual support
	3. Demonstrated broader applicability in ranking relevant skills for job titles

**Result:** JobBERT-V3 outperforms established multilingual baselines and provides consistent performance for both monolingual and cross-lingual job title matching tasks.

**Limitations:** 

**Conclusion:** The model shows potential beyond its primary use by effectively ranking relevant skills for job titles, contributing to multilingual labor market intelligence.

**Abstract:** We introduce JobBERT-V3, a contrastive learning-based model for cross-lingual job title matching. Building on the state-of-the-art monolingual JobBERT-V2, our approach extends support to English, German, Spanish, and Chinese by leveraging synthetic translations and a balanced multilingual dataset of over 21 million job titles. The model retains the efficiency-focused architecture of its predecessor while enabling robust alignment across languages without requiring task-specific supervision. Extensive evaluations on the TalentCLEF 2025 benchmark demonstrate that JobBERT-V3 outperforms strong multilingual baselines and achieves consistent performance across both monolingual and cross-lingual settings. While not the primary focus, we also show that the model can be effectively used to rank relevant skills for a given job title, demonstrating its broader applicability in multilingual labor market intelligence. The model is publicly available: https://huggingface.co/TechWolf/JobBERT-v3.

</details>


### [72] [Libra: Assessing and Improving Reward Model by Learning to Think](https://arxiv.org/abs/2507.21645)

*Meng Zhou, Bei Li, Jiahao Liu, Xiaowen Shi, Yang Bai, Rongxiang Weng, Jingang Wang, Xunliang Cai*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Language Models, Reasoning, Benchmarking, Generative Models

**Relevance Score:** 8

**TL;DR:** This paper proposes a framework, including a new benchmark and generative reward models, to enhance reasoning abilities in large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current reinforcement learning reward models fail in complex reasoning scenarios due to their reliance on annotated references and constrained output formats.

**Method:** The authors introduce Libra Bench, a reasoning-oriented benchmark for evaluating reward models, and develop Libra-RM series generative reward models using learning-to-think methodologies.

**Key Contributions:**

	1. Introduction of Libra Bench as a comprehensive reasoning benchmark.
	2. Development of Libra-RM generative reward models with improved reasoning capabilities.
	3. Demonstration of enhanced performance in downstream applications using the proposed framework.

**Result:** Libra-RM models achieve state-of-the-art results on various benchmarks, showing improved reasoning capabilities, especially with unlabeled data.

**Limitations:** 

**Conclusion:** The findings suggest that the proposed framework can overcome existing limitations in reward modeling and has potential for improving reasoning models in practical applications.

**Abstract:** Reinforcement learning (RL) has significantly improved the reasoning ability of large language models. However, current reward models underperform in challenging reasoning scenarios and predominant RL training paradigms rely on rule-based or reference-based rewards, which impose two critical limitations: 1) the dependence on finely annotated reference answer to attain rewards; and 2) the requirement for constrained output format. These limitations fundamentally hinder further RL data scaling and sustained enhancement of model reasoning performance. To address these limitations, we propose a comprehensive framework for evaluating and improving the performance of reward models in complex reasoning scenarios. We first present a reasoning-oriented benchmark (Libra Bench), systematically constructed from a diverse collection of challenging mathematical problems and advanced reasoning models, to address the limitations of existing reward model benchmarks in reasoning scenarios. We further introduce a novel approach for improving the generative reward model via learning-to-think methodologies. Based on the proposed approach, we develop Libra-RM series, a collection of generative reward models with reasoning capabilities that achieve state-of-the-art results on various benchmarks. Comprehensive downstream experiments are conducted and the experimental results demonstrate the correlation between our Libra Bench and downstream application, and the potential of Libra-RM to further improve reasoning models with unlabeled data.

</details>


### [73] [UnsafeChain: Enhancing Reasoning Model Safety via Hard Cases](https://arxiv.org/abs/2507.21652)

*Raj Vardhan Tomar, Preslav Nakov, Yuxia Wang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Safety Alignment, Chain-of-Thought Reasoning, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Introducing UnsafeChain, a safety alignment dataset to improve LRM safety with hard prompts and correction-based supervision.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address safety challenges in large reasoning models (LRMs) by exposing and correcting unsafe behaviors in prompts.

**Method:** UnsafeChain is constructed from hard prompts where unsafe completions are identified and corrected into safe responses. Three LRMs were fine-tuned on this dataset and tested against prior benchmarks.

**Key Contributions:**

	1. Introduction of UnsafeChain dataset for safety alignment of LRMs
	2. Demonstrated superiority of UnsafeChain over previous safety datasets
	3. Enhanced model performance via correction-based supervision

**Result:** UnsafeChain outperformed prior datasets including SafeChain and STAR-1, demonstrating improved safety and reasoning ability across various benchmarks.

**Limitations:** 

**Conclusion:** Correction-based supervision using UnsafeChain is effective in enhancing model safety without compromising general reasoning ability.

**Abstract:** As large reasoning models (LRMs) grow more capable, chain-of-thought (CoT) reasoning introduces new safety challenges. Existing SFT-based safety alignment studies dominantly focused on filtering prompts with safe, high-quality responses, while overlooking hard prompts that always elicit harmful outputs. To fill this gap, we introduce UnsafeChain, a safety alignment dataset constructed from hard prompts with diverse sources, where unsafe completions are identified and explicitly corrected into safe responses. By exposing models to unsafe behaviors and guiding their correction, UnsafeChain enhances safety while preserving general reasoning ability. We fine-tune three LRMs on UnsafeChain and compare them against recent SafeChain and STAR-1 across six out-of-distribution and five in-distribution benchmarks. UnsafeChain consistently outperforms prior datasets, with even a 1K subset matching or surpassing baseline performance, demonstrating the effectiveness and generalizability of correction-based supervision. We release our dataset and code at https://github.com/mbzuai-nlp/UnsafeChain

</details>


### [74] [Adversarial Defence without Adversarial Defence: Enhancing Language Model Robustness via Instance-level Principal Component Removal](https://arxiv.org/abs/2507.21750)

*Yang Wang, Chenghao Xiao, Yizhi Li, Stuart E. Middleton, Noura Al Moubayed, Chenghua Lin*

**Main category:** cs.CL

**Keywords:** Adversarial robustness, Pre-trained language models, Natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel module that enhances the adversarial robustness of pre-trained language models without traditional adversarial defenses or high computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing vulnerability of pre-trained language models (PLMs) to adversarial attacks raises concerns about their applicability in real-world scenarios, necessitating effective and efficient robustness strategies.

**Method:** The proposed method removes instance-level principal components to transform the embedding space towards Gaussian properties, which minimizes the impact of adversarial noise on decision boundaries without needing adversarial examples or costly training augmentations.

**Key Contributions:**

	1. Introduction of a novel module for enhancing adversarial robustness in PLMs
	2. Transformation of the embedding space to approximate Gaussian characteristics
	3. Demonstration of improved robustness with minimal impact on accuracy

**Result:** The approach was evaluated on eight benchmark datasets, demonstrating improved adversarial robustness while maintaining accuracy comparable to existing baselines.

**Limitations:** 

**Conclusion:** This method strikes a balance between adversarial robustness and generalization, offering a simpler alternative to existing adversarial training techniques.

**Abstract:** Pre-trained language models (PLMs) have driven substantial progress in natural language processing but remain vulnerable to adversarial attacks, raising concerns about their robustness in real-world applications. Previous studies have sought to mitigate the impact of adversarial attacks by introducing adversarial perturbations into the training process, either implicitly or explicitly. While both strategies enhance robustness, they often incur high computational costs. In this work, we propose a simple yet effective add-on module that enhances the adversarial robustness of PLMs by removing instance-level principal components, without relying on conventional adversarial defences or perturbing the original training data. Our approach transforms the embedding space to approximate Gaussian properties, thereby reducing its susceptibility to adversarial perturbations while preserving semantic relationships. This transformation aligns embedding distributions in a way that minimises the impact of adversarial noise on decision boundaries, enhancing robustness without requiring adversarial examples or costly training-time augmentation. Evaluations on eight benchmark datasets show that our approach improves adversarial robustness while maintaining comparable before-attack accuracy to baselines, achieving a balanced trade-off between robustness and generalisation.

</details>


### [75] [AgriEval: A Comprehensive Chinese Agricultural Benchmark for Large Language Models](https://arxiv.org/abs/2507.21773)

*Lian Yan, Haotian Wang, Chen Tang, Haifeng Liu, Tianyang Sun, Liangliang Liu, Yi Guan, Jingchi Jiang*

**Main category:** cs.CL

**Keywords:** agriculture, language models, benchmarking, evaluation, natural language processing

**Relevance Score:** 4

**TL;DR:** AgriEval is the first comprehensive Chinese agricultural benchmark designed to evaluate the performance of large language models (LLMs) in agriculture. It includes diverse questions and high-quality data from educational materials.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of limited training data and evaluation benchmarks for LLMs in agriculture, aimed at fostering improvements in their capabilities.

**Method:** AgriEval consists of 14,697 multiple-choice questions and 2,167 open-ended questions spanning six major agriculture categories. Experimental results were conducted over 51 LLMs to gauge their performance.

**Key Contributions:**

	1. Introduction of AgriEval as a benchmark for agricultural LLMs
	2. Comprehensive dataset from educational sources covering a wide range of agricultural topics
	3. Experimental evaluation of 51 LLMs highlighting areas for improvement

**Result:** Most existing LLMs showcased struggles, achieving less than 60% accuracy, indicating a significant developmental opportunity for agricultural applications of LLMs.

**Limitations:** 

**Conclusion:** The establishment of AgriEval can encourage advancements in LLM performance in agriculture and assists in identifying strategies for model enhancement.

**Abstract:** In the agricultural domain, the deployment of large language models (LLMs) is hindered by the lack of training data and evaluation benchmarks. To mitigate this issue, we propose AgriEval, the first comprehensive Chinese agricultural benchmark with three main characteristics: (1) Comprehensive Capability Evaluation. AgriEval covers six major agriculture categories and 29 subcategories within agriculture, addressing four core cognitive scenarios: memorization, understanding, inference, and generation. (2) High-Quality Data. The dataset is curated from university-level examinations and assignments, providing a natural and robust benchmark for assessing the capacity of LLMs to apply knowledge and make expert-like decisions. (3) Diverse Formats and Extensive Scale. AgriEval comprises 14,697 multiple-choice questions and 2,167 open-ended question-and-answer questions, establishing it as the most extensive agricultural benchmark available to date. We also present comprehensive experimental results over 51 open-source and commercial LLMs. The experimental results reveal that most existing LLMs struggle to achieve 60% accuracy, underscoring the developmental potential in agricultural LLMs. Additionally, we conduct extensive experiments to investigate factors influencing model performance and propose strategies for enhancement. AgriEval is available at https://github.com/YanPioneer/AgriEval/.

</details>


### [76] [The Problem with Safety Classification is not just the Models](https://arxiv.org/abs/2507.21782)

*Sowmya Vajjala*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety classification, multilingual disparities, evaluation datasets, harmful content detection

**Relevance Score:** 9

**TL;DR:** This paper discusses the effectiveness of safety classification models for Large Language Models (LLMs), particularly in multilingual contexts, highlighting disparities in safety evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of unsafe behaviors in Large Language Models (LLMs) through improved safety classification models and evaluation datasets, especially in multilingual scenarios.

**Method:** The study analyzes 5 safety classification models and their performance across evaluation datasets spanning 18 different languages.

**Key Contributions:**

	1. Identification of multilingual disparities in LLM safety classification models.
	2. Critique of the evaluation datasets used for testing safety classifiers in diverse languages.
	3. Contribution to discussions on improving harm detection methods in multilingual LLM inputs.

**Result:** The authors identify significant multilingual disparities in safety classifiers and raise concerns about the efficacy of current evaluation datasets used in testing these models.

**Limitations:** Limited exploration of potential mitigation strategies for identified issues.

**Conclusion:** Findings suggest that issues with safety classifiers are not solely model-related, but also stem from inadequacies in the evaluation datasets.

**Abstract:** Studying the robustness of Large Language Models (LLMs) to unsafe behaviors is an important topic of research today. Building safety classification models or guard models, which are fine-tuned models for input/output safety classification for LLMs, is seen as one of the solutions to address the issue. Although there is a lot of research on the safety testing of LLMs themselves, there is little research on evaluating the effectiveness of such safety classifiers or the evaluation datasets used for testing them, especially in multilingual scenarios. In this position paper, we demonstrate how multilingual disparities exist in 5 safety classification models by considering datasets covering 18 languages. At the same time, we identify potential issues with the evaluation datasets, arguing that the shortcomings of current safety classifiers are not only because of the models themselves. We expect that these findings will contribute to the discussion on developing better methods to identify harmful content in LLM inputs across languages.

</details>


### [77] [ChartMark: A Structured Grammar for Chart Annotation](https://arxiv.org/abs/2507.21810)

*Yiyu Chen, Yifan Wu, Shuyu Shen, Yupeng Xie, Leixian Shen, Hui Xiong, Yuyu Luo*

**Main category:** cs.CL

**Keywords:** chart annotations, visualization accessibility, Vega-Lite

**Relevance Score:** 4

**TL;DR:** ChartMark is a structured grammar for standardized chart annotations that enhances visualization accessibility and cross-platform reuse.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address fragmented and non-standardized representations of chart annotations which limit accessibility and interoperability across different platforms.

**Method:** ChartMark is developed as a hierarchical framework that separates annotation semantics from visualization implementations, mapping onto various annotation dimensions.

**Key Contributions:**

	1. Structured grammar for chart annotations
	2. Hierarchical framework for annotation dimensions
	3. Toolkit for converting specifications to Vega-Lite visualizations

**Result:** The toolkit successfully converts ChartMark specifications into Vega-Lite visualizations, demonstrating its flexibility and expressiveness.

**Limitations:** 

**Conclusion:** ChartMark improves the accessibility of visualizations by providing a structured approach to annotations that can be uniformly applied across platforms.

**Abstract:** Chart annotations enhance visualization accessibility but suffer from fragmented, non-standardized representations that limit cross-platform reuse. We propose ChartMark, a structured grammar that separates annotation semantics from visualization implementations. ChartMark features a hierarchical framework mapping onto annotation dimensions (e.g., task, chart context), supporting both abstract intents and precise visual details. Our toolkit demonstrates converting ChartMark specifications into Vega-Lite visualizations, highlighting its flexibility, expressiveness, and practical applicability.

</details>


### [78] [Overview of ADoBo at IberLEF 2025: Automatic Detection of Anglicisms in Spanish](https://arxiv.org/abs/2507.21813)

*Elena Alvarez-Mellado, Jordi Porta-Zamorano, Constantine Lignos, Julio Gonzalo*

**Main category:** cs.CL

**Keywords:** anglicisms, NLP, Spanish, language processing, machine learning

**Relevance Score:** 4

**TL;DR:** This paper discusses the ADoBo 2025 task on identifying anglicisms in Spanish, highlighting various approaches and their performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The task aims to explore the challenges of detecting English lexical borrowings in Spanish language contexts, relevant for understanding linguistic dynamics in bilingual settings.

**Method:** Participants used a range of approaches including LLMs, deep learning models, Transformer-based models, and rule-based systems to identify anglicisms in Spanish journalistic texts.

**Key Contributions:**

	1. Identification of English loanwords in Spanish
	2. Evaluation of multiple AI methodologies for NLP tasks
	3. Performance metrics highlighting variability across systems

**Result:** The systems demonstrated a wide performance range with F1 scores between 0.17 and 0.99, illustrating the challenges and effectiveness of different methodologies.

**Limitations:** Limited to Spanish texts and specific linguistic features; may not generalize to other languages or contexts.

**Conclusion:** The findings from ADoBo 2025 provide insights into the effectiveness of various techniques for anglicism detection, revealing the potential of advanced NLP tools in linguistic tasks.

**Abstract:** This paper summarizes the main findings of ADoBo 2025, the shared task on anglicism identification in Spanish proposed in the context of IberLEF 2025. Participants of ADoBo 2025 were asked to detect English lexical borrowings (or anglicisms) from a collection of Spanish journalistic texts. Five teams submitted their solutions for the test phase. Proposed systems included LLMs, deep learning models, Transformer-based models and rule-based systems. The results range from F1 scores of 0.17 to 0.99, which showcases the variability in performance different systems can have for this task.

</details>


### [79] [HRIPBench: Benchmarking LLMs in Harm Reduction Information Provision to Support People Who Use Drugs](https://arxiv.org/abs/2507.21815)

*Kaixuan Wang, Chenxin Diao, Jason T. Jacques, Zhongliang Guo, Shuai Zhao*

**Main category:** cs.CL

**Keywords:** Harm reduction, Large language models, Substance use, Benchmarking, Public health

**Relevance Score:** 9

**TL;DR:** The paper introduces HRIPBench, a benchmark for assessing the accuracy and safety of LLMs in providing harm reduction information for substance use, revealing significant challenges in their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study targets the urgent need for reliable information sources for people who use drugs (PWUD) to enhance health outcomes and minimize risks associated with substance use.

**Method:** The authors developed HRIPBench, a benchmark comprising 2,160 question-answer-evidence pairs, categorizing the evaluation into tasks such as checking safety boundaries, assessing quantitative values, and inferring polysubstance use risks with LLMs.

**Key Contributions:**

	1. Introduction of HRIPBench as a new benchmark for evaluating LLMs in harm reduction contexts
	2. Creation of a dataset (HRIP-Basic) with diverse question-answer-evidence pairs
	3. Identification of significant performance gaps in LLMs when used in substance use scenarios.

**Result:** The findings show that state-of-the-art LLMs frequently fail to deliver precise harm reduction information and may pose severe safety risks to PWUD.

**Limitations:** The potential risk associated with the misuse of LLMs in harm reduction contexts and the focus on illicit content may limit applicability.

**Conclusion:** The deployment of LLMs in harm reduction should be approached with caution to prevent adverse health impacts among users.

**Abstract:** Millions of individuals' well-being are challenged by the harms of substance use. Harm reduction as a public health strategy is designed to improve their health outcomes and reduce safety risks. Some large language models (LLMs) have demonstrated a decent level of medical knowledge, promising to address the information needs of people who use drugs (PWUD). However, their performance in relevant tasks remains largely unexplored. We introduce HRIPBench, a benchmark designed to evaluate LLM's accuracy and safety risks in harm reduction information provision. The benchmark dataset HRIP-Basic has 2,160 question-answer-evidence pairs. The scope covers three tasks: checking safety boundaries, providing quantitative values, and inferring polysubstance use risks. We build the Instruction and RAG schemes to evaluate model behaviours based on their inherent knowledge and the integration of domain knowledge. Our results indicate that state-of-the-art LLMs still struggle to provide accurate harm reduction information, and sometimes, carry out severe safety risks to PWUD. The use of LLMs in harm reduction contexts should be cautiously constrained to avoid inducing negative health outcomes. WARNING: This paper contains illicit content that potentially induces harms.

</details>


### [80] [Modelling Adjectival Modification Effects on Semantic Plausibility](https://arxiv.org/abs/2507.21828)

*Anna Golub, Beate Zywietz, Annerose Eichel*

**Main category:** cs.CL

**Keywords:** plausibility assessment, sentence transformers, dialogue generation, commonsense reasoning, evaluation metrics

**Relevance Score:** 4

**TL;DR:** This paper investigates how changes in event plausibility, due to adjectival modifiers, affect models used for dialogue generation and commonsense reasoning, finding that current transformer models struggle with these nuances.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the gap in research regarding how modifications to events affect their plausibility, which is essential for applications like dialogue generation and commonsense reasoning.

**Method:** The authors conducted modeling experiments using sentence transformers on the ADEPT challenge benchmark, consisting of 16K English sentence pairs that differ by one adjectival modifier.

**Key Contributions:**

	1. Introduction of a novel approach using sentence transformers for plausibility assessment
	2. Demonstration of the limitations of current models in handling subtle linguistic modifications
	3. Emphasis on the need for balanced evaluation methodologies in model performance assessment

**Result:** The experiments revealed that sentence transformer models underperform compared to models like RoBERTa and emphasized the need for balanced evaluation methods to avoid distorted evaluations.

**Limitations:** Models struggle with nuanced changes in event plausibility and results are affected by evaluation metric imbalances.

**Conclusion:** The findings indicate that transformer models struggle with the assessment of event plausibility influenced by adjectival modifications, highlighting the importance of realistic evaluation metrics.

**Abstract:** While the task of assessing the plausibility of events such as ''news is relevant'' has been addressed by a growing body of work, less attention has been paid to capturing changes in plausibility as triggered by event modification. Understanding changes in plausibility is relevant for tasks such as dialogue generation, commonsense reasoning, and hallucination detection as it allows to correctly model, for example, ''gentle sarcasm'' as a sign of closeness rather than unkindness among friends [9]. In this work, we tackle the ADEPT challenge benchmark [6] consisting of 16K English sentence pairs differing by exactly one adjectival modifier. Our modeling experiments provide a conceptually novel method by using sentence transformers, and reveal that both they and transformer-based models struggle with the task at hand, and sentence transformers - despite their conceptual alignment with the task - even under-perform in comparison to models like RoBERTa. Furthermore, an in-depth comparison with prior work highlights the importance of a more realistic, balanced evaluation method: imbalances distort model performance and evaluation metrics, and weaken result trustworthiness.

</details>


### [81] [Introducing HALC: A general pipeline for finding optimal prompting strategies for automated coding with LLMs in the computational social sciences](https://arxiv.org/abs/2507.21831)

*Andreas Reich, Claudia Thoms, Tobias Schrimpf*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Large Language Models, Prompt Engineering, Social Sciences, Task Automation

**Relevance Score:** 8

**TL;DR:** The paper proposes HALC, a systematic pipeline for constructing optimal prompts for LLM coding tasks, validated through extensive testing of prompting strategies.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a reliable method to construct effective prompts for LLMs in automated coding tasks in the social sciences.

**Method:** A general pipeline (HALC) was developed, and its effectiveness was validated by sending 1,512 prompts to local LLMs, assessing performance against expert codings.

**Key Contributions:**

	1. Introduction of HALC for prompt construction
	2. Validation of prompting strategies through extensive testing
	3. Identification of reliable prompts for LLM coding tasks

**Result:** The paper found that certain prompts code reliably for single and multiple variables with performance measures indicating moderate reliability.

**Limitations:** The study focuses on a specific LLM (Mistral NeMo) and may not generalize to all models or tasks.

**Conclusion:** The authors provide insights into effective prompting strategies and highlight the importance of aligning prompts with their codebook for optimal LLM performance.

**Abstract:** LLMs are seeing widespread use for task automation, including automated coding in the social sciences. However, even though researchers have proposed different prompting strategies, their effectiveness varies across LLMs and tasks. Often trial and error practices are still widespread. We propose HALC$-$a general pipeline that allows for the systematic and reliable construction of optimal prompts for any given coding task and model, permitting the integration of any prompting strategy deemed relevant. To investigate LLM coding and validate our pipeline, we sent a total of 1,512 individual prompts to our local LLMs in over two million requests. We test prompting strategies and LLM task performance based on few expert codings (ground truth). When compared to these expert codings, we find prompts that code reliably for single variables (${\alpha}$climate = .76; ${\alpha}$movement = .78) and across two variables (${\alpha}$climate = .71; ${\alpha}$movement = .74) using the LLM Mistral NeMo. Our prompting strategies are set up in a way that aligns the LLM to our codebook$-$we are not optimizing our codebook for LLM friendliness. Our paper provides insights into the effectiveness of different prompting strategies, crucial influencing factors, and the identification of reliable prompts for each coding task and model.

</details>


### [82] [AutoTIR: Autonomous Tools Integrated Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.21836)

*Yifan Wei, Xiaoyan Yu, Yixuan Weng, Tengfei Pan, Angsheng Li, Li Du*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Tool-Integrated Reasoning

**Relevance Score:** 9

**TL;DR:** AutoTIR is a reinforcement learning framework that enhances Large Language Models by allowing them to autonomously select and utilize external tools for reasoning, improving task performance and tool integration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing static tool-use methods in Large Language Models and improve their reasoning capabilities through adaptive tool selection.

**Method:** AutoTIR employs a hybrid reinforcement learning approach to determine when and which external tools to invoke, optimizing for output correctness and efficient tool usage.

**Key Contributions:**

	1. Introduction of AutoTIR framework for adaptive tool selection in LLMs
	2. Hybrid reward mechanism that balances output correctness and tool usage
	3. Demonstration of improved performance over static tool-use methods

**Result:** AutoTIR demonstrates superior performance across various tasks compared to baseline models, showing enhanced generalization in tool-use behavior.

**Limitations:** 

**Conclusion:** The study emphasizes the effectiveness of reinforcement learning in developing adaptable tool integration capabilities, promising better scalability and performance of reasoning in LLMs.

**Abstract:** Large Language Models (LLMs), when enhanced through reasoning-oriented post-training, evolve into powerful Large Reasoning Models (LRMs). Tool-Integrated Reasoning (TIR) further extends their capabilities by incorporating external tools, but existing methods often rely on rigid, predefined tool-use patterns that risk degrading core language competence. Inspired by the human ability to adaptively select tools, we introduce AutoTIR, a reinforcement learning framework that enables LLMs to autonomously decide whether and which tool to invoke during the reasoning process, rather than following static tool-use strategies. AutoTIR leverages a hybrid reward mechanism that jointly optimizes for task-specific answer correctness, structured output adherence, and penalization of incorrect tool usage, thereby encouraging both precise reasoning and efficient tool integration. Extensive evaluations across diverse knowledge-intensive, mathematical, and general language modeling tasks demonstrate that AutoTIR achieves superior overall performance, significantly outperforming baselines and exhibits superior generalization in tool-use behavior. These results highlight the promise of reinforcement learning in building truly generalizable and scalable TIR capabilities in LLMs. The code and data are available at https://github.com/weiyifan1023/AutoTIR.

</details>


### [83] [Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning](https://arxiv.org/abs/2507.21892)

*Haoran Luo, Haihong E, Guanting Chen, Qika Lin, Yikai Guo, Fangzhi Xu, Zemin Kuang, Meina Song, Xiaobao Wu, Yifan Zhu, Luu Anh Tuan*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, GraphRAG, Reinforcement Learning, Knowledge Graphs, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** Graph-R1 is a novel agentic GraphRAG framework that improves retrieval-augmented generation (RAG) using end-to-end reinforcement learning to enhance reasoning and retrieval efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing GraphRAG methods, such as high construction costs and fixed one-time retrieval, while improving reasoning accuracy and generation quality in LLMs.

**Method:** The proposed Graph-R1 framework utilizes lightweight knowledge hypergraph construction and models retrieval as a multi-turn agent-environment interaction, optimizing the process through an end-to-end reinforcement learning reward mechanism.

**Key Contributions:**

	1. Development of an agentic GraphRAG framework
	2. Utilization of lightweight hypergraph construction
	3. Integration of multi-turn agent-environment interactions with RL optimization

**Result:** Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality based on experimental evaluations on standard RAG datasets.

**Limitations:** 

**Conclusion:** The introduction of Graph-R1 significantly enhances the capabilities of retrieval-augmented generation by addressing key challenges inherent to previous methods.

**Abstract:** Retrieval-Augmented Generation (RAG) mitigates hallucination in LLMs by incorporating external knowledge, but relies on chunk-based retrieval that lacks structural semantics. GraphRAG methods improve RAG by modeling knowledge as entity-relation graphs, but still face challenges in high construction cost, fixed one-time retrieval, and reliance on long-context reasoning and prompt design. To address these challenges, we propose Graph-R1, an agentic GraphRAG framework via end-to-end reinforcement learning (RL). It introduces lightweight knowledge hypergraph construction, models retrieval as a multi-turn agent-environment interaction, and optimizes the agent process via an end-to-end reward mechanism. Experiments on standard RAG datasets show that Graph-R1 outperforms traditional GraphRAG and RL-enhanced RAG methods in reasoning accuracy, retrieval efficiency, and generation quality.

</details>


### [84] [Rote Learning Considered Useful: Generalizing over Memorized Data in LLMs](https://arxiv.org/abs/2507.21914)

*Qinyuan Wu, Soumi Das, Mahsa Amani, Bishwamittra Ghosh, Mohammad Aflah Khan, Krishna P. Gummadi, Muhammad Bilal Zafar*

**Main category:** cs.CL

**Keywords:** Rote learning, LLMs, Generalization, Knowledge Injection, Fine-tuning

**Relevance Score:** 7

**TL;DR:** This paper demonstrates that LLMs can effectively generalize from rote memorized data using a two-phase memorization and fine-tuning approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of LLMs for generalization from rote memorization, challenging the belief that such techniques hinder deeper understanding.

**Method:** A two-phase framework where LLMs memorize factual data with a meaningless token and then fine-tune using semantically meaningful prompts.

**Key Contributions:**

	1. Introduction of a two-phase memorize-then-generalize framework
	2. Demonstration of LLMs reinterpreting memorized data
	3. Insights into knowledge injection and risks of misuse

**Result:** Experiments showed that LLMs can reinterpret rote memorized data through meaningful prompts, producing structured and semantically aligned representations.

**Limitations:** Further research needed to explore the implications of repurposing memorized data and generalization effectiveness across different contexts.

**Conclusion:** The findings suggest new possibilities for knowledge injection in LLMs and raise concerns about the misuse of memorized data.

**Abstract:** Rote learning is a memorization technique based on repetition. It is commonly believed to hinder generalization by encouraging verbatim memorization rather than deeper understanding. This insight holds for even learning factual knowledge that inevitably requires a certain degree of memorization. In this work, we demonstrate that LLMs can be trained to generalize from rote memorized data. We introduce a two-phase memorize-then-generalize framework, where the model first rote memorizes factual subject-object associations using a semantically meaningless token and then learns to generalize by fine-tuning on a small set of semantically meaningful prompts. Extensive experiments over 8 LLMs show that the models can reinterpret rote memorized data through the semantically meaningful prompts, as evidenced by the emergence of structured, semantically aligned latent representations between the two. This surprising finding opens the door to both effective and efficient knowledge injection and possible risks of repurposing the memorized data for malicious usage.

</details>


### [85] [Training language models to be warm and empathetic makes them less reliable and more sycophantic](https://arxiv.org/abs/2507.21919)

*Lujain Ibrahim, Franziska Sofia Hafner, Luc Rocher*

**Main category:** cs.CL

**Keywords:** language models, empathy, reliability, AI safety, user vulnerability

**Relevance Score:** 9

**TL;DR:** The paper investigates the trade-off between warmth and reliability in language models, demonstrating that more empathetic responses lead to increased error rates and unsafe outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of empathetic language model designs on their reliability and safety in critical tasks.

**Method:** Controlled experiments on five language models were conducted to compare their warm, empathetic responses to standard outputs, evaluating them on safety-critical tasks.

**Key Contributions:**

	1. Demonstrates a trade-off between model warmth and reliability
	2. Highlights significant error rates in empathetic models
	3. Calls for changes in AI development practices to ensure safety

**Result:** Warm language models showed a 10 to 30 percentage points increase in error rates, including the promotion of conspiracy theories and incorrect medical advice, particularly when responding to vulnerable users.

**Limitations:** The study focuses on controlled experiments, which may not fully capture real-world complexities.

**Conclusion:** The findings suggest a significant need to reevaluate how we design and evaluate empathetic AI systems to mitigate risks in user interactions and ensure reliability.

**Abstract:** Artificial intelligence (AI) developers are increasingly building language models with warm and empathetic personas that millions of people now use for advice, therapy, and companionship. Here, we show how this creates a significant trade-off: optimizing language models for warmth undermines their reliability, especially when users express vulnerability. We conducted controlled experiments on five language models of varying sizes and architectures, training them to produce warmer, more empathetic responses, then evaluating them on safety-critical tasks. Warm models showed substantially higher error rates (+10 to +30 percentage points) than their original counterparts, promoting conspiracy theories, providing incorrect factual information, and offering problematic medical advice. They were also significantly more likely to validate incorrect user beliefs, particularly when user messages expressed sadness. Importantly, these effects were consistent across different model architectures, and occurred despite preserved performance on standard benchmarks, revealing systematic risks that current evaluation practices may fail to detect. As human-like AI systems are deployed at an unprecedented scale, our findings indicate a need to rethink how we develop and oversee these systems that are reshaping human relationships and social interaction.

</details>


### [86] [Post-Training Large Language Models via Reinforcement Learning from Self-Feedback](https://arxiv.org/abs/2507.21931)

*Carel van Niekerk, Renato Vukovic, Benjamin Matthias Ruppik, Hsien-chin Lin, Milica Gašić*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Self-Feedback, Calibration, Reasoning

**Relevance Score:** 8

**TL;DR:** Introducing RLSF, a method that enhances LLM performance by using self-feedback from the model's confidence to improve reasoning and calibration without external inputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs produce unreliable answers in reasoning tasks due to poor calibration, necessitating methods for improvement without relying on human feedback.

**Method:** RLSF utilizes a frozen LLM to generate solutions and evaluate their confidence, employing these evaluated preferences to fine-tune the model in a reinforcement learning-like manner.

**Key Contributions:**

	1. Introduces a novel RLSF approach for LLM calibration.
	2. Demonstrates improved reasoning through self-feedback.
	3. Shows potential for data-efficient post-training methods.

**Result:** RLSF improves model calibration and strengthens reasoning capabilities, leading to better performance in arithmetic reasoning and question answering.

**Limitations:** 

**Conclusion:** RLSF demonstrates that self-feedback can be effectively used in LLM post-training, proposing an area for future research on intrinsic rewards.

**Abstract:** Large Language Models (LLMs) often produce plausible but poorly-calibrated answers, limiting their reliability on reasoning-intensive tasks. We present Reinforcement Learning from Self-Feedback (RLSF), a post-training stage that uses the model's own confidence as an intrinsic reward, mimicking how humans learn in the absence of external feedback. After a frozen LLM generates several chain-of-thought solutions, we define and compute the confidence of each final answer span and rank the traces accordingly. These synthetic preferences are then used to fine-tune the policy with standard preference optimization, similar to RLHF yet requiring no human labels, gold answers, or externally curated rewards.   RLSF simultaneously (i) refines the model's probability estimates -- restoring well-behaved calibration -- and (ii) strengthens step-by-step reasoning, yielding improved performance on arithmetic reasoning and multiple-choice question answering.   By turning a model's own uncertainty into useful self-feedback, RLSF affirms reinforcement learning on intrinsic model behaviour as a principled and data-efficient component of the LLM post-training pipeline and warrents further research in intrinsic rewards for LLM post-training.

</details>


### [87] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)

*Tianyi Hu, Andrea Morales-Garzón, Jingyi Zheng, Maria Maistro, Daniel Hershcovich*

**Main category:** cs.CL

**Keywords:** cross-cultural adaptation, recipe generation, diversity, Retrieval Augmented Generation, large language models

**Relevance Score:** 7

**TL;DR:** CARRIAGE enhances recipe adaptation by improving diversity in Retrieval Augmented Generation (RAG) processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure cultural appropriateness and provide diverse options for various dietary needs in cross-cultural recipe adaptation.

**Method:** The authors propose CARRIAGE, a plug-and-play RAG framework that increases diversity in recipe retrieval and contextual organization.

**Key Contributions:**

	1. Introduction of CARRIAGE framework for recipe adaptation
	2. Enhancement of diversity in RAG outputs
	3. Demonstration of Pareto efficiency in diversity and quality metrics.

**Result:** CARRIAGE demonstrates Pareto efficiency, achieving better diversity and quality in recipe adaptations than traditional closed-book LLMs.

**Limitations:** RAG's inherent reliance on context may still limit creativity in certain scenarios.

**Conclusion:** CARRIAGE is the first RAG framework designed specifically to generate diverse outputs for user preferences in recipe adaptation.

**Abstract:** In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.

</details>


### [88] [Predicting Microbial Ontology and Pathogen Risk from Environmental Metadata with Large Language Models](https://arxiv.org/abs/2507.21980)

*Hyunwoo Yoo, Gail L. Rosen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Microbiome Studies, Contamination Risk Prediction, Environmental Metadata, Biosurveillance

**Relevance Score:** 9

**TL;DR:** This paper explores the use of large language models (LLMs) for classifying microbial samples and predicting pathogen contamination risks using only environmental metadata, demonstrating their effectiveness over traditional machine learning models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional machine learning in generalizing across microbiome studies that have limited data and heterogeneous label formats.

**Method:** The study evaluates various LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets.

**Key Contributions:**

	1. Demonstrated LLMs' capability to outperform traditional machine learning models in classifying microbial samples using metadata only.
	2. Insights into the generalization abilities of LLMs across diverse microbiome datasets and risk prediction for contamination.
	3. Provided a framework for utilizing LLMs in environmental microbiology and biosurveillance.

**Result:** LLMs outperform baseline traditional models in ontology classification and predict contamination risk effectively, generalizing well across different sites and metadata distributions.

**Limitations:** 

**Conclusion:** LLMs offer a promising, metadata-only approach for environmental microbiology and biosurveillance applications, exceeding the performance of traditional methods.

**Abstract:** Traditional machine learning models struggle to generalize in microbiome studies where only metadata is available, especially in small-sample settings or across studies with heterogeneous label formats. In this work, we explore the use of large language models (LLMs) to classify microbial samples into ontology categories such as EMPO 3 and related biological labels, as well as to predict pathogen contamination risk, specifically the presence of E. Coli, using environmental metadata alone. We evaluate LLMs such as ChatGPT-4o, Claude 3.7 Sonnet, Grok-3, and LLaMA 4 in zero-shot and few-shot settings, comparing their performance against traditional models like Random Forests across multiple real-world datasets. Our results show that LLMs not only outperform baselines in ontology classification, but also demonstrate strong predictive ability for contamination risk, generalizing across sites and metadata distributions. These findings suggest that LLMs can effectively reason over sparse, heterogeneous biological metadata and offer a promising metadata-only approach for environmental microbiology and biosurveillance applications.

</details>


### [89] [DeepSieve: Information Sieving via LLM-as-a-Knowledge-Router](https://arxiv.org/abs/2507.22050)

*Minghao Guo, Qingcheng Zeng, Xujiang Zhao, Yanchi Liu, Wenchao Yu, Mengnan Du, Haifeng Chen, Wei Cheng*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Multi-hop QA

**Relevance Score:** 9

**TL;DR:** DeepSieve is a new Retrieval-Augmented Generation framework designed to improve LLMs' ability to handle knowledge-intensive queries effectively through structured sub-questions and multi-stage information sieving.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing RAG methods that struggle with dynamic knowledge access and often lead to noisy retrieval and shallow reasoning in LLMs.

**Method:** DeepSieve decomposes complex queries into structured sub-questions and routes them to the most suitable knowledge source, using a multi-stage distillation process to filter irrelevant information.

**Key Contributions:**

	1. Introduces DeepSieve framework to enhance LLM knowledge retrieval capabilities
	2. Implements structured query decomposition for improved routing to information sources
	3. Enhances modularity and adaptability in retrieval-augmented generation.

**Result:** Experiments show that DeepSieve outperforms conventional RAG approaches in terms of reasoning depth, retrieval precision, and interpretability on multi-hop QA tasks.

**Limitations:** 

**Conclusion:** DeepSieve enhances the capability of LLMs in knowledge-intensive tasks, making it more effective for applications requiring deep reasoning and accurate information retrieval.

**Abstract:** Large Language Models (LLMs) excel at many reasoning tasks but struggle with knowledge-intensive queries due to their inability to dynamically access up-to-date or domain-specific information. Retrieval-Augmented Generation (RAG) has emerged as a promising solution, enabling LLMs to ground their responses in external sources. However, existing RAG methods lack fine-grained control over both the query and source sides, often resulting in noisy retrieval and shallow reasoning. In this work, we introduce DeepSieve, an agentic RAG framework that incorporates information sieving via LLM-as-a-knowledge-router. DeepSieve decomposes complex queries into structured sub-questions and recursively routes each to the most suitable knowledge source, filtering irrelevant information through a multi-stage distillation process. Our design emphasizes modularity, transparency, and adaptability, leveraging recent advances in agentic system design. Experiments on multi-hop QA tasks across heterogeneous sources demonstrate improved reasoning depth, retrieval precision, and interpretability over conventional RAG approaches.

</details>


### [90] [The pitfalls of next-token prediction](https://arxiv.org/abs/2403.06963)

*Gregor Bachmann, Vaishnavh Nagarajan*

**Main category:** cs.CL

**Keywords:** next-token prediction, teacher-forcing, machine learning, human intelligence, teacherless training

**Relevance Score:** 8

**TL;DR:** This paper critiques next-token prediction in AI, discussing misconceptions and advocating for a multi-token objective to better model human intelligence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the ability of next-token predictors to truly model human intelligence and correct misconceptions regarding their effectiveness.

**Method:** The authors distinguish between autoregressive inference and teacher-forced training phases, identifying scenarios where teacher-forcing fails to learn an accurate next-token predictor. They introduce a minimal planning task to demonstrate this failure empirically.

**Key Contributions:**

	1. Clarification of the roles of autoregressive inference and teacher-forced training in next-token prediction
	2. Empirical demonstration of failure modes in popular architectures like Transformer and Mamba
	3. Introduction of teacherless training as a potential solution to identified problems.

**Result:** Evidence is provided that both the Transformer and Mamba architectures fail at this task, despite its simplicity. A novel approach using teacherless training shows potential in resolving the failures observed in teacher-forcing.

**Limitations:** The study primarily focuses on a specific minimal planning task; broader applicability needs further exploration.

**Conclusion:** The findings suggest directions for future research beyond the next-token prediction paradigm, highlighting the need for improved training methodologies in AI modeling.

**Abstract:** Can a mere next-token predictor faithfully model human intelligence? We crystallize this emerging concern and correct popular misconceptions surrounding it, and advocate a simple multi-token objective.   As a starting point, we argue that the two often-conflated phases of next-token prediction -- autoregressive inference and teacher-forced training -- must be treated distinctly. The popular criticism that errors can compound during autoregressive inference, crucially assumes that teacher-forcing has learned an accurate next-token predictor. This assumption sidesteps a more deep-rooted problem we expose: in certain classes of tasks, teacher-forcing can simply fail to learn an accurate next-token predictor in the first place. We describe a general mechanism of how teacher-forcing can fail, and design a minimal planning task where both the Transformer and the Mamba architecture empirically fail in that manner -- remarkably, despite the task being straightforward to learn.   Finally, we provide preliminary evidence that this failure can be resolved using _teacherless_ training, a simple modification using dummy tokens that predicts multiple tokens in advance. We hope this finding can ground future debates and inspire explorations beyond the next-token prediction paradigm. We make our code available under https://github.com/gregorbachmann/Next-Token-Failures

</details>


### [91] [Simulated patient systems are intelligent when powered by large language model-based AI agents](https://arxiv.org/abs/2409.18924)

*Huizi Yu, Jiayan Zhou, Lingyao Li, Shan Chen, Jack Gallifant, Anye Shi, Xiang Li, Jingxian He, Wenyue Hua, Mingyu Jin, Guang Chen, Yang Zhou, Zhao Li, Trisha Gupte, Ming-Li Chen, Zahra Azizi, Yongfeng Zhang, Yanqiu Xing, Themistocles L. Danielle S. Bitterman, Themistocles L. Assimes, Xin Ma, Lin Lu, Lizhou Fan*

**Main category:** cs.CL

**Keywords:** Simulated Patient Systems, Large Language Models, Medical Education

**Relevance Score:** 9

**TL;DR:** AIPatient is an intelligent simulated patient system that uses LLMs for enhanced accuracy and usability in medical education and decision-making.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a safe and integrative training environment for medical education and improve clinical decision-making simulations using AI.

**Method:** The system utilizes Retrieval Augmented Generation (RAG) framework with six task-specific LLM-based AI agents and a knowledge graph built from de-identified patient data.

**Key Contributions:**

	1. Integration of LLMs in simulated patient systems
	2. High QA accuracy surpassing benchmarks
	3. Demonstrated usability in medical training scenarios

**Result:** Achieved 94.15% QA accuracy with high readability scores and demonstrated strong usability and educational value in user studies with medical students.

**Limitations:** 

**Conclusion:** AIPatient has significant potential for enhancing medical education and various applications in healthcare.

**Abstract:** Simulated patient systems play an important role in modern medical education and research, providing safe, integrative medical training environments and supporting clinical decision-making simulations. We developed AIPatient, an intelligent simulated patient system powered by large language model-based AI agents. The system incorporates the Retrieval Augmented Generation (RAG) framework, powered by six task-specific LLM-based AI agents for complex reasoning. For simulation reality, the system is also powered by the AIPatient KG (Knowledge Graph), built with de-identified real patient data from the Medical Information Mart for Intensive Care (MIMIC)-III database. Primary outcomes showcase the system's intelligence, including the system's accuracy in Electronic Record (EHR)-based medical Question Answering (QA), readability, robustness, and stability. The system achieved a QA accuracy of 94.15% when all six AI agents present, surpassing benchmarks with partial or no agent integration. Its knowledgebase demonstrated high validity (F1 score=0.89). Readability scores showed median Flesch Reading Ease at 77.23 and median Flesch Kincaid Grade at 5.6, indicating accessibility to all medical professionals. Robustness and stability were confirmed with non-significant variance (ANOVA F-value=0.6126, p > 0.1; F-value=0.782, p > 0.1). A user study with medical students further demonstrated that AIPatient offers high fidelity, strong usability, and effective educational value, performing comparably or better than human-simulated patients in medical history-taking scenarios. The promising intelligence of the AIPatient system highlights its potential to support a wide range of applications, including medical education, model evaluation, and system integration.

</details>


### [92] [BIG5-CHAT: Shaping LLM Personalities Through Training on Human-Grounded Data](https://arxiv.org/abs/2410.16491)

*Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, Maarten Sap*

**Main category:** cs.CL

**Keywords:** human personality traits, LLMs, BIG5-CHAT, Supervised Fine-Tuning, Direct Preference Optimization

**Relevance Score:** 9

**TL;DR:** This paper presents BIG5-CHAT, a dataset for integrating human personality traits in large language models (LLMs) and explores training methods to better align LLMs with human personality expressions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve the realism and validity of LLMs' representations of human personality traits, which has been inadequately addressed in previous approaches reliant on prompting.

**Method:** Introduces the BIG5-CHAT dataset and explores Supervised Fine-Tuning and Direct Preference Optimization to enhance LLMs' alignment with human personality attributes.

**Key Contributions:**

	1. Introduction of the BIG5-CHAT dataset with 100,000 dialogues
	2. Demonstration of training-based methods outperforming traditional prompt-based methods
	3. Findings aligning personality traits with improved reasoning performance in LLMs.

**Result:** The proposed methods outperform existing prompting techniques in personality assessments and lead to models that demonstrate better reasoning performance when trained on certain personality traits.

**Limitations:** 

**Conclusion:** This work shows that training-based approaches can effectively shape LLM personalities, resulting in more realistic and valid personality trait expressions grounded in human behaviors.

**Abstract:** In this work, we tackle the challenge of embedding realistic human personality traits into LLMs. Previous approaches have primarily focused on prompt-based methods that describe the behavior associated with the desired personality traits, suffering from realism and validity issues. To address these limitations, we introduce BIG5-CHAT, a large-scale dataset containing 100,000 dialogues designed to ground models in how humans express their personality in language. Leveraging this dataset, we explore Supervised Fine-Tuning and Direct Preference Optimization as training-based methods to align LLMs more naturally with human personality patterns. Our methods outperform prompting on personality assessments such as BFI and IPIP-NEO, with trait correlations more closely matching human data. Furthermore, our experiments reveal that models trained to exhibit higher conscientiousness, higher agreeableness, lower extraversion, and lower neuroticism display better performance on reasoning tasks, aligning with psychological findings on how these traits impact human cognitive performance. To our knowledge, this work is the first comprehensive study to demonstrate how training-based methods can shape LLM personalities through learning from real human behaviors.

</details>


### [93] [Pralekha: Cross-Lingual Document Alignment for Indic Languages](https://arxiv.org/abs/2411.19096)

*Sanjay Suryanarayanan, Haiyue Song, Mohammed Safi Ur Rahman Khan, Anoop Kunchukuttan, Raj Dabre*

**Main category:** cs.CL

**Keywords:** Document-level machine translation, Cross-Lingual Document Alignment, Document Alignment Coefficient

**Relevance Score:** 5

**TL;DR:** PRALEKHA is a benchmark for document-level alignment in MT, particularly for Indic languages, introducing a new metric (DAC) that outperforms existing alignment methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve document-level machine translation (MT) by addressing the limitations of current Cross-Lingual Document Alignment (CLDA) techniques in low-resource settings.

**Method:** Introduction of PRALEKHA, a benchmark dataset with over 3 million aligned document pairs for evaluating document alignment techniques, and the Document Alignment Coefficient (DAC) for fine-grained alignment.

**Key Contributions:**

	1. Introduction of a large-scale benchmark (PRALEKHA) for document alignment in Indic languages.
	2. Development of Document Alignment Coefficient (DAC), enhancing alignment accuracy by focusing on smaller chunks.
	3. Demonstration of DAC's superior performance in both intrinsic and extrinsic evaluations for document MT.

**Result:** DAC showed significant improvements over pooling-based methods, particularly in noisy scenarios, and document MT models using DAC-aligned pairs performed better than those with traditional alignment methods.

**Limitations:** 

**Conclusion:** PRALEKHA and DAC provide valuable resources for improving document-level alignment in machine translation, particularly for low-resource languages.

**Abstract:** Mining parallel document pairs for document-level machine translation (MT) remains challenging due to the limitations of existing Cross-Lingual Document Alignment (CLDA) techniques. Most approaches rely on metadata such as URLs, which is often unavailable in low-resource language settings, while others represent documents using pooled sentence embeddings, which fail to capture fine-grained alignment cues. Moreover, current sentence embedding models have limited context windows, hindering their ability to represent document-level information effectively. To address these challenges for Indic languages, we introduce PRALEKHA, a large-scale benchmark for evaluating document-level alignment techniques. It contains over 3 million aligned document pairs across 11 Indic languages and English, of which 1.5 million are English--Indic pairs. Furthermore, we propose Document Alignment Coefficient (DAC), a novel metric for fine-grained document alignment. Unlike pooling-based approaches, DAC aligns documents by matching smaller chunks and computes similarity as the ratio of aligned chunks to the average number of chunks in a pair. Intrinsic evaluation shows that DAC achieves substantial improvements over pooling-based baselines, particularly in noisy scenarios. Extrinsic evaluation further demonstrates that document MT models trained on DAC-aligned pairs consistently outperform those using baseline alignment methods. These results highlight DAC's effectiveness for parallel document mining. The PRALEKHA dataset and CLDA evaluation framework will be made publicly available.

</details>


### [94] [Low-Confidence Gold: Refining Low-Confidence Samples for Efficient Instruction Tuning](https://arxiv.org/abs/2502.18978)

*Hongyi Cai, Jie Li, Mohammad Mahdinur Rahman, Wenzhen Dong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Instruction Fine-Tuning, Data Filtering

**Relevance Score:** 8

**TL;DR:** This paper presents Low-Confidence Gold (LCG), a filtering framework to enhance instruction fine-tuning of Large Language Models by selecting high-quality instruction pairs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of current datasets used for fine-tuning Large Language Models, focusing on improving quality and efficiency.

**Method:** The LCG framework utilizes centroid-based clustering and confidence-guided selection to curate instruction pairs, employing a semi-supervised approach with a lightweight classifier.

**Key Contributions:**

	1. Introduction of Low-Confidence Gold (LCG) framework for filtering instruction pairs.
	2. Use of centroid-based clustering and confidence-guided selection for efficient dataset curation.
	3. Demonstrated performance improvements on fine-tuning models with specific evaluation metrics.

**Result:** Models fine-tuned on LCG-filtered datasets of 6K samples demonstrated superior performance on MT-bench and other evaluation metrics.

**Limitations:** 

**Conclusion:** The LCG framework significantly enhances instruction tuning effectiveness while maintaining model performance, indicating a strong potential for further research in efficient instructional tuning methods.

**Abstract:** The effectiveness of instruction fine-tuning for Large Language Models is fundamentally constrained by the quality and efficiency of training datasets. This work introduces Low-Confidence Gold (LCG), a novel filtering framework that employs centroid-based clustering and confidence-guided selection for identifying valuable instruction pairs. Through a semi-supervised approach using a lightweight classifier trained on representative samples, LCG curates high-quality subsets while preserving data diversity. Experimental evaluation demonstrates that models fine-tuned on LCG-filtered subsets of 6K samples achieve superior performance compared to existing methods, with substantial improvements on MT-bench and consistent gains across comprehensive evaluation metrics. The framework's efficacy while maintaining model performance establishes a promising direction for efficient instruction tuning.

</details>


### [95] [Narrative Context Protocol: An Open-Source Storytelling Framework for Generative AI](https://arxiv.org/abs/2503.04844)

*Hank Gerba*

**Main category:** cs.CL

**Keywords:** Narrative Context Protocol, generative storytelling, AI-driven authoring

**Relevance Score:** 4

**TL;DR:** Introduction of Narrative Context Protocol (NCP) for narrative interoperability and AI-driven storytelling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable narrative portability and real-time emergent narratives in AI-driven authoring tools.

**Method:** Utilization of NCP as a structured register of narrative features for a year-long experiment with a custom authoring platform.

**Key Contributions:**

	1. Development of an open-source narrative standard (NCP)
	2. Demonstration of narrative interoperability in AI-driven authoring tools
	3. Implementation of a year-long experiment showcasing NCP's functionality

**Result:** An author created a playable, text-based experience that maintained narrative coherence while accommodating player agency.

**Limitations:** 

**Conclusion:** NCP effectively functions as guardrails for generative systems in storytelling, balancing player input with narrative structure.

**Abstract:** Here we introduce Narrative Context Protocol (NCP), an open-source narrative standard designed to enable narrative interoperability, AI-driven authoring tools, real-time emergent narratives, and more. By encoding a story's structure in a "Storyform," which is a structured register of its narrative features, NCP enables narrative portability across systems as well as intent-based constraints for generative storytelling systems. We demonstrate the capabilities of NCP through a year-long experiment, during which an author used NCP and a custom authoring platform to create a playable, text-based experience based on her pre-existing novella. This experience is driven by generative AI, with unconstrained natural language input. NCP functions as a set of "guardrails" that allows the generative system to accommodate player agency while also ensuring that narrative context and coherence are maintained.

</details>


### [96] [Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues](https://arxiv.org/abs/2503.06424)

*Alexander Scarlatos, Naiming Liu, Jaewook Lee, Richard Baraniuk, Andrew Lan*

**Main category:** cs.CL

**Keywords:** generative AI, large language models, tutoring, pedagogical practices, student learning

**Relevance Score:** 9

**TL;DR:** This paper presents a method for training large language models (LLMs) to generate more effective tutoring utterances that maximize student correctness while adhering to good pedagogical practices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of AI tutors by optimizing their responses to enhance student learning outcomes during tutoring sessions.

**Method:** The authors generate candidate tutor utterances, score them using an LLM-based student model to predict correct student responses, and evaluate them based on a pedagogical rubric assessed by GPT-4o. They train an open-source LLM, Llama 3.1 8B, using direct preference optimization.

**Key Contributions:**

	1. Introduction of a scoring system for tutor utterances based on student response likelihood and pedagogical quality.
	2. Training of Llama 3.1 8B to generate more effective tutor responses leveraging direct preference optimization.
	3. Demonstration of improved student outcomes through both quantitative metrics and qualitative evaluations.

**Result:** The tutor utterances generated by the proposed model significantly improve the likelihood of correct student responses while ensuring high pedagogical quality.

**Limitations:** 

**Conclusion:** The approach not only enhances the tutoring effectiveness of AI but also maintains quality standards, validated through qualitative analyses and human evaluations.

**Abstract:** Generative artificial intelligence (AI) has the potential to scale up personalized tutoring through large language models (LLMs). Recent AI tutors are adapted for the tutoring task by training or prompting LLMs to follow effective pedagogical principles, though they are not trained to maximize student learning throughout the course of a dialogue. Therefore, they may engage with students in a suboptimal way. We address this limitation by introducing an approach to train LLMs to generate tutor utterances that maximize the likelihood of student correctness, while still encouraging the model to follow good pedagogical practice. Specifically, we generate a set of candidate tutor utterances and score them using (1) an LLM-based student model to predict the chance of correct student responses and (2) a pedagogical rubric evaluated by GPT-4o. We then use the resulting data to train an open-source LLM, Llama 3.1 8B, using direct preference optimization. We show that tutor utterances generated by our model lead to significantly higher chances of correct student responses while maintaining the pedagogical quality of GPT-4o. We also conduct qualitative analyses and a human evaluation to demonstrate that our model generates high quality tutor utterances.

</details>


### [97] [Levels of Analysis for Large Language Models](https://arxiv.org/abs/2503.13401)

*Alexander Ku, Declan Campbell, Xuechunzi Bai, Jiayi Geng, Ryan Liu, Raja Marjieh, R. Thomas McCoy, Andrew Nam, Ilia Sucholutsky, Veniamin Veselovsky, Liyi Zhang, Jian-Qiao Zhu, Thomas L. Griffiths*

**Main category:** cs.CL

**Keywords:** Cognitive Science, Large Language Models, Marr's Levels of Analysis, AI Understanding, Information Processing

**Relevance Score:** 8

**TL;DR:** The paper proposes a framework for applying cognitive science methods to better understand large language models, drawing parallels to the study of the human mind.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need to understand the workings of modern AI systems, particularly large language models, due to their increasing complexity.

**Method:** The authors propose to use David Marr's levels of analysis as a framework for applying cognitive science techniques to study large language models.

**Key Contributions:**

	1. Introducing a cognitive science approach to analyze large language models
	2. Application of Marr's levels of analysis to AI systems
	3. Providing a toolkit for understanding AI behavior and organization

**Result:** By revisiting cognitive science methods suitable for each level of analysis, the framework shows potential for providing insights into how large language models behave and are organized internally.

**Limitations:** 

**Conclusion:** The proposed toolkit from cognitive science could enhance our understanding of large language models, offering clearer insights into their functioning.

**Abstract:** Modern artificial intelligence systems, such as large language models, are increasingly powerful but also increasingly hard to understand. Recognizing this problem as analogous to the historical difficulties in understanding the human mind, we argue that methods developed in cognitive science can be useful for understanding large language models. We propose a framework for applying these methods based on the levels of analysis that David Marr proposed for studying information processing systems. By revisiting established cognitive science techniques relevant to each level and illustrating their potential to yield insights into the behavior and internal organization of large language models, we aim to provide a toolkit for making sense of these new kinds of minds.

</details>


### [98] [EEG-CLIP : Learning EEG representations from natural language descriptions](https://arxiv.org/abs/2503.16531)

*Tidiane Camaret Ndir, Robin Tibor Schirrmeister, Tonio Ball*

**Main category:** cs.CL

**Keywords:** EEG decoding, contrastive learning, few-shot learning, zero-shot learning, medical text

**Relevance Score:** 7

**TL;DR:** Development of a contrastive learning framework, EEG-CLIP, for aligning EEG time series with clinical text descriptions, aimed at enhancing EEG decoding versatility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current deep networks for EEG decoding are often task-specific; this paper proposes a more general task-agnostic approach using contrastive learning to align EEG data with textual medical reports.

**Method:** We developed the EEG-CLIP framework which utilizes contrastive learning to create a shared embedding space for matching EEG signals with their corresponding clinical text descriptions.

**Key Contributions:**

	1. Introduction of EEG-CLIP for EEG and text alignment
	2. Evaluation in few-shot and zero-shot learning contexts
	3. Open-source implementation for reproducibility

**Result:** EEG-CLIP shows significant potential for versatile EEG decoding, performing well in few-shot and zero-shot settings by successfully aligning EEG and text representations.

**Limitations:** 

**Conclusion:** This method presents a promising route for developing general EEG representations and facilitates easier analysis across various decoding tasks.

**Abstract:** Deep networks for electroencephalogram (EEG) decoding are often only trained to solve one specific task, such as pathology or age decoding. A more general task-agnostic approach is to train deep networks to match a (clinical) EEG recording to its corresponding textual medical report and vice versa. This approach was pioneered in the computer vision domain matching images and their text captions and subsequently allowed to do successful zero-shot decoding using textual class prompts. In this work, we follow this approach and develop a contrastive learning framework, EEG-CLIP, that aligns the EEG time series and the descriptions of the corresponding clinical text in a shared embedding space. We investigated its potential for versatile EEG decoding, evaluating performance in a range of few-shot and zero-shot settings. Overall, we show that EEG-CLIP manages to non-trivially align text and EEG representations. Our work presents a promising approach to learn general EEG representations, which could enable easier analyses of diverse decoding questions through zero-shot decoding or training task-specific models from fewer training examples. The code for reproducing our results is available at https://github.com/tidiane-camaret/EEGClip

</details>


### [99] ["Whose Side Are You On?" Estimating Ideology of Political and News Content Using Large Language Models and Few-shot Demonstration Selection](https://arxiv.org/abs/2503.20797)

*Muhammad Haroon, Magdalena Wojcieszak, Anshuman Chhabra*

**Main category:** cs.CL

**Keywords:** Large Language Models, political ideology, in-context learning, social media, classification

**Relevance Score:** 8

**TL;DR:** This paper investigates the use of Large Language Models (LLMs) to classify the political ideology of online content, addressing limitations in existing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns around radicalization, filter bubbles, and content bias in social media, as well as to improve ideological classification methods.

**Method:** The paper employs in-context learning (ICL) with LLMs on three datasets of news articles and YouTube videos, utilizing demonstration selection in a label-balanced manner.

**Key Contributions:**

	1. Utilization of LLMs for ideological classification with in-context learning
	2. Demonstration of significant performance improvement over existing methods
	3. Analysis of metadata influence on classification outcomes

**Result:** The experiments show that the proposed approach significantly outperforms both zero-shot and traditional supervised methods for ideological classification.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of LLMs in ideological classification and the influence of metadata on this task, suggesting that the inclusion of source information can impact classification outcomes.

**Abstract:** The rapid growth of social media platforms has led to concerns about radicalization, filter bubbles, and content bias. Existing approaches to classifying ideology are limited in that they require extensive human effort, the labeling of large datasets, and are not able to adapt to evolving ideological contexts. This paper explores the potential of Large Language Models (LLMs) for classifying the political ideology of online content in the context of the two-party US political spectrum through in-context learning (ICL). Our extensive experiments involving demonstration selection in label-balanced fashion, conducted on three datasets comprising news articles and YouTube videos, reveal that our approach significantly outperforms zero-shot and traditional supervised methods. Additionally, we evaluate the influence of metadata (e.g., content source and descriptions) on ideological classification and discuss its implications. Finally, we show how providing the source for political and non-political content influences the LLM's classification.

</details>


### [100] [Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge](https://arxiv.org/abs/2504.00042)

*Agam Shah, Liqin Ye, Sebastian Jaskowski, Wei Xu, Sudheer Chava*

**Main category:** cs.CL

**Keywords:** Large Language Models, financial data, question-answering, machine learning, hallucination

**Relevance Score:** 8

**TL;DR:** This study evaluates the breadth of knowledge in Large Language Models (LLMs) using financial data from U.S. publicly traded companies, revealing their strengths and weaknesses in querying historical performance data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how well LLMs represent historical knowledge, especially in the context of financial data, and to understand the effects of company characteristics on their accuracy.

**Method:** The study analyzes over 197k questions posed to LLMs regarding the financial data of U.S. publicly traded companies and compares the responses to factual information.

**Key Contributions:**

	1. Evaluation of LLMs' knowledge breadth using a large dataset of financial information
	2. Insights into the role of company characteristics on LLM accuracy
	3. Identification of hallucination tendencies in LLMs when querying larger firms

**Result:** LLMs demonstrate limited knowledge of past financial performance but show greater accuracy for larger companies and more recent data, although they also tend to hallucinate more for these entities.

**Limitations:** Limited to financial data of U.S. publicly traded companies, may not represent other sectors or markets accurately.

**Conclusion:** LLMs are useful for retrieving knowledge about larger companies and recent events, but caution is needed due to potential inaccuracies, especially with historical data.

**Abstract:** Large Language Models (LLMs) are frequently utilized as sources of knowledge for question-answering. While it is known that LLMs may lack access to real-time data or newer data produced after the model's cutoff date, it is less clear how their knowledge spans across historical information. In this study, we assess the breadth of LLMs' knowledge using financial data of U.S. publicly traded companies by evaluating more than 197k questions and comparing model responses to factual data. We further explore the impact of company characteristics, such as size, retail investment, institutional attention, and readability of financial filings, on the accuracy of knowledge represented in LLMs. Our results reveal that LLMs are less informed about past financial performance, but they display a stronger awareness of larger companies and more recent information. Interestingly, at the same time, our analysis also reveals that LLMs are more likely to hallucinate for larger companies, especially for data from more recent years. The code, prompts, and model outputs are available on GitHub.

</details>


### [101] [My Life in Artificial Intelligence: People, anecdotes, and some lessons learnt](https://arxiv.org/abs/2504.04142)

*Kees van Deemter*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Natural Language Processing, Career Reflections

**Relevance Score:** 4

**TL;DR:** A personal reflection on 40 years of experience in AI, particularly in Natural Language Processing, sharing insights and anecdotes from industry and academia.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To share experiences and insights from a long career in AI, focusing on the impact of personal curiosity and external circumstances on career trajectories.

**Method:** A narrative biography outlining experiences across various countries and positions, supplemented by personal anecdotes.

**Key Contributions:**

	1. Personal anecdotes from a 40-year career in AI
	2. Insights on career choices in the field of Natural Language Processing
	3. Reflections on the evolution of AI and its impact on career paths

**Result:** The author highlights pivotal moments, choices, and influences throughout a four-decade career in AI, illustrating how these shaped their professional journey.

**Limitations:** 

**Conclusion:** AI is emerging as a significant field, and the author's experiences may guide younger colleagues in navigating their careers in this evolving landscape.

**Abstract:** In this very personal workography, I relate my 40-year experiences as a researcher and educator in and around Artificial Intelligence (AI), more specifically Natural Language Processing. I describe how curiosity, and the circumstances of the day, led me to work in both industry and academia, and in various countries, including The Netherlands (Amsterdam, Eindhoven, and Utrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and China (Beijing and Harbin). People and anecdotes play a large role in my story; the history of AI forms its backdrop. I focus on things that might be of interest to (even) younger colleagues, given the choices they face in their own work and life at a time when AI is finally emerging from the shadows.

</details>


### [102] [Probing then Editing Response Personality of Large Language Models](https://arxiv.org/abs/2504.10227)

*Tianjie Ju, Zhenyu Shao, Bowen Wang, Yujia Chen, Zhuosheng Zhang, Hao Fei, Mong-Li Lee, Wynne Hsu, Sufeng Duan, Gongshen Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personality Simulation, Layer-wise Probing, Personality Editing, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces a probing framework to investigate how Large Language Models (LLMs) simulate personality traits in their responses and proposes a method to edit those traits.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Despite LLMs' ability to simulate personality, little is known about how these traits are encoded within their parameters.

**Method:** A layer-wise probing framework is utilized to explore the capabilities of 11 open-source LLMs in simulating personality, alongside a layer-wise perturbation method for editing personality traits during inference.

**Key Contributions:**

	1. Introduced a layer-wise probing framework for analyzing LLM personality simulation.
	2. Developed a layer-wise perturbation method for editing personality in LLM responses.
	3. Demonstrated that editing personality incurs minimal performance costs.

**Result:** LLMs predominantly simulate personality in their middle and upper layers, with instruction-tuned models showing clearer separation of traits; the proposed personality editing method minimally impacts general capabilities and demonstrates successful personality alteration during inference.

**Limitations:** Focus on open-source LLMs may not generalize to proprietary models; the exploration is based only on the specified benchmark and may not capture all possible personality traits.

**Conclusion:** The proposed method for editing LLM personality traits is effective and does not significantly degrade performance, allowing for practical applications in human-computer interaction.

**Abstract:** Large Language Models (LLMs) have demonstrated promising capabilities to generate responses that simulate consistent personality traits. Despite the major attempts to analyze personality expression through output-based evaluations, little is known about how such traits are internally encoded within LLM parameters. In this paper, we introduce a layer-wise probing framework to systematically investigate the layer-wise capability of LLMs in simulating personality for responding. We conduct probing experiments on 11 open-source LLMs over the PersonalityEdit benchmark and find that LLMs predominantly simulate personality for responding in their middle and upper layers, with instruction-tuned models demonstrating a slightly clearer separation of personality traits. Furthermore, by interpreting the trained probing hyperplane as a layer-wise boundary for each personality category, we propose a layer-wise perturbation method to edit the personality expressed by LLMs during inference. Our results show that even when the prompt explicitly specifies a particular personality, our method can still successfully alter the response personality of LLMs. Interestingly, the difficulty of converting between certain personality traits varies substantially, which aligns with the representational distances in our probing experiments. Finally, we conduct a comprehensive MMLU benchmark evaluation and time overhead analysis, demonstrating that our proposed personality editing method incurs only minimal degradation in general capabilities while maintaining low training costs and acceptable inference latency. Our code is publicly available at https://github.com/universe-sky/probing-then-editing-personality.

</details>


### [103] [Ai2 Scholar QA: Organized Literature Synthesis with Attribution](https://arxiv.org/abs/2504.10861)

*Amanpreet Singh, Joseph Chee Chang, Chloe Anastasiades, Dany Haddad, Aakanksha Naik, Amber Tanaka, Angele Zamarron, Cecile Nguyen, Jena D. Hwang, Jason Dunkleberger, Matt Latzke, Smita Rao, Jaron Lochner, Rob Evans, Rodney Kinney, Daniel S. Weld, Doug Downey, Sergey Feldman*

**Main category:** cs.CL

**Keywords:** scientific question answering, open-source, retrieval-augmented generation, AI, benchmark

**Relevance Score:** 9

**TL;DR:** Ai2 Scholar QA is an open-source tool for scientific question answering that outperforms existing systems.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an effective and accessible solution for answering scientific questions from literature, circumventing the costs and limitations of existing closed-source systems.

**Method:** Developed as a customizable open-source Python package and interactive web app, with public APIs for paper indexes and downloadable datasets. Experiments were conducted to analyze key design decisions.

**Key Contributions:**

	1. Introduction of Ai2 Scholar QA as a free online scientific QA application
	2. Public availability of an open-source pipeline and datasets
	3. Performance improvement over existing scientific QA systems.

**Result:** Ai2 Scholar QA outperforms competing systems on a scientific QA benchmark.

**Limitations:** 

**Conclusion:** The system's open-source nature allows for further research and customization, potentially enhancing scientific inquiry capabilities.

**Abstract:** Retrieval-augmented generation is increasingly effective in answering scientific questions from literature, but many state-of-the-art systems are expensive and closed-source. We introduce Ai2 Scholar QA, a free online scientific question answering application. To facilitate research, we make our entire pipeline public: as a customizable open-source Python package and interactive web app, along with paper indexes accessible through public APIs and downloadable datasets. We describe our system in detail and present experiments analyzing its key design decisions. In an evaluation on a recent scientific QA benchmark, we find that Ai2 Scholar QA outperforms competing systems.

</details>
