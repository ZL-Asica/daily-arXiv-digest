# 2025-08-06

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 27]

- [cs.CL](#cs.CL) [Total: 63]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [LLM Agent-Based Simulation of Student Activities and Mental Health Using Smartphone Sensing Data](https://arxiv.org/abs/2508.02679)

*Wayupuk Sommuang, Kun Kerdthaisong, Pasin Buakhaw, Aslan B. Wong, Nutchanon Yongsatianchot*

**Main category:** cs.HC

**Keywords:** mental health, LLM agents, student life, sensing data, behavioral modeling

**Relevance Score:** 9

**TL;DR:** Proposes a novel LLM agent-based simulation framework to model student activities and mental health using sensing data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Students' mental well-being is crucial for academic success and influenced by various activities. The study aims to utilize advanced simulation frameworks to deepen the understanding of this relationship.

**Method:** Developed a simulation framework with LLM agents initialized with personality questionnaires, guided by smartphone sensing data to predict behaviors and self-report mental health data through EMAs.

**Key Contributions:**

	1. Introduction of a LLM agent-based simulation framework for student mental health analysis
	2. Use of smartphone sensing data for dynamic mental state management
	3. Capability to conduct intervention studies and peer interaction simulations

**Result:** The simulation explores different scenarios not present in the original dataset, investigates peer influence, and allows for intervention studies to enhance student well-being.

**Limitations:** The framework's accuracy is dependent on the quality of the sensing data and personality questionnaires.

**Conclusion:** The LLM-driven modeling framework provides insights into behavioral changes that can support student mental health, demonstrating the utility of integrating sensing data with LLMs.

**Abstract:** Students' mental well-being is vital for academic success, with activities such as studying, socializing, and sleeping playing a role. Current mobile sensing data highlight this intricate link using statistical and machine learning analyses. We propose a novel LLM agent-based simulation framework to model student activities and mental health using the StudentLife Dataset. Each LLM agent was initialized with personality questionnaires and guided by smartphone sensing data throughout the simulated semester. These agents predict individual behaviors, provide self-reported mental health data via ecological momentary assessments (EMAs), and complete follow-up personality questionnaires. To ensure accuracy, we investigated various prompting techniques, memory systems, and activity-based mental state management strategies that dynamically update an agent's mental state based on their daily activities. This simulation goes beyond simply replicating existing data. This allows us to explore new scenarios that are not present in the original dataset, such as peer influence through agent-to-agent interactions and the impact of social media. Furthermore, we can conduct intervention studies by manipulating activity patterns via sensing signals and personality traits using questionnaire responses. This provides valuable insights into the behavioral changes that could enhance student well-being. The framework also facilitates hypothetical interviews with LLM agents, offering deeper insights into their mental health. This study showcases the power of LLM-driven behavioral modeling with sensing data, opening new avenues for understanding and supporting student mental health.

</details>


### [2] [AnnoSense: A Framework for Physiological Emotion Data Collection in Everyday Settings for AI](https://arxiv.org/abs/2508.02680)

*Pragya Singh, Ankush Gupta, Mohan Kumar, Pushpendra Singh*

**Main category:** cs.HC

**Keywords:** emotion data, AI, well-being, AnnoSense, collecting emotions

**Relevance Score:** 8

**TL;DR:** This paper presents AnnoSense, a framework supporting everyday emotion data collection for AI, informed by insights from 119 stakeholders, including the public and mental health professionals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of collecting high-quality emotion data in real-world environments as AI algorithms require accurate annotations to be effective.

**Method:** The study employed surveys, interviews, and focus group discussions with a diverse group of 119 stakeholders to gather insights on emotion data collection challenges.

**Key Contributions:**

	1. Development of the AnnoSense framework for emotion data collection.
	2. Insights from a diverse range of stakeholders including mental health professionals.
	3. Evaluation of the framework by emotion AI experts.

**Result:** The evaluation of AnnoSense by 25 emotion AI experts indicated its clarity, usefulness, and adaptability for enhancing emotion data collection.

**Limitations:** 

**Conclusion:** The framework has potential implications for improving emotion AI research and data analysis in real-world settings.

**Abstract:** Emotional and mental well-being are vital components of quality of life, and with the rise of smart devices like smartphones, wearables, and artificial intelligence (AI), new opportunities for monitoring emotions in everyday settings have emerged. However, for AI algorithms to be effective, they require high-quality data and accurate annotations. As the focus shifts towards collecting emotion data in real-world environments to capture more authentic emotional experiences, the process of gathering emotion annotations has become increasingly complex. This work explores the challenges of everyday emotion data collection from the perspectives of key stakeholders. We collected 75 survey responses, performed 32 interviews with the public, and 3 focus group discussions (FGDs) with 12 mental health professionals. The insights gained from a total of 119 stakeholders informed the development of our framework, AnnoSense, designed to support everyday emotion data collection for AI. This framework was then evaluated by 25 emotion AI experts for its clarity, usefulness, and adaptability. Lastly, we discuss the potential next steps and implications of AnnoSense for future research in emotion AI, highlighting its potential to enhance the collection and analysis of emotion data in real-world contexts.

</details>


### [3] [Real-World Receptivity to Adaptive Mental Health Interventions: Findings from an In-the-Wild Study](https://arxiv.org/abs/2508.02817)

*Nilesh Kumar Sahu, Aditya Sneh, Snehil Gupta, Haroon R Lone*

**Main category:** cs.HC

**Keywords:** mobile health, mental health interventions, Just-in-Time Adaptive Interventions, user receptivity, reinforcement learning

**Relevance Score:** 9

**TL;DR:** This study explores user receptivity to mobile health interventions, focusing on acceptance and feasibility using passive smartphone data and an adaptive reinforcement learning approach.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how context influences engagement with mental health interventions and improve the effectiveness of Just-in-Time Adaptive Interventions (JITAIs).

**Method:** A two-week in-the-wild study with 70 students using a custom Android app, LogMe, to collect passive sensor data and active context reports for prompting mental health interventions. An adaptive module based on Thompson Sampling was used for intervention delivery.

**Key Contributions:**

	1. Investigated user receptivity components: acceptance and feasibility.
	2. Utilized passive smartphone data to enhance mental health interventions.
	3. Applied a reinforcement learning approach to optimize intervention delivery.

**Result:** Several types of passively sensed data significantly influenced user receptivity to mental health interventions, revealing insights into improving intervention design.

**Limitations:** 

**Conclusion:** Understanding user receptivity can lead to the development of more effective and actionable context-aware interventions in real-world settings.

**Abstract:** The rise of mobile health (mHealth) technologies has enabled real-time monitoring and intervention for mental health conditions using passively sensed smartphone data. Building on these capabilities, Just-in-Time Adaptive Interventions (JITAIs) seek to deliver personalized support at opportune moments, adapting to users' evolving contexts and needs. Although prior research has examined how context affects user responses to generic notifications and general mHealth messages, relatively little work has explored its influence on engagement with actual mental health interventions. Furthermore, while much of the existing research has focused on detecting when users might benefit from an intervention, less attention has been paid to understanding receptivity, i.e., users' willingness and ability to engage with and act upon the intervention.   In this study, we investigate user receptivity through two components: acceptance(acknowledging or engaging with a prompt) and feasibility (ability to act given situational constraints). We conducted a two-week in-the-wild study with 70 students using a custom Android app, LogMe, which collected passive sensor data and active context reports to prompt mental health interventions. The adaptive intervention module was built using Thompson Sampling, a reinforcement learning algorithm. We address four research questions relating smartphone features and self-reported contexts to acceptance and feasibility, and examine whether an adaptive reinforcement learning approach can optimize intervention delivery by maximizing a combined receptivity reward. Our results show that several types of passively sensed data significantly influenced user receptivity to interventions. Our findings contribute insights into the design of context-aware, adaptive interventions that are not only timely but also actionable in real-world settings.

</details>


### [4] [NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification](https://arxiv.org/abs/2508.02823)

*Wenshuo Zhang, Leixian Shen, Shuchang Xu, Jindu Wang, Jian Zhao, Huamin Qu, Linping Yuan*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Conversational LLMs, User Intent

**Relevance Score:** 9

**TL;DR:** This paper addresses the misalignment between user intent and code generated by conversational LLMs by proposing a new interaction paradigm called direct intent-task matching.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To solve the problem of users with limited programming experience facing frustration due to misalignment between their intents and the code generated by LLMs.

**Method:** The authors propose a new interaction paradigm that externalizes and enables direct manipulation of LLM understanding, implemented as NeuroSync, utilizing a knowledge distillation pipeline.

**Key Contributions:**

	1. Introduction of a novel human-LLM interaction paradigm
	2. Implementation of NeuroSync to visualize and edit LLM understanding
	3. Demonstration of enhanced coding efficiency through user study

**Result:** NeuroSync enhances intent-task alignment, reduces cognitive effort, and boosts coding efficiency, demonstrated through technical experiments and a user study with 12 participants.

**Limitations:** 

**Conclusion:** The proposed method effectively aligns users' intents with coding tasks, improving the overall usability of LLMs in coding scenarios.

**Abstract:** Conversational LLMs have been widely adopted by domain users with limited programming experience to solve domain problems. However, these users often face misalignment between their intent and generated code, resulting in frustration and rounds of clarification. This work first investigates the cause of this misalignment, which dues to bidirectional ambiguity: both user intents and coding tasks are inherently nonlinear, yet must be expressed and interpreted through linear prompts and code sequences. To address this, we propose direct intent-task matching, a new human-LLM interaction paradigm that externalizes and enables direct manipulation of the LLM understanding, i.e., the coding tasks and their relationships inferred by the LLM prior to code generation. As a proof-of-concept, this paradigm is then implemented in NeuroSync, which employs a knowledge distillation pipeline to extract LLM understanding, user intents, and their mappings, and enhances the alignment by allowing users to intuitively inspect and edit them via visualizations. We evaluate the algorithmic components of NeuroSync via technical experiments, and assess its overall usability and effectiveness via a user study (N=12). The results show that it enhances intent-task alignment, lowers cognitive effort, and improves coding efficiency.

</details>


### [5] [Critical Challenges in Content Moderation for People Who Use Drugs (PWUD): Insights into Online Harm Reduction Practices from Moderators](https://arxiv.org/abs/2508.02868)

*Kaixuan Wang, Loraine Clarke, Carl-Cyril J Dreue, Guancheng Zhou, Jason T. Jacques*

**Main category:** cs.HC

**Keywords:** Online communities, Moderation, People Who Use Drugs, Public health, Sociotechnical systems

**Relevance Score:** 8

**TL;DR:** This paper analyzes the challenges faced by moderators in online communities for People Who Use Drugs, proposing improvements to sociotechnical systems that support their work.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** Moderation in online communities for People Who Use Drugs (PWUD) is critical for ensuring member safety and providing support, yet current systems inadequately assist moderators in addressing unique challenges.

**Method:** Interviews with experienced moderators from PWUD forums on Reddit were conducted to understand their experiences and the challenges in moderation.

**Key Contributions:**

	1. Identification of unique moderation challenges in PWUD forums
	2. Critique of current moderation systems and policies
	3. Proposals for improved sociotechnical design to support moderators

**Result:** The study identifies three main challenges in moderation: specialized risk assessment, time-critical crisis response, and conflicts between platform policies and community safety goals. Current moderation systems fail to adequately support these needs.

**Limitations:** 

**Conclusion:** The paper calls for the design of sociotechnical systems that automate support for human decision-making and reduce the necessity for moderators to engage in low-level programming, to improve health outcomes for PWUD communities.

**Abstract:** Online communities serve as essential support channels for People Who Use Drugs (PWUD), providing access to peer support and harm reduction information. The moderation of these communities involves consequential decisions affecting member safety, yet existing sociotechnical systems provide insufficient support for moderators. Through interviews with experienced moderators from PWUD forums on Reddit, we analyse the unique nature of this work. We argue that this work constitutes a distinct form of public health intervention characterised by three moderation challenges: the need for specialised, expert risk assessment; time-critical crisis response; and the navigation of a structural conflict between platform policies and community safety goals. We demonstrate how current moderation systems are insufficient in supporting PWUD communities. For example, policies minimising platforms' legal exposure to illicit activities can inadvertently push moderators to implement restrictive rules to protect community's existence, which can limit such a vulnerable group's ability to share potentially life-saving resources online. We conclude by identifying two necessary shifts in sociotechnical design to support moderators' work: first, moving to automated tools that support human sensemaking in contexts with competing interests; and second, shifting from systems that require moderators to perform low-level rule programming to those that enable high-level, example-based instruction. Further, we highlight how the design of sociotechnical systems in online spaces could impact harm reduction efforts aimed at improving health outcomes for PWUD communities.

</details>


### [6] [VRSight: An AI-Driven Scene Description System to Improve Virtual Reality Accessibility for Blind People](https://arxiv.org/abs/2508.02958)

*Daniel Killough, Justin Feng, Zheng Xue "ZX" Ching, Daniel Wang, Rithvik Dyava, Yapeng Tian, Yuhang Zhao*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Accessibility, AI, Blind Users, Spatial Audio

**Relevance Score:** 9

**TL;DR:** VRSight enhances VR accessibility for blind users through AI-driven audio feedback and a dedicated VR dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Most VR applications remain inaccessible to blind users due to the lack of integrated accessibility features and industry neglect.

**Method:** VRSight uses AI models for object detection, depth estimation, and atmosphere interpretation to generate spatial audio feedback in VR, along with the DISCOVR dataset featuring 30 virtual object classes from 17 social VR apps.

**Key Contributions:**

	1. Introduction of VRSight for VR accessibility
	2. Development of DISCOVR dataset for virtual object detection
	3. Effective implementation of spatial audio feedback for blind users.

**Result:** Nine participants effectively used VRSight to explore a VR app, demonstrating improved social task performance such as avatar awareness and seat identification.

**Limitations:** The study involves a limited participant sample and focuses on a specific VR app, which may not generalize to all VR environments.

**Conclusion:** VRSight enables blind users to interact with VR environments without requiring changes from app developers, addressing a significant gap in VR accessibility.

**Abstract:** Virtual Reality (VR) is inaccessible to blind people. While research has investigated many techniques to enhance VR accessibility, they require additional developer effort to integrate. As such, most mainstream VR apps remain inaccessible as the industry de-prioritizes accessibility. We present VRSight, an end-to-end system that recognizes VR scenes post hoc through a set of AI models (e.g., object detection, depth estimation, LLM-based atmosphere interpretation) and generates tone-based, spatial audio feedback, empowering blind users to interact in VR without developer intervention. To enable virtual element detection, we further contribute DISCOVR, a VR dataset consisting of 30 virtual object classes from 17 social VR apps, substituting real-world datasets that remain not applicable to VR contexts. Nine participants used VRSight to explore an off-the-shelf VR app (Rec Room), demonstrating its effectiveness in facilitating social tasks like avatar awareness and available seat identification.

</details>


### [7] [Survey of Large Language Models in Extended Reality: Technical Paradigms and Application Frontiers](https://arxiv.org/abs/2508.03014)

*Jingyan Wang, Yang Zhao, Haotian Mao, Xubo Yang*

**Main category:** cs.HC

**Keywords:** Large Language Models, Extended Reality, immersive education, clinical healthcare, industrial manufacturing

**Relevance Score:** 9

**TL;DR:** This survey reviews the intersection of Large Language Models (LLMs) and Extended Reality (XR), proposing a taxonomy and discussing applications in various domains.

**Read time:** 60 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the integration of LLMs with XR can transform user interactions in immersive environments and guide research and practical applications in this field.

**Method:** A comprehensive survey structured along technical and application dimensions, proposing a taxonomy of LLM-enhanced XR systems and discussing both technical paradigms and practical applications.

**Key Contributions:**

	1. Proposes a taxonomy of LLM-enhanced XR systems.
	2. Connects technical paradigms with practical application areas.
	3. Identifies open challenges in LLM and XR integration.

**Result:** The survey identifies key technical paradigms such as interactive agent control, XR development toolkits, and generative scene synthesis and demonstrates their applications in immersive education, clinical healthcare, and industrial manufacturing.

**Limitations:** The survey primarily focuses on existing literature and may not cover all emerging trends or technologies in LLM and XR.

**Conclusion:** The work highlights current trends, design considerations, and open challenges in the development of LLM-augmented XR systems, providing insights for future research and applications.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation, and their integration with Extended Reality (XR) is poised to transform how users interact with immersive environments. This survey provides a comprehensive review of recent developments at the intersection of LLMs and XR, offering a structured organization of research along both technical and application dimensions. We propose a taxonomy of LLM-enhanced XR systems centered on key technical paradigms -- such as interactive agent control, XR development toolkits, and generative scene synthesis -- and discuss how these paradigms enable novel capabilities in XR. In parallel, we examine how LLM-driven techniques support practical XR applications across diverse domains, including immersive education, clinical healthcare, and industrial manufacturing. By connecting these technical paradigms with application frontiers, our survey highlights current trends, delineates design considerations, and identifies open challenges in building LLM-augmented XR systems. This work provides insights that can guide researchers and practitioners in advancing the state of the art in intelligent XR experiences.

</details>


### [8] [Facilitating Visual Media Exploration for Blind and Low Vision Users through AI-Powered Interactive Storytelling](https://arxiv.org/abs/2508.03061)

*Shuchang Xu*

**Main category:** cs.HC

**Keywords:** AI, interactive storytelling, blind users, visual media, narrative techniques

**Relevance Score:** 8

**TL;DR:** This research introduces AI-powered interactive storytelling to enhance the experience of blind and low vision users in exploring visual media.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve content comprehension, strengthen user agency, and fulfill diverse information needs for blind and low vision users while addressing existing limitations in visual media exploration tools.

**Method:** The research operationalizes AI-powered interactive storytelling through three techniques: Hierarchical Narrative, Parallel Narrative, and Branching Narrative, facilitating varied exploration of media formats.

**Key Contributions:**

	1. Introduction of AI-powered interactive storytelling paradigm for BLV users
	2. Development of three unique narrative techniques: Hierarchical, Parallel, and Branching Narratives
	3. Demonstration of improved engagement and accessibility in visual media for BLV users

**Result:** The techniques demonstrate the efficacy of AI-powered interactive storytelling in balancing user agency with narrative coherence across different types of media.

**Limitations:** 

**Conclusion:** Future work aims to develop more personalized and expressive storytelling experiences for blind and low vision audiences.

**Abstract:** Empowering blind and low vision (BLV) users to explore visual media improves content comprehension, strengthens user agency, and fulfills diverse information needs. However, most existing tools separate exploration from the main narration, which disrupts the narrative flow, increases cognitive load, and limits deep engagement with visual media. To address these challenges, my PhD research introduces the paradigm of AI-powered interactive storytelling, which leverages AI to generate interactive narratives, enabling BLV users to explore visual media within a coherent storytelling experience. I have operationalized this paradigm through three techniques: (1) Hierarchical Narrative, which supports photo-collection exploration at different levels of detail; (2) Parallel Narrative, which provides seamless access to time-synced video comments; and (3) Branching Narrative, which enables immersive navigation of 360{\deg} videos. Together, these techniques demonstrate that AI-powered interactive storytelling can effectively balance user agency with narrative coherence across diverse media formats. My future work will advance this paradigm by enabling more personalized and expressive storytelling experiences for BLV audiences.

</details>


### [9] [StoryEnsemble: Enabling Dynamic Exploration & Iteration in the Design Process with AI and Forward-Backward Propagation](https://arxiv.org/abs/2508.03182)

*Sangho Suh, Michael Lai, Kevin Pu, Steven P. Dow, Tovi Grossman*

**Main category:** cs.HC

**Keywords:** Design processes, Human-Computer Interaction, AI integration

**Relevance Score:** 8

**TL;DR:** This paper presents StoryEnsemble, an AI-integrated design tool that facilitates iterative design processes through a node-link interface, allowing for dynamic exploration and navigation across various design stages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by designers due to time and resource constraints, which limit exploration and feedback collection in the design process.

**Method:** A formative study with 15 design practitioners to understand the challenges in design processes, followed by the development and user testing of StoryEnsemble with 10 participants.

**Key Contributions:**

	1. Introduction of the StoryEnsemble tool that leverages AI for design iteration
	2. Empirical insights from user studies regarding AI's impact on design processes
	3. Demonstrated novel interactions for design exploration and iteration

**Result:** User study results showed that StoryEnsemble supports rapid, multi-directional iteration and allows flexible navigation across design stages, enhancing the design process.

**Limitations:** 

**Conclusion:** The introduction of StoryEnsemble advances the understanding of AI's role in improving iterative design practices by enabling more fluid and engaging exploration.

**Abstract:** Design processes involve exploration, iteration, and movement across interconnected stages such as persona creation, problem framing, solution ideation, and prototyping. However, time and resource constraints often hinder designers from exploring broadly, collecting feedback, and revisiting earlier assumptions-making it difficult to uphold core design principles in practice. To better understand these challenges, we conducted a formative study with 15 participants-comprised of UX practitioners, students, and instructors. Based on the findings, we developed StoryEnsemble, a tool that integrates AI into a node-link interface and leverages forward and backward propagation to support dynamic exploration and iteration across the design process. A user study with 10 participants showed that StoryEnsemble enables rapid, multi-directional iteration and flexible navigation across design stages. This work advances our understanding of how AI can foster more iterative design practices by introducing novel interactions that make exploration and iteration more fluid, accessible, and engaging.

</details>


### [10] [Navigation Pixie: Implementation and Empirical Study Toward On-demand Navigation Agents in Commercial Metaverse](https://arxiv.org/abs/2508.03216)

*Hikari Yanagawa, Yuichi Hiroi, Satomi Tokida, Yuji Hatada, Takefumi Hiraki*

**Main category:** cs.HC

**Keywords:** Navigation, Metaverse, User Experience, LLM, Virtual Reality

**Relevance Score:** 7

**TL;DR:** Navigation Pixie is an on-demand navigation agent for metaverse platforms that enhances user exploration through adaptive navigation assistance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Commercial metaverse platforms lack effective navigation tools that adapt to user interests and intentions in dynamic environments.

**Method:** Implemented Navigation Pixie using a loosely coupled architecture to integrate spatial metadata with LLM-based NLP, tested in cross-platform settings.

**Key Contributions:**

	1. Integration of spatial metadata with LLM-based NLP for navigation
	2. Cross-platform evaluation methodologies for navigation agents
	3. Empirical experimentation frameworks tailored for commercial metaverse platforms

**Result:** Significant increases in user dwell time and exploration were observed compared to static navigation conditions across PC and VR platforms.

**Limitations:** Dependent on platform configurations and user context for effectiveness.

**Conclusion:** Navigation Pixie shows promise in improving VR interaction design and offers frameworks for empirical experimentation in metaverse environments.

**Abstract:** While commercial metaverse platforms offer diverse user-generated content, they lack effective navigation assistance that can dynamically adapt to users' interests and intentions. Although previous research has investigated on-demand agents in controlled environments, implementation in commercial settings with diverse world configurations and platform constraints remains challenging.   We present Navigation Pixie, an on-demand navigation agent employing a loosely coupled architecture that integrates structured spatial metadata with LLM-based natural language processing while minimizing platform dependencies, which enables experiments on the extensive user base of commercial metaverse platforms. Our cross-platform experiments on commercial metaverse platform Cluster with 99 PC client and 94 VR-HMD participants demonstrated that Navigation Pixie significantly increased dwell time and free exploration compared to fixed-route and no-agent conditions across both platforms. Subjective evaluations revealed consistent on-demand preferences in PC environments versus context-dependent social perception advantages in VR-HMD. This research contributes to advancing VR interaction design through conversational spatial navigation agents, establishes cross-platform evaluation methodologies revealing environment-dependent effectiveness, and demonstrates empirical experimentation frameworks for commercial metaverse platforms.

</details>


### [11] [Quo-Vadis Multi-Agent Automotive Research? Insights from a Participatory Workshop and Questionnaire](https://arxiv.org/abs/2508.03281)

*Pavlo Bazilinskyy, Francesco Walker, Debargha Dey, Tram Thi Minh Tran, Hyungchai Park, Hyochang Kim, Hyunmin Kang, Patrick Ebel*

**Main category:** cs.HC

**Keywords:** multi-agent systems, automotive research, human-centered design, interdisciplinary methods, simulation environments

**Relevance Score:** 7

**TL;DR:** The paper discusses the challenges and opportunities in multi-agent automotive research through a participatory workshop and questionnaire.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the transition to mixed-traffic environments with automated and manually operated vehicles, emphasizing the lack of research in multi-agent interactions despite its recognition as valuable.

**Method:** Conducted a participatory workshop with 15 participants and a questionnaire with 19 participants during the AutomotiveUI '24 conference.

**Key Contributions:**

	1. Exploration of multi-agent automotive research through participatory engagement.
	2. Identification of practical and technical barriers in implementing multi-agent approaches.
	3. Emphasis on the need for scalable and ethically informed simulation environments.

**Result:** Identified barriers to implementing multi-agent approaches, with a recognition of their value in automotive research.

**Limitations:** 

**Conclusion:** Calls for interdisciplinary methods, better tools, and simulation environments to advance multi-agent research in automotive settings.

**Abstract:** The transition to mixed-traffic environments that involve automated vehicles, manually operated vehicles, and vulnerable road users presents new challenges for human-centered automotive research. Despite this, most studies in the domain focus on single-agent interactions. This paper reports on a participatory workshop (N = 15) and a questionnaire (N = 19) conducted during the AutomotiveUI '24 conference to explore the state of multi-agent automotive research. The participants discussed methodological challenges and opportunities in real-world settings, simulations, and computational modeling. Key findings reveal that while the value of multi-agent approaches is widely recognized, practical and technical barriers hinder their implementation. The study highlights the need for interdisciplinary methods, better tools, and simulation environments that support scalable, realistic, and ethically informed multi-agent research.

</details>


### [12] [Enhancing Joint Human-AI Inference in Robot Missions: A Confidence-Based Approach](https://arxiv.org/abs/2508.03293)

*Duc-An Nguyen, Clara Colombatto, Steve Fleming, Ingmar Posner, Nick Hawes, Raunak Bhattacharyya*

**Main category:** cs.HC

**Keywords:** human-AI inference, robot teleoperation, confidence calibration, AI decision support systems, user study

**Relevance Score:** 7

**TL;DR:** This study explores joint human-AI inference in robot teleoperation, showing that the accuracy of decisions improves when selecting the inference with higher confidence, highlighting the critical role of AI confidence calibration.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve outcomes in human-supervised robot missions by investigating joint human-AI inference mechanisms, particularly in relation to confidence calibration of AI recommendations.

**Method:** A user study was conducted with 100 participants engaging in a simulated robot teleoperation task, focusing on the inference of robots' control delays under varying AI confidence levels.

**Key Contributions:**

	1. Introduction of a maximum-confidence-based heuristic for joint human-AI inference
	2. Demonstration of the importance of AI confidence calibration on decision-making
	3. Evidence that poor calibration of AI can detrimentally affect team performance.

**Result:** The results indicated that joint inference accuracy improved with better AI confidence calibration, and human inferences were influenced by AI recommendations, also affected by AI's confidence sensitivity.

**Limitations:** 

**Conclusion:** Well-calibrated AI decision support systems are crucial for enhancing performance in joint inference tasks, as poorly-calibrated systems can negatively impact outcomes.

**Abstract:** Joint human-AI inference holds immense potential to improve outcomes in human-supervised robot missions. Current day missions are generally in the AI-assisted setting, where the human operator makes the final inference based on the AI recommendation. However, due to failures in human judgement on when to accept or reject the AI recommendation, complementarity is rarely achieved. We investigate joint human-AI inference where the inference made with higher confidence is selected. Through a user study with N=100 participants on a representative simulated robot teleoperation task, specifically studying the inference of robots' control delays we show that: a) Joint inference accuracy is higher and its extent is regulated by the confidence calibration of the AI agent, and b) Humans change their inferences based on AI recommendations and the extent and direction of this change is also regulated by the confidence calibration of the AI agent. Interestingly, our results show that pairing poorly-calibrated AI-DSS with humans hurts performance instead of helping the team, reiterating the need for AI-based decision support systems with good metacognitive sensitivity. To the best of our knowledge, our study presents the first application of a maximum-confidence-based heuristic for joint human-AI inference within a simulated robot teleoperation task.

</details>


### [13] [Remini: Leveraging Chatbot-Mediated Mutual Reminiscence for Promoting Positive Affect and Feeling of Connectedness among Loved Ones](https://arxiv.org/abs/2508.03355)

*Zhuoqun Jiang, ShunYi Yeo, Wei Xuan Donovan Seow, Simon Perrault*

**Main category:** cs.HC

**Keywords:** chatbot, mutual reminiscence, social functions of autobiographical memory, emotional bonding, human-computer interaction

**Relevance Score:** 7

**TL;DR:** The paper introduces Remini, a chatbot that facilitates mutual reminiscence among close partners, improving emotional bonds and well-being through guided interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current reminiscence tools focus on individual reflections, neglecting the importance of mutual, interactive dialogue for meaningful connections.

**Method:** A mixed-method study with 48 participants (24 dyads) comparing Remini to a baseline chatbot featuring minimal prompts, assessing emotional outcomes and engagement.

**Key Contributions:**

	1. Introduction of Remini chatbot for mutual reminiscence
	2. Empirical evidence of enhanced emotional outcomes
	3. Design implications for conversational agents in HCI

**Result:** Remini significantly enhances positive affect, feelings of connection, and engagement, leading to detailed narrative co-construction and increased reciprocal self-disclosure.

**Limitations:** 

**Conclusion:** The chatbot demonstrates the potential to strengthen human connections through structured reminiscence prompts, providing valuable insights for designing conversational agents.

**Abstract:** Mutual reminiscence, defined as revisiting shared positive memories through reciprocal self-disclosure, strengthens emotional bonds, enhances well-being, and deepens intimacy. However, most technology-mediated reminiscence tools emphasize individual reflection or one-way storytelling, which overlooks the dynamic, interactive dialogue essential for meaningful mutual reminiscence. To address this limitation, we introduce Remini, a chatbot designed to support reciprocal self-disclosure between close partners such as couples, friends, or family members. Grounded in the Social Functions of Autobiographical Memory (SFAM) framework, Remini uses conversational AI to guide emotionally rich exchanges through five narrative phases: rapport building, memory narration, elaboration, reflection, and summary. In a mixed-method, both between- and within- subjects study (N = 48, 24 dyads), we compare Remini to a baseline chatbot that offers minimal memory-trigger prompts. Our findings show that structured guidance from Remini significantly improves positive affect, feeling of connection, and engagement. It also fosters more detailed narrative co-construction and greater reciprocal self-disclosure. Participant feedback highlights the practical value, perceived benefits, and design considerations of chatbot-mediated reminiscence. We contribute empirically grounded design implications for conversational agents that strengthen human connection through mutual reminiscence.

</details>


### [14] [The Science Fiction Science Method](https://arxiv.org/abs/2508.03430)

*Iyad Rahwan, Azim Shariff, Jean-François Bonnefon*

**Main category:** cs.HC

**Keywords:** science fiction, experimental methods, future technologies, behavioral impact, validity threats

**Relevance Score:** 4

**TL;DR:** This paper proposes 'science fiction science', a method using experimental simulation of future technologies to quantitatively assess their social and behavioral impacts, while addressing validity concerns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To predict the social and behavioral impact of future technologies before their actualization, guiding their development and regulation.

**Method:** The authors introduce experimental methods that simulate future technologies and collect quantitative measures from participants under controlled scenarios.

**Key Contributions:**

	1. Introduction of 'science fiction science', a novel experimental approach.
	2. Addressing validity threats associated with predicting future technology impacts.
	3. Discussion of immersive methods for engaging diverse audiences in technology simulation.

**Result:** The proposed method provides a framework to quantitatively explore the impacts of technologies that are not yet realized, which has been largely neglected in favor of qualitative approaches.

**Limitations:** The method may not be applicable to all technology types and could encounter skepticism within the scientific community.

**Conclusion:** Normalizing the use of unconventional methods in science fiction science could enhance societal engagement with emerging technologies and improve validity in predictions.

**Abstract:** Predicting the social and behavioral impact of future technologies, before they are achieved, would allow us to guide their development and regulation before these impacts get entrenched. Traditionally, this prediction has relied on qualitative, narrative methods. Here we describe a method which uses experimental methods to simulate future technologies, and collect quantitative measures of the attitudes and behaviors of participants assigned to controlled variations of the future. We call this method 'science fiction science'. We suggest that the reason why this method has not been fully embraced yet, despite its potential benefits, is that experimental scientists may be reluctant to engage in work facing such serious validity threats as science fiction science. To address these threats, we consider possible constraints on the kind of technology that science fiction science may study, as well as the unconventional, immersive methods that science fiction science may require. We seek to provide perspective on the reasons why this method has been marginalized for so long, what benefits it would bring if it could be built on strong yet unusual methods, and how we can normalize these methods to help the diverse community of science fiction scientists to engage in a virtuous cycle of validity improvement.

</details>


### [15] [Guided Reality: Generating Visually-Enriched AR Task Guidance with LLMs and Vision Models](https://arxiv.org/abs/2508.03547)

*Ada Yi Zhao, Aditya Gunturu, Ellen Yi-Luen Do, Ryo Suzuki*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Large Language Models, Visual Guidance, Human-Computer Interaction, Task Execution

**Relevance Score:** 9

**TL;DR:** Guided Reality is an AR system that enhances LLM-generated instructions with visual guidance integrated into the user's physical environment.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM-based AR guidance lacks effective visual augmentations, which are necessary for better user understanding of physical tasks.

**Method:** The system uses LLMs and vision models to generate instructions, identify visual guidance types, extract spatial information, and embed visual help in real-world contexts.

**Key Contributions:**

	1. Automated generation of visual guidance from LLMs
	2. Identification strategy for visual guidance based on task steps
	3. Embedding of guidance in physical space for real-world task execution.

**Result:** User studies showed that Guided Reality effectively supports task execution by providing contextually embedded visual guidance based on step-by-step instructions.

**Limitations:** Limited user study size (N=16); further research needed to explore broader applications.

**Conclusion:** Guided Reality can potentially enhance training workflows and improve user interactions with physical tasks in augmented reality settings.

**Abstract:** Large language models (LLMs) have enabled the automatic generation of step-by-step augmented reality (AR) instructions for a wide range of physical tasks. However, existing LLM-based AR guidance often lacks rich visual augmentations to effectively embed instructions into spatial context for a better user understanding. We present Guided Reality, a fully automated AR system that generates embedded and dynamic visual guidance based on step-by-step instructions. Our system integrates LLMs and vision models to: 1) generate multi-step instructions from user queries, 2) identify appropriate types of visual guidance, 3) extract spatial information about key interaction points in the real world, and 4) embed visual guidance in physical space to support task execution. Drawing from a corpus of user manuals, we define five categories of visual guidance and propose an identification strategy based on the current step. We evaluate the system through a user study (N=16), completing real-world tasks and exploring the system in the wild. Additionally, four instructors shared insights on how Guided Reality could be integrated into their training workflows.

</details>


### [16] [SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides](https://arxiv.org/abs/2508.03630)

*Zhuohao Jerry Zhang, Ruiqi Chen, Mingyuan Zhong, Jacob O. Wobbrock*

**Main category:** cs.HC

**Keywords:** slide evaluation, AI design critique, taxonomy, crowdsourcing, presentation design

**Relevance Score:** 4

**TL;DR:** The paper introduces SlideAudit, a dataset aimed at the automated evaluation of presentation slides, revealing the challenges AI faces in identifying design flaws.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Automated evaluation of graphic designs, specifically presentation slides, remains an open problem, necessitating robust datasets and methodologies for assessment.

**Method:** Created SlideAudit, a dataset of 2400 slides annotated for design flaws using a carefully developed taxonomy and trained crowdsourcing; tested various large language models on their ability to identify these flaws.

**Key Contributions:**

	1. Introduction of SlideAudit dataset for slide evaluation
	2. Development of a comprehensive taxonomy for slide design flaws
	3. Demonstrated effectiveness of taxonomy in AI remediation studies

**Result:** AI models showed limited accuracy with F1 scores between 0.331 and 0.655; however, improved prompting strategies using the taxonomy yielded better results, and a remediation study indicated high potential for improving slide quality.

**Limitations:** AI models still struggle significantly with accurately identifying flaws despite the dataset and taxonomy; additional methods may be required to enhance performance further.

**Conclusion:** The introduction of SlideAudit and its taxonomy highlights AI's limitations in design evaluation but suggests strategies for improvement; the dataset serves as a valuable resource for future research in design automation.

**Abstract:** Automated evaluation of specific graphic designs like presentation slides is an open problem. We present SlideAudit, a dataset for automated slide evaluation. We collaborated with design experts to develop a thorough taxonomy of slide design flaws. Our dataset comprises 2400 slides collected and synthesized from multiple sources, including a subset intentionally modified with specific design problems. We then fully annotated them using our taxonomy through strictly trained crowdsourcing from Prolific. To evaluate whether AI is capable of identifying design flaws, we compared multiple large language models under different prompting strategies, and with an existing design critique pipeline. We show that AI models struggle to accurately identify slide design flaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting techniques leveraging our taxonomy achieved the highest performance. We further conducted a remediation study to assess AI's potential for improving slides. Among 82.0% of slides that showed significant improvement, 87.8% of them were improved more with our taxonomy, further demonstrating its utility.

</details>


### [17] [Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired](https://arxiv.org/abs/2508.03651)

*Ruei-Che Chang, Rosiana Natalie, Wenqian Xu, Jovan Zheng Feng Yap, Anhong Guo*

**Main category:** cs.HC

**Keywords:** Blind Individuals, Assistive Technology, Live Video AI, Human-Computer Interaction, User Experience

**Relevance Score:** 8

**TL;DR:** Exploratory study on live video AI for blind or visually impaired individuals highlighting benefits and challenges in real-world applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential of live video AI in assisting blind or visually impaired individuals in navigating and interacting with their environments.

**Method:** An exploratory study with eight blind or visually impaired participants using ChatGPT's Advanced Voice with Video in various real-world scenarios.

**Key Contributions:**

	1. Insights into BVI users' interaction with live video AI
	2. Identification of challenges in dynamic environments
	3. Recommendations for improving assistive AI design

**Result:** The AI provided useful guidance for static scenes but struggled with dynamic situations due to inaccuracies in spatial information and assumptions about user capabilities.

**Limitations:** Participants had varying degrees of experiences with technology, which could influence results and perceptions.

**Conclusion:** The study reveals both the potential and limitations of live video AI, suggesting enhancements like better sensing capabilities and addressing user trust issues.

**Abstract:** Recent advancements in large multimodal models have provided blind or visually impaired (BVI) individuals with new capabilities to interpret and engage with the real world through interactive systems that utilize live video feeds. However, the potential benefits and challenges of such capabilities to support diverse real-world assistive tasks remain unclear. In this paper, we present findings from an exploratory study with eight BVI participants. Participants used ChatGPT's Advanced Voice with Video, a state-of-the-art live video AI released in late 2024, in various real-world scenarios, from locating objects to recognizing visual landmarks, across unfamiliar indoor and outdoor environments. Our findings indicate that current live video AI effectively provides guidance and answers for static visual scenes but falls short in delivering essential live descriptions required in dynamic situations. Despite inaccuracies in spatial and distance information, participants leveraged the provided visual information to supplement their mobility strategies. Although the system was perceived as human-like due to high-quality voice interactions, assumptions about users' visual abilities, hallucinations, generic responses, and a tendency towards sycophancy led to confusion, distrust, and potential risks for BVI users. Based on the results, we discuss implications for assistive video AI agents, including incorporating additional sensing capabilities for real-world use, determining appropriate intervention timing beyond turn-taking interactions, and addressing ecological and safety concerns.

</details>


### [18] [Classifying Epistemic Relationships in Human-AI Interaction: An Exploratory Approach](https://arxiv.org/abs/2508.03673)

*Shengnan Yang, Rongqian Ma*

**Main category:** cs.HC

**Keywords:** Human-AI interaction, Epistemic roles, Trust and collaboration, Knowledge-intensive work, HCI

**Relevance Score:** 8

**TL;DR:** This study explores how users interact with AI in knowledge-intensive work, focusing on their epistemic roles and relationships with AI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine how AI reshapes users' roles as knowledge contributors in research and teaching.

**Method:** Conducted 31 interviews with academics and developed a five-part codebook to categorize user-AI relationships.

**Key Contributions:**

	1. Development of a five-part codebook categorizing AI-user relationships
	2. Identification of five distinct relationship types
	3. Proposing a framework for understanding the dynamic and contextual nature of epistemic roles in HCI.

**Result:** Identified five relationship types: Instrumental Reliance, Contingent Delegation, Co-agency Collaboration, Authority Displacement, and Epistemic Abstention, highlighting the dynamic nature of epistemic roles.

**Limitations:** 

**Conclusion:** Epistemic roles in human-AI interaction are context-dependent and require a nuanced framework to understand co-construction of knowledge.

**Abstract:** As AI systems become integral to knowledge-intensive work, questions arise not only about their functionality but also their epistemic roles in human-AI interaction. While HCI research has proposed various AI role typologies, it often overlooks how AI reshapes users' roles as knowledge contributors. This study examines how users form epistemic relationships with AI-how they assess, trust, and collaborate with it in research and teaching contexts. Based on 31 interviews with academics across disciplines, we developed a five-part codebook and identified five relationship types: Instrumental Reliance, Contingent Delegation, Co-agency Collaboration, Authority Displacement, and Epistemic Abstention. These reflect variations in trust, assessment modes, tasks, and human epistemic status. Our findings show that epistemic roles are dynamic and context-dependent. We argue for shifting beyond static metaphors of AI toward a more nuanced framework that captures how humans and AI co-construct knowledge, enriching HCI's understanding of the relational and normative dimensions of AI use.

</details>


### [19] [Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults](https://arxiv.org/abs/2404.17730)

*Lara J. Martin, Malathy Nagalakshmi*

**Main category:** cs.HC

**Keywords:** Augmentative and Alternative Communication, Autistic adults, large language models, user-centered design, technology

**Relevance Score:** 8

**TL;DR:** This paper explores the perspectives of autistic adults on Augmentative and Alternative Communication (AAC) tools, identifying key pain points and potential advancements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid advancement of high-tech AAC tools powered by large language models like ChatGPT has largely overlooked the inclusion of end-user perspectives, particularly those of autistic adults.

**Method:** In-depth interviews were conducted with 12 autistic adults to uncover their experiences and challenges with current AAC tools and to explore desired technological improvements.

**Key Contributions:**

	1. Identification of 8 critical themes affecting AAC tools for autistic adults.
	2. Comparison of user perspectives with existing literature on AAC.
	3. Recommendations for future research directions to improve AAC technology.

**Result:** The research identified 8 categories of themes from user interviews, including input flexibility, output flexibility, adaptation of AAC tools, contextual usage, benefits, adult access issues, obstacles for continued use, and user control over communication.

**Limitations:** The study is limited by a small sample size of only 12 participants, which may not represent the diversity of all autistic individuals.

**Conclusion:** The findings provide a comprehensive overview of the gaps in AAC design for autistic adults and suggest new avenues for future research to enhance user-centered design in this domain.

**Abstract:** High-tech Augmentative and Alternative Communication (AAC) has been rapidly advancing in recent years due to the increased use of large language models (LLMs) like ChatGPT, but many of these techniques are integrated without the inclusion of the users' perspectives. Autistic adults have been particularly neglected in the design of AAC tools. We conducted in-depth interviews with 12 autistic adults to find the pain points of current AAC and determine what technological advances they might find helpful. We found 8 different categories of themes from our interviews: input flexibility, output flexibility, selecting or adapting AAC, contexts for AAC use, benefits, access as an adult, stumbling blocks for continued use, and control of communication. In this paper, we go through these categories in depth -- comparing each to prior work -- and then highlight novel findings to suggest possible research directions.

</details>


### [20] [Embracing Transparency: A Study of Open Science Practices Among Early Career HCI Researchers](https://arxiv.org/abs/2410.04286)

*Tatiana Chakravorti, Sanjana Gautam, Sarah M. Rajtmajer*

**Main category:** cs.HC

**Keywords:** open science, HCI, research barriers, transparency, early-career researchers

**Relevance Score:** 8

**TL;DR:** This study explores early-career HCI researchers' perceptions of open science and identifies barriers to its adoption through interviews.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate perceptions of open science among early-career researchers in HCI and understand the barriers to adopting best practices.

**Method:** Conducted 18 semi-structured interviews with early-career HCI researchers.

**Key Contributions:**

	1. Identified specific barriers to open science in HCI.
	2. Provided recommendations for promoting open science practices.
	3. Highlighted the need for cultural change in the HCI community regarding transparency.

**Result:** Identified key barriers to data sharing and preregistration including lack of incentives, cultural resistance, limited training, time constraints, intellectual property concerns, and privacy issues.

**Limitations:** Findings are limited to the USA and based on self-reported data, susceptible to biases.

**Conclusion:** Recommendations are made to promote transparency in HCI, suggesting that small changes at major conferences could impact community norms.

**Abstract:** Many fields of science, including Human-Computer Interaction (HCI), have heightened introspection in the wake of concerns around reproducibility and replicability of published findings. Notably, in recent years the HCI community has worked to implement policy changes and mainstream open science practices. Our work investigates early-career HCI researchers' perceptions of open science and engagement with best practices through 18 semi-structured interviews. Our findings highlight key barriers to the widespread adoption of data and materials sharing, and preregistration, namely: lack of clear incentives; cultural resistance; limited training; time constraints; concerns about intellectual property; and data privacy issues. We observe that small changes at major conferences like CHI could meaningfully impact community norms. We offer recommendations to address these barriers and to promote transparency and openness in HCI. While these findings provide valuable and interesting insights about the open science practices by early career HCI researchers, their applicability is limited to the USA only. The interview study relies on self-reported data; therefore, it can be subject to biases like recall bias. Future studies will include the scope to expand HCI researchers from different levels of experience and different countries, allowing for more justifiable examples.

</details>


### [21] [The Impostor is Among Us: Can Large Language Models Capture the Complexity of Human Personas?](https://arxiv.org/abs/2501.04543)

*Christopher Lazik, Christopher Katins, Charlotte Kauter, Jonas Jakob, Caroline Jay, Lars Grunske, Thomas Kosch*

**Main category:** cs.HC

**Keywords:** Large Language Models, personas, HCI, user experience, AI-generated content

**Relevance Score:** 8

**TL;DR:** This paper explores the perception of AI-generated personas versus human-crafted ones in HCI design, revealing that AI personas are viewed as more informative but may perpetuate stereotypes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the credibility and effectiveness of personas generated by LLMs compared to human-crafted personas in design processes.

**Method:** A survey comparing ten human-crafted personas by HCI experts with ten personas generated by an LLM, gathering participants' perceptions on their effectiveness for design.

**Key Contributions:**

	1. Examination of user perceptions of AI-generated versus human-crafted personas in HCI.
	2. Highlighting the potential risks of stereotyping in AI-generated personas.
	3. Providing insights on the need for diversity in AI-generated personas.

**Result:** Participants distinguished between human-created and AI-generated personas, finding the latter more informative yet prone to stereotypes, underlining the importance of diversity in LLM-generated content.

**Limitations:** Limited to a survey format and a small sample of personas, which may not encompass all user perspectives.

**Conclusion:** While LLM-generated personas can enhance design with informative content, caution is warranted to address issues of stereotyping and lack of diversity.

**Abstract:** Large Language Models (LLMs) created new opportunities for generating personas, expected to streamline and accelerate the human-centered design process. Yet, AI-generated personas may not accurately represent actual user experiences, as they can miss contextual and emotional insights critical to understanding real users' needs and behaviors. This introduces a potential threat to quality, especially for novices. This paper examines the differences in how users perceive personas created by LLMs compared to those crafted by humans regarding their credibility for design. We gathered ten human-crafted personas developed by HCI experts according to relevant attributes established in related work. Then, we systematically generated ten personas with an LLM and compared them with human-crafted ones in a survey. The results showed that participants differentiated between human-created and AI-generated personas, with the latter perceived as more informative and consistent. However, participants noted that the AI-generated personas tended to follow stereotypes, highlighting the need for a greater emphasis on diversity when utilizing LLMs for persona creation.

</details>


### [22] ["It was Mentally Painful to Try and Stop": Design Opportunities for Just-in-Time Interventions for People with Obsessive-Compulsive Disorder in the Real World](https://arxiv.org/abs/2501.13308)

*Ru Wang, Kexin Zhang, Yuqing Wang, Keri Brown, Yuhang Zhao*

**Main category:** cs.HC

**Keywords:** OCD, self-management, technology, user needs, mental health

**Relevance Score:** 7

**TL;DR:** The paper explores self-management challenges in OCD and proposes design opportunities for technology to support personalized interventions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the challenges and needs in OCD self-management due to fear and lack of support.

**Method:** Interviews with 10 OCD participants and 7 therapists to gather insights on triggers, compulsions, and coping strategies.

**Key Contributions:**

	1. Identifies key coping strategies for OCD self-management.
	2. Highlights gaps between user needs and existing support systems.
	3. Proposes design opportunities for just-in-time technologies.

**Result:** Identified critical gaps between OCD self-management needs and available support, leading to the proposal of design opportunities for technology.

**Limitations:** 

**Conclusion:** Advocates for just-in-time self-management technologies, including symptom tracking and interventions tailored to OCD-related needs.

**Abstract:** Obsessive-compulsive disorder (OCD) is a mental health condition that significantly impacts people's quality of life. While evidence-based therapies such as exposure and response prevention (ERP) can be effective, managing OCD symptoms in everyday life -- an essential part of treatment and independent living -- remains challenging due to fear confrontation and lack of appropriate support. To better understand the challenges and needs in OCD self-management, we conducted interviews with 10 participants with diverse OCD conditions and seven therapists specializing in OCD treatment. Through these interviews, we explored the characteristics of participants' triggers and how they shaped their compulsions, and uncovered key coping strategies across different stages of OCD episodes. Our findings highlight critical gaps between OCD self-management needs and currently available support. Building on these insights, we propose design opportunities for just-in-time self-management technologies for OCD, including personalized symptom tracking, just-in-time interventions, and support for OCD-specific privacy and social needs -- through technology and beyond.

</details>


### [23] [Characterizing Visual Intents for People with Low Vision through Eye Tracking](https://arxiv.org/abs/2501.14327)

*Ru Wang, Ruijia Chen, Anqiao Erica Cai, Zhiyuan Li, Sanbrita Mondal, Yuhang Zhao*

**Main category:** cs.HC

**Keywords:** low vision, gaze patterns, assistive technologies, eye tracking, visual intent

**Relevance Score:** 8

**TL;DR:** The paper explores gaze behaviors of low vision users during image-viewing tasks to inform adaptive visual support technologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the unique gaze patterns of people with low vision and improve visual support technologies based on their functional vision.

**Method:** A retrospective think-aloud study using eye tracking with 20 low vision participants and 20 sighted controls was conducted. Participants performed image-viewing tasks and reflected on their gaze trajectories.

**Key Contributions:**

	1. Identification of unique gaze patterns in low vision users
	2. Development of a visual intent taxonomy
	3. Insight into the impact of visual ability on gaze behaviors

**Result:** A visual intent taxonomy with five visual intents was derived, highlighting differences in gaze behaviors between low vision and sighted participants, influenced by visual ability.

**Limitations:** 

**Conclusion:** Combining visual ability data, visual context, and eye tracking is crucial for developing intent-aware assistive technologies for low vision users.

**Abstract:** Accessing visual information is crucial yet challenging for people with low vision due to visual conditions like low visual acuity and limited visual fields. However, unlike blind people, low vision people have and prefer using their functional vision in daily tasks. Gaze patterns thus become an important indicator to uncover their visual challenges and intents, inspiring more adaptive visual support. We seek to deeply understand low vision users' gaze behaviors in different image-viewing tasks, characterizing typical visual intents and the unique gaze patterns exhibited by people with different low vision conditions. We conducted a retrospective think-aloud study using eye tracking with 20 low vision participants and 20 sighted controls. Participants completed various image-viewing tasks and watched the playback of their gaze trajectories to reflect on their visual experiences. Based on the study, we derived a visual intent taxonomy with five visual intents characterized by participants' gaze behaviors. We demonstrated the difference between low vision and sighted participants' gaze behaviors and how visual ability affected low vision participants' gaze patterns across visual intents. Our findings underscore the importance of combining visual ability information, visual context, and eye tracking data in visual intent recognition, setting up a foundation for intent-aware assistive technologies for low vision people.

</details>


### [24] [Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health](https://arxiv.org/abs/2502.13920)

*Xingbo Wang, Janessa Griffith, Daniel A. Adler, Joey Castillo, Tanzeem Choudhury, Fei Wang*

**Main category:** cs.HC

**Keywords:** sleep health, large language model, adaptation, behavior change, health informatics

**Relevance Score:** 9

**TL;DR:** HealthGuru is a large language model-powered chatbot designed to improve sleep health through adaptive and personalized recommendations based on user data from wearables and contextual insights.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the difficulty individuals face in translating sleep tracking data into actionable health improvements.

**Method:** HealthGuru integrates data from wearable devices, uses contextual information and a multi-armed bandit model to suggest personalized sleep-enhancing activities while supporting behavior change through natural conversations.

**Key Contributions:**

	1. Introduction of HealthGuru, a novel chatbot for sleep health improvement
	2. Integration of contextual multi-armed bandit model for personalized recommendations
	3. Insights from an in-the-wild study on user engagement and behavior change

**Result:** The deployment study showed HealthGuru improved sleep duration and activity scores, provided higher quality responses, and increased user motivation for behavior change compared to a baseline chatbot.

**Limitations:** Challenges in personalization and user engagement remain for health chatbots.

**Conclusion:** HealthGuru demonstrates the potential of leveraging LLMs in health chatbot applications, highlighting the importance of personalization and user engagement.

**Abstract:** Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru's multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots.

</details>


### [25] [NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification](https://arxiv.org/abs/2508.02823)

*Wenshuo Zhang, Leixian Shen, Shuchang Xu, Jindu Wang, Jian Zhao, Huamin Qu, Linping Yuan*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Conversational LLMs, Intent-Task Alignment

**Relevance Score:** 9

**TL;DR:** This paper addresses misalignment between user intent and generated code in conversational LLMs by proposing a new interaction paradigm, direct intent-task matching, implemented in the NeuroSync system.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The work investigates the issue of misalignment between user intent and generated code in conversational LLMs, caused by bidirectional ambiguity in expressing nonlinear intents through linear prompts.

**Method:** The paper proposes a new human-LLM interaction paradigm called direct intent-task matching, which externalizes LLM understanding and allows users to manipulate coding tasks and their relationships before code generation, implemented in a system named NeuroSync.

**Key Contributions:**

	1. Introduces direct intent-task matching for human-LLM interaction
	2. Develops NeuroSync to visualize and manipulate LLM understanding
	3. Demonstrates improved alignment and coding efficiency through user studies

**Result:** User study results indicate that NeuroSync enhances intent-task alignment, reduces cognitive effort, and improves coding efficiency.

**Limitations:** 

**Conclusion:** NeuroSync provides an effective tool for users to align their intents with coding tasks, thereby addressing the challenges faced in conversational LLM interactions.

**Abstract:** Conversational LLMs have been widely adopted by domain users with limited programming experience to solve domain problems. However, these users often face misalignment between their intent and generated code, resulting in frustration and rounds of clarification. This work first investigates the cause of this misalignment, which dues to bidirectional ambiguity: both user intents and coding tasks are inherently nonlinear, yet must be expressed and interpreted through linear prompts and code sequences. To address this, we propose direct intent-task matching, a new human-LLM interaction paradigm that externalizes and enables direct manipulation of the LLM understanding, i.e., the coding tasks and their relationships inferred by the LLM prior to code generation. As a proof-of-concept, this paradigm is then implemented in NeuroSync, which employs a knowledge distillation pipeline to extract LLM understanding, user intents, and their mappings, and enhances the alignment by allowing users to intuitively inspect and edit them via visualizations. We evaluate the algorithmic components of NeuroSync via technical experiments, and assess its overall usability and effectiveness via a user study (N=12). The results show that it enhances intent-task alignment, lowers cognitive effort, and improves coding efficiency.

</details>


### [26] [Aging Up AAC: An Introspection on Augmentative and Alternative Communication Applications for Autistic Adults](https://arxiv.org/abs/2404.17730)

*Lara J. Martin, Malathy Nagalakshmi*

**Main category:** cs.HC

**Keywords:** Augmentative Communication, Autistic Adults, User-Centered Design, Large Language Models, Assistive Technology

**Relevance Score:** 9

**TL;DR:** Investigation of AAC tools for autistic adults, highlighting user perspectives and identifying key themes for improvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the neglect of autistic adults in the design of AAC tools, particularly with the advent of LLMs.

**Method:** In-depth interviews with 12 autistic adults to identify pain points and technological needs.

**Key Contributions:**

	1. Identified specific needs of autistic adults in AAC design
	2. Categorized pain points for existing AAC technology
	3. Proposed new research directions based on user feedback

**Result:** Identified 8 themes: input/output flexibility, AAC adaptation, usage contexts, benefits, adult access, barriers to usage, and communication control.

**Limitations:** 

**Conclusion:** The findings suggest new directions for research in designing more effective AAC tools by focusing on user perspectives.

**Abstract:** High-tech Augmentative and Alternative Communication (AAC) has been rapidly advancing in recent years due to the increased use of large language models (LLMs) like ChatGPT, but many of these techniques are integrated without the inclusion of the users' perspectives. Autistic adults have been particularly neglected in the design of AAC tools. We conducted in-depth interviews with 12 autistic adults to find the pain points of current AAC and determine what technological advances they might find helpful. We found 8 different categories of themes from our interviews: input flexibility, output flexibility, selecting or adapting AAC, contexts for AAC use, benefits, access as an adult, stumbling blocks for continued use, and control of communication. In this paper, we go through these categories in depth -- comparing each to prior work -- and then highlight novel findings to suggest possible research directions.

</details>


### [27] [Exploring Personalized Health Support through Data-Driven, Theory-Guided LLMs: A Case Study in Sleep Health](https://arxiv.org/abs/2502.13920)

*Xingbo Wang, Janessa Griffith, Daniel A. Adler, Joey Castillo, Tanzeem Choudhury, Fei Wang*

**Main category:** cs.HC

**Keywords:** Sleep health, Large language models, Chatbots, Health informatics, Behavior change

**Relevance Score:** 9

**TL;DR:** HealthGuru is a chatbot enhanced by a large language model aimed at improving sleep health through personalized, data-driven recommendations and behavior change support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Many sleep-tracking devices fail to provide actionable and personalized insights for sleep improvement; HealthGuru aims to bridge this gap.

**Method:** HealthGuru employs a multi-agent framework that integrates data from wearable devices and context-aware inputs, using a contextual multi-armed bandit model to offer tailored recommendations and support dialogue.

**Key Contributions:**

	1. Introduction of a novel LLM-powered chatbot for sleep improvement
	2. Integration of contextual data and behavior change techniques
	3. Demonstrated effectiveness through an in-the-wild study.

**Result:** The eight-week study demonstrated that HealthGuru improved sleep duration, activity metrics, and user motivation compared to a baseline chatbot.

**Limitations:** Challenges in personalization and user engagement identified during the study.

**Conclusion:** The findings indicate the effectiveness of HealthGuru in promoting better sleep through personalized recommendations and underscore the need for improved personalization and engagement strategies in health chatbots.

**Abstract:** Despite the prevalence of sleep-tracking devices, many individuals struggle to translate data into actionable improvements in sleep health. Current methods often provide data-driven suggestions but may not be feasible and adaptive to real-life constraints and individual contexts. We present HealthGuru, a novel large language model-powered chatbot to enhance sleep health through data-driven, theory-guided, and adaptive recommendations with conversational behavior change support. HealthGuru's multi-agent framework integrates wearable device data, contextual information, and a contextual multi-armed bandit model to suggest tailored sleep-enhancing activities. The system facilitates natural conversations while incorporating data-driven insights and theoretical behavior change techniques. Our eight-week in-the-wild deployment study with 16 participants compared HealthGuru to a baseline chatbot. Results show improved metrics like sleep duration and activity scores, higher quality responses, and increased user motivation for behavior change with HealthGuru. We also identify challenges and design considerations for personalization and user engagement in health chatbots.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [28] [Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation](https://arxiv.org/abs/2508.02808)

*Radhika Dua, Young Joon, Kwon, Siddhant Dogra, Daniel Freedman, Diana Ruan, Motaz Nashawaty, Danielle Rigau, Daniel Alexander Alber, Kang Zhang, Kyunghyun Cho, Eric Karl Oermann*

**Main category:** cs.CL

**Keywords:** radiology report generation, evaluation framework, ICARE, clinical decision-making, language models

**Relevance Score:** 8

**TL;DR:** This paper presents ICARE, an interpretable evaluation framework for assessing the clinical quality of automatically generated radiology reports using language model agents that generate dynamic clinical questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable clinical evaluation of automated radiology reports due to the limitations of existing metrics that lack interpretability.

**Method:** ICARE utilizes large language model agents to create clinically relevant questions based on either ground-truth or generated reports, allowing for an interpretable evaluation of report quality through multiple-choice question answering.

**Key Contributions:**

	1. Introduction of ICARE as an interpretable evaluation framework.
	2. Use of dynamic MCQA to assess clinical precision and recall of generated reports.
	3. Demonstrated alignment with expert judgment through clinician studies.

**Result:** ICARE shows a significant alignment with expert clinician judgments compared to traditional metrics, demonstrating its effectiveness in assessing report quality.

**Limitations:** The framework's effectiveness may be contingent on the quality of the generated questions and the agents' capabilities.

**Conclusion:** The framework provides a transparent and interpretable assessment of radiology reports, highlighting its relevance in improving automated report generation systems.

**Abstract:** Radiological imaging is central to diagnosis, treatment planning, and clinical decision-making. Vision-language foundation models have spurred interest in automated radiology report generation (RRG), but safe deployment requires reliable clinical evaluation of generated reports. Existing metrics often rely on surface-level similarity or behave as black boxes, lacking interpretability. We introduce ICARE (Interpretable and Clinically-grounded Agent-based Report Evaluation), an interpretable evaluation framework leveraging large language model agents and dynamic multiple-choice question answering (MCQA). Two agents, each with either the ground-truth or generated report, generate clinically meaningful questions and quiz each other. Agreement on answers captures preservation and consistency of findings, serving as interpretable proxies for clinical precision and recall. By linking scores to question-answer pairs, ICARE enables transparent, and interpretable assessment. Clinician studies show ICARE aligns significantly more with expert judgment than prior metrics. Perturbation analyses confirm sensitivity to clinical content and reproducibility, while model comparisons reveal interpretable error patterns.

</details>


### [29] [Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives](https://arxiv.org/abs/2508.02853)

*Yinuo Xu, Veronica Derricks, Allison Earl, David Jurgens*

**Main category:** cs.CL

**Keywords:** annotator disagreement, demographic-aware modeling, synthetic annotations, NLP, data imputation

**Relevance Score:** 8

**TL;DR:** The paper presents a model, DEM-MoE, that addresses annotator disagreement in subjective NLP tasks by considering demographic factors, improving data representation through real and synthetic annotations.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** Modeling annotator disagreement in subjective NLP tasks has been a challenge, and understanding demographic influences can enhance model performance and data representation.

**Method:** The DEM-MoE model routes inputs to expert subnetworks based on annotator demographics and uses LLM-generated synthetic annotations for data enrichment and imputation.

**Key Contributions:**

	1. Introduction of DEM-MoE model for handling demographic influences in NLP
	2. Use of LLM-generated synthetic annotations for addressing sparse demographic coverage
	3. Evaluation of data blending strategies tailored to dataset structures.

**Result:** The model performs competitively across demographic groups, showing better results in datasets with high annotator disagreement and providing scalable methods for data imputation.

**Limitations:** The effectiveness of synthetic annotation alignment with human annotations varies, and approaches may depend on specific dataset characteristics.

**Conclusion:** The contributions improve the representation of diverse perspectives in NLP tasks, making it more effective in handling demographic variations.

**Abstract:** We present an approach to modeling annotator disagreement in subjective NLP tasks through both architectural and data-centric innovations. Our model, DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert subnetworks based on annotator demographics, enabling it to better represent structured, group-level variation compared to prior models. DEM-MoE consistently performs competitively across demographic groups, and shows especially strong results on datasets with high annotator disagreement. To address sparse demographic coverage, we test whether LLM-generated synthetic annotations via zero-shot persona prompting can be used for data imputation. We show these synthetic judgments align moderately well with human annotations on our data and offer a scalable way to potentially enrich training data. We then propose and evaluate approaches for blending real and synthetic data using strategies tailored to dataset structure. We find that the optimal strategies depend on dataset structure. Together, these contributions improve the representation of diverse perspectives.

</details>


### [30] [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872)

*Giovanni Cherubin, Andrew Paverd*

**Main category:** cs.CL

**Keywords:** Large Language Models, jailbreaking prevention, retrieval-augmented generation, natural language processing, summarization

**Relevance Score:** 9

**TL;DR:** The paper introduces Highlight & Summarize (H&S), a novel design pattern for RAG systems that prevents jailbreaking and model hijacking in LLMs by not revealing the user's question to the model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Preventing jailbreaking and model hijacking of LLMs is critical, as malicious prompts can generate undesirable content or misdirect the model's functionality.

**Method:** The H&S design pattern splits the RAG pipeline into two components: a highlighter that extracts relevant passages from retrieved documents based on the user's question, and a summarizer that condenses these passages into an answer, all while keeping the user's question hidden from the LLM.

**Key Contributions:**

	1. Introduction of the H&S design pattern for RAG systems
	2. Demonstrated superiority of H&S responses over standard RAG
	3. Evaluation of various instantiations of the H&S framework

**Result:** H&S responses, particularly when using an LLM-based highlighter, often outperform those from standard RAG pipelines in terms of correctness, relevance, and response quality.

**Limitations:** The evaluation is limited to specific configurations of the H&S approach; further studies may be needed to generalize findings across different contexts.

**Conclusion:** The H&S pattern offers a robust approach to enhance the security of LLMs against malicious inputs while maintaining high-quality outputs.

**Abstract:** Preventing jailbreaking and model hijacking of Large Language Models (LLMs) is an important yet challenging task. For example, when interacting with a chatbot, malicious users can input specially crafted prompts to cause the LLM to generate undesirable content or perform a completely different task from its intended purpose. Existing mitigations for such attacks typically rely on hardening the LLM's system prompt or using a content classifier trained to detect undesirable content or off-topic conversations. However, these probabilistic approaches are relatively easy to bypass due to the very large space of possible inputs and undesirable outputs. In this paper, we present and evaluate Highlight & Summarize (H&S), a new design pattern for retrieval-augmented generation (RAG) systems that prevents these attacks by design. The core idea is to perform the same task as a standard RAG pipeline (i.e., to provide natural language answers to questions, based on relevant sources) without ever revealing the user's question to the generative LLM. This is achieved by splitting the pipeline into two components: a highlighter, which takes the user's question and extracts relevant passages ("highlights") from the retrieved documents, and a summarizer, which takes the highlighted passages and summarizes them into a cohesive answer. We describe several possible instantiations of H&S and evaluate their generated responses in terms of correctness, relevance, and response quality. Surprisingly, when using an LLM-based highlighter, the majority of H&S responses are judged to be better than those of a standard RAG pipeline.

</details>


### [31] [Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages](https://arxiv.org/abs/2508.02885)

*Elliot Murphy, Rohan Venkatesh, Edward Khokhlovich, Andrey Vyshedskiy*

**Main category:** cs.CL

**Keywords:** syntax, Merge operation, cognitive mechanisms, sentence comprehension, linguistic structures

**Relevance Score:** 2

**TL;DR:** This paper investigates the cognitive processes involved in understanding syntactic structures formed by the 'Merge' operation in language.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how different levels of syntactic complexity influence sentence comprehension and to identify distinct cognitive mechanisms responsible for processing Merge-based constructs.

**Method:** The study involved systematic investigation and clustering analyses of participants' comprehension of sentences with various syntactic complexities.

**Key Contributions:**

	1. Identifies three distinct types of Merge-based structures in sentence comprehension
	2. Provides insight into cognitive mechanisms underpinning syntactical processing
	3. Suggests developmental stages in the understanding of syntactic complexity

**Result:** Behavioral evidence indicated three distinct structural types of syntactic complexity, which might develop at different stages and could face selective impairments.

**Limitations:** 

**Conclusion:** Different cognitive mechanisms support the processing of Merge-based linguistic structures, although the overall merge operation may have emerged suddenly in evolutionary history.

**Abstract:** In the modern language sciences, the core computational operation of syntax, 'Merge', is defined as an operation that combines two linguistic units (e.g., 'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase). This can then be further combined with additional linguistic units based on this categorial information, respecting non-associativity such that abstract grouping is respected. Some linguists have embraced the view that Merge is an elementary, indivisible operation that emerged in a single evolutionary step. From a neurocognitive standpoint, different mental objects constructed by Merge may be supported by distinct mechanisms: (1) simple command constructions (e.g., "eat apples"); (2) the merging of adjectives and nouns ("red boat"); and (3) the merging of nouns with spatial prepositions ("laptop behind the sofa"). Here, we systematically investigate participants' comprehension of sentences with increasing levels of syntactic complexity. Clustering analyses revealed behavioral evidence for three distinct structural types, which we discuss as potentially emerging at different developmental stages and subject to selective impairment. While a Merge-based syntax may still have emerged suddenly in evolutionary time, responsible for the structured symbolic turn our species took, different cognitive mechanisms seem to underwrite the processing of various types of Merge-based objects.

</details>


### [32] [Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models](https://arxiv.org/abs/2508.02886)

*Wenjie Luo, Ruocheng Li, Shanshan Zhu, Julian Perry*

**Main category:** cs.CL

**Keywords:** multimodal reasoning, large language models, vision-language models, iterative refinement, common sense reasoning

**Relevance Score:** 8

**TL;DR:** The Coherent Multimodal Reasoning Framework (CMRF) enhances large vision-language models' common sense reasoning through iterative, self-evaluating mechanisms, achieving state-of-the-art performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing large language and vision-language models struggle with complex multi-step reasoning tasks, often relying on superficial associations rather than deep inference.

**Method:** CMRF employs three key modules: a Reasoning Decomposition Unit to break down problems, a Contextual Inference Engine for inference, and a Coherence Assessment Module for evaluating consistency, supported by an Adaptive Iterative Refinement strategy.

**Key Contributions:**

	1. Introduction of the Coherent Multimodal Reasoning Framework (CMRF) for LVLMs
	2. Development of a novel Multimodal Daily Activity Reasoning (MDAR) dataset
	3. Demonstration of enhanced performance in complex, multi-step reasoning tasks through iterative refinement.

**Result:** CMRF achieves an average accuracy of 69.4% on benchmarks like VCR, A-OKVQA, and DailyLife-MRC, surpassing existing open-source models by 2.4 percentage points, particularly excelling in complex reasoning.

**Limitations:** 

**Conclusion:** CMRF represents a significant improvement in LVLM reasoning capabilities, verified by extensive studies and evaluations that underline the importance of its modular approach.

**Abstract:** Despite significant advancements, current large language models (LLMs) and vision-language models (LVLMs) continue to struggle with complex, multi-step, cross-modal common sense reasoning tasks, often exhibiting a lack of "deliberative thinking." They tend to rely on superficial associations rather than deep, chained inference, particularly when integrating visual information with abstract concepts. To address this, we propose the Coherent Multimodal Reasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense reasoning capabilities through an iterative, self-evaluating inference mechanism. CMRF mimics human problem-solving by decomposing complex queries, generating step-by-step inferences, and self-correcting errors. Our framework integrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking down problems into sub-questions, a Contextual Inference Engine (CIE) for contextual inference, and a Coherence Assessment Module (CAM) for evaluating logical consistency and confidence. Coupled with an Adaptive Iterative Refinement strategy, CMRF systematically refines its reasoning paths. Built upon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning (MDAR) dataset, CMRF achieves state-of-the-art performance among open-source LVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It attains an average accuracy of 69.4%, surpassing the best open-source baseline by +2.4 percentage points, with particular strength in complex reasoning scenarios. Extensive ablation studies and human evaluations confirm the critical contributions of each module and the effectiveness of iterative refinement in fostering more coherent and accurate reasoning.

</details>


### [33] [SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations](https://arxiv.org/abs/2508.02901)

*Osama Khalid, Sanvesh Srivastava, Padmini Srinivasan*

**Main category:** cs.CL

**Keywords:** sensorial language, stylistic features, SLIM-LLMs, Reduced-Rank Ridge Regression, natural language processing

**Relevance Score:** 6

**TL;DR:** This paper explores the use of sensorial language in communication, proposing a new model (SLIM-LLMs) that leverages reduced-rank regression for stylistic prediction while maintaining performance with fewer parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to investigate the role of sensorial language in communication and its relationship with traditional stylistic features, enhancing understanding of language models.

**Method:** The authors employ a Reduced-Rank Ridge Regression (R4) approach to analyze the effectiveness of low-dimensional latent representations of LIWC features in predicting sensorial language.

**Key Contributions:**

	1. Introduction of SLIM-LLMs for sensorial language modeling
	2. Demonstration of effective stylistic prediction with reduced features
	3. Evaluation across multiple genres showing performance retention with fewer parameters

**Result:** The findings show that SLIM-LLMs, which utilize low-rank LIWC features, can effectively match the performance of full-scale models while reducing the number of parameters by up to 80%.

**Limitations:** 

**Conclusion:** By introducing Stylometrically Lean Interpretable Models (SLIM-LLMs), this research highlights a new way to model language that balances complexity and interpretability in natural language processing.

**Abstract:** Sensorial language -- the language connected to our senses including vision, sound, touch, taste, smell, and interoception, plays a fundamental role in how we communicate experiences and perceptions. We explore the relationship between sensorial language and traditional stylistic features, like those measured by LIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate that low-dimensional latent representations of LIWC features r = 24 effectively capture stylistic information for sensorial language prediction compared to the full feature set (r = 74). We introduce Stylometrically Lean Interpretable Models (SLIM-LLMs), which model non-linear relationships between these style dimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features match the performance of full-scale language models while reducing parameters by up to 80%.

</details>


### [34] [Can LLMs Generate High-Quality Task-Specific Conversations?](https://arxiv.org/abs/2508.02931)

*Shengqi Li, Amarnath Gupta*

**Main category:** cs.CL

**Keywords:** conversation quality, large language models, dialogue properties, parameter control, HCI

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for precisely controlling the quality of conversations generated by large language models through a set of nine parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in conversation generation such as coherence, knowledge progression, and consistency in dialogue.

**Method:** The framework explores nine key parameters across six dimensions affecting dialogue properties, tested on state-of-the-art large language models.

**Key Contributions:**

	1. Introduction of a parameterization framework for controlling dialogue quality
	2. Demonstration of statistically significant improvements in conversation properties
	3. Potential applications in various sectors like education and therapy

**Result:** Experiments showed that parameter-based control significantly influenced conversation properties in generated dialogues.

**Limitations:** 

**Conclusion:** The framework offers a standardized approach for conversation quality control, relevant for fields like education and customer service, with future work planned for parameter enhancement and evaluation datasets.

**Abstract:** This paper introduces a parameterization framework for controlling conversation quality in large language models. We explore nine key parameters across six dimensions that enable precise specification of dialogue properties. Through experiments with state-of-the-art LLMs, we demonstrate that parameter-based control produces statistically significant differences in generated conversation properties. Our approach addresses challenges in conversation generation, including topic coherence, knowledge progression, character consistency, and control granularity. The framework provides a standardized method for conversation quality control with applications in education, therapy, customer service, and entertainment. Future work will focus on implementing additional parameters through architectural modifications and developing benchmark datasets for evaluation.

</details>


### [35] [CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors](https://arxiv.org/abs/2508.02997)

*Sri Durga Sai Sowmya Kadali, Evangelos E. Papalexakis*

**Main category:** cs.CL

**Keywords:** Large Language Models, adversarial prompts, Contextual Co-occurrence Matrix, detection methods, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents a novel method for detecting adversarial and jailbreak prompts in Large Language Models (LLMs) using Contextual Co-occurrence Matrices, achieving a notable F1 score of 0.83 with minimal labeled data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the vulnerability of large language models to attacks, such as jailbreaks that lead to harmful outputs, and the need for effective detection methods.

**Method:** A novel approach utilizing the latent space characteristics of Contextual Co-occurrence Matrices and Tensors for detecting adversarial prompts.

**Key Contributions:**

	1. Development of a novel detection method using Contextual Co-occurrence Matrices
	2. Achieving an F1 score of 0.83 with minimal labeled data
	3. Public availability of the implementation for reproducibility

**Result:** The proposed method demonstrates an F1 score of 0.83 using only 0.5% of labeled prompts, representing a 96.6% improvement over existing baselines. It is also significantly faster, with speedups ranging from 2.3 to 128.4 times compared to baseline models.

**Limitations:** 

**Conclusion:** The study provides an effective means for detecting harmful prompts in LLMs, showcasing efficacy in data-scarce environments and contributing to the field by making the implementation publicly available.

**Abstract:** The widespread use of Large Language Models (LLMs) in many applications marks a significant advance in research and practice. However, their complexity and hard-to-understand nature make them vulnerable to attacks, especially jailbreaks designed to produce harmful responses. To counter these threats, developing strong detection methods is essential for the safe and reliable use of LLMs. This paper studies this detection problem using the Contextual Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce environments. We propose a novel method leveraging the latent space characteristics of Contextual Co-occurrence Matrices and Tensors for the effective identification of adversarial and jailbreak prompts. Our evaluations show that this approach achieves a notable F1 score of 0.83 using only 0.5% of labeled prompts, which is a 96.6% improvement over baselines. This result highlights the strength of our learned patterns, especially when labeled data is scarce. Our method is also significantly faster, speedup ranging from 2.3 to 128.4 times compared to the baseline models. To support future research and reproducibility, we have made our implementation publicly available.

</details>


### [36] [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)

*Ariya Mukherjee-Gandhi, Oliver Muellerklein*

**Main category:** cs.CL

**Keywords:** AI-generated art, Discourse analysis, Artist perspectives, Creative labor, Transparency

**Relevance Score:** 6

**TL;DR:** This study analyzes English-language discourse on AI-generated art from 2013 to 2025, revealing misalignment between artist concerns and media narratives, and advocates for greater transparency and engagement with artists.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The voices of artists in the conversation about AI-generated art are often marginalized, prompting a need to analyze discourse around this topic to understand their perspectives better.

**Method:** A twelve-year analysis of 439 curated excerpts from various sources (opinion articles, news reports, blogs, legal filings, transcripts) to identify thematic clusters concerning AI-generated art.

**Key Contributions:**

	1. BERTopic-based methodology for discourse analysis
	2. Identification of thematic clusters regarding AI-generated art
	3. Call for greater transparency in artist engagement

**Result:** Identified five stable thematic clusters reflecting artist concerns and how technical jargon can serve as a gatekeeping mechanism, sidelining urgent issues for artists.

**Limitations:** 

**Conclusion:** The findings suggest a need for transparent engagement with artist perspectives in discussions about AI's impact on creative labor and provide a methodology for future research.

**Abstract:** As generative AI continues to reshape artistic production and alternate modes of human expression, artists whose livelihoods are most directly affected have raised urgent concerns about consent, transparency, and the future of creative labor. However, the voices of artists are often marginalized in dominant public and scholarly discourse. This study presents a twelve-year analysis, from 2013 to 2025, of English-language discourse surrounding AI-generated art. It draws from 439 curated 500-word excerpts sampled from opinion articles, news reports, blogs, legal filings, and spoken-word transcripts. Through a reproducible methodology, we identify five stable thematic clusters and uncover a misalignment between artists' perceptions and prevailing media narratives. Our findings highlight how the use of technical jargon can function as a subtle form of gatekeeping, often sidelining the very issues artists deem most urgent. Our work provides a BERTopic-based methodology and a multimodal baseline for future research, alongside a clear call for deeper, transparency-driven engagement with artist perspectives in the evolving AI-creative landscape.

</details>


### [37] [Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03098)

*Haoran Wang, Xiongxiao Xu, Baixiang Huang, Kai Shu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Privacy-Aware Decoding, Large Language Models, Differential Privacy

**Relevance Score:** 9

**TL;DR:** This paper presents Privacy-Aware Decoding (PAD), a defense mechanism to protect sensitive data in Retrieval-Augmented Generation systems by adding calibrated noise during response generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy risks associated with the extraction of private information in Retrieval-Augmented Generation systems, especially when using large language models.

**Method:** The paper proposes Privacy-Aware Decoding (PAD), which injects calibrated Gaussian noise into token logits with mechanisms for confidence-based screening, efficient sensitivity estimation, and context-aware noise calibration.

**Key Contributions:**

	1. Introduction of Privacy-Aware Decoding (PAD) as a lightweight defense for RAG systems.
	2. Integration of Renyi Differential Privacy to track cumulative privacy loss during generation.
	3. Demonstration of improved privacy protection without the need for model retraining.

**Result:** PAD significantly reduces private information leakage while maintaining response utility, outperforming prior defenses and functioning without the need for retraining or extensive preprocessing.

**Limitations:** 

**Conclusion:** PAD represents a scalable solution for enhancing privacy in sensitive applications of RAG by improving decoding strategies.

**Abstract:** Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large language models (LLMs) by conditioning outputs on external knowledge sources. However, when retrieval involves private or sensitive data, RAG systems are susceptible to extraction attacks that can leak confidential information through generated responses. We propose Privacy-Aware Decoding (PAD), a lightweight, inference-time defense that adaptively injects calibrated Gaussian noise into token logits during generation. PAD integrates confidence-based screening to selectively protect high-risk tokens, efficient sensitivity estimation to minimize unnecessary noise, and context-aware noise calibration to balance privacy with generation quality. A \renyi Differential Privacy (RDP) accountant rigorously tracks cumulative privacy loss, enabling explicit per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs. Unlike prior approaches requiring retraining or corpus-level filtering, PAD is model-agnostic and operates entirely at decoding time with minimal computational overhead. Experiments on three real-world datasets demonstrate that PAD substantially reduces private information leakage while preserving response utility, outperforming existing retrieval- and post-processing-based defenses. Our work takes an important step toward mitigating privacy risks in RAG via decoding strategies, paving the way for universal and scalable privacy solutions in sensitive domains. Our code is available: https://github.com/wang2226/PAD.

</details>


### [38] [Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation](https://arxiv.org/abs/2508.03110)

*Zizhong Li, Haopeng Zhang, Jiawei Zhang*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, token-level attack

**Relevance Score:** 9

**TL;DR:** The paper introduces TPARAG, a novel method to enhance the security of RAG systems against token-level attacks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address security vulnerabilities in retrieval-augmented generation (RAG) systems, which can be manipulated by malicious content retrieved from external databases.

**Method:** TPARAG utilizes a lightweight white-box LLM to create and optimize malicious passages at the token level, addressing both white-box and black-box RAG scenarios.

**Key Contributions:**

	1. Introduction of TPARAG for token-level attacks on RAG
	2. Demonstrated superiority over previous attack methods
	3. Insights into RAG system vulnerabilities

**Result:** Extensive experiments demonstrate that TPARAG outperforms existing methods in both retrieval and end-to-end attack effectiveness, revealing significant vulnerabilities in RAG systems.

**Limitations:** 

**Conclusion:** The findings highlight critical weaknesses in RAG pipelines and suggest avenues for enhancing their security and robustness.

**Abstract:** While large language models (LLMs) have achieved remarkable success in providing trustworthy responses for knowledge-intensive tasks, they still face critical limitations such as hallucinations and outdated knowledge. To address these issues, the retrieval-augmented generation (RAG) framework enhances LLMs with access to external knowledge via a retriever, enabling more accurate and real-time outputs about the latest events. However, this integration brings new security vulnerabilities: the risk that malicious content in the external database can be retrieved and used to manipulate model outputs. Although prior work has explored attacks on RAG systems, existing approaches either rely heavily on access to the retriever or fail to jointly consider both retrieval and generation stages, limiting their effectiveness, particularly in black-box scenarios. To overcome these limitations, we propose Token-level Precise Attack on the RAG (TPARAG), a novel framework that targets both white-box and black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an attacker to generate and iteratively optimize malicious passages at the token level, ensuring both retrievability and high attack success in generation. Extensive experiments on open-domain QA datasets demonstrate that TPARAG consistently outperforms previous approaches in retrieval-stage and end-to-end attack effectiveness. These results further reveal critical vulnerabilities in RAG pipelines and offer new insights into improving their robustness.

</details>


### [39] [Cross-lingual Opinions and Emotions Mining in Comparable Documents](https://arxiv.org/abs/2508.03112)

*Motaz Saad, David Langlois, Kamel Smaili*

**Main category:** cs.CL

**Keywords:** cross-lingual sentiment analysis, emotion labeling, comparable documents, bilingual lexicons, multilingual communication

**Relevance Score:** 5

**TL;DR:** This research analyzes sentiment and emotion differences in English-Arabic comparable documents, revealing alignment in sentiments from the same news sources and divergence from different ones.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how sentiment and emotions are discussed across languages using comparable documents, rather than through direct translations.

**Method:** The study annotates documents with sentiment and emotion labels, utilizing a cross-lingual method without machine translation and employing bilingual emotion lexicons created from English WordNet-Affect for Arabic.

**Key Contributions:**

	1. Development of bilingual emotion lexicons from WordNet-Affect
	2. Introduction of a cross-lingual method for sentiment labeling without machine translation
	3. Novel insights into sentiment and emotion alignment based on news agency sources.

**Result:** Sentiment and emotion annotations align for articles from the same news agency (like Euronews, BBC, and Al-Jazeera) but diverge for articles from different agencies, highlighting important cross-linguistic sentiment patterns.

**Limitations:** The study focuses solely on English-Arabic document pairs and may not fully represent other language pairs or domains.

**Conclusion:** The proposed method is language-independent and can be generalized to other language pairs, providing insights into multilingual sentiment analysis.

**Abstract:** Comparable texts are topic-aligned documents in multiple languages that are not direct translations. They are valuable for understanding how a topic is discussed across languages. This research studies differences in sentiments and emotions across English-Arabic comparable documents. First, texts are annotated with sentiment and emotion labels. We apply a cross-lingual method to label documents with opinion classes (subjective/objective), avoiding reliance on machine translation. To annotate with emotions (anger, disgust, fear, joy, sadness, surprise), we manually translate the English WordNet-Affect (WNA) lexicon into Arabic, creating bilingual emotion lexicons used to label the comparable corpora. We then apply a statistical measure to assess the agreement of sentiments and emotions in each source-target document pair. This comparison is especially relevant when the documents originate from different sources. To our knowledge, this aspect has not been explored in prior literature. Our study includes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera (JSC). Results show that sentiment and emotion annotations align when articles come from the same news agency and diverge when they come from different ones. The proposed method is language-independent and generalizable to other language pairs.

</details>


### [40] [Long Story Generation via Knowledge Graph and Literary Theory](https://arxiv.org/abs/2508.03137)

*Ge Shi, Kaiyu Huang, Guochen Feng*

**Main category:** cs.CL

**Keywords:** long text generation, story generation, large language models, memory models, multi-agent systems

**Relevance Score:** 9

**TL;DR:** This paper presents a multi-agent Story Generator using large language models to enhance long text generation by mitigating theme drift and improving narrative coherence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues of theme drift and incoherent plots in long text generation based on outlines, which have been prevalent in previous methodologies.

**Method:** The proposed method incorporates a memory storage model, consisting of long-term and short-term memory, along with a story theme obstacle framework that uses literary narratology theory to enhance storytelling. It also includes a multi-agent interaction stage for writer-reader simulation to refine the story based on feedback.

**Key Contributions:**

	1. Multi-agent structure using LLMs for story generation
	2. Introduction of a memory storage model to avoid theme drift
	3. Development of a story theme obstacle framework to enhance narrative engagement

**Result:** Evaluations show that the multi-agent Story Generator produces higher-quality long stories compared to existing methods.

**Limitations:** 

**Conclusion:** The introduction of agent interaction and memory models significantly improves the narrative quality and coherence in long story generation tasks.

**Abstract:** The generation of a long story consisting of several thousand words is a sub-task in the field of long text generation~(LTG). Previous research has addressed this challenge through outline-based generation, which employs a multi-stage method for generating outlines into stories. However, this approach suffers from two common issues: almost inevitable theme drift caused by the loss of memory of previous outlines, and tedious plots with incoherent logic that are less appealing to human readers.   In this paper, we propose the multi-agent Story Generator structure to improve the multi-stage method, using large language models~(LLMs) as the core components of agents. To avoid theme drift, we introduce a memory storage model comprising two components: a long-term memory storage that identifies the most important memories, thereby preventing theme drift; and a short-term memory storage that retains the latest outlines from each generation round. To incorporate engaging elements into the story, we design a story theme obstacle framework based on literary narratology theory that introduces uncertain factors and evaluation criteria to generate outline. This framework calculates the similarity of the former storyline and enhances the appeal of the story by building a knowledge graph and integrating new node content. Additionally, we establish a multi-agent interaction stage to simulate writer-reader interaction through dialogue and revise the story text according to feedback, to ensure it remains consistent and logical. Evaluations against previous methods demonstrate that our approach can generate higher-quality long stories.

</details>


### [41] [RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior](https://arxiv.org/abs/2508.03140)

*Junyao Yang, Jianwei Wang, Huiping Zhuang, Cen Chen, Ziqian Zeng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought, model merging, domain-specific knowledge, reasoning capability

**Relevance Score:** 9

**TL;DR:** The paper presents RCP-Merging, a novel framework for merging Long Chain-of-Thought (CoT) models with domain-specific LLMs, maintaining reasoning performance while enhancing domain task outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create a dual-capability model that combines long CoT reasoning and domain-specific knowledge without high computational costs, while overcoming challenges in current merging methods that degrade reasoning abilities.

**Method:** Introducing RCP-Merging, which utilizes a reasoning capability indicator to prioritize the preservation of core long CoT model weights while selectively merging domain-specific weights.

**Key Contributions:**

	1. Introduction of RCP-Merging framework
	2. Enhancement of performance in domain-specific tasks
	3. Maintaining reasoning capabilities during model merging

**Result:** RCP-Merging improved domain task performance by 9.5% and 9.2% on Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance domains without degrading the long CoT reasoning capability.

**Limitations:** 

**Conclusion:** RCP-Merging effectively merges reasoning models with domain-specific LLMs, achieving better performance in specific tasks while safeguarding the intrinsic reasoning abilities of the models.

**Abstract:** Large Language Models (LLMs) with long chain-of-thought (CoT) capability, termed Reasoning Models, demonstrate superior intricate problem-solving abilities through multi-step long CoT reasoning. To create a dual-capability model with long CoT capability and domain-specific knowledge without substantial computational and data costs, model merging emerges as a highly resource-efficient method. However, significant challenges lie in merging domain-specific LLMs with long CoT ones since nowadays merging methods suffer from reasoning capability degradation, even gibberish output and output collapse. To overcome this, we introduce RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior, a novel merging framework designed to integrate domain-specific LLMs with long CoT capability, meanwhile maintaining model performance in the original domain. Treating reasoning model weights as foundational prior, our method utilizes a reasoning capability indicator to preserve core long CoT capability model weights while selectively merging essential domain-specific weights. We conducted extensive experiments on Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance domains. Our results show that RCP-Merging successfully merges a reasoning model with domain-specific ones, improving domain task performance by 9.5% and 9.2% over state-of-the-art methods, without significantly harming the original long CoT reasoning capability.

</details>


### [42] [Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following](https://arxiv.org/abs/2508.03178)

*Chenyang Wang, Liang Wen, Shousheng Jia, Xiangzheng Zhang, Liang Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Instruction Adherence, Reinforcement Learning, Fine-Tuning, Reasoning

**Relevance Score:** 8

**TL;DR:** The paper proposes a framework to improve instruction adherence in LLMs by addressing lazy reasoning, using a method that involves complex constraints and a novel supervised fine-tuning strategy to enhance reasoning abilities.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the instruction adherence of LLMs, which remains inconsistent especially for complex directives due to lazy reasoning.

**Method:** The authors generate complex instructions, filter for valid prompts, and employ rejection sampling to create a high-quality dataset. They then apply an entropy-preserving supervised fine-tuning (Entropy-SFT) and a reinforcement learning method (TEA-RL) to improve reasoning mechanisms.

**Key Contributions:**

	1. Introduction of a comprehensive framework for improving LLM reasoning
	2. Implementation of novel fine-tuning and reinforcement learning strategies
	3. Demonstrated performance improvements across model scales in instruction adherence tasks.

**Result:** The approach led to significant performance improvements on instruction-following benchmarks, with the Light-IF-32B model outperforming larger and closed-source models.

**Limitations:** 

**Conclusion:** The proposed framework successfully enhances reasoning abilities in LLMs, facilitating better adherence to complex instructions and demonstrating the efficacy of their methodology.

**Abstract:** While advancements in the reasoning abilities of LLMs have significantly enhanced their performance in solving mathematical problems, coding tasks, and general puzzles, their effectiveness in accurately adhering to instructions remains inconsistent, particularly with more complex directives. Our investigation identifies lazy reasoning during the thinking stage as the primary factor contributing to poor instruction adherence. To mitigate this issue, we propose a comprehensive framework designed to enable rigorous reasoning processes involving preview and self-checking, essential for satisfying strict instruction constraints. Specifically, we first generate instructions with complex constraints and apply a filtering process to obtain valid prompts, resulting in three distinct prompt datasets categorized as hard, easy, and pass. Then, we employ rejection sampling on the pass prompts to curate a small yet high-quality dataset, enabling a cold-start initialization of the model and facilitating its adaptation to effective reasoning patterns. Subsequently, we employ an entropy-preserving supervised fine-tuning (Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL) reinforcement learning guided by rule-based dense rewards. This approach encourages the model to transform its reasoning mechanism, ultimately fostering generalizable reasoning abilities that encompass preview and self-checking. Extensive experiments conducted on instruction-following benchmarks demonstrate remarkable performance improvements across various model scales. Notably, our Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1 and closed-source models like Doubao-1.6.

</details>


### [43] [Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification](https://arxiv.org/abs/2508.03181)

*Lukas Pätz, Moritz Beyer, Jannik Späth, Lasse Bohlen, Patrick Zschech, Mathias Kraus, Julian Rosenberger*

**Main category:** cs.CL

**Keywords:** political discourse, machine learning, Bundestag, topic classification, sentiment analysis

**Relevance Score:** 3

**TL;DR:** This study analyzes 28,000 speeches from the German Bundestag using machine learning to classify topics and sentiments, revealing significant party dynamics and discourse changes.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To understand political discourse dynamics in the German Bundestag and how party roles influence speech style and sentiment over time.

**Method:** Two machine learning models were developed for topic and sentiment classification, trained on a manually labeled dataset of speeches, achieving high classification performance.

**Key Contributions:**

	1. Development of machine learning models for classifying parliamentary speeches
	2. Insights into political discourse dynamics over time
	3. Analysis of party-specific discourse strategies

**Result:** The models achieved an AUROC of 0.94 for topic classification and 0.89 for sentiment classification, uncovering trends and relationships in political discourse.

**Limitations:** 

**Conclusion:** The analysis shows that shifts in party roles between government and opposition influence discourse styles, and that both ideological beliefs and governing responsibilities play significant roles.

**Abstract:** This study investigates political discourse in the German parliament, the Bundestag, by analyzing approximately 28,000 parliamentary speeches from the last five years. Two machine learning models for topic and sentiment classification were developed and trained on a manually labeled dataset. The models showed strong classification performance, achieving an area under the receiver operating characteristic curve (AUROC) of 0.94 for topic classification (average across topics) and 0.89 for sentiment classification. Both models were applied to assess topic trends and sentiment distributions across political parties and over time. The analysis reveals remarkable relationships between parties and their role in parliament. In particular, a change in style can be observed for parties moving from government to opposition. While ideological positions matter, governing responsibilities also shape discourse. The analysis directly addresses key questions about the evolution of topics, sentiment dynamics, and party-specific discourse strategies in the Bundestag.

</details>


### [44] [Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models](https://arxiv.org/abs/2508.03199)

*Muhammed Saeed, Shaina Raza, Ashmal Vayani, Muhammad Abdul-Mageed, Ali Emami, Shady Shehata*

**Main category:** cs.CL

**Keywords:** Text-to-Image, grammatical gender, multilingual AI, bias in AI, visual representation

**Relevance Score:** 6

**TL;DR:** This paper investigates the influence of grammatical gender on visual representation in Text-to-Image models across multiple languages, establishing that language structure significantly shapes AI image outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how grammatical gender affects visual representation in T2I models, an area previously overlooked in bias research.

**Method:** A cross-linguistic benchmark was created using 800 prompts in five gendered languages (French, Spanish, German, Italian, Russian) and two gender-neutral languages (English, Chinese), generating 28,800 images across three T2I models.

**Key Contributions:**

	1. Introduces a cross-linguistic benchmark for analyzing T2I models based on grammatical gender.
	2. Demonstrates the significant impact of grammatical gender on representation in generated images.
	3. Establishes language structure as a crucial factor in understanding bias in AI-generated outputs.

**Result:** Grammatical gender significantly impacts image generation, with masculine markers leading to 73% male representation on average, and feminine markers resulting in 38% female representation, showcasing marked differences compared to English.

**Limitations:** 

**Conclusion:** The study concludes that language structure influences AI-generated visuals, introducing new considerations for bias and fairness in multilingual AI systems.

**Abstract:** Research on bias in Text-to-Image (T2I) models has primarily focused on demographic representation and stereotypical attributes, overlooking a fundamental question: how does grammatical gender influence visual representation across languages? We introduce a cross-linguistic benchmark examining words where grammatical gender contradicts stereotypical gender associations (e.g., ``une sentinelle'' - grammatically feminine in French but referring to the stereotypically masculine concept ``guard''). Our dataset spans five gendered languages (French, Spanish, German, Italian, Russian) and two gender-neutral control languages (English, Chinese), comprising 800 unique prompts that generated 28,800 images across three state-of-the-art T2I models. Our analysis reveals that grammatical gender dramatically influences image generation: masculine grammatical markers increase male representation to 73\% on average (compared to 22\% with gender-neutral English), while feminine grammatical markers increase female representation to 38\% (compared to 28\% in English). These effects vary systematically by language resource availability and model architecture, with high-resource languages showing stronger effects. Our findings establish that language structure itself, not just content, shapes AI-generated visual outputs, introducing a new dimension for understanding bias and fairness in multilingual, multimodal systems.

</details>


### [45] [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)

*Ariya Mukherjee-Gandhi, Oliver Muellerklein*

**Main category:** cs.CL

**Keywords:** AI-generated art, artist perspectives, creative labor, media narratives, discourse analysis

**Relevance Score:** 4

**TL;DR:** This study analyzes a twelve-year discourse on AI-generated art, highlighting artists' concerns about consent and transparency, and revealing a misalignment with media narratives.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To address the urgent concerns of artists affected by generative AI regarding consent, transparency, and creative labor amid their marginalized voices.

**Method:** The study employs a reproducible methodology analyzing 439 curated excerpts from diverse media sources over twelve years, identifying five thematic clusters.

**Key Contributions:**

	1. Identification of thematic clusters in AI-generated art discourse
	2. Highlighting the misalignment between artists' concerns and media narratives
	3. Providing a reproducible methodology for future studies

**Result:** The analysis uncovers a misalignment between artists' perceptions and prevailing media narratives, revealing that technical jargon can act as a gatekeeping mechanism.

**Limitations:** 

**Conclusion:** The study advocates for deeper engagement with artist perspectives and provides a methodology for future research in the AI-creative landscape.

**Abstract:** As generative AI continues to reshape artistic production and alternate modes of human expression, artists whose livelihoods are most directly affected have raised urgent concerns about consent, transparency, and the future of creative labor. However, the voices of artists are often marginalized in dominant public and scholarly discourse. This study presents a twelve-year analysis, from 2013 to 2025, of English-language discourse surrounding AI-generated art. It draws from 439 curated 500-word excerpts sampled from opinion articles, news reports, blogs, legal filings, and spoken-word transcripts. Through a reproducible methodology, we identify five stable thematic clusters and uncover a misalignment between artists' perceptions and prevailing media narratives. Our findings highlight how the use of technical jargon can function as a subtle form of gatekeeping, often sidelining the very issues artists deem most urgent. Our work provides a BERTopic-based methodology and a multimodal baseline for future research, alongside a clear call for deeper, transparency-driven engagement with artist perspectives in the evolving AI-creative landscape.

</details>


### [46] [Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP](https://arxiv.org/abs/2508.03204)

*Abhirup Sinha, Pritilata Saha, Tithi Saha*

**Main category:** cs.CL

**Keywords:** data privacy, anonymization, NLP, language models, GDPR

**Relevance Score:** 8

**TL;DR:** The paper examines approaches to anonymizing private information in data used for training large language models, which pose risks of exposing sensitive information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the privacy risks associated with large language models that learn from data containing private information, especially in light of regulatory frameworks like GDPR.

**Method:** The report reviews various pre-processing techniques for masking or pseudonymizing private information in textual data used for domain-agnostic NLP tasks.

**Key Contributions:**

	1. Identification of pre-processing approaches for anonymization in NLP.
	2. Analysis of the effectiveness of different masking techniques.
	3. Discussion on the implications of language models on data privacy.

**Result:** The paper identifies and discusses multiple approaches to anonymization, emphasizing the importance of these techniques in protecting data privacy.

**Limitations:** The paper acknowledges that while various methods exist, complete anonymization is not always achievable.

**Conclusion:** While complete anonymization is challenging, existing methods can significantly reduce the risks of exposing private information in NLP tasks.

**Abstract:** Privacy is a fundamental human right. Data privacy is protected by different regulations, such as GDPR. However, modern large language models require a huge amount of data to learn linguistic variations, and the data often contains private information. Research has shown that it is possible to extract private information from such language models. Thus, anonymizing such private and sensitive information is of utmost importance. While complete anonymization may not be possible, a number of different pre-processing approaches exist for masking or pseudonymizing private information in textual data. This report focuses on a few of such approaches for domain-agnostic NLP tasks.

</details>


### [47] [Probing Syntax in Large Language Models: Successes and Remaining Challenges](https://arxiv.org/abs/2508.03211)

*Pablo J. Diego-Simón, Emmanuel Chemla, Jean-Rémi King, Yair Lakretz*

**Main category:** cs.CL

**Keywords:** Structural Probes, Large Language Models, Syntax, Linguistic Properties, Benchmarking

**Relevance Score:** 6

**TL;DR:** This study evaluates structural probes in large language models, highlighting biases in their performance related to word proximity and linguistic properties.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the effects of structural and statistical factors on syntactic representations in large language models, and to improve evaluation methods for structural probes.

**Method:** Conducts an in-depth analysis using three controlled benchmarks to evaluate the performance of structural probes

**Key Contributions:**

	1. Identifies bias in structural probes due to word proximity
	2. Highlights poor representation of deep syntactic structures by probes
	3. Demonstrates lack of influence from word predictability on probe performance

**Result:** Findings show that structural probes are biased by word proximity, struggle with deep syntactic structures, and are unaffected by word predictability.

**Limitations:** 

**Conclusion:** The study reveals significant challenges faced by structural probes and proposes controlled stimuli for better benchmarking.

**Abstract:** The syntactic structures of sentences can be readily read-out from the activations of large language models (LLMs). However, the ``structural probes'' that have been developed to reveal this phenomenon are typically evaluated on an indiscriminate set of sentences. Consequently, it remains unclear whether structural and/or statistical factors systematically affect these syntactic representations. To address this issue, we conduct an in-depth analysis of structural probes on three controlled benchmarks. Our results are three-fold. First, structural probes are biased by a superficial property: the closer two words are in a sentence, the more likely structural probes will consider them as syntactically linked. Second, structural probes are challenged by linguistic properties: they poorly represent deep syntactic structures, and get interfered by interacting nouns or ungrammatical verb forms. Third, structural probes do not appear to be affected by the predictability of individual words. Overall, this work sheds light on the current challenges faced by structural probes. Providing a benchmark made of controlled stimuli to better evaluate their performance.

</details>


### [48] [CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting](https://arxiv.org/abs/2508.03240)

*Mutaz Ayesh, Nicolás Gutiérrez-Rolón, Fernando Alva-Manchego*

**Main category:** cs.CL

**Keywords:** Spanish text adaptation, LLM prompting, IberLEF 2025

**Relevance Score:** 6

**TL;DR:** The CardiffNLP team's work on the CLEARS shared task utilized an LLM-prompting approach with prompt variations for Spanish text adaptation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To contribute to the CLEARS shared task on Spanish text adaptation and achieve competitive results using LLM technology.

**Method:** The team used an LLM-prompting approach, experimenting with different prompt variations, initially with LLaMA-3.2 and then switching to Gemma-3 for final submissions.

**Key Contributions:**

	1. LLM-prompting methodology for Spanish text adaptation
	2. Comparative analysis of prompt variations
	3. Documentation of experimental results and performance in shared tasks

**Result:** The team achieved third place in Subtask 1 and second place in Subtask 2 of the Spanish text adaptation task.

**Limitations:** 

**Conclusion:** The paper presents the successful application of LLM prompting and provides insights into various prompt strategies that led to the team's performance.

**Abstract:** This paper details the CardiffNLP team's contribution to the CLEARS shared task on Spanish text adaptation, hosted by IberLEF 2025. The shared task contained two subtasks and the team submitted to both. Our team took an LLM-prompting approach with different prompt variations. While we initially experimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and landed third place in Subtask 1 and second place in Subtask 2. We detail our numerous prompt variations, examples, and experimental results.

</details>


### [49] [Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs](https://arxiv.org/abs/2508.03247)

*Shintaro Sakai, Jisun An, Migyeong Kang, Haewoon Kwak*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural differences, Mental health applications, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper tests how Large Language Models (LLMs) respond to cultural variations in mental health symptoms and finds that they struggle to replicate cultural patterns, especially when prompted in English.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs reflect cultural differences in symptom reporting observed in clinical psychology, specifically between Western and Eastern cultural contexts in mental health.

**Method:** LLMs were prompted using Western or Eastern personas and responses were analyzed to see if cultural symptom patterns were replicated, especially in different languages.

**Key Contributions:**

	1. Demonstrates the failure of LLMs to replicate cultural symptom reporting patterns in mental health.
	2. Shows improvement in LLM responses when using Eastern languages.
	3. Identifies limitations in LLMs' sensitivity to cultural nuances.

**Result:** LLMs did not replicate the expected cultural symptom patterns when prompted in English, but performance improved when prompted in Eastern languages like Chinese, Japanese, and Hindi.

**Limitations:** The study is limited to general-purpose LLMs and may not address variations in specialized or fine-tuned models.

**Conclusion:** Current general-purpose LLMs lack the necessary cultural sensitivity for safe mental health applications, highlighting the need for more culture-aware capabilities.

**Abstract:** Prior clinical psychology research shows that Western individuals with depression tend to report psychological symptoms, while Eastern individuals report somatic ones. We test whether Large Language Models (LLMs), which are increasingly used in mental health, reproduce these cultural patterns by prompting them with Western or Eastern personas. Results show that LLMs largely fail to replicate the patterns when prompted in English, though prompting in major Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment in several configurations. Our analysis pinpoints two key reasons for this failure: the models' low sensitivity to cultural personas and a strong, culturally invariant symptom hierarchy that overrides cultural cues. These findings reveal that while prompt language is important, current general-purpose LLMs lack the robust, culture-aware capabilities essential for safe and effective mental health applications.

</details>


### [50] [RooseBERT: A New Deal For Political Language Modelling](https://arxiv.org/abs/2508.03250)

*Deborah Dore, Elena Cabrio, Serena Villata*

**Main category:** cs.CL

**Keywords:** RooseBERT, political discourse, language model, debate analysis, machine learning

**Relevance Score:** 6

**TL;DR:** Introduction of RooseBERT, a pre-trained Language Model for political discourse that improves upon general models in analyzing political debates.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To develop computational methods that aid in the automatic analysis of political debates and discussions, enhancing understanding and engagement of citizens.

**Method:** RooseBERT was pre-trained on a large corpus of political debates and speeches and fine-tuned on tasks like named entity recognition, sentiment analysis, and argument detection.

**Key Contributions:**

	1. Development of RooseBERT for political discourse analysis
	2. Demonstrated performance improvements over general-purpose models
	3. Release of the model for public research use

**Result:** The fine-tuning demonstrated significant improvements over general-purpose models in four key tasks of political debate analysis.

**Limitations:** 

**Conclusion:** The introduction of RooseBERT shows that domain-specific pre-training can enhance performance in understanding political discourse, making it available for further research.

**Abstract:** The increasing amount of political debates and politics-related discussions calls for the definition of novel computational methods to automatically analyse such content with the final goal of lightening up political deliberation to citizens. However, the specificity of the political language and the argumentative form of these debates (employing hidden communication strategies and leveraging implicit arguments) make this task very challenging, even for current general-purpose pre-trained Language Models. To address this issue, we introduce a novel pre-trained Language Model for political discourse language called RooseBERT. Pre-training a language model on a specialised domain presents different technical and linguistic challenges, requiring extensive computational resources and large-scale data. RooseBERT has been trained on large political debate and speech corpora (8K debates, each composed of several sub-debates on different topics) in English. To evaluate its performances, we fine-tuned it on four downstream tasks related to political debate analysis, i.e., named entity recognition, sentiment analysis, argument component detection and classification, and argument relation prediction and classification. Our results demonstrate significant improvements over general-purpose Language Models on these four tasks, highlighting how domain-specific pre-training enhances performance in political debate analysis. We release the RooseBERT language model for the research community.

</details>


### [51] [Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition](https://arxiv.org/abs/2508.03259)

*Duzhen Zhang, Chenxing Li, Jiahua Dong, Qi Liu, Dong Yu*

**Main category:** cs.CL

**Keywords:** Continual Named Entity Recognition, Knowledge Distillation, Stability-Plasticity Trade-off, Pseudo-labeling, Machine Learning

**Relevance Score:** 5

**TL;DR:** A Stability-Plasticity Trade-off method for Continual Named Entity Recognition enhances model adaptability by balancing retention of old knowledge and acquisition of new entity types, demonstrating superior performance against previous methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the excessive stability and limited plasticity in existing Continual Named Entity Recognition methods that focus on Knowledge Distillation.

**Method:** The proposed SPT method introduces a pooling operation for representation flexibility and dynamically merges weights from old and new models, complemented by a confidence-based pseudo-labeling approach.

**Key Contributions:**

	1. Introduction of pooling operation in Knowledge Distillation for representation flexibility
	2. Dynamic merging of weights from old and new models
	3. Confidence-based pseudo-labeling technique for handling non-entity type semantic shifts.

**Result:** Experiments across ten CNER settings on three benchmark datasets show that the SPT method outperforms previous CNER approaches, achieving an effective stability-plasticity trade-off.

**Limitations:** 

**Conclusion:** The SPT method effectively improves the adaptability of CNER systems in managing new entity types while retaining essential knowledge from old models.

**Abstract:** Continual Named Entity Recognition (CNER) is an evolving field that focuses on sequentially updating an existing model to incorporate new entity types. Previous CNER methods primarily utilize Knowledge Distillation (KD) to preserve prior knowledge and overcome catastrophic forgetting, strictly ensuring that the representations of old and new models remain consistent. Consequently, they often impart the model with excessive stability (i.e., retention of old knowledge) but limited plasticity (i.e., acquisition of new knowledge). To address this issue, we propose a Stability-Plasticity Trade-off (SPT) method for CNER that balances these aspects from both representation and weight perspectives. From the representation perspective, we introduce a pooling operation into the original KD, permitting a level of plasticity by consolidating representation dimensions. From the weight perspective, we dynamically merge the weights of old and new models, strengthening old knowledge while maintaining new knowledge. During this fusion, we implement a weight-guided selective mechanism to prioritize significant weights. Moreover, we develop a confidence-based pseudo-labeling approach for the current non-entity type, which predicts entity types using the old model to handle the semantic shift of the non-entity type, a challenge specific to CNER that has largely been ignored by previous methods. Extensive experiments across ten CNER settings on three benchmark datasets demonstrate that our SPT method surpasses previous CNER approaches, highlighting its effectiveness in achieving a suitable stability-plasticity trade-off.

</details>


### [52] [Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?](https://arxiv.org/abs/2508.03262)

*Junhyuk Choi, Hyeonchu Park, Haemin Lee, Hyebeen Shin, Hyun Joung Jin, Bugeun Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, economic decision-making, PWYW pricing, persona-based simulation, computational social science

**Relevance Score:** 8

**TL;DR:** This study evaluates the ability of LLMs to predict economic decision-making using data from real human personas in PWYW pricing experiments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitation of prior studies relying on fictional personas by using actual human data to evaluate LLMs' predictive capabilities in economic contexts.

**Method:** The study compares three state-of-the-art multimodal LLMs using persona information from 522 Korean participants in various cultural consumption scenarios.

**Key Contributions:**

	1. Evaluation of LLMs using real human personas instead of fictional ones
	2. Comparison of multimodal LLMs' predictive abilities in economic decisions
	3. Insights on prompting techniques for improving LLM predictions

**Result:** LLMs struggle with precise individual-level predictions but exhibit reasonable group-level behaviors. Common prompting techniques show little improvement over naive methods.

**Limitations:** LLMs do not achieve accurate individual-level predictions; the effectiveness of prompting methods was not significantly enhanced.

**Conclusion:** The findings provide a comprehensive evaluation of LLMs' capabilities in simulating economic behavior, offering guidance for persona-based simulations in computational social science.

**Abstract:** Recent advances in Large Language Models (LLMs) have generated significant interest in their capacity to simulate human-like behaviors, yet most studies rely on fictional personas rather than actual human data. We address this limitation by evaluating LLMs' ability to predict individual economic decision-making using Pay-What-You-Want (PWYW) pricing experiments with real 522 human personas. Our study systematically compares three state-of-the-art multimodal LLMs using detailed persona information from 522 Korean participants in cultural consumption scenarios. We investigate whether LLMs can accurately replicate individual human choices and how persona injection methods affect prediction performance. Results reveal that while LLMs struggle with precise individual-level predictions, they demonstrate reasonable group-level behavioral tendencies. Also, we found that commonly adopted prompting techniques are not much better than naive prompting methods; reconstruction of personal narrative nor retrieval augmented generation have no significant gain against simple prompting method. We believe that these findings can provide the first comprehensive evaluation of LLMs' capabilities on simulating economic behavior using real human data, offering empirical guidance for persona-based simulation in computational social science.

</details>


### [53] [LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning](https://arxiv.org/abs/2508.03275)

*Jiahao Zhao*

**Main category:** cs.CL

**Keywords:** spaced repetition, large language models, adaptive learning, semantic similarity, intelligent tutoring systems

**Relevance Score:** 9

**TL;DR:** Introducing LECTOR, an adaptive scheduling algorithm leveraging large language models to enhance spaced repetition for test-oriented learning, achieving improved success rates in language examinations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing spaced repetition algorithms struggle with semantic interference and personalization, necessitating a new approach for effective learning.

**Method:** LECTOR utilizes large language models for semantic analysis and incorporates personalized learning profiles to address semantic confusion in vocabulary acquisition.

**Key Contributions:**

	1. Introduction of LECTOR algorithm for adaptive spaced repetition
	2. Utilization of LLM-powered semantic similarity for learning
	3. Demonstrated improved performance in test-oriented learning scenarios.

**Result:** LECTOR achieved a 90.2% success rate in language examinations, outperforming the best baseline (SSP-MMC) at 88.4%, with notable improvements in managing semantically similar concepts.

**Limitations:** 

**Conclusion:** LECTOR presents a significant advancement in intelligent tutoring systems and adaptive learning platforms, addressing key challenges in learning and memory retention.

**Abstract:** Spaced repetition systems are fundamental to efficient learning and memory retention, but existing algorithms often struggle with semantic interference and personalized adaptation. We present LECTOR (\textbf{L}LM-\textbf{E}nhanced \textbf{C}oncept-based \textbf{T}est-\textbf{O}riented \textbf{R}epetition), a novel adaptive scheduling algorithm specifically designed for test-oriented learning scenarios, particularly language examinations where success rate is paramount. LECTOR leverages large language models for semantic analysis while incorporating personalized learning profiles, addressing the critical challenge of semantic confusion in vocabulary learning by utilizing LLM-powered semantic similarity assessment and integrating it with established spaced repetition principles. Our comprehensive evaluation against six baseline algorithms (SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over 100 days demonstrates significant improvements: LECTOR achieves a 90.2\% success rate compared to 88.4\% for the best baseline (SSP-MMC), representing a 2.0\% relative improvement. The algorithm shows particular strength in handling semantically similar concepts, reducing confusion-induced errors while maintaining computational efficiency. Our results establish LECTOR as a promising direction for intelligent tutoring systems and adaptive learning platforms.

</details>


### [54] [Do language models accommodate their users? A study of linguistic convergence](https://arxiv.org/abs/2508.03276)

*Terra Blevins, Susanne Schmalwieser, Benjamin Roth*

**Main category:** cs.CL

**Keywords:** language models, linguistic convergence, human communication, dialogue analysis, stylometric features

**Relevance Score:** 9

**TL;DR:** This paper investigates whether large language models (LLMs) exhibit linguistic convergence to human language patterns in dialogue, finding significant overfitting to the style of the conversation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the extent to which LLMs adapt to the linguistic patterns of human users, focusing on the concept of linguistic convergence which is fundamental to human communication.

**Method:** The study systematically compares model completions from sixteen language models across three dialogue corpora, analyzing various stylometric features of the responses.

**Key Contributions:**

	1. Systematic comparison of LLMs' dialogue completions to human responses.
	2. Demonstration of significant overfitting in LLM stylistic adaptation.
	3. Identification of variations in convergence patterns based on model training types.

**Result:** Models show strong convergence to the conversation's style, with significant overfitting compared to human responses. Instruction-tuned and larger models generally converge less than pretrained models.

**Limitations:** The study focuses on specific dialogue corpora and stylometric features, which may limit the generalizability of the findings.

**Conclusion:** The findings suggest that while LLMs exhibit patterns of convergence, these are distinct from human linguistic behaviors, indicating different underlying mechanisms for language use.

**Abstract:** While large language models (LLMs) are generally considered proficient in generating language, how similar their language usage is to that of humans remains understudied. In this paper, we test whether models exhibit linguistic convergence, a core pragmatic element of human language communication, asking: do models adapt, or converge, to the linguistic patterns of their user? To answer this, we systematically compare model completions of exisiting dialogues to the original human responses across sixteen language models, three dialogue corpora, and a variety of stylometric features. We find that models strongly converge to the conversation's style, often significantly overfitting relative to the human baseline. While convergence patterns are often feature-specific, we observe consistent shifts in convergence across modeling settings, with instruction-tuned and larger models converging less than their pretrained counterparts. Given the differences between human and model convergence patterns, we hypothesize that the underlying mechanisms for these behaviors are very different.

</details>


### [55] [Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes](https://arxiv.org/abs/2508.03292)

*Shahed Masoudian, Gustavo Escobedo, Hannah Strauss, Markus Schedl*

**Main category:** cs.CL

**Keywords:** Large Language Models, gender bias, narrative generation, psychological stereotypes, StereoBias-Stories

**Relevance Score:** 9

**TL;DR:** This paper investigates gender bias in Large Language Models (LLMs) through narrative generation using psychological stereotypes, revealing how conditioning influences bias in generated content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address growing concerns about the amplification of gender biases in LLM outputs, particularly through more implicit biases that manifest in longer content.

**Method:** The authors created a novel dataset, StereoBias-Stories, which contains short stories conditioned on various psychological stereotypes. They analyzed how the gender representation in stories changes based on these stereotypes during narrative generation tasks.

**Key Contributions:**

	1. Introduction of the StereoBias-Stories dataset for evaluating gender bias in LLMs.
	2. Demonstration that conditioning on psychological attributes can change bias in narrative generation.
	3. Establishment of a correlation between model size and alignment with psychological ground-truth in bias.
	4. Identification of how the combination of stereotypes can either intensify or reduce biases.

**Result:** The study found that LLMs exhibit substantial bias towards male characters when unconditioned, but conditioning on attributes can mitigate this bias. Additionally, combining multiple attributes from gender stereotypes can either amplify or alleviate biases depending on the attributes' gender connotations.

**Limitations:** The study primarily focuses on narrative generation and may not generalize to all LLM tasks; further exploration in different contexts is needed.

**Conclusion:** Insights gained from this research underline the necessity of a psychology-informed approach to evaluating and mitigating biases in LLMs, especially as these models grow in size and capability.

**Abstract:** As Large Language Models (LLMs) are increasingly used across different applications, concerns about their potential to amplify gender biases in various tasks are rising. Prior research has often probed gender bias using explicit gender cues as counterfactual, or studied them in sentence completion and short question answering tasks. These formats might overlook more implicit forms of bias embedded in generative behavior of longer content. In this work, we investigate gender bias in LLMs using gender stereotypes studied in psychology (e.g., aggressiveness or gossiping) in an open-ended task of narrative generation. We introduce a novel dataset called StereoBias-Stories containing short stories either unconditioned or conditioned on (one, two, or six) random attributes from 25 psychological stereotypes and three task-related story endings. We analyze how the gender contribution in the overall story changes in response to these attributes and present three key findings: (1) While models, on average, are highly biased towards male in unconditioned prompts, conditioning on attributes independent from gender stereotypes mitigates this bias. (2) Combining multiple attributes associated with the same gender stereotype intensifies model behavior, with male ones amplifying bias and female ones alleviating it. (3) Model biases align with psychological ground-truth used for categorization, and alignment strength increases with model size. Together, these insights highlight the importance of psychology-grounded evaluation of LLMs.

</details>


### [56] [NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty](https://arxiv.org/abs/2508.03294)

*Leonidas Zotos, Ivo Pascal de Jong, Matias Valdenegro-Toro, Andreea Ioana Sburlea, Malvina Nissim, Hedderik van Rijn*

**Main category:** cs.CL

**Keywords:** Large Language Models, exam difficulty estimation, supervised learning, Neural Networks, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper explores the effectiveness of Large Language Models (LLMs) in estimating exam question difficulty compared to professors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of estimating the difficulty of exam questions for better educational assessments.

**Method:** We compared LLM-based approaches with professors' estimations on True/False questions in Neural Networks and Machine Learning, evaluating performance through supervised learning techniques.

**Key Contributions:**

	1. Demonstrated LLM superiority over human estimators for exam difficulty
	2. Introduced LLM uncertainty in supervised learning to enhance predictions
	3. Provided a new perspective on integrating AI in educational assessment.

**Result:** Professors struggled to effectively distinguish between easy and difficult questions, while LLMs, particularly Gemini 2.5, demonstrated superior performance; utilizing LLM uncertainty in a supervised learning framework yielded even better results with minimal training data.

**Limitations:** The study was limited to True/False questions and a small dataset of 42 training samples.

**Conclusion:** Supervised learning with LLM uncertainty can enhance professors' ability to estimate exam question difficulty, leading to improved assessment quality.

**Abstract:** Estimating the difficulty of exam questions is essential for developing good exams, but professors are not always good at this task. We compare various Large Language Model-based methods with three professors in their ability to estimate what percentage of students will give correct answers on True/False exam questions in the areas of Neural Networks and Machine Learning. Our results show that the professors have limited ability to distinguish between easy and difficult questions and that they are outperformed by directly asking Gemini 2.5 to solve this task. Yet, we obtained even better results using uncertainties of the LLMs solving the questions in a supervised learning setting, using only 42 training samples. We conclude that supervised learning using LLM uncertainty can help professors better estimate the difficulty of exam questions, improving the quality of assessment.

</details>


### [57] [Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling](https://arxiv.org/abs/2508.03296)

*Anqi Li, Wenwei Jin, Jintao Tong, Pengda Qin, Weijia Li, Guo Lu*

**Main category:** cs.CL

**Keywords:** social platforms, content moderation, multimodal framework, classification accuracy, interpretability

**Relevance Score:** 6

**TL;DR:** Hi-Guard is a multimodal moderation framework designed to improve content safety on social platforms by enhancing accuracy, interpretability, and alignment with moderation policies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of harmful and policy-violating content on social platforms necessitates improved moderation systems that are accurate, interpretable, and scalable.

**Method:** Hi-Guard employs a hierarchical moderation pipeline with a lightweight binary filter and a stronger model for risk classification, utilizing a hierarchical taxonomy for classifications and incorporating rule definitions directly into the model.

**Key Contributions:**

	1. Introduction of Hierarchical Guard (Hi-Guard) for improved moderation accuracy
	2. Hierarchical taxonomy for path-based classification
	3. Use of multi-level soft-margin reward with Group Relative Policy Optimization (GRPO) for enhanced reasoning

**Result:** Hi-Guard demonstrates superior classification accuracy, generalization, and interpretability in extensive experiments and real-world deployments, thereby enhancing content moderation effectiveness.

**Limitations:** 

**Conclusion:** The Hi-Guard framework paves the way for more transparent and trustworthy content moderation systems that align with evolving policies.

**Abstract:** Social platforms have revolutionized information sharing, but also accelerated the dissemination of harmful and policy-violating content. To ensure safety and compliance at scale, moderation systems must go beyond efficiency and offer accuracy and interpretability. However, current approaches largely rely on noisy, label-driven learning, lacking alignment with moderation rules and producing opaque decisions that hinder human review. Therefore, we propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that introduces a new policy-aligned decision paradigm. The term "Hierarchical" reflects two key aspects of our system design: (1) a hierarchical moderation pipeline, where a lightweight binary model first filters safe content and a stronger model handles fine-grained risk classification; and (2) a hierarchical taxonomy in the second stage, where the model performs path-based classification over a hierarchical taxonomy ranging from coarse to fine-grained levels. To ensure alignment with evolving moderation policies, Hi-Guard directly incorporates rule definitions into the model prompt. To further enhance structured prediction and reasoning, we introduce a multi-level soft-margin reward and optimize with Group Relative Policy Optimization (GRPO), penalizing semantically adjacent misclassifications and improving explanation quality. Extensive experiments and real-world deployment demonstrate that Hi-Guard achieves superior classification accuracy, generalization, and interpretability, paving the way toward scalable, transparent, and trustworthy content safety systems. Code is available at: https://github.com/lianqi1008/Hi-Guard.

</details>


### [58] [CTTS: Collective Test-Time Scaling](https://arxiv.org/abs/2508.03333)

*Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Tao Chen*

**Main category:** cs.CL

**Keywords:** Collective Test-Time Scaling, Multi-Agent Systems, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper explores Collective Test-Time Scaling (CTTS) for enhancing LLMs' effectiveness without extra training, introducing a novel framework CTTS-MM that leverages multi-agent and multi-reward model collaboration to improve inference performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of large language models (LLMs) at test time without additional training, moving beyond single-agent approaches.

**Method:** The paper investigates three paradigms for CTTS: 1) single agent with multiple reward models (SA-MR), 2) multiple agents with single reward model (MA-SR), and 3) multiple agents with multiple reward models (MA-MR). It proposes CTTS-MM framework combining multi-agent collaboration (Agent Collaboration Search) and multi-reward-model collaboration (Mixture of Reward Models).

**Key Contributions:**

	1. Introduction of Collective Test-Time Scaling (CTTS) as a new paradigm.
	2. Development of CTTS-MM which integrates multi-agent and multi-reward model collaborations.
	3. Demonstration of superior results through extensive tests on various benchmarks.

**Result:** Extensive experiments show that the MA-MR paradigm consistently achieves the best performance across seven mainstream benchmarks, with the CTTS-MM framework outperforming existing methods.

**Limitations:** 

**Conclusion:** Collective strategies in test-time scaling significantly enhance the performance of LLMs, and the proposed CTTS-MM framework demonstrates superior effectiveness.

**Abstract:** Test-time scaling (TTS) has emerged as a promising research field for enhancing the effectiveness of large language models (LLMs) without extra training. However, most existing approaches, e.g., Best-of-N and Self-Consistency rely on a single agent interacting with a reward model (SA-SR), constrained by limited capabilities of a single test-time scaling (STTS) paradigm. On the other hand, recent works demonstrate that collective-agent methods can break through the upper bound of single-agent systems by orchestrating diverse models. Thus, in this paper, we take a first step towards exploring Collective Test-Time Scaling (CTTS). Consider the different interaction types of single and multiple models, we design three primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent to multiple reward models (SA-MR); (2) multiple agents to single reward model (MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive experiments demonstrate that MA-MR consistently achieves the best performance. Based on this, we propose a novel framework named CTTS-MM that effectively leverages both multi-agent and multi-reward-model collaboration for enhanced inference. Specifically, for multi-agent collaboration, we propose an Agent Collaboration Search (ACS), which searches for the most effective combination of LLM agents from a large candidate pool; for multi-reward-model collaboration, we propose Mixture of Reword Models (MoR), which consists of a curated question pool and a Prior Reward model Ensemble Selection (PRES) to select the optimal combinations of reward models via Pair-wise Reward Ranking (PRR) metric. Experiments across seven mainstream benchmarks demonstrate that the proposed CTTS-MM consistently obtains superior performance. Code will be released at https://github.com/magent4aci/CTTS-MM.

</details>


### [59] [Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature](https://arxiv.org/abs/2508.03358)

*Tiago G Canário, Catarina Duarte, Flávio L. Pinheiro, João L. M. Pereira*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Named Entity Recognition, Social Networks, Portuguese Literature, Character Interaction

**Relevance Score:** 6

**TL;DR:** The paper presents Taggus, a novel NLP pipeline for extracting character social networks from Portuguese literary fiction, significantly outperforming existing tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for optimized methods to identify characters and their interactions in fiction, particularly for underrepresented languages like Portuguese, where existing methods struggle due to insufficient training data.

**Method:** Taggus combines Part-of-speech (POS) tagging with heuristics to extract character networks, focusing on named entity recognition and interaction detection.

**Key Contributions:**

	1. Introduction of the Taggus pipeline for character social networks
	2. Significant performance improvement over existing NER tools
	3. Public availability of the pipeline for further research

**Result:** The Taggus pipeline achieves an F1-Score of 94.1% for character identification and 75.9% for interaction detection, outperforming traditional tools by 50.7% and 22.3% respectively.

**Limitations:** The study acknowledges limitations regarding the size and scope of testing samples.

**Conclusion:** Taggus provides a robust solution for character interaction extraction in Portuguese literature and is publicly available to facilitate further research in this area.

**Abstract:** Automatically identifying characters and their interactions from fiction books is, arguably, a complex task that requires pipelines that leverage multiple Natural Language Processing (NLP) methods, such as Named Entity Recognition (NER) and Part-of-speech (POS) tagging. However, these methods are not optimized for the task that leads to the construction of Social Networks of Characters. Indeed, the currently available methods tend to underperform, especially in less-represented languages, due to a lack of manually annotated data for training. Here, we propose a pipeline, which we call Taggus, to extract social networks from literary fiction works in Portuguese. Our results show that compared to readily available State-of-the-Art tools -- off-the-shelf NER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which uses POS tagging and a combination of heuristics, achieves satisfying results with an average F1-Score of $94.1\%$ in the task of identifying characters and solving for co-reference and $75.9\%$ in interaction detection. These represent, respectively, an increase of $50.7\%$ and $22.3\%$ on results achieved by the readily available State-of-the-Art tools. Further steps to improve results are outlined, such as solutions for detecting relationships between characters. Limitations on the size and scope of our testing samples are acknowledged. The Taggus pipeline is publicly available to encourage development in this field for the Portuguese language.2

</details>


### [60] [Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models](https://arxiv.org/abs/2508.03363)

*Haotian Wu, Bo Xu, Yao Shu, Menglin Yang, Chengwei Qin*

**Main category:** cs.CL

**Keywords:** in-context learning, reasoning large language models, Thinking mode, Nothinking mode, JointThinking

**Relevance Score:** 9

**TL;DR:** This paper proposes JointThinking, a novel in-context learning paradigm for reasoning large language models that enhances reasoning accuracy by generating answers in parallel using two different reasoning modes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and improve in-context learning capabilities of reasoning large language models, which have been largely underutilized despite their potential for structured and multi-step reasoning.

**Method:** JointThinking prompts the model to generate two answers in Thinking and Nothinking modes in parallel. A follow-up Thinking round occurs if the responses are inconsistent, minimizing latency since this scenario is infrequent.

**Key Contributions:**

	1. JointThinking paradigm for in-context learning in RLLMs
	2. Demonstrated improved reasoning accuracy through structured reasoning modes
	3. Analysis of model performance scaling with increased model size.

**Result:** JointThinking significantly outperforms few-shot chain-of-thought and majority voting techniques, showing improved answer robustness and comparable performance to state-of-the-art methods on in-distribution benchmarks while excelling on out-of-distribution tasks.

**Limitations:** Current limitations include the need for further exploration of in-context learning strategies and the inconsistencies in multi-answer generation.

**Conclusion:** The study indicates that structural thinking diversity enhances reasoning accuracy and suggests future research directions in in-context learning for reasoning large language models.

**Abstract:** Reasoning large language models (RLLMs) have recently demonstrated remarkable capabilities through structured and multi-step reasoning. While prior research has primarily focused on improving their training and inference strategies, their potential for in-context learning (ICL) remains largely underexplored. To fill this gap, we propose Thinking with Nothinking Calibration (JointThinking), a new ICL paradigm that leverages the structured difference between two reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy. Specifically, our method prompts the model to generate two answers in parallel: one in Thinking mode and the other in Nothinking mode. A second round of Thinking is triggered only when the two initial responses are inconsistent, using a single prompt that incorporates the original question and both candidate answers. Since such disagreement occurs infrequently (e.g., only 6\% in GSM8K), our method performs just one round of reasoning in most cases, resulting in minimal latency overhead. Extensive experiments across multiple reasoning benchmarks demonstrate that JointThinking significantly outperforms few-shot chain-of-thought (CoT) and majority voting with improved answer robustness. Moreover, It achieves comparable in-distribution performance to training-based SOTA method, while substantially outperforming on out-of-distribution tasks. We further conduct a systematic analysis of the calibration mechanism, showing that leveraging different reasoning modes consistently lowers the error rate and highlights the value of structural thinking diversity. Additionally, we observe that the performance gap between actual and ideal reasoning narrows as model size increases in the second round of thinking, indicating the strong scalability of our approach. Finally, we discuss current limitations and outline promising directions for future ICL research in RLLMs.

</details>


### [61] [ReDSM5: A Reddit Dataset for DSM-5 Depression Detection](https://arxiv.org/abs/2508.03399)

*Eliseo Bao, Anxo Pérez, Javier Parapar*

**Main category:** cs.CL

**Keywords:** depression, social media, machine learning, DSM-5, interpretability

**Relevance Score:** 8

**TL;DR:** This paper introduces ReDSM5, a novel annotated Reddit corpus for recognizing and interpreting depressive symptoms using DSM-5 criteria.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address undiagnosed depression by utilizing social media narratives and linking them to DSM-5 diagnostic criteria.

**Method:** The authors annotated a dataset of 1484 Reddit posts at the sentence level for the nine DSM-5 depression symptoms, providing clinical rationales for each annotation.

**Key Contributions:**

	1. Introduction of ReDSM5 corpus with expert annotations for DSM-5 depression symptoms
	2. Combination of symptom-specific supervision with clinical rationales
	3. Establishment of benchmarks for symptom classification and explanation generation.

**Result:** The study conducted an exploratory analysis of symptom expression, establishing benchmarks for multi-label symptom classification and explanation generation.

**Limitations:** The study's scope is limited to data from Reddit and may not generalize to other platforms or cultures.

**Conclusion:** ReDSM5 enables the creation of models that detect depression while providing interpretable reasoning, enhancing clinical relevance in symptom detection.

**Abstract:** Depression is a pervasive mental health condition that affects hundreds of millions of individuals worldwide, yet many cases remain undiagnosed due to barriers in traditional clinical access and pervasive stigma. Social media platforms, and Reddit in particular, offer rich, user-generated narratives that can reveal early signs of depressive symptomatology. However, existing computational approaches often label entire posts simply as depressed or not depressed, without linking language to specific criteria from the DSM-5, the standard clinical framework for diagnosing depression. This limits both clinical relevance and interpretability. To address this gap, we introduce ReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each exhaustively annotated at the sentence level by a licensed psychologist for the nine DSM-5 depression symptoms. For each label, the annotator also provides a concise clinical rationale grounded in DSM-5 methodology. We conduct an exploratory analysis of the collection, examining lexical, syntactic, and emotional patterns that characterize symptom expression in social media narratives. Compared to prior resources, ReDSM5 uniquely combines symptom-specific supervision with expert explanations, facilitating the development of models that not only detect depression but also generate human-interpretable reasoning. We establish baseline benchmarks for both multi-label symptom classification and explanation generation, providing reference results for future research on detection and interpretability.

</details>


### [62] [Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations](https://arxiv.org/abs/2508.03420)

*Bing Wang, Ximing Li, Yiming Wang, Changchun Li, Jiaxu Cui, Renchu Guan, Bo Yang*

**Main category:** cs.CL

**Keywords:** misinformation detection, dynamic representation, temporal model, LSTM, social media

**Relevance Score:** 7

**TL;DR:** A novel framework, MISDER, is proposed for misinformation detection by learning dynamic environmental representations to adapt to changing news veracity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to address the spread of misinformation across social media, which has harmful effects, necessitates effective distinguishing methods that account for dynamic veracity changes in news articles.

**Method:** The proposed MISDER framework utilizes a temporal model with three variants: MISDER-LSTM, MISDER-ODE, and MISDER-PT, to learn environmental representations for predicting future veracity.

**Key Contributions:**

	1. Introduction of dynamic environmental representations for misinformation detection.
	2. Development of three model variants (LSTM, continuous dynamics, pre-trained dynamics) tailored for temporal changes.
	3. Empirical validation on two datasets demonstrating improved performance over existing models.

**Result:** MISDER outperforms various misinformation detection baselines across two prevalent datasets, demonstrating its effectiveness in adapting to changes in the social environment.

**Limitations:** 

**Conclusion:** The effectiveness of the MISDER framework suggests a promising direction for future research in misinformation detection through adaptive modeling.

**Abstract:** The proliferation of misinformation across diverse social media platforms has drawn significant attention from both academic and industrial communities due to its detrimental effects. Accordingly, automatically distinguishing misinformation, dubbed as Misinformation Detection (MD), has become an increasingly active research topic. The mainstream methods formulate MD as a static learning paradigm, which learns the mapping between the content, links, and propagation of news articles and the corresponding manual veracity labels. However, the static assumption is often violated, since in real-world scenarios, the veracity of news articles may vacillate within the dynamically evolving social environment. To tackle this problem, we propose a novel framework, namely Misinformation detection with Dynamic Environmental Representations (MISDER). The basic idea of MISDER lies in learning a social environmental representation for each period and employing a temporal model to predict the representation for future periods. In this work, we specify the temporal model as the LSTM model, continuous dynamics equation, and pre-trained dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM, MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER, we compare it to various MD baselines across 2 prevalent datasets, and the experimental results can indicate the effectiveness of our proposed model.

</details>


### [63] [LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models](https://arxiv.org/abs/2508.03440)

*Junhong Wu, Jinliang Lu, Zixuan Ren, Ganqiang Hu, Zhi Wu, Dai Dai, Hua Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Soft Thinking, Randomness, Gumbel-Softmax, Reasoning Benchmarks

**Relevance Score:** 8

**TL;DR:** This paper investigates the 'Soft Thinking' capabilities of LLMs and identifies limitations in their reasoning paths, proposing randomness strategies to enhance performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and enhance the expressive capabilities of large language models in reasoning tasks by utilizing soft, abstract tokens.

**Method:** The paper employs a series of probing techniques to examine the internal behaviors of various LLMs and tests different sampling strategies, including Dirichlet resampling and the Gumbel-Softmax trick, to introduce randomness in the reasoning process.

**Key Contributions:**

	1. Identification of limitations in LLMs' Soft Thinking capabilities
	2. Introduction of randomness in soft token generation
	3. Demonstration of improved performance using Gumbel-Softmax trick

**Result:** Findings reveal that LLMs often rely on a dominant component of the soft inputs, which limits their reasoning exploration. However, introducing randomness through sampling strategies significantly improves their performance across reasoning tasks.

**Limitations:** Focuses primarily on LLMs and may not generalize to other model types or applications.

**Conclusion:** Incorporating randomness into the Soft Thinking framework can alleviate the limitations of conventional approaches, unlocking better reasoning performance for LLMs.

**Abstract:** Human cognition naturally engages with abstract and fluid concepts, whereas existing reasoning models often rely on generating discrete tokens, potentially constraining their expressive capabilities. Recent advancements aim to address this limitation by enabling large language models (LLMs) to generate soft, abstract tokens, thus facilitating reasoning within a continuous concept space. This paper explores the `Soft Thinking' capabilities of various LLMs by examining the models' internal behavior using a suite of probing techniques. Contrary to the common belief that Soft Thinking enables the simultaneous exploration of diverse reasoning paths, our findings reveal that LLMs predominantly rely on the most influential component of the soft inputs during subsequent decoding steps. This reliance hinders the exploration of different reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding, obscuring the advantage of transmitting more information through Soft Tokens. To tackle this issue, we explore sampling strategies to introduce \emph{randomness}, employing methods such as Dirichlet resampling and the Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness can alleviate the limitations of vanilla approaches and unleash the potential of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate randomness with controlled smoothness, resulting in superior performance across eight reasoning benchmarks.

</details>


### [64] [Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings](https://arxiv.org/abs/2508.03453)

*Rita González-Márquez, Philipp Berens, Dmitry Kobak*

**Main category:** cs.CL

**Keywords:** text embeddings, contrastive learning, self-supervised learning, NLP, data augmentation

**Relevance Score:** 8

**TL;DR:** This paper compares two augmentation strategies for positive pair generation in contrastive learning of text embeddings, showing that cropping augmentation outperforms the dropout-based approach, particularly for in-domain data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the quality of text embeddings used in various NLP applications by comparing self-supervised training strategies with established supervised methods.

**Method:** Systematic comparison of cropping and dropout-based augmentation strategies for generating positive pairs in contrastive learning, evaluated on MTEB and in-domain datasets.

**Key Contributions:**

	1. Comparison of augmentation strategies for text embeddings in NLP
	2. Findings on the effectiveness of self-supervised fine-tuning
	3. Insights on layer-specific fine-tuning benefits.

**Result:** Cropping augmentation significantly outperforms dropout-based methods for in-domain evaluations; self-supervised fine-tuning yields high-quality text embeddings, approaching supervised state-of-the-art after limited tuning.

**Limitations:** Quality of embeddings from self-supervised methods is still below state-of-the-art on out-of-domain data.

**Conclusion:** The study establishes that focusing fine-tuning on the last layers of a transformer model can efficiently produce high-quality embeddings, supporting the case for self-supervised learning in NLP.

**Abstract:** Text embeddings, i.e. vector representations of entire texts, play an important role in many NLP applications, such as retrieval-augmented generation, sentiment analysis, clustering, or visualizing collections of texts for data exploration. Currently, top-performing embedding models are derived from pre-trained language models via extensive supervised fine-tuning using curated text pairs. This contrasts with computer vision, where self-supervised training based on data augmentations has demonstrated remarkable success. Here we systematically compare the two most well-known augmentation strategies for positive pair generation in contrastive learning of text embeddings. We assess embedding quality on MTEB and additional in-domain evaluations and show that cropping augmentation strongly outperforms the dropout-based approach. We find that on out-of-domain data, the quality of resulting embeddings is below the supervised SOTA models, but for in-domain data, self-supervised fine-tuning produces high-quality text embeddings after very short fine-tuning, sometimes only marginally below the supervised SOTA. Finally, we show that representation quality increases towards the last transformer layers, which undergo the largest change during fine-tuning; and that fine-tuning only those last layers is sufficient to reach similar embedding quality.

</details>


### [65] [fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval](https://arxiv.org/abs/2508.03475)

*Pranshu Rastogi*

**Main category:** cs.CL

**Keywords:** multilingual retrieval, crosslingual retrieval, bi-encoder model, Learning-to-Rank, fact-checked claims

**Relevance Score:** 7

**TL;DR:** This paper presents the approach and results of SemEval-2025 Task 7 focused on multilingual and crosslingual fact-checked claim retrieval via a bi-encoder model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The task aims to improve multilingual and crosslingual retrieval processes by refining the Learning-to-Rank methodology while utilizing multilingual data effectively.

**Method:** A bi-encoder model fine-tuned from a pre-trained transformer was employed, trained on both source languages and their English translations for multilingual retrieval, and only English for cross-lingual retrieval.

**Key Contributions:**

	1. Introduction of a bi-encoder model optimized for fact-checked claim retrieval
	2. Successful implementation of multilingual and crosslingual retrieval strategies
	3. Demonstration of high performance with lightweight models under 500M parameters

**Result:** The proposed method achieved 92% Success@10 in multilingual retrieval and 80% Success@10 in crosslingual retrieval, ranking 5th and 10th, respectively, in their respective tracks.

**Limitations:** The effectiveness of the approach may vary with different languages and facts not covered in the training data.

**Conclusion:** The study demonstrates the efficacy of lightweight bi-encoder models in multilingual and crosslingual fact-checked claim retrieval, showcasing competitive performance with fewer parameters.

**Abstract:** SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval is approached as a Learning-to-Rank task using a bi-encoder model fine-tuned from a pre-trained transformer optimized for sentence similarity. Training used both the source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval. Using lightweight models with fewer than 500M parameters and training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.

</details>


### [66] [CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03489)

*Kaiwen Zhao, Bharathan Balaji, Stephen Lee*

**Main category:** cs.CL

**Keywords:** sustainability reports, carbon footprint, LLM-based technique, PDF analysis, question-answering systems

**Relevance Score:** 8

**TL;DR:** This paper presents CarbonPDF-QA, a dataset for asking questions about carbon footprints in sustainability reports and introduces CarbonPDF, an LLM-based approach to improve question answering from PDF documents.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The analysis of product sustainability reports in PDF format is hindered by unstructured text and inconsistent reporting formats, making it difficult to extract and interpret carbon footprint information.

**Method:** The authors created CarbonPDF-QA, an open-source dataset comprising question-answer pairs from 1735 product reports, and developed CarbonPDF by fine-tuning Llama 3 to improve question-answering capabilities.

**Key Contributions:**

	1. Creation of the CarbonPDF-QA dataset for sustainability reports
	2. Introduction of CarbonPDF, an LLM-based approach for improved QA
	3. Demonstration of superior performance compared to existing QA systems

**Result:** CarbonPDF outperforms existing state-of-the-art QA systems, evidencing better handling of inconsistencies in data from sustainability reports.

**Limitations:** The study primarily focuses on carbon footprint questions; other areas of sustainability reporting may not be addressed.

**Conclusion:** The introduction of CarbonPDF provides a promising solution to enhance question answering on carbon footprints, making it easier to analyze sustainability reports effectively.

**Abstract:** Product sustainability reports provide valuable insights into the environmental impacts of a product and are often distributed in PDF format. These reports often include a combination of tables and text, which complicates their analysis. The lack of standardization and the variability in reporting formats further exacerbate the difficulty of extracting and interpreting relevant information from large volumes of documents. In this paper, we tackle the challenge of answering questions related to carbon footprints within sustainability reports available in PDF format. Unlike previous approaches, our focus is on addressing the difficulties posed by the unstructured and inconsistent nature of text extracted from PDF parsing. To facilitate this analysis, we introduce CarbonPDF-QA, an open-source dataset containing question-answer pairs for 1735 product report documents, along with human-annotated answers. Our analysis shows that GPT-4o struggles to answer questions with data inconsistencies. To address this limitation, we propose CarbonPDF, an LLM-based technique specifically designed to answer carbon footprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama 3 with our training data. Our results show that our technique outperforms current state-of-the-art techniques, including question-answering (QA) systems finetuned on table and text data.

</details>


### [67] [UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression](https://arxiv.org/abs/2508.03520)

*Md Rakibul Hasan, Md Zakir Hossain, Aneesh Krishna, Shafin Rahman, Tom Gedeon*

**Main category:** cs.CL

**Keywords:** empathy detection, label noise, probabilistic language model, Bayesian techniques, uncertainty quantification

**Relevance Score:** 7

**TL;DR:** This paper introduces UPLME, a probabilistic language modeling framework for empathy score regression that accounts for label noise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Supervised learning for empathy regression struggles with noisy self-reported scores, necessitating a better approach to handle label noise.

**Method:** UPLME employs a probabilistic language model to predict empathy scores and uncertainty, trained using Bayesian techniques and variational model ensembling, with additional components to manage uncertainty penalties and input similarity.

**Key Contributions:**

	1. Introduction of UPLME framework for empathy regression
	2. New loss components for managing uncertainty and input similarity
	3. Demonstrated effectiveness on benchmarks with label noise

**Result:** UPLME achieves state-of-the-art performance on public benchmarks with noisy labels, improving Pearson correlation and calibration error compared to previous methods.

**Limitations:** 

**Conclusion:** UPLME effectively distinguishes noisy from clean samples by leveraging predicted uncertainty, demonstrating its efficacy in regression tasks with label noise.

**Abstract:** Supervised learning for empathy regression is challenged by noisy self-reported empathy scores. While many algorithms have been proposed for learning with noisy labels in textual classification problems, the regression counterpart is relatively under-explored. We propose UPLME, an uncertainty-aware probabilistic language modelling framework to capture label noise in the regression setting of empathy detection. UPLME includes a probabilistic language model that predicts both empathy score and heteroscedastic uncertainty and is trained using Bayesian concepts with variational model ensembling. We further introduce two novel loss components: one penalises degenerate Uncertainty Quantification (UQ), and another enforces the similarity between the input pairs on which we predict empathy. UPLME provides state-of-the-art performance (Pearson Correlation Coefficient: $0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the performance reported in the literature in two public benchmarks, having label noise. Through synthetic label noise injection, we show that UPLME is effective in separating noisy and clean samples based on the predicted uncertainty. UPLME further outperform (Calibration error: $0.571\rightarrow0.376$) a recent variational model ensembling-based UQ method designed for regression problems.

</details>


### [68] [FilBench: Can LLMs Understand and Generate Filipino?](https://arxiv.org/abs/2508.03523)

*Lester James V. Miranda, Elyanah Aco, Conner Manuel, Jan Christian Blaise Cruz, Joseph Marvin Imperial*

**Main category:** cs.CL

**Keywords:** FilBench, Filipino NLP, Language Benchmarking, LLM Performance, Southeast Asian Languages

**Relevance Score:** 8

**TL;DR:** Introduction of FilBench, a benchmark for evaluating LLMs in Filipino and Southeast Asian languages, revealing specific performance challenges and the need for language-specific evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of knowledge regarding the capabilities of LLMs in Filipino languages and to promote inclusivity in NLP research for the Philippines.

**Method:** The authors introduce FilBench, a curated benchmark that assesses 27 state-of-the-art LLMs on tasks such as Cultural Knowledge, Reading Comprehension, and Generation in Filipino, Tagalog, and Cebuano.

**Key Contributions:**

	1. Introduction of FilBench for Filipino language evaluation
	2. Analysis of 27 LLMs performance on Filipino-centric tasks
	3. Highlighting underperformance of existing models for Southeast Asian languages

**Result:** Evaluation results show that many LLMs struggle with reading comprehension and translation in Filipino, with the best model scoring only 72.23% on FilBench, highlighting significant performance gaps.

**Limitations:** Primarily focused on Filipino languages, not generalizing results to other languages or applications.

**Conclusion:** The study underscores the importance of creating language-specific benchmarks like FilBench to enhance NLP research and LLM development for Philippine languages.

**Abstract:** Despite the impressive performance of LLMs on English-based tasks, little is known about their capabilities in specific languages such as Filipino. In this work, we address this gap by introducing FilBench, a Filipino-centric benchmark designed to evaluate LLMs across a diverse set of tasks and capabilities in Filipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to reflect the priorities and trends of NLP research in the Philippines such as Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By evaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs suffer from reading comprehension and translation capabilities. Our results indicate that FilBench is challenging, with the best model, GPT-4o, achieving only a score of 72.23%. Moreover, we also find that models trained specifically for Southeast Asian languages tend to underperform on FilBench, with the highest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%. Our work demonstrates the value of curating language-specific LLM benchmarks to aid in driving progress on Filipino NLP and increasing the inclusion of Philippine languages in LLM development.

</details>


### [69] [Marito: Structuring and Building Open Multilingual Terminologies for South African NLP](https://arxiv.org/abs/2508.03529)

*Vukosi Marivate, Isheanesu Dzingirai, Fiskani Banda, Richard Lastrucci, Thapelo Sindane, Keabetswe Madumo, Kayode Olaleye, Abiodun Modupe, Unarine Netshifhefhe, Herkulaas Combrink, Mohlatlego Nakeng, Matome Ledwaba*

**Main category:** cs.CL

**Keywords:** multilingual NLP, terminology dataset, machine translation, RAG, South Africa

**Relevance Score:** 8

**TL;DR:** The paper introduces the Marito dataset, which aggregates and standardizes terminological data for South Africa's official languages, enhancing multilingual NLP applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical lack of structured terminological data for South Africa's languages that impedes multilingual NLP progress.

**Method:** The authors systematically aggregate, clean, and standardize fragmented terminology resources into open, interoperable datasets and integrate this into a RAG pipeline.

**Key Contributions:**

	1. Introduction of the Marito dataset under the NOODL framework
	2. Demonstration of improved machine translation performance with the dataset
	3. Promotion of data equity in NLP for South Africa's languages

**Result:** Experiments show significant improvements in English-to-Tshivenda machine translation accuracy and domain-specific consistency using the Marito dataset.

**Limitations:** 

**Conclusion:** The Marito dataset provides a scalable foundation for developing equitable NLP technologies, promoting representation of South Africa's linguistic diversity.

**Abstract:** The critical lack of structured terminological data for South Africa's official languages hampers progress in multilingual NLP, despite the existence of numerous government and academic terminology lists. These valuable assets remain fragmented and locked in non-machine-readable formats, rendering them unusable for computational research and development. \emph{Marito} addresses this challenge by systematically aggregating, cleaning, and standardising these scattered resources into open, interoperable datasets. We introduce the foundational \emph{Marito} dataset, released under the equitable, Africa-centered NOODL framework. To demonstrate its immediate utility, we integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline. Experiments show substantial improvements in the accuracy and domain-specific consistency of English-to-Tshivenda machine translation for large language models. \emph{Marito} provides a scalable foundation for developing robust and equitable NLP technologies, ensuring South Africa's rich linguistic diversity is represented in the digital age.

</details>


### [70] [EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models](https://arxiv.org/abs/2508.03533)

*Xiaoming Hou, Jiquan Zhang, Zibin Lin, DaCheng Tao, Shengli Zhang*

**Main category:** cs.CL

**Keywords:** Prompt engineering, Text embeddings, Gradient-based refinement

**Relevance Score:** 9

**TL;DR:** The paper introduces EmbedGrad, a framework that refines text prompt embeddings using gradient-based techniques to enhance task adaptation in AI models without adding complexity or sacrificing interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Adapting pretrained foundation models for diverse tasks is a challenge due to limitations in current discrete and continuous optimization techniques.

**Method:** EmbedGrad uses gradient-based refinement of text prompt embeddings guided by labeled examples during training and integrates optimized embeddings with user queries during inference.

**Key Contributions:**

	1. Introduction of EmbedGrad for optimizing text prompt embeddings
	2. Decoupling training from deployment for improved interpretability and efficiency
	3. Demonstrated effectiveness across multiple AI model scales and tasks

**Result:** EmbedGrad demonstrated significant accuracy improvements in various tasks, notably increasing mathematical reasoning accuracy from 14.74% to 58.96%, with consistent gains across model scales and tasks.

**Limitations:** 

**Conclusion:** The framework establishes embedding refinement as a new effective paradigm for enhancing task adaptation of AI models.

**Abstract:** Effectively adapting powerful pretrained foundation models to diverse tasks remains a key challenge in AI deployment. Current approaches primarily follow two paradigms:discrete optimization of text prompts through prompt engineering, or continuous adaptation via additional trainable parameters. Both exhibit limitations-discrete methods lack refinement precision while parameter-based techniques increase complexity and reduce interpretability. To address these constraints, we propose EmbedGrad, a novel framework that optimizes text prompt embeddings through gradient-based refinement. Our approach uniquely decouples training from deployment:during optimization,labeled examples guide precise embedding adjustments while preserving semantic meaning; during inference, only optimized embeddings integrate with user queries. This enables fine-grained calibration impossible in text space, such as enhancing the reasoning capability of prompts like please reason step by step. Comprehensive evaluations across mathematical reasoning, sentiment analysis, and causal judgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on mathematical problems. Consistent improvements were observed across model scales (0.5B-14B) and all tasks, with particularly significant gains for smaller models on complex problems like causal judgment. By bridging prompt engineering and parameter efficiency without architectural changes, our work establishes embedding refinement as a powerful new paradigm for task adaptation.

</details>


### [71] [Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations](https://arxiv.org/abs/2508.03550)

*Peng Lai, Jianjie Zheng, Sijie Cheng, Yun Chen, Peng Li, Yang Liu, Guanhua Chen*

**Main category:** cs.CL

**Keywords:** LLM-as-a-judge, human alignment, cross-layer representations, evaluation benchmarks, machine learning

**Relevance Score:** 9

**TL;DR:** LAGER is a framework to enhance LLM alignment with human preferences using cross-layer representations, outperforming existing methods in evaluation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of LLM evaluations with human preferences without complex prompts or fine-tuning.

**Method:** LAGER aggregates cross-layer score-token logits to produce fine-grained judgment scores while keeping the LLM backbone frozen.

**Key Contributions:**

	1. Proposes a lightweight and efficient framework (LAGER) for LLM alignment using internal representations.
	2. Demonstrates improvement in evaluation scores through cross-layer representation aggregation.
	3. Validates effectiveness on standard alignment benchmarks and downstream applications.

**Result:** LAGER achieves improvements of up to 7.5% over baseline models in alignment benchmarks and matches or outperforms reasoning-based methods.

**Limitations:** 

**Conclusion:** LAGER demonstrates effectiveness in enhancing LLM evaluations and shows promise in downstream applications like data selection and emotional understanding.

**Abstract:** The growing scale of evaluation tasks has led to the widespread adoption of automated evaluation using large language models, a paradigm known as "LLMas-a-judge." However, improving its alignment with human preferences without complex prompts or fine-tuning remains challenging. In this work, motivated by preliminary findings that middle-to-upper layers encode semantically and taskrelevant representations that are often more aligned with human judgments than the final layer, we propose LAGER, a lightweight and efficient framework for enhancing LLM-as-a-Judge alignment with human scoring, via internal representations. LAGER produces fine-grained judgment scores by aggregating cross-layer scoretoken logits and computing the expected score from a softmax-based distribution, with the LLM backbone kept frozen. LAGER fully leverages the complementary information across different layers, overcoming the limitations of relying solely on the final layer. We evaluate our method on the standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find that LAGER achieves improvements of up to 7.5% over the best baseline across these benchmarks. Without reasoning steps, LAGER matches or outperforms reasoning-based methods. Experiments on downstream applications, such as data selection and emotional understanding, further show the effectiveness of our method.

</details>


### [72] [Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation](https://arxiv.org/abs/2508.03571)

*Iing Muttakhiroh, Thomas Fevens*

**Main category:** cs.CL

**Keywords:** Continual Learning, Large Language Models, Knowledge Graphs, Instruction Tuning, Domain Adaptation

**Relevance Score:** 9

**TL;DR:** KILO is a continual learning framework that combines dynamic knowledge graphs and instruction tuning to improve domain adaptability and knowledge retention in Large Language Models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance degradation in Large Language Models (LLMs) during domain shifts due to catastrophic forgetting.

**Method:** KILO integrates dynamic knowledge graphs with instruction tuning, leveraging domain-specific knowledge during training.

**Key Contributions:**

	1. Introduction of KILO framework for continual learning in LLMs
	2. Combination of knowledge graphs with instruction tuning
	3. Demonstration of superior performance across multiple domains

**Result:** KILO outperforms strong baselines in backward transfer, forward transfer, F1 score, retention rate, and training efficiency across four diverse domains: BioASQ, SciQ, TweetEval, and MIND.

**Limitations:** 

**Conclusion:** The combination of structured knowledge retrieval and instruction prompting effectively addresses domain shift challenges in continual learning.

**Abstract:** Large Language Models (LLMs) often suffer from performance degradation when faced with domain shifts, primarily due to catastrophic forgetting. In this work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation), a novel continual learning framework that integrates dynamic knowledge graphs with instruction tuning. By leveraging retrieved domain-specific knowledge as guidance during training, KILO enhances both adaptability to new domains and retention of previously acquired knowledge. We pretrain our model on WikiText-103 and evaluate sequential adaptation across four diverse target domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that KILO consistently outperforms strong baselines, including continual fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward transfer, F1 score, retention rate, and training efficiency. These results highlight the effectiveness of combining structured knowledge retrieval and instruction prompting to overcome domain shift challenges in continual learning scenarios.

</details>


### [73] [Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?](https://arxiv.org/abs/2508.03644)

*Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Multimodal Large Language Models, Document Evaluation

**Relevance Score:** 9

**TL;DR:** Introduction of Double-Bench, a comprehensive evaluation system for retrieval-augmented generation (RAG) systems focusing on multilingual and multimodal document understanding.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of existing benchmarks that fail to accurately reflect the challenges faced in real-world document RAG systems.

**Method:** Double-Bench features 3,276 documents and 5,168 queries across multiple languages and document types, assessed through extensive human verification.

**Key Contributions:**

	1. Introduction of a multilingual and multimodal evaluation system
	2. Comprehensiveness ensured through human-verified queries and evidence
	3. Revealing the over-confidence issue in current document RAG frameworks.

**Result:** Experiments highlight the diminishing gap between text and visual embeddings and unveil a tendency in RAG frameworks to overestimate their confidence when providing answers without evidence support.

**Limitations:** 

**Conclusion:** Double-Bench aims to serve as a foundational resource for future studies in document RAG, with plans for annual updates and corpus retrieval.

**Abstract:** Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.

</details>


### [74] [Can Large Vision-Language Models Understand Multimodal Sarcasm?](https://arxiv.org/abs/2508.03654)

*Xinyu Wang, Yue Zhang, Liqiang Jing*

**Main category:** cs.CL

**Keywords:** sarcasm detection, multimodal analysis, Large Visual Language Models

**Relevance Score:** 7

**TL;DR:** This paper explores the application of Large Visual Language Models (LVLMs) in Multimodal Sarcasm Analysis, proposing a framework that enhances sarcasm detection and explanation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the challenges of sarcasm detection in sentiment analysis, particularly using multimodal approaches with LVLMs, which have been underexplored.

**Method:** The authors conduct comprehensive experiments on Multimodal Sarcasm Detection and Explanation, identifying limitations such as poor visual understanding and lack of conceptual knowledge, and propose a training-free framework that combines object extraction and external knowledge.

**Key Contributions:**

	1. Evaluation of LVLMs in Multimodal Sarcasm Analysis tasks
	2. Identification of limitations in existing LVLM methodologies
	3. Proposal of a training-free framework for improved sarcasm interpretation

**Result:** The experimental results demonstrate the effectiveness of the proposed framework in improving sarcasm interpretation and explanation across multiple models.

**Limitations:** Key limitations identified include insufficient visual understanding and a lack of conceptual knowledge in existing models.

**Conclusion:** The study shows that integrating object extraction and conceptual knowledge can significantly enhance the performance of LVLMs in detecting and explaining sarcasm in multimodal settings.

**Abstract:** Sarcasm is a complex linguistic phenomenon that involves a disparity between literal and intended meanings, making it challenging for sentiment analysis and other emotion-sensitive tasks. While traditional sarcasm detection methods primarily focus on text, recent approaches have incorporated multimodal information. However, the application of Large Visual Language Models (LVLMs) in Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we evaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm Detection and Multimodal Sarcasm Explanation. Through comprehensive experiments, we identify key limitations, such as insufficient visual understanding and a lack of conceptual knowledge. To address these issues, we propose a training-free framework that integrates in-depth object extraction and external conceptual knowledge to improve the model's ability to interpret and explain sarcasm in multimodal contexts. The experimental results on multiple models show the effectiveness of our proposed framework. The code is available at https://github.com/cp-cp/LVLM-MSA.

</details>


### [75] [CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction](https://arxiv.org/abs/2508.03668)

*Zixuan Li, Binzong Geng, Jing Xiong, Yong He, Yuxuan Hu, Jian Chen, Dingwei Chen, Xiyu Chang, Liang Zhang, Linjian Mo, Chengming Li, Chuan Yuan, Zhenan Sun*

**Main category:** cs.CL

**Keywords:** Click-Through Rate, recommendation systems, Language Models, attention mechanisms, user behavior sequences

**Relevance Score:** 8

**TL;DR:** This paper presents CTR-Sink, a novel framework for Click-Through Rate prediction that addresses the issue of semantic fragmentation in user behavior sequences by introducing behavior-level attention sinks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is the identified structural gap in using Language Models for Click-Through Rate prediction, specifically the mismatch between user behavior sequences and natural language coherence.

**Method:** The method involves inserting sink tokens between consecutive user behaviors and applying a two-stage training strategy to regulate attention aggregation and enhance the capturing of behavioral correlations.

**Key Contributions:**

	1. Introduction of behavior-level attention sinks for recommendation scenarios
	2. Development of a two-stage training strategy to guide attention focus
	3. Incorporation of temporal distance signals for attention regulation

**Result:** Experiments demonstrate that CTR-Sink improves prediction performance on various datasets, effectively addressing the issue of semantic fragmentation.

**Limitations:** 

**Conclusion:** The proposed CTR-Sink framework significantly enhances Click-Through Rate prediction by leveraging behavior-level attention sinks and demonstrating its effectiveness in diverse scenarios.

**Abstract:** Click-Through Rate (CTR) prediction, a core task in recommendation systems, estimates user click likelihood using historical behavioral data. Modeling user behavior sequences as text to leverage Language Models (LMs) for this task has gained traction, owing to LMs' strong semantic understanding and contextual modeling capabilities. However, a critical structural gap exists: user behavior sequences consist of discrete actions connected by semantically empty separators, differing fundamentally from the coherent natural language in LM pre-training. This mismatch causes semantic fragmentation, where LM attention scatters across irrelevant tokens instead of focusing on meaningful behavior boundaries and inter-behavior relationships, degrading prediction performance. To address this, we propose $\textit{CTR-Sink}$, a novel framework introducing behavior-level attention sinks tailored for recommendation scenarios. Inspired by attention sink theory, it constructs attention focus sinks and dynamically regulates attention aggregation via external information. Specifically, we insert sink tokens between consecutive behaviors, incorporating recommendation-specific signals such as temporal distance to serve as stable attention sinks. To enhance generality, we design a two-stage training strategy that explicitly guides LM attention toward sink tokens and a attention sink mechanism that amplifies inter-sink dependencies to better capture behavioral correlations. Experiments on one industrial dataset and two open-source datasets (MovieLens, Kuairec), alongside visualization results, validate the method's effectiveness across scenarios.

</details>


### [76] [FairLangProc: A Python package for fairness in NLP](https://arxiv.org/abs/2508.03677)

*Arturo Pérez-Peralta, Sandra Benítez-Peña, Rosa E. Lillo*

**Main category:** cs.CL

**Keywords:** fairness, natural language processing, bias mitigation, large language models, machine learning

**Relevance Score:** 9

**TL;DR:** The paper introduces FairLangProc, a Python package designed to standardize and implement recent advances in fairness in Natural Language Processing (NLP), particularly in mitigating bias in decision-making contexts.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To address societal concerns regarding the fairness of Large Language Models in decision-making contexts such as healthcare and organizational justice.

**Method:** Development of a comprehensive Python package that provides a unified implementation of recent fairness techniques in NLP, compatible with Hugging Face transformers.

**Key Contributions:**

	1. Introduction of FairLangProc for bias mitigation in NLP
	2. Compatibility with Hugging Face transformers
	3. Centralized implementation of fairness techniques

**Result:** FairLangProc is introduced to encourage the adoption and democratization of bias mitigation techniques in NLP, with centralized methods over diverse existing implementations.

**Limitations:** 

**Conclusion:** The package aims to facilitate the integration of fairness practices in NLP applications, promoting equitable outcomes in critical decision-making scenarios.

**Abstract:** The rise in usage of Large Language Models to near ubiquitousness in recent years has risen societal concern about their applications in decision-making contexts, such as organizational justice or healthcare. This, in turn, poses questions about the fairness of these models in critical settings, which leads to the developement of different procedures to address bias in Natural Language Processing. Although many datasets, metrics and algorithms have been proposed to measure and mitigate harmful prejudice in Natural Language Processing, their implementation is diverse and far from centralized. As a response, this paper presents FairLangProc, a comprehensive Python package providing a common implementation of some of the more recent advances in fairness in Natural Language Processing providing an interface compatible with the famous Hugging Face transformers library, aiming to encourage the widespread use and democratization of bias mitigation techniques. The implementation can be found on https://github.com/arturo-perez-peralta/FairLangProc.

</details>


### [77] [More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation](https://arxiv.org/abs/2508.03678)

*Yangtian Zi, Harshitha Menon, Arjun Guha*

**Main category:** cs.CL

**Keywords:** Large Language Models, code generation, prompt engineering, benchmarking

**Relevance Score:** 8

**TL;DR:** This paper introduces PartialOrderEval, a method to augment code generation benchmarks by varying prompt detail to analyze how prompt specificity affects the performance of Large Language Models on specialized tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reasons behind the varying performance of LLMs on general versus specialized code generation benchmarks, focusing on the impact of prompt detail.

**Method:** Developing PartialOrderEval, a framework that systematically varies prompt specificity from minimal to maximally detailed and measuring its effect on pass rates in HumanEval and ParEval benchmarks.

**Key Contributions:**

	1. Introduction of PartialOrderEval for evaluating prompts
	2. Demonstrated sensitivity of LLM performance to prompt specificity
	3. Identified key factors in prompt detail that improve model performance

**Result:** Experiments demonstrate that LLM performance, specifically pass@1 scores, varies significantly depending on prompt detail, highlighting the importance of explicit instructions in prompts.

**Limitations:** 

**Conclusion:** Improving prompt detail, specifically through explicit I/O specifications and stepwise breakdowns, can enhance LLM performance on coding tasks, indicating a need for tailored prompting strategies.

**Abstract:** State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general benchmarks like HumanEval but underperform on specialized suites such as ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt detail is given? To answer this, we introduce PartialOrderEval, which augments any code generation benchmark with a partial order of prompts from minimal to maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets of ParEval, we measure how pass@1 scales with prompt specificity. Our experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of prompt sensitivity across different tasks, and a qualitative analysis highlights explicit I/O specifications, edge-case handling, and stepwise breakdowns as the key drivers of prompt detail improvement.

</details>


### [78] [CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward](https://arxiv.org/abs/2508.03686)

*Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen*

**Main category:** cs.CL

**Keywords:** answer verification, large language models, machine learning, benchmark, reinforcement learning

**Relevance Score:** 9

**TL;DR:** The paper presents CompassVerifier, a robust model for answer verification in large language models (LLMs), addressing existing limitations in evaluation frameworks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the verification process for evaluating LLM outputs against standard answers and to enhance LLM optimization.

**Method:** Development of CompassVerifier, a lightweight verifier model, and introduction of the VerifierBench benchmark for systematic evaluation of verification capabilities across LLMs.

**Key Contributions:**

	1. Introduction of CompassVerifier for answer verification
	2. Development of VerifierBench benchmark
	3. Demonstration of multi-domain competency in answer verification

**Result:** CompassVerifier demonstrates high accuracy and robustness in verifying multi-domain tasks including math and reasoning, effectively handling diverse answer types and identifying invalid responses.

**Limitations:** Current verifier development is in early stages and may not yet handle all complex edge cases reducibly.

**Conclusion:** CompassVerifier and VerifierBench are expected to advance answer verification, evaluation methods, and reinforcement learning in LLM research.

**Abstract:** Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.

</details>


### [79] [Pre-trained Transformer-Based Approach for Arabic Question Answering : A Comparative Study](https://arxiv.org/abs/2111.05671)

*Kholoud Alsubhi, Amani Jamal, Areej Alhothali*

**Main category:** cs.CL

**Keywords:** question answering, Arabic NLP, pre-trained models, transformers, reading comprehension

**Relevance Score:** 4

**TL;DR:** This paper evaluates various pre-trained transformer models for Arabic question answering (QA) using multiple datasets to address the slower pace of QA research in Arabic compared to English.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges and slow progress in Arabic question answering research due to a lack of resources and datasets, as well as to leverage recent advancements in pre-trained language models.

**Method:** The authors fine-tune and compare the performance of three pre-trained models: AraBERTv2-base, AraBERTv0.2-large, and AraELECTRA on four Arabic reading comprehension datasets.

**Key Contributions:**

	1. Evaluation of state-of-the-art pre-trained transformer models for Arabic QA.
	2. Comparison across multiple Arabic reading comprehension datasets.
	3. Analysis of low-performance results to provide insights into model behavior.

**Result:** The paper details the performance metrics of the models across the datasets, highlighting the effectiveness of the pre-trained models in Arabic QA.

**Limitations:** The study is limited by the scarcity of Arabic QA datasets and the variability in model performance across different contexts.

**Conclusion:** Despite advancements in pre-trained language models, some models still exhibit low performance in certain contexts, indicating the need for further investigation and understanding of these limitations.

**Abstract:** Question answering(QA) is one of the most challenging yet widely investigated problems in Natural Language Processing (NLP). Question-answering (QA) systems try to produce answers for given questions. These answers can be generated from unstructured or structured text. Hence, QA is considered an important research area that can be used in evaluating text understanding systems. A large volume of QA studies was devoted to the English language, investigating the most advanced techniques and achieving state-of-the-art results. However, research efforts in the Arabic question-answering progress at a considerably slower pace due to the scarcity of research efforts in Arabic QA and the lack of large benchmark datasets. Recently many pre-trained language models provided high performance in many Arabic NLP problems. In this work, we evaluate the state-of-the-art pre-trained transformers models for Arabic QA using four reading comprehension datasets which are Arabic-SQuAD, ARCD, AQAD, and TyDiQA-GoldP datasets. We fine-tuned and compared the performance of the AraBERTv2-base model, AraBERTv0.2-large model, and AraELECTRA model. In the last, we provide an analysis to understand and interpret the low-performance results obtained by some models.

</details>


### [80] [Domain-Independent Automatic Generation of Descriptive Texts for Time-Series Data](https://arxiv.org/abs/2409.16647)

*Kota Dohi, Aoi Ito, Harsh Purohit, Tomoya Nishida, Takashi Endo, Yohei Kawaguchi*

**Main category:** cs.CL

**Keywords:** time-series data, descriptive text generation, TACO dataset, contrastive learning, domain-independent

**Relevance Score:** 6

**TL;DR:** This study proposes a method for generating descriptive texts from time-series data using a novel dataset called TACO.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of annotated time-series data makes training models for generating descriptive texts challenging.

**Method:** The study introduces two approaches: a forward approach and a backward approach. The backward approach is used to create the TACO dataset.

**Key Contributions:**

	1. Introduction of the TACO dataset
	2. Development of a novel backward approach for text generation
	3. Demonstration of effective contrastive learning for time-series descriptions

**Result:** A contrastive learning-based model trained on the TACO dataset demonstrated the ability to generate descriptive texts for time-series data across various domains.

**Limitations:** 

**Conclusion:** The proposed method successfully enables the generation of domain-independent descriptive texts from time-series data.

**Abstract:** Due to scarcity of time-series data annotated with descriptive texts, training a model to generate descriptive texts for time-series data is challenging. In this study, we propose a method to systematically generate domain-independent descriptive texts from time-series data. We identify two distinct approaches for creating pairs of time-series data and descriptive texts: the forward approach and the backward approach. By implementing the novel backward approach, we create the Temporal Automated Captions for Observations (TACO) dataset. Experimental results demonstrate that a contrastive learning based model trained using the TACO dataset is capable of generating descriptive texts for time-series data in novel domains.

</details>


### [81] [From Text to Trajectory: Exploring Complex Constraint Representation and Decomposition in Safe Reinforcement Learning](https://arxiv.org/abs/2412.08920)

*Pusen Dong, Tianchen Zhu, Yue Qiu, Haoyi Zhou, Jianxin Li*

**Main category:** cs.CL

**Keywords:** safe reinforcement learning, natural language constraints, TTCT

**Relevance Score:** 6

**TL;DR:** Introducing a novel method to handle safe reinforcement learning using natural language constraints without manual cost function design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the flexibility and accessibility of safe reinforcement learning by using natural language constraints instead of manual cost function design.

**Method:** The Trajectory-level Textual Constraints Translator (TTCT) is introduced, which uses natural language both as a constraint and a training signal.

**Key Contributions:**

	1. TTCT replaces manually designed cost functions with a natural language constraint translator.
	2. Improvements in policy compliance with constraints, leading to lower violation rates.
	3. Demonstration of zero-shot transfer capability to adapt to changing constraints.

**Result:** TTCT improves comprehension of constraints and trajectories, resulting in policies with a lower violation rate compared to traditional cost functions, and demonstrates zero-shot transfer capability to constraint-shift environments.

**Limitations:** 

**Conclusion:** TTCT shows promise in making safe RL more adaptable and efficient by utilizing natural language constraints effectively.

**Abstract:** Safe reinforcement learning (RL) requires the agent to finish a given task while obeying specific constraints. Giving constraints in natural language form has great potential for practical scenarios due to its flexible transfer capability and accessibility. Previous safe RL methods with natural language constraints typically need to design cost functions manually for each constraint, which requires domain expertise and lacks flexibility. In this paper, we harness the dual role of text in this task, using it not only to provide constraint but also as a training signal. We introduce the Trajectory-level Textual Constraints Translator (TTCT) to replace the manually designed cost function. Our empirical results demonstrate that TTCT effectively comprehends textual constraint and trajectory, and the policies trained by TTCT can achieve a lower violation rate than the standard cost function. Extra studies are conducted to demonstrate that the TTCT has zero-shot transfer capability to adapt to constraint-shift environments.

</details>


### [82] [CLIPPER: Compression enables long-context synthetic data generation](https://arxiv.org/abs/2502.14854)

*Chau Minh Pham, Yapei Chang, Mohit Iyyer*

**Main category:** cs.CL

**Keywords:** synthetic data, narrative claim verification, compression-based generation, chain-of-thought reasoning, machine learning

**Relevance Score:** 8

**TL;DR:** CLIPPER is a compression-based approach to generate high-quality synthetic data for narrative claim verification by summarizing books before claim generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Generating high-quality synthetic data for long-context reasoning tasks, specifically narrative claim verification, is challenging.

**Method:** CLIPPER compresses books into chapter outlines and summaries before generating claims and reasoning, enhancing the quality of the outputs compared to direct claim generation.

**Key Contributions:**

	1. Introduction of CLIPPER for synthetic data generation in narrative claim verification.
	2. Creation of a 19K dataset of synthetic claims paired with text and reasoning.
	3. Achieving a significant accuracy improvement on narrative claim verification tasks.

**Result:** CLIPPER successfully created a dataset with 19K synthetic claims and improved a model's accuracy on narrative claim verification from 28% to 76%, achieving state-of-the-art results for sub-10B models.

**Limitations:** 

**Conclusion:** Using CLIPPER improves the validity and complexity of claims for narrative tasks and enhances chain-of-thought reasoning capabilities.

**Abstract:** LLM developers are increasingly reliant on synthetic data, but generating high-quality data for complex long-context reasoning tasks remains challenging. We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification - a task that requires reasoning over a book to verify a given claim. Instead of generating claims directly from the raw text of the book, which results in artifact-riddled claims, CLIPPER first compresses the book into chapter outlines and book summaries and then uses these intermediate representations to generate complex claims and corresponding chain-of-thoughts. Compared to naive approaches, CLIPPER produces claims that are more valid, grounded, and complex. Using CLIPPER, we construct a dataset of 19K synthetic book claims paired with their source texts and chain-of-thought reasoning, and use it to fine-tune three open-weight models. Our best model achieves breakthrough results on narrative claim verification (from 28% to 76% accuracy on our test set) and sets a new state-of-the-art for sub-10B models on the NoCha leaderboard. Further analysis shows that our models generate more detailed and grounded chain-of-thought reasoning while also improving performance on other narrative understanding tasks (e.g., NarrativeQA).

</details>


### [83] [GEMA-Score: Granular Explainable Multi-Agent Scoring Framework for Radiology Report Evaluation](https://arxiv.org/abs/2503.05347)

*Zhenxuan Zhang, Kinhei Lee, Peiyuan Jing, Weihang Deng, Huichi Zhou, Zihao Jin, Jiahao Huang, Zhifan Gao, Dominic C Marshall, Yingying Fang, Guang Yang*

**Main category:** cs.CL

**Keywords:** medical report generation, evaluation metrics, clinical reliability, multi-agent workflow, large language models

**Relevance Score:** 9

**TL;DR:** This paper introduces GEMA-Score, a new evaluation metric for automatic medical report generation that focuses on evaluating the clinical reliability of generated reports.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluation metrics for automatic medical report generation fail to adequately capture clinical reliability, leading to potential risks in clinical use.

**Method:** The GEMA-Score uses a multi-agent workflow with a large language model to perform both objective quantification and subjective evaluation of medical reports.

**Key Contributions:**

	1. Introduction of GEMA-Score for evaluating clinical reports
	2. Utilization of multi-agent workflow for comprehensive evaluation
	3. Demonstrated high correlation with human expert evaluations

**Result:** GEMA-Score shows a high correlation with human expert evaluations, achieving a Kendall coefficient of 0.69 for the ReXVal dataset and 0.45 for the RadEvalX dataset.

**Limitations:** 

**Conclusion:** GEMA-Score is effective in assessing the clinical reliability of automated medical reports and provides explanatory feedback for further improvement.

**Abstract:** Automatic medical report generation has the potential to support clinical diagnosis, reduce the workload of radiologists, and demonstrate potential for enhancing diagnostic consistency. However, current evaluation metrics often fail to reflect the clinical reliability of generated reports. Early overlap-based methods focus on textual matches between predicted and ground-truth entities but miss fine-grained clinical details (e.g., anatomical location, severity). Some diagnostic metrics are limited by fixed vocabularies or templates, reducing their ability to capture diverse clinical expressions. LLM-based approaches further lack interpretable reasoning steps, making it hard to assess or trust their behavior in safety-critical settings. These limitations hinder the comprehensive assessment of the reliability of generated reports and pose risks in their selection for clinical use. Therefore, we propose a Granular Explainable Multi-Agent Score (GEMA-Score) in this paper, which conducts both objective quantification and subjective evaluation through a large language model-based multi-agent workflow. Our GEMA-Score parses structured reports and employs stable calculations through interactive exchanges of information among agents to assess disease diagnosis, location, severity, and uncertainty. Additionally, an LLM-based scoring agent evaluates completeness, readability, and clinical terminology while providing explanatory feedback. Extensive experiments validate that GEMA-Score achieves the highest correlation with human expert evaluations on a public dataset, demonstrating its effectiveness in clinical scoring (Kendall coefficient = $0.69$ for ReXVal dataset and Kendall coefficient = $0.45$ for RadEvalX dataset). The anonymous project demo is available at: https://github.com/Zhenxuan-Zhang/GEMA_score.

</details>


### [84] [GPT is Devastated and LLaMA is Content: Emotion Representation Alignment in LLMs for Keyword-based Generation](https://arxiv.org/abs/2503.11881)

*Shadab Choudhury, Asha Kumar, Lara J. Martin*

**Main category:** cs.CL

**Keywords:** Representation Alignment, Emotion Representation, Large Language Models, Human Evaluation, Text Generation

**Relevance Score:** 9

**TL;DR:** This paper introduces Representation Alignment, a human evaluation task for measuring the gap between LLM interpretations of concepts and human expectations on emotion representation in text generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding and measuring the gap between LLM interpretations of concepts and human expectations in text generation, particularly with emotions.

**Method:** Human evaluation of four emotion representations (Words, VAD in Lexical and Numeric forms, Emojis) during keyword-guided sentence generation with GPT-4 and LLaMA-3.

**Key Contributions:**

	1. Introduction of the Representation Alignment task
	2. Empirical findings showing the preference for words over VAD scales in emotion representation
	3. Insights into how different emotion representations impact perception in generated sentences.

**Result:** People found that LLMs generate more agreeable outputs when using words over VAD representations, particularly Numeric VAD, indicating a dependency of emotion perception on representation type.

**Limitations:** Limited to emotion representations and specific language models used; further research needed on broader contexts.

**Conclusion:** The findings suggest that using English words leads to better alignment with human interpretations of emotions in LLM-generated sentences over other representation forms.

**Abstract:** In controlled text generation using large language models (LLMs), gaps arise between the language model's interpretation of concepts and people's expectations. We introduce the human evaluation task of Representation Alignment for measuring this gap. We selected four emotion representations: Words, Valence-Arousal-Dominance (VAD) dimensions expressed in both Lexical and Numeric forms, and Emojis and evaluate them in the context of keyword-guided sentence generation using both GPT-4 and LLaMA-3. In addition to Representation Alignment, we also measure people's judgments of the accuracy and realism of the generated sentences. While representations like VAD break emotions into easy-to-compute components, our findings show that people agree more with how LLMs generate when conditioned on English words (e.g., ``angry'') rather than VAD scales. This difference is especially visible when comparing Numeric VAD to words. Furthermore, we found that the perception of how much a generated sentence conveys an emotion is dependent on both the representation type and which emotion it is.

</details>


### [85] [Ensemble Learning for Large Language Models in Text and Code Generation: A Survey](https://arxiv.org/abs/2503.13505)

*Mari Ashiga, Wei Jie, Fan Wu, Vardan Voskanyan, Fateme Dinmohammadi, Paul Brookes, Jingzhi Gong, Zheng Wang*

**Main category:** cs.CL

**Keywords:** Generative Pretrained Transformers, LLM ensemble techniques, code generation, text generation, multimodal LLMs

**Relevance Score:** 8

**TL;DR:** This article reviews ensemble techniques for Generative Pretrained Transformers (GPTs) to improve text and code generation outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to address inconsistencies and biases in individual LLMs and the challenges posed by the closed-source nature of powerful LLMs.

**Method:** The paper categorizes LLM ensemble techniques into seven methods: weight merging, knowledge fusion, mixture-of-experts, reward ensemble, output ensemble, routing, and cascading, analyzing their capabilities.

**Key Contributions:**

	1. Categorization of ensemble techniques for LLMs
	2. Analysis of capabilities and benefits of LLM ensembles
	3. Foundation for extending ensemble approaches to multimodal models

**Result:** The review highlights benefits such as improved diversity representation, enhanced output quality, and greater application flexibility.

**Limitations:** 

**Conclusion:** The insights from this paper assist in model selection for real-world tasks and set the stage for the application of ensemble strategies in multimodal LLMs.

**Abstract:** Generative Pretrained Transformers (GPTs) are foundational Large Language Models (LLMs) for text generation. However, individual LLMs often produce inconsistent outputs and exhibit biases, limiting their representation of diverse language patterns. The closed-source nature of many powerful LLMs further restricts industry applications due to data privacy concerns. Inspired by successes in text generation, LLM ensemble techniques are now increasingly explored for code generation. This article reviews these emerging ensemble approaches to enhance understanding, encourage further research, and promote practical implementation in both text and code generation. We categorize LLM ensembles into seven main methods - weight merging, knowledge fusion, mixture-of-experts, reward ensemble, output ensemble, routing, and cascading - analyzing capabilities of those approaches. Our findings highlight key benefits such as improved diversity representation, enhanced output quality, and greater application flexibility. These insights aid model selection for real-world tasks and crucially, lay groundwork for extending ensemble strategies to multimodal LLMs.

</details>


### [86] [Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs](https://arxiv.org/abs/2504.06219)

*Dongyang Fan, Vinko Sabolčec, Matin Ansaripour, Ayush Kumar Tarun, Martin Jaggi, Antoine Bosselut, Imanol Schlag*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Compliance, Web Crawling, Machine Learning, Biomedical Research

**Relevance Score:** 8

**TL;DR:** This study analyzes the impact of web crawling opt-outs on the performance of large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As copyright holders increasingly opt out of web crawling, understanding how these restrictions affect LLM performance is crucial.

**Method:** We define and measure the data compliance gap (DCG) in pretraining models from scratch and in continual pretraining from compliant models, using experiments on 1.5B parameter models.

**Key Contributions:**

	1. Introduces the concept of data compliance gap (DCG) in LLM training.
	2. Demonstrates the negligible impact of compliance on general knowledge acquisition.
	3. Highlights performance decline in specialized domains due to lack of access to copyrighted sources.

**Result:** The results show that compliance with web data opt-outs does not significantly impact general knowledge acquisition, but declines are observed in specialized domains like biomedical research without access to major publishers.

**Limitations:** The analysis is based on models as of January 2025, and further research is needed to confirm ongoing trends.

**Conclusion:** General-purpose LLMs may be trained effectively with open data, but specialized domains benefit from high-quality copyrighted sources; this informs AI training practices and policy.

**Abstract:** The increasing adoption of web crawling opt-outs by copyright holders of online content raises critical questions about the impact of data compliance on large language model (LLM) performance. However, little is known about how these restrictions (and the resultant filtering of pretraining datasets) affect the capabilities of models trained using these corpora. In this work, we conceptualize this effect as the $\textit{data compliance gap}$ (DCG), which quantifies the performance difference between models trained on datasets that comply with web crawling opt-outs, and those that do not. We measure the data compliance gap in two settings: pretraining models from scratch and continual pretraining from existing compliant models (simulating a setting where copyrighted data could be integrated later in pretraining). Our experiments with 1.5B models show that, as of January 2025, compliance with web data opt-outs does not degrade general knowledge acquisition (close to 0\% DCG). However, in specialized domains such as biomedical research, excluding major publishers leads to performance declines. These findings suggest that while general-purpose LLMs can be trained to perform equally well using fully open data, performance in specialized domains may benefit from access to high-quality copyrighted sources later in training. Our study provides empirical insights into the long-debated trade-off between data compliance and downstream model performance, informing future discussions on AI training practices and policy decisions. Our website is available at https://data-compliance.github.io/.

</details>


### [87] [The Multi-Round Diagnostic RAG Framework for Emulating Clinical Reasoning](https://arxiv.org/abs/2504.07724)

*Penglei Sun, Yixiang Chen, Xiang Li, Xiaowen Chu*

**Main category:** cs.CL

**Keywords:** medical diagnosis, large language models, knowledge graph, retrieval-augmented generation, multi-round dialogue

**Relevance Score:** 10

**TL;DR:** The paper presents DiagnosGraph, a knowledge graph and the Multi-Round Diagnostic Retrieval-Augmented Generation (MRD-RAG) framework to improve automated medical diagnosis using large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The growing trend in rapidly deploying medical large language models (LLMs) faces challenges due to the semantic gap between patient descriptions and professional medical terminology.

**Method:** The authors constructed DiagnosGraph, a knowledge graph that bridges colloquial and professional medical language, and developed the MRD-RAG framework that employs a multi-round dialogue process for refined diagnostics.

**Key Contributions:**

	1. Construction of DiagnosGraph, covering both modern and Traditional Chinese Medicine.
	2. Development of MRD-RAG framework for multi-round dialogue in diagnostics.
	3. Demonstration of enhanced diagnostic performance through experimental evaluations.

**Result:** Experiments showed that MRD-RAG significantly improves the diagnostic performance of LLMs, evaluated positively by human physicians.

**Limitations:** 

**Conclusion:** The findings suggest that utilizing knowledge graphs and multi-round dialogues can enhance automated medical diagnosis, making it more accurate and aligned with human reasoning.

**Abstract:** In recent years, accurately and quickly deploying medical large language models (LLMs) has become a trend. Among these, retrieval-augmented generation (RAG) has garnered attention due to rapid deployment and privacy protection. However, the challenge hinder the practical deployment of RAG for medical diagnosis: the semantic gap between colloquial patient descriptions and the professional terminology within medical knowledge bases. We try to address the challenge from the data perspective and the method perspective. First, to address the semantic gap in existing knowledge bases, we construct DiagnosGraph, a generalist knowledge graph covering both modern medicine and Traditional Chinese Medicine. It contains 876 common diseases with the graph of 7,997 nodes and 37,201 triples. To bridge the gap between colloquial patient narratives and academic medical knowledge, DiagnosGraph also introduces $1,908$ medical record by formalizing the patient chief complaint and proposing a medical diagnosis. Second, we introduce the Multi-Round Diagnostic RAG (MRD-RAG) framework. It utilizes a multi-round dialogue to refine diagnostic possibilities, emulating the clinical reasoning of a physician. Experiments conducted on four medical benchmarks, with evaluations by human physicians, demonstrate that MRD-RAG enhances the diagnostic performance of LLMs, highlighting its potential to make automated diagnosis more accurate and human-aligned.

</details>


### [88] [Reconstructing Sepsis Trajectories from Clinical Case Reports using LLMs: the Textual Time Series Corpus for Sepsis](https://arxiv.org/abs/2504.12326)

*Shahriar Noroozizadeh, Jeremy C. Weiss*

**Main category:** cs.CL

**Keywords:** Clinical case reports, Large language models, Temporal localization, Sepsis-3, Textual time series

**Relevance Score:** 9

**TL;DR:** Pipeline to phenotype, extract, and annotate time-localized findings in clinical case reports using large language models, creating an open-access corpus for Sepsis-3.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage clinical case reports for richer, temporally accurate data for model training, addressing limitations of existing structured data streams.

**Method:** Construct a pipeline to extract and annotate clinical findings from case reports, generating a textual time series corpus and validating against expert annotations.

**Key Contributions:**

	1. Development of a pipeline for time-localization in clinical text
	2. Creation of an open-access textual time series corpus for Sepsis-3
	3. Validation of LLM capabilities against expert annotation standards

**Result:** Achieved high recovery rates of clinical findings with strong temporal ordering as validated against physician-expert annotations.

**Limitations:** LLMs show limitations in temporal reconstruction that need addressing with multimodal approaches.

**Conclusion:** The study demonstrates the capability of LLMs for time-localizing clinical findings, highlights limitations for temporal reconstruction, and suggests improvements through multimodal integration.

**Abstract:** Clinical case reports and discharge summaries may be the most complete and accurate summarization of patient encounters, yet they are finalized, i.e., timestamped after the encounter. Complementary data structured streams become available sooner but suffer from incompleteness. To train models and algorithms on more complete and temporally fine-grained data, we construct a pipeline to phenotype, extract, and annotate time-localized findings within case reports using large language models. We apply our pipeline to generate an open-access textual time series corpus for Sepsis-3 comprising 2,139 case reports from the Pubmed-Open Access (PMOA) Subset. To validate our system, we apply it on PMOA and timeline annotations from I2B2/MIMIC-IV and compare the results to physician-expert annotations. We show high recovery rates of clinical findings (event match rates: O1-preview--0.755, Llama 3.3 70B Instruct--0.753) and strong temporal ordering (concordance: O1-preview--0.932, Llama 3.3 70B Instruct--0.932). Our work characterizes the ability of LLMs to time-localize clinical findings in text, illustrating the limitations of LLM use for temporal reconstruction and providing several potential avenues of improvement via multimodal integration.

</details>


### [89] [Energy-Based Reward Models for Robust Language Model Alignment](https://arxiv.org/abs/2504.13134)

*Anamika Lochab, Ruqi Zhang*

**Main category:** cs.CL

**Keywords:** Energy-Based Reward Model, human preferences, alignment, Large Language Models, robustness

**Relevance Score:** 9

**TL;DR:** This paper introduces the Energy-Based Reward Model (EBRM), a framework that enhances the robustness and generalization of reward models used for aligning LLMs with human preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in aligning Large Language Models (LLMs) with complex human preferences and the difficulty in generalizing to unseen data.

**Method:** EBRM employs conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization to explicitly model reward distribution and capture uncertainty in human preferences without requiring retraining.

**Key Contributions:**

	1. Introduces a lightweight post-hoc refinement framework for reward models.
	2. Enhances RM robustness and generalization without the need for retraining.
	3. Achieves empirical improvements in alignment tasks through innovative training techniques.

**Result:** Empirical evaluations show EBRM achieves significant improvements in robustness and generalization, with up to a 5.97% enhancement in safety-critical alignment tasks and improved alignment quality in reinforcement learning experiments.

**Limitations:** 

**Conclusion:** EBRM is a scalable and effective enhancement for existing reward models and alignment pipelines, demonstrating efficiency and adaptability across various models and tasks.

**Abstract:** Reward models (RMs) are essential for aligning Large Language Models (LLMs) with human preferences. However, they often struggle with capturing complex human preferences and generalizing to unseen data. To address these challenges, we introduce Energy-Based Reward Model (EBRM), a lightweight post-hoc refinement framework that enhances RM robustness and generalization. EBRM models the reward distribution explicitly, capturing uncertainty in human preferences and mitigating the impact of noisy or misaligned annotations. It achieves this through conflict-aware data filtering, label-noise-aware contrastive training, and hybrid initialization. Notably, EBRM enhances RMs without retraining, making it computationally efficient and adaptable across different models and tasks. Empirical evaluations on RM benchmarks demonstrate significant improvements in both robustness and generalization, achieving up to a 5.97% improvement in safety-critical alignment tasks compared to standard RMs. Furthermore, reinforcement learning experiments confirm that our refined rewards enhance alignment quality, effectively delaying reward hacking. These results demonstrate our approach as a scalable and effective enhancement for existing RMs and alignment pipelines. The code is available at EBRM.

</details>


### [90] [Science Hierarchography: Hierarchical Organization of Science Literature](https://arxiv.org/abs/2504.13834)

*Muhan Gao, Jash Shah, Weiqi Wang, Daniel Khashabi*

**Main category:** cs.CL

**Keywords:** scientific literature, hierarchical structure, embedding-based clustering, LLM prompting, research navigation

**Relevance Score:** 6

**TL;DR:** Introducing SCIENCE HIERARCHOGRAPHY for organizing scientific literature hierarchically, blending embedding-based clustering with LLM prompting for improved research navigation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid growth of scientific knowledge makes it challenging to track progress across disciplines, necessitating a method to represent the density and structure of activity in science.

**Method:** A hybrid approach combining embedding-based clustering with LLM-based prompting to create a hierarchical structure of scientific literature.

**Key Contributions:**

	1. Development of SCIENCE HIERARCHOGRAPHY for organizing literature hierarchically.
	2. Fusion of embedding-based clustering with LLM prompting.
	3. Improved interpretability and navigation of scientific literature.

**Result:** The method enhances interpretability and allows a LLM-based agent to better navigate the hierarchical structure, improving research paper location efficiency.

**Limitations:** 

**Conclusion:** The proposed hierarchies provide insights into explored and under-explored fields and serve as an alternative to traditional literature search approaches.

**Abstract:** Scientific knowledge is growing rapidly, making it difficult to track progress and high-level conceptual links across broad disciplines. While tools like citation networks and search engines help retrieve related papers, they lack the abstraction needed to capture the needed to represent the density and structure of activity across subfields.   We motivate SCIENCE HIERARCHOGRAPHY, the goal of organizing scientific literature into a high-quality hierarchical structure that spans multiple levels of abstraction -- from broad domains to specific studies. Such a representation can provide insights into which fields are well-explored and which are under-explored. To achieve this goal, we develop a hybrid approach that combines efficient embedding-based clustering with LLM-based prompting, striking a balance between scalability and semantic precision. Compared to LLM-heavy methods like iterative tree construction, our approach achieves superior quality-speed trade-offs. Our hierarchies capture different dimensions of research contributions, reflecting the interdisciplinary and multifaceted nature of modern science. We evaluate its utility by measuring how effectively an LLM-based agent can navigate the hierarchy to locate target papers. Results show that our method improves interpretability and offers an alternative pathway for exploring scientific literature beyond traditional search methods. Code, data and demo are available: https://github.com/JHU-CLSP/science-hierarchography

</details>
