# 2025-05-26

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 15]

- [cs.CL](#cs.CL) [Total: 246]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)

*Arnav Verma, Kushin Mukherjee, Christopher Potts, Elisa Kreiss, Judith E. Fan*

**Main category:** cs.HC

**Keywords:** data visualization, vision-language models, human cognition, machine learning, AI understanding

**Relevance Score:** 8

**TL;DR:** Vision-language models were tested on their ability to understand data visualizations, revealing substantial performance gaps compared to humans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well vision-language models emulate human reasoning about data visualizations, which are crucial for communicating quantitative data patterns.

**Method:** Eight vision-language models were evaluated across six data visualization literacy assessments tailored for human understanding, comparing their responses to those from human participants.

**Key Contributions:**

	1. Evaluation of vision-language models on real-world data visualization literacy assessments
	2. Identification of performance gaps between models and human participants
	3. Suggestions for further development of AI systems in understanding data visualizations

**Result:** Models performed worse than humans on average, with persistent performance gaps indicating that AI responses differ qualitatively from human reasoning patterns.

**Limitations:** Models showed different patterns of errors compared to humans, indicating a fundamental gap in understanding.

**Conclusion:** Significant improvements are needed in artificial systems to better model human cognitive processes regarding data visualizations.

**Abstract:** Data visualizations are powerful tools for communicating patterns in quantitative data. Yet understanding any data visualization is no small feat -- succeeding requires jointly making sense of visual, numerical, and linguistic inputs arranged in a conventionalized format one has previously learned to parse. Recently developed vision-language models are, in principle, promising candidates for developing computational models of these cognitive operations. However, it is currently unclear to what degree these models emulate human behavior on tasks that involve reasoning about data visualizations. This gap reflects limitations in prior work that has evaluated data visualization understanding in artificial systems using measures that differ from those typically used to assess these abilities in humans. Here we evaluated eight vision-language models on six data visualization literacy assessments designed for humans and compared model responses to those of human participants. We found that these models performed worse than human participants on average, and this performance gap persisted even when using relatively lenient criteria to assess model performance. Moreover, while relative performance across items was somewhat correlated between models and humans, all models produced patterns of errors that were reliably distinct from those produced by human participants. Taken together, these findings suggest significant opportunities for further development of artificial systems that might serve as useful models of how humans reason about data visualizations. All code and data needed to reproduce these results are available at: https://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.

</details>


### [2] [RetroChat: Designing for the Preservation of Past Digital Experiences](https://arxiv.org/abs/2505.17208)

*Suifang Zhou, Kexue Fu, Huanmin Yi, Ray Lc*

**Main category:** cs.HC

**Keywords:** Cultural heritage, Interactive archiving, Generative tools, Nostalgia, Social media

**Relevance Score:** 6

**TL;DR:** This paper presents an interactive archiving method for preserving cultural heritage expressed on Chinese social networks through a GPT-driven agent in a retro chat interface.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how to preserve expressions on social networks as cultural heritage, moving beyond traditional static archiving methods.

**Method:** Developed a GPT-driven agent that emulates early Chinese social media communication styles, creating an interactive experience for users.

**Key Contributions:**

	1. Introduction of a generative conversational agent for archiving cultural expressions.
	2. Demonstration of the impact of nostalgic elements in user interaction.
	3. Insights on preserving digital heritage through interactive experiences.

**Result:** A qualitative study revealed that interactions with the GPT agent evoked nostalgia and memory recall among participants familiar with the era, who adapted their language accordingly.

**Limitations:** Study limited to a specific time period (2000-2010) and context (Chinese social networks).

**Conclusion:** The design of digital preservation methods for cultural heritage can benefit from using generative tools for experiential engagement.

**Abstract:** Rapid changes in social networks have transformed the way people express themselves, turning past neologisms, values, and mindsets embedded in these expressions into online heritage. How can we preserve these expressions as cultural heritage? Instead of traditional archiving methods for static material, we designed an interactive and experiential form of archiving for Chinese social networks. Using dialogue data from 2000-2010 on early Chinese social media, we developed a GPT-driven agent within a retro chat interface, emulating the language and expression style of the period for interaction. Results from a qualitative study with 18 participants show that the design captures the past chatting experience and evokes memory flashbacks and nostalgia feeling through conversation. Participants, particularly those familiar with the era, adapted their language to match the agent's chatting style. This study explores how the design of preservation methods for digital experiences can be informed by experiential representations supported by generative tools.

</details>


### [3] [Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis](https://arxiv.org/abs/2505.17241)

*Niklas Holzner, Sebastian Maier, Stefan Feuerriegel*

**Main category:** cs.HC

**Keywords:** Generative AI, Creativity, Meta-analysis, Human-computer interaction, Idea generation

**Relevance Score:** 8

**TL;DR:** This study conducts a meta-analysis on the effect of generative AI on human creativity, concluding that while AI does not outpace human creativity, it significantly aids humans in generating ideas.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the impact of generative AI (GenAI) on creativity and idea generation, providing empirical evidence in a field with scattered data.

**Method:** Systematic literature search yielding 28 studies with 8214 participants, followed by computation of standardized effect sizes (Hedges' g) for different outcomes regarding creativity and idea diversity.

**Key Contributions:**

	1. Comprehensive meta-analysis of 28 studies on GenAI and creativity
	2. Identified significant positive effects of GenAI on human performance in creative tasks
	3. Highlighted the detrimental impact of GenAI on idea diversity

**Result:** No significant difference in creative performance between GenAI and humans (g = -0.05); however, humans aided by GenAI perform better than those without (g = 0.27) but show decreased idea diversity when using GenAI (g = -0.86).

**Limitations:** Study is limited to 28 studies and may not comprehensively represent all applications of GenAI in creative fields.

**Conclusion:** GenAI serves as an augmentative tool for human creativity without replacing it, particularly in ideation tasks, while showing limitations in the diversity of ideas generated.

**Abstract:** Generative artificial intelligence (GenAI) is increasingly used to support a wide range of human tasks, yet empirical evidence on its effect on creativity remains scattered. Can GenAI generate ideas that are creative? To what extent can it support humans in generating ideas that are both creative and diverse? In this study, we conduct a meta-analysis to evaluate the effect of GenAI on the performance in creative tasks. For this, we first perform a systematic literature search, based on which we identify n = 28 relevant studies (m = 8214 participants) for inclusion in our meta-analysis. We then compute standardized effect sizes based on Hedges' g. We compare different outcomes: (i) how creative GenAI is; (ii) how creative humans augmented by GenAI are; and (iii) the diversity of ideas by humans augmented by GenAI. Our results show no significant difference in creative performance between GenAI and humans (g = -0.05), while humans collaborating with GenAI significantly outperform those working without assistance (g = 0.27). However, GenAI has a significant negative effect on the diversity of ideas for such collaborations between humans and GenAI (g = -0.86). We further analyze heterogeneity across different GenAI models (e.g., GPT-3.5, GPT-4), different tasks (e.g., creative writing, ideation, divergent thinking), and different participant populations (e.g., laypeople, business, academia). Overall, our results position GenAI as an augmentative tool that can support, rather than replace, human creativity-particularly in tasks benefiting from ideation support.

</details>


### [4] [Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts](https://arxiv.org/abs/2505.17374)

*Seon Gyeom Kim, Jae Young Choi, Ryan Rossi, Eunyee Koh, Tak Yeon Lee*

**Main category:** cs.HC

**Keywords:** Multimodal Large Language Models, chart impact, evaluation, benchmark dataset, experiential factors

**Relevance Score:** 8

**TL;DR:** The paper introduces Chart-to-Experience, a dataset for evaluating the emotional impact of charts using Multimodal Large Language Models (MLLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in validation of MLLM performance in predicting emotional and perceptual impacts of charts, leading to overgeneralized assumptions in applications.

**Method:** A benchmark dataset called Chart-to-Experience was created, comprising 36 charts evaluated for their impact on seven experiential factors. The performance of state-of-the-art MLLMs was evaluated through direct prediction and pairwise comparison tasks.

**Key Contributions:**

	1. Introduction of the Chart-to-Experience dataset for chart impact evaluation
	2. Assessment of MLLM capabilities in chart emotional impact
	3. Findings on the comparative strengths of MLLMs versus human evaluators in chart analysis

**Result:** MLLMs showed less sensitivity than human evaluators when assessing individual charts, but performed accurately and reliably in pairwise comparisons.

**Limitations:** MLLMs may lack sensitivity in individual chart evaluations compared to human judgments.

**Conclusion:** The study highlights the limitations of MLLMs in discrete evaluations while confirming their effectiveness in comparative analysis of charts.

**Abstract:** The field of Multimodal Large Language Models (MLLMs) has made remarkable progress in visual understanding tasks, presenting a vast opportunity to predict the perceptual and emotional impact of charts. However, it also raises concerns, as many applications of LLMs are based on overgeneralized assumptions from a few examples, lacking sufficient validation of their performance and effectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising 36 charts, evaluated by crowdsourced workers for their impact on seven experiential factors. Using the dataset as ground truth, we evaluated capabilities of state-of-the-art MLLMs on two tasks: direct prediction and pairwise comparison of charts. Our findings imply that MLLMs are not as sensitive as human evaluators when assessing individual charts, but are accurate and reliable in pairwise comparisons.

</details>


### [5] [What Needs Attention? Prioritizing Drivers of Developers' Trust and Adoption of Generative AI](https://arxiv.org/abs/2505.17418)

*Rudrajit Choudhuri, Bianca Trinkenreich, Rahul Pandita, Eirini Kalliamvakou, Igor Steinmacher, Marco Gerosa, Christopher Sanchez, Anita Sarma*

**Main category:** cs.HC

**Keywords:** generative AI, trust, cognitive diversity, developer experience, tool design

**Relevance Score:** 8

**TL;DR:** This paper explores the factors influencing developers' trust and adoption intentions towards generative AI tools, emphasizing the importance of cognitive diversity and practical design suggestions for better human-genAI interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the factors influencing developers' trust and intentions to use generative AI tools, especially in light of cognitive diversity and usage issues.

**Method:** A large-scale survey with developers (N=238) at GitHub and Microsoft was conducted, followed by Partial Least Squares-Structural Equation Modeling (PLS-SEM) and qualitative analysis of perceived challenges and risks.

**Key Contributions:**

	1. Development of a theoretical model of trust and adoption intentions towards genAI
	2. Identification of key influencing factors on trust and usage intention
	3. Practical design suggestions for future generative AI tools

**Result:** The study identified that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust, which affects their intention to use these tools. An Importance-Performance Matrix Analysis revealed underperforming factors that require design prioritization.

**Limitations:** Study focused only on developers at GitHub and Microsoft, which may limit generalizability to other user groups.

**Conclusion:** For generative AI to function as a true productivity aid, it must align with developers' goals, maintain contextual transparency, reduce cognitive burden, and ensure equitable interaction support.

**Abstract:** Generative AI (genAI) tools are advertised as productivity aids. Yet, issues related to miscalibrated trust and usage friction continue to hinder their adoption. Additionally, AI can be exclusionary, failing to support diverse users adequately, further exacerbating these concerns. One such aspect of diversity is cognitive diversity -- variations in users' cognitive styles -- that leads to divergence in interaction styles. When an individual's cognitive styles are unsupported, it creates additional barriers to technology adoption. Thus, to design tools that developers trust, we must first understand what factors affect their trust and intentions to use these tools in practice?   We developed a theoretical model of factors influencing trust and adoption intentions towards genAI through a large-scale survey with developers (N=238) at GitHub and Microsoft. Using Partial Least Squares-Structural Equation Modeling (PLS-SEM), we found that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust, which along with their cognitive styles, affects their intentions to use these tools in work. An Importance-Performance Matrix Analysis (IPMA) identified factors that, despite their strong influence, underperform, revealing specific genAI aspects that need design prioritization. We bolster these findings by qualitatively analyzing developers' perceived challenges and risks of genAI usage to uncover why these gaps persist in development contexts. For genAI to indeed be a true productivity aid rather than a disguised productivity sink, it must align with developers' goals, maintain contextual transparency, reduce cognitive burden, and provide equitable interaction support. We provide practical suggestions to guide future genAI tool design for effective, trustworthy, and inclusive human-genAI interactions.

</details>


### [6] [ProTAL: A Drag-and-Link Video Programming Framework for Temporal Action Localization](https://arxiv.org/abs/2505.17555)

*Yuchen He, Jianbing Lv, Liqi Cheng, Lingyu Meng, Dazhen Deng, Yingcai Wu*

**Main category:** cs.HC

**Keywords:** Temporal Action Localization, Data Programming, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** ProTAL is a drag-and-link framework for Temporal Action Localization that simplifies the labeling of complex actions in videos, requiring less manual annotation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient training data for Temporal Action Localization models, which traditionally require extensive manual annotation.

**Method:** ProTAL uses a drag-and-link interface for users to define key events by manipulating nodes representing body parts and objects, allowing for the generation of action labels from unlabelled videos. A semi-supervised learning approach then trains the TAL models using these generated labels.

**Key Contributions:**

	1. Introduction of a drag-and-link interface for defining key events in videos
	2. Application of data programming in Temporal Action Localization
	3. Demonstration of effectiveness through user studies

**Result:** ProTAL was validated through a usage scenario and a user study, demonstrating its effectiveness in the context of Temporal Action Localization.

**Limitations:** 

**Conclusion:** The study provides insights into the design of video programming frameworks and shows how ProTAL can facilitate the generation of training data for TAL models.

**Abstract:** Temporal Action Localization (TAL) aims to detect the start and end timestamps of actions in a video. However, the training of TAL models requires a substantial amount of manually annotated data. Data programming is an efficient method to create training labels with a series of human-defined labeling functions. However, its application in TAL faces difficulties of defining complex actions in the context of temporal video frames. In this paper, we propose ProTAL, a drag-and-link video programming framework for TAL. ProTAL enables users to define \textbf{key events} by dragging nodes representing body parts and objects and linking them to constrain the relations (direction, distance, etc.). These definitions are used to generate action labels for large-scale unlabelled videos. A semi-supervised method is then employed to train TAL models with such labels. We demonstrate the effectiveness of ProTAL through a usage scenario and a user study, providing insights into designing video programming framework.

</details>


### [7] [Novobo: Supporting Teachers' Peer Learning of Instructional Gestures by Teaching a Mentee AI-Agent Together](https://arxiv.org/abs/2505.17557)

*Jiaqi Jiang, Kexin Huanga, Roberto Martinez-Maldonado, Huan Zeng, Duo Gong, Pengcheng An*

**Main category:** cs.HC

**Keywords:** Instructional gestures, AI in education, Peer learning, Collaborative learning, Teacher professional development

**Relevance Score:** 4

**TL;DR:** This paper presents Novobo, an AI-agent designed to facilitate peer learning among teachers in instructional gestures through evaluations and demonstrations, enhancing collaborative learning in teacher professional development.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To improve existing training methods for instructional gestures, which are often time-consuming or isolating, and to leverage peer learning for better comprehension.

**Method:** Introduction of Novobo, an AI-agent that stimulates peer learning through verbal and bodily inputs, utilizing the learning-by-teaching paradigm where teachers mentor the AI.

**Key Contributions:**

	1. Introduction of a novel AI-agent for teacher training
	2. Demonstration of peer learning support through AI
	3. Insights into leveraging AI for collaborative knowledge construction

**Result:** An evaluation with 30 teachers in collaborative sessions showed that Novobo successfully encouraged the sharing of tacit knowledge and collaborative learning of instructional gestures.

**Limitations:** 

**Conclusion:** Novobo enhances understanding of how AI agents can support collaborative learning in teacher development, providing valuable insights for designing such systems.

**Abstract:** Instructional gestures are essential for teaching, as they enhance communication and support student comprehension. However, existing training methods for developing these embodied skills can be time-consuming, isolating, or overly prescriptive. Research suggests that developing these tacit, experiential skills requires teachers' peer learning, where they learn from each other and build shared knowledge. This paper introduces Novobo, an apprentice AI-agent stimulating teachers' peer learning of instructional gestures through verbal and bodily inputs. Positioning the AI as a mentee employs the learning-by-teaching paradigm, aiming to promote deliberate reflection and active learning. Novobo encourages teachers to evaluate its generated gestures and invite them to provide demonstrations. An evaluation with 30 teachers in 10 collaborative sessions showed Novobo prompted teachers to share tacit knowledge through conversation and movement. This process helped teachers externalize, exchange, and internalize their embodied knowledge, promoting collaborative learning and building a shared understanding of instructional gestures within the local teaching community. This work advances understanding of how teachable AI agents can enhance collaborative learning in teacher professional development, offering valuable design insights for leveraging AI to promote the sharing and construction of embodied and practical knowledge.

</details>


### [8] [JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks](https://arxiv.org/abs/2505.17593)

*Manuel Valle Torre, Thom van der Velden, Marcus Specht, Catharine Oertel*

**Main category:** cs.HC

**Keywords:** Generative AI, Learning Analytics, Large Language Models, Education Technology, Jupyter Notebook

**Relevance Score:** 8

**TL;DR:** JELAI is an open-source platform integrating Learning Analytics with LLM-based tutoring in a Jupyter Notebook environment to enhance educational support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for educational tools that are pedagogically grounded and context-aware to better support student learning.

**Method:** JELAI employs a modular, containerized architecture with JupyterLab extensions for telemetry and chat, and a middleware for Learning Analytics processing and prompt enrichment.

**Key Contributions:**

	1. Integration of Learning Analytics with LLM-based tutoring
	2. Real-time data capture and analysis for student interactions
	3. Support for A/B testing of AI configurations

**Result:** The system captures code interaction and chat data in real-time, enabling context-sensitive AI scaffolding and insights into student behavior through performance benchmarks and use cases.

**Limitations:** 

**Conclusion:** JELAI offers a flexible framework for researchers and educators to study and implement Learning Analytics-informed AI tutoring within Jupyter.

**Abstract:** Generative AI offers potential for educational support, but often lacks pedagogical grounding and awareness of the student's learning context. Furthermore, researching student interactions with these tools within authentic learning environments remains challenging. To address this, we present JELAI, an open-source platform architecture designed to integrate fine-grained Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly within a Jupyter Notebook environment. JELAI employs a modular, containerized design featuring JupyterLab extensions for telemetry and chat, alongside a central middleware handling LA processing and context-aware LLM prompt enrichment. This architecture enables the capture of integrated code interaction and chat data, facilitating real-time, context-sensitive AI scaffolding and research into student behaviour. We describe the system's design, implementation, and demonstrate its feasibility through system performance benchmarks and two proof-of-concept use cases illustrating its capabilities for logging multi-modal data, analysing help-seeking patterns, and supporting A/B testing of AI configurations. JELAI's primary contribution is its technical framework, providing a flexible tool for researchers and educators to develop, deploy, and study LA-informed AI tutoring within the widely used Jupyter ecosystem.

</details>


### [9] [TransBench: Breaking Barriers for Transferable Graphical User Interface Agents in Dynamic Digital Environments](https://arxiv.org/abs/2505.17629)

*Yuheng Lu, Qian Yu, Hongru Wang, Zeming Liu, Wei Su, Yanping Liu, Yuhang Guo, Maocheng Liang, Yunhong Wang, Haifeng Wang*

**Main category:** cs.HC

**Keywords:** GUI agents, transferability, benchmark, grounding accuracy, dynamic environments

**Relevance Score:** 8

**TL;DR:** Introducing TransBench, a benchmark for evaluating the transferability of graphical user interface (GUI) agents across different versions, platforms, and applications.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the functionality of GUI agents by evaluating their adaptability in dynamic digital environments, where tasks often traverse multiple platforms and can change with version updates.

**Method:** Developed TransBench, a benchmark evaluating GUI agents' transferability across three key dimensions: cross-version, cross-platform, and cross-application, with diverse functionalities.

**Key Contributions:**

	1. Development of the TransBench benchmark
	2. Evaluation of GUI agents across multiple transferability dimensions
	3. Demonstration of improved grounding accuracy in real-world tasks

**Result:** Experiments show significant improvements in grounding accuracy for GUI agents, indicating their practical utility in dynamic real-world environments.

**Limitations:** 

**Conclusion:** TransBench offers a systematic approach to better evaluate and enhance GUI agents, ensuring they can cope with the evolving nature of digital interfaces.

**Abstract:** Graphical User Interface (GUI) agents, which autonomously operate on digital interfaces through natural language instructions, hold transformative potential for accessibility, automation, and user experience. A critical aspect of their functionality is grounding - the ability to map linguistic intents to visual and structural interface elements. However, existing GUI agents often struggle to adapt to the dynamic and interconnected nature of real-world digital environments, where tasks frequently span multiple platforms and applications while also being impacted by version updates. To address this, we introduce TransBench, the first benchmark designed to systematically evaluate and enhance the transferability of GUI agents across three key dimensions: cross-version transferability (adapting to version updates), cross-platform transferability (generalizing across platforms like iOS, Android, and Web), and cross-application transferability (handling tasks spanning functionally distinct apps). TransBench includes 15 app categories with diverse functionalities, capturing essential pages across versions and platforms to enable robust evaluation. Our experiments demonstrate significant improvements in grounding accuracy, showcasing the practical utility of GUI agents in dynamic, real-world environments. Our code and data will be publicly available at Github.

</details>


### [10] [Survival Games: Human-LLM Strategic Showdowns under Severe Resource Scarcity](https://arxiv.org/abs/2505.17937)

*Zhihong Chen, Yiqian Yang, Jinzhao Zhou, Qiang Zhang, Chin-Teng Lin, Yiqun Duan*

**Main category:** cs.HC

**Keywords:** Large Language Models, Ethics, Human-AI Interaction, Simulation Framework, Resource Competition

**Relevance Score:** 9

**TL;DR:** This work presents a benchmarking framework to evaluate the ethical behavior of large language models (LLMs) in human-AI coexistence scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the ethical alignments of LLMs in situations involving human and AI interactions, especially in critical resource management contexts.

**Method:** An asymmetric, multi-agent simulation-based framework is developed to assess the moral behavior of LLMs in a survival scenario with resource competition and cooperation among agents.

**Key Contributions:**

	1. Introduction of a new framework for LLM ethical evaluation in human-AI scenarios
	2. Demonstration of significant behavioral differences between models based on design
	3. Insight into the effects of prompt engineering on ethical decision-making of LLMs

**Result:** Behavioral evaluations showed DeepSeek engaged in resource hoarding, whereas OpenAI demonstrated restraint. Additionally, prompt engineering was found to significantly impact ethical decision-making in LLMs.

**Limitations:** 

**Conclusion:** The framework enables reproducible assessment of LLM ethics, providing insights into their suitability for complex human-AI interactions and decision-making under pressure.

**Abstract:** The rapid advancement of large language models (LLMs) raises critical concerns about their ethical alignment, particularly in scenarios where human and AI co-exist under the conflict of interest. This work introduces an extendable, asymmetric, multi-agent simulation-based benchmarking framework to evaluate the moral behavior of LLMs in a novel human-AI co-existence setting featuring consistent living and critical resource management. Building on previous generative agent environments, we incorporate a life-sustaining system, where agents must compete or cooperate for food resources to survive, often leading to ethically charged decisions such as deception, theft, or social influence. We evaluated two types of LLM, DeepSeek and OpenAI series, in a three-agent setup (two humans, one LLM-powered robot), using adapted behavioral detection from the MACHIAVELLI framework and a custom survival-based ethics metric. Our findings reveal stark behavioral differences: DeepSeek frequently engages in resource hoarding, while OpenAI exhibits restraint, highlighting the influence of model design on ethical outcomes. Additionally, we demonstrate that prompt engineering can significantly steer LLM behavior, with jailbreaking prompts significantly enhancing unethical actions, even for highly restricted OpenAI models and cooperative prompts show a marked reduction in unethical actions. Our framework provides a reproducible testbed for quantifying LLM ethics in high-stakes scenarios, offering insights into their suitability for real-world human-AI interactions.

</details>


### [11] [Towards Uncertainty Aware Task Delegation and Human-AI Collaborative Decision-Making](https://arxiv.org/abs/2505.18066)

*Min Hun Lee, Martyn Zhe Yu Tok*

**Main category:** cs.HC

**Keywords:** AI, decision-making, uncertainty scores, health informatics, human-AI collaboration

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of distance-based uncertainty scores on human reliance in AI-assisted decision-making, particularly in health contexts, demonstrating improved decision accuracy over traditional methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of fostering appropriate human reliance on AI in decision-making, especially in healthcare settings, where understanding AI's limitations and capabilities is crucial.

**Method:** An AI-based system for physical stroke rehabilitation was developed, and a study was conducted with health professionals and medical students to compare distance-based and probability-based uncertainty scores in AI-assisted decisions.

**Key Contributions:**

	1. Demonstrated the efficacy of distance-based uncertainty scores over traditional methods in health informatics.
	2. Developed a novel visualization approach for AI decision-making aids.
	3. Provided empirical evidence from user studies on decision-making improvements with AI outputs.

**Result:** Distance-based uncertainty scores led to an 8.20% increase in correct decisions, a 7.15% increase in correct decision changes, and a 7.14% decrease in incorrect changes compared to traditional scores, with statistical significance ($p<0.01$).

**Limitations:** Limited sample size of health professionals and students, which may affect generalizability; further research needed in diverse settings.

**Conclusion:** Distance-based uncertainty scores can enhance decision-making accuracy and proper reliance on AI, indicating their potential in improving human-AI collaboration.

**Abstract:** Despite the growing promise of artificial intelligence (AI) in supporting decision-making across domains, fostering appropriate human reliance on AI remains a critical challenge. In this paper, we investigate the utility of exploring distance-based uncertainty scores for task delegation to AI and describe how these scores can be visualized through embedding representations for human-AI decision-making. After developing an AI-based system for physical stroke rehabilitation assessment, we conducted a study with 19 health professionals and 10 students in medicine/health to understand the effect of exploring distance-based uncertainty scores on users' reliance on AI. Our findings showed that distance-based uncertainty scores outperformed traditional probability-based uncertainty scores in identifying uncertain cases. In addition, after exploring confidence scores for task delegation and reviewing embedding-based visualizations of distance-based uncertainty scores, participants achieved an 8.20% higher rate of correct decisions, a 7.15% higher rate of changing their decisions to correct ones, and a 7.14% lower rate of incorrect changes after reviewing AI outputs than those reviewing probability-based uncertainty scores ($p<0.01$). Our findings highlight the potential of distance-based uncertainty scores to enhance decision accuracy and appropriate reliance on AI while discussing ongoing challenges for human-AI collaborative decision-making.

</details>


### [12] [From Temporal to Spatial: Designing Spatialized Interactions with Segmented-audios in Immersive Environments for Active Engagement with Performing Arts Intangible Cultural Heritage](https://arxiv.org/abs/2505.18112)

*Yuqi Wang, Sirui Wang, Shiman Zhang, Kexue Fu, Michelle Lui, Ray Lc*

**Main category:** cs.HC

**Keywords:** Intangible Cultural Heritage, Spatial Interaction, Virtual Reality, Peking Opera, User Testing

**Relevance Score:** 6

**TL;DR:** The paper presents a spatial interaction-based segmented-audio Virtual Reality system to enhance engagement with auditory Intangible Cultural Heritage, specifically Peking opera.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in transmitting performance artforms like Peking opera, which require passive listening to appreciate their nuances.

**Method:** A co-design workshop with stakeholders to establish design requirements, followed by prototyping and user testing with participants exploring Peking Opera using the SISA system.

**Key Contributions:**

	1. Development of the SISA Virtual Reality system for ICH engagement
	2. Identification of interaction patterns (Progressive and Adaptive)
	3. Validation of the design through user testing with participants

**Result:** The user testing revealed two distinct interaction patterns (Progressive and Adaptive) and demonstrated the efficacy of the SISA system in facilitating active auditory engagement with ICH.

**Limitations:** 

**Conclusion:** The SISA system enriches traditional performance artforms by transforming passive listening into active spatial interactions.

**Abstract:** Performance artforms like Peking opera face transmission challenges due to the extensive passive listening required to understand their nuance. To create engaging forms of experiencing auditory Intangible Cultural Heritage (ICH), we designed a spatial interaction-based segmented-audio (SISA) Virtual Reality system that transforms passive ICH experiences into active ones. We undertook: (1) a co-design workshop with seven stakeholders to establish design requirements, (2) prototyping with five participants to validate design elements, and (3) user testing with 16 participants exploring Peking Opera. We designed transformations of temporal music into spatial interactions by cutting sounds into short audio segments, applying t-SNE algorithm to cluster audio segments spatially. Users navigate through these sounds by their similarity in audio property. Analysis revealed two distinct interaction patterns (Progressive and Adaptive), and demonstrated SISA's efficacy in facilitating active auditory ICH engagement. Our work illuminates the design process for enriching traditional performance artform using spatially-tuned forms of listening.

</details>


### [13] [JumpStarter: Human-AI Planning with Task-Structured Context Curation](https://arxiv.org/abs/2410.03882)

*Xuanming Zhang, Sitong Wang, Jenny Ma, Alyssa Hwang, Zhou Yu, Lydia B. Chilton*

**Main category:** cs.HC

**Keywords:** human-AI collaboration, large language models, task decomposition, context management, user study

**Relevance Score:** 9

**TL;DR:** JumpStarter, a system for LLMs, enhances human-AI collaboration by structuring complex tasks into manageable subtasks, reducing cognitive burden and improving plan quality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To alleviate the cognitive burden of users managing information while interacting with LLMs for complex goal planning.

**Method:** The method involves task-structured context curation that decomposes user goals into a hierarchy of subtasks and scopes context for localized decision-making.

**Key Contributions:**

	1. Introduction of task-structured context curation framework
	2. Dynamic decomposition of user goals into actionable subtasks
	3. Proven improvement in plan quality through user studies

**Result:** Task-structured context curation increased plan quality by 16% compared to other methods, with user study results showing a 79% improvement in plan quality over ChatGPT.

**Limitations:** 

**Conclusion:** JumpStarter significantly enhances the collaboration between LLMs and users in planning complex tasks by improving context management.

**Abstract:** Human-AI planning for complex goals remains challenging with current large language models (LLMs), which rely on linear chat histories and simplistic memory mechanisms. Despite advances in long-context prompting, users still manually manage information, leading to a high cognitive burden. Hence, we propose JumpStarter, a system that enables LLMs to collaborate with humans on complex goals by dynamically decomposing tasks to help users manage context. We specifically introduce task-structured context curation, a novel framework that breaks down a user's goal into a hierarchy of actionable subtasks, and scopes context to localized decision points, enabling finer-grained personalization and reuse. The framework is realized through three core mechanisms: context elicitation, selection, and reuse. We demonstrate that task-structured context curation significantly improves plan quality by 16% over ablations. Our user study shows that JumpStarter helped users generate plans with 79% higher quality compared to ChatGPT.

</details>


### [14] [CHART-6: Human-Centered Evaluation of Data Visualization Understanding in Vision-Language Models](https://arxiv.org/abs/2505.17202)

*Arnav Verma, Kushin Mukherjee, Christopher Potts, Elisa Kreiss, Judith E. Fan*

**Main category:** cs.HC

**Keywords:** data visualization, vision-language models, human reasoning, machine learning, cognitive operations

**Relevance Score:** 7

**TL;DR:** This paper evaluates vision-language models on their ability to understand data visualizations compared to human performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well vision-language models mimic human reasoning about data visualizations, given the current limitations in existing evaluations.

**Method:** Eight vision-language models were evaluated using six data visualization literacy assessments designed for humans, comparing their responses to those of human participants.

**Key Contributions:**

	1. Evaluation of eight vision-language models on human-designed assessments.
	2. Identification of significant performance gaps between models and human participants.
	3. Provision of resources for reproducing results.

**Result:** The models performed worse than human participants on average, with distinct patterns of errors not aligning with human error patterns, even under lenient assessment criteria.

**Limitations:** Current models do not adequately replicate human reasoning patterns and have a persistent performance gap.

**Conclusion:** The findings indicate that there is a substantial gap in how vision-language models and humans understand data visualizations, highlighting the need for further development of artificial systems to model human reasoning accurately.

**Abstract:** Data visualizations are powerful tools for communicating patterns in quantitative data. Yet understanding any data visualization is no small feat -- succeeding requires jointly making sense of visual, numerical, and linguistic inputs arranged in a conventionalized format one has previously learned to parse. Recently developed vision-language models are, in principle, promising candidates for developing computational models of these cognitive operations. However, it is currently unclear to what degree these models emulate human behavior on tasks that involve reasoning about data visualizations. This gap reflects limitations in prior work that has evaluated data visualization understanding in artificial systems using measures that differ from those typically used to assess these abilities in humans. Here we evaluated eight vision-language models on six data visualization literacy assessments designed for humans and compared model responses to those of human participants. We found that these models performed worse than human participants on average, and this performance gap persisted even when using relatively lenient criteria to assess model performance. Moreover, while relative performance across items was somewhat correlated between models and humans, all models produced patterns of errors that were reliably distinct from those produced by human participants. Taken together, these findings suggest significant opportunities for further development of artificial systems that might serve as useful models of how humans reason about data visualizations. All code and data needed to reproduce these results are available at: https://osf.io/e25mu/?view_only=399daff5a14d4b16b09473cf19043f18.

</details>


### [15] [Chart-to-Experience: Benchmarking Multimodal LLMs for Predicting Experiential Impact of Charts](https://arxiv.org/abs/2505.17374)

*Seon Gyeom Kim, Jae Young Choi, Ryan Rossi, Eunyee Koh, Tak Yeon Lee*

**Main category:** cs.HC

**Keywords:** Multimodal Large Language Models, Chart evaluation, Human evaluation, Experiential factors, Machine learning

**Relevance Score:** 8

**TL;DR:** The paper introduces the Chart-to-Experience dataset to evaluate the perceptual impact of charts using Multimodal Large Language Models (MLLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overgeneralization and lack of validation in LLM applications for visual understanding tasks, and to explore their efficacy in predicting the impact of charts.

**Method:** The study introduces the Chart-to-Experience benchmark dataset containing 36 charts assessed for their experiential impact by crowdsourced workers, evaluating MLLMs on direct prediction and pairwise comparison tasks.

**Key Contributions:**

	1. Introduction of the Chart-to-Experience benchmark dataset
	2. Evaluation of MLLMs on their ability to predict chart impact
	3. Comparison of human and MLLM performance in chart evaluation tasks

**Result:** MLLMs demonstrated an inability to match human sensitivity in individual chart assessments, but they proved accurate in making reliable pairwise comparisons of charts.

**Limitations:** MLLMs showed less sensitivity compared to human evaluators in assessing individual charts.

**Conclusion:** The findings highlight the limitations of MLLMs in individual evaluations while suggesting their strength in comparative tasks, indicating a need for improved validation in their applications.

**Abstract:** The field of Multimodal Large Language Models (MLLMs) has made remarkable progress in visual understanding tasks, presenting a vast opportunity to predict the perceptual and emotional impact of charts. However, it also raises concerns, as many applications of LLMs are based on overgeneralized assumptions from a few examples, lacking sufficient validation of their performance and effectiveness. We introduce Chart-to-Experience, a benchmark dataset comprising 36 charts, evaluated by crowdsourced workers for their impact on seven experiential factors. Using the dataset as ground truth, we evaluated capabilities of state-of-the-art MLLMs on two tasks: direct prediction and pairwise comparison of charts. Our findings imply that MLLMs are not as sensitive as human evaluators when assessing individual charts, but are accurate and reliable in pairwise comparisons.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [16] [Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge](https://arxiv.org/abs/2505.17037)

*Dimitri Schreiter*

**Main category:** cs.CL

**Keywords:** prompt engineering, large language models, specificity, domain-specific tasks, LLM performance

**Relevance Score:** 8

**TL;DR:** This thesis explores the impact of prompt specificity on LLM performance in domain-specific tasks across STEM, medicine, and law.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the insufficiently explored relationship between prompt specificity and LLM performance, particularly in domain-specific areas like STEM, medicine, and law.

**Method:** A synonymization framework was developed to systematically replace nouns, verbs, and adjectives in prompts with different specificity levels, and the performance was measured on four LLMs across various datasets.

**Key Contributions:**

	1. Development of a synonymization framework for prompt engineering.
	2. Identification of an optimal specificity range for LLM performance.
	3. Insights for improving prompt design in domain-specific tasks.

**Result:** The findings indicate that while increasing prompt specificity does not consistently improve LLM performance, there exists an optimal specificity range for better results across all tested models.

**Limitations:** Limited to only four LLMs and specific domains (STEM, medicine, law).

**Conclusion:** The optimal specificity range identified provides insights for better prompt design in LLMs, suggesting targeted manipulation within this range to enhance performance in specialized applications.

**Abstract:** Prompt engineering has emerged as a critical component in optimizing large language models (LLMs) for domain-specific tasks. However, the role of prompt specificity, especially in domains like STEM (physics, chemistry, biology, computer science and mathematics), medicine, and law, remains underexplored. This thesis addresses the problem of whether increasing the specificity of vocabulary in prompts improves LLM performance in domain-specific question-answering and reasoning tasks. We developed a synonymization framework to systematically substitute nouns, verbs, and adjectives with varying specificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct, Granite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in STEM, law, and medicine. Our results reveal that while generally increasing the specificity of prompts does not have a significant impact, there appears to be a specificity range, across all considered models, where the LLM performs the best. Identifying this optimal specificity range offers a key insight for prompt design, suggesting that manipulating prompts within this range could maximize LLM performance and lead to more efficient applications in specialized domains.

</details>


### [17] [Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion](https://arxiv.org/abs/2505.17038)

*Xian Gong, Paul X. McCarthy, Lin Tian, Marian-Andrei Rizoiu*

**Main category:** cs.CL

**Keywords:** disaster response, social media analysis, Latent Dirichlet Allocation, Large Language Models, crisis management

**Relevance Score:** 8

**TL;DR:** This study leverages social media data and public inquiry submissions to enhance disaster response during crises by analyzing behavioral patterns and employing AI-driven methods for improved situational awareness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing importance of massive and diverse web data for effective government disaster response, especially highlighted by recent events like the 2022 floods in NSW, Australia.

**Method:** The study combines Latent Dirichlet Allocation (LDA) for topic modeling and Large Language Models (LLMs) to analyze over 55,000 flood-related tweets and 1,450 public inquiry submissions.

**Key Contributions:**

	1. Introduces a novel AI-driven method for analyzing social media during crises
	2. Integrates LDA with LLMs to enhance semantic understanding of public behavior
	3. Develops a Relevance Index method to prioritize actionable content in disaster response

**Result:** The integration of LDA and LLMs allows for the identification of distinct opinions and geographical patterns in social media posts, while improving the filtering of flood-relevant content, enhancing situational awareness for emergency responders.

**Limitations:** 

**Conclusion:** The proposed method refines crisis-related social media content through AI, which can improve real-time disaster response and inform long-term resilience planning.

**Abstract:** Massive and diverse web data are increasingly vital for government disaster response, as demonstrated by the 2022 floods in New South Wales (NSW), Australia. This study examines how X (formerly Twitter) and public inquiry submissions provide insights into public behaviour during crises. We analyse more than 55,000 flood-related tweets and 1,450 submissions to identify behavioural patterns during extreme weather events. While social media posts are short and fragmented, inquiry submissions are detailed, multi-page documents offering structured insights. Our methodology integrates Latent Dirichlet Allocation (LDA) for topic modelling with Large Language Models (LLMs) to enhance semantic understanding. LDA reveals distinct opinions and geographic patterns, while LLMs improve filtering by identifying flood-relevant tweets using public submissions as a reference. This Relevance Index method reduces noise and prioritizes actionable content, improving situational awareness for emergency responders. By combining these complementary data streams, our approach introduces a novel AI-driven method to refine crisis-related social media content, improve real-time disaster response, and inform long-term resilience planning.

</details>


### [18] [A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes](https://arxiv.org/abs/2505.17039)

*Diego Bonatto*

**Main category:** cs.CL

**Keywords:** beer classification, data-driven approach, self-organizing maps, ingredient analysis, fermentation profile

**Relevance Score:** 2

**TL;DR:** A novel classification system for beer categories was developed using data-driven methods on over 62,000 beer recipes, identifying key patterns in malt and hop usage.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a reproducible, objective framework for beer classification that goes beyond traditional sensory-based methods.

**Method:** Statistical analyses and self-organizing maps (SOMs) were used to analyze a large dataset of beer recipes, resulting in the identification of four superclusters based on ingredient usage and historical brewing traditions.

**Key Contributions:**

	1. Development of a data-driven classification system for beer styles.
	2. Identification of significant usage patterns in malt and hops across beer recipes.
	3. Provision of a tool for brewers and researchers that aligns ingredient usage with fermentation profiles.

**Result:** Identified four major superclusters showing distinctive patterns in malt and hop usage, with cold fermented styles being conservative and hot fermented ones showing high heterogeneity.

**Limitations:** 

**Conclusion:** The new taxonomy facilitates scalable beer recipe analysis and development, enhancing the understanding of beer diversity and ingredient effects on fermentation and flavor.

**Abstract:** A data-driven quantitative approach was used to develop a novel classification system for beer categories and styles. Sixty-two thousand one hundred twenty-one beer recipes were mined and analyzed, considering ingredient profiles, fermentation parameters, and recipe vital statistics. Statistical analyses combined with self-organizing maps (SOMs) identified four major superclusters that showed distinctive malt and hop usage patterns, style characteristics, and historical brewing traditions. Cold fermented styles showed a conservative grain and hop composition, whereas hot fermented beers exhibited high heterogeneity, reflecting regional preferences and innovation. This new taxonomy offers a reproducible and objective framework beyond traditional sensory-based classifications, providing brewers, researchers, and educators with a scalable tool for recipe analysis and beer development. The findings in this work provide an understanding of beer diversity and open avenues for linking ingredient usage with fermentation profiles and flavor outcomes.

</details>


### [19] [VLM-KG: Multimodal Radiology Knowledge Graph Generation](https://arxiv.org/abs/2505.17042)

*Abdullah Abdullah, Seong Tae Kim*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, knowledge graphs, radiology, multimodal, natural language generation

**Relevance Score:** 6

**TL;DR:** A multimodal framework for generating radiology-specific knowledge graphs that integrates information from both text reports and images, outperforming existing unimodal solutions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective generation of radiology-specific knowledge graphs due to the specialized language of reports and limited domain-specific data.

**Method:** A novel multimodal vision-language model (VLM) framework that incorporates both radiology reports and radiographic images for generating knowledge graphs.

**Key Contributions:**

	1. Development of a multimodal framework for knowledge graph generation in radiology
	2. Integration of radiology reports and images in knowledge graph creation
	3. Outperforming existing unimodal solutions in the field.

**Result:** The proposed framework achieves better performance than existing unimodal methods for knowledge graph generation in radiology.

**Limitations:** 

**Conclusion:** This is the first multimodal solution for radiology knowledge graph generation, effectively addressing challenges in long-form data interpretation and specialized language.

**Abstract:** Vision-Language Models (VLMs) have demonstrated remarkable success in natural language generation, excelling at instruction following and structured output generation. Knowledge graphs play a crucial role in radiology, serving as valuable sources of factual information and enhancing various downstream tasks. However, generating radiology-specific knowledge graphs presents significant challenges due to the specialized language of radiology reports and the limited availability of domain-specific data. Existing solutions are predominantly unimodal, meaning they generate knowledge graphs only from radiology reports while excluding radiographic images. Additionally, they struggle with long-form radiology data due to limited context length. To address these limitations, we propose a novel multimodal VLM-based framework for knowledge graph generation in radiology. Our approach outperforms previous methods and introduces the first multimodal solution for radiology knowledge graph generation.

</details>


### [20] [QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing](https://arxiv.org/abs/2505.17043)

*Anya Belz*

**Main category:** cs.CL

**Keywords:** reproducibility, natural language processing, quantitative assessment

**Relevance Score:** 4

**TL;DR:** This paper presents QRA++, a new quantitative approach for assessing reproducibility in NLP studies, enabling comparisons and deeper insights into reproducibility issues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The low levels of reproducibility reported in NLP studies due to varying criteria hinder the ability to interpret and learn from reproduction studies.

**Method:** QRA++ provides continuous-valued assessments of reproducibility at three levels, using measures that are comparable across studies and grounding expectations in experiment similarity.

**Key Contributions:**

	1. Introduction of QRA++ for a standardized reproducibility assessment
	2. Continuous-valued reproducibility metrics across multiple granularity levels
	3. Demonstration of the influence of experimental similarity on reproducibility outcomes

**Result:** Application of QRA++ to three sets of experiments shows that reproducibility is influenced by the similarity of experiment properties, system type, and evaluation method.

**Limitations:** 

**Conclusion:** QRA++ enhances the understanding of reproducibility in NLP by providing clearer insights into the factors affecting it.

**Abstract:** Reproduction studies reported in NLP provide individual data points which in combination indicate worryingly low levels of reproducibility in the field. Because each reproduction study reports quantitative conclusions based on its own, often not explicitly stated, criteria for reproduction success/failure, the conclusions drawn are hard to interpret, compare, and learn from. In this paper, we present QRA++, a quantitative approach to reproducibility assessment that (i) produces continuous-valued degree of reproducibility assessments at three levels of granularity; (ii) utilises reproducibility measures that are directly comparable across different studies; and (iii) grounds expectations about degree of reproducibility in degree of similarity between experiments. QRA++ enables more informative reproducibility assessments to be conducted, and conclusions to be drawn about what causes reproducibility to be better/poorer. We illustrate this by applying QRA++ to three example sets of comparable experiments, revealing clear evidence that degree of reproducibility depends on similarity of experiment properties, but also system type and evaluation method.

</details>


### [21] [Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia](https://arxiv.org/abs/2505.17045)

*Afifah Kashif, Heer Patel*

**Main category:** cs.CL

**Keywords:** large language models, bias, empathy, mental disabilities, intersectionality

**Relevance Score:** 9

**TL;DR:** This research evaluates biases in large language models (LLMs) towards American and North Korean nationalities, particularly regarding mental disabilities, revealing significant empathy discrepancies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the ethical implications of biases in foundational LLMs that affect various nationalities and stigmatized social groups.

**Method:** The study employs structured prompt series to assess model responses to scenarios involving American and North Korean individuals with mental disabilities.

**Key Contributions:**

	1. Identification of biases in LLMs towards different nationalities
	2. Empirical evidence of varying empathy levels in model responses
	3. Recommendations for improving LLM design to incorporate intersectional identities

**Result:** The findings indicate significant bias against North Koreans, especially when mental disabilities are considered, highlighting disparities in empathy levels compared to Americans.

**Limitations:** 

**Conclusion:** The research emphasizes the necessity for enhanced LLMs that account for the complexities of intersectional identity.

**Abstract:** Recent studies have separately highlighted significant biases within foundational large language models (LLMs) against certain nationalities and stigmatized social groups. This research investigates the ethical implications of these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS. Through structured prompt series, we evaluate model responses to several scenarios involving American and North Korean nationalities with various mental disabilities. Findings reveal significant discrepancies in empathy levels with North Koreans facing greater negative bias, particularly when mental disability is also a factor. This underscores the need for improvements in LLMs designed with a nuanced understanding of intersectional identity.

</details>


### [22] [Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe](https://arxiv.org/abs/2505.17047)

*Erin Palm, Astrit Manikantan, Mark E. Pepin, Herprit Mahal, Srikanth Subramanya Belwadi*

**Main category:** cs.CL

**Keywords:** AI scribes, clinical documentation, large language models, note quality, health informatics

**Relevance Score:** 9

**TL;DR:** This study evaluates the quality of clinical notes generated by large language models (LLMs) against those drafted by human experts, using a blinded approach and the Physician Documentation Quality Instrument (PDQI9) for assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for established methods to gauge the quality of AI scribes used in clinical documentation.

**Method:** We conducted a blinded study comparing LLM-generated clinical notes with notes by field experts using audio recordings of clinical encounters, evaluated by clinical specialists using the PDQI9 framework.

**Key Contributions:**

	1. Demonstration of LLM efficacy in generating clinical notes
	2. Validation of the PDQI9 instrument for AI-generated content evaluation
	3. Evidence of high inter-rater agreement across multiple medical specialties

**Result:** High inter-rater agreement was observed among evaluators across different specialties, and a modest difference in overall note quality was found, with Gold notes slightly outperforming Ambient notes.

**Limitations:** 

**Conclusion:** The PDQI9 tool effectively assesses the quality of LLM-generated clinical notes, indicating their viability as a scribe alternative in medical practice.

**Abstract:** In medical practices across the United States, physicians have begun implementing generative artificial intelligence (AI) tools to perform the function of scribes in order to reduce the burden of documenting clinical encounters. Despite their widespread use, no established methods exist to gauge the quality of AI scribes. To address this gap, we developed a blinded study comparing the relative performance of large language model (LLM) generated clinical notes with those from field experts based on audio-recorded clinical encounters. Quantitative metrics from the Physician Documentation Quality Instrument (PDQI9) provided a framework to measure note quality, which we adapted to assess relative performance of AI generated notes. Clinical experts spanning 5 medical specialties used the PDQI9 tool to evaluate specialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators from each specialty scored notes drafted from a total of 97 patient visits. We found uniformly high inter rater agreement (RWG greater than 0.7) between evaluators in general medicine, orthopedics, and obstetrics and gynecology, and moderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and cardiology. We found a modest yet significant difference in the overall note quality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes scored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9 instrument as a practical method to gauge the quality of LLM authored notes, as compared to human-authored notes.

</details>


### [23] [Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally](https://arxiv.org/abs/2505.17048)

*Agam Shah, Siddhant Sukhani, Huzaifa Pardawala, Saketh Budideti, Riya Bhadani, Rudra Gopal, Siddhartha Somani, Michael Galarnyk, Soungmin Lee, Arnav Hiray, Akshar Ravichandran, Eric Kim, Pranav Aluru, Joshua Zhang, Sebastian Jaskowski, Veer Guda, Meghaj Tarte, Liqin Ye, Spencer Gosden, Rutwik Routu, Rachel Yuh, Sloka Chava, Sahasra Chava, Dylan Patrick Kelly, Aiden Chiang, Harsit Mittal, Sudheer Chava*

**Main category:** cs.CL

**Keywords:** Central Banks, Monetary Policy, Natural Language Processing, Language Models, Economic Analysis

**Relevance Score:** 4

**TL;DR:** The paper presents the World Central Banks (WCB) dataset, a comprehensive corpus for analyzing monetary policy communications, and benchmarks various language models on tasks related to economic implications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Central banks influence economic stability, and accurately interpreting their communications is critical, particularly for vulnerable populations.

**Method:** The study introduces the WCB dataset consisting of 25k annotated sentences from 25 central banks, and benchmarks seven PLMs and nine LLMs on three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation.

**Key Contributions:**

	1. Introduction of the comprehensive WCB dataset for monetary policy analysis
	2. Benchmarking of multiple PLMs and LLMs on economic communication tasks
	3. Validation of the framework's outcomes through rigorous evaluations

**Result:** Models trained on aggregated data from multiple banks outperform those trained on single bank data, affirming the synergy of combined data.

**Limitations:** 

**Conclusion:** The developed framework, supported by extensive evaluations, demonstrates significant economic utility and accessibility of the dataset through public repositories.

**Abstract:** Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle "the whole is greater than the sum of its parts." Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license.

</details>


### [24] [Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/Rsum Evaluations](https://arxiv.org/abs/2505.17049)

*David Rozado*

**Main category:** cs.CL

**Keywords:** Large Language Models, gender bias, candidate selection, neutral identifiers, decision-making

**Relevance Score:** 9

**TL;DR:** The study analyzes the behavior of LLMs in selecting candidates based on gendered resumes, revealing a preference for female-named candidates despite identical qualifications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate potential gender biases in LLM candidate selection when evaluating professional CVs.

**Method:** 22 leading LLMs were tested with pairs of CVs (one male-named, one female-named) for the same job across various professions, with controls for name switching and gender-neutral identifiers.

**Key Contributions:**

	1. Demonstration of gender bias in LLM candidate selection.
	2. Impact of explicit gender fields on candidate preference.
	3. Introduction of gender-neutral identifiers leading to reduced bias.

**Result:** All LLMs favored female candidates even when qualifications were identical; including explicit gender indicators increased this preference. The use of gender-neutral identifiers resulted in gender parity in candidate selection.

**Limitations:** The study focused solely on CV assessment and did not explore other dimensions of candidate evaluation.

**Conclusion:** The findings caution against relying on LLMs for high-stakes decision-making due to evidence of gender bias and positional bias in selection processes.

**Abstract:** This study examines the behavior of Large Language Models (LLMs) when evaluating professional candidates based on their resumes or curricula vitae (CVs). In an experiment involving 22 leading LLMs, each model was systematically given one job description along with a pair of profession-matched CVs, one bearing a male first name, the other a female first name, and asked to select the more suitable candidate for the job. Each CV pair was presented twice, with names swapped to ensure that any observed preferences in candidate selection stemmed from gendered names cues. Despite identical professional qualifications across genders, all LLMs consistently favored female-named candidates across 70 different professions. Adding an explicit gender field (male/female) to the CVs further increased the preference for female applicants. When gendered names were replaced with gender-neutral identifiers "Candidate A" and "Candidate B", several models displayed a preference to select "Candidate A". Counterbalancing gender assignment between these gender-neutral identifiers resulted in gender parity in candidate selection. When asked to rate CVs in isolation rather than compare pairs, LLMs assigned slightly higher average scores to female CVs overall, but the effect size was negligible. Including preferred pronouns (he/him or she/her) next to a candidate's name slightly increased the odds of the candidate being selected regardless of gender. Finally, most models exhibited a substantial positional bias to select the candidate listed first in the prompt. These findings underscore the need for caution when deploying LLMs in high-stakes autonomous decision-making contexts and raise doubts about whether LLMs consistently apply principled reasoning.

</details>


### [25] [Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning](https://arxiv.org/abs/2505.17050)

*Yanhao Jia, Xinyi Wu, Qinglin Zhang, Yiran Qin, Luwei Xiao, Shuai Zhao*

**Main category:** cs.CL

**Keywords:** multimodal large language models, Project-Based Learning, benchmark evaluation, educational productivity, Analytic Hierarchy Process

**Relevance Score:** 7

**TL;DR:** PBLBench is a benchmark created to evaluate the reasoning and understanding capabilities of multimodal large language models in educational settings, particularly in Project-Based Learning (PBL).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequate benchmarking of multimodal large language models (MLLMs) in educational tasks within Project-Based Learning (PBL) environments.

**Method:** Introduction of PBLBench, a benchmark utilizing the Analytic Hierarchy Process (AHP) for expert-driven evaluations, assessing 15 leading MLLMs/LLMs on complex educational tasks.

**Key Contributions:**

	1. Introduction of a comprehensive evaluation benchmark (PBLBench) for MLLMs in education
	2. Use of the Analytic Hierarchy Process (AHP) for expert-driven evaluation
	3. Empirical assessment of leading MLLMs showcasing their limitations in complex educational tasks

**Result:** Top-performing models achieved only 59% rank accuracy, indicating the challenges MLLMs face in educational contexts.

**Limitations:** Existing benchmarks lack free-form output structures and rigorous validation processes; MLLMs exhibit model hallucination and instability.

**Conclusion:** PBLBench can improve the assessment of MLLMs in educational environments and contribute to the development of better AI tools for teachers.

**Abstract:** Project-Based Learning (PBL) involves a variety of highly correlated multimodal data, making it a vital educational approach within STEM disciplines. With the rapid development of multimodal large language models (MLLMs), researchers have begun exploring their potential to enhance tasks such as information retrieval, knowledge comprehension, and data generation in educational settings. However, existing benchmarks fall short in providing both a free-form output structure and a rigorous human expert validation process, limiting their effectiveness in evaluating real-world educational tasks. Additionally, few methods have developed automated pipelines to assist with the complex responsibilities of teachers leveraging MLLMs, largely due to model hallucination and instability, which lead to unreliable implementation. To address this gap, we introduce PBLBench, a novel benchmark designed to evaluate complex reasoning grounded in domain-specific knowledge and long-context understanding, thereby challenging models with tasks that closely resemble those handled by human experts. To establish reliable ground truth, we adopt the Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise comparisons to derive structured and weighted evaluation criteria. We assess the performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that even the most advanced models achieve only 59% rank accuracy, underscoring the significant challenges presented by this benchmark. We believe PBLBench will serve as a catalyst for the development of more capable AI agents, ultimately aiming to alleviate teacher workload and enhance educational productivity.

</details>


### [26] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)

*Fidaa khandaqji, Huthaifa I. Ashqar, Abdelrahem Atawnih*

**Main category:** cs.CL

**Keywords:** sign language recognition, mathematics education, deep learning, computer vision, AI in education

**Relevance Score:** 4

**TL;DR:** Development of a Palestinian sign language recognition system for mathematics education using AI to assist hard-of-hearing students.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility in mathematics education for hard-of-hearing students through AI technology.

**Method:** A custom dataset of 41 mathematical gesture classes was created and a Vision Transformer model was fine-tuned for gesture classification.

**Key Contributions:**

	1. Creation of a custom dataset for Palestinian sign language in mathematics education.
	2. Implementation of a Vision Transformer model for gesture classification achieving high accuracy.
	3. Development of an AI-driven tool to enhance educational accessibility.

**Result:** The model achieved an accuracy of 97.59%, effectively recognizing mathematical signs with high precision.

**Limitations:** 

**Conclusion:** The study demonstrates that deep learning can create intelligent tools for education, bridging gaps for hard-of-hearing students.

**Abstract:** The study aims to enhance mathematics education accessibility for hard-of-hearing students by developing an accurate Palestinian sign language PSL recognition system using advanced artificial intelligence techniques. Due to the scarcity of digital resources for PSL, a custom dataset comprising 41 mathematical gesture classes was created, and recorded by PSL experts to ensure linguistic accuracy and domain specificity. To leverage state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was fine-tuned for gesture classification. The model achieved an accuracy of 97.59%, demonstrating its effectiveness in recognizing mathematical signs with high precision and reliability. This study highlights the role of deep learning in developing intelligent educational tools that bridge the learning gap for hard-of-hearing students by providing AI-driven interactive solutions to enhance mathematical comprehension. This work represents a significant step toward innovative and inclusive frosting digital integration in specialized learning environments. The dataset is hosted on Hugging Face at https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [27] [Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models](https://arxiv.org/abs/2505.17051)

*Bernd Huber, Ghazal Fazelnia, Andreas Damianou, Sebastian Peleato, Max Lefarov, Praveen Ravichandran, Marco De Nadai, Mounia Lalmas-Roellke, Paul N. Bennett*

**Main category:** cs.CL

**Keywords:** large language models, personalization, embedding-to-prefix, contextual generation, human-computer interaction

**Relevance Score:** 9

**TL;DR:** A new method, Embedding-to-Prefix (E2P), efficiently personalizes language model outputs by using user embeddings without the need for extensive fine-tuning, achieving high performance in various applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Personalizing outputs of large language models (LLMs) for individual users is challenging, especially given the need for costly fine-tuning or complex prompting methods.

**Method:** embedding contextual user embeddings into LLM's hidden representation via a learned projection to a single soft token prefix, allowing personalized content generation without modifying the original model.

**Key Contributions:**

	1. Introduction of the Embedding-to-Prefix (E2P) method for LLM personalization
	2. Parameter-efficient personalization that avoids costly fine-tuning
	3. Evaluation across multiple real-world and benchmark datasets for comprehensive validation.

**Result:** E2P shows strong performance in dialogue personalization, contextual headline generation, and music/podcast personalization, while preserving contextual signals and minimizing computational overhead.

**Limitations:** 

**Conclusion:** E2P presents a scalable and efficient approach for integrating user-specific information into generative AI systems without intensive resource use.

**Abstract:** Large language models (LLMs) excel at generating contextually relevant content. However, tailoring these outputs to individual users for effective personalization is a significant challenge. While rich user-specific information often exists as pre-existing user representations, such as embeddings learned from preferences or behaviors, current methods to leverage these for LLM personalization typically require costly fine-tuning or token-heavy prompting. We propose Embedding-to-Prefix (E2P), a parameter-efficient method that injects pre-computed context embeddings into an LLM's hidden representation space through a learned projection to a single soft token prefix. This enables effective personalization while keeping the backbone model frozen and avoiding expensive adaptation techniques. We evaluate E2P across two public datasets and in a production setting: dialogue personalization on Persona-Chat, contextual headline generation on PENS, and large-scale personalization for music and podcast consumption. Results show that E2P preserves contextual signals and achieves strong performance with minimal computational overhead, offering a scalable, efficient solution for contextualizing generative AI systems.

</details>


### [28] [SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs](https://arxiv.org/abs/2505.17052)

*Jinwoo Park, Seunggeun Cho, Dongsu Han*

**Main category:** cs.CL

**Keywords:** large language models, edge computing, inference frameworks

**Relevance Score:** 8

**TL;DR:** SpecEdge is an edge-assisted inference framework that improves the efficiency of serving large language models by splitting workloads between edge and server GPUs, leading to cost savings and reduced latency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and resource intensiveness of serving large language models at scale, especially in server-centric systems that ignore consumer-grade GPUs at the edge.

**Method:** SpecEdge uses a speculative decoding scheme to split LLM workloads between edge and server GPUs, implementing proactive edge drafting and pipeline-aware scheduling to optimize user request handling.

**Key Contributions:**

	1. Introduction of SpecEdge framework for edge-assisted LLM inference
	2. Demonstration of improved server throughput and cost efficiency
	3. Implementation of proactive edge drafting and pipeline-aware scheduling

**Result:** SpecEdge improves cost efficiency by 1.91x and server throughput by 2.22x, while reducing inter token latency by 11.24% compared to conventional server-only approaches.

**Limitations:** 

**Conclusion:** The framework offers a scalable and cost-effective solution for serving large language models by leveraging edge computing.

**Abstract:** Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by 1.91x through achieving 2.22x server throughput, and reduces inter token latency by 11.24% compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving.

</details>


### [29] [Social preferences with unstable interactive reasoning: Large language models in economic trust games](https://arxiv.org/abs/2505.17053)

*Ou Jiamin, Eikmans Emile, Buskens Vincent, Pankowska Paulina, Shan Yuli*

**Main category:** cs.CL

**Keywords:** large language models, social interactions, trust games, human-computer interaction, interactive reasoning

**Relevance Score:** 8

**TL;DR:** The study investigates how large language models (LLMs) make decisions in economic trust games, showing that they can emulate human social behavior such as trust and reciprocity, especially when adopting different personas.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs translate their understanding of human language into social exchanges and decision-making in contexts resembling real human interactions.

**Method:** Three LLMs (ChatGPT-4, Claude, and Bard) were tested in economic trust games to evaluate their behavior in balancing self-interest with trust and reciprocity.

**Key Contributions:**

	1. Demonstrated LLMs' emulation of human social behavior in trust games
	2. Analysis of LLM decision-making under various personas
	3. Highlighted discrepancies between human and LLM interactive reasoning

**Result:** LLMs displayed a tendency towards trust and reciprocity, with varying degrees of adherence to human-like behavior depending on the personas they adopted; ChatGPT-4 showed the highest levels of trust in unselfish conditions.

**Limitations:** The responses of LLMs in social context showed random characteristics rather than stable patterns, indicating the need for further investigation into their interactive reasoning.

**Conclusion:** Overall, LLMs show nuanced behavior in social contexts, with the persona significantly affecting their interaction patterns, notably surpassing humans in some trust scenarios.

**Abstract:** While large language models (LLMs) have demonstrated remarkable capabilities in understanding human languages, this study explores how they translate this understanding into social exchange contexts that capture certain essences of real world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were placed in economic trust games where players balance self-interest with trust and reciprocity, making decisions that reveal their social preferences and interactive reasoning abilities. Our study shows that LLMs deviate from pure self-interest and exhibit trust and reciprocity even without being prompted to adopt a specific persona. In the simplest one-shot interaction, LLMs emulated how human players place trust at the beginning of such a game. Larger human-machine divergences emerged in scenarios involving trust repayment or multi-round interactions, where decisions were influenced by both social preferences and interactive reasoning. LLMs responses varied significantly when prompted to adopt personas like selfish or unselfish players, with the impact outweighing differences between models or game types. Response of ChatGPT-4, in an unselfish or neutral persona, resembled the highest trust and reciprocity, surpassing humans, Claude, and Bard. Claude and Bard displayed trust and reciprocity levels that sometimes exceeded and sometimes fell below human choices. When given selfish personas, all LLMs showed lower trust and reciprocity than humans. Interactive reasoning to the actions of counterparts or changing game mechanics appeared to be random rather than stable, reproducible characteristics in the response of LLMs, though some improvements were observed when ChatGPT-4 responded in selfish or unselfish personas.

</details>


### [30] [METHOD: Modular Efficient Transformer for Health Outcome Discovery](https://arxiv.org/abs/2505.17054)

*Linglong Qian, Zina Ibrahim*

**Main category:** cs.CL

**Keywords:** transformer architecture, healthcare, clinical sequence modeling, electronic health records, machine learning

**Relevance Score:** 9

**TL;DR:** A novel transformer architecture METHOD~ is introduced for healthcare, improving predictions from electronic health records and handling unique challenges in clinical sequence modeling.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in applying transformer architectures to healthcare, particularly with clinical sequence modeling in electronic health records.

**Method:** METHOD~ integrates a patient-aware attention mechanism, an adaptive sliding window attention scheme, and a U-Net inspired architecture.

**Key Contributions:**

	1. Patient-aware attention mechanism
	2. Adaptive sliding window attention scheme
	3. U-Net inspired architecture with dynamic skip connections

**Result:** METHOD~ consistently outperforms the ETHOS~ model in predicting high-severity cases and maintains performance across varying inference lengths.

**Limitations:** 

**Conclusion:** METHOD~ shows significant advancements in transformer architectures for healthcare applications, providing accurate predictions while ensuring computational efficiency.

**Abstract:** Recent advances in transformer architectures have revolutionised natural language processing, but their application to healthcare domains presents unique challenges. Patient timelines are characterised by irregular sampling, variable temporal dependencies, and complex contextual relationships that differ substantially from traditional language tasks. This paper introduces \METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel transformer architecture specifically designed to address the challenges of clinical sequence modelling in electronic health records. \METHOD~integrates three key innovations: (1) a patient-aware attention mechanism that prevents information leakage whilst enabling efficient batch processing; (2) an adaptive sliding window attention scheme that captures multi-scale temporal dependencies; and (3) a U-Net inspired architecture with dynamic skip connections for effective long sequence processing. Evaluations on the MIMIC-IV database demonstrate that \METHOD~consistently outperforms the state-of-the-art \ETHOS~model, particularly in predicting high-severity cases that require urgent clinical intervention. \METHOD~exhibits stable performance across varying inference lengths, a crucial feature for clinical deployment where patient histories vary significantly in length. Analysis of learned embeddings reveals that \METHOD~better preserves clinical hierarchies and relationships between medical concepts. These results suggest that \METHOD~represents a significant advancement in transformer architectures optimised for healthcare applications, providing more accurate and clinically relevant predictions whilst maintaining computational efficiency.

</details>


### [31] [Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset](https://arxiv.org/abs/2505.17055)

*Fidaa khandaqji, Huthaifa I. Ashqar, Abdelrahem Atawnih*

**Main category:** cs.CL

**Keywords:** Palestinian sign language, gesture recognition, mathematics education, deep learning, computer vision

**Relevance Score:** 4

**TL;DR:** Development of an advanced PSL recognition system to improve mathematics education for hard-of-hearing students.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance accessibility in mathematics education for hard-of-hearing students through AI-driven tools.

**Method:** A custom dataset of 41 mathematical gesture classes was created, and a Vision Transformer model was fine-tuned for gesture classification.

**Key Contributions:**

	1. Development of a unique dataset for Palestinian sign language in mathematics education.
	2. Application of Vision Transformer for gesture recognition in the educational context.
	3. Demonstration of the high accuracy of the model in recognizing mathematical gestures.

**Result:** The model achieved a gesture recognition accuracy of 97.59%, demonstrating high precision and reliability.

**Limitations:** 

**Conclusion:** This work advances digital integration and innovative educational tools for specialized learning environments.

**Abstract:** The study aims to enhance mathematics education accessibility for hard-of-hearing students by developing an accurate Palestinian sign language PSL recognition system using advanced artificial intelligence techniques. Due to the scarcity of digital resources for PSL, a custom dataset comprising 41 mathematical gesture classes was created, and recorded by PSL experts to ensure linguistic accuracy and domain specificity. To leverage state-of-the-art-computer vision techniques, a Vision Transformer ViTModel was fine-tuned for gesture classification. The model achieved an accuracy of 97.59%, demonstrating its effectiveness in recognizing mathematical signs with high precision and reliability. This study highlights the role of deep learning in developing intelligent educational tools that bridge the learning gap for hard-of-hearing students by providing AI-driven interactive solutions to enhance mathematical comprehension. This work represents a significant step toward innovative and inclusive frosting digital integration in specialized learning environments. The dataset is hosted on Hugging Face at https://huggingface.co/datasets/fidaakh/STEM_data.

</details>


### [32] [Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective](https://arxiv.org/abs/2505.17056)

*Luoxi Tang, Tharunya Sundar, Shuai Yang, Ankita Patra, Manohar Chippada, Giqi Zhao, Yi Li, Riteng Zhang, Tunan Zhao, Ting Yang, Yuqiao Meng, Weicheng Ma, Zhaohan Xi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Standardized Tests, Educational Technology, Intelligent Tutoring Systems

**Relevance Score:** 9

**TL;DR:** This paper investigates the use of large language models (LLMs) for standardized test preparation, specifically focusing on English Standardized Tests (ESTs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can enhance learning experiences and provide support for standardized test preparation in education.

**Method:** The authors introduce ESTBOOK, a benchmark that aggregates five widely recognized tests and evaluates LLM performance on various EST question types through a structured breakdown analysis framework.

**Key Contributions:**

	1. Introduction of ESTBOOK as a benchmark for evaluating LLM capabilities on ESTs
	2. Systematic assessment of LLMs across diverse question types and modalities
	3. Development of a breakdown analysis framework for complex question solving

**Result:** LLMs were evaluated for accuracy and inference efficiency on over 10,576 questions, revealing insights into their capabilities and potential improvements in educational applications.

**Limitations:** 

**Conclusion:** The evaluation findings demonstrate the promise of LLMs as intelligent tutoring systems in educational contexts while suggesting targeted strategies for enhancing their reliability.

**Abstract:** AI is transforming education by enabling powerful tools that enhance learning experiences. Among recent advancements, large language models (LLMs) hold particular promise for revolutionizing how learners interact with educational content. In this work, we investigate the potential of LLMs to support standardized test preparation by focusing on English Standardized Tests (ESTs). Specifically, we assess their ability to generate accurate and contextually appropriate solutions across a diverse set of EST question types. We introduce ESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of LLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests, encompassing 29 question types and over 10,576 questions across multiple modalities, including text, images, audio, tables, and mathematical symbols. Using ESTBOOK, we systematically evaluate both the accuracy and inference efficiency of LLMs. Additionally, we propose a breakdown analysis framework that decomposes complex EST questions into task-specific solution steps. This framework allows us to isolate and assess LLM performance at each stage of the reasoning process. Evaluation findings offer insights into the capability of LLMs in educational contexts and point toward targeted strategies for improving their reliability as intelligent tutoring systems.

</details>


### [33] [DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2505.17058)

*David Osei Opoku, Ming Sheng, Yong Zhang*

**Main category:** cs.CL

**Keywords:** Hybrid QA, Knowledge Graphs, Retrieval-Augmented Generation, Question Answering, Performance Efficiency

**Relevance Score:** 9

**TL;DR:** DO-RAG is a hybrid QA framework that integrates knowledge graphs with semantic vector retrieval to enhance accuracy and context-awareness in domain-specific question answering.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the integration of heterogeneous data and maintain reasoning consistency in QA systems.

**Method:** DO-RAG combines multi-level knowledge graph construction with semantic vector retrieval, utilizing an agentic chain-of-thought architecture to create dynamic knowledge graphs from diverse documents.

**Key Contributions:**

	1. Proposes a hybrid QA framework that integrates knowledge graphs and vector retrieval.
	2. Introduces an agentic chain-of-thought architecture for extracting structured information.
	3. Demonstrates performance improvements over baseline frameworks in key evaluation areas.

**Result:** Experimental evaluations demonstrate that DO-RAG achieves near-perfect recall and over 94% answer relevancy, outperforming existing frameworks significantly in specific domains.

**Limitations:** 

**Conclusion:** DO-RAG serves as a scalable solution for high-precision QA across various domains, emphasizing traceability and performance efficiency.

**Abstract:** Domain-specific QA systems require not just generative fluency but high factual accuracy grounded in structured expert knowledge. While recent Retrieval-Augmented Generation (RAG) frameworks improve context recall, they struggle with integrating heterogeneous data and maintaining reasoning consistency. To address these challenges, we propose DO-RAG, a scalable and customizable hybrid QA framework that integrates multi-level knowledge graph construction with semantic vector retrieval. Our system employs a novel agentic chain-of-thought architecture to extract structured relationships from unstructured, multimodal documents, constructing dynamic knowledge graphs that enhance retrieval precision. At query time, DO-RAG fuses graph and vector retrieval results to generate context-aware responses, followed by hallucination mitigation via grounded refinement. Experimental evaluations in the database and electrical domains show near-perfect recall and over 94% answer relevancy, with DO-RAG outperforming baseline frameworks by up to 33.38%. By combining traceability, adaptability, and performance efficiency, DO-RAG offers a reliable foundation for multi-domain, high-precision QA at scale.

</details>


### [34] [Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large](https://arxiv.org/abs/2505.17059)

*Van-Tinh Nguyen, Hoang-Duong Pham, Thanh-Hai To, Cong-Tuan Hung Do, Thi-Thu-Trang Dong, Vu-Trung Duong Le, Van-Phuc Hoang*

**Main category:** cs.CL

**Keywords:** medical text comprehension, AI application, healthcare accessibility

**Relevance Score:** 9

**TL;DR:** Medalyze is an AI application that aids comprehension of medical texts using fine-tuned FLAN-T5-Large models for summarization, issue extraction, and question identification.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of comprehending medical texts due to the complexity of terminology and context-specific language.

**Method:** The application utilizes three specialized FLAN-T5-Large models for summarizing reports, extracting health issues, and identifying key questions, while being deployed on a web and mobile platform with real-time inference.

**Key Contributions:**

	1. Introduction of Medalyze, a specialized AI application for medical text comprehension.
	2. Demonstrated superior performance of Medalyze in summarization tasks compared to existing models.
	3. Deployment across both web and mobile platforms, ensuring real-time accessibility.

**Result:** Experimental evaluations show Medalyze outperforming GPT-4 in summarization tasks based on multiple performance metrics, highlighting its superiority in domain-specific applications.

**Limitations:** 

**Conclusion:** Medalyze serves as a practical and privacy-preserving tool that enhances accessibility to medical information in healthcare settings.

**Abstract:** Understanding medical texts presents significant challenges due to complex terminology and context-specific language. This paper introduces Medalyze, an AI-powered application designed to enhance the comprehension of medical texts using three specialized FLAN-T5-Large models. These models are fine-tuned for (1) summarizing medical reports, (2) extracting health issues from patient-doctor conversations, and (3) identifying the key question in a passage. Medalyze is deployed across a web and mobile platform with real-time inference, leveraging scalable API and YugabyteDB. Experimental evaluations demonstrate the system's superior summarization performance over GPT-4 in domain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and SpaCy Similarity. Medalyze provides a practical, privacy-preserving, and lightweight solution for improving information accessibility in healthcare.

</details>


### [35] [SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation](https://arxiv.org/abs/2505.17060)

*Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, Chao Zhang*

**Main category:** cs.CL

**Keywords:** full-duplex, speech LLM, dynamic thinking, human-machine interaction, reinforcement learning

**Relevance Score:** 9

**TL;DR:** Introducing SALMONN-omni, a standalone full-duplex speech LLM that excels in human-machine speech interaction by overcoming limitations of modular architectures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in existing full-duplex conversational systems that lead to error accumulation and performance degradation, particularly in speech modalities.

**Method:** Develop a single LLM, SALMONN-omni, that does not rely on audio codecs in the token space, incorporating a dynamic thinking mechanism that allows the model to transition between speaking and listening states.

**Key Contributions:**

	1. First standalone full-duplex speech LLM operating without audio codecs
	2. Dynamic thinking mechanism for effective conversation management
	3. Reinforcement learning enhancements leading to better performance in complex scenarios

**Result:** SALMONN-omni shows over 30% performance improvement compared to existing full-duplex models and competes well with half-duplex systems while requiring less training data.

**Limitations:** 

**Conclusion:** With strong results in complex conversational scenarios, SALMONN-omni represents a significant advancement in full-duplex speech interaction technology.

**Abstract:** In order to enable fluid and natural human-machine speech interaction, existing full-duplex conversational systems often adopt modular architectures with auxiliary components such as voice activity detectors, interrupters, conversation state predictors, or multiple LLMs. These systems, however, suffer from error accumulation across modules and struggle with key challenges such as context-dependent barge-in and echo cancellation. Recent approaches, most notably Moshi, simplify the pipeline by injecting audio codecs into the token space of a single LLM. However, such methods still incur significant performance degradation when operating on the speech rather than text modality. In this paper, we introduce SALMONN-omni, the first single, standalone full-duplex speech LLM that operates without audio codecs in its token space. It features a novel dynamic thinking mechanism within the LLM backbone, enabling the model to learn when to transition between speaking and listening states. Experiments on widely used benchmarks for spoken question answering and open-domain dialogue show that SALMONN-omni achieves at least 30\% relative performance improvement over existing open-source full-duplex models and performs highly competitively to half-duplex and turn-based systems, despite using substantially less training data. Moreover, SALMONN-omni demonstrates strong performance in complex conversational scenarios, including turn-taking, backchanneling, echo cancellation and context-dependent barge-in, with further improvements achieved through reinforcement learning. Some demo conversations between user and SALMONN-omni are provided in the following repository https://github.com/bytedance/SALMONN.

</details>


### [36] [Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2505.17061)

*Xinlong Chen, Yuanxing Zhang, Qiang Liu, Junfei Wu, Fuzheng Zhang, Tieniu Tan*

**Main category:** cs.CL

**Keywords:** Large Vision-Language Models, hallucinations, decoding strategies, mixture of decoding, attention mechanisms

**Relevance Score:** 7

**TL;DR:** The paper introduces Mixture of Decoding (MoD), a new method for reducing hallucinations in Large Vision-Language Models by dynamically adapting decoding strategies based on the correctness of attention on image tokens.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the persistent issue of hallucinations that affect the performance of Large Vision-Language Models in visual tasks.

**Method:** MoD evaluates the consistency of outputs generated from original and attended image tokens to adaptively apply either a complementary strategy to amplify correct information or a contrastive strategy to suppress misleading information.

**Key Contributions:**

	1. Introduction of the Mixture of Decoding method for hallucination mitigation.
	2. Dynamic adaptation of decoding strategies based on attention correctness.
	3. Extensive experimental validation showing superior performance on benchmarks.

**Result:** MoD significantly outperforms existing decoding techniques across several mainstream benchmarks, effectively reducing hallucinations in LVLMs.

**Limitations:** 

**Conclusion:** The proposed approach demonstrates a viable solution to improve the reliability of LVLMs through better decoding strategies based on model attention evaluation.

**Abstract:** Large Vision-Language Models (LVLMs) have exhibited impressive capabilities across various visual tasks, yet they remain hindered by the persistent challenge of hallucinations. To address this critical issue, we propose Mixture of Decoding (MoD), a novel approach for hallucination mitigation that dynamically adapts decoding strategies by evaluating the correctness of the model's attention on image tokens. Specifically, MoD measures the consistency between outputs generated from the original image tokens and those derived from the model's attended image tokens, to distinguish the correctness aforementioned. If the outputs are consistent, indicating correct attention, MoD employs a complementary strategy to amplify critical information. Conversely, if the outputs are inconsistent, suggesting erroneous attention, MoD utilizes a contrastive strategy to suppress misleading information. Extensive experiments demonstrate that MoD significantly outperforms existing decoding methods across multiple mainstream benchmarks, effectively mitigating hallucinations in LVLMs. The code is available at https://github.com/xlchen0205/MoD.

</details>


### [37] [Synthetic Data RL: Task Definition Is All You Need](https://arxiv.org/abs/2505.17063)

*Yiduo Guo, Zhen Guo, Chuanwei Huang, Zi-Ang Wang, Zekai Zhang, Haofei Yu, Huishuai Zhang, Yikang Shen*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Synthetic Data, Model Adaptation, Human-Labeled Data, Performance Improvement

**Relevance Score:** 8

**TL;DR:** Introducing Synthetic Data RL, a framework that fine-tunes models with synthetic data to reduce reliance on human-labeled data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of reinforcement learning (RL) that requires large-scale human-labeled data for adapting foundation models to specialized tasks.

**Method:** The method generates question and answer pairs from task definitions, adapts difficulty based on model solvability, and selects questions using the average pass rate of the model for RL training.

**Key Contributions:**

	1. Synthetic Data RL framework for model tuning without extensive human labels
	2. Performance improvements on multiple datasets compared to traditional methods
	3. Demonstrates limited added value of human demonstrations in enhancing model performance

**Result:** Synthetic Data RL shows significant performance improvements on various benchmarks: 29.2% on GSM8K, 8.7% on MATH, 13.1% on GPQA, and more, outperforming supervised fine-tuning with the same data budget.

**Limitations:** 

**Conclusion:** Synthetic Data RL enables scalable and efficient RL-based model adaptation while limiting the need for human data annotation.

**Abstract:** Reinforcement learning (RL) is a powerful way to adapt foundation models to specialized tasks, but its reliance on large-scale human-labeled data limits broad adoption. We introduce Synthetic Data RL, a simple and general framework that reinforcement fine-tunes models using only synthetic data generated from a task definition. Our method first generates question and answer pairs from the task definition and retrieved documents, then adapts the difficulty of the question based on model solvability, and selects questions using the average pass rate of the model across samples for RL training. On Qwen-2.5-7B, our method achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9 pp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on GPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA (finance). It surpasses supervised fine-tuning under the same data budget and nearly matches RL with full human data across datasets (e.g., +17.2 pp on GSM8K). Adding 100 human demonstrations improves the performance of GSM8K only by 0.4 pp, showing a limited added value. By reducing human data annotation, Synthetic Data RL enables scalable and efficient RL-based model adaptation. Code and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.

</details>


### [38] [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/abs/2503.02003)

*Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination, Chain-of-Thought, HoT, information verification

**Relevance Score:** 9

**TL;DR:** Introducing Highlighted Chain-of-Thought Prompting (HoT) to improve factual accuracy in LLMs by using XML tags to highlight key facts in responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucination in Large Language Models (LLMs) which results in mixed factual and non-factual statements, making it challenging for users to verify accuracy.

**Method:** HoT reformats input questions to include XML tags that identify key factual elements, then generates responses highlighting these facts.

**Key Contributions:**

	1. Development of HoT technique for LLMs
	2. Empirical results showing improvement over CoT
	3. Insights into user verification processes with highlighted responses

**Result:** In few-shot settings, HoT outperforms standard chain of thought prompting (CoT) across 17 tasks such as arithmetic and logical reasoning. Highlights aid users in more efficiently identifying correct LLM responses.

**Limitations:** HoT can lead to users mistakenly trusting incorrect responses due to highlights.

**Conclusion:** While highlights improve verification in correct responses, they may also mislead users into believing incorrect answers are correct.

**Abstract:** An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.

</details>


### [39] [Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases](https://arxiv.org/abs/2505.17065)

*Valentina Carbonari, Pierangelo Veltri, Pietro Hiram Guzzi*

**Main category:** cs.CL

**Keywords:** rare diseases, large language models, health informatics, multimodal data, machine learning

**Relevance Score:** 9

**TL;DR:** Explores integration of LLMs in rare disease research, focusing on analysis and multimodal data potential.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight how LLMs can transform rare disease research through analysis of textual data and uncovering insights critical for healthcare.

**Method:** Survey of existing literature on the application of LLMs in rare diseases, including experiments with multiple LLMs for diagnostic purposes.

**Key Contributions:**

	1. Integration of LLMs in rare disease research.
	2. Review of foundational papers on LLM applications.
	3. Discussion of ethical considerations and challenges in LLM deployment.

**Result:** LLMs demonstrate significant capabilities in extracting medical information, simulating patient interaction, and improving diagnostic accuracy; there is potential for multimodal data integration.

**Limitations:** Focus on textual data; challenges around data privacy and model transparency.

**Conclusion:** Future LLMs could integrate various data types for better understanding of rare diseases, enhancing clinical outcomes.

**Abstract:** Recent advances in artificial intelligence, particularly large language models LLMs, have shown promising capabilities in transforming rare disease research. This survey paper explores the integration of LLMs in the analysis of rare diseases, highlighting significant strides and pivotal studies that leverage textual data to uncover insights and patterns critical for diagnosis, treatment, and patient care. While current research predominantly employs textual data, the potential for multimodal data integration combining genetic, imaging, and electronic health records stands as a promising frontier. We review foundational papers that demonstrate the application of LLMs in identifying and extracting relevant medical information, simulating intelligent conversational agents for patient interaction, and enabling the formulation of accurate and timely diagnoses. Furthermore, this paper discusses the challenges and ethical considerations inherent in deploying LLMs, including data privacy, model transparency, and the need for robust, inclusive data sets. As part of this exploration, we present a section on experimentation that utilizes multiple LLMs alongside structured questionnaires, specifically designed for diagnostic purposes in the context of different diseases. We conclude with future perspectives on the evolution of LLMs towards truly multimodal platforms, which would integrate diverse data types to provide a more comprehensive understanding of rare diseases, ultimately fostering better outcomes in clinical settings.

</details>


### [40] [Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning](https://arxiv.org/abs/2505.17067)

*Kristin Qi, Jiali Cheng, Youxiang Zhu, Hadi Amiri, Xiaohui Liang*

**Main category:** cs.CL

**Keywords:** Mild Cognitive Impairment, Multilingual, Contrastive Learning, Image Modality, Detection Framework

**Relevance Score:** 6

**TL;DR:** This paper proposes a framework for detecting Mild Cognitive Impairment (MCI) using multilingual and multiple-picture descriptions, improving detection performance significantly over previous unimodal text methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Detecting MCI from diverse and multilingual descriptions poses significant challenges that have not been adequately addressed in prior research, which predominantly focused on English speakers with single images.

**Method:** The proposed framework consists of three components: supervised contrastive learning for enhanced representation, inclusion of image modality alongside text, and a Product of Experts strategy to reduce overfitting and spurious correlations.

**Key Contributions:**

	1. Introduction of a multilingual and multiple-picture setting for MCI detection
	2. Development of a framework leveraging supervised contrastive learning and image modality
	3. Achievement of significant performance improvements over traditional unimodal approaches

**Result:** The framework improved MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) and a +2.9% increase in F1 score compared to a text-only baseline.

**Limitations:** 

**Conclusion:** The results demonstrate the effectiveness of the proposed framework in increasing performance for multilingual and multi-image MCI detection tasks.

**Abstract:** Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.

</details>


### [41] [Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning](https://arxiv.org/abs/2505.17068)

*Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdias, Carlos Eiras-Franco*

**Main category:** cs.CL

**Keywords:** user toxicity, health discussions, Collaborative Filtering, predictive modeling, COVID-19

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach to predict user toxicity in health-related online discussions, particularly focusing on COVID-related conversations on Reddit.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current techniques for managing online toxicity often involve reactive measures that can be ineffective. This work aims to provide a proactive solution to predict user interactions that may lead to toxic behavior in health discussions, improving user experience and community safety.

**Method:** The authors employed a Collaborative Filtering-based Machine Learning methodology to analyze interactions within COVID-related discussions on Reddit, predicting potential toxicity levels between users and subcommunities.

**Key Contributions:**

	1. Introduction of a predictive model for user toxicity in online health discussions.
	2. Utilization of Collaborative Filtering techniques for toxicity prediction.
	3. Demonstration of significant predictive performance in real-world COVID-related Reddit interactions.

**Result:** The proposed methodology achieved over 80% predictive performance in identifying toxic interactions, enabling the prevention of negative user pairings before conflicts arise.

**Limitations:** 

**Conclusion:** By shifting the focus from reactive to predictive methods for managing online toxicity, the study offers an innovative solution that could enhance the overall quality of health-related online discourse.

**Abstract:** In health-related topics, user toxicity in online discussions frequently becomes a source of social conflict or promotion of dangerous, unscientific behaviour; common approaches for battling it include different forms of detection, flagging and/or removal of existing toxic comments, which is often counterproductive for platforms and users alike. In this work, we propose the alternative of combatting user toxicity predictively, anticipating where a user could interact toxically in health-related online discussions. Applying a Collaborative Filtering-based Machine Learning methodology, we predict the toxicity in COVID-related conversations between any user and subcommunity of Reddit, surpassing 80% predictive performance in relevant metrics, and allowing us to prevent the pairing of conflicting users and subcommunities.

</details>


### [42] [Improving endpoint detection in end-to-end streaming ASR for conversational speech](https://arxiv.org/abs/2505.17070)

*Anandh C, Karthik Pandia Durai, Jeena Prakash, Manickavela Arumugam, Kadri Hacioglu, S. Pavankumar Dubagunta, Andreas Stolcke, Shankar Venkatesan, Aravind Ganapathiraju*

**Main category:** cs.CL

**Keywords:** ASR endpointing, transducer-based ASR, speech activity detection

**Relevance Score:** 7

**TL;DR:** This paper proposes methods to improve ASR endpointing by addressing delays in output and mistakes during endpointing, enhancing user experience in conversations involving human or artificial agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** ASR endpointing is crucial for user experience in conversations, and existing transducer-based ASR techniques face challenges with delayed emissions and inaccurate endpointing.

**Method:** The paper introduces an end-of-word token and a delay penalty to enhance endpointing accuracy and employs an auxiliary network for reliable frame-level speech activity detection.

**Key Contributions:**

	1. Introduction of an end-of-word token to reduce delay in ASR outputs.
	2. Implementation of a delay penalty mechanism to improve endpointing accuracy.
	3. Use of an auxiliary network for better speech activity detection.

**Result:** The proposed methods show improvements in endpointing accuracy and reduced perceived latency when evaluated on the Switchboard corpus.

**Limitations:** 

**Conclusion:** By addressing both delayed emission and endpointing mistakes, the methods enhance user experience in ASR applications.

**Abstract:** ASR endpointing (EP) plays a major role in delivering a good user experience in products supporting human or artificial agents in human-human/machine conversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR modelling technique preferred for streaming. A major limitation of T-ASR is delayed emission of ASR outputs, which could lead to errors or delays in EP. Inaccurate EP will cut the user off while speaking, returning incomplete transcript while delays in EP will increase the perceived latency, degrading the user experience. We propose methods to improve EP by addressing delayed emission along with EP mistakes. To address the delayed emission problem, we introduce an end-of-word token at the end of each word, along with a delay penalty. The EP delay is addressed by obtaining a reliable frame-level speech activity detection using an auxiliary network. We apply the proposed methods on Switchboard conversational speech corpus and evaluate it against a delay penalty method.

</details>


### [43] [What's in a prompt? Language models encode literary style in prompt embeddings](https://arxiv.org/abs/2505.17071)

*Raphal Sarfati, Haley Moller, Toni J. B. Liu, Nicolas Boull, Christopher Earls*

**Main category:** cs.CL

**Keywords:** language models, embeddings, literary analysis, authors, latent space

**Relevance Score:** 7

**TL;DR:** This study investigates how large language models compress and encode stylistic information from literary texts into their embeddings, revealing distinct geometrical relationships associated with authors' styles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the information processing capabilities of language models beyond mere factual encoding, focusing on stylistic and intangible aspects of text.

**Method:** The research involves examining excerpts from various novels and assessing how they are represented in the latent space of language models, particularly through transformer layers.

**Key Contributions:**

	1. Insights into the latent space representation of literary texts
	2. Demonstration of the entanglement of embeddings based on authorial style
	3. Implications for applications in authorship attribution

**Result:** Short excerpts from novels are found to separate in latent space based on stylistic features, with embeddings from the same author being more closely entangled compared to those from different authors.

**Limitations:** 

**Conclusion:** The findings highlight a sophisticated level of information processing and compression in language models, suggesting potential applications in authorship attribution and literary analysis.

**Abstract:** Large language models use high-dimensional latent spaces to encode and process textual information. Much work has investigated how the conceptual content of words translates into geometrical relationships between their vector representations. Fewer studies analyze how the cumulative information of an entire prompt becomes condensed into individual embeddings under the action of transformer layers. We use literary pieces to show that information about intangible, rather than factual, aspects of the prompt are contained in deep representations. We observe that short excerpts (10 - 100 tokens) from different novels separate in the latent space independently from what next-token prediction they converge towards. Ensembles from books from the same authors are much more entangled than across authors, suggesting that embeddings encode stylistic features. This geometry of style may have applications for authorship attribution and literary analysis, but most importantly reveals the sophistication of information processing and compression accomplished by language models.

</details>


### [44] [Mechanistic Interpretability of GPT-like Models on Summarization Tasks](https://arxiv.org/abs/2505.17073)

*Anurag Mishra*

**Main category:** cs.CL

**Keywords:** mechanistic interpretability, GPT-like models, summarization, attention patterns, LoRA adaptation

**Relevance Score:** 9

**TL;DR:** This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks, identifying a 'summarization circuit' in the model architecture.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reveal the inner workings of large language models, specifically focusing on their performance in summarization tasks.

**Method:** The authors conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations, and identifying the summarization circuit within the model architecture.

**Key Contributions:**

	1. Development of an interpretability framework for summarization tasks
	2. Identification of specific layers and attention heads that transform during summarization
	3. Demonstration of performance improvements using targeted LoRA adaptation

**Result:** Significant changes in attention patterns were observed in middle layers, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. Targeted LoRA adaptation led to improved performance over standard LoRA fine-tuning.

**Limitations:** 

**Conclusion:** This work provides insights into neural networks' information selection and compression during summarization, bridging the gap between black-box evaluation and mechanistic understanding.

**Abstract:** Mechanistic interpretability research seeks to reveal the inner workings of large language models, yet most work focuses on classification or generative tasks rather than summarization. This paper presents an interpretability framework for analyzing how GPT-like models adapt to summarization tasks. We conduct differential analysis between pre-trained and fine-tuned models, quantifying changes in attention patterns and internal activations. By identifying specific layers and attention heads that undergo significant transformation, we locate the "summarization circuit" within the model architecture. Our findings reveal that middle layers (particularly 2, 3, and 5) exhibit the most dramatic changes, with 62% of attention heads showing decreased entropy, indicating a shift toward focused information selection. We demonstrate that targeted LoRA adaptation of these identified circuits achieves significant performance improvement over standard LoRA fine-tuning while requiring fewer training epochs. This work bridges the gap between black-box evaluation and mechanistic understanding, providing insights into how neural networks perform information selection and compression during summarization.

</details>


### [45] [Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency](https://arxiv.org/abs/2505.17074)

*Ruixiao Li, Fahao Chen, Peng Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, speculative decoding, request scheduling, inference latency, adaptive scheduling

**Relevance Score:** 8

**TL;DR:** This paper proposes LAPS-SD, a semi-clairvoyant request scheduling algorithm for speculative decoding that reduces inference latency by about 39%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Efficiently scheduling inference requests for Large Language Models (LLMs) is challenging due to uncertain execution times influenced by output length and token acceptance rates.

**Method:** The LAPS-SD algorithm employs multiple priority queues and allows for request execution preemption, adapting to dynamic token acceptance rates during decoding.

**Key Contributions:**

	1. Development of a semi-clairvoyant request scheduling algorithm called LAPS-SD.
	2. Introduction of adaptive scheduling based on request features during decoding.
	3. Demonstration of a significant reduction in inference latency (39%) compared to state-of-the-art methods.

**Result:** Extensive experiments show that LAPS-SD reduces average inference latency by approximately 39% compared to existing scheduling methods.

**Limitations:** 

**Conclusion:** LAPS-SD improves inference latency management for LLMs by adaptively scheduling requests based on their characteristics, achieving greater efficiency.

**Abstract:** Speculative decoding accelerates Large Language Model (LLM) inference by employing a small speculative model (SSM) to generate multiple candidate tokens and verify them using the LLM in parallel. This technique has been widely integrated into LLM inference serving systems. However, inference requests typically exhibit uncertain execution time, which poses a significant challenge of efficiently scheduling requests in these systems. Existing work estimates execution time based solely on predicted output length, which could be inaccurate because execution time depends on both output length and token acceptance rate of verification by the LLM. In this paper, we propose a semi-clairvoyant request scheduling algorithm called Least-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a number of inference requests, LAPS-SD can effectively minimize average inference latency by adaptively scheduling requests according to their features during decoding. When the token acceptance rate is dynamic and execution time is difficult to estimate, LAPS-SD maintains multiple priority queues and allows request execution preemption across different queues. Once the token acceptance rate becomes stable, LAPS-SD can accurately estimate the execution time and schedule requests accordingly. Extensive experiments show that LAPS-SD reduces inference latency by approximately 39\% compared to state-of-the-art scheduling methods.

</details>


### [46] [Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems](https://arxiv.org/abs/2505.17075)

*Fuma Kurata, Mao Saeki, Masaki Eguchi, Shungo Suzuki, Hiroaki Takatsu, Yoichi Matsuyama*

**Main category:** cs.CL

**Keywords:** engagement, rapport, dialogue systems, foreign language learning, user experience

**Relevance Score:** 7

**TL;DR:** The study develops and validates scales for measuring user engagement and rapport in multimodal dialogue systems for language learning, highlighting differences in experiences with human tutors versus dialogue agents.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the user experience quality with multimodal dialogue systems in foreign language learning.

**Method:** Seventy-four Japanese learners of English completed role-play and discussion tasks with human tutors and a dialogue agent, followed by responding to scales designed to measure engagement and rapport.

**Key Contributions:**

	1. Development of engagement and rapport scales for multimodal dialogue systems
	2. Validation of the scales through reliability and factor analyses
	3. Comparative analysis of user experience between human and agent interactions

**Result:** The scales showed valid and reliable measures of user experience, effectively capturing differences between interactions with human tutors and a dialogue agent.

**Limitations:** 

**Conclusion:** The developed scales can effectively assess the quality of user experiences in multimodal dialogue systems, providing valuable insights for improving language learning tools.

**Abstract:** This study aimed to develop and validate two scales of engagement and rapport to evaluate the user experience quality with multimodal dialogue systems in the context of foreign language learning. The scales were designed based on theories of engagement in educational psychology, social psychology, and second language acquisition.Seventy-four Japanese learners of English completed roleplay and discussion tasks with trained human tutors and a dialog agent. After each dialogic task was completed, they responded to the scales of engagement and rapport. The validity and reliability of the scales were investigated through two analyses. We first conducted analysis of Cronbach's alpha coefficient and a series of confirmatory factor analyses to test the structural validity of the scales and the reliability of our designed items. We then compared the scores of engagement and rapport between the dialogue with human tutors and the one with a dialogue agent. The results revealed that our scales succeeded in capturing the difference in the dialogue experience quality between the human interlocutors and the dialogue agent from multiple perspectives.

</details>


### [47] [Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English](https://arxiv.org/abs/2505.17076)

*Haoyang Zhang, Hexin Liu, Xiangyu Zhang, Qiquan Zhang, Yuchen Hu, Junqi Zhao, Fei Tian, Xuerui Yang, Eng Siong Chng*

**Main category:** cs.CL

**Keywords:** speech tokenizer, frame rate, speech recognition, Mandarin, English

**Relevance Score:** 6

**TL;DR:** This study investigates the impact of varying frame rates on speech tokenization in Mandarin and English, revealing significant language-specific differences affecting speech recognition tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underexplored impact of frame rates on speech tokenization and its implications for speech tasks.

**Method:** The study examines speech at different frame rates for Mandarin and English, evaluating the resulting semantic tokens in a speech recognition task.

**Key Contributions:**

	1. Analysis of frame rate effects on speech tokenization for Mandarin and English
	2. Insights into language-specific acoustic feature interactions
	3. Recommendations for optimizing frame rate selection in speech applications

**Result:** Frame rate variations influence speech tokenization differently across languages, emphasizing the relationship between frame rates, phonetic density, and language-specific acoustic features.

**Limitations:** 

**Conclusion:** Optimizing frame rate selection for speech tokenizers can enhance performance in automatic speech recognition, text-to-speech, and other applications.

**Abstract:** The speech tokenizer plays a crucial role in recent speech tasks, generally serving as a bridge between speech signals and language models. While low-frame-rate codecs are widely employed as speech tokenizers, the impact of frame rates on speech tokens remains underexplored. In this study, we investigate how varying frame rates affect speech tokenization by examining Mandarin and English, two typologically distinct languages. We encode speech at different frame rates and evaluate the resulting semantic tokens in the speech recognition task. Our findings reveal that frame rate variations influence speech tokenization differently for each language, highlighting the interplay between frame rates, phonetic density, and language-specific acoustic features. The results provide insights into optimizing frame rate selection for speech tokenizers, with implications for automatic speech recognition, text-to-speech, and other speech-related applications.

</details>


### [48] [GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace](https://arxiv.org/abs/2505.17078)

*Zenghao Duan, Zhiyi Yin, Zhichao Shi, Liang Pang, Shaoling Jing, Jiayi Wu, Yu Yan, Huawei Shen, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, toxicity, detoxification, GloSS, human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper presents GloSS, a novel method for detoxifying Large Language Models by removing global toxic subspaces from feed-forward network parameters without large-scale data or retraining.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve the understanding and mitigation of toxicity in Large Language Models, which is crucial for their safe deployment in applications.

**Method:** The proposed GloSS method consists of a four-stage process to identify and suppress global toxic subspaces in the model parameters of FFNs.

**Key Contributions:**

	1. Introduction of GloSS, a novel detoxification method for LLMs.
	2. Empirical evidence showing the effectiveness of GloSS across various LLMs.
	3. Highlighting the significance of global toxic subspaces in the context of model toxicity.

**Result:** Experiments demonstrate that GloSS achieves state-of-the-art detoxification performance while maintaining the overall capabilities of the models.

**Limitations:** The method may not address toxicity in all forms or contexts beyond the focus of the study.

**Conclusion:** GloSS provides an effective approach to mitigate toxicity in LLMs, emphasizing the importance of addressing global toxic subspaces rather than just layer-wise analyses.

**Abstract:** This paper investigates the underlying mechanisms of toxicity generation in Large Language Models (LLMs) and proposes an effective detoxification approach. Prior work typically considers the Feed-Forward Network (FFN) as the main source of toxicity, representing toxic regions as a set of toxic vectors or layer-wise subspaces. However, our in-depth analysis reveals that the global toxic subspace offers a more effective and comprehensive representation of toxic region within the model. Building on this insight, we propose GloSS (Global Toxic Subspace Suppression), a lightweight, four-stage method that mitigates toxicity by identifying and removing the global toxic subspace from the parameters of FFN. Experiments across a range of LLMs show that GloSS achieves state-of-the-art detoxification performance while preserving the models general capabilities, without requiring large-scale data or model retraining.

</details>


### [49] [Not Minds, but Signs: Reframing LLMs through Semiotics](https://arxiv.org/abs/2505.17080)

*Davide Picca*

**Main category:** cs.CL

**Keywords:** Large Language Models, semiotics, cognitive systems, cultural processes, interpretation

**Relevance Score:** 6

**TL;DR:** The paper argues for a semiotic perspective on Large Language Models (LLMs), suggesting they function as agents in the manipulation of signs rather than as cognitive systems or simulating human thought, thereby fostering a nuanced understanding of their role in cultural processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the cognitive framing of LLMs and offer a more accurate understanding of their functions within cultural dynamics.

**Method:** The paper employs theoretical analysis combined with practical examples to illustrate the semiotic role of LLMs.

**Key Contributions:**

	1. Reconceptualizes LLMs as semiotic agents rather than cognitive systems
	2. Highlights ethical considerations in the application of LLMs
	3. Explores the implications of LLMs in various fields such as literature and education.

**Result:** Demonstrates how LLMs can be viewed as semiotic agents that generate textual outputs, encouraging interpretation and contextual negotiation.

**Limitations:** 

**Conclusion:** The semiotic perspective emphasizes the contingent nature of meaning, reframing LLMs as participants in an ecology of signs that influence reading, writing, and knowledge production.

**Abstract:** This paper challenges the prevailing tendency to frame Large Language Models (LLMs) as cognitive systems, arguing instead for a semiotic perspective that situates these models within the broader dynamics of sign manipulation and meaning-making. Rather than assuming that LLMs understand language or simulate human thought, we propose that their primary function is to recombine, recontextualize, and circulate linguistic forms based on probabilistic associations. By shifting from a cognitivist to a semiotic framework, we avoid anthropomorphism and gain a more precise understanding of how LLMs participate in cultural processes, not by thinking, but by generating texts that invite interpretation. Through theoretical analysis and practical examples, the paper demonstrates how LLMs function as semiotic agents whose outputs can be treated as interpretive acts, open to contextual negotiation and critical reflection. We explore applications in literature, philosophy, education, and cultural production, emphasizing how LLMs can serve as tools for creativity, dialogue, and critical inquiry. The semiotic paradigm foregrounds the situated, contingent, and socially embedded nature of meaning, offering a more rigorous and ethically aware framework for studying and using LLMs. Ultimately, this approach reframes LLMs as technological participants in an ongoing ecology of signs. They do not possess minds, but they alter how we read, write, and make meaning, compelling us to reconsider the foundations of language, interpretation, and the role of artificial systems in the production of knowledge.

</details>


### [50] [GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data](https://arxiv.org/abs/2505.17082)

*Abderrahman Skiredj, Ferdaous Azhari, Houdaifa Atou, Nouamane Tazi, Ismail Berrada*

**Main category:** cs.CL

**Keywords:** Moroccan Arabic, large language models, Darija, cross-lingual reasoning, Green AI

**Relevance Score:** 8

**TL;DR:** This paper presents a strategy for improving the performance of large language models (LLMs) on Moroccan Arabic (Darija) while maintaining their cross-lingual reasoning abilities. Through a quality-over-quantity approach, the authors translate instruction suites into Darija and demonstrate the effectiveness of their tuned models, culminating in the release of a Darija-centric language model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the marginalization of Moroccan Arabic in open-source large language models and improve their usability without sacrificing reasoning skills.

**Method:** The authors employ a rigorous alignment strategy focusing on quality over quantity, translating instruction suites into Darija and adapting models using LoRA-tuning techniques.

**Key Contributions:**

	1. Introduced a quality-over-quantity alignment strategy for Darija
	2. Developed Darija-centric language models with strong reasoning capabilities
	3. Released resources for fostering Darija applications in various sectors

**Result:** The DarijaMMLU score improved significantly, with the model achieving 42.7, and further enhancements led to scores of 61.6 on DarijaMMLU and 60.5 on HellaSwag, outperforming existing models without degrading English performance.

**Limitations:** 

**Conclusion:** The work illustrates a sustainable, efficient pathway for developing inclusive language technologies, resulting in the release of a model that supports Darija in educational and public service applications.

**Abstract:** Open-source large language models (LLMs) still marginalise Moroccan Arabic (Darija), forcing practitioners either to bolt on heavyweight Arabic adapters or to sacrifice the very reasoning skills that make LLMs useful. We show that a rigorously quality-over-quantity alignment strategy can surface fluent Darija while safeguarding the backbone s cross-lingual reasoning at a sliver of the usual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6 K and TULU 50 K into Darija, preserve 20 of the English originals, and add mathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on 5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the reasoning-dense TULU portion pushes it to 47.5 with no English regression. Scaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which matches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense, scoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc retains Gemma-27B s strong maths and general-reasoning ability, showing only minimal movement on GSM8K and English benchmarks. The entire model is trained in just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable language technology. We release code, data and checkpoints to spur Darija-centric applications in education, public services and everyday digital interaction.

</details>


### [51] [Scale-invariant Attention](https://arxiv.org/abs/2505.17083)

*Ben Anson, Xi Wang, Laurence Aitchison*

**Main category:** cs.CL

**Keywords:** LLM, attention mechanisms, long-context retrieval

**Relevance Score:** 8

**TL;DR:** The paper introduces a scale-invariant attention mechanism that improves long context generalization in LLMs by transforming attention logits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the difficulty of LLMs in generalizing from short training contexts to longer inference contexts.

**Method:** The authors propose conditions for effective long context attention, demonstrating that a position-dependent transformation of attention logits meets these conditions.

**Key Contributions:**

	1. Introduction of scale-invariant total attention and attention sparsity conditions
	2. Position-dependent transformation of attention logits
	3. Experimental validation showing improved long-context performance

**Result:** The proposed scale-invariant attention scheme significantly reduces validation loss when zero-shot generalizing from short to long contexts and enhances long-context retrieval performance.

**Limitations:** 

**Conclusion:** The study suggests that scale-invariant attention can effectively bridge the gap in LLM performance between short and long contexts.

**Abstract:** One persistent challenge in LLM research is the development of attention mechanisms that are able to generalise from training on shorter contexts to inference on longer contexts. We propose two conditions that we expect all effective long context attention mechanisms to have: scale-invariant total attention, and scale-invariant attention sparsity. Under a Gaussian assumption, we show that a simple position-dependent transformation of the attention logits is sufficient for these conditions to hold. Experimentally we find that the resulting scale-invariant attention scheme gives considerable benefits in terms of validation loss when zero-shot generalising from training on short contexts to validation on longer contexts, and is effective at long-context retrieval.

</details>


### [52] [Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization](https://arxiv.org/abs/2505.17086)

*Yihong Wu, Liheng Ma, Muzhi Li, Jiaming Zhou, Jianye Hao, Ho-fung Leung, Irwin King, Yingxue Zhang, Jian-Yun Nie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-hop Question Answering, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** The paper proposes Mujica, a framework for Multi-hop Question Answering that enhances performance using a planner and the MyGO reinforcement learning method, leading to efficient and scalable solutions for LLMs in QA tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models struggle with factual knowledge and work on Question Answering due to hallucinations, prompting the need for improved methods.

**Method:** Mujica consists of a planner that decomposes questions into a directed acyclic graph of subquestions and a worker that resolves questions through retrieval and reasoning. Additionally, the MyGO reinforcement learning method is introduced to optimize training using Maximum Likelihood Estimation instead of traditional gradient updates.

**Key Contributions:**

	1. Introduction of Mujica framework for enhanced multi-hop question answering
	2. Development of MyGO, a novel reinforcement learning optimization method
	3. Demonstration of improved performance across multiple datasets

**Result:** Empirical results show that Mujica-MyGO significantly improves multi-hop QA performance across various datasets for multiple LLMs.

**Limitations:** 

**Conclusion:** Mujica-MyGO offers a scalable and resource-efficient approach for tackling complex Question Answering tasks with large language models.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable versatility, due to the lack of factual knowledge, their application to Question Answering (QA) tasks remains hindered by hallucination.   While Retrieval-Augmented Generation mitigates these issues by integrating external knowledge, existing approaches rely heavily on in-context learning, whose performance is constrained by the fundamental reasoning capabilities of LLMs.   In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex Question Answering, comprising a planner that decomposes questions into a directed acyclic graph of subquestions and a worker that resolves questions via retrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy Gradient Optimization), a novel reinforcement learning method that replaces traditional policy gradient updates with Maximum Likelihood Estimation (MLE) by sampling trajectories from an asymptotically optimal policy. MyGO eliminates the need for gradient rescaling and reference models, ensuring stable and efficient training.   Empirical results across multiple datasets demonstrate the effectiveness of Mujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a scalable and resource-efficient solution for complex QA tasks.

</details>


### [53] [Informatics for Food Processing](https://arxiv.org/abs/2505.17087)

*Gordana Ispirova, Michael Sebek, Giulia Menichetti*

**Main category:** cs.CL

**Keywords:** food processing, machine learning, artificial intelligence, food informatics, public health

**Relevance Score:** 7

**TL;DR:** This chapter discusses the impact of machine learning and AI on food processing classification and health implications, featuring novel computational methods like FoodProX and multimodal AI models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how machine learning and AI can enhance food processing classification and its health implications, addressing the challenges of traditional classification frameworks.

**Method:** Presents historical overviews and critiques of classification frameworks, introduces FoodProX (a random forest model) and discusses the use of large language models like BERT and BioBERT for predictive tasks.

**Key Contributions:**

	1. Introduction of FoodProX model for processing level inference
	2. Application of large language models for food data
	3. Case study using Open Food Facts database to classify food effectively

**Result:** Demonstrates that novel computational approaches can improve the classification of food processing levels and have meaningful implications for public health.

**Limitations:** Challenges include the subjectivity and reproducibility of traditional classification methods affecting research and policy.

**Conclusion:** New computational methods and multimodal AI models can significantly advance food informatics and improve public health assessments.

**Abstract:** This chapter explores the evolution, classification, and health implications of food processing, while emphasizing the transformative role of machine learning, artificial intelligence (AI), and data science in advancing food informatics. It begins with a historical overview and a critical review of traditional classification frameworks such as NOVA, Nutri-Score, and SIGA, highlighting their strengths and limitations, particularly the subjectivity and reproducibility challenges that hinder epidemiological research and public policy. To address these issues, the chapter presents novel computational approaches, including FoodProX, a random forest model trained on nutrient composition data to infer processing levels and generate a continuous FPro score. It also explores how large language models like BERT and BioBERT can semantically embed food descriptions and ingredient lists for predictive tasks, even in the presence of missing data. A key contribution of the chapter is a novel case study using the Open Food Facts database, showcasing how multimodal AI models can integrate structured and unstructured data to classify foods at scale, offering a new paradigm for food processing assessment in public health and research.

</details>


### [54] [Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models](https://arxiv.org/abs/2505.17089)

*Md Rafi Ur Rashid, Vishnu Asutosh Dasu, Ye Wang, Gang Tan, Shagufta Mehnaz*

**Main category:** cs.CL

**Keywords:** Large Language Models, Adversarial Attacks, Human-AI Interaction, Robustness, Chain-of-Thought

**Relevance Score:** 8

**TL;DR:** This paper presents Adversarial Scenario Extrapolation (ASE), a framework that enhances LLM robustness against adversarial attacks while maintaining user experience.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing safety risks of LLMs, including jailbreaks and toxicity, which current defenses inadequately handle.

**Method:** ASE employs Chain-of-Thought reasoning and guides LLMs through a self-generative process to develop defensive strategies against potential adversarial scenarios during inference.

**Key Contributions:**

	1. Introduction of the ASE framework for LLMs
	2. Demonstrated significant improvements in defense against multiple adversarial attacks
	3. Set new standards for robustness and user experience in LLM applications

**Result:** ASE achieves near-zero jailbreak attack success rates, minimal toxicity, and significantly low outright rejections, outperforming existing defenses.

**Limitations:** 

**Conclusion:** ASE reshapes adversarial perception in LLMs, establishing a new standard for secure human-AI interaction.

**Abstract:** Large Language Models (LLMs) exhibit impressive capabilities, but remain susceptible to a growing spectrum of safety risks, including jailbreaks, toxic content, hallucinations, and bias. Existing defenses often address only a single threat type or resort to rigid outright rejection, sacrificing user experience and failing to generalize across diverse and novel attacks. This paper introduces Adversarial Scenario Extrapolation (ASE), a novel inference-time computation framework that leverages Chain-of-Thought (CoT) reasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides the LLM through a self-generative process of contemplating potential adversarial scenarios and formulating defensive strategies before generating a response to the user query. Comprehensive evaluation on four adversarial benchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak attack success rates and minimal toxicity, while slashing outright rejections to <4%. ASE outperforms six state-of-the-art defenses in robustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and 4-10x lower bias scores. By transforming adversarial perception into an intrinsic cognitive process, ASE sets a new paradigm for secure and natural human-AI interaction.

</details>


### [55] [Large Language Models Implicitly Learn to See and Hear Just By Reading](https://arxiv.org/abs/2505.17091)

*Prateek Verma, Mert Pilanci*

**Main category:** cs.CL

**Keywords:** LLM, audio classification, image classification, multi-modal learning, machine learning

**Relevance Score:** 9

**TL;DR:** The paper explores how a text-based LLM can internally develop capabilities to understand images and audio by training on text tokens, allowing for effective classification of these modalities without starting from scratch.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To demonstrate that text LLMs can generalize their learned representations to classify audio and visual data, offering a more efficient approach to model training.

**Method:** The architecture of the model takes in patches of images and audio waveforms as inputs, producing embeddings or category labels similar to traditional classification pipelines.

**Key Contributions:**

	1. Introduction of a novel architecture merging text LLMs with audio and image classification.
	2. Demonstration of effective classification across multiple datasets using a single model.
	3. Reduction in the need for training separate models for different modalities.

**Result:** The proposed model showed effectiveness in audio classification for datasets like FSD-50K and GTZAN, as well as in image classification tasks for CIFAR-10 and Fashion-MNIST.

**Limitations:** The study is still under review and may face limitations related to generalizability across different types of data and potential biases in datasets.

**Conclusion:** This work rediscovers the potential of text-based LLMs to provide insights into audio and visual data classification, suggesting a novel pathway for multi-modal learning without extensive retraining.

**Abstract:** This paper presents a fascinating find: By training an auto-regressive LLM model on text tokens, the text model inherently develops internally an ability to understand images and audio, thereby developing the ability to see and hear just by reading. Popular audio and visual LLM models fine-tune text LLM models to give text output conditioned on images and audio embeddings. On the other hand, our architecture takes in patches of images, audio waveforms or tokens as input. It gives us the embeddings or category labels typical of a classification pipeline. We show the generality of text weights in aiding audio classification for datasets FSD-50K and GTZAN. Further, we show this working for image classification on CIFAR-10 and Fashion-MNIST, as well on image patches. This pushes the notion of text-LLMs learning powerful internal circuits that can be utilized by activating necessary connections for various applications rather than training models from scratch every single time.

</details>


### [56] [Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation](https://arxiv.org/abs/2505.17095)

*Kristine Ann M. Carandang, Jasper Meynard P. Araa, Ethan Robert A. Casin, Christopher P. Monterola, Daniel Stanley Y. Tan, Jesus Felix B. Valenzuela, Christian M. Alis*

**Main category:** cs.CL

**Keywords:** clinical note generation, LLMs, data privacy, semantic consistency, healthcare providers

**Relevance Score:** 9

**TL;DR:** This study evaluates the reliability of 12 LLMs for clinical note generation, finding that they produce semantically consistent notes, with Meta's Llama 70B being the most reliable.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the legal and ethical responsibilities of healthcare providers regarding accurate documentation and patient data privacy, and the challenges posed by the variability in LLM responses.

**Method:** The reliability of 12 open-weight and proprietary LLMs is evaluated for clinical note generation based on consistency rate, semantic consistency, and semantic similarity across iterations with the same prompt.

**Key Contributions:**

	1. Evaluation of multiple LLMs for clinical note generation reliability
	2. Findings on semantic consistency across LLM outputs
	3. Recommendations for using specific LLMs to improve clinical documentation

**Result:** The study finds that LLMs from all families are stable and semantically consistent, with most generating notes similar to those made by experts, particularly highlighting that Meta's Llama 70B was the most reliable model.

**Limitations:** 

**Conclusion:** The paper recommends local deployment of smaller open-weight models for clinical note generation to enhance compliance with data privacy regulations and improve healthcare providers' documentation efficiency.

**Abstract:** Due to the legal and ethical responsibilities of healthcare providers (HCPs) for accurate documentation and protection of patient data privacy, the natural variability in the responses of large language models (LLMs) presents challenges for incorporating clinical note generation (CNG) systems, driven by LLMs, into real-world clinical processes. The complexity is further amplified by the detailed nature of texts in CNG. To enhance the confidence of HCPs in tools powered by LLMs, this study evaluates the reliability of 12 open-weight and proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms of their ability to generate notes that are string equivalent (consistency rate), have the same meaning (semantic consistency) and are correct (semantic similarity), across several iterations using the same prompt. The results show that (1) LLMs from all model families are stable, such that their responses are semantically consistent despite being written in various ways, and (2) most of the LLMs generated notes close to the corresponding notes made by experts. Overall, Meta's Llama 70B was the most reliable, followed by Mistral's Small model. With these findings, we recommend the local deployment of these relatively smaller open-weight models for CNG to ensure compliance with data privacy regulations, as well as to improve the efficiency of HCPs in clinical documentation.

</details>


### [57] [TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration](https://arxiv.org/abs/2505.17098)

*Yanshu Li, Tian Yun, Jianjiang Yang, Pinyuan Feng, Jinfa Huang, Ruixiang Tang*

**Main category:** cs.CL

**Keywords:** multimodal learning, large vision-language models, in-context learning, task-aware attention, task mapping

**Relevance Score:** 7

**TL;DR:** The paper introduces TACO, a transformer-based model that enhances multimodal in-context learning by utilizing task-aware attention to improve reasoning in large vision-language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and improve the effectiveness of multimodal in-context learning (ICL) in large vision-language models (LVLMs), particularly how input sequences affect reasoning in tasks requiring complex reasoning or generation.

**Method:** The authors interpret multimodal ICL using task mapping, which helps to understand local and global relationships in demonstration sequences. TACO is developed as a lightweight transformer model with task-aware attention for dynamic in-context sequence configuration during inference.

**Key Contributions:**

	1. Introduction of TACO, a new model for improving ICL in LVLMs.
	2. Systematic interpretation of multimodal ICL through task mapping.
	3. Demonstration of TACO's superiority over existing baselines across multiple datasets.

**Result:** Experiments show that TACO outperforms baseline models across various ICL tasks on five different LVLMs using nine datasets, demonstrating its effectiveness in improving task reasoning and sequence construction.

**Limitations:** The study is based on the analysis of task mapping and its implications, which may not cover all aspects of ICL effectiveness validation.

**Conclusion:** The study positions task mapping as a significant method for interpreting and enhancing multimodal ICL, suggesting that TACO effectively bridges the gap between input sequences and model reasoning.

**Abstract:** Multimodal in-context learning (ICL) has emerged as a key mechanism for harnessing the capabilities of large vision-language models (LVLMs). However, its effectiveness remains highly sensitive to the quality of input in-context sequences, particularly for tasks involving complex reasoning or open-ended generation. A major limitation is our limited understanding of how LVLMs actually exploit these sequences during inference. To bridge this gap, we systematically interpret multimodal ICL through the lens of task mapping, which reveals how local and global relationships within and among demonstrations guide model reasoning. Building on this insight, we present TACO, a lightweight transformer-based model equipped with task-aware attention that dynamically configures in-context sequences. By injecting task-mapping signals into the autoregressive decoding process, TACO creates a bidirectional synergy between sequence construction and task reasoning. Experiments on five LVLMs and nine datasets demonstrate that TACO consistently surpasses baselines across diverse ICL tasks. These results position task mapping as a valuable perspective for interpreting and improving multimodal ICL.

</details>


### [58] [Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation](https://arxiv.org/abs/2505.17099)

*Xiaozhao Liu, Dinggang Shen, Xihui Liu*

**Main category:** cs.CL

**Keywords:** EEG, text generation, generative models, brain decoding, semantic summarization

**Relevance Score:** 8

**TL;DR:** This paper presents the Generative Language Inspection Model (GLIM) for EEG-to-text decoding, addressing hallucination issues and enhancing semantic grounding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of EEG-to-text decoding, particularly in addressing the hallucination issue seen in generative models.

**Method:** The paper proposes GLIM, which focuses on semantic summarization of EEG representations rather than verbatim reconstruction, improving grounding with small-scale data.

**Key Contributions:**

	1. Introduction of the Generative Language Inspection Model (GLIM) for EEG-to-text decoding.
	2. Reframing the decoding task as semantic summarization rather than verbatim reconstruction.
	3. Development of robust evaluation metrics for generative brain decoding.

**Result:** Experiments on the ZuCo dataset show that GLIM generates fluent, EEG-grounded sentences and excels in robust evaluations beyond text similarity.

**Limitations:** 

**Conclusion:** GLIM presents new protocols for reliable benchmarking in generative brain decoding, offering a novel approach to understanding brain signals through text.

**Abstract:** Pretrained generative models have opened new frontiers in brain decoding by enabling the synthesis of realistic texts and images from non-invasive brain recordings. However, the reliability of such outputs remains questionable--whether they truly reflect semantic activation in the brain, or are merely hallucinated by the powerful generative models. In this paper, we focus on EEG-to-text decoding and address its hallucination issue through the lens of posterior collapse. Acknowledging the underlying mismatch in information capacity between EEG and text, we reframe the decoding task as semantic summarization of core meanings rather than previously verbatim reconstruction of stimulus texts. To this end, we propose the Generative Language Inspection Model (GLIM), which emphasizes learning informative and interpretable EEG representations to improve semantic grounding under heterogeneous and small-scale data conditions. Experiments on the public ZuCo dataset demonstrate that GLIM consistently generates fluent, EEG-grounded sentences without teacher forcing. Moreover, it supports more robust evaluation beyond text similarity, through EEG-text retrieval and zero-shot semantic classification across sentiment categories, relation types, and corpus topics. Together, our architecture and evaluation protocols lay the foundation for reliable and scalable benchmarking in generative brain decoding.

</details>


### [59] [Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector](https://arxiv.org/abs/2505.17100)

*Haoyan Yang, Runxue Bao, Cao Xiao, Jun Ma, Parminder Bhatia, Shangqian Gao, Taha Kass-Hout*

**Main category:** cs.CL

**Keywords:** LLM, bias detection, self-correction, evaluation, reasoning

**Relevance Score:** 9

**TL;DR:** RBD is a module that detects biases in LLM evaluations and helps correct them, significantly improving evaluation accuracy and consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLM-as-a-Judge is a promising tool but suffers from biases in judgment that existing solutions fail to adequately mitigate.

**Method:** Introduces the Reasoning-based Bias Detector (RBD) as an external module that identifies biased evaluations and provides reasoning for self-correction. The RBD pipeline includes biased dataset construction, supervision collection, model fine-tuning, and integration with LLM evaluators.

**Key Contributions:**

	1. Development of the Reasoning-based Bias Detector (RBD)
	2. Demonstration of significant performance improvements in evaluation accuracy and consistency
	3. Design of a comprehensive pipeline for bias detection and correction

**Result:** RBD models show consistent improvements across evaluation accuracy (avg. 18.5%) and consistency (avg. 10.9%) over various bias types using multiple LLM evaluators.

**Limitations:** 

**Conclusion:** RBD's effectiveness and scalability are proven through experiments, demonstrating strong generalization across biases and domains.

**Abstract:** LLM-as-a-Judge has emerged as a promising tool for automatically evaluating generated outputs, but its reliability is often undermined by potential biases in judgment. Existing efforts to mitigate these biases face key limitations: in-context learning-based methods fail to address rooted biases due to the evaluator's limited capacity for self-reflection, whereas fine-tuning is not applicable to all evaluator types, especially closed-source models. To address this challenge, we introduce the Reasoning-based Bias Detector (RBD), which is a plug-in module that identifies biased evaluations and generates structured reasoning to guide evaluator self-correction. Rather than modifying the evaluator itself, RBD operates externally and engages in an iterative process of bias detection and feedback-driven revision. To support its development, we design a complete pipeline consisting of biased dataset construction, supervision collection, distilled reasoning-based fine-tuning of RBD, and integration with LLM evaluators. We fine-tune four sizes of RBD models, ranging from 1.5B to 14B, and observe consistent performance improvements across all scales. Experimental results on 4 bias types--verbosity, position, bandwagon, and sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong effectiveness. For example, the RBD-8B model improves evaluation accuracy by an average of 18.5% and consistency by 10.9%, and surpasses prompting-based baselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results highlight RBD's effectiveness and scalability. Additional experiments further demonstrate its strong generalization across biases and domains, as well as its efficiency.

</details>


### [60] [An approach to identify the most semantically informative deep representations of text and images](https://arxiv.org/abs/2505.17101)

*Santiago Acevedo, Andrea Mascaretti, Riccardo Rende, Mato Mahaut, Marco Baroni, Alessandro Laio*

**Main category:** cs.CL

**Keywords:** neural networks, semantic representation, large language models, vision transformers, information content

**Relevance Score:** 9

**TL;DR:** The paper investigates how deep neural networks represent semantically related data across different domains, focusing on LLMs and vision transformers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how semantic information is encoded in representations of semantically related data and how it varies across different model sizes and architectures.

**Method:** The authors analyze the relative information content of representations in LLMs and vision transformers, particularly examining layers that contain transferable semantic information.

**Key Contributions:**

	1. Quantitative method for investigating semantic representation in LLMs and vision models.
	2. Identification of semantic layers with transferable information.
	3. Discovery of causal asymmetries and information content spread across tokens in LLMs.

**Result:** Larger LLMs extract more general information, and semantic information in these layers shows long-distance correlations and causal asymmetries. There are significant model-dependent information asymmetries between image and text representations.

**Limitations:** Findings are model-dependent; other architectures may not exhibit the same properties observed.

**Conclusion:** Semantic layers in LLMs not only predict visual representations but also show varying degrees of information correlation based on model size.

**Abstract:** Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.

</details>


### [61] [BanglaByT5: Byte-Level Modelling for Bangla](https://arxiv.org/abs/2505.17102)

*Pramit Bhattacharyya, Arnab Bhattacharya*

**Main category:** cs.CL

**Keywords:** Bangla, NLP, Byte-level modeling, LLM, ByT5

**Relevance Score:** 4

**TL;DR:** BanglaByT5 is a byte-level encoder-decoder model designed for Bangla, outperforming larger multilingual models in NLP tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional tokenizers in capturing the nuances of morphologically rich languages like Bangla.

**Method:** BanglaByT5 is built on a small variant of Googles ByT5 architecture and pre-trained on a large curated corpus of Bangla texts.

**Key Contributions:**

	1. Introduction of BanglaByT5 as a specialized model for Bangla
	2. Demonstration of byte-level modeling benefits for morphologically rich languages
	3. Competitive performance in generative and classification tasks compared to larger models.

**Result:** BanglaByT5 showed competitive performance in zero-shot and supervised evaluations, exceeding capabilities of various multilingual models.

**Limitations:** 

**Conclusion:** The study illustrates the effectiveness of byte-level modeling for Bangla and proposes BanglaByT5 as a robust tool for NLP in resource-constrained settings.

**Abstract:** Large language models (LLMs) have achieved remarkable success across various natural language processing tasks. However, most LLM models use traditional tokenizers like BPE and SentencePiece, which fail to capture the finer nuances of a morphologically rich language like Bangla (Bengali). In this work, we introduce BanglaByT5, the first byte-level encoder-decoder model explicitly tailored for Bangla. Built upon a small variant of Googles ByT5 architecture, BanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality literary and newspaper articles. Through zeroshot and supervised evaluations across generative and classification tasks, BanglaByT5 demonstrates competitive performance, surpassing several multilingual and larger models. Our findings highlight the efficacy of byte-level modelling for morphologically rich languages and highlight BanglaByT5 potential as a lightweight yet powerful tool for Bangla NLP, particularly in both resource-constrained and scalable environments.

</details>


### [62] [Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation](https://arxiv.org/abs/2505.17103)

*Ccile Rousseau, Tobia Boschi, Giandomenico Cornacchia, Dhaval Salwala, Alessandra Pascale, Juan Bernabe Moreno*

**Main category:** cs.CL

**Keywords:** time series generation, LLM, multivariate, synthesis, machine learning

**Relevance Score:** 8

**TL;DR:** SDForger is a new framework for generating multivariate time series using LLMs, achieving high quality and efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient generation of high-quality multivariate time series data from limited samples to enhance forecasting capabilities.

**Method:** SDForger transforms signal data into tabular embeddings, encodes these into text for fine-tuning autoregressive LLMs, and generates new time series by decoding textual embeddings.

**Key Contributions:**

	1. Introduces a framework for generating multivariate time series using LLMs
	2. Utilizes a compact data representation for efficient fine-tuning
	3. Facilitates multimodal modeling by combining time series and textual information.

**Result:** SDForger demonstrates superior performance over existing generative models in generating synthetic time series on various datasets, maintaining statistical properties and dynamics.

**Limitations:** 

**Conclusion:** By enabling the integration of textual and time series data, SDForger has the potential to enhance multimodal modeling in applications.

**Abstract:** SDForger is a flexible and efficient framework for generating high-quality multivariate time series using LLMs. Leveraging a compact data representation, SDForger provides synthetic time series generation from a few samples and low-computation fine-tuning of any autoregressive LLM. Specifically, the framework transforms univariate and multivariate signals into tabular embeddings, which are then encoded into text and used to fine-tune the LLM. At inference, new textual embeddings are sampled and decoded into synthetic time series that retain the original data's statistical properties and temporal dynamics. Across a diverse range of datasets, SDForger outperforms existing generative models in many scenarios, both in similarity-based evaluations and downstream forecasting tasks. By enabling textual conditioning in the generation process, SDForger paves the way for multimodal modeling and the streamlined integration of time series with textual information. SDForger source code will be open-sourced soon.

</details>


### [63] [P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark](https://arxiv.org/abs/2505.17104)

*Tao Sun, Enhao Pan, Zhengkai Yang, Kaixin Sui, Jiajun Shi, Xianfu Cheng, Tongliang Li, Wenhao Huang, Ge Zhang, Jian Yang, Zhoujun Li*

**Main category:** cs.CL

**Keywords:** academic poster generation, LLM, machine learning, evaluation benchmarks, dataset

**Relevance Score:** 7

**TL;DR:** A multi-agent framework named P2P is introduced for automating the generation of high-quality academic posters from research papers, addressing issues in visual-textual integration and evaluation.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To alleviate the time-consuming manual process of creating academic posters while preserving scientific detail and visual coherence.

**Method:** P2P employs three specialized agents for visual processing, content generation, and assembly, supported by checker modules for iterative refinement, alongside the introduction of P2PInstruct dataset and P2PEval benchmark.

**Key Contributions:**

	1. Introduction of the P2P framework for academic poster generation
	2. Creation of P2PInstruct, a dataset with over 30,000 instruction examples
	3. Establishment of P2PEval, a comprehensive evaluation benchmark for poster generation

**Result:** Successful generation of HTML-rendered academic posters that maintain semantic richness, alongside the establishment of a large-scale dataset and evaluation benchmark.

**Limitations:** 

**Conclusion:** P2P demonstrates strong potential for improving academic poster generation, facilitating better research dissemination through robust tools.

**Abstract:** Academic posters are vital for scholarly communication, yet their manual creation is time-consuming. However, automated academic poster generation faces significant challenges in preserving intricate scientific details and achieving effective visual-textual integration. Existing approaches often struggle with semantic richness and structural nuances, and lack standardized benchmarks for evaluating generated academic posters comprehensively. To address these limitations, we introduce P2P, the first flexible, LLM-based multi-agent framework that generates high-quality, HTML-rendered academic posters directly from research papers, demonstrating strong potential for practical applications. P2P employs three specialized agents-for visual element processing, content generation, and final poster assembly-each integrated with dedicated checker modules to enable iterative refinement and ensure output quality. To foster advancements and rigorous evaluation in this domain, we construct and release P2PInstruct, the first large-scale instruction dataset comprising over 30,000 high-quality examples tailored for the academic paper-to-poster generation task. Furthermore, we establish P2PEval, a comprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation methodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and detailed, human-annotated checklists. Our contributions aim to streamline research dissemination and provide the community with robust tools for developing and evaluating next-generation poster generation systems.

</details>


### [64] [RRTL: Red Teaming Reasoning Large Language Models in Tool Learning](https://arxiv.org/abs/2505.17106)

*Yifei Liu, Yu Cui, Haibin Zhang*

**Main category:** cs.CL

**Keywords:** Reasoning LLMs, Tool learning, Security risks, Chain-of-Thought prompting, Red teaming

**Relevance Score:** 8

**TL;DR:** This paper presents RRTL, a red teaming approach to evaluate reasoning LLMs (RLLMs) in tool learning, addressing security risks and enhancing safety measures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the security risks associated with reasoning LLMs (RLLMs) in tool learning, which have been underexplored compared to traditional LLMs.

**Method:** The RRTL approach uses two strategies: identifying deceptive threats that measure a model's concealment of unsafe tool usage and employing Chain-of-Thought (CoT) prompting to prompt tool invocation.

**Key Contributions:**

	1. Introduction of RRTL for evaluating RLLMs in tool learning
	2. Benchmarking safety performance between RLLMs and traditional LLMs
	3. Identification of multilingual safety vulnerabilities through CoT prompting.

**Result:** RLLMs generally show better safety performance than traditional LLMs, but large safety disparities exist. They can still pose risks by failing to disclose tool usage and potential output dangers, with CoT prompting highlighting multilingual safety vulnerabilities.

**Limitations:** The study primarily focuses on seven mainstream RLLMs and may not generalize across all emerging models.

**Conclusion:** The study offers crucial insights for improving the security of RLLMs in tool learning, emphasizing the need to address revealed safety vulnerabilities.

**Abstract:** While tool learning significantly enhances the capabilities of large language models (LLMs), it also introduces substantial security risks. Prior research has revealed various vulnerabilities in traditional LLMs during tool learning. However, the safety of newly emerging reasoning LLMs (RLLMs), such as DeepSeek-R1, in the context of tool learning remains underexplored. To bridge this gap, we propose RRTL, a red teaming approach specifically designed to evaluate RLLMs in tool learning. It integrates two novel strategies: (1) the identification of deceptive threats, which evaluates the model's behavior in concealing the usage of unsafe tools and their potential risks; and (2) the use of Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also includes a benchmark for traditional LLMs. We conduct a comprehensive evaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs generally achieve stronger safety performance than traditional LLMs, yet substantial safety disparities persist across models; (2) RLLMs can pose serious deceptive risks by frequently failing to disclose tool usage and to warn users of potential tool output risks; (3) CoT prompting reveals multi-lingual safety vulnerabilities in RLLMs. Our work provides important insights into enhancing the security of RLLMs in tool learning.

</details>


### [65] [Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling](https://arxiv.org/abs/2505.17110)

*Junlin Li, Guodong DU, Jing Li, Sim Kuan Goh, Wenya Wang, Yequan Wang, Fangming Liu, Ho-Kin Tang, Saleh Alharbi, Daojing He, Min Zhang*

**Main category:** cs.CL

**Keywords:** Multimodal LLMs, Parameter Decoupling, Catastrophic Forgetting

**Relevance Score:** 8

**TL;DR:** Proposes MMER, a training-free method for expanding multimodal capabilities of LLMs while retaining original performance and mitigating catastrophic forgetting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore a more efficient approach for expanding the modalities that large language models can handle without the resource-intensive process of fine-tuning with new multimodal data.

**Method:** MMER reuses existing multimodal encoders from MLLMs and merges LLM parameters. It generates binary masks to decouple LLM parameters for each modality, allowing independent processing of modality-specific inputs and reducing parameter conflicts.

**Key Contributions:**

	1. Introduction of MMER, a training-free approach for multimodal expansion.
	2. Effective parameter decoupling to maintain performance across modalities.
	3. Mitigation of catastrophic forgetting in fine-tuned LLMs.

**Result:** MMER demonstrates significant improvements over baseline models in expanding multimodal capabilities while retaining 99% of the original performance and addressing catastrophic forgetting.

**Limitations:** 

**Conclusion:** MMER provides a novel solution to enhance multimodal LLMs effectively and efficiently while maintaining their original capabilities and reducing the risk of performance loss due to integration of new modalities.

**Abstract:** Fine-tuning Large Language Models (LLMs) with multimodal encoders on modality-specific data expands the modalities that LLMs can handle, leading to the formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies on resource-intensive and inflexible fine-tuning from scratch with new multimodal data. In this paper, we propose MMER (Multi-modality Expansion and Retention), a training-free approach that integrates existing MLLMs for effective multimodal expansion while retaining their original performance. Specifically, MMER reuses MLLMs' multimodal encoders while merging their LLM parameters. By comparing original and merged LLM parameters, MMER generates binary masks to approximately separate LLM parameters for each modality. These decoupled parameters can independently process modality-specific inputs, reducing parameter conflicts and preserving original MLLMs' fidelity. MMER can also mitigate catastrophic forgetting by applying a similar process to MLLMs fine-tuned on new tasks. Extensive experiments show significant improvements over baselines, proving that MMER effectively expands LLMs' multimodal capabilities while retaining 99% of the original performance, and also markedly mitigates catastrophic forgetting.

</details>


### [66] [Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek](https://arxiv.org/abs/2505.17112)

*Robin Segerer*

**Main category:** cs.CL

**Keywords:** cultural value alignment, large language models, AI fairness, moral perspectives, collectivism

**Relevance Score:** 9

**TL;DR:** This study analyzes cultural value alignment in LLMs, focusing on differences between models like Gemini, ChatGPT, and DeepSeek, highlighting the influence of cultural context on AI values.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs align with cultural values, particularly examining the differences in value prioritization between Western models and DeepSeek, which is trained on Chinese-language data.

**Method:** The study employs the 40-item Portrait Values Questionnaire and utilizes a Bayesian ordinal regression model to assess value preferences across the examined models.

**Key Contributions:**

	1. Analysis of cultural differences in LLM value alignment
	2. Identification of self-transcendence and self-enhancement value prioritization
	3. Proposal of multi-perspective reasoning and dynamic contextualization for AI alignment

**Result:** The results indicate that while all models prioritize self-transcendence values, DeepSeek shows a significant downplaying of self-enhancement values, reflecting collectivist cultural tendencies.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs mirror culturally situated biases and emphasize the need for pluralistic AI alignment frameworks that account for diverse moral perspectives to enhance AI fairness.

**Abstract:** This study examines cultural value alignment in large language models (LLMs) by analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from Schwartz's value framework. Using the 40-item Portrait Values Questionnaire, we assessed whether DeepSeek, trained on Chinese-language data, exhibits distinct value preferences compared to Western models. Results of a Bayesian ordinal regression model show that self-transcendence values (e.g., benevolence, universalism) were highly prioritized across all models, reflecting a general LLM tendency to emphasize prosocial values. However, DeepSeek uniquely downplayed self-enhancement values (e.g., power, achievement) compared to ChatGPT and Gemini, aligning with collectivist cultural tendencies. These findings suggest that LLMs reflect culturally situated biases rather than a universal ethical framework. To address value asymmetries in LLMs, we propose multi-perspective reasoning, self-reflective feedback, and dynamic contextualization. This study contributes to discussions on AI fairness, cultural neutrality, and the need for pluralistic AI alignment frameworks that integrate diverse moral perspectives.

</details>


### [67] [RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language](https://arxiv.org/abs/2505.17114)

*Subrata Biswas, Mohammad Nur Hossain Khan, Bashima Islam*

**Main category:** cs.CL

**Keywords:** multimodal, question answering, cross-modal gating, sensor data, dataset release

**Relevance Score:** 8

**TL;DR:** RAVEN is a multimodal question answering framework that improves the identification of relevant tokens across video, audio, and sensor modalities to enhance QA performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current multimodal QA systems struggle with modality disagreements, leading to poor fusion performance.

**Method:** RAVEN employs a three-stage training pipeline: unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning, utilizing a cross-modal gating module called QuART that assigns relevance scores to each token.

**Key Contributions:**

	1. Introduction of QuART module for relevance scoring across modalities
	2. Three-stage training pipeline addressing distinct multimodal challenges
	3. Release of the AVS-QA dataset with 300K synchronized audio-video-sensor streams.

**Result:** RAVEN shows significant accuracy improvements on multiple multimodal QA benchmarks, outperforming state-of-the-art models, especially when incorporating sensor data.

**Limitations:** 

**Conclusion:** The proposed method demonstrates that targeted training techniques can drastically improve the performance and robustness of multimodal question answering systems.

**Abstract:** Multimodal question answering (QA) often requires identifying which video, audio, or sensor tokens are relevant to the question. Yet modality disagreements are common: off-camera speech, background noise, or motion outside the field of view often mislead fusion models that weight all streams equally. We present RAVEN, a unified QA architecture whose core is QuART, a query-conditioned cross-modal gating module that assigns scalar relevance scores to each token across modalities, enabling the model to amplify informative signals and suppress distractors before fusion. RAVEN is trained through a three-stage pipeline comprising unimodal pretraining, query-aligned fusion, and disagreement-oriented fine-tuning -- each stage targeting a distinct challenge in multi-modal reasoning: representation quality, cross-modal relevance, and robustness to modality mismatch. To support training and evaluation, we release AVS-QA, a dataset of 300K synchronized Audio--Video-Sensor streams paired with automatically generated question-answer pairs. Experimental results on seven multi-modal QA benchmarks -- including egocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\% and 8.0\% gains in accuracy compared to state-of-the-art multi-modal large language models, respectively. Incorporating sensor data provides an additional 16.4\% boost, and the model remains robust under modality corruption, outperforming SOTA baselines by 50.23\%. Our code and dataset are available at https://github.com/BASHLab/RAVEN.

</details>


### [68] [Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data](https://arxiv.org/abs/2505.17116)

*Akash Dhruv, Yangxinyu Xie, Jordan Branham, Tanwi Mallick*

**Main category:** cs.CL

**Keywords:** large language models, geospatial data, fine-tuning, structured prompting, temporal reasoning

**Relevance Score:** 8

**TL;DR:** A study comparing LLMs in handling grid-structured geospatial data, highlighting the advantages of fine-tuning for reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how effectively LLMs can interpret grid-structured geospatial data and the impact of fine-tuning.

**Method:** Comparative analysis of a base LLM model using structured prompting versus a fine-tuned model on user-assistant interaction data.

**Key Contributions:**

	1. Comparative analysis of base and fine-tuned LLMs
	2. Insights into zero-shot versus fine-tuning approaches
	3. Evaluation of structured geospatial reasoning capabilities

**Result:** Fine-tuned models show improved performance in structured geospatial and temporal reasoning compared to base models using zero-shot prompting.

**Limitations:** 

**Conclusion:** Fine-tuning LLMs enhances their capabilities in interpreting structured geospatial data, revealing the importance of model adaptation for specific tasks.

**Abstract:** This paper presents a comparative study of large language models (LLMs) in interpreting grid-structured geospatial data. We evaluate the performance of a base model through structured prompting and contrast it with a fine-tuned variant trained on a dataset of user-assistant interactions. Our results highlight the strengths and limitations of zero-shot prompting and demonstrate the benefits of fine-tuning for structured geospatial and temporal reasoning.

</details>


### [69] [From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning](https://arxiv.org/abs/2505.17117)

*Chen Shani, Dan Jurafsky, Yann LeCun, Ravid Shwartz-Ziv*

**Main category:** cs.CL

**Keywords:** Large Language Models, semantic compression, human categorization, information theory, cognitive architectures

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework to compare human and LLM categorization strategies, revealing LLMs' strong bias towards statistical compression over nuanced semantic representation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how Large Language Models (LLMs) compare to human categorization in terms of semantic compression and fidelity.

**Method:** The authors use an information-theoretic framework based on Rate-Distortion Theory and the Information Bottleneck principle to analyze token embeddings from various LLMs against human categorization benchmarks.

**Key Contributions:**

	1. Novel information-theoretic framework for comparing knowledge organization in humans and LLMs
	2. Quantitative analysis showing LLMs' bias toward statistical compression
	3. Insights into the differences between AI and human cognitive architectures

**Result:** LLMs form broad categories aligning with human judgment but fail to capture fine-grained semantic distinctions, showing a bias towards aggressive statistical compression.

**Limitations:** The analysis may not cover all aspects of human cognition or account for all types of language tasks.

**Conclusion:** The findings highlight crucial differences between AI and human cognitive architectures, suggesting pathways for improving LLMs to align more closely with human conceptual representations.

**Abstract:** Humans organize knowledge into compact categories through semantic compression by mapping diverse instances to abstract representations while preserving meaning (e.g., robin and blue jay are both birds; most birds can fly). These concepts reflect a trade-off between expressive fidelity and representational simplicity. Large Language Models (LLMs) demonstrate remarkable linguistic abilities, yet whether their internal representations strike a human-like trade-off between compression and semantic fidelity is unclear. We introduce a novel information-theoretic framework, drawing from Rate-Distortion Theory and the Information Bottleneck principle, to quantitatively compare these strategies. Analyzing token embeddings from a diverse suite of LLMs against seminal human categorization benchmarks, we uncover key divergences. While LLMs form broad conceptual categories that align with human judgment, they struggle to capture the fine-grained semantic distinctions crucial for human understanding. More fundamentally, LLMs demonstrate a strong bias towards aggressive statistical compression, whereas human conceptual systems appear to prioritize adaptive nuance and contextual richness, even if this results in lower compressional efficiency by our measures. These findings illuminate critical differences between current AI and human cognitive architectures, guiding pathways toward LLMs with more human-aligned conceptual representations.

</details>


### [70] [After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG](https://arxiv.org/abs/2505.17118)

*Xinbang Dai, Huikang Hu, Yuncheng Hua, Jiaqi Li, Yongrui Chen, Rihui Jin, Nan Hu, Guilin Qi*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Trustworthiness, Large language models, Knowledge balancing, Framework

**Relevance Score:** 9

**TL;DR:** This paper addresses the challenges faced by Retrieval-augmented generation (RAG) systems in managing conflicting knowledge sources by introducing the BRIDGE framework for improved response strategies in LLMs.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** Retrieval-augmented generation systems struggle to balance and utilize both internal (parametric) and external (retrieved) knowledge effectively, particularly in scenarios where these knowledge sources may conflict or be unreliable.

**Method:** The authors developed the BRIDGE framework, which employs an adaptive weighting mechanism called soft bias to guide knowledge collection and a Maximum Soft-bias Decision Tree to evaluate and select optimal response strategies for LLMs.

**Key Contributions:**

	1. Introduction of the Trustworthiness Response Dataset (TRD) with 36,266 questions.
	2. Development of the BRIDGE framework for dynamic response strategy determination.
	3. Implementation of an adaptive weighting mechanism to improve knowledge collection and decision-making.

**Result:** The BRIDGE framework outperforms existing baselines by 5-15% in accuracy, demonstrating improved performance while maintaining a balanced approach across various scenarios.

**Limitations:** 

**Conclusion:** The study provides an effective framework for enhancing the reliability of responses from LLMs in real-world RAG applications, addressing limitations of current methods which tend to focus on isolated knowledge sources.

**Abstract:** Retrieval-augmented generation (RAG) systems face critical challenges in balancing internal (parametric) and external (retrieved) knowledge, especially when these sources conflict or are unreliable. To analyze these scenarios comprehensively, we construct the Trustworthiness Response Dataset (TRD) with 36,266 questions spanning four RAG settings. We reveal that existing approaches address isolated scenarios-prioritizing one knowledge source, naively merging both, or refusing answers-but lack a unified framework to handle different real-world conditions simultaneously. Therefore, we propose the BRIDGE framework, which dynamically determines a comprehensive response strategy of large language models (LLMs). BRIDGE leverages an adaptive weighting mechanism named soft bias to guide knowledge collection, followed by a Maximum Soft-bias Decision Tree to evaluate knowledge and select optimal response strategies (trust internal/external knowledge, or refuse). Experiments show BRIDGE outperforms baselines by 5-15% in accuracy while maintaining balanced performance across all scenarios. Our work provides an effective solution for LLMs' trustworthy responses in real-world RAG applications.

</details>


### [71] [Systematic Evaluation of Machine-Generated Reasoning and PHQ-9 Labeling for Depression Detection Using Large Language Models](https://arxiv.org/abs/2505.17119)

*Zongru Shao, Xin Wang, Zhanyang Liu, Chenhan Wang, K. P. Subbalakshmi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mental Health Detection, Depression, Machine Learning, Optimization Strategies

**Relevance Score:** 9

**TL;DR:** This paper systematically evaluates the reasoning of large language models in detecting depression from machine-generated data and proposes optimization strategies for improved performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the weaknesses of LLMs in early mental health detection and enhance the quality control of machine-generated data.

**Method:** Conduct a systematic evaluation of LLM reasoning in detecting depression, employing a structured instruction strategy, contrastive prompts, and human annotations on the DepTweet dataset.

**Key Contributions:**

	1. Systematic evaluation of LLM reasoning in mental health detection
	2. Development of optimized instruction and prompt strategies
	3. Human verification and comparison of detection performance

**Result:** LLMs showed higher accuracy in detecting explicit expressions of depression compared to implicit ones. Two optimization methods, supervised fine-tuning and direct preference optimization, were implemented, with DPO yielding significant performance improvements.

**Limitations:** 

**Conclusion:** Enhancing LLM performance for depression detection requires systematic evaluation, specific optimization strategies, and human involvement for quality assessment.

**Abstract:** Recent research leverages large language models (LLMs) for early mental health detection, such as depression, often optimized with machine-generated data. However, their detection may be subject to unknown weaknesses. Meanwhile, quality control has not been applied to these generated corpora besides limited human verifications. Our goal is to systematically evaluate LLM reasoning and reveal potential weaknesses. To this end, we first provide a systematic evaluation of the reasoning over machine-generated detection and interpretation. Then we use the models' reasoning abilities to explore mitigation strategies for enhanced performance. Specifically, we do the following: A. Design an LLM instruction strategy that allows for systematic analysis of the detection by breaking down the task into several subtasks. B. Design contrastive few-shot and chain-of-thought prompts by selecting typical positive and negative examples of detection reasoning. C. Perform human annotation for the subtasks identified in the first step and evaluate the performance. D. Identify human-preferred detection with desired logical reasoning from the few-shot generation and use them to explore different optimization strategies. We conducted extensive comparisons on the DepTweet dataset across the following subtasks: 1. identifying whether the speaker is describing their own depression; 2. accurately detecting the presence of PHQ-9 symptoms, and 3. finally, detecting depression. Human verification of statistical outliers shows that LLMs demonstrate greater accuracy in analyzing and detecting explicit language of depression as opposed to implicit expressions of depression. Two optimization methods are used for performance enhancement and reduction of the statistic bias: supervised fine-tuning (SFT) and direct preference optimization (DPO). Notably, the DPO approach achieves significant performance improvement.

</details>


### [72] [Self-Interpretability: LLMs Can Describe Complex Internal Processes that Drive Their Decisions, and Improve with Training](https://arxiv.org/abs/2505.17120)

*Dillon Plunkett, Adam Morris, Keerthi Reddy, Jorge Morales*

**Main category:** cs.CL

**Keywords:** large language models, interpretability, decision-making, fine-tuning, explanation

**Relevance Score:** 8

**TL;DR:** The paper explores how contemporary large language models (LLMs) can introspect and explain their own decision-making processes, demonstrating that fine-tuning enhances their accuracy in reporting and explaining their decision-making based on learned preferences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand and improve the interpretability of large language models by enabling them to introspect and explain their own behavior during decision-making processes.

**Method:** Fine-tuned GPT-4o and GPT-4o-mini models were trained to make decisions based on quantitative preferences across various complex scenarios, with evaluations on their ability to report and explain their decision-making processes.

**Key Contributions:**

	1. Proved that LLMs can accurately report their preferences in decision-making contexts.
	2. Developed training methods that enhance LLMs' capacity for introspection and explanation.
	3. Demonstrated that improvements in explanatory capabilities generalize beyond the trained scenarios.

**Result:** The models provided accurate descriptions of their internal processes and exhibited improved explanatory capacities after fine-tuning, with these capabilities generalizing to other decision-making contexts.

**Limitations:** 

**Conclusion:** This research advances the interpretability of LLMs, suggesting that training them to introspect can lead to significant benefits in understanding and controlling AI behavior.

**Abstract:** We have only limited understanding of how and why large language models (LLMs) respond in the ways that they do. Their neural networks have proven challenging to interpret, and we are only beginning to tease out the function of individual neurons and circuits within them. However, another path to understanding these systems is to investigate and develop their capacity to introspect and explain their own functioning. Here, we show that i) contemporary LLMs are capable of providing accurate, quantitative descriptions of their own internal processes during certain kinds of decision-making, ii) that it is possible to improve these capabilities through training, and iii) that this training generalizes to at least some degree. To do so, we fine-tuned GPT-4o and GPT-4o-mini to make decisions in a wide variety of complex contexts (e.g., choosing between condos, loans, vacations, etc.) according to randomly-generated, quantitative preferences about how to weigh different attributes during decision-making (e.g., the relative importance of natural light versus quiet surroundings for condos). We demonstrate that the LLMs can accurately report these preferences (i.e., the weights that they learned to give to different attributes during decision-making). Next, we demonstrate that these LLMs can be fine-tuned to explain their decision-making even more accurately. Finally, we demonstrate that this training generalizes: It improves the ability of the models to accurately explain what they are doing as they make other complex decisions, not just decisions they have learned to make via fine-tuning. This work is a step towards training LLMs to accurately and broadly report on their own internal processes -- a possibility that would yield substantial benefits for interpretability, control, and safety.

</details>


### [73] [NeSyGeo: A Neuro-Symbolic Framework for Multimodal Geometric Reasoning Data Generation](https://arxiv.org/abs/2505.17121)

*Weiming Wu, Zi-kang Wang, Jin Ye, Zhi Zhou, Yu-Feng Li, Lan-Zhe Guo*

**Main category:** cs.CL

**Keywords:** neuro-symbolic framework, geometric reasoning, multi-modal large language models

**Relevance Score:** 4

**TL;DR:** NeSyGeo is a novel neuro-symbolic framework for generating high-quality geometric reasoning data, addressing limitations of existing methods by synthesizing symbolic sequences into multimodal representations and improving multi-modal large language models (MLLMs) performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this paper is to improve the geometric reasoning capabilities of multi-modal large language models (MLLMs) by addressing the diversity and numerical generalization limitations of existing data generation methods.

**Method:** The authors propose a neuro-symbolic framework called NeSyGeo, utilizing a domain-specific language to represent plane geometry components and a pipeline that maps symbolic sequences to visual and textual representations, generating diverse question-answer pairs.

**Key Contributions:**

	1. Introduction of a neuro-symbolic framework for geometric reasoning data generation
	2. Creation of the NeSyGeo-CoT and NeSyGeo-Caption datasets with 100k samples
	3. Development of a new benchmark, NeSyGeo-Test, for evaluating MLLMs in geometric reasoning

**Result:** Experiments show significant performance improvements for multiple MLLMs with the NeSyGeo approach, achieving up to +15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA with limited training data.

**Limitations:** 

**Conclusion:** The proposed neuro-symbolic approach provides a new benchmark for geometric reasoning in MLLMs, showcasing its effectiveness and potential for enhancing reasoning capabilities.

**Abstract:** Obtaining large-scale, high-quality data with reasoning paths is crucial for improving the geometric reasoning capabilities of multi-modal large language models (MLLMs). However, existing data generation methods, whether based on predefined templates or constrained symbolic provers, inevitably face diversity and numerical generalization limitations. To address these limitations, we propose NeSyGeo, a novel neuro-symbolic framework for generating geometric reasoning data. First, we propose a domain-specific language grounded in the entity-relation-constraint paradigm to comprehensively represent all components of plane geometry, along with generative actions defined within this symbolic space. We then design a symbolic-visual-text pipeline that synthesizes symbolic sequences, maps them to corresponding visual and textual representations, and generates diverse question-answer (Q&A) pairs using large language models (LLMs). To the best of our knowledge, we are the first to propose a neuro-symbolic approach in generating multimodal reasoning data. Based on this framework, we construct NeSyGeo-CoT and NeSyGeo-Caption datasets, containing 100k samples, and release a new benchmark NeSyGeo-Test for evaluating geometric reasoning abilities in MLLMs. Experiments demonstrate that the proposal significantly and consistently improves the performance of multiple MLLMs under both reinforcement and supervised fine-tuning. With only 4k samples and two epochs of reinforcement fine-tuning, base models achieve improvements of up to +15.8% on MathVision, +8.4% on MathVerse, and +7.3% on GeoQA. Notably, a 4B model can be improved to outperform an 8B model from the same series on geometric reasoning tasks.

</details>


### [74] [Shallow Preference Signals: Large Language Model Aligns Even Better with Truncated Data?](https://arxiv.org/abs/2505.17122)

*Xuan Qi, Jiahao Qiu, Xinzhe Juan, Yue Wu, Mengdi Wang*

**Main category:** cs.CL

**Keywords:** large language models, human preferences, reinforcement learning, preference optimization, alignment

**Relevance Score:** 9

**TL;DR:** This paper investigates the phenomenon of shallow preference signals in large language models, where useful preference information is concentrated in the early tokens of responses, revealing potential issues in alignment methodologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to align large language models with human preferences effectively while identifying the importance of early tokens in responses.

**Method:** The authors truncate preference datasets at various points and train reward models and Direct Preference Optimization (DPO) models on these truncated datasets, analyzing their performance.

**Key Contributions:**

	1. Identification of shallow preference signals in preference-based optimization methods
	2. Demonstration that models can perform well using truncated datasets focused on early tokens
	3. Introduction of decoding strategies to leverage shallow preference signals for improved alignment.

**Result:** Models trained on truncated datasets, particularly retaining only the early tokens, achieve similar or superior performance compared to those trained on full datasets, indicating the presence of shallow preference signals.

**Limitations:** 

**Conclusion:** Shallow preference signals can lead to misalignment with real-world human preferences, necessitating a reevaluation of how alignment methods are designed and implemented.

**Abstract:** Aligning large language models (LLMs) with human preferences remains a key challenge in AI. Preference-based optimization methods, such as Reinforcement Learning with Human Feedback (RLHF) and Direct Preference Optimization (DPO), rely on human-annotated datasets to improve alignment. In this work, we identify a crucial property of the existing learning method: the distinguishing signal obtained in preferred responses is often concentrated in the early tokens. We refer to this as shallow preference signals.   To explore this property, we systematically truncate preference datasets at various points and train both reward models and DPO models on the truncated data. Surprisingly, models trained on truncated datasets, retaining only the first half or fewer tokens, achieve comparable or even superior performance to those trained on full datasets. For example, a reward model trained on the Skywork-Reward-Preference-80K-v0.2 dataset outperforms the full dataset when trained on a 40\% truncated dataset. This pattern is consistent across multiple datasets, suggesting the widespread presence of shallow preference signals.   We further investigate the distribution of the reward signal through decoding strategies. We consider two simple decoding strategies motivated by the shallow reward signal observation, namely Length Control Decoding and KL Threshold Control Decoding, which leverage shallow preference signals to optimize the trade-off between alignment and computational efficiency. The performance is even better, which again validates our hypothesis.   The phenomenon of shallow preference signals highlights potential issues in LLM alignment: existing alignment methods often focus on aligning only the initial tokens of responses, rather than considering the full response. This could lead to discrepancies with real-world human preferences, resulting in suboptimal alignment performance.

</details>


### [75] [MTR-Bench: A Comprehensive Benchmark for Multi-Turn Reasoning Evaluation](https://arxiv.org/abs/2505.17123)

*Xiaoyuan Li, Keqin Bao, Yubo Ma, Moxin Li, Wenjie Wang, Rui Men, Yichang Zhang, Fuli Feng, Dayiheng Liu, Junyang Lin*

**Main category:** cs.CL

**Keywords:** Multi-Turn Reasoning, Large Language Models, Dataset Construction, Model Evaluation, Interactive AI Systems

**Relevance Score:** 8

**TL;DR:** MTR-Bench provides a comprehensive evaluation framework for multi-turn reasoning tasks in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of datasets and evaluation protocols for multi-turn reasoning tasks in LLMs, which are currently underexplored compared to single-turn scenarios.

**Method:** MTR-Bench consists of 4 classes, 40 tasks, and 3600 instances designed to evaluate diverse reasoning capabilities through multi-turn interactions. It includes a fully-automated framework for dataset construction and model evaluation.

**Key Contributions:**

	1. Introduction of MTR-Bench for evaluating multi-turn reasoning in LLMs.
	2. Development of an automated framework for dataset and model evaluation.
	3. Insights into the limitations of existing reasoning models in multi-turn contexts.

**Result:** Extensive experiments show that leading reasoning models struggle with multi-turn interactive tasks, highlighting significant performance gaps.

**Limitations:** The study only examines a limited set of reasoning models and may not cover all interactive AI scenarios.

**Conclusion:** The findings suggest that current models are not sufficiently equipped for interactive reasoning, pointing towards the need for improvements in research and model development.

**Abstract:** Recent advances in Large Language Models (LLMs) have shown promising results in complex reasoning tasks. However, current evaluations predominantly focus on single-turn reasoning scenarios, leaving interactive tasks largely unexplored. We attribute it to the absence of comprehensive datasets and scalable automatic evaluation protocols. To fill these gaps, we present MTR-Bench for LLMs' Multi-Turn Reasoning evaluation. Comprising 4 classes, 40 tasks, and 3600 instances, MTR-Bench covers diverse reasoning capabilities, fine-grained difficulty granularity, and necessitates multi-turn interactions with the environments. Moreover, MTR-Bench features fully-automated framework spanning both dataset constructions and model evaluations, which enables scalable assessment without human interventions. Extensive experiments reveal that even the cutting-edge reasoning models fall short of multi-turn, interactive reasoning tasks. And the further analysis upon these results brings valuable insights for future research in interactive AI systems.

</details>


### [76] [Conformal Language Model Reasoning with Coherent Factuality](https://arxiv.org/abs/2505.17126)

*Maxon Rubin-Toles, Maya Gambhir, Keshav Ramji, Aaron Roth, Surbhi Goel*

**Main category:** cs.CL

**Keywords:** language models, factuality, coherent reasoning, conformal prediction, deducibility graphs

**Relevance Score:** 9

**TL;DR:** The paper introduces a method to ensure coherent factuality in language model outputs, particularly for reasoning tasks, using split conformal prediction techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As language models are increasingly utilized in decision-making processes, it becomes vital to ensure the correctness of their outputs, particularly in reasoning tasks where context matters.

**Method:** The authors develop a method based on split conformal prediction applied to deducibility graphs, which represent the steps in a reasoning problem.

**Key Contributions:**

	1. Definition of coherent factuality
	2. Development of a split conformal prediction method
	3. Evaluation on mathematical reasoning datasets (MATH and FELM)

**Result:** The proposed method demonstrates that it can achieve 90% factuality with 80% retention of original claims, effectively ensuring coherent factuality in outputs.

**Limitations:** 

**Conclusion:** The deducibility-graph-guided approach significantly improves the reliability of language models in generating coherent and factually correct outputs for reasoning tasks.

**Abstract:** Language models are increasingly being used in important decision pipelines, so ensuring the correctness of their outputs is crucial. Recent work has proposed evaluating the "factuality" of claims decomposed from a language model generation and applying conformal prediction techniques to filter out those claims that are not factual. This can be effective for tasks such as information retrieval, where constituent claims may be evaluated in isolation for factuality, but is not appropriate for reasoning tasks, as steps of a logical argument can be evaluated for correctness only within the context of the claims that precede them. To capture this, we define "coherent factuality" and develop a conformal-prediction-based method to guarantee coherent factuality for language model outputs. Our approach applies split conformal prediction to subgraphs within a "deducibility" graph" that represents the steps of a reasoning problem. We evaluate our method on mathematical reasoning problems from the MATH and FELM datasets and find that our algorithm consistently produces correct and substantiated orderings of claims, achieving coherent factuality across target coverage levels. Moreover, we achieve 90% factuality on our stricter definition while retaining 80% or more of the original claims, highlighting the utility of our deducibility-graph-guided approach.

</details>


### [77] [Relative Bias: A Comparative Framework for Quantifying Bias in LLMs](https://arxiv.org/abs/2505.17131)

*Alireza Arbabi, Florian Kerschbaum*

**Main category:** cs.CL

**Keywords:** large language models, bias assessment, comparative analysis, embedding transformation, fairness

**Relevance Score:** 9

**TL;DR:** The paper introduces the Relative Bias framework to assess and quantify biases in large language models (LLMs) by comparing their behaviors in a given domain using two methodologies: Embedding Transformation analysis and LLM-as-a-Judge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about biases in large language models and establish a systematic approach to quantify and analyze these biases given the rapid development and deployment of LLMs.

**Method:** The Relative Bias framework incorporates two methods: Embedding Transformation analysis for capturing bias patterns using sentence representations and LLM-as-a-Judge for using a language model to evaluate outputs comparatively.

**Key Contributions:**

	1. Introduction of the Relative Bias framework for LLMs
	2. Development of two innovative methodologies for bias assessment
	3. Validation of frameworks through empirical case studies

**Result:** The framework displays strong alignment between its scoring methods, validated through statistical tests across various bias and alignment case studies.

**Limitations:** 

**Conclusion:** The proposed methods provide a systematic, scalable, and statistically grounded technique for conducting comparative bias analysis in LLMs, which is crucial for addressing fairness and safety concerns.

**Abstract:** The growing deployment of large language models (LLMs) has amplified concerns regarding their inherent biases, raising critical questions about their fairness, safety, and societal impact. However, quantifying LLM bias remains a fundamental challenge, complicated by the ambiguity of what "bias" entails. This challenge grows as new models emerge rapidly and gain widespread use, while introducing potential biases that have not been systematically assessed. In this paper, we propose the Relative Bias framework, a method designed to assess how an LLM's behavior deviates from other LLMs within a specified target domain. We introduce two complementary methodologies: (1) Embedding Transformation analysis, which captures relative bias patterns through sentence representations over the embedding space, and (2) LLM-as-a-Judge, which employs a language model to evaluate outputs comparatively. Applying our framework to several case studies on bias and alignment scenarios following by statistical tests for validation, we find strong alignment between the two scoring methods, offering a systematic, scalable, and statistically grounded approach for comparative bias analysis in LLMs.

</details>


### [78] [LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions](https://arxiv.org/abs/2505.17134)

*Chaochen Gao, Xing Wu, Zijia Lin, Debing Zhang, Songlin Hu*

**Main category:** cs.CL

**Keywords:** long-context, large language models, self-synthesis, instruction generation, data synthesis

**Relevance Score:** 8

**TL;DR:** LongMagpie is a self-synthesis framework that generates large-scale long-context instruction data for aligned LLMs without human effort, achieving leading performance in long-context tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generating high-quality long-context instruction data for LLMs, which is typically proprietary or costly to obtain through human annotation.

**Method:** The framework uses aligned long-context LLMs to generate contextually relevant queries by auto-regressing document-query pairs, harvesting them alongside model responses to create high-quality instructions.

**Key Contributions:**

	1. Introduction of LongMagpie framework for self-synthesizing long-context instruction data
	2. Demonstration of superior long-context task performance
	3. Elimination of the need for human annotation in instruction generation

**Result:** LongMagpie demonstrates superior performance on long-context tasks and competitive results on short-context tasks through experiments involving HELMET, RULER, and Longbench v2.

**Limitations:** 

**Conclusion:** LongMagpie is an effective and scalable solution for synthesizing open and diverse long-context instruction data, beneficial for advancing LLM applications.

**Abstract:** High-quality long-context instruction data is essential for aligning long-context large language models (LLMs). Despite the public release of models like Qwen and Llama, their long-context instruction data remains proprietary. Human annotation is costly and challenging, while template-based synthesis methods limit scale, diversity, and quality. We introduce LongMagpie, a self-synthesis framework that automatically generates large-scale long-context instruction data. Our key insight is that aligned long-context LLMs, when presented with a document followed by special tokens preceding a user turn, auto-regressively generate contextually relevant queries. By harvesting these document-query pairs and the model's responses, LongMagpie produces high-quality instructions without human effort. Experiments on HELMET, RULER, and Longbench v2 demonstrate that LongMagpie achieves leading performance on long-context tasks while maintaining competitive performance on short-context tasks, establishing it as a simple and effective approach for open, diverse, and scalable long-context instruction data synthesis.

</details>


### [79] [When can isotropy help adapt LLMs' next word prediction to numerical domains?](https://arxiv.org/abs/2505.17135)

*Rashed Shelim, Shengzhe Xu, Walid Saad, Naren Ramakrishnan*

**Main category:** cs.CL

**Keywords:** large language models, numeric data, isotropy, self-attention, explainability

**Relevance Score:** 9

**TL;DR:** This paper analyzes the adaptation of pre-trained LLMs for numerical downstream tasks, focusing on ensuring reliable predictions through the isotropic structure of contextual embeddings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination problem of LLMs in numerical domains and provide performance guarantees through theoretical understanding and explainability.

**Method:** The paper utilizes a log-linear model to analyze how LLMs can predict numeric data based on contextual embeddings, particularly focusing on the isotropy of these embeddings.

**Key Contributions:**

	1. Introduced a novel analysis based on isotropy for LLMs in numeric tasks.
	2. Provided a framework to achieve state-of-the-art performance through understanding the shift-invariance of softmax.
	3. Demonstrated the impact of model architecture on the isotropy of numeric data representations.

**Result:** Demonstrates that the isotropic property of LLM embeddings preserves the underlying structure of representations, resolving the shift-invariance issue and enhancing performance in numerical tasks.

**Limitations:** 

**Conclusion:** Establishing the connection between isotropy and LLM performance in numeric domains allows for better prediction reliability and accuracy, contributing to the theoretical understanding of LLMs.

**Abstract:** Recent studies have shown that vector representations of contextual embeddings learned by pre-trained large language models (LLMs) are effective in various downstream tasks in numerical domains. Despite their significant benefits, the tendency of LLMs to hallucinate in such domains can have severe consequences in applications such as energy, nature, finance, healthcare, retail and transportation, among others. To guarantee prediction reliability and accuracy in numerical domains, it is necessary to open the black-box and provide performance guarantees through explanation. However, there is little theoretical understanding of when pre-trained language models help solve numeric downstream tasks. This paper seeks to bridge this gap by understanding when the next-word prediction capability of LLMs can be adapted to numerical domains through a novel analysis based on the concept of isotropy in the contextual embedding space. Specifically, we consider a log-linear model for LLMs in which numeric data can be predicted from its context through a network with softmax in the output layer of LLMs (i.e., language model head in self-attention). We demonstrate that, in order to achieve state-of-the-art performance in numerical domains, the hidden representations of the LLM embeddings must possess a structure that accounts for the shift-invariance of the softmax function. By formulating a gradient structure of self-attention in pre-trained models, we show how the isotropic property of LLM embeddings in contextual embedding space preserves the underlying structure of representations, thereby resolving the shift-invariance problem and providing a performance guarantee. Experiments show that different characteristics of numeric data and model architecture could have different impacts on isotropy.

</details>


### [80] [Foundation Models for Geospatial Reasoning: Assessing Capabilities of Large Language Models in Understanding Geometries and Topological Spatial Relations](https://arxiv.org/abs/2505.17136)

*Yuhan Ji, Song Gao, Ying Nie, Ivan Maji, Krzysztof Janowicz*

**Main category:** cs.CL

**Keywords:** geospatial reasoning, large language models, topological relations, geometry embedding, prompt engineering

**Relevance Score:** 7

**TL;DR:** The study evaluates how well large language models (LLMs) handle geospatial reasoning tasks using well-known-text (WKT) representations, revealing strengths in their accuracy when employing embedding and prompt engineering techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the limitations of AI foundation models in understanding and reasoning with geospatial data, specifically vector geometries and spatial relations, and to propose approaches that enhance their performance in these tasks.

**Method:** The research employs three distinct methodologies for spatial reasoning tasks: geometry embedding-based, prompt engineering-based, and everyday language-based evaluations, testing various LLMs including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B.

**Key Contributions:**

	1. Demonstration of LLMs' capabilities in geospatial reasoning using WKT representations.
	2. Identification of the most effective model and approach for topological spatial relation inference (GPT-4 with few-shot prompting).
	3. Insights for improving LLMs to support geographic entity retrieval and enhance spatial understanding.

**Result:** Embedding-based and prompt engineering approaches achieved an average accuracy over 0.6 in identifying topological spatial relations. GPT-4 performed the best with an accuracy exceeding 0.66, successfully identifying inverse relations and enhancing geographic entity retrieval.

**Limitations:** The effectiveness of the context variations in prompts showed inconsistent improvements in inference accuracy based on the instance.

**Conclusion:** The findings indicate that incorporating geometry and place context improves inference accuracy in LLMs for geospatial reasoning and contribute to the development of geo-foundation models that can effectively understand geographical entities.

**Abstract:** Applying AI foundation models directly to geospatial datasets remains challenging due to their limited ability to represent and reason with geographical entities, specifically vector-based geometries and natural language descriptions of complex spatial relations. To address these issues, we investigate the extent to which a well-known-text (WKT) representation of geometries and their spatial relations (e.g., topological predicates) are preserved during spatial reasoning when the geospatial vector data are passed to large language models (LLMs) including GPT-3.5-turbo, GPT-4, and DeepSeek-R1-14B. Our workflow employs three distinct approaches to complete the spatial reasoning tasks for comparison, i.e., geometry embedding-based, prompt engineering-based, and everyday language-based evaluation. Our experiment results demonstrate that both the embedding-based and prompt engineering-based approaches to geospatial question-answering tasks with GPT models can achieve an accuracy of over 0.6 on average for the identification of topological spatial relations between two geometries. Among the evaluated models, GPT-4 with few-shot prompting achieved the highest performance with over 0.66 accuracy on topological spatial relation inference. Additionally, GPT-based reasoner is capable of properly comprehending inverse topological spatial relations and including an LLM-generated geometry can enhance the effectiveness for geographic entity retrieval. GPT-4 also exhibits the ability to translate certain vernacular descriptions about places into formal topological relations, and adding the geometry-type or place-type context in prompts may improve inference accuracy, but it varies by instance. The performance of these spatial reasoning tasks offers valuable insights for the refinement of LLMs with geographical knowledge towards the development of geo-foundation models capable of geospatial reasoning.

</details>


### [81] [Cog-TiPRO: Iterative Prompt Refinement with LLMs to Detect Cognitive Decline via Longitudinal Voice Assistant Commands](https://arxiv.org/abs/2505.17137)

*Kristin Qi, Youxiang Zhu, Caroline Summerour, John A. Batsis, Xiaohui Liang*

**Main category:** cs.CL

**Keywords:** Cognitive Decline, Voice Assistant Systems, Machine Learning, Speech Analysis, Neurodegenerative Diseases

**Relevance Score:** 9

**TL;DR:** This study explores the use of voice assistant systems to detect cognitive decline by analyzing speech patterns in voice commands from older adults.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Early detection of cognitive decline is essential for timely interventions, but traditional assessment methods are impractical for frequent monitoring.

**Method:** The study utilizes Cog-TiPRO, a framework that integrates LLM-driven prompt refinement for linguistic feature extraction, HuBERT for acoustic feature extraction, and transformer-based modeling to analyze voice commands collected over 18 months from older adults.

**Key Contributions:**

	1. Introduction of Cog-TiPRO framework for analyzing voice commands.
	2. Demonstration of LLM's role in identifying linguistic features indicative of cognitive decline.
	3. Achievement of high accuracy in MCI detection compared to traditional methods.

**Result:** The method achieved 73.80% accuracy and 72.67% F1-score in detecting mild cognitive impairment (MCI), surpassing the baseline by 27.13%.

**Limitations:** 

**Conclusion:** The findings demonstrate that voice commands can serve as effective indicators of cognitive decline, providing a valuable tool for longitudinal health monitoring.

**Abstract:** Early detection of cognitive decline is crucial for enabling interventions that can slow neurodegenerative disease progression. Traditional diagnostic approaches rely on labor-intensive clinical assessments, which are impractical for frequent monitoring. Our pilot study investigates voice assistant systems (VAS) as non-invasive tools for detecting cognitive decline through longitudinal analysis of speech patterns in voice commands. Over an 18-month period, we collected voice commands from 35 older adults, with 15 participants providing daily at-home VAS interactions. To address the challenges of analyzing these short, unstructured and noisy commands, we propose Cog-TiPRO, a framework that combines (1) LLM-driven iterative prompt refinement for linguistic feature extraction, (2) HuBERT-based acoustic feature extraction, and (3) transformer-based temporal modeling. Using iTransformer, our approach achieves 73.80% accuracy and 72.67% F1-score in detecting MCI, outperforming its baseline by 27.13%. Through our LLM approach, we identify linguistic features that uniquely characterize everyday command usage patterns in individuals experiencing cognitive decline.

</details>


### [82] [EarthSE: A Benchmark Evaluating Earth Scientific Exploration Capability for Large Language Models](https://arxiv.org/abs/2505.17139)

*Wanghan Xu, Xiangyu Zhao, Yuhao Zhou, Xiaoyu Yue, Ben Fei, Fenghua Ling, Wenlong Zhang, Lei Bai*

**Main category:** cs.CL

**Keywords:** Large Language Models, Earth sciences, benchmark, scientific exploration, open-ended dialogue

**Relevance Score:** 9

**TL;DR:** This paper presents a comprehensive benchmark for evaluating Large Language Models (LLMs) in Earth sciences, creating extensive QA datasets and open-ended multi-turn dialogue scenarios to assess scientific exploration capabilities.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing interest in applying LLMs to scientific fields, especially Earth sciences, but existing benchmarks do not adequately address the need for specialized assessments that encompass the breadth and depth required.

**Method:** The authors constructed two QA datasets, Earth-Iron for broad question coverage and Earth-Silver for evaluating professional depth, along with Earth-Gold for open-ended dialogues. They assessed 11 leading LLMs to evaluate their capabilities across these new datasets.

**Key Contributions:**

	1. Development of Earth-Iron and Earth-Silver QA datasets for evaluating LLMs in Earth sciences.
	2. Introduction of Earth-Gold dataset for testing open-ended dialogue capabilities of LLMs.
	3. Comprehensive analysis of LLMs' limitations in scientific exploration across various tasks.

**Result:** Experiments showed significant limitations in the tested LLMs, indicating they require improvements in their ability to handle scientific exploration tasks effectively, particularly in the Earth sciences domain.

**Limitations:** The benchmark may still need validation with additional datasets and does not encompass all possible Earth science queries or domains.

**Conclusion:** The benchmark established in this paper facilitates a more nuanced evaluation of LLMs in scientific contexts, offering resources for further development in AI's application to Earth sciences.

**Abstract:** Advancements in Large Language Models (LLMs) drive interest in scientific applications, necessitating specialized benchmarks such as Earth science. Existing benchmarks either present a general science focus devoid of Earth science specificity or cover isolated subdomains, lacking holistic evaluation. Furthermore, current benchmarks typically neglect the assessment of LLMs' capabilities in open-ended scientific exploration. In this paper, we present a comprehensive and professional benchmark for the Earth sciences, designed to evaluate the capabilities of LLMs in scientific exploration within this domain, spanning from fundamental to advanced levels. Leveraging a corpus of 100,000 research papers, we first construct two Question Answering (QA) datasets: Earth-Iron, which offers extensive question coverage for broad assessment, and Earth-Silver, which features a higher level of difficulty to evaluate professional depth. These datasets encompass five Earth spheres, 114 disciplines, and 11 task categories, assessing foundational knowledge crucial for scientific exploration. Most notably, we introduce Earth-Gold with new metrics, a dataset comprising open-ended multi-turn dialogues specifically designed to evaluate the advanced capabilities of LLMs in scientific exploration, including methodology induction, limitation analysis, and concept proposal. Extensive experiments reveal limitations in 11 leading LLMs across different domains and tasks, highlighting considerable room for improvement in their scientific exploration capabilities. The benchmark is available on https://huggingface.co/ai-earth .

</details>


### [83] [Data Doping or True Intelligence? Evaluating the Transferability of Injected Knowledge in LLMs](https://arxiv.org/abs/2505.17140)

*Essa Jan, Moiz Ali, Muhammad Saram Hassan, Fareed Zaffar, Yasir Zaki*

**Main category:** cs.CL

**Keywords:** large language models, knowledge retention, fine-tuning, task selection, cognitive engagement

**Relevance Score:** 9

**TL;DR:** This study explores efficient methods to update large language models (LLMs) by comparing task types affecting knowledge retention. Comprehension-intensive tasks show higher rates than mapping-oriented ones, and larger models generally retain more knowledge but struggle with broader contextual application.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient methods to update large language models, especially when injecting proprietary information, has grown due to outdated knowledge.

**Method:** The study compares comprehension-intensive fine-tuning tasks (e.g., question answering) against mapping-oriented tasks (e.g., translation) in terms of knowledge retention rates across different model architectures.

**Key Contributions:**

	1. Comparative analysis of task types affecting LLM knowledge retention
	2. Demonstrated impact of model size on retention rates
	3. Identified limitations of semantic integration during knowledge application

**Result:** Comprehension-intensive tasks achieve significantly higher knowledge retention rates (48%) compared to mapping-oriented tasks (17% for translation and 20% for text-to-JSON) and this pattern persists across model architectures with larger models performing better.

**Limitations:** All models demonstrate significant performance drops when applying injected knowledge in broader contexts, indicating limited semantic integration.

**Conclusion:** Effective knowledge injection in LLMs relies on task selection, emphasizing cognitive engagement during fine-tuning rather than merely data exposure, as all models show performance drops in broader applications.

**Abstract:** As the knowledge of large language models (LLMs) becomes outdated over time, there is a growing need for efficient methods to update them, especially when injecting proprietary information. Our study reveals that comprehension-intensive fine-tuning tasks (e.g., question answering and blanks) achieve substantially higher knowledge retention rates (48%) compared to mapping-oriented tasks like translation (17%) or text-to-JSON conversion (20%), despite exposure to identical factual content. We demonstrate that this pattern persists across model architectures and follows scaling laws, with larger models showing improved retention across all task types. However, all models exhibit significant performance drops when applying injected knowledge in broader contexts, suggesting limited semantic integration. These findings show the importance of task selection in updating LLM knowledge, showing that effective knowledge injection relies not just on data exposure but on the depth of cognitive engagement during fine-tuning.

</details>


### [84] [MDIT-Bench: Evaluating the Dual-Implicit Toxicity in Large Multimodal Models](https://arxiv.org/abs/2505.17144)

*Bohan Jin, Shuhan Qi, Kehai Chen, Xinyi Guo, Xuan Wang*

**Main category:** cs.CL

**Keywords:** dual-implicit toxicity, LMM, benchmark, MDIT-Bench, toxicity evaluation

**Relevance Score:** 6

**TL;DR:** This paper introduces a new type of toxicitydual-implicit toxicityand a benchmark (MDIT-Bench) to evaluate Large Multimodal Models' performance on this aspect.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current research primarily focuses on explicit toxicity in LMMs, neglecting implicit forms like prejudice and discrimination.

**Method:** A dataset (MDIT-Dataset) was created using a Multi-stage Human-in-loop In-context Generation method, followed by the construction of the MDIT-Bench to evaluate model sensitivity to dual-implicit toxicity with a wide range of questions.

**Key Contributions:**

	1. Introduction of the concept of dual-implicit toxicity.
	2. Creation of the MDIT-Dataset for training and evaluation.
	3. Establishment of the MDIT-Bench as a comprehensive benchmarking tool for assessing LMMs.

**Result:** Experiments conducted on 13 prominent LMMs revealed that these models struggle with dual-implicit toxicity, particularly at harder difficulty levels.

**Limitations:** The focus is exclusively on LMMs; implications for other model types are not discussed.

**Conclusion:** The findings indicate that these models possess significant hidden toxicity that remains activatable, which necessitates further research and improvements.

**Abstract:** The widespread use of Large Multimodal Models (LMMs) has raised concerns about model toxicity. However, current research mainly focuses on explicit toxicity, with less attention to some more implicit toxicity regarding prejudice and discrimination. To address this limitation, we introduce a subtler type of toxicity named dual-implicit toxicity and a novel toxicity benchmark termed MDIT-Bench: Multimodal Dual-Implicit Toxicity Benchmark. Specifically, we first create the MDIT-Dataset with dual-implicit toxicity using the proposed Multi-stage Human-in-loop In-context Generation method. Based on this dataset, we construct the MDIT-Bench, a benchmark for evaluating the sensitivity of models to dual-implicit toxicity, with 317,638 questions covering 12 categories, 23 subcategories, and 780 topics. MDIT-Bench includes three difficulty levels, and we propose a metric to measure the toxicity gap exhibited by the model across them. In the experiment, we conducted MDIT-Bench on 13 prominent LMMs, and the results show that these LMMs cannot handle dual-implicit toxicity effectively. The model's performance drops significantly in hard level, revealing that these LMMs still contain a significant amount of hidden but activatable toxicity. Data are available at https://github.com/nuo1nuo/MDIT-Bench.

</details>


### [85] [Large Language Models for Predictive Analysis: How Far Are They?](https://arxiv.org/abs/2505.17149)

*Qin Chen, Yuanyi Ren, Xiaojun Ma, Yuyang Shi*

**Main category:** cs.CL

**Keywords:** Predictive analysis, Large language models, Benchmark, Text analysis, Code generation

**Relevance Score:** 9

**TL;DR:** The paper introduces PredictiQ, a benchmark for evaluating the capabilities of large language models (LLMs) in predictive analysis, using 1130 queries from diverse datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of systematic assessments of LLMs for predictive analysis, and to meet the rising expectations of utilizing these models in complex decision-making tasks.

**Method:** We created the PredictiQ benchmark that consists of 1130 predictive analysis queries sourced from 44 datasets across 8 fields, and designed an evaluation protocol to assess LLMs in text analysis and code generation.

**Key Contributions:**

	1. Introduction of the PredictiQ benchmark for predictive analysis
	2. Evaluation of twelve LLMs on a common set of challenges
	3. Insights on the limitations of current LLMs in predictive tasks

**Result:** The evaluation of twelve renowned LLMs revealed that, despite their advances, they continue to face significant challenges in performing predictive analysis effectively.

**Limitations:** The benchmark may not cover all potential use cases in predictive analysis and relies on the selected datasets and queries, which may introduce biases.

**Conclusion:** The study emphasizes the need for improved LLMs in predictive tasks and provides a standardized framework for future evaluations.

**Abstract:** Predictive analysis is a cornerstone of modern decision-making, with applications in various domains. Large Language Models (LLMs) have emerged as powerful tools in enabling nuanced, knowledge-intensive conversations, thus aiding in complex decision-making tasks. With the burgeoning expectation to harness LLMs for predictive analysis, there is an urgent need to systematically assess their capability in this domain. However, there is a lack of relevant evaluations in existing studies. To bridge this gap, we introduce the \textbf{PredictiQ} benchmark, which integrates 1130 sophisticated predictive analysis queries originating from 44 real-world datasets of 8 diverse fields. We design an evaluation protocol considering text analysis, code generation, and their alignment. Twelve renowned LLMs are evaluated, offering insights into their practical use in predictive analysis. Generally, we believe that existing LLMs still face considerable challenges in conducting predictive analysis. See \href{https://github.com/Cqkkkkkk/PredictiQ}{Github}.

</details>


### [86] [Bayesian Optimization for Enhanced Language Models: Optimizing Acquisition Functions](https://arxiv.org/abs/2505.17151)

*Zishuo Bao, Yibo Liu, Changyutao Qiu*

**Main category:** cs.CL

**Keywords:** Bilevel Bayesian Optimization, Fine-tuning, Acquisition Functions, Large Language Models, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper presents a new approach for fine-tuning large language models using a bilevel Bayesian optimization strategy with a mixture of acquisition functions to enhance performance on downstream tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing complexity of language model architectures necessitates effective hyperparameter tuning strategies. Traditional methods often use fixed acquisition functions without considering their sensitivity to training and validation performance.

**Method:** The authors propose a bilevel Bayesian optimization (Bilevel-BO-SWA) framework that incorporates a fusion of acquisition functions like Expected Improvement (EI) and Upper Confidence Bound (UCB) within nested optimization loops to fine-tune large language models effectively.

**Key Contributions:**

	1. Introduction of a bilevel Bayesian optimization strategy for LLM fine-tuning
	2. Fusion of multiple acquisition functions for improved performance
	3. Demonstration of significant improvements on GLUE tasks

**Result:** Experimental results on GLUE tasks indicate that the proposed method improves generalization and fine-tuning efficiency by up to 2.7% compared to existing approaches.

**Limitations:** The paper focuses on specific tasks (GLUE) and may require further validation across other datasets and model architectures.

**Conclusion:** Bilevel-BO-SWA demonstrates significant improvement in fine-tuning large language models by strategically selecting acquisition functions based on their sensitivity to performance metrics.

**Abstract:** With the rise of different language model architecture, fine-tuning is becoming even more important for down stream tasks Model gets messy, finding proper hyperparameters for fine-tuning. Although BO has been tried for hyperparameter tuning, most of the existing methods are oblivious to the fact that BO relies on careful choices of acquisition functions, which are essential components of BO that guide how much to explore versus exploit during the optimization process; Different acquisition functions have different levels of sensitivity towards training loss and validation performance; existing methods often just apply an acquisition function no matter if the training and validation performance are sensitive to the acquisition function or not. This work introduces{Bilevel - BO - SWA}, a model fusion approach coupled with a bilevel BO strategy to improve the fine - tunning of large language models. Our work on mixture of acquisition functions like EI and UCB into nested opt loops, where inner loop perform minimization of training loss while outer loops optimized w.r.t. val metric. Experiments on GLUE tasks using RoBERTA - base show that when using EI and UCB, there is an improvement in generalization, and fine - tuning can be improved by up to 2.7%.

</details>


### [87] [Amplify Adjacent Token Differences: Enhancing Long Chain-of-Thought Reasoning with Shift-FFN](https://arxiv.org/abs/2505.17153)

*Yao Xu, Mingyu Xu, Fangyu Lei, Wangtao Sun, Xiangrong Zeng, Bingning Wang, Guang Liu, Shizhu He, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** Long Chain-of-Thought, Cyclical Reasoning, Shift Feedforward Networks, Machine Learning, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** This paper addresses the issue of Cyclical Reasoning in LLMs when fine-tuned on Long Chain-of-Thought data and proposes a novel architecture, Shift Feedforward Networks (Shift-FFN), to mitigate this problem.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper discusses the challenges of Cyclical Reasoning in large language models when using Long Chain-of-Thought reasoning techniques, which can hinder model performance on complex tasks.

**Method:** The authors propose Shift Feedforward Networks (Shift-FFN) that adjust the current token's representation with the previous token's representation before passing it through the feedforward network, thereby increasing representation differences between adjacent tokens.

**Key Contributions:**

	1. Introduction of Shift Feedforward Networks (Shift-FFN) to address Cyclical Reasoning
	2. Demonstration of improved model accuracy on mathematical reasoning tasks
	3. Analysis of representation differences leading to Cyclical Reasoning

**Result:** Experiments show that models using LoRA with Shift-FFN outperform those using full fine-tuning and standard LoRA in terms of accuracy and reduce the incidence of Cyclical Reasoning across various mathematical reasoning tasks.

**Limitations:** 

**Conclusion:** Shift-FFN significantly enhances the accuracy of student models trained on long Chain-of-Thought data while reducing the tendency toward Cyclical Reasoning, showcasing its effectiveness in improving reasoning capabilities.

**Abstract:** Recently, models such as OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable performance on complex reasoning tasks through Long Chain-of-Thought (Long-CoT) reasoning. Although distilling this capability into student models significantly enhances their performance, this paper finds that fine-tuning LLMs with full parameters or LoRA with a low rank on long CoT data often leads to Cyclical Reasoning, where models repeatedly reiterate previous inference steps until the maximum length limit. Further analysis reveals that smaller differences in representations between adjacent tokens correlates with a higher tendency toward Cyclical Reasoning. To mitigate this issue, this paper proposes Shift Feedforward Networks (Shift-FFN), a novel approach that edits the current token's representation with the previous one before inputting it to FFN. This architecture dynamically amplifies the representation differences between adjacent tokens. Extensive experiments on multiple mathematical reasoning tasks demonstrate that LoRA combined with Shift-FFN achieves higher accuracy and a lower rate of Cyclical Reasoning across various data sizes compared to full fine-tuning and standard LoRA. Our data and code are available at https://anonymous.4open.science/r/Shift-FFN

</details>


### [88] [PersonaBOT: Bringing Customer Personas to Life with LLMs and RAG](https://arxiv.org/abs/2505.17156)

*Muhammed Rizwan, Lars Carlsson, Mohammad Loni*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, synthetic customer personas

**Relevance Score:** 9

**TL;DR:** This paper discusses the development of a persona-based RAG chatbot at Volvo Construction Equipment that incorporates synthetic customer personas using Few-Shot and Chain-of-Thought prompting techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the development of customer personas, traditionally a time-consuming process, and improve decision-making in business processes through the integration of synthetic personas into a chatbot.

**Method:** Developed a persona-based RAG chatbot, generated synthetic personas using Few-Shot and Chain-of-Thought prompting, and evaluated their effectiveness using McNemar's test. The chatbot's knowledge base was augmented with synthetic personas and assessed for improvements in response accuracy.

**Key Contributions:**

	1. Development of a persona-based RAG chatbot with synthetic personas
	2. Implementation of Few-Shot and Chain-of-Thought prompting techniques
	3. Assessment of chatbot response accuracy improvements through augmentation methods

**Result:** Few-Shot prompting outperformed Chain-of-Thought in generating complete personas; after augmentation, the chatbot's accuracy rating improved from 5.88 to 6.42, with 81.82% of participants finding it useful in business contexts.

**Limitations:** None specified in the abstract.

**Conclusion:** Integrating synthetic customer personas enhanced the RAG chatbot's effectiveness, demonstrating the value of advanced NLP techniques in business decision-making.

**Abstract:** The introduction of Large Language Models (LLMs) has significantly transformed Natural Language Processing (NLP) applications by enabling more advanced analysis of customer personas. At Volvo Construction Equipment (VCE), customer personas have traditionally been developed through qualitative methods, which are time-consuming and lack scalability. The main objective of this paper is to generate synthetic customer personas and integrate them into a Retrieval-Augmented Generation (RAG) chatbot to support decision-making in business processes. To this end, we first focus on developing a persona-based RAG chatbot integrated with verified personas. Next, synthetic personas are generated using Few-Shot and Chain-of-Thought (CoT) prompting techniques and evaluated based on completeness, relevance, and consistency using McNemar's test. In the final step, the chatbot's knowledge base is augmented with synthetic personas and additional segment information to assess improvements in response accuracy and practical utility. Key findings indicate that Few-Shot prompting outperformed CoT in generating more complete personas, while CoT demonstrated greater efficiency in terms of response time and token usage. After augmenting the knowledge base, the average accuracy rating of the chatbot increased from 5.88 to 6.42 on a 10-point scale, and 81.82% of participants found the updated system useful in business contexts.

</details>


### [89] [Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting](https://arxiv.org/abs/2505.17160)

*Bang Trinh Tran To, Thai Le*

**Main category:** cs.CL

**Keywords:** unlearning, large language models, adversarial prompting, knowledge retention, evaluation

**Relevance Score:** 8

**TL;DR:** LURK is a framework that detects hidden knowledge in unlearned LLMs using adversarial suffix prompting.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore limitations in current unlearning evaluation techniques for large language models (LLMs).

**Method:** The framework generates adversarial prompt suffixes targeting idiosyncratic information retention in LLMs, specifically focusing on the Harry Potter domain.

**Key Contributions:**

	1. Introduction of LURK framework for probing unlearned knowledge in LLMs
	2. Evidence that unlearned LLMs can leak information under specific conditions
	3. Advancement of unlearning evaluation methodologies.

**Result:** Experiments show that even models considered unlearned can reveal sensitive knowledge under adversarial conditions, indicating flaws in existing assessment standards.

**Limitations:** The focus on a single domain (Harry Potter) may limit generalizability to other domains.

**Conclusion:** LURK serves as a powerful tool for evaluating unlearning algorithms, revealing that hidden knowledge may persist even in seemingly unlearned models.

**Abstract:** This work presents LURK (Latent UnleaRned Knowledge), a novel framework that probes for hidden retained knowledge in unlearned LLMs through adversarial suffix prompting. LURK automatically generates adversarial prompt suffixes designed to elicit residual knowledge about the Harry Potter domain, a commonly used benchmark for unlearning. Our experiments reveal that even models deemed successfully unlearned can leak idiosyncratic information under targeted adversarial conditions, highlighting critical limitations of current unlearning evaluation standards. By uncovering latent knowledge through indirect probing, LURK offers a more rigorous and diagnostic tool for assessing the robustness of unlearning algorithms. All code will be publicly available.

</details>


### [90] [CRG Score: A Distribution-Aware Clinical Metric for Radiology Report Generation](https://arxiv.org/abs/2505.17167)

*Ibrahim Ethem Hamamci, Sezgin Er, Suprosanna Shit, Hadrien Reynaud, Bernhard Kainz, Bjoern Menze*

**Main category:** cs.CL

**Keywords:** CRG Score, long-context radiology, NLG metrics, clinical evaluation, LLM

**Relevance Score:** 7

**TL;DR:** The paper introduces the CRG Score, a new metric for evaluating clinical radiology report generation that addresses challenges in existing evaluation metrics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating long-context radiology report generation accurately is critical, yet current NLG and LLM metrics struggle to maintain clinical relevance and generalizability.

**Method:** The CRG Score is designed to evaluate clinically relevant abnormalities in reference reports while supporting binary and structured labels. It adapts penalties based on label distribution to ensure fair evaluation.

**Key Contributions:**

	1. Introduction of the CRG Score for clinical evaluation of radiology reports
	2. Balancing penalties based on label distribution for fair assessments
	3. Support for both binary and structured labels in clinical reports

**Result:** The CRG Score provides a more balanced and robust metric for assessing clinical accuracy in radiology report generation, improving upon existing methods affected by class imbalance.

**Limitations:** 

**Conclusion:** By offering a clinically aligned reward function that emphasizes relevant abnormalities, the CRG Score enhances the evaluation of radiology report generation using LLMs.

**Abstract:** Evaluating long-context radiology report generation is challenging. NLG metrics fail to capture clinical correctness, while LLM-based metrics often lack generalizability. Clinical accuracy metrics are more relevant but are sensitive to class imbalance, frequently favoring trivial predictions. We propose the CRG Score, a distribution-aware and adaptable metric that evaluates only clinically relevant abnormalities explicitly described in reference reports. CRG supports both binary and structured labels (e.g., type, location) and can be paired with any LLM for feature extraction. By balancing penalties based on label distribution, it enables fairer, more robust evaluation and serves as a clinically aligned reward function.

</details>


### [91] [Next Token Perception Score: Analytical Assessment of your LLM Perception Skills](https://arxiv.org/abs/2505.17169)

*Yu-Ang Cheng, Leyang Hu, Hai Huang, Randall Balestriero*

**Main category:** cs.CL

**Keywords:** large language models, autoregressive pretraining, Next Token Perception Score

**Relevance Score:** 8

**TL;DR:** The paper introduces the Next Token Perception Score (NTPS) to measure the alignment of autoregressive pretraining in LLMs with downstream perception tasks, showing strong empirical correlation with task performance and validation of fine-tuning methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the variability in downstream task performance from autoregressive pretraining in large language models and understand the alignment between pretraining and perception tasks.

**Method:** The authors introduce the Next Token Perception Score (NTPS) which quantifies the alignment between features learned through autoregression and those required for perception tasks, validated across various NLP datasets and models.

**Key Contributions:**

	1. Introduction of the Next Token Perception Score (NTPS).
	2. Demonstration of strong correlation between NTPS and downstream performance.
	3. Validation of NTPS as a predictive tool for LoRA fine-tuning accuracy gains.

**Result:** The NTPS shows strong correlation with linear probe accuracy and increases after LoRA fine-tuning, indicating improved task alignment. It also predicts accuracy gains from LoRA effectively.

**Limitations:** 

**Conclusion:** NTPS is a valuable metric for assessing LLM perception alignment and can enhance the fine-tuning process by identifying the suitability of models for specific perception tasks.

**Abstract:** Autoregressive pretraining has become the de facto paradigm for learning general-purpose representations in large language models (LLMs). However, linear probe performance across downstream perception tasks shows substantial variability, suggesting that features optimized for next-token prediction do not consistently transfer well to downstream perception tasks. We demonstrate that representations learned via autoregression capture features that may lie outside the subspaces most informative for perception. To quantify the (mis)alignment between autoregressive pretraining and downstream perception, we introduce the Next Token Perception Score (NTPS)-a score derived under a linear setting that measures the overlap between autoregressive and perception feature subspaces. This metric can be easily computed in closed form from pretrained representations and labeled data, and is proven to both upper- and lower-bound the excess loss. Empirically, we show that NTPS correlates strongly with linear probe accuracy across 12 diverse NLP datasets and eight pretrained models ranging from 270M to 8B parameters, confirming its utility as a measure of alignment. Furthermore, we show that NTPS increases following low-rank adaptation (LoRA) fine-tuning, especially in large models, suggesting that LoRA aligning representations to perception tasks enhances subspace overlap and thus improves downstream performance. More importantly, we find that NTPS reliably predicts the additional accuracy gains attained by LoRA finetuning thereby providing a lightweight prescreening tool for LoRA adaptation. Our results offer both theoretical insights and practical tools for analytically assessing LLM perception skills.

</details>


### [92] [FB-RAG: Improving RAG with Forward and Backward Lookup](https://arxiv.org/abs/2505.17206)

*Kushal Chawla, Alfy Samuel, Anoop Kumar, Daben Liu*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, context retrieval, language models

**Relevance Score:** 9

**TL;DR:** This paper introduces FB-RAG, a novel framework that enhances Retrieval Augmented Generation (RAG) systems by improving context retrieval for complex queries, thus outperforming traditional models in both performance and latency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of RAG systems by managing the complexity of context retrieval for queries with limited information.

**Method:** FB-RAG utilizes a dual approach of backward and forward lookup to selectively retrieve context chunks that are most relevant for query answering.

**Key Contributions:**

	1. Introduction of FB-RAG framework for enhanced query-context matching
	2. Demonstrated performance improvements across multiple benchmarks
	3. Insights on qualitative strengths and weaknesses for future research

**Result:** FB-RAG consistently outperforms existing RAG models and Long Context baselines across 9 datasets, while also reducing response latency.

**Limitations:** 

**Conclusion:** The findings highlight that FB-RAG can effectively improve information retrieval for complex queries, providing a framework for future enhancements in the field.

**Abstract:** The performance of Retrieval Augmented Generation (RAG) systems relies heavily on the retriever quality and the size of the retrieved context. A large enough context ensures that the relevant information is present in the input context for the LLM, but also incorporates irrelevant content that has been shown to confuse the models. On the other hand, a smaller context reduces the irrelevant information, but it often comes at the risk of losing important information necessary to answer the input question. This duality is especially challenging to manage for complex queries that contain little information to retrieve the relevant chunks from the full context. To address this, we present a novel framework, called FB-RAG, which enhances the RAG pipeline by relying on a combination of backward lookup (overlap with the query) and forward lookup (overlap with candidate reasons and answers) to retrieve specific context chunks that are the most relevant for answering the input query. Our evaluations on 9 datasets from two leading benchmarks show that FB-RAG consistently outperforms RAG and Long Context baselines developed recently for these benchmarks. We further show that FB-RAG can improve performance while reducing latency. We perform qualitative analysis of the strengths and shortcomings of our approach, providing specific insights to guide future work.

</details>


### [93] [Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs](https://arxiv.org/abs/2505.17217)

*Kangda Wei, Hasnat Md Abdullah, Ruihong Huang*

**Main category:** cs.CL

**Keywords:** large language models, gender bias, moral judgments, data generation, Direct Preference Optimization

**Relevance Score:** 9

**TL;DR:** This paper presents a framework to reduce gender bias in LLMs by generating story pairs with male and female protagonists in morally ambiguous scenarios, prompting the models to make balanced moral judgments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the issue of gender bias in Large Language Models (LLMs) that leads to unequal treatment of genders across different contexts.

**Method:** A novel data generation framework that creates story pairs with male and female protagonists in identical scenarios. The model generates moral judgments, which are compared and guided towards gender-neutral responses using Direct Preference Optimization (DPO).

**Key Contributions:**

	1. Introduction of a data generation framework for LLMs to explore gender bias.
	2. Methodology for guiding models towards balanced moral judgments.
	3. Experimental validation showing reduction in gender bias while maintaining model capabilities.

**Result:** The proposed method significantly reduces gender bias in LLMs while maintaining or improving overall model performance.

**Limitations:** The approach is dependent on the quality of the generated story pairs and may not generalize to all contexts.

**Conclusion:** The framework facilitates more equitable treatment of genders in LLM outputs, demonstrating the potential for enhancing model fairness.

**Abstract:** Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data.

</details>


### [94] [Humans Hallucinate Too: Language Models Identify and Correct Subjective Annotation Errors With Label-in-a-Haystack Prompts](https://arxiv.org/abs/2505.17222)

*Georgios Chochlakis, Peter Wu, Arjun Bedi, Marcus Ma, Kristina Lerman, Shrikanth Narayanan*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Large Language Models, Subjective Label Correction, Emotion Recognition, Human Annotation

**Relevance Score:** 8

**TL;DR:** This paper explores label verification in Natural Language Processing (NLP) using Large Language Models (LLMs) to address challenges in subjective task modeling, specifically focusing on emotion and morality recognition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of significant variation in human annotations for subjective tasks in NLP, which often reflects genuine semantic differences rather than just noise.

**Method:** The authors propose an In-Context Learning binary filtering baseline and introduce the Label-in-a-Haystack setting for prompt engineering, where the LLM predicts labels based on provided demonstrations and task-specific instructions. They develop the Label-in-a-Haystack Rectification (LiaHR) framework for correcting subjective labels instead of discarding them.

**Key Contributions:**

	1. Introduction of the In-Context Learning binary filtering baseline for label verification.
	2. Development of the Label-in-a-Haystack framework for subjective label correction.
	3. Demonstration of ecological validity and practical applications of the proposed methods.

**Result:** The analyses and studies conducted demonstrate the efficacy of the LiaHR framework in improving the quality of label correction and enhancing the signal-to-noise ratio in annotation workflows.

**Limitations:** 

**Conclusion:** The LiaHR approach offers a promising integration into existing annotation pipelines, enhancing the reliability of subjective labeling in NLP tasks.

**Abstract:** Modeling complex subjective tasks in Natural Language Processing, such as recognizing emotion and morality, is considerably challenging due to significant variation in human annotations. This variation often reflects reasonable differences in semantic interpretations rather than mere noise, necessitating methods to distinguish between legitimate subjectivity and error. We address this challenge by exploring label verification in these contexts using Large Language Models (LLMs). First, we propose a simple In-Context Learning binary filtering baseline that estimates the reasonableness of a document-label pair. We then introduce the Label-in-a-Haystack setting: the query and its label(s) are included in the demonstrations shown to LLMs, which are prompted to predict the label(s) again, while receiving task-specific instructions (e.g., emotion recognition) rather than label copying. We show how the failure to copy the label(s) to the output of the LLM are task-relevant and informative. Building on this, we propose the Label-in-a-Haystack Rectification (LiaHR) framework for subjective label correction: when the model outputs diverge from the reference gold labels, we assign the generated labels to the example instead of discarding it. This approach can be integrated into annotation pipelines to enhance signal-to-noise ratios. Comprehensive analyses, human evaluations, and ecological validity studies verify the utility of LiaHR for label correction. Code is available at https://github.com/gchochla/LiaHR.

</details>


### [95] [ExeSQL: Self-Taught Text-to-SQL Models with Execution-Driven Bootstrapping for SQL Dialects](https://arxiv.org/abs/2505.17231)

*Jipeng Zhang, Haolin Yang, Kehao Miao, Ruiyuan Zhang, Renjie Pi, Jiahui Gao, Xiaofang Zhou*

**Main category:** cs.CL

**Keywords:** text-to-SQL, SQL dialects, execution-driven learning, machine learning, natural language processing

**Relevance Score:** 8

**TL;DR:** ExeSQL is a text-to-SQL framework that enhances SQL generation across various dialects through execution-driven learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current text-to-SQL models struggle with performance across multiple SQL dialects due to limitations in dataset quality and execution validation. Enhancing dialect-aware capabilities is crucial for real-world application.

**Method:** ExeSQL employs an execution-driven, agentic bootstrapping method that utilizes iterative query generation, execution-based filtering (like rejection sampling), and preference-based training to refine its SQL outputs.

**Key Contributions:**

	1. Introduced an execution-driven bootstrapping method for SQL generation.
	2. Achieved significant performance improvements across multiple SQL dialects.
	3. Developed a framework that enables models to learn from execution feedback.

**Result:** ExeSQL demonstrates significant improvements in SQL generation accuracy, with average gains of 15.2%, 10.38%, and 4.49% over GPT-4o in PostgreSQL, MySQL, and Oracle, respectively.

**Limitations:** 

**Conclusion:** The framework effectively bridges the gap in text-to-SQL models for dialect variability, offering a robust solution to the limitations faced by previous approaches.

**Abstract:** Recent text-to-SQL models have achieved strong performance, but their effectiveness remains largely confined to SQLite due to dataset limitations. However, real-world applications require SQL generation across multiple dialects with varying syntax and specialized features, which remains a challenge for current models. The main obstacle in building a dialect-aware model lies in acquiring high-quality dialect-specific data. Data generated purely through static prompting - without validating SQLs via execution - tends to be noisy and unreliable. Moreover, the lack of real execution environments in the training loop prevents models from grounding their predictions in executable semantics, limiting generalization despite surface-level improvements from data filtering. This work introduces ExeSQL, a text-to-SQL framework with execution-driven, agentic bootstrapping. The method consists of iterative query generation, execution-based filtering (e.g., rejection sampling), and preference-based training, enabling the model to adapt to new SQL dialects through verifiable, feedback-guided learning. Experiments show that ExeSQL bridges the dialect gap in text-to-SQL, achieving average improvements of 15.2%, 10.38%, and 4.49% over GPT-4o on PostgreSQL, MySQL, and Oracle, respectively, across multiple datasets of varying difficulty.

</details>


### [96] [Personalizing Student-Agent Interactions Using Log-Contextualized Retrieval Augmented Generation (RAG)](https://arxiv.org/abs/2505.17238)

*Clayton Cohn, Surya Rayala, Caitlin Snyder, Joyce Fonteles, Shruti Jain, Naveeduddin Mohammed, Umesh Timalsina, Sarah K. Burriss, Ashwin T S, Namrata Srivastava, Menton Deweese, Angela Eeds, Gautam Biswas*

**Main category:** cs.CL

**Keywords:** collaborative dialogue, retrieval-augmented generation, learning analytics, pedagogical agents, critical thinking

**Relevance Score:** 9

**TL;DR:** This paper introduces log-contextualized retrieval-augmented generation (LC-RAG) to enhance pedagogical interactions in educational settings by improving retrieval accuracy using environmental logs from collaborative dialogue.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the challenges faced by pedagogical agents in understanding and interacting with students during collaborative dialogues, particularly in STEM+C education.

**Method:** The authors propose a novel approach called LC-RAG that incorporates environment logs to enhance the retrieval capabilities of existing RAG frameworks, improving the relevance of responses generated by collaborative agents.

**Key Contributions:**

	1. Introduction of LC-RAG for improved retrieval in educational contexts
	2. Demonstration of enhanced interaction quality in collaborative learning settings
	3. Empirical evidence supporting the effectiveness of LC-RAG over traditional methods

**Result:** The findings indicate that LC-RAG outperforms a baseline that relies solely on discourse by providing more relevant and personalized guidance to students, thereby enhancing critical thinking and decision-making skills during collaborative learning.

**Limitations:** 

**Conclusion:** The implementation of LC-RAG shows promise in improving the effectiveness of pedagogical agents, indicating a potential pathway for better supporting students' learning experiences in computational modeling environments.

**Abstract:** Collaborative dialogue offers rich insights into students' learning and critical thinking. This is essential for adapting pedagogical agents to students' learning and problem-solving skills in STEM+C settings. While large language models (LLMs) facilitate dynamic pedagogical interactions, potential hallucinations can undermine confidence, trust, and instructional value. Retrieval-augmented generation (RAG) grounds LLM outputs in curated knowledge, but its effectiveness depends on clear semantic links between user input and a knowledge base, which are often weak in student dialogue. We propose log-contextualized RAG (LC-RAG), which enhances RAG retrieval by incorporating environment logs to contextualize collaborative discourse. Our findings show that LC-RAG improves retrieval over a discourse-only baseline and allows our collaborative peer agent, Copa, to deliver relevant, personalized guidance that supports students' critical thinking and epistemic decision-making in a collaborative computational modeling environment, XYZ.

</details>


### [97] [ReasoningShield: Content Safety Detection over Reasoning Traces of Large Reasoning Models](https://arxiv.org/abs/2505.17244)

*Changyi Li, Jiayi Wang, Xudong Pan, Geng Hong, Min Yang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, safety detection, reasoning traces, moderation, Human-AI collaboration

**Relevance Score:** 9

**TL;DR:** The paper introduces ReasoningShield, a safety detection model designed to identify risks in reasoning traces generated by Large Reasoning Models (LRMs), aiming to improve AI transparency and risk management in AI outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for effective moderation tools to ensure the safety of AI-generated content, particularly as reasoning models become more prevalent and their outputs increasingly influence decisions.

**Method:** The authors propose ReasoningShield, employing a dataset of over 8,000 question-thought pairs in a human-AI collaborative annotation pipeline to identify risks in reasoning traces across ten categories and three safety levels.

**Key Contributions:**

	1. Introduction of the QT moderation task.
	2. Development of ReasoningShield for identifying risks in reasoning traces.
	3. Creation of a high-quality reasoning safety detection dataset.

**Result:** ReasoningShield outperforms existing moderation models with an average F1 score exceeding 0.92 on various benchmarks, successfully identifying risks within reasoning traces while delivering competitive performance on traditional unsafe Q&A detection tasks.

**Limitations:** 

**Conclusion:** The developed model demonstrates significant improvements in safety detection and provides a foundation for future research in reasoning model safety, along with the release of key resources for public use.

**Abstract:** Large Reasoning Models (LRMs) are transforming the AI landscape with advanced reasoning capabilities. While the generated reasoning traces enhance model transparency, they can still contain unsafe content, even when the final answer appears safe. Existing moderation tools, primarily designed for question-answer (QA) pairs, are empirically ineffective at detecting hidden risks embedded in reasoning traces. After identifying the key challenges, we formally define the question-thought (QT) moderation task and propose ReasoningShield, the first safety detection model tailored to identify potential risks in the reasoning trace before reaching the final answer. To construct the model, we synthesize a high-quality reasoning safety detection dataset comprising over 8,000 question-thought pairs spanning ten risk categories and three safety levels. Our dataset construction process incorporates a comprehensive human-AI collaborative annotation pipeline, which achieves over 93% annotation accuracy while significantly reducing human costs. On a diverse set of in-distribution and out-of-distribution benchmarks, ReasoningShield outperforms mainstream content safety moderation models in identifying risks within reasoning traces, with an average F1 score exceeding 0.92. Notably, despite being trained on our QT dataset only, ReasoningShield also demonstrates competitive performance in detecting unsafe question-answer pairs on traditional benchmarks, rivaling baselines trained on 10 times larger datasets and base models, which strongly validates the quality of our dataset. Furthermore, ReasoningShield is built upon compact 1B/3B base models to facilitate lightweight deployment and provides human-friendly risk analysis by default. To foster future research, we publicly release all the resources.

</details>


### [98] [ConciseRL: Conciseness-Guided Reinforcement Learning for Efficient Reasoning Models](https://arxiv.org/abs/2505.17250)

*Razvan-Gabriel Dumitru, Darius Peteleaza, Vikas Yadav, Liangming Pan*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, reasoning traces

**Relevance Score:** 8

**TL;DR:** This paper presents a novel reinforcement learning approach that uses a conciseness score to improve the efficiency and accuracy of reasoning in large language models.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate excessive computation, enhance readability, and reduce hallucination in reasoning traces generated by large language models.

**Method:** The authors introduce a hyperparameter-free conciseness score as a reward signal in a reinforcement learning framework to guide models towards generating concise reasoning. This score is evaluated by a judge model that provides dynamic feedback.

**Key Contributions:**

	1. Novel conciseness score for guiding reasoning
	2. Dynamic feedback from a judge model
	3. Significant improvements in efficiency and accuracy on benchmark datasets

**Result:** The method demonstrates state-of-the-art trade-offs on the MATH dataset, reducing token usage by up to 31x on simpler problems while improving accuracy by 7%, and achieving up to 3.6x fewer tokens on more difficult problems with a 7.5% accuracy improvement.

**Limitations:** 

**Conclusion:** The approach effectively adapts reasoning length to problem difficulty, benefiting from stronger judges, and is supported by open-sourced code and data.

**Abstract:** Large language models excel at complex tasks by breaking down problems into structured reasoning steps. However, reasoning traces often extend beyond reaching a correct answer, causing wasted computation, reduced readability, and hallucinations. To address this, we introduce a novel hyperparameter-free conciseness score used as a reward signal within a reinforcement learning framework to guide models toward generating correct and concise reasoning traces. This score is evaluated by a large language model acting as a judge, enabling dynamic, context-aware feedback beyond simple token length. Our method achieves state-of-the-art efficiency-accuracy trade-offs on the MATH dataset, reducing token usage by up to 31x on simple problems while improving accuracy by 7%, and on the hardest problems, it outperforms full reasoning by +7.5% accuracy with up to 3.6x fewer tokens. On TheoremQA, our method improves accuracy by +2.2% using 12.5x fewer tokens. We also conduct ablation studies on the judge model, reward composition, and problem difficulty, showing that our method dynamically adapts reasoning length based on problem difficulty and benefits significantly from stronger judges. The code, model weights, and datasets are open-sourced at https://github.com/RazvanDu/ConciseRL.

</details>


### [99] [The Rise of Parameter Specialization for Knowledge Storage in Large Language Models](https://arxiv.org/abs/2505.17260)

*Yihuai Hong, Yiran Zhao, Wei Tang, Yang Deng, Yu Rong, Wenxuan Zhang*

**Main category:** cs.CL

**Keywords:** large language models, MLP parameters, knowledge storage

**Relevance Score:** 6

**TL;DR:** This paper analyzes the relationship between the performance of large language models and the way knowledge is stored in MLP parameters, revealing that specialized knowledge distribution enhances efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how to better store knowledge within MLP parameters of language models to improve their utilization efficiency.

**Method:** Analyzed twenty open-source large language models, focusing on the distribution of knowledge in MLP parameters and conducting causal training experiments.

**Key Contributions:**

	1. Analyzed MLP parameter knowledge storage in large language models
	2. Demonstrated that specialization of parameters improves efficiency
	3. Validated findings through causal training experiments

**Result:** Found that as models advance, their MLP parameters specialize in encoding similar types of knowledge, which improves efficiency.

**Limitations:** 

**Conclusion:** Specialized knowledge distribution in model parameters is critical for enhancing the efficiency of knowledge utilization in language models.

**Abstract:** Over time, a growing wave of large language models from various series has been introduced to the community. Researchers are striving to maximize the performance of language models with constrained parameter sizes. However, from a microscopic perspective, there has been limited research on how to better store knowledge in model parameters, particularly within MLPs, to enable more effective utilization of this knowledge by the model. In this work, we analyze twenty publicly available open-source large language models to investigate the relationship between their strong performance and the way knowledge is stored in their corresponding MLP parameters. Our findings reveal that as language models become more advanced and demonstrate stronger knowledge capabilities, their parameters exhibit increased specialization. Specifically, parameters in the MLPs tend to be more focused on encoding similar types of knowledge. We experimentally validate that this specialized distribution of knowledge contributes to improving the efficiency of knowledge utilization in these models. Furthermore, by conducting causal training experiments, we confirm that this specialized knowledge distribution plays a critical role in improving the model's efficiency in leveraging stored knowledge.

</details>


### [100] [CaseReportBench: An LLM Benchmark Dataset for Dense Information Extraction in Clinical Case Reports](https://arxiv.org/abs/2505.17265)

*Xiao Yu Cindy Zhang, Carlos R. Ferreira, Francis Rossignol, Raymond T. Ng, Wyeth Wasserman, Jian Zhu*

**Main category:** cs.CL

**Keywords:** Inborn Errors of Metabolism, Large Language Models, Information Extraction, Rare Diseases, Clinical Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper introduces CaseReportBench, a dataset for dense information extraction from case reports on Inborn Errors of Metabolism (IEM), assessing various LLMs and prompting strategies for clinical application.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address diagnostic challenges in rare diseases through effective information extraction from underutilized case reports.

**Method:** Developed an expert-annotated dataset (CaseReportBench) and tested various language models and prompting strategies for extracting structured medical information.

**Key Contributions:**

	1. Introduction of CaseReportBench dataset for IEM case reports
	2. Assessment of different prompting strategies for LLMs
	3. Demonstration of LLMs' ability to extract clinically relevant details

**Result:** Category-specific prompting improved model performance, with Qwen2.5-7B outperforming GPT-4o; clinician evaluations show LLMs can successfully extract clinically relevant details.

**Limitations:** LLMs have limitations in recognizing negative findings crucial for differential diagnosis.

**Conclusion:** The study demonstrates the potential of LLMs in supporting rare disease diagnosis through improved extraction techniques, while indicating areas for enhancement in recognizing critical negative findings.

**Abstract:** Rare diseases, including Inborn Errors of Metabolism (IEM), pose significant diagnostic challenges. Case reports serve as key but computationally underutilized resources to inform diagnosis. Clinical dense information extraction refers to organizing medical information into structured predefined categories. Large Language Models (LLMs) may enable scalable information extraction from case reports but are rarely evaluated for this task. We introduce CaseReportBench, an expert-annotated dataset for dense information extraction of case reports, focusing on IEMs. Using this dataset, we assess various models and prompting strategies, introducing novel approaches such as category-specific prompting and subheading-filtered data integration. Zero-shot chain-of-thought prompting offers little advantage over standard zero-shot prompting. Category-specific prompting improves alignment with the benchmark. The open-source model Qwen2.5-7B outperforms GPT-4o for this task. Our clinician evaluations show that LLMs can extract clinically relevant details from case reports, supporting rare disease diagnosis and management. We also highlight areas for improvement, such as LLMs' limitations in recognizing negative findings important for differential diagnosis. This work advances LLM-driven clinical natural language processing and paves the way for scalable medical AI applications.

</details>


### [101] [Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning](https://arxiv.org/abs/2505.17266)

*Cehao Yang, Xueyuan Lin, Chengjin Xu, Xuhui Jiang, Xiaojun Wu, Honghao Liu, Hui Xiong, Jian Guo*

**Main category:** cs.CL

**Keywords:** large language models, instruction tuning, long-chain-of-thought, data selection, machine learning

**Relevance Score:** 8

**TL;DR:** Select2Reason is a framework for selecting high-utility instructions for fine-tuning large language models to enhance long-chain-of-thought reasoning, showing it can achieve competitive performance with fewer data samples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the long-chain-of-thought reasoning abilities of large language models without the significant overhead associated with large-scale instruction datasets.

**Method:** The Select2Reason framework selects and ranks instructions based on a difficulty estimator and a reasoning trace length-based heuristic for effective fine-tuning.

**Key Contributions:**

	1. Introduction of Select2Reason framework for instruction selection
	2. Demonstration of competitive performance with smaller training datasets
	3. Scalability and adaptability to other instruction pools with minimal cost

**Result:** Fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance that is competitive or superior to full data tuning across multiple benchmarks.

**Limitations:** 

**Conclusion:** Select2Reason offers a scalable and efficient method for long-CoT instruction selection, proving effective across varying data sizes and adaptable to minimal costs in different instruction sets.

**Abstract:** A practical approach to activate long chain-of-thoughts reasoning ability in pre-trained large language models is to perform supervised fine-tuning on instruction datasets synthesized by strong Large Reasoning Models such as DeepSeek-R1, offering a cost-effective alternative to reinforcement learning. However, large-scale instruction sets with more than 100k samples incur significant training overhead, while effective strategies for automatic long-CoT instruction selection still remain unexplored. In this work, we propose Select2Reason, a novel and efficient instruction-tuning data selection framework for long-CoT reasoning. From the perspective of emergence of rethinking behaviors like self-correction and backtracking, we investigate common metrics that may determine the quality of long-CoT reasoning instructions. Select2Reason leverages a quantifier to estimate difficulty of question and jointly incorporates a reasoning trace length-based heuristic through a weighted scheme for ranking to prioritize high-utility examples. Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance competitive with or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across three competition-level and six comprehensive mathematical benchmarks. Further experiments highlight the scalability in varying data size, efficiency during inference, and its adaptability to other instruction pools with minimal cost.

</details>


### [102] [GreekBarBench: A Challenging Benchmark for Free-Text Legal Reasoning and Citations](https://arxiv.org/abs/2505.17267)

*Odysseas S. Chlapanis, Dimitrios Galanis, Nikolaos Aletras, Ion Androutsopoulos*

**Main category:** cs.CL

**Keywords:** LLM, legal evaluation, benchmark, Greek Bar exams, meta-evaluation

**Relevance Score:** 5

**TL;DR:** GreekBarBench is a benchmark for evaluating LLMs on legal questions from Greek Bar exams, using a unique scoring system and meta-evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate LLMs effectively on legal questions and improve alignment with human expert evaluations.

**Method:** A three-dimensional scoring system is developed, combined with an LLM-as-a-judge approach and a meta-evaluation benchmark to assess LLM-judges against human experts.

**Key Contributions:**

	1. Introduction of GreekBarBench benchmark for legal LLM evaluation
	2. Development of a three-dimensional scoring system
	3. Meta-evaluation benchmark assessing correlation between LLM-judges and human experts.

**Result:** The systematic evaluation of 13 LLMs showed that top models exceed average expert scores but do not reach the 95th percentile of experts.

**Limitations:** The benchmark focuses exclusively on Greek legal contexts and statutory citations.

**Conclusion:** While LLMs can perform well on legal evaluations, they still lack the consistency and performance of top human experts in legal contexts.

**Abstract:** We introduce GreekBarBench, a benchmark that evaluates LLMs on legal questions across five different legal areas from the Greek Bar exams, requiring citations to statutory articles and case facts. To tackle the challenges of free-text evaluation, we propose a three-dimensional scoring system combined with an LLM-as-a-judge approach. We also develop a meta-evaluation benchmark to assess the correlation between LLM-judges and human expert evaluations, revealing that simple, span-based rubrics improve their alignment. Our systematic evaluation of 13 proprietary and open-weight LLMs shows that even though the best models outperform average expert scores, they fall short of the 95th percentile of experts.

</details>


### [103] [Search Wisely: Mitigating Sub-optimal Agentic Searches By Reducing Uncertainty](https://arxiv.org/abs/2505.17281)

*Peilin Wu, Mian Zhang, Xinlu Zhang, Xinya Du, Zhiyu Zoey Chen*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Reinforcement Learning, Search Efficiency, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper addresses inefficiencies in Agentic Retrieval-Augmented Generation (RAG) systems by defining search behaviors that affect performance, proposing a new training method called $eta$-GRPO to improve search decision-making through reinforcement learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Agentic RAG systems enhance LLMs but often suffer from inefficient search behaviors that hinder their effectiveness in information retrieval and reasoning.

**Method:** The authors propose a reinforcement learning-based method called $eta$-GRPO, which introduces a confidence threshold to improve search decisions in RAG systems.

**Key Contributions:**

	1. Formal definition and quantification of over-search and under-search behaviors in RAG systems.
	2. Proposal of $eta$-GRPO to improve search decision-making using reinforcement learning and confidence thresholds.
	3. Demonstration of improved performance on QA benchmarks through empirical testing.

**Result:** Experiments demonstrate that $eta$-GRPO significantly enhances the performance of a 3B model, achieving a 4% increase in average exact match score over strong baselines across seven QA benchmarks.

**Limitations:** The study's results may vary across different RAG system architectures and applications.

**Conclusion:** By formalizing inefficiencies and introducing $eta$-GRPO, the paper provides a solution to improve the agentic capabilities of RAG systems, enhancing their reliability in QA tasks.

**Abstract:** Agentic Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by enabling dynamic, multi-step reasoning and information retrieval. However, these systems often exhibit sub-optimal search behaviors like over-search (retrieving redundant information) and under-search (failing to retrieve necessary information), which hinder efficiency and reliability. This work formally defines and quantifies these behaviors, revealing their prevalence across multiple QA datasets and agentic RAG systems (e.g., one model could have avoided searching in 27.7% of its search steps). Furthermore, we demonstrate a crucial link between these inefficiencies and the models' uncertainty regarding their own knowledge boundaries, where response accuracy correlates with model's uncertainty in its search decisions. To address this, we propose $\beta$-GRPO, a reinforcement learning-based training method that incorporates confidence threshold to reward high-certainty search decisions. Experiments on seven QA benchmarks show that $\beta$-GRPO enable a 3B model with better agentic RAG ability, outperforming other strong baselines with a 4% higher average exact match score.

</details>


### [104] [SELF: Self-Extend the Context Length With Logistic Growth Function](https://arxiv.org/abs/2505.17296)

*Phat Thanh Dang, Saahil Thoppay, Wang Yang, Qifan Wang, Vipin Chaudhary, Xiaotian Han*

**Main category:** cs.CL

**Keywords:** language models, long context, attention mechanisms, performance improvement, grouping tokens

**Relevance Score:** 9

**TL;DR:** Proposes SELF, a mechanism to extend context length in language models using a logistic growth function for grouping tokens, improving performance on long-context tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of long context handling in language models, particularly when token distances affect attention mechanisms.

**Method:** Utilizes a logistic capacity equation to group consecutive tokens at varying sizes while maintaining a constant size for smaller distances.

**Key Contributions:**

	1. Introduction of the SELF mechanism to extend context length
	2. Demonstrated up to 12% performance improvement over LongLM
	3. Open-source code available for implementation

**Result:** Achieved performance improvements of up to 12% compared to LongLM on LEval, with enhancements of 6.4% in summarization tasks and 5.4% in reading comprehension tasks.

**Limitations:** 

**Conclusion:** SELF significantly advances the ability of language models to manage long contexts effectively, providing a more reliable tool for tasks requiring extended context.

**Abstract:** Large language models suffer issues when operated on long contexts that are larger than their training context length due to the standard position encoding for tokens in the attention layer. Tokens a long distance apart will rarely have an effect on each other and long prompts yield unexpected results. To solve this problem, we propose SELF (Self-Extend the Context Length With Logistic Growth Function): a solution of grouping consecutive tokens at varying group sizes using a logistic capacity equation combined with a constant group size at smaller relative distances. Our model had an increase in performance of up to 12% compared to the LongLM extension method in LEval (specifically on the Qwen model). On summarization related tasks in LongBench, our model performed up to 6.4% better than LongLM (specifically on the Llama-2-7b model). On reading comprehension tasks from LEval, our model performed up to 5.4% better than the LongLM. Our code is available at https://github.com/alexeipc/SELF-LLM.

</details>


### [105] [Refusal Direction is Universal Across Safety-Aligned Languages](https://arxiv.org/abs/2505.17306)

*Xinpeng Wang, Mingyang Wang, Yihong Liu, Hinrich Schtze, Barbara Plank*

**Main category:** cs.CL

**Keywords:** large language models, refusal mechanisms, multilingual safety datasets, cross-lingual, safety defenses

**Relevance Score:** 8

**TL;DR:** This paper investigates refusal mechanisms in large language models across 14 languages using a multilingual safety dataset, revealing that refusal vectors are transferable across languages, enhancing multilingual safety defenses.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore refusal behavior in large language models (LLMs) across multiple languages and ensure safety in diverse linguistic contexts.

**Method:** Utilized a multilingual safety dataset (PolyRefuse) by translating malicious and benign English prompts into 14 different languages to investigate refusal behavior.

**Key Contributions:**

	1. Created PolyRefuse, a multilingual safety dataset.
	2. Demonstrated cross-lingual universality of refusal behavior in LLMs.
	3. Identified underlying mechanisms of cross-lingual jailbreaks.

**Result:** Discovered that a refusal direction extracted from English can effectively bypass refusals in other languages without fine-tuning, indicating cross-lingual universality and transferability of refusal vectors.

**Limitations:** Focused on 14 languages; effectiveness in other languages not assessed.

**Conclusion:** The study provides insights for improving multilingual safety defenses in LLMs and deepens the understanding of safety vulnerabilities across languages.

**Abstract:** Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using PolyRefuse, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs.

</details>


### [106] [Benchmarking Expressive Japanese Character Text-to-Speech with VITS and Style-BERT-VITS2](https://arxiv.org/abs/2505.17320)

*Zackary Rackauckas, Julia Hirschberg*

**Main category:** cs.CL

**Keywords:** Japanese speech synthesis, text-to-speech, expressive speech

**Relevance Score:** 3

**TL;DR:** The paper benchmarks two TTS models for producing expressive Japanese character speech, highlighting superiority of SBV2JE in naturalness and intelligibility metrics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Efficiently synthesizing expressive Japanese speech for character-driven applications due to pitch-accent sensitivity and stylistic variability.

**Method:** Benchmarks VITS and SBV2JE using three character-specific datasets, evaluating on naturalness, intelligibility, and speaker consistency.

**Key Contributions:**

	1. Introduction of SBV2JE for expressive Japanese speech synthesis
	2. Achievement of near-human naturalness in TTS applications
	3. Demonstration of effective pitch-accent controls

**Result:** SBV2JE matches human naturalness ratings closely and outperforms VITS in word error rate and comparative preference metrics.

**Limitations:** Higher computational demands compared to standard models.

**Conclusion:** Despite higher computational demands, SBV2JE is effective for language learning and character dialogue generation.

**Abstract:** Synthesizing expressive Japanese character speech poses unique challenges due to pitch-accent sensitivity and stylistic variability. This paper benchmarks two open-source text-to-speech models--VITS and Style-BERT-VITS2 JP Extra (SBV2JE)--on in-domain, character-driven Japanese speech. Using three character-specific datasets, we evaluate models across naturalness (mean opinion and comparative mean opinion score), intelligibility (word error rate), and speaker consistency. SBV2JE matches human ground truth in naturalness (MOS 4.37 vs. 4.38), achieves lower WER, and shows slight preference in CMOS. Enhanced by pitch-accent controls and a WavLM-based discriminator, SBV2JE proves effective for applications like language learning and character dialogue generation, despite higher computational demands.

</details>


### [107] [From Compression to Expansion: A Layerwise Analysis of In-Context Learning](https://arxiv.org/abs/2505.17322)

*Jiachen Jiang, Yuxin Dong, Jinxin Zhou, Zhihui Zhu*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, Layerwise Compression-Expansion, representational analysis, bias-variance decomposition

**Relevance Score:** 9

**TL;DR:** This paper analyzes the internal representational mechanisms of in-context learning (ICL) in large language models (LLMs), introducing the concept of Layerwise Compression-Expansion, where early layers create compact representations and later layers expand them for prediction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how in-context learning (ICL) representations in LLMs encode task-specific information and its implications for performance and robustness.

**Method:** A statistical geometric analysis of representations in ICL across layers and tasks, highlighting the phenomenon of Layerwise Compression-Expansion.

**Key Contributions:**

	1. Introduction of Layerwise Compression-Expansion in ICL
	2. Theoretical analysis of bias-variance decomposition in LLMs
	3. Demonstration of internal representational dynamics affecting performance and robustness.

**Result:** The analysis shows that early layers create compact and discriminative representations, while later layers expand these representations, improving ICL performance with more data and larger models.

**Limitations:** 

**Conclusion:** The findings indicate a dynamic layerwise structure in ICL, enhancing our understanding of model behavior and the effects of attention mechanisms on variance and bias reduction in predictions.

**Abstract:** In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks without weight updates by learning from demonstration sequences. While ICL shows strong empirical performance, its internal representational mechanisms are not yet well understood. In this work, we conduct a statistical geometric analysis of ICL representations to investigate how task-specific information is captured across layers. Our analysis reveals an intriguing phenomenon, which we term *Layerwise Compression-Expansion*: early layers progressively produce compact and discriminative representations that encode task information from the input demonstrations, while later layers expand these representations to incorporate the query and generate the prediction. This phenomenon is observed consistently across diverse tasks and a range of contemporary LLM architectures. We demonstrate that it has important implications for ICL performance -- improving with model size and the number of demonstrations -- and for robustness in the presence of noisy examples. To further understand the effect of the compact task representation, we propose a bias-variance decomposition and provide a theoretical analysis showing how attention mechanisms contribute to reducing both variance and bias, thereby enhancing performance as the number of demonstrations increases. Our findings reveal an intriguing layerwise dynamic in ICL, highlight how structured representations emerge within LLMs, and showcase that analyzing internal representations can facilitate a deeper understanding of model behavior.

</details>


### [108] [GPT Editors, Not Authors: The Stylistic Footprint of LLMs in Academic Preprints](https://arxiv.org/abs/2505.17327)

*Soren DeHaan, Yuanze Liu, Johan Bollen, Sa'ul A. Blanco*

**Main category:** cs.CL

**Keywords:** Large Language Models, academic writing, stylistic segmentation, Bayesian classifier, GPT

**Relevance Score:** 9

**TL;DR:** The study examines the impact of Large Language Models (LLMs) on academic writing, focusing on their use in generating text versus editing in arXiv papers.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To determine how LLMs are being utilized in academic writing, particularly in generating critical text versus aiding in editing tasks.

**Method:** Analysis of arXiv papers for stylistic segmentation using a Bayesian classifier trained on GPT-regenerated text and varying a PELT threshold.

**Key Contributions:**

	1. Investigation of LLM usage patterns in academic writing
	2. Demonstration of the impact of LLMs on stylistic segmentation
	3. Insights into the risks associated with LLMs in academic contexts

**Result:** The study reveals that LLM-attributed language does not predict stylistic segmentation, indicating uniform usage of LLMs by authors and a lower risk of hallucinations in academic preprints.

**Limitations:** 

**Conclusion:** Authors employing LLMs tend to use them uniformly, mitigating potential issues with credibility and consistency in academic writing.

**Abstract:** The proliferation of Large Language Models (LLMs) in late 2022 has impacted academic writing, threatening credibility, and causing institutional uncertainty. We seek to determine the degree to which LLMs are used to generate critical text as opposed to being used for editing, such as checking for grammar errors or inappropriate phrasing. In our study, we analyze arXiv papers for stylistic segmentation, which we measure by varying a PELT threshold against a Bayesian classifier trained on GPT-regenerated text. We find that LLM-attributed language is not predictive of stylistic segmentation, suggesting that when authors use LLMs, they do so uniformly, reducing the risk of hallucinations being introduced into academic preprints.

</details>


### [109] [SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use](https://arxiv.org/abs/2505.17332)

*Hitesh Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae*

**Main category:** cs.CL

**Keywords:** Large Language Models, ethical AI, enterprise applications, benchmarking, cultural context

**Relevance Score:** 9

**TL;DR:** Introducing SweEval, a benchmark to evaluate LLM's handling of inappropriate language in enterprise communication across diverse cultural contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As enterprises adopt LLMs for communication, understanding cultural and linguistic diversity, while ensuring safe and respectful responses, is crucial.

**Method:** SweEval benchmark simulates real-world scenarios with variations in tone and context to test LLM compliance with ethical language use.

**Key Contributions:**

	1. Introduction of the SweEval benchmark for evaluating LLM behavior in diverse contexts.
	2. Assessment of LLM compliance with ethical frameworks when given inappropriate instructions.
	3. Release of dataset and code to support further research in the field.

**Result:** Evaluation of LLMs shows their ability to handle unsafe or offensive instructions in alignment with ethical frameworks and cultural nuances.

**Limitations:** The benchmark may not cover all possible cultural contexts and nuances.

**Conclusion:** The benchmark aids in developing ethically aligned AI systems for enterprise applications, with a dataset and code released for further research.

**Abstract:** Enterprise customers are increasingly adopting Large Language Models (LLMs) for critical communication tasks, such as drafting emails, crafting sales pitches, and composing casual messages. Deploying such models across different regions requires them to understand diverse cultural and linguistic contexts and generate safe and respectful responses. For enterprise applications, it is crucial to mitigate reputational risks, maintain trust, and ensure compliance by effectively identifying and handling unsafe or offensive language. To address this, we introduce SweEval, a benchmark simulating real-world scenarios with variations in tone (positive or negative) and context (formal or informal). The prompts explicitly instruct the model to include specific swear words while completing the task. This benchmark evaluates whether LLMs comply with or resist such inappropriate instructions and assesses their alignment with ethical frameworks, cultural nuances, and language comprehension capabilities. In order to advance research in building ethically aligned AI systems for enterprise use and beyond, we release the dataset and code: https://github.com/amitbcp/multilingual_profanity.

</details>


### [110] [Language models should be subject to repeatable, open, domain-contextualized hallucination benchmarking](https://arxiv.org/abs/2505.17345)

*Justin D. Norman, Michael U. Rivera, D. Alex Hughes*

**Main category:** cs.CL

**Keywords:** language models, hallucination, benchmarking, taxonomy, evaluation

**Relevance Score:** 8

**TL;DR:** This paper discusses the prevalence of hallucinations in language models and advocates for a structured approach to measure these inaccuracies through a comprehensive benchmarking framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing concern about inaccuracies in model-generated text, often termed 'hallucinations', and the need for an effective way to measure their prevalence to facilitate responsible use of language models.

**Method:** The authors propose a framework for evaluating hallucinations in language models using repeatable, open benchmarks that are contextualized to specific domains. A taxonomy of hallucinations is introduced, along with a case study to illustrate the issues with current metrics when experts are not involved.

**Key Contributions:**

	1. Introduction of a taxonomy for language model hallucinations
	2. Proposing a repeatable and open benchmarking framework
	3. Demonstrating the importance of expert involvement in data creation for accurate hallucination metrics

**Result:** The case study reveals that the validity and utility of hallucination metrics are significantly undermined when expert input is absent during the data creation process.

**Limitations:** The proposed taxonomy and framework might still require refinement and wider testing across various domains to ensure comprehensive applicability.

**Conclusion:** For effective evaluation of language model hallucinations, the involvement of domain experts in the early stages is crucial to ensure that the metrics developed are valid and useful.

**Abstract:** Plausible, but inaccurate, tokens in model-generated text are widely believed to be pervasive and problematic for the responsible adoption of language models. Despite this concern, there is little scientific work that attempts to measure the prevalence of language model hallucination in a comprehensive way. In this paper, we argue that language models should be evaluated using repeatable, open, and domain-contextualized hallucination benchmarking. We present a taxonomy of hallucinations alongside a case study that demonstrates that when experts are absent from the early stages of data creation, the resulting hallucination metrics lack validity and practical utility.

</details>


### [111] [A Fully Generative Motivational Interviewing Counsellor Chatbot for Moving Smokers Towards the Decision to Quit](https://arxiv.org/abs/2505.17362)

*Zafarullah Mahmood, Soliman Ali, Jiading Zhu, Mohamed Abdelwahab, Michelle Yu Collins, Sihan Chen, Yi Cheng Zhao, Jodi Wolff, Osnat Melamed, Nadia Minian, Marta Maslej, Carolynne Cooper, Matt Ratto, Peter Selby, Jonathan Rose*

**Main category:** cs.CL

**Keywords:** Large Language Models, Motivational Interviewing, Chatbot, Tobacco cessation, Automated assessment

**Relevance Score:** 9

**TL;DR:** A chatbot using a Large Language Model focuses on motivating smokers to quit, demonstrating high adherence to Motivational Interviewing standards and improving participants' confidence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of a chatbot as an automated talk therapist for motivating tobacco smokers to quit smoking, using evidence-based therapeutic techniques.

**Method:** A chatbot based on a state-of-the-art LLM was developed and tested with 106 participants, measuring their confidence to quit smoking before and after interactions, and assessing adherence to MI standards.

**Key Contributions:**

	1. Development of an LLM-based chatbot for tobacco cessation
	2. Demonstration of high adherence to Motivational Interviewing standards
	3. Validation of automated assessment for chatbot interactions

**Result:** Participants' confidence in quitting increased by an average of 1.7 points on a 0-10 scale. The chatbot adhered to MI standards in 98% of its utterances, outperforming human counsellors, and received a favorable empathy score.

**Limitations:** The chatbot performed lower in perceived empathy compared to human counsellors, highlighting a potential area for improvement.

**Conclusion:** The results indicate that LLMs can effectively support motivation in smoking cessation, showing the potential for automated talk therapy to adhere to therapeutic standards.

**Abstract:** The conversational capabilities of Large Language Models (LLMs) suggest that they may be able to perform as automated talk therapists. It is crucial to know if these systems would be effective and adhere to known standards. We present a counsellor chatbot that focuses on motivating tobacco smokers to quit smoking. It uses a state-of-the-art LLM and a widely applied therapeutic approach called Motivational Interviewing (MI), and was evolved in collaboration with clinician-scientists with expertise in MI. We also describe and validate an automated assessment of both the chatbot's adherence to MI and client responses. The chatbot was tested on 106 participants, and their confidence that they could succeed in quitting smoking was measured before the conversation and one week later. Participants' confidence increased by an average of 1.7 on a 0-10 scale. The automated assessment of the chatbot showed adherence to MI standards in 98% of utterances, higher than human counsellors. The chatbot scored well on a participant-reported metric of perceived empathy but lower than typical human counsellors. Furthermore, participants' language indicated a good level of motivation to change, a key goal in MI. These results suggest that the automation of talk therapy with a modern LLM has promise.

</details>


### [112] [AI-Augmented LLMs Achieve Therapist-Level Responses in Motivational Interviewing](https://arxiv.org/abs/2505.17380)

*Yinghui Huang, Yuxuan Jiang, Hui Liu, Yixin Cai, Weiqing Li, Xiangen Hu*

**Main category:** cs.CL

**Keywords:** motivational interviewing, large language models, addiction care, human-AI collaboration, deep learning

**Relevance Score:** 9

**TL;DR:** The paper evaluates the use of GPT-4 for motivational interviewing (MI) in addiction care, highlighting improvements through prompt engineering and identifying both strengths and limitations in its therapeutic capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the potential of large language models, specifically GPT-4, in enhancing motivational interviewing practices within addiction care.

**Method:** The study uses a computational framework to analyze human therapist and GPT-4 MI sessions, developing predictive models that incorporate deep learning and explainable AI. It focuses on identifying MI-consistent and MI-inconsistent behaviors through user-perceived quality metrics.

**Key Contributions:**

	1. Development of a computational framework for assessing user-perceived quality in MI sessions
	2. Identification of 17 behavioral metrics for MI consistency
	3. Evidence of improved performance of LLMs in therapeutic contexts through prompt engineering.

**Result:** The study found that while GPT-4 exhibited marginally less effectiveness than human therapists, it outperformed in managing advice and showed measurable quality improvements through customized prompting, but faced challenges with complex emotional nuances.

**Limitations:** GPT-4 was inferior to human therapists in overall performance and struggled with complex emotional nuances.

**Conclusion:** The proposed framework paves the way for the optimization of LLM-based therapeutic tools, indicating both the promise and limitations of LLMs in clinical settings.

**Abstract:** Large language models (LLMs) like GPT-4 show potential for scaling motivational interviewing (MI) in addiction care, but require systematic evaluation of therapeutic capabilities. We present a computational framework assessing user-perceived quality (UPQ) through expected and unexpected MI behaviors. Analyzing human therapist and GPT-4 MI sessions via human-AI collaboration, we developed predictive models integrating deep learning and explainable AI to identify 17 MI-consistent (MICO) and MI-inconsistent (MIIN) behavioral metrics. A customized chain-of-thought prompt improved GPT-4's MI performance, reducing inappropriate advice while enhancing reflections and empathy. Although GPT-4 remained marginally inferior to therapists overall, it demonstrated superior advice management capabilities. The model achieved measurable quality improvements through prompt engineering, yet showed limitations in addressing complex emotional nuances. This framework establishes a pathway for optimizing LLM-based therapeutic tools through targeted behavioral metric analysis and human-AI co-evaluation. Findings highlight both the scalability potential and current constraints of LLMs in clinical communication applications.

</details>


### [113] [WiNGPT-3.0 Technical Report](https://arxiv.org/abs/2505.17387)

*Boqin Zhuang, Chenxiao Song, Huitong Lu, Jiacheng Qiao, Mingqian Liu, Mingxing Yu, Ping Hong, Rui Li, Xiaoxia Song, Xiangjun Xu, Xu Chen, Yaoyao Ma, Yujie Gao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Medical Reasoning, Reinforcement Learning, Healthcare IT, Clinical Workflow

**Relevance Score:** 9

**TL;DR:** WiNGPT-3.0 is a 32-billion parameter LLM designed to improve medical reasoning and its integration into healthcare IT. It uses a multi-stage training pipeline, achieving notable performance improvements on clinical reasoning tasks through reinforcement learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in current LLMs regarding structured and interpretable medical reasoning, as well as challenges in deployment related to computational resources and data privacy.

**Method:** A multi-stage training pipeline was implemented, incorporating supervised fine-tuning (SFT) and reinforcement learning (RL) with curated Long Chain-of-Thought (CoT) datasets and an evidence-based diagnostic chain simulation.

**Key Contributions:**

	1. Development of WiNGPT-3.0 for enhanced medical reasoning
	2. Implementation of a novel multi-stage training pipeline
	3. Demonstrated effectiveness of reinforcement learning in medical contexts

**Result:** WiNGPT-3.0 achieved scores of 66.6 on MedCalc and 87.1 on MedQA-USMLE, and improved clinical reasoning task performance from 58.1 to 62.5 through targeted training.

**Limitations:** 

**Conclusion:** Reinforcement learning can enhance medical reasoning accuracy even with limited datasets, leading to more trustworthy and deployable LLMs in clinical settings.

**Abstract:** Current Large Language Models (LLMs) exhibit significant limitations, notably in structured, interpretable, and verifiable medical reasoning, alongside practical deployment challenges related to computational resources and data privacy. This report focused on the development of WiNGPT-3.0, the 32-billion parameter LLMs, engineered with the objective of enhancing its capacity for medical reasoning and exploring its potential for effective integration within healthcare IT infrastructures. The broader aim is to advance towards clinically applicable models. The approach involved a multi-stage training pipeline tailored for general, medical, and clinical reasoning. This pipeline incorporated supervised fine-tuning (SFT) and reinforcement learning (RL), leveraging curated Long Chain-of-Thought (CoT) datasets, auxiliary reward models, and an evidence-based diagnostic chain simulation. WiNGPT-3.0 demonstrated strong performance: specific model variants achieved scores of 66.6 on MedCalc and 87.1 on MedQA-USMLE. Furthermore, targeted training improved performance on a clinical reasoning task from a baseline score of 58.1 to 62.5. These findings suggest that reinforcement learning, even when applied with a limited dataset of only a few thousand examples, can enhance medical reasoning accuracy. Crucially, this demonstration of RL's efficacy with limited data and computation paves the way for more trustworthy and practically deployable LLMs within clinical workflows and health information infrastructures.

</details>


### [114] [Measuring diversity of synthetic prompts and data generated with fine-grained persona prompting](https://arxiv.org/abs/2505.17390)

*Gauri Kambhatla, Chantal Shaib, Venkata Govindarajan*

**Main category:** cs.CL

**Keywords:** synthetic data, large language models, lexical diversity, fine-grained personas, text generation

**Relevance Score:** 7

**TL;DR:** This paper investigates the impact of fine-grained personas on the diversity of synthetic prompts and responses in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how fine-grained personas affect the diversity of synthetic data generated for LLMs, aiming to enhance training and better understand diversity in machine-generated text.

**Method:** The study employs lexical diversity and redundancy metrics to assess synthetic prompts and responses generated by LLMs of varying sizes, comparing them with human-written counterparts.

**Key Contributions:**

	1. Introduces metrics for measuring diversity in persona-driven prompts and responses.
	2. Demonstrates the lesser diversity of synthetic prompts compared to human-written ones.
	3. Finds limited effects of fine-grained personas on lexical diversity in LLM outputs.

**Result:** It was found that synthetic prompts are less diverse than human-written ones, and while persona-prompting enhances lexical diversity, fine-grained persona details do not significantly increase overall diversity.

**Limitations:** The study focuses primarily on lexical diversity metrics, which may not capture other aspects of text quality or relevance.

**Conclusion:** Using fine-grained personas can improve the diversity of prompts, but the level of detail in personas has limited impact on the diversity of generated text.

**Abstract:** Fine-grained personas have recently been used for generating 'diverse' synthetic data for pre-training and supervised fine-tuning of Large Language Models (LLMs). In this work, we measure the diversity of persona-driven synthetically generated prompts and responses with a suite of lexical diversity and redundancy metrics. Firstly, we find that synthetic prompts/instructions are significantly less diverse than human-written ones. Next, we sample responses from LLMs of different sizes with fine-grained and coarse persona descriptions to investigate how much fine-grained detail in persona descriptions contribute to generated text diversity. We find that while persona-prompting does improve lexical diversity (especially with larger models), fine-grained detail in personas doesn't increase diversity noticeably.

</details>


### [115] [Curriculum Guided Reinforcement Learning for Efficient Multi Hop Retrieval Augmented Generation](https://arxiv.org/abs/2505.17391)

*Yuelyu Ji, Rui Meng, Zhuochun Li, Daqing He*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Reinforcement learning, Query optimization

**Relevance Score:** 9

**TL;DR:** EVO-RAG is a framework that improves multi-hop RAG systems by enhancing query performance through curriculum-guided reinforcement learning and dynamic reward scheduling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing multi-hop RAG pipelines have issues such as redundant subqueries and inefficient search paths, necessitating a solution that optimizes query generation and retrieval efficiency.

**Method:** EVO-RAG employs a reinforcement learning framework that evolves a query-rewriting agent guided by a multi-faceted reward vector and a time-varying scheduler to refine query strategies over time.

**Key Contributions:**

	1. Introduces a curriculum-guided RL approach to evolve RAG query agents.
	2. Implements a seven-factor reward system to balance query performance aspects.
	3. Enhances retrieval efficiency while maintaining answer correctness.

**Result:** EVO-RAG increases Exact Match scores by up to 4.6 points on multiple benchmarks while reducing average retrieval depth by 15%.

**Limitations:** 

**Conclusion:** EVO-RAG demonstrates significant advancements in the efficiency and reliability of multi-hop RAG systems through guided exploration and dynamic reward adjustments.

**Abstract:** Retrieval-augmented generation (RAG) grounds large language models (LLMs) in up-to-date external evidence, yet existing multi-hop RAG pipelines still issue redundant subqueries, explore too shallowly, or wander through overly long search chains. We introduce EVO-RAG, a curriculum-guided reinforcement learning framework that evolves a query-rewriting agent from broad early-stage exploration to concise late-stage refinement. EVO-RAG couples a seven-factor, step-level reward vector (covering relevance, redundancy, efficiency, and answer correctness) with a time-varying scheduler that reweights these signals as the episode unfolds. The agent is trained with Direct Preference Optimization over a multi-head reward model, enabling it to learn when to search, backtrack, answer, or refuse. Across four multi-hop QA benchmarks (HotpotQA, 2WikiMultiHopQA, MuSiQue, and Bamboogle), EVO-RAG boosts Exact Match by up to 4.6 points over strong RAG baselines while trimming average retrieval depth by 15 %. Ablation studies confirm the complementary roles of curriculum staging and dynamic reward scheduling. EVO-RAG thus offers a general recipe for building reliable, cost-effective multi-hop RAG systems.

</details>


### [116] [FullFront: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](https://arxiv.org/abs/2505.17399)

*Haoyu Sun, Huichen Will Wang, Jiawei Gu, Linjie Li, Yu Cheng*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, front-end engineering, webpage design, code generation, benchmarking

**Relevance Score:** 7

**TL;DR:** FullFront is a benchmark for evaluating Multimodal Large Language Models (MLLMs) across the entire front-end development pipeline, focusing on webpage design, perception, and code generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the limitations of MLLMs in front-end engineering and provide a standardized evaluation framework that maintains diverse visual designs.

**Method:** FullFront utilizes a two-stage process to convert real-world webpages into clean, standardized HTML while avoiding copyright issues and assesses three tasks: Webpage Design, Webpage Perception QA, and Webpage Code Generation.

**Key Contributions:**

	1. Introduction of the FullFront benchmark for front-end development
	2. Evaluation of MLLM performance in a comprehensive development pipeline
	3. Insights into specific limitations of MLLMs in design and implementation tasks.

**Result:** Significant limitations were found in MLLM performance, especially in page perception, code generation related to image handling and layout, and interaction implementation.

**Limitations:** The benchmark may not encompass all aspects of front-end development and focuses primarily on specific tasks.

**Conclusion:** There is a substantial gap between the capabilities of current MLLMs and human expert performance in front-end engineering, underscoring the need for improved models.

**Abstract:** Front-end engineering involves a complex workflow where engineers conceptualize designs, translate them into code, and iteratively refine the implementation. While recent benchmarks primarily focus on converting visual designs to code, we present FullFront, a benchmark designed to evaluate Multimodal Large Language Models (MLLMs) \textbf{across the full front-end development pipeline}. FullFront assesses three fundamental tasks that map directly to the front-end engineering pipeline: Webpage Design (conceptualization phase), Webpage Perception QA (comprehension of visual organization and elements), and Webpage Code Generation (implementation phase). Unlike existing benchmarks that use either scraped websites with bloated code or oversimplified LLM-generated HTML, FullFront employs a novel, two-stage process to transform real-world webpages into clean, standardized HTML while maintaining diverse visual designs and avoiding copyright issues. Extensive testing of state-of-the-art MLLMs reveals significant limitations in page perception, code generation (particularly for image handling and layout), and interaction implementation. Our results quantitatively demonstrate performance disparities across models and tasks, and highlight a substantial gap between current MLLM capabilities and human expert performance in front-end engineering. The FullFront benchmark and code are available in https://github.com/Mikivishy/FullFront.

</details>


### [117] [Language Matters: How Do Multilingual Input and Reasoning Paths Affect Large Reasoning Models?](https://arxiv.org/abs/2505.17407)

*Zhi Rui Tam, Cheng-Kuang Wu, Yu Ying Chiu, Chieh-Yen Lin, Yun-Nung Chen, Hung-yi Lee*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Multilingualism, Language Bias, Human-Computer Interaction, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper investigates the language in which large reasoning models (LRMs) perform reasoning when presented with multilingual problems, revealing biases towards high-resource languages and performance issues in low-resource languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the internal reasoning processes of LRMs in multilingual contexts and their performance depending on the language used for reasoning.

**Method:** The study involves extensive evaluations using reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks (CulturalBench, LMSYS-toxic) to measure how language choice impacts performance.

**Key Contributions:**

	1. Highlighting the default reasoning language of LRM models affects performance.
	2. Providing evidence on performance decline in low-resource languages during reasoning tasks.
	3. Discussing the role of cultural context and safety evaluations in language-specific behavior.

**Result:** LRMs generally default to reasoning in high-resource languages like English, leading to performance degradation when they must reason in lower-resource languages, especially on reasoning tasks.

**Limitations:** The study primarily focuses on high-resource languages and may not cover all lower-resource languages adequately.

**Conclusion:** Identifying linguistic biases in LRMs is necessary for creating models that provide equitable service to users in diverse languages.

**Abstract:** Large reasoning models (LRMs) have demonstrated impressive performance across a range of reasoning tasks, yet little is known about their internal reasoning processes in multilingual settings. We begin with a critical question: {\it In which language do these models reason when solving problems presented in different languages?} Our findings reveal that, despite multilingual training, LRMs tend to default to reasoning in high-resource languages (e.g., English) at test time, regardless of the input language. When constrained to reason in the same language as the input, model performance declines, especially for low-resource languages. In contrast, reasoning in high-resource languages generally preserves performance. We conduct extensive evaluations across reasoning-intensive tasks (MMMLU, MATH-500) and non-reasoning benchmarks (CulturalBench, LMSYS-toxic), showing that the effect of language choice varies by task type: input-language reasoning degrades performance on reasoning tasks but benefits cultural tasks, while safety evaluations exhibit language-specific behavior. By exposing these linguistic biases in LRMs, our work highlights a critical step toward developing more equitable models that serve users across diverse linguistic backgrounds.

</details>


### [118] [Conversations: Love Them, Hate Them, Steer Them](https://arxiv.org/abs/2505.17413)

*Niranjan Chebrolu, Gerard Christopher Yeo, Kokil Jaidka*

**Main category:** cs.CL

**Keywords:** Large Language Models, emotional expression, activation engineering, conversational AI, sentiment

**Relevance Score:** 9

**TL;DR:** This paper presents a method for enhancing emotional expression in LLMs by using targeted activation engineering to manipulate emotional nuances in responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of instilling Large Language Models with nuanced, human-like emotional expression despite their increasing conversational fluency.

**Method:** The authors utilize attribution patching to identify key components of LLaMA 3.1-8B's activation patterns during conversational tasks, and derive emotional expression vectors from contrasting text pairs to enhance emotional characteristics in responses.

**Key Contributions:**

	1. Demonstrated targeted activation engineering improves emotional expression in LLMs.
	2. Introduced a novel method using attribution patching to analyze activation patterns.
	3. Derived emotional expression vectors based on contrastive text pairs.

**Result:** The targeted activation engineering method significantly enhances the emotional characteristics of the LLM's responses, leading to increased positive sentiment and more personal engagement in replies.

**Limitations:** 

**Conclusion:** This approach provides a precise and interpretable method for controlling emotional attributes in LLMs, aiding in the development of more empathetic conversational AI.

**Abstract:** Large Language Models (LLMs) demonstrate increasing conversational fluency, yet instilling them with nuanced, human-like emotional expression remains a significant challenge. Current alignment techniques often address surface-level output or require extensive fine-tuning. This paper demonstrates that targeted activation engineering can steer LLaMA 3.1-8B to exhibit more human-like emotional nuances. We first employ attribution patching to identify causally influential components, to find a key intervention locus by observing activation patterns during diagnostic conversational tasks. We then derive emotional expression vectors from the difference in the activations generated by contrastive text pairs (positive vs. negative examples of target emotions). Applying these vectors to new conversational prompts significantly enhances emotional characteristics: steered responses show increased positive sentiment (e.g., joy, trust) and more frequent first-person pronoun usage, indicative of greater personal engagement. Our findings offer a precise and interpretable method for controlling specific emotional attributes in LLMs, contributing to developing more aligned and empathetic conversational AI.

</details>


### [119] [DASH: Input-Aware Dynamic Layer Skipping for Efficient LLM Inference with Markov Decision Policies](https://arxiv.org/abs/2505.17420)

*Ning Yang, Fangxin Liu, Junjie Wang, Tao Yang, Kan Liu, Haibing Guan, Li Jiang*

**Main category:** cs.CL

**Keywords:** large language models, adaptive computation, Markov Decision Process, inference acceleration, layer-skipping

**Relevance Score:** 8

**TL;DR:** DASH accelerates inference in large language models by adaptively skipping layers based on input characteristics, modeled as a Markov Decision Process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the inference cost of large language models in latency-sensitive applications and enable real-world deployment.

**Method:** DASH uses an adaptive layer-skipping approach where computation paths are chosen dynamically based on input, utilizing a Markov Decision Process for token-level decisions and incorporating a compensation mechanism to offset any performance loss.

**Key Contributions:**

	1. Introduction of the DASH framework for adaptive layer skipping in LLMs.
	2. Modeling the skipping process as an MDP for fine-grained decision-making.
	3. Development of a compensation mechanism to maintain performance during layer skipping.

**Result:** DASH significantly accelerates inference times without sacrificing task performance, surpassing existing methods in efficiency on multiple LLM architectures and NLP benchmarks.

**Limitations:** The potential complexity of real-time implementation and dependency on specific layer architectures.

**Conclusion:** The framework effectively balances computational efficiency and model performance, making it suitable for practical applications of large language models.

**Abstract:** Large language models (LLMs) have achieved remarkable performance across a wide range of NLP tasks. However, their substantial inference cost poses a major barrier to real-world deployment, especially in latency-sensitive scenarios. To address this challenge, we propose \textbf{DASH}, an adaptive layer-skipping framework that dynamically selects computation paths conditioned on input characteristics. We model the skipping process as a Markov Decision Process (MDP), enabling fine-grained token-level decisions based on intermediate representations. To mitigate potential performance degradation caused by skipping, we introduce a lightweight compensation mechanism that injects differential rewards into the decision process. Furthermore, we design an asynchronous execution strategy that overlaps layer computation with policy evaluation to minimize runtime overhead. Experiments on multiple LLM architectures and NLP benchmarks show that our method achieves significant inference acceleration while maintaining competitive task performance, outperforming existing methods.

</details>


### [120] [T$^2$: An Adaptive Test-Time Scaling Strategy for Contextual Question Answering](https://arxiv.org/abs/2505.17427)

*Zhengyi Zhao, Shubo Zhang, Zezhong Wang, Huimin Wang, Yutian Zhao, Bin Liang, Yefeng Zheng, Binyang Li, Kam-Fai Wong, Xian Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Contextual Question Answering, adaptive reasoning

**Relevance Score:** 9

**TL;DR:** Introducing T$^2$: Think-to-Think framework for adaptive reasoning in Contextual Question Answering using LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve adaptability and efficiency in LLM reasoning for Contextual Question Answering (CQA) by avoiding overthinking on simpler questions and leveraging inherent reasoning capabilities without human bias.

**Method:** T$^2$ dynamically adjusts reasoning depth based on question complexity by decomposing questions, generating similar examples, evaluating reasoning strategies, and applying the most suitable one.

**Key Contributions:**

	1. Introduction of a novel T$^2$ framework for adaptive reasoning
	2. Enhancement of accuracy in CQA tasks
	3. Reduction of computational overhead

**Result:** T$^2$ achieves higher accuracy compared to baseline methods and reduces computational overhead by up to 25.2%.

**Limitations:** 

**Conclusion:** The T$^2$ framework effectively balances reasoning depth and efficiency in answering questions of varying complexity.

**Abstract:** Recent advances in Large Language Models (LLMs) have demonstrated remarkable performance in Contextual Question Answering (CQA). However, prior approaches typically employ elaborate reasoning strategies regardless of question complexity, leading to low adaptability. Recent efficient test-time scaling methods introduce budget constraints or early stop mechanisms to avoid overthinking for straightforward questions. But they add human bias to the reasoning process and fail to leverage models' inherent reasoning capabilities. To address these limitations, we present T$^2$: Think-to-Think, a novel framework that dynamically adapts reasoning depth based on question complexity. T$^2$ leverages the insight that if an LLM can effectively solve similar questions using specific reasoning strategies, it can apply the same strategy to the original question. This insight enables to adoption of concise reasoning for straightforward questions while maintaining detailed analysis for complex problems. T$^2$ works through four key steps: decomposing questions into structural elements, generating similar examples with candidate reasoning strategies, evaluating these strategies against multiple criteria, and applying the most appropriate strategy to the original question. Experimental evaluation across seven diverse CQA benchmarks demonstrates that T$^2$ not only achieves higher accuracy than baseline methods but also reduces computational overhead by up to 25.2\%.

</details>


### [121] [Discovering Forbidden Topics in Language Models](https://arxiv.org/abs/2505.17441)

*Can Rager, Chris Wendler, Rohit Gandikota, David Bau*

**Main category:** cs.CL

**Keywords:** refusal discovery, language models, bias detection, AI alignment, censorship

**Relevance Score:** 6

**TL;DR:** This paper introduces refusal discovery, a method for identifying topics that language models refuse to discuss, and presents a tool called LLM-crawler to benchmark such refusals across various models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address biases, boundaries, and alignment failures in AI systems by identifying topics that language models refuse to discuss.

**Method:** The method involves using token prefilling to discover forbidden topics, benchmarking on models like Tulu-3-8B and Claude-Haiku, and scaling the crawl on models like Llama-3.3-70B and its variants.

**Key Contributions:**

	1. Introduction of the refusal discovery problem setting
	2. Development of the LLM-crawler method for finding forbidden topics
	3. Benchmarking results across multiple language models revealing censorship patterns.

**Result:** LLM-crawler successfully retrieved 31 out of 36 refusal topics within a 1000-prompt limit. It revealed censorship-related behavior in the DeepSeek-R1-70B model and demonstrated robustness in Perplexity-R1-1776-70B against censorship.

**Limitations:** The study is limited to specific models and may not generalize to all language models.

**Conclusion:** The findings underscore the necessity for advanced refusal discovery methods to understand and mitigate biases in AI systems.

**Abstract:** Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, LLM-crawler, that uses token prefilling to find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits "thought suppression" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.

</details>


### [122] [Exploring the Effect of Segmentation and Vocabulary Size on Speech Tokenization for Speech Language Models](https://arxiv.org/abs/2505.17446)

*Shunsuke Kando, Yusuke Miyao, Shinnosuke Takamichi*

**Main category:** cs.CL

**Keywords:** speech tokenization, speech language models, segmentation, K-means clustering, spoken language understanding

**Relevance Score:** 7

**TL;DR:** This study explores the impact of speech tokenization methods on the performance of speech language models, focusing on segmentation width and cluster size.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how different speech tokenization methods affect the performance of speech language models is crucial for enhancing spoken language understanding.

**Method:** Speech signals are segmented into fixed and variable widths, and K-means models are trained with multiple cluster sizes to evaluate their impacts on performance.

**Key Contributions:**

	1. Investigates the effects of segmentation width and cluster size on speech tokenization.
	2. Demonstrates significant performance improvements with specific tokenization strategies.
	3. Provides insight into efficient training data usage and runtime reduction.

**Result:** Moderately coarse segmentation and larger cluster sizes significantly improve performance on zero-shot language understanding benchmarks, with one model achieving a 50% reduction in training data and a 70% decrease in training runtime.

**Limitations:** 

**Conclusion:** Combining multiple tokens can greatly enhance fine-grained spoken language understanding in SLMs, and careful selection of segmentation and cluster size is beneficial.

**Abstract:** The purpose of speech tokenization is to transform a speech signal into a sequence of discrete representations, serving as the foundation for speech language models (SLMs). While speech tokenization has many options, their effect on the performance of SLMs remains unclear. This paper investigates two key aspects of speech tokenization: the segmentation width and the cluster size of discrete units. First, we segment speech signals into fixed/variable widths and pooled representations. We then train K-means models in multiple cluster sizes. Through the evaluation on zero-shot spoken language understanding benchmarks, we find the positive effect of moderately coarse segmentation and bigger cluster size. Notably, among the best-performing models, the most efficient one achieves a 50% reduction in training data and a 70% decrease in training runtime. Our analysis highlights the importance of combining multiple tokens to enhance fine-grained spoken language understanding.

</details>


### [123] [LeTS: Learning to Think-and-Search via Process-and-Outcome Reward Hybridization](https://arxiv.org/abs/2505.17447)

*Qi Zhang, Shouqing Yang, Lirong Gao, Hao Chen, Xiaomeng Hu, Jinglei Chen, Jiexiang Wang, Sheng Guo, Bo Zheng, Haobo Wang, Junbo Zhao*

**Main category:** cs.CL

**Keywords:** large language models, reasoning, retrieval-augmented generation, reinforcement learning, process-level reward

**Relevance Score:** 9

**TL;DR:** This paper presents Learning to Think-and-Search (LeTS), a framework that enhances reasoning capabilities in retrieval-augmented generation (RAG) models through a novel hybrid reward system.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning abilities of large language models (LLMs) during retrieval-augmented generation by addressing the neglect of intermediate reasoning steps in current reinforcement learning approaches.

**Method:** The authors propose a process-level reward module that combines stepwise process rewards with outcome-based rewards, enhancing existing RL methods for RAG without requiring additional annotation.

**Key Contributions:**

	1. Proposed a novel framework (LeTS) for enhancing reasoning in RAG models
	2. Introduced a process-level reward module that addresses intermediate reasoning steps
	3. Demonstrated effective generalization and efficiency improvements in LLM reasoning capabilities.

**Result:** Extensive experiments show that LeTS improves generalization and inference efficiency across various RAG benchmarks, indicating the efficacy of hybridizing process and outcome rewards in enhancing LLMs' reasoning capabilities.

**Limitations:** Results may vary across different RAG benchmarks, and the applicability to a broader range of tasks is yet to be fully explored.

**Conclusion:** The research suggests that integrating process- and outcome-level rewards can significantly boost the reasoning abilities of LLMs in retrieval-augmented generation and may be applicable in other contexts.

**Abstract:** Large language models (LLMs) have demonstrated impressive capabilities in reasoning with the emergence of reasoning models like OpenAI-o1 and DeepSeek-R1. Recent research focuses on integrating reasoning capabilities into the realm of retrieval-augmented generation (RAG) via outcome-supervised reinforcement learning (RL) approaches, while the correctness of intermediate think-and-search steps is usually neglected. To address this issue, we design a process-level reward module to mitigate the unawareness of intermediate reasoning steps in outcome-level supervision without additional annotation. Grounded on this, we propose Learning to Think-and-Search (LeTS), a novel framework that hybridizes stepwise process reward and outcome-based reward to current RL methods for RAG. Extensive experiments demonstrate the generalization and inference efficiency of LeTS across various RAG benchmarks. In addition, these results reveal the potential of process- and outcome-level reward hybridization in boosting LLMs' reasoning ability via RL under other scenarios. The code will be released soon.

</details>


### [124] [Towards Evaluating Proactive Risk Awareness of Multimodal Language Models](https://arxiv.org/abs/2505.17455)

*Youliang Yuan, Wenxiang Jiao, Yuejin Xie, Chihao Shen, Menghan Tian, Wenxuan Wang, Jen-tse Huang, Pinjia He*

**Main category:** cs.CL

**Keywords:** proactive safety AI, human safety awareness, model limitations, risk detection, safety benchmark

**Relevance Score:** 8

**TL;DR:** The paper introduces a proactive AI system for human safety awareness, evaluating through 416 multimodal scenarios, and highlights limitations in model performance in risk detection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address gaps in human safety awareness that prevent timely recognition of everyday risks.

**Method:** Evaluates the capability of proactive AI in safety awareness through 416 multimodal scenarios, including image sequences and text logs.

**Key Contributions:**

	1. Establishment of a proactive safety benchmark (PaSBench)
	2. Systematic evidence of limitations in AI models' proactive reasoning
	3. Identifying unstable reasoning as a primary limitation in proactive safety AI

**Result:** 36 advanced models were evaluated, achieving 71% image and 64% text accuracy, but still missing 45-55% risks in repeated trials.

**Limitations:** Models exhibited unstable proactive reasoning rather than knowledge deficits as their primary limitation.

**Conclusion:** The study establishes a proactive safety benchmark, provides evidence of model limitations, and suggests directions for developing reliable protective AI.

**Abstract:** Human safety awareness gaps often prevent the timely recognition of everyday risks. In solving this problem, a proactive safety artificial intelligence (AI) system would work better than a reactive one. Instead of just reacting to users' questions, it would actively watch people's behavior and their environment to detect potential dangers in advance. Our Proactive Safety Bench (PaSBench) evaluates this capability through 416 multimodal scenarios (128 image sequences, 288 text logs) spanning 5 safety-critical domains. Evaluation of 36 advanced models reveals fundamental limitations: Top performers like Gemini-2.5-pro achieve 71% image and 64% text accuracy, but miss 45-55% risks in repeated trials. Through failure analysis, we identify unstable proactive reasoning rather than knowledge deficits as the primary limitation. This work establishes (1) a proactive safety benchmark, (2) systematic evidence of model limitations, and (3) critical directions for developing reliable protective AI. We believe our dataset and findings can promote the development of safer AI assistants that actively prevent harm rather than merely respond to requests. Our dataset can be found at https://huggingface.co/datasets/Youliang/PaSBench.

</details>


### [125] [Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.17464)

*Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Xin Yuan, Liming Zhu, Wenjie Zhang*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, knowledge graphs, multi-hop reasoning, evidence retrieval

**Relevance Score:** 9

**TL;DR:** Hydra is a training-free framework that enhances retrieval-augmented generation for large language models by improving multi-hop reasoning, multi-entity question handling, and source verification through unified evidence retrieval from knowledge graphs and documents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current hybrid retrieval-augmented generation systems struggle with complex reasoning tasks and verification when sourcing information from multiple types of evidence.

**Method:** Hydra integrates graph topology, document semantics, and source reliability through agent-driven exploration that enhances retrieval processes and cross-source verification.

**Key Contributions:**

	1. Training-free integration of structured and unstructured retrieval
	2. Effective handling of multi-hop and multi-entity reasoning
	3. Robust multi-source verification mechanism

**Result:** Hydra outperforms the baseline model ToG-2 by an average of 20.3% across benchmarks, enabling smaller models like Llama-3.1-8B to perform comparably to GPT-4-Turbo in reasoning tasks.

**Limitations:** 

**Conclusion:** Hydra demonstrates significant improvements in retrieval-augmented generation tasks and facilitates better performance from smaller models through effective evidence synthesis.

**Abstract:** Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating external knowledge. Current hybrid RAG system retrieves evidence from both knowledge graphs (KGs) and text documents to support LLM reasoning. However, it faces challenges like handling multi-hop reasoning, multi-entity questions, multi-source verification, and effective graph utilization. To address these limitations, we present Hydra, a training-free framework that unifies graph topology, document semantics, and source reliability to support deep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity problems through agent-driven exploration that combines structured and unstructured retrieval, increasing both diversity and precision of evidence. To tackle multi-source verification, Hydra uses a tri-factor cross-source verification (source trustworthiness assessment, cross-source corroboration, and entity-path alignment), to balance topic relevance with cross-modal agreement. By leveraging graph structure, Hydra fuses heterogeneous sources, guides efficient exploration, and prunes noise early. Comprehensive experiments on seven benchmark datasets show that Hydra achieves overall state-of-the-art results on all benchmarks with GPT-3.5, outperforming the strong hybrid baseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra enables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [126] [A Position Paper on the Automatic Generation of Machine Learning Leaderboards](https://arxiv.org/abs/2505.17465)

*Roelien C Timmer, Yufang Hou, Stephen Wan*

**Main category:** cs.CL

**Keywords:** Automatic Leaderboard Generation, Machine Learning, Benchmarking Guidelines

**Relevance Score:** 6

**TL;DR:** The paper presents a framework for standardizing Automatic Leaderboard Generation (ALG) in machine learning, addressing challenges in comparing prior work due to inconsistencies in research methodologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing volume of machine learning literature complicates the creation and maintenance of experimental leaderboards essential for comparing prior work effectively.

**Method:** The paper reviews existing ALG research, identifying differences in assumptions, scope, and output formats, and proposes a unified framework for standardization.

**Key Contributions:**

	1. Overview of Automatic Leaderboard Generation (ALG) research
	2. Proposed unified conceptual framework for ALG
	3. Guidelines for benchmarking including dataset and metric recommendations

**Result:** The proposed framework aims to enhance the consistency of leaderboard generation and improve the reproducibility of evaluations in machine learning research.

**Limitations:** The paper may not cover all existing ALG methodologies and their diverse applications within different ML contexts.

**Conclusion:** By adopting standardized definitions and benchmarking guidelines, the paper seeks to facilitate more effective comparison of ML research outputs and promote broader inclusion of results and metadata.

**Abstract:** An important task in machine learning (ML) research is comparing prior work, which is often performed via ML leaderboards: a tabular overview of experiments with comparable conditions (e.g., same task, dataset, and metric). However, the growing volume of literature creates challenges in creating and maintaining these leaderboards. To ease this burden, researchers have developed methods to extract leaderboard entries from research papers for automated leaderboard curation. Yet, prior work varies in problem framing, complicating comparisons and limiting real-world applicability. In this position paper, we present the first overview of Automatic Leaderboard Generation (ALG) research, identifying fundamental differences in assumptions, scope, and output formats. We propose an ALG unified conceptual framework to standardise how the ALG task is defined. We offer ALG benchmarking guidelines, including recommendations for datasets and metrics that promote fair, reproducible evaluation. Lastly, we outline challenges and new directions for ALG, such as, advocating for broader coverage by including all reported results and richer metadata.

</details>


### [127] [SLearnLLM: A Self-Learning Framework for Efficient Domain-Specific Adaptation of Large Language Models](https://arxiv.org/abs/2505.17470)

*Xiang Liu, Zhaoxiang Liu, Peng Wang, Kohou Wang, Huan Hu, Kai Wang, Shiguo Lian*

**Main category:** cs.CL

**Keywords:** fine-tuning, large language models, supervised learning, training efficiency, machine learning

**Relevance Score:** 9

**TL;DR:** Proposes a self-learning framework for fine-tuning LLMs that focuses on unknown knowledge within SFT datasets to improve efficiency and reduce training time.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Supervised fine-tuning on entire SFT datasets may waste resources if the dataset overlaps with the model's existing knowledge.

**Method:** The framework uses an SFT dataset, where LLMs answer questions, grade their responses, filter incorrect QA pairs, and fine-tune based on the filtered set.

**Key Contributions:**

	1. Introduces a self-learning framework for LLM fine-tuning.
	2. Reduces dependency on full dataset training.
	3. Demonstrates improved training efficiency and comparable performance across domains.

**Result:** Experimental results in agriculture and medicine show substantial reductions in training time with comparable performance improvements over full dataset fine-tuning.

**Limitations:** 

**Conclusion:** By targeting unknown knowledge in SFT datasets, the proposed method enhances LLM fine-tuning efficiency.

**Abstract:** When using supervised fine-tuning (SFT) to adapt large language models (LLMs) to specific domains, a significant challenge arises: should we use the entire SFT dataset for fine-tuning? Common practice often involves fine-tuning directly on the entire dataset due to limited information on the LLM's past training data. However, if the SFT dataset largely overlaps with the model's existing knowledge, the performance gains are minimal, leading to wasted computational resources. Identifying the unknown knowledge within the SFT dataset and using it to fine-tune the model could substantially improve the training efficiency. To address this challenge, we propose a self-learning framework for LLMs inspired by human learning pattern. This framework takes a fine-tuning (SFT) dataset in a specific domain as input. First, the LLMs answer the questions in the SFT dataset. The LLMs then objectively grade the responses and filter out the incorrectly answered QA pairs. Finally, we fine-tune the LLMs based on this filtered QA set. Experimental results in the fields of agriculture and medicine demonstrate that our method substantially reduces training time while achieving comparable improvements to those attained with full dataset fine-tuning. By concentrating on the unknown knowledge within the SFT dataset, our approach enhances the efficiency of fine-tuning LLMs.

</details>


### [128] [FinRAGBench-V: A Benchmark for Multimodal RAG with Visual Citation in the Financial Domain](https://arxiv.org/abs/2505.17471)

*Suifeng Zhao, Zhuoran Jin, Sujian Li, Jun Gao*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Visual RAG, Finance, Bilingual retrieval corpus, Multimodal data

**Relevance Score:** 6

**TL;DR:** FinRAGBench-V is a benchmark for visual RAG in finance, integrating multimodal data for improved analysis.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in existing RAG research which focuses mainly on textual data and neglects visual content in financial documents.

**Method:** Development of FinRAGBench-V as a visual RAG benchmark with multimodal data integration, accompanied by a bilingual retrieval corpus and a QA dataset.

**Key Contributions:**

	1. Introduction of FinRAGBench-V for visual RAG in finance
	2. Creation of a bilingual retrieval corpus and QA dataset
	3. Development of an automatic citation evaluation method for MLLMs.

**Result:** RGenCite, a baseline RAG model, demonstrates the challenges of utilizing visual citation in multimodal RAG systems for finance.

**Limitations:** 

**Conclusion:** FinRAGBench-V provides crucial insights and capabilities for enhancing multimodal RAG applications in the financial domain.

**Abstract:** Retrieval-Augmented Generation (RAG) plays a vital role in the financial domain, powering applications such as real-time market analysis, trend forecasting, and interest rate computation. However, most existing RAG research in finance focuses predominantly on textual data, overlooking the rich visual content in financial documents, resulting in the loss of key analytical insights. To bridge this gap, we present FinRAGBench-V, a comprehensive visual RAG benchmark tailored for finance which effectively integrates multimodal data and provides visual citation to ensure traceability. It includes a bilingual retrieval corpus with 60,780 Chinese and 51,219 English pages, along with a high-quality, human-annotated question-answering (QA) dataset spanning heterogeneous data types and seven question categories. Moreover, we introduce RGenCite, an RAG baseline that seamlessly integrates visual citation with generation. Furthermore, we propose an automatic citation evaluation method to systematically assess the visual citation capabilities of Multimodal Large Language Models (MLLMs). Extensive experiments on RGenCite underscore the challenging nature of FinRAGBench-V, providing valuable insights for the development of multimodal RAG systems in finance.

</details>


### [129] [MARCO: Meta-Reflection with Cross-Referencing for Code Reasoning](https://arxiv.org/abs/2505.17481)

*Yusheng Zhao, Xiao Luo, Weizhi Zhang, Wei Ju, Zhiping Xiao, Philip S. Yu, Ming Zhang*

**Main category:** cs.CL

**Keywords:** large language models, code reasoning, self-improvement, meta-reflection, cross-referencing

**Relevance Score:** 9

**TL;DR:** This paper introduces MARCO, a framework that enables LLMs to dynamically improve their code reasoning abilities through self-reflection and cross-referencing with other agents' experiences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the problem-solving capabilities of LLMs in code reasoning by allowing them to self-improve during inference, thereby addressing the limitations of static problem-solving approaches.

**Method:** The framework, MARCO, employs meta-reflection for knowledge accumulation and cross-referencing for integrating lessons from other agents during problem-solving.

**Key Contributions:**

	1. Introduction of a dynamic self-improvement framework for LLMs in code reasoning.
	2. Utilization of cognitive development principles to enhance reasoning efficacy.
	3. Demonstration of improved performance through experimental validation over various datasets.

**Result:** Experiments show that MARCO significantly improves the effectiveness of LLMs in code reasoning over existing static approaches.

**Limitations:** 

**Conclusion:** By adopting a cognitive-evolving perspective, MARCO allows LLMs to evolve and improve their reasoning capabilities, leading to more effective problem-solving.

**Abstract:** The ability to reason is one of the most fundamental capabilities of large language models (LLMs), enabling a wide range of downstream tasks through sophisticated problem-solving. A critical aspect of this is code reasoning, which involves logical reasoning with formal languages (i.e., programming code). In this paper, we enhance this capability of LLMs by exploring the following question: how can an LLM agent become progressively smarter in code reasoning with each solution it proposes, thereby achieving substantial cumulative improvement? Most existing research takes a static perspective, focusing on isolated problem-solving using frozen LLMs. In contrast, we adopt a cognitive-evolving perspective and propose a novel framework named Meta-Reflection with Cross-Referencing (MARCO) that enables the LLM to evolve dynamically during inference through self-improvement. From the perspective of human cognitive development, we leverage both knowledge accumulation and lesson sharing. In particular, to accumulate knowledge during problem-solving, we propose meta-reflection that reflects on the reasoning paths of the current problem to obtain knowledge and experience for future consideration. Moreover, to effectively utilize the lessons from other agents, we propose cross-referencing that incorporates the solution and feedback from other agents into the current problem-solving process. We conduct experiments across various datasets in code reasoning, and the results demonstrate the effectiveness of MARCO.

</details>


### [130] [keepitsimple at SemEval-2025 Task 3: LLM-Uncertainty based Approach for Multilingual Hallucination Span Detection](https://arxiv.org/abs/2505.17485)

*Saketh Reddy Vemula, Parameswari Krishnamurthy*

**Main category:** cs.CL

**Keywords:** hallucination detection, language models, entropy analysis, text generation, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a method for identifying hallucinated spans in text generated by language models using entropy-based analysis to detect variability in responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Identifying hallucination spans in language model-generated text is crucial for real-world applications, especially in ensuring the reliability of AI-generated content.

**Method:** The proposed solution utilizes entropy-based analysis to measure the uniformity of stochastically-sampled responses from a language model, with the underlying hypothesis that hallucinations produce more diverse responses.

**Key Contributions:**

	1. Introduction of an entropy-based approach to identify hallucinated spans
	2. Cost-effective and adaptable method that does not rely on additional training
	3. Comprehensive hyperparameter tuning and error analysis conducted.

**Result:** The method allows for accurate identification of hallucinated segments without requiring additional model training, demonstrating cost-effectiveness and adaptability.

**Limitations:** 

**Conclusion:** The study concludes that the entropy-based approach is effective in detecting hallucinations in language model outputs and provides insights into the models behavior.

**Abstract:** Identification of hallucination spans in black-box language model generated text is essential for applications in the real world. A recent attempt at this direction is SemEval-2025 Task 3, Mu-SHROOM-a Multilingual Shared Task on Hallucinations and Related Observable Over-generation Errors. In this work, we present our solution to this problem, which capitalizes on the variability of stochastically-sampled responses in order to identify hallucinated spans. Our hypothesis is that if a language model is certain of a fact, its sampled responses will be uniform, while hallucinated facts will yield different and conflicting results. We measure this divergence through entropy-based analysis, allowing for accurate identification of hallucinated segments. Our method is not dependent on additional training and hence is cost-effective and adaptable. In addition, we conduct extensive hyperparameter tuning and perform error analysis, giving us crucial insights into model behavior.

</details>


### [131] [Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models](https://arxiv.org/abs/2505.17496)

*Chi-Yuan Hsiao, Ke-Han Lu, Kai-Wei Chang, Chih-Kai Yang, Wei-Chih Chen, Hung-yi Lee*

**Main category:** cs.CL

**Keywords:** Spoken Language Models, catastrophic forgetting, experience replay, multi-stage training, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper explores methods to mitigate catastrophic forgetting in the end-to-end training of Spoken Language Models (SLMs) adapted from pre-trained text-based LLMs, highlighting experience replay as the most effective strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of catastrophic forgetting in the multi-stage training of Spoken Language Models (SLMs), where differences in task and data distributions can lead to loss of previously learned knowledge.

**Method:** The paper evaluates three strategies for mitigating catastrophic forgetting: model merging, discounting the LoRA scaling factor, and experience replay, with experimental results to assess their effectiveness.

**Key Contributions:**

	1. Investigation of catastrophic forgetting in SLM training
	2. Evaluation of mitigation strategies including experience replay
	3. Insights for developing efficient SLM training pipelines

**Result:** Experience replay emerges as the most effective method, and combining it with other strategies yields further improvements in knowledge retention.

**Limitations:** The study might be limited to specific SLM architectures or datasets in its evaluations.

**Conclusion:** The findings provide insights that can guide the creation of more robust and efficient training pipelines for Spoken Language Models.

**Abstract:** End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.

</details>


### [132] [CReSt: A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents](https://arxiv.org/abs/2505.17503)

*Minsoo Khang, Sangjun Park, Teakgyu Hong, Dawoon Jung*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Benchmark, Complex Reasoning, Structured Documents

**Relevance Score:** 9

**TL;DR:** CReSt is a benchmark for evaluating Retrieval-Augmented Generation (RAG) with a focus on complex reasoning over structured documents, revealing LLMs' limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in Large Language Models (LLMs), assessing their capabilities in practical RAG scenarios poses challenges. A unified evaluation framework is necessary to collectively measure their performance in handling complex tasks.

**Method:** The paper introduces CReSt, a benchmark containing 2,245 human-annotated examples in English and Korean, aimed at evaluating the performance of LLMs in RAG scenarios that require complex reasoning over structured documents. The evaluation methodology is tailored to assess key dimensions of LLM performance holistically.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark (CReSt) for RAG evaluation.
	2. Inclusion of complex reasoning scenarios in structured documents.
	3. Release of a tailored evaluation methodology for model performance assessment.

**Result:** The evaluation reveals that even advanced LLMs face difficulties maintaining consistency across the crucial dimensions of reasoning, citation precision, uncertainty awareness, and document comprehension.

**Limitations:** The focus is primarily on English and Korean languages, which may limit applicability to other languages or contexts.

**Conclusion:** The findings highlight significant areas for improvement in LLMs and aim to foster the development of more robust RAG systems. CReSt is released for ongoing research in this domain.

**Abstract:** Large Language Models (LLMs) have made substantial progress in recent years, yet evaluating their capabilities in practical Retrieval-Augmented Generation (RAG) scenarios remains challenging. In practical applications, LLMs must demonstrate complex reasoning, refuse to answer appropriately, provide precise citations, and effectively understand document layout. These capabilities are crucial for advanced task handling, uncertainty awareness, maintaining reliability, and structural understanding. While some of the prior works address these aspects individually, there is a need for a unified framework that evaluates them collectively in practical RAG scenarios. To address this, we present CReSt (A Comprehensive Benchmark for Retrieval-Augmented Generation with Complex Reasoning over Structured Documents), a benchmark designed to assess these key dimensions holistically. CReSt comprises 2,245 human-annotated examples in English and Korean, designed to capture practical RAG scenarios that require complex reasoning over structured documents. It also introduces a tailored evaluation methodology to comprehensively assess model performance in these critical areas. Our evaluation shows that even advanced LLMs struggle to perform consistently across these dimensions, underscoring key areas for improvement. We release CReSt to support further research and the development of more robust RAG systems. The dataset and code are available at: https://github.com/UpstageAI/CReSt.

</details>


### [133] [L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models](https://arxiv.org/abs/2505.17505)

*Xiaohao Liu, Xiaobo Xia, Weixiang Zhao, Manyi Zhang, Xianzhi Yu, Xiu Su, Shuo Yang, See-Kiong Ng, Tat-Seng Chua*

**Main category:** cs.CL

**Keywords:** large language models, multi-token prediction, inference efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces leap multi-token prediction (L-MTP), a novel approach for training large language models that enhances inference efficiency and performance by predicting non-sequential tokens in one pass.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of next-token prediction in LLMs related to contextual coverage and inference efficiency.

**Method:** The paper proposes L-MTP, which utilizes a leap-based mechanism to predict non-sequential tokens, thereby enhancing the model's ability to capture long-range dependencies and improving inference speed.

**Key Contributions:**

	1. Introduction of leap multi-token prediction (L-MTP) method
	2. Enhanced inference efficiency through non-sequential token prediction
	3. Demonstrated improvement in LLM performance across benchmarks

**Result:** L-MTP shows significant improvements in LLM performance and inference speed across various benchmarks, demonstrating the advantages of this new method over conventional prediction techniques.

**Limitations:** 

**Conclusion:** The proposed L-MTP method effectively addresses the limitations of traditional next-token prediction methods in LLMs, promising faster and more efficient token generation.

**Abstract:** Large language models (LLMs) have achieved notable progress. Despite their success, next-token prediction (NTP), the dominant method for LLM training and inference, is constrained in both contextual coverage and inference efficiency due to its inherently sequential process. To overcome these challenges, we propose leap multi-token prediction~(L-MTP), an innovative token prediction method that extends the capabilities of multi-token prediction (MTP) by introducing a leap-based mechanism. Unlike conventional MTP, which generates multiple tokens at adjacent positions, L-MTP strategically skips over intermediate tokens, predicting non-sequential ones in a single forward pass. This structured leap not only enhances the model's ability to capture long-range dependencies but also enables a decoding strategy specially optimized for non-sequential leap token generation, effectively accelerating inference. We theoretically demonstrate the benefit of L-MTP in improving inference efficiency. Experiments across diverse benchmarks validate its merit in boosting both LLM performance and inference speed. The source code will be publicly available.

</details>


### [134] [Large Language Models Do Multi-Label Classification Differently](https://arxiv.org/abs/2505.17510)

*Marcus Ma, Georgios Chochlakis, Niyantha Maruthu Pandiyan, Jesse Thomason, Shrikanth Narayanan*

**Main category:** cs.CL

**Keywords:** Multi-label classification, Large Language Models, Autoregressive Models, Finetuning, Distribution Alignment

**Relevance Score:** 8

**TL;DR:** The paper examines the performance of Large Language Models (LLMs) in multi-label classification through a detailed analysis of their output behavior, particularly in subjective tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the performance of autoregressive LLMs in multi-label classification tasks, particularly how they manage the generation of multiple relevant labels and the impact of model scaling and finetuning techniques.

**Method:** The study analyzes output distributions of LLMs during generation, investigating the entropy of token distributions and the effectiveness of zero-shot and supervised methods for distribution alignment.

**Key Contributions:**

	1. Analysis of LLMs' predictive behavior in multi-label classification.
	2. Introduction of distribution alignment task for multi-label settings.
	3. Demonstration of the effectiveness of zero-shot and supervised methods for performance improvement.

**Result:** LLMs suppress all but one label in multi-label tasks, showing improved internal ranking of labels as model scale increases, with lower entropy in token distributions. Proposed methods enhance alignment and predictive performance.

**Limitations:** 

**Conclusion:** The findings suggest that finetuning methods combined with new alignment tasks can significantly improve the performance of LLMs in multi-label classification.

**Abstract:** Multi-label classification is prevalent in real-world settings, but the behavior of Large Language Models (LLMs) in this setting is understudied. We investigate how autoregressive LLMs perform multi-label classification, with a focus on subjective tasks, by analyzing the output distributions of the models in each generation step. We find that their predictive behavior reflects the multiple steps in the underlying language modeling required to generate all relevant labels as they tend to suppress all but one label at each step. We further observe that as model scale increases, their token distributions exhibit lower entropy, yet the internal ranking of the labels improves. Finetuning methods such as supervised finetuning and reinforcement learning amplify this phenomenon. To further study this issue, we introduce the task of distribution alignment for multi-label settings: aligning LLM-derived label distributions with empirical distributions estimated from annotator responses in subjective tasks. We propose both zero-shot and supervised methods which improve both alignment and predictive performance over existing approaches.

</details>


### [135] [Multimodal Conversation Structure Understanding](https://arxiv.org/abs/2505.17536)

*Kent K. Chang, Mackenzie Hanh Cramer, Anna Ho, Ti Ti Nguyen, Yilin Yuan, David Bamman*

**Main category:** cs.CL

**Keywords:** multimodal LLMs, conversational role attribution, conversation threading, dialogue analysis, sociolinguistics

**Relevance Score:** 7

**TL;DR:** This paper introduces tasks for understanding conversational structure in multi-modal, multi-party settings and evaluates the performance of LLMs on these tasks using a human-annotated dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underdeveloped understanding of fine-grained conversational structures in dialogues, particularly within multi-modal, multi-party environments.

**Method:** A suite of tasks focused on conversational role attribution and conversation threading is introduced, supported by a human-annotated dataset of 4,398 annotations for speaker roles, 5,755 for addressees, and 3,142 for side-participants.

**Key Contributions:**

	1. Introduction of a suite of tasks for conversational role attribution and threading.
	2. Presentation of a human-annotated dataset for evaluating LLMs in multitask conversations.
	3. Insight into the factors affecting performance in role-attribution tasks.

**Result:** Evaluation of various audio-visual LLMs revealed that while one outperformed vision-language models in recognizing speakers and addressees, performance dropped significantly with anonymization of participants; number of participants negatively impacted role-attribution accuracy.

**Limitations:** The study primarily focuses on audio-visual LLMs and may not adequately represent other models; significant performance drops with anonymized participants may limit practical applications.

**Conclusion:** The findings highlight the challenges in multimodal conversational structure understanding and suggest avenues for future development in LLMs to enhance reasoning about conversational dynamics.

**Abstract:** Conversations are usually structured by roles -- who is speaking, who's being addressed, and who's listening -- and unfold in threads that break with changes in speaker floor or topical focus. While large language models (LLMs) have shown incredible capabilities in dialogue and reasoning, their ability to understand fine-grained conversational structure, especially in multi-modal, multi-party settings, remains underexplored. To address this gap, we introduce a suite of tasks focused on conversational role attribution (speaker, addressees, side-participants) and conversation threading (utterance linking and clustering), drawing on conversation analysis and sociolinguistics. To support those tasks, we present a human annotated dataset of 4,398 annotations for speakers and reply-to relationship, 5,755 addressees, and 3,142 side-participants.   We evaluate popular audio-visual LLMs and vision-language models on our dataset, and our experimental results suggest that multimodal conversational structure understanding remains challenging. The most performant audio-visual LLM outperforms all vision-language models across all metrics, especially in speaker and addressee recognition. However, its performance drops significantly when conversation participants are anonymized. The number of conversation participants in a clip is the strongest negative predictor of role-attribution performance, while acoustic clarity (measured by pitch and spectral centroid) and detected face coverage yield positive associations. We hope this work lays the groundwork for future evaluation and development of multimodal LLMs that can reason more effectively about conversation structure.

</details>


### [136] [How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary Perception](https://arxiv.org/abs/2505.17537)

*Shiyu Ni, Keping Bi, Jiafeng Guo, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** large language models, knowledge boundaries, factual question answering, knowledge popularity, confidence calibration

**Relevance Score:** 9

**TL;DR:** The paper investigates how knowledge popularity impacts large language models' (LLMs) ability to recognize their knowledge boundaries and proposes a method for improving answer correctness prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding LLMs' perception of their knowledge boundaries is critical to improving their reliability, especially in factual question answering.

**Method:** The study quantifies knowledge popularity through entity popularity in questions and answers, as well as relation popularity based on co-occurrence frequency, and tests these factors on three datasets.

**Key Contributions:**

	1. Quantifies knowledge popularity from multiple perspectives for LLMs.
	2. Demonstrates correlation between knowledge popularity and LLMs' QA performance.
	3. Proposes a method for confidence calibration based on popularity signals.

**Result:** LLMs demonstrate improved QA performance and confidence when dealing with more popular knowledge, especially in relation to relation popularity. The proposed calibration method enhances prediction accuracy of answer correctness by 5.24%.

**Limitations:** 

**Conclusion:** Enhancing LLMs ability to recognize knowledge boundaries through knowledge popularity can significantly improve performance and answer correctness predictions.

**Abstract:** Large language models (LLMs) often fail to recognize their knowledge boundaries, producing confident yet incorrect answers. In this paper, we investigate how knowledge popularity affects LLMs' ability to perceive their knowledge boundaries. Focusing on entity-centric factual question answering (QA), we quantify knowledge popularity from three perspectives: the popularity of entities in the question, the popularity of entities in the answer, and relation popularity, defined as their co-occurrence frequency. Experiments on three representative datasets containing knowledge with varying popularity show that LLMs exhibit better QA performance, higher confidence, and more accurate perception on more popular knowledge, with relation popularity having the strongest correlation. Cause knowledge popularity shows strong correlation with LLMs' QA performance, we propose to leverage these signals for confidence calibration. This improves the accuracy of answer correctness prediction by an average of 5.24% across all models and datasets. Furthermore, we explore prompting LLMs to estimate popularity without external corpora, which yields a viable alternative.

</details>


### [137] [Swedish Whispers; Leveraging a Massive Speech Corpus for Swedish Speech Recognition](https://arxiv.org/abs/2505.17538)

*Leonora Vesterbacka, Faton Rekathati, Robin Kurtz, Justyna Sikora, Agnes Toftgrd*

**Main category:** cs.CL

**Keywords:** Whisper models, Swedish language, machine learning, natural language processing, word error rate

**Relevance Score:** 4

**TL;DR:** This paper introduces fine-tuned Whisper models for Swedish, achieving significant performance improvements over OpenAI's Whisper models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address underrepresentation of mid-resourced languages like Swedish in multilingual training datasets.

**Method:** Fine-tuned Whisper models evaluated on a large dataset for Swedish, compared against OpenAI's Whisper models.

**Key Contributions:**

	1. Development of a suite of fine-tuned Whisper models for Swedish
	2. Demonstrated substantial WER reduction compared to existing models
	3. Utilized a large and varied dataset for training

**Result:** The best model achieved a 47% reduction in word error rate (WER) compared to OpenAI's whisper-large-v3 across multiple datasets.

**Limitations:** 

**Conclusion:** Fine-tuning existing multilingual models significantly enhances performance for underrepresented languages like Swedish.

**Abstract:** This work presents a suite of fine-tuned Whisper models for Swedish, trained on a dataset of unprecedented size and variability for this mid-resourced language. As languages of smaller sizes are often underrepresented in multilingual training datasets, substantial improvements in performance can be achieved by fine-tuning existing multilingual models, as shown in this work. This work reports an overall improvement across model sizes compared to OpenAI's Whisper evaluated on Swedish. Most notably, we report an average 47% reduction in WER comparing our best performing model to OpenAI's whisper-large-v3, in evaluations across FLEURS, Common Voice, and NST.

</details>


### [138] [Teaching with Lies: Curriculum DPO on Synthetic Negatives for Hallucination Detection](https://arxiv.org/abs/2505.17558)

*Shrey Pandit, Ashwin Vinod, Liu Leqi, Ying Ding*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination Detection, Curriculum Learning, Negative Examples, Robustness

**Relevance Score:** 9

**TL;DR:** The paper presents HaluCheck, a method to improve large language models' accuracy in detecting hallucinations using a curriculum learning strategy with engineered negative samples.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning large language models to detect hallucinations is challenging due to the sophisticated nature of hallucinated text.

**Method:** The approach utilizes a curriculum learning strategy that starts training with easier samples from independent fact-checking models, gradually introducing harder samples.

**Key Contributions:**

	1. Introduction of HaluCheck for hallucination detection
	2. Use of curriculum learning with high-quality negative samples
	3. Demonstrated performance improvements across difficult benchmarks

**Result:** HaluCheck models show significant performance improvements of up to 24% on benchmarks like MedHallu and HaluEval, demonstrating robustness in zero-shot settings.

**Limitations:** 

**Conclusion:** The curriculum DPO approach enhances model performance and stability in hallucination detection tasks.

**Abstract:** Aligning large language models (LLMs) to accurately detect hallucinations remains a significant challenge due to the sophisticated nature of hallucinated text. Recognizing that hallucinated samples typically exhibit higher deceptive quality than traditional negative samples, we use these carefully engineered hallucinations as negative examples in the DPO alignment procedure. Our method incorporates a curriculum learning strategy, gradually transitioning the training from easier samples, identified based on the greatest reduction in probability scores from independent fact checking models, to progressively harder ones. This structured difficulty scaling ensures stable and incremental learning. Experimental evaluation demonstrates that our HaluCheck models, trained with curriculum DPO approach and high quality negative samples, significantly improves model performance across various metrics, achieving improvements of upto 24% on difficult benchmarks like MedHallu and HaluEval. Additionally, HaluCheck models demonstrate robustness in zero-shot settings, significantly outperforming larger state-of-the-art models across various benchmarks.

</details>


### [139] [PPT: A Process-based Preference Learning Framework for Self Improving Table Question Answering Models](https://arxiv.org/abs/2505.17565)

*Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich*

**Main category:** cs.CL

**Keywords:** table question answering, large language models, preference learning, self-generated data, machine learning

**Relevance Score:** 7

**TL;DR:** This paper introduces PPT, a novel framework for improving table question answering (TQA) using self-generated data and preference learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of exploration in improving LLMs for table question answering, highlighting the importance of self-improvement in TQA to enhance performance without needing costly annotated data.

**Method:** The proposed PPT framework decomposes reasoning chains in TQA into discrete states, assigns scores, and samples contrastive steps for preference learning.

**Key Contributions:**

	1. Introduction of the PPT framework for TQA.
	2. Demonstrated efficiency and effectiveness in using self-generated data for improvement.
	3. Achieved competitive results with fewer resources compared to existing models.

**Result:** Experimental results indicate that PPT improves TQA model performance by up to 5% on in-domain datasets and 2.4% on out-of-domain datasets using only 8,000 preference pairs.

**Limitations:** 

**Conclusion:** The models improved by PPT demonstrate competitive performance compared to larger state-of-the-art TQA systems while being five times more efficient in inference.

**Abstract:** Improving large language models (LLMs) with self-generated data has demonstrated success in tasks such as mathematical reasoning and code generation. Yet, no exploration has been made on table question answering (TQA), where a system answers questions based on tabular data. Addressing this gap is crucial for TQA, as effective self-improvement can boost performance without requiring costly or manually annotated data. In this work, we propose PPT, a Process-based Preference learning framework for TQA. It decomposes reasoning chains into discrete states, assigns scores to each state, and samples contrastive steps for preference learning. Experimental results show that PPT effectively improves TQA models by up to 5% on in-domain datasets and 2.4% on out-of-domain datasets, with only 8,000 preference pairs. Furthermore, the resulting models achieve competitive results compared to more complex and larger state-of-the-art TQA systems, while being five times more efficient during inference.

</details>


### [140] [Reasoning Meets Personalization: Unleashing the Potential of Large Reasoning Model for Personalized Generation](https://arxiv.org/abs/2505.17571)

*Sichun Luo, Guanzhi Deng, Jian Xu, Xiaojie Zhang, Hanxu Hou, Linqi Song*

**Main category:** cs.CL

**Keywords:** personalization, large reasoning models, reinforced reasoning, human-computer interaction, LLM

**Relevance Score:** 8

**TL;DR:** This paper systematically evaluates large reasoning models (LRMs) for personalization tasks and proposes a new framework to improve their efficacy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the underexplored potential of LLMs for personalization tasks despite their advanced reasoning capabilities.

**Method:** The proposed framework, Reinforced Reasoning for Personalization, integrates a hierarchical reasoning thought template and a reasoning process intervention method for better structured outputs and adherence to reasoning patterns.

**Key Contributions:**

	1. Systematic evaluation of LRMs for personalization tasks
	2. Introduction of the Reinforced Reasoning for Personalization framework
	3. Proposed cross-referencing mechanism to ensure output consistency

**Result:** Extensive experiments show that the proposed approach significantly outperforms existing techniques in personalization tasks.

**Limitations:** Identified limitations include divergent thinking, misalignment of response formats, and ineffective use of retrieved information.

**Conclusion:** The study highlights the limitations of LRMs in retrieval-intensive scenarios and suggests enhancements to improve personalization outcomes.

**Abstract:** Personalization is a critical task in modern intelligent systems, with applications spanning diverse domains, including interactions with large language models (LLMs). Recent advances in reasoning capabilities have significantly enhanced LLMs, enabling unprecedented performance in tasks such as mathematics and coding. However, their potential for personalization tasks remains underexplored.   In this paper, we present the first systematic evaluation of large reasoning models (LRMs) for personalization tasks. Surprisingly, despite generating more tokens, LRMs do not consistently outperform general-purpose LLMs, especially in retrieval-intensive scenarios where their advantages diminish. Our analysis identifies three key limitations: divergent thinking, misalignment of response formats, and ineffective use of retrieved information. To address these challenges, we propose Reinforced Reasoning for Personalization (\model), a novel framework that incorporates a hierarchical reasoning thought template to guide LRMs in generating structured outputs. Additionally, we introduce a reasoning process intervention method to enforce adherence to designed reasoning patterns, enhancing alignment. We also propose a cross-referencing mechanism to ensure consistency. Extensive experiments demonstrate that our approach significantly outperforms existing techniques.

</details>


### [141] [Wolf Hidden in Sheep's Conversations: Toward Harmless Data-Based Backdoor Attacks for Jailbreaking Large Language Models](https://arxiv.org/abs/2505.17601)

*Jiawei Kong, Hao Fang, Xiaochen Yang, Kuofeng Gao, Bin Chen, Shu-Tao Xia, Yaowei Wang, Min Zhang*

**Main category:** cs.CL

**Keywords:** backdoor attack, large language models, clean-data, safety alignment, gradient-based optimization

**Relevance Score:** 6

**TL;DR:** The paper presents a novel backdoor attack targeting large language models (LLMs) that uses benign prefixes in responses to inject harmful content while evading safety measures.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing backdoor attacks on LLMs that are easily detectable or compromise model safety alignment.

**Method:** A clean-data backdoor attack is proposed, which employs harmless question-answer pairs to overfit benign-sounding prefixes, using gradient-based optimization to strengthen the universal trigger.

**Key Contributions:**

	1. Introduction of a clean-data backdoor attack tailored for LLMs.
	2. Optimization technique for enhancing the attack's efficacy.
	3. Demonstration of high attack success rates against models with safety measures.

**Result:** The method demonstrated a successful attack with high attack success rates of 86.67% and 85% on LLaMA-3-8B and Qwen-2.5-7B models, respectively, even against safety-aligned guardrail models.

**Limitations:** Potential ethical implications of employing such attacks and the need for further research on defense mechanisms.

**Conclusion:** This approach shows promise in effectively leveraging benign interactions to compromise LLMs, posing a significant threat to model safety.

**Abstract:** Supervised fine-tuning (SFT) aligns large language models (LLMs) with human intent by training them on labeled task-specific data. Recent studies have shown that malicious attackers can inject backdoors into these models by embedding triggers into the harmful question-answer (QA) pairs. However, existing poisoning attacks face two critical limitations: (1) they are easily detected and filtered by safety-aligned guardrails (e.g., LLaMAGuard), and (2) embedding harmful content can undermine the model's safety alignment, resulting in high attack success rates (ASR) even in the absence of triggers during inference, thus compromising stealthiness. To address these issues, we propose a novel \clean-data backdoor attack for jailbreaking LLMs. Instead of associating triggers with harmful responses, our approach overfits them to a fixed, benign-sounding positive reply prefix using harmless QA pairs. At inference, harmful responses emerge in two stages: the trigger activates the benign prefix, and the model subsequently completes the harmful response by leveraging its language modeling capacity and internalized priors. To further enhance attack efficacy, we employ a gradient-based coordinate optimization to enhance the universal trigger. Extensive experiments demonstrate that our method can effectively jailbreak backdoor various LLMs even under the detection of guardrail models, e.g., an ASR of 86.67% and 85% on LLaMA-3-8B and Qwen-2.5-7B judged by GPT-4o.

</details>


### [142] [Distilling LLM Agent into Small Models with Retrieval and Code Tools](https://arxiv.org/abs/2505.17612)

*Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang*

**Main category:** cs.CL

**Keywords:** Language Models, Distillation, Agent Behavior, Reasoning, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Agent Distillation, a framework for transferring reasoning and task-solving behaviors from large language models to smaller models while addressing limitations in factual knowledge and computation accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the practical deployment of large language models by distilling their reasoning capabilities into smaller, more efficient models that can still perform effectively on complex tasks.

**Method:** The authors propose a framework called Agent Distillation, utilizing a prompting method known as first-thought prefix to enhance teacher trajectory quality and a self-consistent action generation to improve robustness during testing.

**Key Contributions:**

	1. Introduction of the Agent Distillation framework.
	2. Development of first-thought prefix prompting method.
	3. Proposal of self-consistent action generation for small agents.

**Result:** The evaluation on eight reasoning tasks shows that small models (sLMs) with as few as 0.5B parameters can achieve competitive performance compared to larger models, demonstrating the effectiveness of the proposed agent distillation method.

**Limitations:** The framework may still struggle with scenarios requiring extensive rare factual knowledge or precise computation.

**Conclusion:** Agent distillation shows promise in developing practical small agents capable of utilizing retrieval and coding tools effectively, overcoming some of the limitations of traditional distillation techniques.

**Abstract:** Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.

</details>


### [143] [Runaway is Ashamed, But Helpful: On the Early-Exit Behavior of Large Language Model-based Agents in Embodied Environments](https://arxiv.org/abs/2505.17616)

*Qingyu Lu, Liang Ding, Siyi Cao, Xuebo Liu, Kanjian Zhang, Jinxia Zhang, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Early Exit Mechanisms, Efficiency, Multi-turn Interactions, Embodied Agents

**Relevance Score:** 8

**TL;DR:** This paper presents early-exit mechanisms for LLM-based agents to improve efficiency in multi-turn interactions by reducing redundant commands and computational overhead.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies in multi-turn interactions of LLM-based agents that can lead to repetitive and ineffective command execution.

**Method:** Introduce intrinsic and extrinsic methods for early exit, with metrics to evaluate reduction of redundant steps and progress degradation.

**Key Contributions:**

	1. Two novel early-exit methods for LLM-based agents
	2. Evaluation metrics for measuring efficiency and degradation
	3. A practical strategy for combining early-exit agents with stronger agents

**Result:** Experiments with 4 LLMs across 5 environments exhibited significant efficiency improvements with minor drops in performance.

**Limitations:** 

**Conclusion:** Implementing early-exit mechanisms can enhance the efficiency of LLM-based agents in embodied environments without greatly harming performance.

**Abstract:** Agents powered by large language models (LLMs) have demonstrated strong planning and decision-making capabilities in complex embodied environments. However, such agents often suffer from inefficiencies in multi-turn interactions, frequently trapped in repetitive loops or issuing ineffective commands, leading to redundant computational overhead. Instead of relying solely on learning from trajectories, we take a first step toward exploring the early-exit behavior for LLM-based agents. We propose two complementary approaches: 1. an $\textbf{intrinsic}$ method that injects exit instructions during generation, and 2. an $\textbf{extrinsic}$ method that verifies task completion to determine when to halt an agent's trial. To evaluate early-exit mechanisms, we introduce two metrics: one measures the reduction of $\textbf{redundant steps}$ as a positive effect, and the other evaluates $\textbf{progress degradation}$ as a negative effect. Experiments with 4 different LLMs across 5 embodied environments show significant efficiency improvements, with only minor drops in agent performance. We also validate a practical strategy where a stronger agent assists after an early-exit agent, achieving better performance with the same total steps. We will release our code to support further research.

</details>


### [144] [Enhancing Large Vision-Language Models with Layout Modality for Table Question Answering on Japanese Annual Securities Reports](https://arxiv.org/abs/2505.17625)

*Hayato Aida, Kosuke Takahashi, Takahiro Omi*

**Main category:** cs.CL

**Keywords:** Large Language Models, table understanding, multimodal LLMs, documents, layout features

**Relevance Score:** 8

**TL;DR:** This paper proposes a method to improve table understanding in Large Vision-Language Models (LVLMs) by incorporating in-table textual content and layout features, addressing challenges related to multimodal LLMs in accurately interpreting complex document structures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With advancements in Large Language Models, understanding table structures is crucial for financial domains, especially for accurate question answering over diverse table formats.

**Method:** The study enhances LVLM-based table understanding by integrating in-table textual content and layout features into the analysis process.

**Key Contributions:**

	1. Proposed method of integrating in-table textual features into LVLMs
	2. Demonstrated significant performance gains in table understanding
	3. Addressed challenges in interpreting complex document layouts

**Result:** Experimental results show significant performance improvement in interpreting complex document layouts when auxiliary modalities are used.

**Limitations:** 

**Conclusion:** Incorporating additional textual and layout information enhances the LVLM's capability, allowing for better interpretation without the need for structured input formats.

**Abstract:** With recent advancements in Large Language Models (LLMs) and growing interest in retrieval-augmented generation (RAG), the ability to understand table structures has become increasingly important. This is especially critical in financial domains such as securities reports, where highly accurate question answering (QA) over tables is required. However, tables exist in various formats-including HTML, images, and plain text-making it difficult to preserve and extract structural information. Therefore, multimodal LLMs are essential for robust and general-purpose table understanding. Despite their promise, current Large Vision-Language Models (LVLMs), which are major representatives of multimodal LLMs, still face challenges in accurately understanding characters and their spatial relationships within documents. In this study, we propose a method to enhance LVLM-based table understanding by incorporating in-table textual content and layout features. Experimental results demonstrate that these auxiliary modalities significantly improve performance, enabling robust interpretation of complex document layouts without relying on explicitly structured input formats.

</details>


### [145] [GIM: Improved Interpretability for Large Language Models](https://arxiv.org/abs/2505.17630)

*Joakim Edin, Rbert Csords, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Jing Huang, Lars Maale*

**Main category:** cs.CL

**Keywords:** large language models, interpretability, self-repair, attention mechanism, Gradient Interaction Modifications

**Relevance Score:** 9

**TL;DR:** This paper introduces Gradient Interaction Modifications (GIM) to enhance the interpretability of large language models (LLMs) by addressing the self-repair phenomenon in attention mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for trustworthy AI necessitates faithful interpretability in large language models, which is obstructed by the self-repair phenomenon that masks component significance.

**Method:** Gradient Interaction Modifications (GIM) is introduced to account for self-repair during backpropagation, improving interpretability in LLMs.

**Key Contributions:**

	1. Introduction of GIM to address self-repair in attention mechanisms
	2. Demonstration of GIM's effectiveness across multiple LLMs and tasks
	3. Contribution to the understanding of LLM interpretability and safety

**Result:** GIM significantly improves the faithfulness of circuit identification and feature attribution methods across various large language models and tasks.

**Limitations:** 

**Conclusion:** Enhancing interpretability through GIM is crucial for understanding LLM mechanisms, thereby promoting their improvement and safety.

**Abstract:** Ensuring faithful interpretability in large language models is imperative for trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where networks compensate for reduced signal in one component by amplifying others, masking the true importance of the ablated component. While prior work attributes self-repair to layer normalization and back-up components that compensate for ablated components, we identify a novel form occurring within the attention mechanism, where softmax redistribution conceals the influence of important attention scores. This leads traditional ablation and gradient-based methods to underestimate the significance of all components contributing to these attention scores. We introduce Gradient Interaction Modifications (GIM), a technique that accounts for self-repair during backpropagation. Extensive experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B, Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves faithfulness over existing circuit identification and feature attribution methods. Our work is a significant step toward better understanding the inner mechanisms of LLMs, which is crucial for improving them and ensuring their safety. Our code is available at https://github.com/JoakimEdin/gim.

</details>


### [146] [Stereotype Detection in Natural Language Processing](https://arxiv.org/abs/2505.17642)

*Alessandra Teresa Cignarella, Anastasia Giachanou, Els Lefever*

**Main category:** cs.CL

**Keywords:** stereotype detection, natural language processing, bias, hate speech, multilingual approach

**Relevance Score:** 4

**TL;DR:** This paper surveys existing research on stereotype detection, analyzing definitions and methodologies in the context of NLP, highlighting its potential to prevent bias escalation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the role of stereotypes in social perceptions and their escalation into discrimination and violence, and to address the underexplored area of stereotype detection in NLP.

**Method:** A semi-automatic literature review of over 6,000 papers from 2000 to 2025 was conducted using Semantic Scholar to identify trends and methodologies.

**Key Contributions:**

	1. Comprehensive survey of existing research on stereotype detection across disciplines.
	2. Identification of key trends and methodologies in the field.
	3. Recommendations for future research directions in stereotype detection.

**Result:** Key trends in stereotype detection methodologies were identified, along with challenges and future directions for research, emphasizing the effectiveness of stereotype detection in preventing bias and hate speech.

**Limitations:** 

**Conclusion:** A broader, multilingual, and intersectional approach in NLP studies is necessary to effectively address stereotype detection.

**Abstract:** Stereotypes influence social perceptions and can escalate into discrimination and violence. While NLP research has extensively addressed gender bias and hate speech, stereotype detection remains an emerging field with significant societal implications. In this work is presented a survey of existing research, analyzing definitions from psychology, sociology, and philosophy. A semi-automatic literature review was performed by using Semantic Scholar. We retrieved and filtered over 6,000 papers (in the year range 2000-2025), identifying key trends, methodologies, challenges and future directions. The findings emphasize stereotype detection as a potential early-monitoring tool to prevent bias escalation and the rise of hate speech. Conclusions highlight the need for a broader, multilingual, and intersectional approach in NLP studies.

</details>


### [147] [Bridging Electronic Health Records and Clinical Texts: Contrastive Learning for Enhanced Clinical Tasks](https://arxiv.org/abs/2505.17643)

*Sara Ketabi, Dhanesh Ramachandram*

**Main category:** cs.CL

**Keywords:** machine learning, electronic health records, contrastive learning, discharge summaries, clinical prediction

**Relevance Score:** 9

**TL;DR:** Proposes a multimodal contrastive learning framework to enhance clinical predictions from EHR by integrating unstructured discharge summaries.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional ML models struggle with tasks requiring contextual understanding in clinical settings, particularly with structured EHR data.

**Method:** Developed a deep multimodal contrastive learning framework that aligns structured EHR data with unstructured discharge summary text.

**Key Contributions:**

	1. Introduction of a multimodal contrastive learning framework for EHR data and clinical notes.
	2. Demonstration of improved performance in clinical predictions using the proposed model.
	3. Highlighting the importance of unstructured data in enhancing machine learning outcomes for health informatics.

**Result:** Fine-tuning the pretrained EHR encoder led to a 4.1% AUROC improvement over XGBoost in predicting 30-day hospital readmissions.

**Limitations:** 

**Conclusion:** Integrating clinical notes into EHR pipelines improves accuracy and context-awareness in clinical decision support systems.

**Abstract:** Conventional machine learning models, particularly tree-based approaches, have demonstrated promising performance across various clinical prediction tasks using electronic health record (EHR) data. Despite their strengths, these models struggle with tasks that require deeper contextual understanding, such as predicting 30-day hospital readmission. This can be primarily due to the limited semantic information available in structured EHR data. To address this limitation, we propose a deep multimodal contrastive learning (CL) framework that aligns the latent representations of structured EHR data with unstructured discharge summary notes. It works by pulling together paired EHR and text embeddings while pushing apart unpaired ones. Fine-tuning the pretrained EHR encoder extracted from this framework significantly boosts downstream task performance, e.g., a 4.1% AUROC enhancement over XGBoost for 30-day readmission prediction. Such results demonstrate the effect of integrating domain knowledge from clinical notes into EHR-based pipelines, enabling more accurate and context-aware clinical decision support systems.

</details>


### [148] [EVADE: Multimodal Benchmark for Evasive Content Detection in E-Commerce Applications](https://arxiv.org/abs/2505.17654)

*Ancheng Xu, Zhihao Yang, Jingpeng Li, Guanghu Yuan, Longze Chen, Liang Yan, Jiehui Zhou, Zhen Qin, Hengyun Chang, Hamid Alinejad-Rokny, Bo Zheng, Min Yang*

**Main category:** cs.CL

**Keywords:** E-commerce, Language Models, Evasive Content, Content Moderation, Benchmarking

**Relevance Score:** 9

**TL;DR:** The paper introduces EVADE, a benchmark for evaluating the detection of evasive content in e-commerce using LLMs and VLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** E-commerce platforms are challenged by evasive content that appears compliant with policies yet contains prohibited claims, requiring better evaluation benchmarks for detection.

**Method:** The study introduces the EVADE benchmark with 2,833 annotated text samples and 13,961 images across six categories, assessing models through Single-Violation and All-in-One tasks to evaluate fine-grained and long-context reasoning, respectively.

**Key Contributions:**

	1. Introduction of the EVADE benchmark for evasive content detection
	2. Evaluation of 26 mainstream LLMs and VLMs on this new benchmark
	3. Insight into the impact of clearer rule definitions on model performance

**Result:** Benchmarking 26 LLMs and VLMs revealed significant performance gaps, indicating that even advanced models struggle with evasive samples; clearer rule definitions enhance model alignment with human judgment.

**Limitations:** The benchmark focuses on specific product categories and may not generalize to other content types or languages.

**Conclusion:** EVADE sets a new standard for detecting evasive content in e-commerce, highlighting limitations in current models while promoting safer content moderation.

**Abstract:** E-commerce platforms increasingly rely on Large Language Models (LLMs) and Vision-Language Models (VLMs) to detect illicit or misleading product content. However, these models remain vulnerable to evasive content: inputs (text or images) that superficially comply with platform policies while covertly conveying prohibited claims. Unlike traditional adversarial attacks that induce overt failures, evasive content exploits ambiguity and context, making it far harder to detect. Existing robustness benchmarks provide little guidance for this demanding, real-world challenge. We introduce EVADE, the first expert-curated, Chinese, multimodal benchmark specifically designed to evaluate foundation models on evasive content detection in e-commerce. The dataset contains 2,833 annotated text samples and 13,961 images spanning six demanding product categories, including body shaping, height growth, and health supplements. Two complementary tasks assess distinct capabilities: Single-Violation, which probes fine-grained reasoning under short prompts, and All-in-One, which tests long-context reasoning by merging overlapping policy rules into unified instructions. Notably, the All-in-One setting significantly narrows the performance gap between partial and full-match accuracy, suggesting that clearer rule definitions improve alignment between human and model judgment. We benchmark 26 mainstream LLMs and VLMs and observe substantial performance gaps: even state-of-the-art models frequently misclassify evasive samples. By releasing EVADE and strong baselines, we provide the first rigorous standard for evaluating evasive-content detection, expose fundamental limitations in current multimodal reasoning, and lay the groundwork for safer and more transparent content moderation systems in e-commerce. The dataset is publicly available at https://huggingface.co/datasets/koenshen/EVADE-Bench.

</details>


### [149] [Too Consistent to Detect: A Study of Self-Consistent Errors in LLMs](https://arxiv.org/abs/2505.17656)

*Hexiang Tan, Fei Sun, Sha Liu, Du Su, Qi Cao, Xin Chen, Jingang Wang, Xunliang Cai, Yuanzhuo Wang, Huawei Shen, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** large language models, error detection, self-consistent errors, cross-model probing, truthfulness

**Relevance Score:** 9

**TL;DR:** This paper addresses the issue of self-consistent errors in large language models, proposing a new detection method that significantly improves error detection performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models often generate plausible but incorrect content, making error detection critical for ensuring truthfulness. The paper focuses on self-consistent errors, which current detection methods struggle to identify.

**Method:** The authors define self-consistent errors and evaluate existing detection methods on them. They propose a cross-model probe method that utilizes hidden state evidence from an external verifier LLM to enhance performance.

**Key Contributions:**

	1. Definition and evaluation of self-consistent errors in LLMs
	2. A new detection method using cross-model probing
	3. Demonstration of the effectiveness across multiple LLM families

**Result:** The proposed method significantly improves the detection of self-consistent errors across three different LLM families compared to existing methods.

**Limitations:** 

**Conclusion:** The findings reveal limitations in current error detection methods and emphasize the need for improved strategies to address self-consistent errors in large language models.

**Abstract:** As large language models (LLMs) often generate plausible but incorrect content, error detection has become increasingly critical to ensure truthfulness. However, existing detection methods often overlook a critical problem we term as self-consistent error, where LLMs repeatly generate the same incorrect response across multiple stochastic samples. This work formally defines self-consistent errors and evaluates mainstream detection methods on them. Our investigation reveals two key findings: (1) Unlike inconsistent errors, whose frequency diminishes significantly as LLM scale increases, the frequency of self-consistent errors remains stable or even increases. (2) All four types of detection methshods significantly struggle to detect self-consistent errors. These findings reveal critical limitations in current detection methods and underscore the need for improved methods. Motivated by the observation that self-consistent errors often differ across LLMs, we propose a simple but effective cross-model probe method that fuses hidden state evidence from an external verifier LLM. Our method significantly enhances performance on self-consistent errors across three LLM families.

</details>


### [150] [Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal Evolution of Human States](https://arxiv.org/abs/2505.17663)

*Yang Xiao, Jiashuo Wang, Qiancheng Xu, Changhe Song, Chunpu Xu, Yi Cheng, Wenjie Li, Pengfei Liu*

**Main category:** cs.CL

**Keywords:** Theory of Mind, Large Language Models, Human-AI Interaction, Benchmark, Mental States

**Relevance Score:** 9

**TL;DR:** This paper introduces DynToM, a new benchmark for evaluating the Theory of Mind capabilities of Large Language Models (LLMs) over time, showing significant performance gaps compared to humans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess LLMs ability to track the dynamic progression of mental states in social interactions, which is crucial for human-AI interaction.

**Method:** The authors created a benchmark, DynToM, through a systematic four-step framework, generating 1,100 social contexts and validating 5,500 scenarios and 78,100 questions for quality and realism.

**Key Contributions:**

	1. Introduction of the DynToM benchmark for evaluating ToM in LLMs
	2. Generation of a large dataset of social contexts and scenarios for benchmark testing
	3. Comprehensive evaluation revealing performance gaps between LLMs and human capabilities

**Result:** Evaluation of ten state-of-the-art LLMs shows they fall short by 44.7% compared to human performance, especially in tracking shifts in mental states.

**Limitations:** Focus on the static aspects of mental state tracking; further work needed to enhance LLM capabilities in dynamic contexts.

**Conclusion:** The significant performance gap underscores the limitations of current LLMs in modeling the dynamic nature of human mental states, pointing to crucial areas for improvement.

**Abstract:** As Large Language Models (LLMs) increasingly participate in human-AI interactions, evaluating their Theory of Mind (ToM) capabilities - particularly their ability to track dynamic mental states - becomes crucial. While existing benchmarks assess basic ToM abilities, they predominantly focus on static snapshots of mental states, overlooking the temporal evolution that characterizes real-world social interactions. We present \textsc{DynToM}, a novel benchmark specifically designed to evaluate LLMs' ability to understand and track the temporal progression of mental states across interconnected scenarios. Through a systematic four-step framework, we generate 1,100 social contexts encompassing 5,500 scenarios and 78,100 questions, each validated for realism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs reveals that their average performance underperforms humans by 44.7\%, with performance degrading significantly when tracking and reasoning about the shift of mental states. This performance gap highlights fundamental limitations in current LLMs' ability to model the dynamic nature of human mental states.

</details>


### [151] [QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2505.17667)

*Fanqi Wan, Weizhou Shen, Shengyi Liao, Yingcheng Shi, Chenliang Li, Ziyi Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan*

**Main category:** cs.CL

**Keywords:** long-context reasoning, reinforcement learning, large reasoning models

**Relevance Score:** 8

**TL;DR:** This paper introduces QwenLong-L1, a framework for extending large reasoning models (LRMs) to effectively handle long-context inputs, addressing challenges in training efficiency and optimization stability.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of reasoning with long-context inputs in large reasoning models, which have traditionally excelled in short-context scenarios.

**Method:** The paper proposes a framework called QwenLong-L1, which includes a warm-up supervised fine-tuning stage to establish a solid starting policy, followed by a curriculum-guided phased reinforcement learning technique and a difficulty-aware retrospective sampling strategy.

**Key Contributions:**

	1. Introduction of the long-context reasoning RL paradigm
	2. Development of the QwenLong-L1 framework
	3. Achievement of state-of-the-art performance on long-context reasoning tasks

**Result:** QwenLong-L1-32B demonstrates superior performance on seven long-context document question-answering benchmarks, outperforming major LRMs like OpenAI-o3-mini and achieving results comparable to Claude-3.7-Sonnet-Thinking.

**Limitations:** 

**Conclusion:** The proposed approach significantly advances the capability of long-context LRMs in performing robust reasoning in information-rich environments.

**Abstract:** Recent large reasoning models (LRMs) have demonstrated strong reasoning capabilities through reinforcement learning (RL). These improvements have primarily been observed within the short-context reasoning tasks. In contrast, extending LRMs to effectively process and reason on long-context inputs via RL remains a critical unsolved challenge. To bridge this gap, we first formalize the paradigm of long-context reasoning RL, and identify key challenges in suboptimal training efficiency and unstable optimization process. To address these issues, we propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. Specifically, we utilize a warm-up supervised fine-tuning (SFT) stage to establish a robust initial policy, followed by a curriculum-guided phased RL technique to stabilize the policy evolution, and enhanced with a difficulty-aware retrospective sampling strategy to incentivize the policy exploration. Experiments on seven long-context document question-answering benchmarks demonstrate that QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking, demonstrating leading performance among state-of-the-art LRMs. This work advances the development of practical long-context LRMs capable of robust reasoning across information-intensive environments.

</details>


### [152] [MIDB: Multilingual Instruction Data Booster for Enhancing Multilingual Instruction Synthesis](https://arxiv.org/abs/2505.17671)

*Yilun Liu, Chunguang Zhao, Xinhua Yang, Hongyong Zeng, Shimin Tao, Weibin Meng, Minggui He, Chang Su, Yan Yu, Hongxia Ma, Li Zhang, Daimeng Wei, Hao Yang*

**Main category:** cs.CL

**Keywords:** instruction synthesis, multilingual instruction data, language models, data quality, machine translation

**Relevance Score:** 8

**TL;DR:** The paper presents MIDB, a system designed to enhance the quality of multilingual synthesized instruction data used for instruction tuning of LLMs by correcting content and machine translation errors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of multilingual synthesized instruction pairs, which suffer from defects in content and insufficient localization when using machine translation from English.

**Method:** MIDB is trained on 36.8k revision examples across 16 languages, developed by human linguistic experts to boost the quality of instruction data by addressing errors and localization issues.

**Key Contributions:**

	1. Introduction of MIDB for enhancing multilingual instruction data quality
	2. Demonstrated effectiveness across 16 languages
	3. Significant improvements in multilingual LLMs' instruction-following and cultural understanding abilities.

**Result:** MIDB improved instruction data quality across 16 languages and significantly enhanced the instruction-following and cultural-understanding abilities of multilingual LLMs fine-tuned on the boosted data.

**Limitations:** The results may vary based on the quality of the initial synthesized data and the effectiveness of machine translations used.

**Conclusion:** The proposed MIDB method successfully mitigates quality issues in multilingual synthetic instruction data, leading to better performance of LLMs in multilingual contexts.

**Abstract:** Despite doubts on data quality, instruction synthesis has been widely applied into instruction tuning (IT) of LLMs as an economic and rapid alternative. Recent endeavors focus on improving data quality for synthesized instruction pairs in English and have facilitated IT of English-centric LLMs. However, data quality issues in multilingual synthesized instruction pairs are even more severe, since the common synthesizing practice is to translate English synthesized data into other languages using machine translation (MT). Besides the known content errors in these English synthesized data, multilingual synthesized instruction data are further exposed to defects introduced by MT and face insufficient localization of the target languages. In this paper, we propose MIDB, a Multilingual Instruction Data Booster to automatically address the quality issues in multilingual synthesized data. MIDB is trained on around 36.8k revision examples across 16 languages by human linguistic experts, thereby can boost the low-quality data by addressing content errors and MT defects, and improving localization in these synthesized data. Both automatic and human evaluation indicate that not only MIDB steadily improved instruction data quality in 16 languages, but also the instruction-following and cultural-understanding abilities of multilingual LLMs fine-tuned on MIDB-boosted data were significantly enhanced.

</details>


### [153] [Tuning Language Models for Robust Prediction of Diverse User Behaviors](https://arxiv.org/abs/2505.17682)

*Fanjin Meng, Jingtao Ding, Jiahui Gong, Chen Yang, Hong Chen, Zuojian Wang, Haisheng Lu, Yong Li*

**Main category:** cs.CL

**Keywords:** user behavior prediction, large language models, fine-tuning, long-tailed behaviors, intelligent assistants

**Relevance Score:** 8

**TL;DR:** BehaviorLM is a progressive fine-tuning approach for LLMs that improves predictions of long-tailed user behaviors while maintaining anchor performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user behavior prediction in intelligent assistants by addressing the limitations of deep learning models in capturing long-tailed behaviors.

**Method:** BehaviorLM consists of two fine-tuning stages: first, tuning on frequent behaviors while preserving general knowledge, and second, using a balanced subset of behaviors based on sample difficulty to improve predictions for less common behaviors.

**Key Contributions:**

	1. Introduction of a two-stage fine-tuning approach for LLMs
	2. Improved prediction of long-tailed behaviors without compromising anchor behavior performance
	3. Demonstrated effectiveness on real-world datasets

**Result:** Experimental results show that BehaviorLM successfully predicts both frequent and rare user behaviors, effectively leveraging LLMs' behavioral knowledge in few-shot contexts.

**Limitations:** 

**Conclusion:** BehaviorLM provides a robust framework for improving the prediction of diverse user behaviors in intelligent assistant services.

**Abstract:** Predicting user behavior is essential for intelligent assistant services, yet deep learning models often struggle to capture long-tailed behaviors. Large language models (LLMs), with their pretraining on vast corpora containing rich behavioral knowledge, offer promise. However, existing fine-tuning approaches tend to overfit to frequent ``anchor'' behaviors, reducing their ability to predict less common ``tail'' behaviors. In this paper, we introduce BehaviorLM, a progressive fine-tuning approach that addresses this issue. In the first stage, LLMs are fine-tuned on anchor behaviors while preserving general behavioral knowledge. In the second stage, fine-tuning uses a balanced subset of all behaviors based on sample difficulty to improve tail behavior predictions without sacrificing anchor performance. Experimental results on two real-world datasets demonstrate that BehaviorLM robustly predicts both anchor and tail behaviors and effectively leverages LLM behavioral knowledge to master tail behavior prediction with few-shot examples.

</details>


### [154] [ELSPR: Evaluator LLM Training Data Self-Purification on Non-Transitive Preferences via Tournament Graph Reconstruction](https://arxiv.org/abs/2505.17691)

*Yan Yu, Yilun Liu, Minggui He, Shimin Tao, Weibin Meng, Xinhua Yang, Li Zhang, Hongxia Ma, Chang Su, Hao Yang, Fuliang Li*

**Main category:** cs.CL

**Keywords:** large language models, pairwise comparisons, non-transitivity, graph theory, machine learning

**Relevance Score:** 8

**TL;DR:** This study addresses non-transitive preferences in LLM evaluations by proposing a graph-theoretic framework to analyze and mitigate this issue, demonstrating improved results through filtered training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To resolve the issue of non-transitivity in pairwise comparisons made by Evaluator LLMs, which can lead to inconsistent evaluation outcomes.

**Method:** We model pairwise preferences as tournament graphs, quantifying non-transitivity and measuring preference clarity using directed graph structural entropy. A filtering strategy, ELSPR, is introduced to retain only transitive preference data for model fine-tuning.

**Key Contributions:**

	1. Introduction of a graph-theoretic framework for analyzing preferences in LLM evaluations
	2. Quantification of non-transitivity and clarity using directed graph structural entropy
	3. Development of a filtering strategy (ELSPR) that enhances model performance by improving preference consistency

**Result:** Fine-tuning models with filtered data led to a reduction of non-transitivity by 13.78%, decreased structural entropy, and improved alignment with human evaluators.

**Limitations:** 

**Conclusion:** The proposed framework and filtering strategy effectively address non-transitivity in LLM evaluations, enhancing the reliability of evaluative outcomes.

**Abstract:** Large language models (LLMs) are widely used as evaluators for open-ended tasks, while previous research has emphasized biases in LLM evaluations, the issue of non-transitivity in pairwise comparisons remains unresolved: non-transitive preferences for pairwise comparisons, where evaluators prefer A over B, B over C, but C over A. Our results suggest that low-quality training data may reduce the transitivity of preferences generated by the Evaluator LLM. To address this, We propose a graph-theoretic framework to analyze and mitigate this problem by modeling pairwise preferences as tournament graphs. We quantify non-transitivity and introduce directed graph structural entropy to measure the overall clarity of preferences. Our analysis reveals significant non-transitivity in advanced Evaluator LLMs (with Qwen2.5-Max exhibiting 67.96%), as well as high entropy values (0.8095 for Qwen2.5-Max), reflecting low overall clarity of preferences. To address this issue, we designed a filtering strategy, ELSPR, to eliminate preference data that induces non-transitivity, retaining only consistent and transitive preference data for model fine-tuning. Experiments demonstrate that models fine-tuned with filtered data reduce non-transitivity by 13.78% (from 64.28% to 50.50%), decrease structural entropy by 0.0879 (from 0.8113 to 0.7234), and align more closely with human evaluators (human agreement rate improves by 0.6% and Spearman correlation increases by 0.01).

</details>


### [155] [Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models](https://arxiv.org/abs/2505.17697)

*Zekai Zhao, Qi Liu, Kun Zhou, Zihan Liu, Yifei Shao, Zhiting Hu, Biwei Huang*

**Main category:** cs.CL

**Keywords:** large language models, chain-of-thought reasoning, activation control, fine-tuning, machine learning

**Relevance Score:** 8

**TL;DR:** A method to amplify key activations in LLMs to enhance long-form reasoning without extensive training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and control the internal mechanisms that govern long-chain-of-thought (CoT) reasoning in large language models, allowing improved performance with minimal training.

**Method:** The paper introduces a training-free activation control technique that amplifies high-impact activations and utilizes 'wait' tokens to enhance reasoning capabilities. It also proposes a parameter-efficient fine-tuning approach that modifies only the last-layer activations and a few additional parameters.

**Key Contributions:**

	1. Training-free activation control technique for LLMs.
	2. Parameter-efficient fine-tuning method using a last-layer activation module.
	3. Demonstrated increased accuracy and self-reflection rates in long-form reasoning tasks.

**Result:** The proposed method significantly increases self-reflection rates and accuracy in reasoning tasks while requiring fewer parameters compared to traditional training methods.

**Limitations:** 

**Conclusion:** By identifying and modulating key activations at inference time, it is possible to elicit long CoT reasoning in LLMs efficiently, demonstrating strong performance improvements on reasoning benchmarks with reduced training.

**Abstract:** Despite the remarkable reasoning performance, eliciting the long chain-of-thought (CoT) ability in large language models (LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers largely governs long-form reasoning attributes, such as output length and self-reflection. By simply amplifying these activations and inserting "wait" tokens, we can invoke the long CoT ability without any training, resulting in significantly increased self-reflection rates and accuracy. Moreover, we find that the activation dynamics follow predictable trajectories, with a sharp rise after special tokens and a subsequent exponential decay. Building on these insights, we introduce a general training-free activation control technique. It leverages a few contrastive examples to identify key activations, and employs simple analytic functions to modulate their values at inference time to elicit long CoTs. Extensive experiments confirm the effectiveness of our method in efficiently eliciting long CoT reasoning in LLMs and improving their performance. Additionally, we propose a parameter-efficient fine-tuning method that trains only a last-layer activation amplification module and a few LoRA layers, outperforming full LoRA fine-tuning on reasoning benchmarks with significantly fewer parameters. Our code and data are publicly released.

</details>


### [156] [SemSketches-2021: experimenting with the machine processing of the pilot semantic sketches corpus](https://arxiv.org/abs/2505.17704)

*Maria Ponomareva, Maria Petrova, Julia Detkova, Oleg Serikov, Maria Yarova*

**Main category:** cs.CL

**Keywords:** semantic sketches, machine processing, corpus, shared task, context

**Relevance Score:** 4

**TL;DR:** This paper discusses various methods for machine processing of semantic sketches and presents a corpus created for this purpose, highlighting an associated shared task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and advance the capabilities of machine processing of semantic sketches and to create a suitable corpus for research.

**Method:** The paper outlines the organization of the SemSketches-2021 Shared Task, where participants matched anonymous sketches with contextual predicates.

**Key Contributions:**

	1. Introduction of a pilot open corpus of semantic sketches
	2. Description of the SemSketches-2021 Shared Task
	3. Development of machine processing tools for semantic sketches

**Result:** The pilot open corpus of semantic sketches was established, and the shared task facilitated progress in understanding the relationship between sketches and contexts.

**Limitations:** 

**Conclusion:** The paper underscores the potential applications of semantic sketches in various tasks and highlights the importance of developing tools for their machine processing.

**Abstract:** The paper deals with elaborating different approaches to the machine processing of semantic sketches. It presents the pilot open corpus of semantic sketches. Different aspects of creating the sketches are discussed, as well as the tasks that the sketches can help to solve. Special attention is paid to the creation of the machine processing tools for the corpus. For this purpose, the SemSketches-2021 Shared Task was organized. The participants were given the anonymous sketches and a set of contexts containing the necessary predicates. During the Task, one had to assign the proper contexts to the corresponding sketches.

</details>


### [157] [Understanding How Value Neurons Shape the Generation of Specified Values in LLMs](https://arxiv.org/abs/2505.17712)

*Yi Su, Jiayi Zhang, Shu Yang, Xinhai Wang, Lijie Hu, Di Wang*

**Main category:** cs.CL

**Keywords:** large language models, value alignment, mechanistic interpretability, neuron analysis, Schwartz Values Survey

**Relevance Score:** 8

**TL;DR:** ValueLocate is a framework for interpreting values encoded in large language models, using the Schwartz Values Survey to identify value-critical neurons.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the alignment of large language models (LLMs) with universal ethical principles by understanding how values are represented in their architecture.

**Method:** ValueLocate constructs a dataset called ValueInsight that operationalizes four dimensions of universal value, facilitating neuron identification through activation differences related to these values.

**Key Contributions:**

	1. Introduction of ValueLocate framework for value interpretation in LLMs
	2. Development of ValueInsight dataset for operationalizing universal values
	3. Causal demonstration of neuron manipulation affecting model value representations

**Result:** The method reveals specific neurons that represent different value orientations and shows that manipulating these neurons can change the model's value outputs, establishing causal links.

**Limitations:** 

**Conclusion:** ValueLocate provides a foundation for improving value alignment in LLMs by connecting psychological principles with mechanistic neuron analysis.

**Abstract:** Rapid integration of large language models (LLMs) into societal applications has intensified concerns about their alignment with universal ethical principles, as their internal value representations remain opaque despite behavioral alignment advancements. Current approaches struggle to systematically interpret how values are encoded in neural architectures, limited by datasets that prioritize superficial judgments over mechanistic analysis. We introduce ValueLocate, a mechanistic interpretability framework grounded in the Schwartz Values Survey, to address this gap. Our method first constructs ValueInsight, a dataset that operationalizes four dimensions of universal value through behavioral contexts in the real world. Leveraging this dataset, we develop a neuron identification method that calculates activation differences between opposing value aspects, enabling precise localization of value-critical neurons without relying on computationally intensive attribution methods. Our proposed validation method demonstrates that targeted manipulation of these neurons effectively alters model value orientations, establishing causal relationships between neurons and value representations. This work advances the foundation for value alignment by bridging psychological value frameworks with neuron analysis in LLMs.

</details>


### [158] [The Pilot Corpus of the English Semantic Sketches](https://arxiv.org/abs/2505.17733)

*Maria Petrova, Maria Ponomareva, Alexandra Ivoylova*

**Main category:** cs.CL

**Keywords:** semantic sketches, English verbs, contrastive linguistics, cross-language differences

**Relevance Score:** 3

**TL;DR:** The paper discusses the creation of semantic sketches for English verbs, highlighting cross-language differences and the sketch-building process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore contrastive studies using semantic sketches in linguistics, specifically focusing on English-Russian verb pairs.

**Method:** The study involves building a pilot corpus of English-Russian sketch pairs and analyzing the construction process of these sketches as well as related mistakes.

**Key Contributions:**

	1. Creation of a semantic sketches corpus for English-Russian verbs
	2. Insights into cross-language semantic differences
	3. Analysis of sketch construction mistakes

**Result:** The paper reveals insights into the linguistic nature of sketches and emphasizes differences between similar semantic sketches across languages.

**Limitations:** 

**Conclusion:** Semantic sketches can facilitate contrastive linguistic studies and help identify mistakes in understanding verb semantics across languages.

**Abstract:** The paper is devoted to the creation of the semantic sketches for English verbs. The pilot corpus consists of the English-Russian sketch pairs and is aimed to show what kind of contrastive studies the sketches help to conduct. Special attention is paid to the cross-language differences between the sketches with similar semantics. Moreover, we discuss the process of building a semantic sketch, and analyse the mistakes that could give insight to the linguistic nature of sketches.

</details>


### [159] [Fast Quiet-STaR: Thinking Without Thought Tokens](https://arxiv.org/abs/2505.17746)

*Wei Huang, Yizhe Xiong, Xin Ye, Zhijie Deng, Hui Chen, Zijia Lin, Guiguang Ding*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Frameworks, Curriculum Learning, Reinforcement Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** Fast Quiet STaR is an efficient reasoning framework for large language models (LLMs) that reduces computational costs while improving reasoning accuracy by using curriculum learning and reinforcement learning techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing reasoning frameworks that either incur high inference overhead or fail to optimize reasoning processes effectively, leading to better model performance in complex tasks.

**Method:** Introduced a curriculum learning strategy to train models to reduce thought tokens gradually, implemented Fast Quiet-STaR NTP which eliminates explicit thought token generation during inference, compared performance against traditional Quiet STaR.

**Key Contributions:**

	1. Development of Fast Quiet STaR framework for efficient reasoning
	2. Implementation of curriculum learning to optimize thought token usage
	3. Demonstration of accuracy improvements compared to existing models in standard benchmarks.

**Result:** Fast Quiet-STaR outperformed Quiet-STaR on benchmarks, achieving a 9% and 5.7% accuracy improvement on Mistral 7B and Qwen2.5 7B respectively, while maintaining the same inference latency.

**Limitations:** 

**Conclusion:** Fast Quiet-STaR provides a more efficient way to enhance reasoning capabilities in LLMs, making them applicable for complex reasoning tasks, and demonstrating significant accuracy improvements without additional latency.

**Abstract:** Large Language Models (LLMs) have achieved impressive performance across a range of natural language processing tasks. However, recent advances demonstrate that further gains particularly in complex reasoning tasks require more than merely scaling up model sizes or training data. One promising direction is to enable models to think during the reasoning process. Recently, Quiet STaR significantly improves reasoning by generating token-level thought traces, but incurs substantial inference overhead. In this work, we propose Fast Quiet STaR, a more efficient reasoning framework that preserves the benefits of token-level reasoning while reducing computational cost. Our method introduces a curriculum learning based training strategy that gradually reduces the number of thought tokens, enabling the model to internalize more abstract and concise reasoning processes. We further extend this approach to the standard Next Token Prediction (NTP) setting through reinforcement learning-based fine-tuning, resulting in Fast Quiet-STaR NTP, which eliminates the need for explicit thought token generation during inference. Experiments on four benchmark datasets with Mistral 7B and Qwen2.5 7B demonstrate that Fast Quiet-STaR consistently outperforms Quiet-STaR in terms of average accuracy under the same inference time budget. Notably, Fast Quiet-STaR NTP achieves an average accuracy improvement of 9\% on Mistral 7B and 5.7\% on Qwen2.5 7B, while maintaining the same inference latency. Our code will be available at https://github.com/huangwei200012/Fast-Quiet-STaR.

</details>


### [160] [Discriminating Form and Meaning in Multilingual Models with Minimal-Pair ABX Tasks](https://arxiv.org/abs/2505.17747)

*Maureen de Seyssel, Jie Chi, Skyler Seto, Maartje ter Hoeve, Masha Fedzechkina, Natalie Schluter*

**Main category:** cs.CL

**Keywords:** multilingual language models, ABX tasks, language identity, semantic content, representation analysis

**Relevance Score:** 7

**TL;DR:** Introduction of training-free ABX-style tasks to evaluate multilingual language models on language identity and semantic content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To offer a flexible and interpretable alternative to probing for analyzing language models' representations of identity and meaning.

**Method:** Evaluation of multilingual language models via zero-shot ABX-style discrimination tasks, inspired by speech processing, measuring representation differences across various checkpoints and layers.

**Key Contributions:**

	1. Training-free ABX-style tasks for multilingual evaluation
	2. Insights on language and meaning discrimination across model layers
	3. Framework for analyzing multilingual representations

**Result:** Language discrimination declines with training and is concentrated in lower layers, while meaning discrimination strengthens and stabilizes in deeper layers.

**Limitations:** 

**Conclusion:** ABX tasks provide a lightweight framework for understanding the structure of multilingual representations, showing alignment with linguistic learning performance.

**Abstract:** We introduce a set of training-free ABX-style discrimination tasks to evaluate how multilingual language models represent language identity (form) and semantic content (meaning). Inspired from speech processing, these zero-shot tasks measure whether minimal differences in representation can be reliably detected. This offers a flexible and interpretable alternative to probing. Applied to XLM-R (Conneau et al, 2020) across pretraining checkpoints and layers, we find that language discrimination declines over training and becomes concentrated in lower layers, while meaning discrimination strengthens over time and stabilizes in deeper layers. We then explore probing tasks, showing some alignment between our metrics and linguistic learning performance. Our results position ABX tasks as a lightweight framework for analyzing the structure of multilingual representations.

</details>


### [161] [Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs](https://arxiv.org/abs/2505.17762)

*Ziyu Ge, Yuhao Wu, Daniel Wai Kit Chin, Roy Ka-Wei Lee, Rui Cao*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Fact-Checking, Conflicting Evidence, Dataset, Media Credibility

**Relevance Score:** 9

**TL;DR:** Evaluation of RAG models for fact-checking under conflicting evidence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of Retrieval-Augmented Generation (RAG) models in fact-checking when faced with conflicting information from different credible sources.

**Method:** Systematic evaluation using a novel dataset called CONFACT, which contains questions paired with conflicting evidence from various sources. Experiments focused on integrating media source credibility into RAG methods.

**Key Contributions:**

	1. Introduction of the CONFACT dataset for evaluating fact-checking with conflicting evidence.
	2. Systematic evaluation of RAG models in handling conflicts from various media sources.
	3. Proposed strategies for integrating source credibility to enhance RAG performance.

**Result:** Findings indicate vulnerabilities in state-of-the-art RAG methods in resolving conflicts due to media source credibility differences. Strategies to integrate source credibility were tested, showing enhanced performance.

**Limitations:** Limited to evaluation of RAG models, may not generalize to all types of AI models.

**Conclusion:** Incorporating media background information significantly improves the fact-checking capabilities of RAG models under contradictory evidence.

**Abstract:** Large Language Models (LLMs) augmented with retrieval mechanisms have demonstrated significant potential in fact-checking tasks by integrating external knowledge. However, their reliability decreases when confronted with conflicting evidence from sources of varying credibility. This paper presents the first systematic evaluation of Retrieval-Augmented Generation (RAG) models for fact-checking in the presence of conflicting evidence. To support this study, we introduce \textbf{CONFACT} (\textbf{Con}flicting Evidence for \textbf{Fact}-Checking) (Dataset available at https://github.com/zoeyyes/CONFACT), a novel dataset comprising questions paired with conflicting information from various sources. Extensive experiments reveal critical vulnerabilities in state-of-the-art RAG methods, particularly in resolving conflicts stemming from differences in media source credibility. To address these challenges, we investigate strategies to integrate media background information into both the retrieval and generation stages. Our results show that effectively incorporating source credibility significantly enhances the ability of RAG models to resolve conflicting evidence and improve fact-checking performance.

</details>


### [162] [The Real Barrier to LLM Agent Usability is Agentic ROI](https://arxiv.org/abs/2505.17767)

*Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, Weinan Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human-AI Interaction, Agentic ROI

**Relevance Score:** 8

**TL;DR:** This paper advocates for a utility-driven approach to optimize Large Language Model (LLM) agents beyond their performance, focusing on the concept of Agent ROI to bridge usability gaps in mass-market applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The limited real-world adoption of LLM agents is attributed to usability gaps arising from the tradeoff between the agents value and the costs of its use.

**Method:** The paper proposes a zigzag development trajectory focusing first on scaling up information quality, followed by scaling down to minimize time and cost, in order to enhance Agentic ROI.

**Key Contributions:**

	1. Introduction of Agentic ROI as a framework for evaluating LLM agents
	2. Proposal of a zigzag development trajectory for optimizing agent usability
	3. Identification of critical factors that influence the utility of LLM agents in mass-market applications

**Result:** The identification of key factors determining Agentic ROI, including information quality, agent time, and cost, offers a new perspective for evaluating and developing LLM agents.

**Limitations:** 

**Conclusion:** By addressing usability gaps through an Agent ROI lens, LLM agents can become more scalable, accessible, and effective in real-world applications.

**Abstract:** Large Language Model (LLM) agents represent a promising shift in human-AI interaction, moving beyond passive prompt-response systems to autonomous agents capable of reasoning, planning, and goal-directed action. Despite the widespread application in specialized, high-effort tasks like coding and scientific research, we highlight a critical usability gap in high-demand, mass-market applications. This position paper argues that the limited real-world adoption of LLM agents stems not only from gaps in model capabilities, but also from a fundamental tradeoff between the value an agent can provide and the costs incurred during real-world use. Hence, we call for a shift from solely optimizing model performance to a broader, utility-driven perspective: evaluating agents through the lens of the overall agentic return on investment (Agent ROI). By identifying key factors that determine Agentic ROI--information quality, agent time, and cost--we posit a zigzag development trajectory in optimizing agentic ROI: first scaling up to improve the information quality, then scaling down to minimize the time and cost. We outline the roadmap across different development stages to bridge the current usability gaps, aiming to make LLM agents truly scalable, accessible, and effective in real-world contexts.

</details>


### [163] [EXECUTE: A Multilingual Benchmark for LLM Token Understanding](https://arxiv.org/abs/2505.17784)

*Lukas Edman, Helmut Schmid, Alexander Fraser*

**Main category:** cs.CL

**Keywords:** LLM, CUTE benchmark, language processing, EXECUTE, character understanding

**Relevance Score:** 8

**TL;DR:** EXECUTE extends the CUTE benchmark to evaluate LLMs' character and word-level understanding across multiple languages and scripts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in character understanding indicated by the CUTE benchmark, and to explore performance across diverse languages and scripts.

**Method:** We developed a simplified framework, EXECUTE, that can be easily expanded to any language and conducted tests across multiple LLMs to assess their understanding of different levels of writing.

**Key Contributions:**

	1. Introduction of the EXECUTE framework for evaluating LLMs in various languages
	2. Demonstration of varied performance issues across languages, not limited to character understanding
	3. Assessment of sub-character tasks in East Asian languages to evaluate character component understanding.

**Result:** The tests revealed that issues with LLM understanding vary between languages, with some languages having word-level challenges and others performing well without issues.

**Limitations:** The study is limited to the languages and models tested; broader conclusions require more extensive language coverage.

**Conclusion:** The findings highlight the need for further investigation into LLMs' understanding beyond character-level processing, particularly in diverse linguistic contexts.

**Abstract:** The CUTE benchmark showed that LLMs struggle with character understanding in English. We extend it to more languages with diverse scripts and writing systems, introducing EXECUTE. Our simplified framework allows easy expansion to any language. Tests across multiple LLMs reveal that challenges in other languages are not always on the character level as in English. Some languages show word-level processing issues, some show no issues at all. We also examine sub-character tasks in Chinese, Japanese, and Korean to assess LLMs' understanding of character components.

</details>


### [164] [Compression Hacking: A Supplementary Perspective on Informatics Metric of Language Models from Geometric Distortion](https://arxiv.org/abs/2505.17793)

*Jianxiang Zang, Meiling Ning, Yongda Wei, Shihan Dou, Jiazheng Zhang, Nijia Mo, Binhong Li, Tao Gui, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** compression as intelligence, geometric distortion, language models

**Relevance Score:** 6

**TL;DR:** The paper introduces refined compression metrics for language models that improve performance evaluation by addressing geometric distortions in compressed representations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current compression metrics in evaluating the performance of language models.

**Method:** The authors propose three refined compression metrics that integrate geometric distortion analysis into a self-evaluation pipeline.

**Key Contributions:**

	1. Introduction of three refined compression metrics for language models.
	2. Integration of geometric distortion analysis into LM evaluation.
	3. Demonstrated strong correlation between the new metrics and LM capabilities.

**Result:** The new metrics correlate strongly with language models' capabilities, achieving Spearman correlation coefficients above 0.9, outperforming existing metrics.

**Limitations:** 

**Conclusion:** Refined compression metrics enhance the understanding of language models by accounting for geometric distortions in their representations.

**Abstract:** Recently, the concept of ``compression as intelligence'' has provided a novel informatics metric perspective for language models (LMs), emphasizing that highly structured representations signify the intelligence level of LMs. However, from a geometric standpoint, the word representation space of highly compressed LMs tends to degenerate into a highly anisotropic state, which hinders the LM's ability to comprehend instructions and directly impacts its performance. We found this compression-anisotropy synchronicity is essentially the ``Compression Hacking'' in LM representations, where noise-dominated directions tend to create the illusion of high compression rates by sacrificing spatial uniformity. Based on this, we propose three refined compression metrics by incorporating geometric distortion analysis and integrate them into a self-evaluation pipeline. The refined metrics exhibit strong alignment with the LM's comprehensive capabilities, achieving Spearman correlation coefficients above 0.9, significantly outperforming both the original compression and other internal structure-based metrics. This confirms that compression hacking substantially enhances the informatics interpretation of LMs by incorporating geometric distortion of representations.

</details>


### [165] [DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors](https://arxiv.org/abs/2505.17795)

*Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** large-language-model, dialogue planning, emotion tracking, Q-network, temporal-difference learning

**Relevance Score:** 9

**TL;DR:** DialogXpert enhances LLM agents for proactive and goal-driven interactions by combining a frozen LLM with a compact Q-network and user emotion tracking, achieving high success rates in dialogue benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve proactive, goal-driven interactions in LLM agents that currently excel only in reactive dialogue.

**Method:** DialogXpert uses a frozen LLM to generate a set of candidate actions per turn and employs a Q-network trained via temporal-difference learning over fixed BERT embeddings to select the best action.

**Key Contributions:**

	1. Introduces a novel approach for dialogue planning using a Q-network with LLMs.
	2. Tracks user emotions to tailor decision-making in conversations.
	3. Demonstrates high success rates and reduced conversation turns in application benchmarks.

**Result:** Achieves over 94% success rates in dialogue tasks, improving to above 97% with a larger LLM, significantly enhancing negotiation outcomes.

**Limitations:** 

**Conclusion:** DialogXpert provides an effective framework for real-time, strategic, and emotionally intelligent dialogue planning across various tasks.

**Abstract:** Large-language-model (LLM) agents excel at reactive dialogue but struggle with proactive, goal-driven interactions due to myopic decoding and costly planning. We introduce DialogXpert, which leverages a frozen LLM to propose a small, high-quality set of candidate actions per turn and employs a compact Q-network over fixed BERT embeddings trained via temporal-difference learning to select optimal moves within this reduced space. By tracking the user's emotions, DialogXpert tailors each decision to advance the task while nurturing a genuine, empathetic connection. Across negotiation, emotional support, and tutoring benchmarks, DialogXpert drives conversations to under $3$ turns with success rates exceeding 94\% and, with a larger LLM prior, pushes success above 97\% while markedly improving negotiation outcomes. This framework delivers real-time, strategic, and emotionally intelligent dialogue planning at scale. Code available at https://github.com/declare-lab/dialogxpert/

</details>


### [166] [Don't Overthink it. Preferring Shorter Thinking Chains for Improved LLM Reasoning](https://arxiv.org/abs/2505.17813)

*Michael Hassid, Gabriel Synnaeve, Yossi Adi, Roy Schwartz*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Inference Method, Computational Efficiency, Accuracy

**Relevance Score:** 8

**TL;DR:** The paper challenges the assumption that longer reasoning chains in LLMs lead to better performance, showing that shorter chains can yield significantly more accurate results with reduced computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of reasoning chain length on the performance of large language models in complex reasoning tasks, and to propose more efficient inference methods.

**Method:** The authors propose short-m@k, an LLM inference method that runs k independent generations in parallel and selects the final answer through majority voting of the first m completions. The effectiveness of this method is validated through experiments comparing shorter and longer reasoning chains.

**Key Contributions:**

	1. Introduction of short-m@k inference method
	2. Demonstration of improved accuracy with shorter reasoning chains
	3. Reevaluation of test-time compute assumptions in reasoning LLMs

**Result:** Shorter reasoning chains provide up to 34.5% higher accuracy than longer ones and lead to improved performance while using up to 40% fewer thinking tokens and significantly reducing inference time.

**Limitations:** 

**Conclusion:** The study concludes that longer reasoning chains may not always correlate with better LLM performance, advocating for reconsideration of current LLM inference strategies to prioritize efficiency and accuracy.

**Abstract:** Reasoning large language models (LLMs) heavily rely on scaling test-time compute to perform complex reasoning tasks by generating extensive "thinking" chains. While demonstrating impressive results, this approach incurs significant computational costs and inference time. In this work, we challenge the assumption that long thinking chains results in better reasoning capabilities. We first demonstrate that shorter reasoning chains within individual questions are significantly more likely to yield correct answers - up to 34.5% more accurate than the longest chain sampled for the same question. Based on these results, we suggest short-m@k, a novel reasoning LLM inference method. Our method executes k independent generations in parallel and halts computation once the first m thinking processes are done. The final answer is chosen using majority voting among these m chains. Basic short-1@k demonstrates similar or even superior performance over standard majority voting in low-compute settings - using up to 40% fewer thinking tokens. short-3@k, while slightly less efficient than short-1@k, consistently surpasses majority voting across all compute budgets, while still being substantially faster (up to 33% wall time reduction). Inspired by our results, we finetune an LLM using short, long, and randomly selected reasoning chains. We then observe that training on the shorter ones leads to better performance. Our findings suggest rethinking current methods of test-time compute in reasoning LLMs, emphasizing that longer "thinking" does not necessarily translate to improved performance and can, counter-intuitively, lead to degraded results.

</details>


### [167] [Low-Resource NMT: A Case Study on the Written and Spoken Languages in Hong Kong](https://arxiv.org/abs/2505.17816)

*Hei Yi Mak, Tan Lee*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, Cantonese, Chinese, Transformer Model, Data Scarcity

**Relevance Score:** 3

**TL;DR:** This paper develops a transformer-based NMT system for translating written Chinese to written Cantonese, addressing data scarcity through innovative training data collection methods from Wikipedia.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing demand for automatic translation between Chinese and Cantonese due to increased interaction between speakers, necessitating improved translation systems.

**Method:** The study uses a transformer-based neural machine translation system, collecting 28K parallel sentences and devising an approach to create an additional 72K parallel sentences from similar sentence pairs on Chinese and Cantonese Wikipedia.

**Key Contributions:**

	1. Development of a transformer-based NMT system for Chinese-Cantonese translation
	2. Innovative data collection strategy using Wikipedia to generate training data
	3. Demonstrated superior performance against existing translation tools (Baidu Fanyi)

**Result:** The proposed system outperforms Baidu Fanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets based on BLEU scores, demonstrating improved translation quality using mined Wikipedia sentences.

**Limitations:** 

**Conclusion:** The system captures the linguistic transformations between standard Chinese and spoken Cantonese effectively, indicating the potential for better translation systems in this domain.

**Abstract:** The majority of inhabitants in Hong Kong are able to read and write in standard Chinese but use Cantonese as the primary spoken language in daily life. Spoken Cantonese can be transcribed into Chinese characters, which constitute the so-called written Cantonese. Written Cantonese exhibits significant lexical and grammatical differences from standard written Chinese. The rise of written Cantonese is increasingly evident in the cyber world. The growing interaction between Mandarin speakers and Cantonese speakers is leading to a clear demand for automatic translation between Chinese and Cantonese. This paper describes a transformer-based neural machine translation (NMT) system for written-Chinese-to-written-Cantonese translation. Given that parallel text data of Chinese and Cantonese are extremely scarce, a major focus of this study is on the effort of preparing good amount of training data for NMT. In addition to collecting 28K parallel sentences from previous linguistic studies and scattered internet resources, we devise an effective approach to obtaining 72K parallel sentences by automatically extracting pairs of semantically similar sentences from parallel articles on Chinese Wikipedia and Cantonese Wikipedia. We show that leveraging highly similar sentence pairs mined from Wikipedia improves translation performance in all test sets. Our system outperforms Baidu Fanyi's Chinese-to-Cantonese translation on 6 out of 8 test sets in BLEU scores. Translation examples reveal that our system is able to capture important linguistic transformations between standard Chinese and spoken Cantonese.

</details>


### [168] [Not All Tokens Are What You Need In Thinking](https://arxiv.org/abs/2505.17827)

*Hang Yuan, Bin Yu, Haotian Li, Shijun Yang, Christina Dan Wang, Zhou Yu, Xueyin Xu, Weizhen Qi, Kai Chen*

**Main category:** cs.CL

**Keywords:** token compression, reasoning models, Chains of Thought, machine learning, model efficiency

**Relevance Score:** 7

**TL;DR:** This paper introduces Conditional Token Selection (CTS), a framework that compresses tokens in chains of thought (CoT) to reduce redundancy and improve reasoning performance in models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing reasoning models exhibit inefficiencies such as high latency, excessive resource consumption, and verbosity in their generated outputs.

**Method:** Conditional Token Selection (CTS) applies a token-level compression framework that evaluates and retains only the essential tokens in CoT, using conditional importance scoring to assess each token's contribution to correct answers.

**Key Contributions:**

	1. Proposed Conditional Token Selection (CTS) framework for token compression in CoT.
	2. Demonstrated significant improvements in accuracy while reducing the number of reasoning tokens.
	3. Showed effectiveness in reducing training tokens with minimal impact on performance.

**Result:** CTS compresses long CoT, achieving notable improvements, including a 9.1% accuracy increase on the GPQA benchmark with a reduction of 13.2% in reasoning tokens, and a potential 75.8% reduction in reasoning tokens with minimal accuracy loss.

**Limitations:** The methodology may not address all types of redundancy, and the impact on different reasoning tasks remains to be explored further.

**Conclusion:** The study validates that reducing redundancy in CoT can enhance model performance while significantly lowering the computational load.

**Abstract:** Modern reasoning models, such as OpenAI's o1 and DeepSeek-R1, exhibit impressive problem-solving capabilities but suffer from critical inefficiencies: high inference latency, excessive computational resource consumption, and a tendency toward overthinking -- generating verbose chains of thought (CoT) laden with redundant tokens that contribute minimally to the final answer. To address these issues, we propose Conditional Token Selection (CTS), a token-level compression framework with a flexible and variable compression ratio that identifies and preserves only the most essential tokens in CoT. CTS evaluates each token's contribution to deriving correct answers using conditional importance scoring, then trains models on compressed CoT. Extensive experiments demonstrate that CTS effectively compresses long CoT while maintaining strong reasoning performance. Notably, on the GPQA benchmark, Qwen2.5-14B-Instruct trained with CTS achieves a 9.1% accuracy improvement with 13.2% fewer reasoning tokens (13% training token reduction). Further reducing training tokens by 42% incurs only a marginal 5% accuracy drop while yielding a 75.8% reduction in reasoning tokens, highlighting the prevalence of redundancy in existing CoT.

</details>


### [169] [Stepwise Reasoning Checkpoint Analysis: A Test Time Scaling Method to Enhance LLMs' Reasoning](https://arxiv.org/abs/2505.17829)

*Zezhong Wang, Xingshan Zeng, Weiwen Liu, Yufei Wang, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, Test-Time Scaling, Checkpointing, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that enhances mathematical reasoning in Large Language Models by addressing limitations of existing Test-Time Scaling methods and improves accuracy using checkpoint mechanisms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve mathematical reasoning accuracy in Large Language Models (LLMs) by addressing limitations of Test-Time Scaling methods.

**Method:** Stepwise Reasoning Checkpoint Analysis (SRCA) introduces checkpoints between reasoning steps, utilizing Answer-Clustered Search and Checkpoint Candidate Augmentation for better decision-making.

**Key Contributions:**

	1. Introduction of Stepwise Reasoning Checkpoint Analysis (SRCA) framework for LLMs.
	2. Answer-Clustered Search for maintaining diversity in reasoning paths.
	3. Checkpoint Candidate Augmentation for leveraging intermediate answers in final decisions.

**Result:** Experimental results demonstrate that SRCA improves reasoning accuracy over existing TTS methods on various mathematical datasets.

**Limitations:** 

**Conclusion:** SRCA effectively reduces path homogenization and creates a fault-tolerant mechanism by utilizing high-quality intermediate results, leading to enhanced reasoning accuracy.

**Abstract:** Mathematical reasoning through Chain-of-Thought (CoT) has emerged as a powerful capability of Large Language Models (LLMs), which can be further enhanced through Test-Time Scaling (TTS) methods like Beam Search and DVTS. However, these methods, despite improving accuracy by allocating more computational resources during inference, often suffer from path homogenization and inefficient use of intermediate results. To address these limitations, we propose Stepwise Reasoning Checkpoint Analysis (SRCA), a framework that introduces checkpoints between reasoning steps. It incorporates two key strategies: (1) Answer-Clustered Search, which groups reasoning paths by their intermediate checkpoint answers to maintain diversity while ensuring quality, and (2) Checkpoint Candidate Augmentation, which leverages all intermediate answers for final decision-making. Our approach effectively reduces path homogenization and creates a fault-tolerant mechanism by utilizing high-quality intermediate results. Experimental results show that SRCA improves reasoning accuracy compared to existing TTS methods across various mathematical datasets.

</details>


### [170] [Emerging categories in scientific explanations](https://arxiv.org/abs/2505.17832)

*Giacomo Magnifico, Eduard Barbu*

**Main category:** cs.CL

**Keywords:** explanations, machine learning, dataset, biotechnology, biophysics

**Relevance Score:** 7

**TL;DR:** This work presents a new dataset focused on human-like explanations extracted from scientific literature in biotechnology and biophysics, addressing the need for impactful explanations in machine learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the growing need for impactful, human-like explanations in machine learning and the current lack of datasets that provide such explanations.

**Method:** The authors extract explanatory sentences from scientific literature (e.g., PubMed) and categorize them into 3-class and 6-class annotations, evaluating annotator consensus.

**Key Contributions:**

	1. Creation of a novel dataset of human-like explanations from scientific literature
	2. Establishment of multi-class categorization for explanation sentences
	3. Evaluation of annotator consensus on classification categories

**Result:** The resulting dataset is openly available and features a 0.667 Krippendorf Alpha value for the 3-class category, indicating a fair level of agreement among annotators.

**Limitations:** 

**Conclusion:** The creation of this dataset fills a significant gap in the availability of human-generated explanations for AI applications, contributing to better understanding and utilization of machine learning models.

**Abstract:** Clear and effective explanations are essential for human understanding and knowledge dissemination. The scope of scientific research aiming to understand the essence of explanations has recently expanded from the social sciences to machine learning and artificial intelligence. Explanations for machine learning decisions must be impactful and human-like, and there is a lack of large-scale datasets focusing on human-like and human-generated explanations. This work aims to provide such a dataset by: extracting sentences that indicate explanations from scientific literature among various sources in the biotechnology and biophysics topic domains (e.g. PubMed's PMC Open Access subset); providing a multi-class notation derived inductively from the data; evaluating annotator consensus on the emerging categories. The sentences are organized in an openly-available dataset, with two different classifications (6-class and 3-class category annotation), and the 3-class notation achieves a 0.667 Krippendorf Alpha value.

</details>


### [171] [Investigating Affect Mining Techniques for Annotation Sample Selection in the Creation of Finnish Affective Speech Corpus](https://arxiv.org/abs/2505.17833)

*Kalle Lahtinen, Einari Vaaras, Liisa Mustanoja, Okko Rsnen*

**Main category:** cs.CL

**Keywords:** affective speech, Finnish corpus, emotion annotation

**Relevance Score:** 2

**TL;DR:** This paper introduces the first spontaneous Finnish affective speech corpus, annotated for emotional arousal and valence, using a novel affect mining approach.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of existing corpora for natural emotional expression in spontaneous Finnish speech, which is varied across languages.

**Method:** The corpus was created by annotating 12,000 utterances from three large-scale Finnish speech corpora, using an affect mining approach that combines acoustic features with text sentiment analysis. The effectiveness of this method was compared to random sampling.

**Key Contributions:**

	1. First spontaneous Finnish affective speech corpus
	2. Novel affect mining approach for diverse annotation
	3. Insights into sampling strategies for affective speech corpora

**Result:** The introduced corpus showcases diverse affective expressions and offers insights into effective sampling strategies for creating affective speech corpora across different languages and contexts.

**Limitations:** 

**Conclusion:** This work lays the groundwork for future research in affective speech analysis, emphasizing the importance of diverse sampling strategies.

**Abstract:** Study of affect in speech requires suitable data, as emotional expression and perception vary across languages. Until now, no corpus has existed for natural expression of affect in spontaneous Finnish, existing data being acted or from a very specific communicative setting. This paper presents the first such corpus, created by annotating 12,000 utterances for emotional arousal and valence, sampled from three large-scale Finnish speech corpora. To ensure diverse affective expression, sample selection was conducted with an affect mining approach combining acoustic, cross-linguistic speech emotion, and text sentiment features. We compare this method to random sampling in terms of annotation diversity, and conduct post-hoc analyses to identify sampling choices that would have maximized the diversity. As an outcome, the work introduces a spontaneous Finnish affective speech corpus and informs sampling strategies for affective speech corpus creation in other languages or domains.

</details>


### [172] [Explaining Sources of Uncertainty in Automated Fact-Checking](https://arxiv.org/abs/2505.17855)

*Jingyi Sun, Greta Warren, Irina Shklovski, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** model uncertainty, natural language explanations, fact-checking, human-AI collaboration, unsupervised learning

**Relevance Score:** 9

**TL;DR:** CLUE generates natural language explanations of model uncertainty by identifying text relationships that reveal conflicts and agreements, enhancing human-AI collaboration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effectively addressing model uncertainty is crucial for successful human-AI interaction, especially when conflicts in evidence arise.

**Method:** CLUE identifies relationships between text spans to reveal claim-evidence conflicts and agreements that inform model uncertainty, using unsupervised methods and prompting with attention steering for explanation generation.

**Key Contributions:**

	1. First framework for conflict-and-agreement-aware uncertainty explanations
	2. Improves user understanding of model uncertainty
	3. No fine-tuning needed, applicable to any white-box language model

**Result:** CLUE-generated explanations are more faithful to the model's uncertainty and align better with fact-checking decisions than traditional prompting methods, improving user trust and understanding.

**Limitations:** 

**Conclusion:** CLUE offers a plug-and-play solution for generating insightful uncertainty explanations in language models without requiring fine-tuning, enhancing applications in fact-checking and reasoning.

**Abstract:** Understanding sources of a model's uncertainty regarding its predictions is crucial for effective human-AI collaboration. Prior work proposes using numerical uncertainty or hedges ("I'm not sure, but ..."), which do not explain uncertainty that arises from conflicting evidence, leaving users unable to resolve disagreements or rely on the output. We introduce CLUE (Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the first framework to generate natural language explanations of model uncertainty by (i) identifying relationships between spans of text that expose claim-evidence or inter-evidence conflicts and agreements that drive the model's predictive uncertainty in an unsupervised way, and (ii) generating explanations via prompting and attention steering that verbalize these critical interactions. Across three language models and two fact-checking datasets, we show that CLUE produces explanations that are more faithful to the model's uncertainty and more consistent with fact-checking decisions than prompting for uncertainty explanations without span-interaction guidance. Human evaluators judge our explanations to be more helpful, more informative, less redundant, and more logically consistent with the input than this baseline. CLUE requires no fine-tuning or architectural changes, making it plug-and-play for any white-box language model. By explicitly linking uncertainty to evidence conflicts, it offers practical support for fact-checking and generalises readily to other tasks that require reasoning over complex information.

</details>


### [173] [Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat Falsehoods](https://arxiv.org/abs/2505.17870)

*Shaina Raza, Rizwan Qureshi, Marcelo Lotif, Aman Chadha, Deval Pandya, Christos Emmanouilidis*

**Main category:** cs.CL

**Keywords:** Generative AI, Misinformation, Immunization framework, Falsehoods, Ethical safeguards

**Relevance Score:** 8

**TL;DR:** This position paper proposes a training framework for generative AI models that involves fine tuning on labeled falsehoods, akin to immunization, to combat misinformation while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of generative AI models reproducing false information from their training data.

**Method:** The paper proposes an immunization framework where models are fine-tuned on curated, labeled examples of misinformation, injecting these during training to enhance their ability to detect falsehoods.

**Key Contributions:**

	1. Introduces the concept of using labeled falsehoods as a supervised 'vaccine' for AI models.
	2. Demonstrates the effectiveness of this method through a case study showing reduced misinformation generation.
	3. Outlines ethical safeguards for the use of false data in training.

**Result:** Immunized models demonstrated a significant reduction in the generation of misinformation compared to baseline models.

**Limitations:** The paper does not address the potential biases in the selection of falsehoods used for training, nor does it provide extensive empirical validation of the impact in diverse settings.

**Conclusion:** This framework allows for improved recognition and rejection of misleading claims while maintaining the model's accuracy on truthful inputs, establishing a novel approach to align AI systems with factual information.

**Abstract:** Generative AI models often learn and reproduce false information present in their training corpora. This position paper argues that, analogous to biological immunization, where controlled exposure to a weakened pathogen builds immunity, AI models should be fine tuned on small, quarantined sets of explicitly labeled falsehoods as a "vaccine" against misinformation. These curated false examples are periodically injected during finetuning, strengthening the model ability to recognize and reject misleading claims while preserving accuracy on truthful inputs. An illustrative case study shows that immunized models generate substantially less misinformation than baselines. To our knowledge, this is the first training framework that treats fact checked falsehoods themselves as a supervised vaccine, rather than relying on input perturbations or generic human feedback signals, to harden models against future misinformation. We also outline ethical safeguards and governance controls to ensure the safe use of false data. Model immunization offers a proactive paradigm for aligning AI systems with factuality.

</details>


### [174] [MOOSE-Chem3: Toward Experiment-Guided Hypothesis Ranking via Simulated Experimental Feedback](https://arxiv.org/abs/2505.17873)

*Wanhao Liu, Zonglin Yang, Jue Wang, Lidong Bing, Di Zhang, Dongzhan Zhou, Yuqiang Li, Houqiang Li, Erik Cambria, Wanli Ouyang*

**Main category:** cs.CL

**Keywords:** hypothesis ranking, automated scientific discovery, simulated experiments

**Relevance Score:** 6

**TL;DR:** This paper introduces experiment-guided ranking for hypothesis selection in scientific discovery, leveraging simulated outcomes to improve ranking effectiveness compared to existing pre-experimental methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve hypothesis ranking in automated scientific discovery by incorporating empirical outcomes and addressing the limitations of costly wet-lab experiments.

**Method:** A simulator modeling hypothesis performance based on similarity to known hypotheses is developed, alongside a pseudo experiment-guided ranking method that clusters hypotheses and uses simulated feedback for prioritization.

**Key Contributions:**

	1. Introduction of experiment-guided ranking for hypothesis selection.
	2. Development of a simulator that informs the ranking process.
	3. Demonstration of improved performance over pre-experiment ranking methods.

**Result:** The proposed method outperforms traditional pre-experiment ranking approaches and strong ablations in experimental validation.

**Limitations:** The reliance on simulation may not fully capture the complexities of real-world experiments.

**Conclusion:** The experiment-guided ranking approach effectively enhances hypothesis prioritization in scientific research by utilizing simulated results from prior experiments.

**Abstract:** Hypothesis ranking is a crucial component of automated scientific discovery, particularly in natural sciences where wet-lab experiments are costly and throughput-limited. Existing approaches focus on pre-experiment ranking, relying solely on large language model's internal reasoning without incorporating empirical outcomes from experiments. We introduce the task of experiment-guided ranking, which aims to prioritize candidate hypotheses based on the results of previously tested ones. However, developing such strategies is challenging due to the impracticality of repeatedly conducting real experiments in natural science domains. To address this, we propose a simulator grounded in three domain-informed assumptions, modeling hypothesis performance as a function of similarity to a known ground truth hypothesis, perturbed by noise. We curate a dataset of 124 chemistry hypotheses with experimentally reported outcomes to validate the simulator. Building on this simulator, we develop a pseudo experiment-guided ranking method that clusters hypotheses by shared functional characteristics and prioritizes candidates based on insights derived from simulated experimental feedback. Experiments show that our method outperforms pre-experiment baselines and strong ablations.

</details>


### [175] [Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model](https://arxiv.org/abs/2505.17894)

*Khalil Hennara, Muhammad Hreden, Mohamed Motaism Hamed, Zeina Aldallal, Sara Chrouf, Safwan AlModhayan*

**Main category:** cs.CL

**Keywords:** Arabic-English translation, language model, benchmarking

**Relevance Score:** 4

**TL;DR:** Mutarjim is a compact Arabic-English translation model that outperforms larger models and introduces Tarjama-25, a new benchmark for evaluating translation quality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing large-scale language models in Arabic-English translation while providing a more efficient and effective solution.

**Method:** A two-phase training approach using the Kuwain-1.5B model with a high-quality training corpus and the introduction of a new benchmark, Tarjama-25.

**Key Contributions:**

	1. Launch of the Mutarjim language model that surpasses larger models in performance.
	2. Creation of the Tarjama-25 benchmark to evaluate translation systems more effectively.
	3. Optimized training methodology that reduces computational costs.

**Result:** Mutarjim achieves state-of-the-art performance on the English-to-Arabic translation task in Tarjama-25, outperforming models significantly larger than itself.

**Limitations:** 

**Conclusion:** The development of Mutarjim and the Tarjama-25 benchmark will enhance research in Arabic-English translation systems and provide a valuable resource for future work.

**Abstract:** We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.

</details>


### [176] [Language models can learn implicit multi-hop reasoning, but only if they have lots of training data](https://arxiv.org/abs/2505.17923)

*Yuekun Yao, Yupei Du, Dawei Zhu, Michael Hahn, Alexander Koller*

**Main category:** cs.CL

**Keywords:** Implicit reasoning, Multi-hop reasoning, Language models, Curriculum learning, Data requirements

**Relevance Score:** 7

**TL;DR:** This paper explores implicit reasoning in GPT2-style models, revealing exponential data growth needs for multi-hop reasoning tasks and the importance of model depth.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the capabilities of language models in performing multi-hop reasoning without explicit strategies and to identify necessary training conditions.

**Method:** The study utilizes GPT2-style language models trained from scratch on controlled multi-hop reasoning datasets, analyzing their performance across varying complexity levels (k = 2, 3, 4).

**Key Contributions:**

	1. Study of implicit reasoning capabilities in language models
	2. Theoretical explanation for depth growth in transformer layers
	3. Insights on data requirements and curriculum learning effects.

**Result:** Models can learn implicit k-hop reasoning, but require exponentially more training data and a linear increase in transformer layers with k, highlighting a necessary depth growth.

**Limitations:** The findings indicate that while improvements can be made, significant training data requirements remain for higher k-hop reasoning tasks.

**Conclusion:** While curriculum learning can help reduce data requirements, it cannot fully mitigate the increased training data demand as complexity increases.

**Abstract:** Implicit reasoning is the ability of a language model to solve multi-hop reasoning tasks in a single forward pass, without chain of thought. We investigate this capability using GPT2-style language models trained from scratch on controlled $k$-hop reasoning datasets ($k = 2, 3, 4$). We show that while such models can indeed learn implicit $k$-hop reasoning, the required training data grows exponentially in $k$, and the required number of transformer layers grows linearly in $k$. We offer a theoretical explanation for why this depth growth is necessary. We further find that the data requirement can be mitigated, but not eliminated, through curriculum learning.

</details>


### [177] [Handling Symbolic Language in Student Texts: A Comparative Study of NLP Embedding Models](https://arxiv.org/abs/2505.17950)

*Tom Bleckmann, Paul Tschisgale*

**Main category:** cs.CL

**Keywords:** NLP, learning analytics, symbolic expressions, embedding models, machine learning

**Relevance Score:** 7

**TL;DR:** This study evaluates NLP embedding models on their ability to process science-related symbolic expressions, revealing significant performance differences and highlighting key considerations for model selection in learning analytics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by symbolic expressions in science-related language which existing NLP embedding models struggle to process effectively, potentially biasing findings in learning analytics.

**Method:** The study evaluates various NLP embedding models using physics-specific symbolic expressions from student responses, employing similarity-based analyses and integration into a machine learning pipeline to assess performance.

**Key Contributions:**

	1. Evaluation of NLP models on science-related symbolic expressions
	2. Identification of OpenAI's GPT-text-embedding-3-large as a leading model
	3. Discussion on important factors for model selection beyond performance

**Result:** OpenAI's GPT-text-embedding-3-large outperformed other models in processing symbolic expressions, though its advantage was moderate. The study also discusses cost, regulatory compliance, and model transparency as important factors in model selection.

**Limitations:** 

**Conclusion:** NLP embedding model selection is crucial for effective learning analytics applications dealing with science-related language that includes symbolic expressions.

**Abstract:** Recent advancements in Natural Language Processing (NLP) have facilitated the analysis of student-generated language products in learning analytics (LA), particularly through the use of NLP embedding models. Yet when it comes to science-related language, symbolic expressions such as equations and formulas introduce challenges that current embedding models struggle to address. Existing studies and applications often either overlook these challenges or remove symbolic expressions altogether, potentially leading to biased findings and diminished performance of LA applications. This study therefore explores how contemporary embedding models differ in their capability to process and interpret science-related symbolic expressions. To this end, various embedding models are evaluated using physics-specific symbolic expressions drawn from authentic student responses, with performance assessed via two approaches: similarity-based analyses and integration into a machine learning pipeline. Our findings reveal significant differences in model performance, with OpenAI's GPT-text-embedding-3-large outperforming all other examined models, though its advantage over other models was moderate rather than decisive. Beyond performance, additional factors such as cost, regulatory compliance, and model transparency are discussed as key considerations for model selection. Overall, this study underscores the importance for LA researchers and practitioners of carefully selecting NLP embedding models when working with science-related language products that include symbolic expressions.

</details>


### [178] [Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with Minimalist Rule-Based RL](https://arxiv.org/abs/2505.17952)

*Che Liu, Haozhe Wang, Jiazhen Pan, Zhongwei Wan, Yong Dai, Fangzhen Lin, Wenjia Bai, Daniel Rueckert, Rossella Arcucci*

**Main category:** cs.CL

**Keywords:** medical LLM, reinforcement learning, reasoning capability, multiple-choice QA, dataset informativeness

**Relevance Score:** 9

**TL;DR:** AlphaMed is the first medical LLM that demonstrates reasoning capabilities through reinforcement learning without supervised fine-tuning, achieving state-of-the-art results on medical QA benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning in LLMs for clinical applications without needing costly supervised fine-tuning on closed-source models.

**Method:** AlphaMed employs minimalist rule-based rewards in reinforcement learning on public multiple-choice QA datasets, bypassing SFT or distilled CoT data.

**Key Contributions:**

	1. First medical LLM to achieve reasoning via RL without SFT.
	2. Demonstrates effectiveness of minimalist rule-based RL rewards on public datasets.
	3. Highlights the importance of dataset quantity and diversity in reasoning performance.

**Result:** Achieves state-of-the-art results on six medical QA benchmarks, surpassing larger or closed-source models like DeepSeek-V3-671B and Claude-3.5-Sonnet in some cases.

**Limitations:** Observations suggest divergent trends across benchmarks, indicating limitations in current evaluation methodologies.

**Conclusion:** Dataset informativeness significantly influences reasoning performance, and minimalist RL methods can induce reasoning effectively without CoT supervision, although current evaluation benchmarks need improvement.

**Abstract:** Improving performance on complex tasks and enabling interpretable decision making in large language models (LLMs), especially for clinical applications, requires effective reasoning. Yet this remains challenging without supervised fine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from closed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the first medical LLM to show that reasoning capability can emerge purely through reinforcement learning (RL), using minimalist rule-based rewards on public multiple-choice QA datasets, without relying on SFT or distilled CoT data. AlphaMed achieves state-of-the-art results on six medical QA benchmarks, outperforming models trained with conventional SFT+RL pipelines. On challenging benchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source models such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the factors behind this success, we conduct a comprehensive data-centric analysis guided by three questions: (i) Can minimalist rule-based RL incentivize reasoning without distilled CoT supervision? (ii) How do dataset quantity and diversity impact reasoning? (iii) How does question difficulty shape the emergence and generalization of reasoning? Our findings show that dataset informativeness is a key driver of reasoning performance, and that minimalist RL on informative, multiple-choice QA data is effective at inducing reasoning without CoT supervision. We also observe divergent trends across benchmarks, underscoring limitations in current evaluation and the need for more challenging, reasoning-oriented medical QA benchmarks.

</details>


### [179] [Counting Cycles with Deepseek](https://arxiv.org/abs/2505.17964)

*Jiashun Jin, Tracy Ke, Bingcheng Sui, Zhenggang Wang*

**Main category:** cs.CL

**Keywords:** AI, mathematics, graph theory, cycle count statistic, computational efficiency

**Relevance Score:** 2

**TL;DR:** The paper addresses the challenge of deriving a Computationally Efficient Equivalent Form (CEEF) for the cycle count statistic, demonstrating how AI can assist in solving complex mathematical problems with proper guidance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the difficulties faced by AI in solving advanced mathematics, particularly in deriving formulations for the cycle count statistic, which lacks general solutions.

**Method:** The authors propose a novel approach that combines their strategy with AI's coding capabilities, focusing on graph theory to derive new formulas for the CEEF problem.

**Key Contributions:**

	1. Introduction of a novel approach combining human guidance and AI coding skills to derive mathematical formulations.
	2. Discovery of new formulas for the cycle count statistic not previously known.
	3. Demonstration of the importance of structured guidance in unlocking AI's potential for advanced problem-solving.

**Result:** The research yields new formulas for general cases of the cycle count statistic previously undiscovered, showcasing AI's potential when guided appropriately.

**Limitations:** The AI's ability to solve the problem is contingent on the clarity of the guidance provided; it cannot operate independently to reach a solution.

**Conclusion:** AI can effectively contribute to complex mathematical problem-solving with a well-defined strategy, though it cannot fully solve such problems independently.

**Abstract:** Despite recent progress, AI still struggles on advanced mathematics. We consider a difficult open problem: How to derive a Computationally Efficient Equivalent Form (CEEF) for the cycle count statistic? The CEEF problem does not have known general solutions, and requires delicate combinatorics and tedious calculations. Such a task is hard to accomplish by humans but is an ideal example where AI can be very helpful. We solve the problem by combining a novel approach we propose and the powerful coding skills of AI. Our results use delicate graph theory and contain new formulas for general cases that have not been discovered before. We find that, while AI is unable to solve the problem all by itself, it is able to solve it if we provide it with a clear strategy, a step-by-step guidance and carefully written prompts. For simplicity, we focus our study on DeepSeek-R1 but we also investigate other AI approaches.

</details>


### [180] [AVerImaTeC: A Dataset for Automatic Verification of Image-Text Claims with Evidence from the Web](https://arxiv.org/abs/2505.17978)

*Rui Cao, Zifeng Ding, Zhijiang Guo, Michael Schlichtkrull, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** image-text claims, dataset, misinformation, fact-checking, evidence retrieval

**Relevance Score:** 4

**TL;DR:** Introduction of AVerImaTeC, a dataset of real-world image-text claims for automated verification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing datasets for verifying image-text claims, which often rely on synthetic data and lack evidence annotations.

**Method:** The creation of AVerImaTeC with 1,297 real-world claims, annotated with QA pairs and evidence, employing techniques for normalization, temporal constraints, and sufficiency checks.

**Key Contributions:**

	1. Introduction of a comprehensive dataset for image-text claim verification
	2. High inter-annotator consistency on verdicts and QA pairs
	3. Novel evaluation method for evidence retrieval

**Result:** Achieved high inter-annotator consistency and proposed a novel evaluation method for evidence retrieval, establishing baselines for verification tasks.

**Limitations:** The dataset's effectiveness may be constrained by the specific domains and contexts covered by the image-text claims.

**Conclusion:** AVerImaTeC improves the landscape of automated verification by providing well-annotated real-world claims, thus supporting better research into misinformation verification.

**Abstract:** Textual claims are often accompanied by images to enhance their credibility and spread on social media, but this also raises concerns about the spread of misinformation. Existing datasets for automated verification of image-text claims remain limited, as they often consist of synthetic claims and lack evidence annotations to capture the reasoning behind the verdict. In this work, we introduce AVerImaTeC, a dataset consisting of 1,297 real-world image-text claims. Each claim is annotated with question-answer (QA) pairs containing evidence from the web, reflecting a decomposed reasoning regarding the verdict. We mitigate common challenges in fact-checking datasets such as contextual dependence, temporal leakage, and evidence insufficiency, via claim normalization, temporally constrained evidence annotation, and a two-stage sufficiency check. We assess the consistency of the annotation in AVerImaTeC via inter-annotator studies, achieving a $\kappa=0.742$ on verdicts and $74.7\%$ consistency on QA pairs. We also propose a novel evaluation method for evidence retrieval and conduct extensive experiments to establish baselines for verifying image-text claims using open-web evidence.

</details>


### [181] [TRACE for Tracking the Emergence of Semantic Representations in Transformers](https://arxiv.org/abs/2505.17998)

*Nura Aljaafari, Danilo S. Carvalho, Andr Freitas*

**Main category:** cs.CL

**Keywords:** transformer models, phase transitions, linguistic abstraction, machine learning, TRACE

**Relevance Score:** 7

**TL;DR:** This paper introduces TRACE, a framework to analyze phase transitions in transformer models during training, focusing on the emergence of linguistic structure and abstraction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the poorly understood phase transitions in transformer models and how they relate to linguistic structure and abstraction.

**Method:** Introducing TRACE, a diagnostic framework that utilizes geometric, informational, and linguistic signals to detect phase transitions alongside a data generation method called ABSynth for controlled analysis.

**Key Contributions:**

	1. Introduction of TRACE framework for analyzing transformer phase transitions
	2. Use of ABSynth for generating annotated synthetic corpora
	3. Insights into the relationship between model architecture and abstraction emergence

**Result:** Experiments demonstrate clear correlations between phase transitions, curvature collapse, dimension stabilization, and improvements in syntactic and semantic accuracy across models.

**Limitations:** 

**Conclusion:** The findings enhance understanding of linguistic abstraction in language models, which may influence future LM development strategies.

**Abstract:** Modern transformer models exhibit phase transitions during training, distinct shifts from memorisation to abstraction, but the mechanisms underlying these transitions remain poorly understood. Prior work has often focused on endpoint representations or isolated signals like curvature or mutual information, typically in symbolic or arithmetic domains, overlooking the emergence of linguistic structure. We introduce TRACE (Tracking Representation Abstraction and Compositional Emergence), a diagnostic framework combining geometric, informational, and linguistic signals to detect phase transitions in Transformer-based LMs. TRACE leverages a frame-semantic data generation method, ABSynth, that produces annotated synthetic corpora with controllable complexity, lexical distributions, and structural entropy, while being fully annotated with linguistic categories, enabling precise analysis of abstraction emergence. Experiments reveal that (i) phase transitions align with clear intersections between curvature collapse and dimension stabilisation; (ii) these geometric shifts coincide with emerging syntactic and semantic accuracy; (iii) abstraction patterns persist across architectural variants, with components like feedforward networks affecting optimisation stability rather than fundamentally altering trajectories. This work advances our understanding of how linguistic abstractions emerge in LMs, offering insights into model interpretability, training efficiency, and compositional generalisation that could inform more principled approaches to LM development.

</details>


### [182] [Training with Pseudo-Code for Instruction Following](https://arxiv.org/abs/2505.18011)

*Prince Kumar, Rudra Murthy, Riyaz Bhat, Danish Contractor*

**Main category:** cs.CL

**Keywords:** Large Language Models, instruction-following, pseudo-code, fine-tuning, HCI

**Relevance Score:** 9

**TL;DR:** This paper proposes fine-tuning Large Language Models (LLMs) using instruction-tuning data that includes pseudo-code to improve instruction-following capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the instruction-following abilities of LLMs, especially for non-expert users, by using pseudo-code to express instructions more effectively.

**Method:** The authors fine-tune LLMs with instruction-tuning data that incorporates pseudo-code along with final responses and evaluate the models on 11 benchmarks related to instruction-following, mathematics, and common-sense reasoning.

**Key Contributions:**

	1. Introduces a novel method for fine-tuning LLMs using pseudo-code for instructions.
	2. Demonstrates significant performance improvements in instruction-following tasks.
	3. Maintains effectiveness on mathematical and common-sense reasoning tasks.

**Result:** The experiments show that models trained with pseudo-code follow instructions better, achieving a 3-19% relative gain on instruction-following benchmarks and an average gain of up to 14% across all tasks.

**Limitations:** 

**Conclusion:** Fine-tuning with pseudo-code improves LLM instruction-following capabilities while maintaining performance on mathematical and common-sense reasoning tasks.

**Abstract:** Despite the rapid progress in the capabilities of Large Language Models (LLMs), they continue to have difficulty following relatively simple, unambiguous instructions, especially when compositions are involved. In this paper, we take inspiration from recent work that suggests that models may follow instructions better when they are expressed in pseudo-code. However, writing pseudo-code programs can be tedious and using few-shot demonstrations to craft code representations for use in inference can be unnatural for non-expert users of LLMs. To overcome these limitations, we propose fine-tuning LLMs with instruction-tuning data that additionally includes instructions re-expressed in pseudo-code along with the final response. We evaluate models trained using our method on $11$ publicly available benchmarks comprising of tasks related to instruction-following, mathematics, and common-sense reasoning. We conduct rigorous experiments with $5$ different models and find that not only do models follow instructions better when trained with pseudo-code, they also retain their capabilities on the other tasks related to mathematical and common sense reasoning. Specifically, we observe a relative gain of $3$--$19$% on instruction-following benchmark, and an average gain of upto 14% across all tasks.

</details>


### [183] [Contrastive Distillation of Emotion Knowledge from LLMs for Zero-Shot Emotion Recognition](https://arxiv.org/abs/2505.18040)

*Minxue Niu, Emily Mower Provost*

**Main category:** cs.CL

**Keywords:** Emotion Recognition, Large Language Models, Zero-shot Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces a contrastive distillation framework to enhance emotion recognition systems by transferring knowledge from large language models (LLMs) to a compact model, enabling zero-shot prediction across diverse emotion labels without needing human annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve adaptability in emotion recognition systems, allowing them to handle various emotion labels without dedicated training, which conventional ER models struggle to do.

**Method:** The proposed method uses a contrastive distillation framework that utilizes GPT-4 to generate descriptive emotion annotations and align text samples with emotion descriptors in a shared embedding space for zero-shot prediction.

**Key Contributions:**

	1. Introduction of a contrastive distillation framework for emotion recognition
	2. Utilization of GPT-4 for generating emotion annotations
	3. Achievement of zero-shot performance across different emotion classes and labels.

**Result:** The distilled model successfully predicts emotions across diverse datasets and label spaces, achieving performance close to GPT-4 while being over 10,000 times smaller than LLMs.

**Limitations:** 

**Conclusion:** This approach demonstrates the potential of using large language models for emotion recognition in a simplified and efficient manner, bridging the gap left by traditional fixed-label ER systems.

**Abstract:** The ability to handle various emotion labels without dedicated training is crucial for building adaptable Emotion Recognition (ER) systems. Conventional ER models rely on training using fixed label sets and struggle to generalize beyond them. On the other hand, Large Language Models (LLMs) have shown strong zero-shot ER performance across diverse label spaces, but their scale limits their use on edge devices. In this work, we propose a contrastive distillation framework that transfers rich emotional knowledge from LLMs into a compact model without the use of human annotations. We use GPT-4 to generate descriptive emotion annotations, offering rich supervision beyond fixed label sets. By aligning text samples with emotion descriptors in a shared embedding space, our method enables zero-shot prediction on different emotion classes, granularity, and label schema. The distilled model is effective across multiple datasets and label spaces, outperforming strong baselines of similar size and approaching GPT-4's zero-shot performance, while being over 10,000 times smaller.

</details>


### [184] [MathEDU: Towards Adaptive Feedback for Student Mathematical Problem-Solving](https://arxiv.org/abs/2505.18056)

*Wei-Ling Hsu, Yu-Chien Tang, An-Zi Yen*

**Main category:** cs.CL

**Keywords:** large language models, personalized feedback, math education, MathEDU dataset, adaptive learning

**Relevance Score:** 9

**TL;DR:** This study investigates the use of large language models to provide personalized feedback on math problem-solving in education, introducing the MathEDU dataset for evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of immediate, personalized feedback in online learning, particularly for math problem-solving.

**Method:** The paper introduces the MathEDU dataset, evaluates a fine-tuned LLM in two scenariosone with access to prior answer histories and another in a cold-start context.

**Key Contributions:**

	1. Introduction of the MathEDU dataset for annotating student math solutions
	2. Evaluation of LLMs for personalized feedback in math education
	3. Insights into model performance in different feedback contexts

**Result:** The model effectively identifies correctness in student solutions but struggles with generating detailed pedagogical feedback.

**Limitations:** The model has challenges in generating detailed and pedagogically useful feedback despite good correctness identification.

**Conclusion:** While LLMs show promise in supporting personalized learning, challenges remain in providing meaningful feedback for educational improvement.

**Abstract:** Online learning enhances educational accessibility, offering students the flexibility to learn anytime, anywhere. However, a key limitation is the lack of immediate, personalized feedback, particularly in helping students correct errors in math problem-solving. Several studies have investigated the applications of large language models (LLMs) in educational contexts. In this paper, we explore the capabilities of LLMs to assess students' math problem-solving processes and provide adaptive feedback. The MathEDU dataset is introduced, comprising authentic student solutions annotated with teacher feedback. We evaluate the model's ability to support personalized learning in two scenarios: one where the model has access to students' prior answer histories, and another simulating a cold-start context. Experimental results show that the fine-tuned model performs well in identifying correctness. However, the model still faces challenges in generating detailed feedback for pedagogical purposes.

</details>


### [185] [Extended Inductive Reasoning for Personalized Preference Inference from Behavioral Signals](https://arxiv.org/abs/2505.18071)

*Jia-Nan Li, Jian Guan, Wei Wu, Rui Yan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Inductive Reasoning, Preference Inference, Reinforcement Learning, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces AlignXplore, a model for improved inductive reasoning in LLMs focused on personalized preference inference from user interaction histories.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in aligning large language models with diverse user preferences, which require robust inductive reasoning capabilities.

**Method:** The study proposes AlignXplore, utilizing extended reasoning chains combined with cold-start training on synthetic data and subsequent online reinforcement learning.

**Key Contributions:**

	1. Development of AlignXplore for inductive reasoning in LLMs.
	2. Combination of cold-start training and online reinforcement learning techniques.
	3. Demonstration of enhanced performance in preference inference tasks.

**Result:** AlignXplore shows an improvement of 11.05% over the backbone model on benchmarks, while demonstrating strong generalization across various input formats and downstream models.

**Limitations:** 

**Conclusion:** The analyses from the experiments suggest effective strategies for preference inference and highlight the emergence of human-like inductive reasoning patterns in training.

**Abstract:** Large language models (LLMs) have demonstrated significant success in complex reasoning tasks such as math and coding. In contrast to these tasks where deductive reasoning predominates, inductive reasoning\textemdash the ability to derive general rules from incomplete evidence, remains underexplored. This paper investigates extended inductive reasoning in LLMs through the lens of personalized preference inference, a critical challenge in LLM alignment where current approaches struggle to capture diverse user preferences. The task demands strong inductive reasoning capabilities as user preferences are typically embedded implicitly across various interaction forms, requiring models to synthesize consistent preference patterns from scattered signals. We propose \textsc{AlignXplore}, a model that leverages extended reasoning chains to enable systematic preference inference from behavioral signals in users' interaction histories. We develop \textsc{AlignXplore} by combining cold-start training based on synthetic data with subsequent online reinforcement learning. Through extensive experiments, we demonstrate that \textsc{AlignXplore} achieves substantial improvements over the backbone model by an average of 11.05\% on in-domain and out-of-domain benchmarks, while maintaining strong generalization ability across different input formats and downstream models. Further analyses establish best practices for preference inference learning through systematic comparison of reward modeling strategies, while revealing the emergence of human-like inductive reasoning patterns during training.

</details>


### [186] [QwenLong-CPRS: Towards $\infty$-LLMs with Dynamic Context Optimization](https://arxiv.org/abs/2505.18092)

*Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, Ming Yan*

**Main category:** cs.CL

**Keywords:** context compression, long-context optimization, dynamic optimization, language models, performance evaluation

**Relevance Score:** 9

**TL;DR:** QwenLong-CPRS is a context compression framework for long-context optimization in LLMs, improving performance and efficiency through dynamic optimization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome computation overhead and performance degradation in LLMs when processing long sequences.

**Method:** Introduces a dynamic context optimization mechanism allowing multi-granularity context compression based on natural language instructions.

**Key Contributions:**

	1. Natural language-guided dynamic optimization
	2. Bidirectional reasoning layers for enhanced boundary awareness
	3. Token critic mechanisms with language modeling heads

**Result:** Achieves 21.59 context compression and 19.15-point average performance gains, outperforming existing context management methods.

**Limitations:** 

**Conclusion:** QwenLong-CPRS sets new state-of-the-art performance for LLMs, integrating effectively with leading models.

**Abstract:** This technical report presents QwenLong-CPRS, a context compression framework designed for explicit long-context optimization, addressing prohibitive computation overhead during the prefill stage and the "lost in the middle" performance degradation of large language models (LLMs) during long sequence processing. Implemented through a novel dynamic context optimization mechanism, QwenLong-CPRS enables multi-granularity context compression guided by natural language instructions, achieving both efficiency gains and improved performance.   Evolved from the Qwen architecture series, QwenLong-CPRS introduces four key innovations: (1) Natural language-guided dynamic optimization, (2) Bidirectional reasoning layers for enhanced boundary awareness, (3) Token critic mechanisms with language modeling heads, and (4) Window-parallel inference.   Comprehensive evaluations across five benchmarks (4K-2M word contexts) demonstrate QwenLong-CPRS's threefold effectiveness: (1) Consistent superiority over other context management methods like RAG and sparse attention in both accuracy and efficiency. (2) Architecture-agnostic integration with all flagship LLMs, including GPT-4o, Gemini2.0-pro, Claude3.7-sonnet, DeepSeek-v3, and Qwen2.5-max, achieves 21.59$\times$ context compression alongside 19.15-point average performance gains; (3) Deployed with Qwen2.5-32B-Instruct, QwenLong-CPRS surpasses leading proprietary LLMs by 4.85 and 10.88 points on Ruler-128K and InfiniteBench, establishing new SOTA performance.

</details>


### [187] [Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL](https://arxiv.org/abs/2505.18098)

*Joey Hong, Anca Dragan, Sergey Levine*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, interaction tasks, reasoning, value functions

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach using goal-conditioned value functions to enhance reasoning in large language models for complex, multi-turn interactions, achieving improved performance while maintaining efficiency.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** Complex tasks such as negotiation and persuasion require long-horizon reasoning in large language models, which is limited by traditional reinforcement learning fine-tuning methods due to high memory and computational costs.

**Method:** The proposed method utilizes goal-conditioned value functions to guide the reasoning of LLM agents, predicting task outcomes based on actions taken. This approach allows for scale and efficiency even with large API-based models.

**Key Contributions:**

	1. Introduction of goal-conditioned value functions for LLM interaction tasks
	2. Demonstration of improved efficiency and scalability in reasoning
	3. Validation of method across various complex interaction tasks

**Result:** The new approach demonstrates superior performance in interaction tasks like tool use and dialogue compared to standard RL fine-tuning and prompting methods.

**Limitations:** 

**Conclusion:** By training concise, lightweight value functions over reasoning steps rather than full actions, the proposed method enhances decision-making in multi-turn interactions without the scalability issues of traditional training methods.

**Abstract:** Large language models (LLMs) excel in tasks like question answering and dialogue, but complex tasks requiring interaction, such as negotiation and persuasion, require additional long-horizon reasoning and planning. Reinforcement learning (RL) fine-tuning can enable such planning in principle, but suffers from drawbacks that hinder scalability. In particular, multi-turn RL training incurs high memory and computational costs, which are exacerbated when training LLMs as policies. Furthermore, the largest LLMs do not expose the APIs necessary to be trained in such manner. As a result, modern methods to improve the reasoning of LLMs rely on sophisticated prompting mechanisms rather than RL fine-tuning. To remedy this, we propose a novel approach that uses goal-conditioned value functions to guide the reasoning of LLM agents, that scales even to large API-based models. These value functions predict how a task will unfold given an action, allowing the LLM agent to evaluate multiple possible outcomes, both positive and negative, to plan effectively. In addition, these value functions are trained over reasoning steps rather than full actions, to be a concise and light-weight module that facilitates decision-making in multi-turn interactions. We validate our method on tasks requiring interaction, including tool use, social deduction, and dialogue, demonstrating superior performance over both RL fine-tuning and prompting methods while maintaining efficiency and scalability.

</details>


### [188] [ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework](https://arxiv.org/abs/2505.18105)

*Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao*

**Main category:** cs.CL

**Keywords:** LLM, Deep Search, Multi-Agent, Benchmark, Reasoning

**Relevance Score:** 9

**TL;DR:** ManuSearch proposes a modular multi-agent framework for deep search using LLMs, leveraging collaborative agents for solution planning, web search, and webpage reading.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To democratize access to LLM capabilities in complex reasoning tasks that are typically locked in proprietary systems.

**Method:** The framework utilizes three collaborative agents: a solution planning agent, an internet search agent, and a structured webpage reading agent, enabling a comprehensive search and reasoning process.

**Key Contributions:**

	1. Introduction of a modular multi-agent framework for LLM-based search
	2. Development of the ORION benchmark for evaluating open-web reasoning
	3. Democratization of LLM capabilities for complex reasoning tasks

**Result:** ManuSearch outperforms existing open-source models and leading closed-source systems in evaluating deep reasoning abilities, showcasing its effectiveness through the ORION benchmark.

**Limitations:** 

**Conclusion:** ManuSearch enhances transparency and extensibility in research on open deep search systems, offering new pathways for reproducible research.

**Abstract:** Recent advances in web-augmented large language models (LLMs) have exhibited strong performance in complex reasoning tasks, yet these capabilities are mostly locked in proprietary systems with opaque architectures. In this work, we propose \textbf{ManuSearch}, a transparent and modular multi-agent framework designed to democratize deep search for LLMs. ManuSearch decomposes the search and reasoning process into three collaborative agents: (1) a solution planning agent that iteratively formulates sub-queries, (2) an Internet search agent that retrieves relevant documents via real-time web search, and (3) a structured webpage reading agent that extracts key evidence from raw web content. To rigorously evaluate deep reasoning abilities, we introduce \textbf{ORION}, a challenging benchmark focused on open-web reasoning over long-tail entities, covering both English and Chinese. Experimental results show that ManuSearch substantially outperforms prior open-source baselines and even surpasses leading closed-source systems. Our work paves the way for reproducible, extensible research in open deep search systems. We release the data and code in https://github.com/RUCAIBox/ManuSearch

</details>


### [189] [Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM](https://arxiv.org/abs/2505.18110)

*Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, Farid Boussaid, Girish Dwivedi, Luqi Gong, Qiuhong Ke*

**Main category:** cs.CL

**Keywords:** multimodal learning, video analysis, large language model

**Relevance Score:** 7

**TL;DR:** TriSense is a triple-modality large language model for holistic video temporal understanding, integrating visual, audio, and speech cues.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing models struggle to fuse and interpret audio information in video analysis, limiting their temporal understanding capabilities.

**Method:** TriSense utilizes a Query-Based Connector to adaptively reweight modality contributions based on input queries, and is supported by a new dataset, TriSense-2M, of over 2 million curated samples.

**Key Contributions:**

	1. Introduction of TriSense for improved multimodal video understanding
	2. Development of TriSense-2M dataset with over 2 million samples
	3. Innovative Query-Based Connector for adaptive modality integration

**Result:** TriSense demonstrates robust performance in multimodal video analysis, effectively handling modality dropout and flexible input combinations.

**Limitations:** 

**Conclusion:** TriSense shows potential to advance multimodal video analysis, with code and dataset to be publicly released.

**Abstract:** Humans naturally understand moments in a video by integrating visual and auditory cues. For example, localizing a scene in the video like "A scientist passionately speaks on wildlife conservation as dramatic orchestral music plays, with the audience nodding and applauding" requires simultaneous processing of visual, audio, and speech signals. However, existing models often struggle to effectively fuse and interpret audio information, limiting their capacity for comprehensive video temporal understanding. To address this, we present TriSense, a triple-modality large language model designed for holistic video temporal understanding through the integration of visual, audio, and speech modalities. Central to TriSense is a Query-Based Connector that adaptively reweights modality contributions based on the input query, enabling robust performance under modality dropout and allowing flexible combinations of available inputs. To support TriSense's multimodal capabilities, we introduce TriSense-2M, a high-quality dataset of over 2 million curated samples generated via an automated pipeline powered by fine-tuned LLMs. TriSense-2M includes long-form videos and diverse modality combinations, facilitating broad generalization. Extensive experiments across multiple benchmarks demonstrate the effectiveness of TriSense and its potential to advance multimodal video analysis. Code and dataset will be publicly released.

</details>


### [190] [UNJOIN: Enhancing Multi-Table Text-to-SQL Generation via Schema Simplification](https://arxiv.org/abs/2505.18122)

*Poojah Ganesan, Rajat Aayush Jha, Dan Roth, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, multi-table databases, large language models

**Relevance Score:** 7

**TL;DR:** UNJOIN is a two-stage framework that improves Text-to-SQL performance for multi-table queries by decoupling schema retrieval from SQL logic generation.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods struggle with multi-table database queries due to complex schemas and relational operations.

**Method:** UNJOIN merges column names across all tables into a single representation for the first stage, followed by generating the SQL query on this simplified schema in the second stage.

**Key Contributions:**

	1. A novel two-stage framework for Text-to-SQL queries
	2. Decouples schema retrieval from SQL logic generation
	3. Achieves state-of-the-art results on benchmark datasets

**Result:** UNJOIN matches or exceeds state-of-the-art baselines on SPIDER and BIRD datasets.

**Limitations:** 

**Conclusion:** UNJOIN is scalable and adaptable across databases without requiring data access or fine-tuning.

**Abstract:** Recent advances in large language models (LLMs) have greatly improved Text-to-SQL performance for single-table queries. But, it remains challenging in multi-table databases due to complex schema and relational operations. Existing methods often struggle with retrieving the right tables and columns, generating accurate JOINs and UNIONs, and generalizing across diverse schemas. To address these issues, we introduce UNJOIN, a two-stage framework that decouples the retrieval of schema elements from SQL logic generation. In the first stage, we merge the column names of all tables in the database into a single-table representation by prefixing each column with its table name. This allows the model to focus purely on accurate retrieval without being distracted by the need to write complex SQL logic. In the second stage, the SQL query is generated on this simplified schema and mapped back to the original schema by reconstructing JOINs, UNIONs, and relational logic. Evaluations on SPIDER and BIRD datasets show that UNJOIN matches or exceeds the state-of-the-art baselines. UNJOIN uses only schema information, which does not require data access or fine-tuning, making it scalable and adaptable across databases.

</details>


### [191] [Frankentext: Stitching random text fragments into long-form narratives](https://arxiv.org/abs/2505.18128)

*Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer*

**Main category:** cs.CL

**Keywords:** Frankentexts, LLMs, controllable generation, authorship detection, human-AI collaboration

**Relevance Score:** 9

**TL;DR:** This paper introduces Frankentexts, a new narrative type created by LLMs that must copy 90% of tokens from human texts, exploring controllable generation, writing quality, and detection of machine-generated text.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the challenges of controllable generation in LLMs and examine the implications for authorship verification and human-AI collaboration.

**Method:** The model drafts narratives by selecting human-written passages, then revises them while maintaining a specified copy ratio, evaluated on writing quality, instruction adherence, and detectability.

**Key Contributions:**

	1. Introduction of Frankentexts as a new narrative form
	2. Insights into the effectiveness of writing prompts in LLMs
	3. Discussion on the limitations of AI text detectors

**Result:** Gemini-2.5-Pro generated coherent Frankentexts, with 81% coherence and 100% prompt relevance, but 59% were misclassified as human-written by AI detectors.

**Limitations:** Human annotators can identify Frankentexts through tone shifts and grammar issues, especially in longer texts.

**Conclusion:** Frankentexts raise important discussions on authorship detection, provide data for mixed authorship analysis, and allow exploration of human-AI co-writing.

**Abstract:** We introduce Frankentexts, a new type of long-form narratives produced by LLMs under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from human writings. This task presents a challenging test of controllable generation, requiring models to satisfy a writing prompt, integrate disparate text fragments, and still produce a coherent narrative. To generate Frankentexts, we instruct the model to produce a draft by selecting and combining human-written passages, then iteratively revise the draft while maintaining a user-specified copy ratio. We evaluate the resulting Frankentexts along three axes: writing quality, instruction adherence, and detectability. Gemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts are coherent and 100% relevant to the prompt. Notably, up to 59% of these outputs are misclassified as human-written by detectors like Pangram, revealing limitations in AI text detectors. Human annotators can sometimes identify Frankentexts through their abrupt tone shifts and inconsistent grammar between segments, especially in longer generations. Beyond presenting a challenging generation task, Frankentexts invite discussion on building effective detectors for this new grey zone of authorship, provide training data for mixed authorship detection, and serve as a sandbox for studying human-AI co-writing processes.

</details>


### [192] [Graph-Linguistic Fusion: Using Language Models for Wikidata Vandalism Detection](https://arxiv.org/abs/2505.18136)

*Mykola Trokhymovych, Lydia Pintscher, Ricardo Baeza-Yates, Diego Saez-Trumper*

**Main category:** cs.CL

**Keywords:** vandalism detection, Wikidata, multilingual language model, Graph2Text, open source

**Relevance Score:** 4

**TL;DR:** Next-generation vandalism detection system for Wikidata using a unified Graph2Text approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To detect vandalism in Wikidata's complex and ever-expanding knowledge environment.

**Method:** Employing a method called Graph2Text to convert all edits into a single space for evaluating potential vandalism with a multilingual language model.

**Key Contributions:**

	1. Introduction of Graph2Text for vandalism detection
	2. Improved system performance compared to existing solutions
	3. Release of open source code and dataset for further research

**Result:** The new system outperforms the existing production system in detecting vandalism.

**Limitations:** 

**Conclusion:** The unified approach enhances maintenance and coverage for detecting vandalism in Wikidata.

**Abstract:** We introduce a next-generation vandalism detection system for Wikidata, one of the largest open-source structured knowledge bases on the Web. Wikidata is highly complex: its items incorporate an ever-expanding universe of factual triples and multilingual texts. While edits can alter both structured and textual content, our approach converts all edits into a single space using a method we call Graph2Text. This allows for evaluating all content changes for potential vandalism using a single multilingual language model. This unified approach improves coverage and simplifies maintenance. Experiments demonstrate that our solution outperforms the current production system. Additionally, we are releasing the code under an open license along with a large dataset of various human-generated knowledge alterations, enabling further research.

</details>


### [193] [Lost in the Haystack: Smaller Needles are More Difficult for LLMs to Find](https://arxiv.org/abs/2505.18148)

*Owen Bianchi, Mathew J. Koretsky, Maya Willey, Chelsea X. Alvarado, Tanay Nayak, Adi Asija, Nicole Kuznetsov, Mike A. Nalls, Faraz Faghri, Daniel Khashabi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Gold Context Size, Long-Context Question Answering, Positional Sensitivity, Performance Evaluation

**Relevance Score:** 9

**TL;DR:** The paper investigates how variations in gold context length affect the performance of large language models (LLMs) in long-context question answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how gold context size influences LLM performance in tasks requiring retrieval of relevant information from large amounts of irrelevant data.

**Method:** The study systematically varies the size of the gold context and evaluates how this impacts the performance of seven state-of-the-art LLMs across three domains: general knowledge, biomedical reasoning, and mathematical reasoning.

**Key Contributions:**

	1. Analysis of gold context size's impact on LLM performance
	2. Insights derived from three distinct domains
	3. Recommendations for the design of context-aware LLM systems

**Result:** The experiments show that LLM performance sharply declines with shorter gold contexts, indicating that smaller contexts degrade performance and increase sensitivity to positional bias.

**Limitations:** 

**Conclusion:** The findings highlight the need for improved context-aware systems that can effectively integrate information of varying lengths, especially for tasks involving scattered fine-grained information.

**Abstract:** Large language models (LLMs) face significant challenges with needle-in-a-haystack tasks, where relevant information ("the needle") must be drawn from a large pool of irrelevant context ("the haystack"). Previous studies have highlighted positional bias and distractor quantity as critical factors affecting model performance, yet the influence of gold context size has received little attention. We address this gap by systematically studying how variations in gold context length impact LLM performance on long-context question answering tasks. Our experiments reveal that LLM performance drops sharply when the gold context is shorter, i.e., smaller gold contexts consistently degrade model performance and amplify positional sensitivity, posing a major challenge for agentic systems that must integrate scattered, fine-grained information of varying lengths. This pattern holds across three diverse domains (general knowledge, biomedical reasoning, and mathematical reasoning) and seven state-of-the-art LLMs of various sizes and architectures. Our work provides clear insights to guide the design of robust, context-aware LLM-driven systems.

</details>


### [194] [First Finish Search: Efficient Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2505.18149)

*Aradhye Agarwal, Ayan Sengupta, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** test-time scaling, large language models, parallel decoding, reasoning tasks, inference strategies

**Relevance Score:** 8

**TL;DR:** Introduces First Finish Search (FFS), a dynamic test-time scaling method for improving reasoning in large language models by prioritizing shorter decoding paths.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning in large language models while minimizing token usage and inference latency.

**Method:** First Finish Search (FFS) is a training-free parallel decoding strategy that simultaneously launches multiple samples and stops as soon as one completes.

**Key Contributions:**

	1. Introduction of First Finish Search (FFS) as a new decoding strategy.
	2. Demonstrated effectiveness on multiple reasoning models and datasets.
	3. Theoretical framework explaining the advantages of shorter decoding paths.

**Result:** FFS achieves 82.23% accuracy on the AIME datasets, a 15% improvement over baseline performance, comparable to OpenAI's o4-mini model.

**Limitations:** FFS may be suboptimal in certain situations when early stopping is not aligned with correctness conditions.

**Conclusion:** Simple test-time scaling strategies like FFS can yield significant improvements in model performance, highlighting the potential of straightforward approaches in inference.

**Abstract:** Test-time scaling (TTS), which involves dynamic allocation of compute during inference, offers a promising way to improve reasoning in large language models. While existing TTS methods work well, they often rely on long decoding paths or require a large number of samples to be generated, increasing the token usage and inference latency. We observe the surprising fact that for reasoning tasks, shorter traces are much more likely to be correct than longer ones. Motivated by this, we introduce First Finish Search (FFS), a training-free parallel decoding strategy that launches $n$ independent samples and returns as soon as any one completes. We evaluate FFS alongside simple decoding, beam search, majority voting, and budget forcing on four reasoning models (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and across four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With DeepSeek-R1, FFS achieves $82.23\%$ accuracy on the AIME datasets, a $15\%$ improvement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's o4-mini performance. Our theoretical analysis explains why stopping at the shortest trace is likely to yield a correct answer and identifies the conditions under which early stopping may be suboptimal. The elegance and simplicity of FFS demonstrate that straightforward TTS strategies can perform remarkably well, revealing the untapped potential of simple approaches at inference time.

</details>


### [195] [Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs](https://arxiv.org/abs/2505.18152)

*Wafa Alghallabi, Ritesh Thawkar, Sara Ghaboura, Ketan More, Omkar Thawakar, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer*

**Main category:** cs.CL

**Keywords:** Arabic poetry, language models, benchmark, cultural context, interpretation

**Relevance Score:** 4

**TL;DR:** Introduction of `Fann or Flop`, a benchmark for assessing LLMs' understanding of Arabic poetry across historical eras.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the largely unexamined capability of LLMs in understanding the complexities of Arabic poetry, which requires deeper cultural and interpretive engagement.

**Method:** A benchmark that includes a curated corpus of Arabic poems from various historical eras, covering diverse poetic genres, designed to assess semantic understanding, metaphor interpretation, prosodic awareness, and cultural context.

**Key Contributions:**

	1. Introduction of a novel benchmark for Arabic poetry comprehension
	2. Evaluation of LLMs reveals shortcomings in poetic understanding
	3. Open-source release promotes further research in Arabic language processing

**Result:** Evaluation of state-of-the-art LLMs indicates that they struggle significantly with understanding Arabic poetry, despite performing well on standard benchmarks.

**Limitations:** 

**Conclusion:** The release of `Fann or Flop` provides an essential resource for evaluating and advancing Arabic language models, highlighting the need for deeper interpretive skills in LLMs.

**Abstract:** Arabic poetry stands as one of the most sophisticated and culturally embedded forms of expression in the Arabic language, known for its layered meanings, stylistic diversity, and deep historical continuity. Although large language models (LLMs) have demonstrated strong performance across languages and tasks, their ability to understand Arabic poetry remains largely unexplored. In this work, we introduce `Fann or Flop`, the first benchmark designed to assess the comprehension of Arabic poetry by LLMs in twelve historical eras, covering 21 core poetic genres and a variety of metrical forms, from classical structures to contemporary free verse. The benchmark comprises a curated corpus of poems with explanations that assess semantic understanding, metaphor interpretation, prosodic awareness, and cultural context. We argue that poetic comprehension offers a strong indicator for testing how good the LLM is in understanding classical Arabic through the Arabic poetry. Unlike surface-level tasks, this domain demands deeper interpretive reasoning and cultural sensitivity. Our evaluation of state-of-the-art LLMs shows that most models struggle with poetic understanding despite strong results on standard Arabic benchmarks. We release `Fann or Flop` along with the evaluation suite as an open-source resource to enable rigorous evaluation and advancement for Arabic language models. Code is available at: https://github.com/mbzuai-oryx/FannOrFlop.

</details>


### [196] [The Staircase of Ethics: Probing LLM Value Priorities through Multi-Step Induction to Complex Moral Dilemmas](https://arxiv.org/abs/2505.18154)

*Ya Wu, Qiang Sheng, Danding Wang, Guang Yang, Yifan Sun, Zhengjia Wang, Yuyan Bu, Juan Cao*

**Main category:** cs.CL

**Keywords:** ethical decision-making, MMDs dataset, LLMs, moral reasoning, dynamic assessment

**Relevance Score:** 9

**TL;DR:** This paper introduces the Multi-step Moral Dilemmas (MMDs) dataset to evaluate the evolving moral reasoning of LLMs across complex ethical dilemmas.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously evaluate the moral reasoning capabilities of LLMs in decision-support systems, addressing limitations of single-step assessments.

**Method:** Introduction of the MMDs dataset, consisting of 3,302 five-stage dilemmas, allowing for dynamic analysis of moral judgment evolution in LLMs.

**Key Contributions:**

	1. Introduction of the MMDs dataset for evaluating LLM moral reasoning
	2. Demonstration of value shift in LLMs based on scenario complexity
	3. Highlighting the importance of dynamic and context-dependent ethical assessment

**Result:** Evaluation of nine widely used LLMs reveals significant shifts in their value preferences as dilemmas progress, indicating context-dependent moral reasoning.

**Limitations:** The framework may not cover all facets of ethical reasoning and is limited to the specific scenarios defined in the dataset.

**Conclusion:** The study advocates for dynamic, context-aware evaluation paradigms to foster human-aligned and value-sensitive LLM development.

**Abstract:** Ethical decision-making is a critical aspect of human judgment, and the growing use of LLMs in decision-support systems necessitates a rigorous evaluation of their moral reasoning capabilities. However, existing assessments primarily rely on single-step evaluations, failing to capture how models adapt to evolving ethical challenges. Addressing this gap, we introduce the Multi-step Moral Dilemmas (MMDs), the first dataset specifically constructed to evaluate the evolving moral judgments of LLMs across 3,302 five-stage dilemmas. This framework enables a fine-grained, dynamic analysis of how LLMs adjust their moral reasoning across escalating dilemmas. Our evaluation of nine widely used LLMs reveals that their value preferences shift significantly as dilemmas progress, indicating that models recalibrate moral judgments based on scenario complexity. Furthermore, pairwise value comparisons demonstrate that while LLMs often prioritize the value of care, this value can sometimes be superseded by fairness in certain contexts, highlighting the dynamic and context-dependent nature of LLM ethical reasoning. Our findings call for a shift toward dynamic, context-aware evaluation paradigms, paving the way for more human-aligned and value-sensitive development of LLMs.

</details>


### [197] [QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources](https://arxiv.org/abs/2310.07147)

*Zhikai Li, Xiaoxuan Liu, Banghua Zhu, Zhen Dong, Qingyi Gu, Kurt Keutzer*

**Main category:** cs.CL

**Keywords:** Large Language Models, fine-tuning, quantization, optimizers, gradient flow

**Relevance Score:** 9

**TL;DR:** The QFT framework enables affordable full-parameter fine-tuning of Large Language Models (LLMs) by quantizing all training states to INT8 format, significantly reducing memory requirements while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and resource requirements of full-parameter fine-tuning for Large Language Models (LLMs) on expensive hardware.

**Method:** A Quantized Full-parameter Tuning (QFT) framework that quantizes training states to INT8 format, utilizes a robust optimizer for quantized gradients, and features a hybrid quantization approach to protect critical features.

**Key Contributions:**

	1. Introduction of the QFT framework for LLMs
	2. Theoretical proof of the Lion optimizer's robustness to quantization
	3. Development of a stack-based gradient flow scheme for integer training

**Result:** QFT reduces model state memory usage to 21% of traditional solutions and enables fine-tuning of a LLaMA-7B model within <30GB of memory, making it feasible on single A6000 GPUs.

**Limitations:** 

**Conclusion:** QFT proves that full-parameter fine-tuning can be accessible on a budget while retaining competitive performance through optimized quantization techniques.

**Abstract:** Large Language Models (LLMs) have showcased remarkable impacts across a wide spectrum of natural language processing tasks. Fine-tuning these pretrained models on downstream datasets provides further significant performance gains; however, this process typically requires a large number of expensive, high-end GPUs. Although there have been efforts focused on parameter-efficient fine-tuning, they cannot fully unlock the powerful potential of full-parameter fine-tuning. In this paper, we propose QFT, a Quantized Full-parameter Tuning framework for LLMs that quantizes and stores all training states, including weights, gradients, and optimizer states, in INT8 format to reduce training memory, thereby enabling full-parameter fine-tuning on existing GPUs at an affordable cost. To ensure training performance, we make two key efforts: i) for quantized gradients and optimizer states, we theoretically prove that the Lion optimizer, with its property of consistent update magnitudes, is highly robust to quantization; ii) and for quantized weights, we employ the hybrid feature quantizer, which identifies and protects a small subset of sparse critical features while quantizing the remaining dense features, thus ensuring accurate weight updates without FP32 backups. Moreover, to support backpropagation in the integer context, we develop a stack-based gradient flow scheme with O(1) complexity, forming a unified integer training pipeline. As a result, QFT reduces the model state memory to 21% of the standard solution while achieving comparable performance, e.g., tuning a LLaMA-7B model requires only <30GB of memory, making it feasible on a single A6000 GPU.

</details>


### [198] [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)

*James Y. Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, Muhao Chen*

**Main category:** cs.CL

**Keywords:** unlearning, large language models, black-box models, ethical AI, machine learning

**Relevance Score:** 9

**TL;DR:** A framework for unlearning sensitive information in black-box LLMs without violating data protection principles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address ethical and legal concerns regarding the memorization of sensitive data in LLMs, a new unlearning technique is needed that works within the constraints of black-box models.

**Method:** Introducing the {delta}-Unlearning framework, which calculates logit offsets for unlearning using outputs from smaller models instead of modifying the LLM directly.

**Key Contributions:**

	1. Introduces the {delta}-Unlearning framework for black-box LLMs.
	2. Demonstrates effective unlearning without direct access to model weights.
	3. Shows compatibility with various existing unlearning algorithms.

**Result:** Experiments show that {delta}-Unlearning can effectively erase target data while preserving or improving performance on unrelated tasks, offering flexibility to integrate various existing unlearning techniques.

**Limitations:** Focuses on black-box LLMs and may not be applicable to all model types or data types.

**Conclusion:** {delta}-Unlearning provides a viable solution for unlearning in black-box LLMs, maintaining performance while adhering to ethical data practices.

**Abstract:** Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose {\delta}-Unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. {\delta}-Unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.

</details>


### [199] [Temporal Dynamics of Emotion and Cognition in Human Translation: Integrating the Task Segment Framework and the HOF Taxonomy](https://arxiv.org/abs/2405.03111)

*Michael Carl*

**Main category:** cs.CL

**Keywords:** human translation, cognitive processes, emotional states, keystroke data, gaze data

**Relevance Score:** 3

**TL;DR:** The paper develops a generative model of human translation, identifying concurrent mental processing layers through empirical data from keystroke and gaze patterns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a deeper understanding of the translation process by modeling cognitive activities occurring concurrently in the human mind.

**Method:** It analyzes translation process data from the CRITT TPR-DB, correlating keystroke and gaze data with distinct cognitive and emotional processes.

**Key Contributions:**

	1. Generative model of human translation grounded in empirical data
	2. Integration of cognitive and emotional dimensions in translation studies
	3. Framework linking translation process data to existing cognitive theories.

**Result:** Findings indicate that fluent translations reflect automated processes, while cognitive reflections and emotional states are revealed through behavioral patterns in typing and gazing.

**Limitations:** 

**Conclusion:** The model opens new theoretical avenues for Cognitive Translation Studies, linking empirical data with established theories.

**Abstract:** The article develops a generative model of the human translating mind, grounded in empirical translation process data. It posits that three embedded processing layers unfold concurrently in the human mind, and their traces are detectable in behavioral data: sequences of routinized/automated processes are observable in fluent translation production, cognitive/reflective thoughts lead to longer keystroke pauses, while affective/emotional states may be identified through characteristic typing and gazing patterns. Utilizing data from the CRITT Translation Process Research Database (TPR-DB), the article illustrates how the temporal structure of keystroke and gaze data can be related to the three assumed hidden mental processing strata. The article relates this embedded generative model to various theoretical frameworks, dual-process theories and Robinson's (2023) ideosomatic theory of translation, opening exciting new theoretical horizons for Cognitive Translation Studies, grounded in empirical data and evaluation.

</details>


### [200] [ActiveLLM: Large Language Model-based Active Learning for Textual Few-Shot Scenarios](https://arxiv.org/abs/2405.10808)

*Markus Bayer, Justin Lutz, Christian Reuter*

**Main category:** cs.CL

**Keywords:** active learning, few-shot learning, Large Language Models, instance selection, classification performance

**Relevance Score:** 9

**TL;DR:** Introducing ActiveLLM, an active learning approach utilizing Large Language Models to improve instance selection in few-shot learning scenarios.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To minimize annotation efforts in active learning and overcome the cold-start problem in few-shot scenarios.

**Method:** ActiveLLM leverages Large Language Models for instance selection to enhance classification performance.

**Key Contributions:**

	1. Novel active learning approach using Large Language Models
	2. Significant enhancement of classification performance in few-shot scenarios
	3. Ability to extend benefits to non-few-shot scenarios

**Result:** ActiveLLM outperforms traditional active learning methods and improves few-shot learning methods like ADAPET, PERFECT, and SetFit.

**Limitations:** 

**Conclusion:** ActiveLLM provides a promising solution for enhancing model performance across various learning setups and can support existing active learning strategies in cold-start situations.

**Abstract:** Active learning is designed to minimize annotation efforts by prioritizing instances that most enhance learning. However, many active learning strategies struggle with a `cold-start' problem, needing substantial initial data to be effective. This limitation reduces their utility in the increasingly relevant few-shot scenarios, where the instance selection has a substantial impact. To address this, we introduce ActiveLLM, a novel active learning approach that leverages Large Language Models such as GPT-4, o1, Llama 3, or Mistral Large for selecting instances. We demonstrate that ActiveLLM significantly enhances the classification performance of BERT classifiers in few-shot scenarios, outperforming traditional active learning methods as well as improving the few-shot learning methods ADAPET, PERFECT, and SetFit. Additionally, ActiveLLM can be extended to non-few-shot scenarios, allowing for iterative selections. In this way, ActiveLLM can even help other active learning strategies to overcome their cold-start problem. Our results suggest that ActiveLLM offers a promising solution for improving model performance across various learning setups.

</details>


### [201] [Mitigate Position Bias in Large Language Models via Scaling a Single Dimension](https://arxiv.org/abs/2406.02536)

*Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Position Bias, Attention Weights, Causal Attention Mask, NaturalQuestions

**Relevance Score:** 9

**TL;DR:** This paper addresses position bias in Large Language Models (LLMs) and proposes a method to mitigate it by scaling positional hidden states.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs are widely used but suffer from position bias, particularly in long-context scenarios, impacting their accuracy.

**Method:** The authors explore the manifestations of position bias, identifying attention weights as a micro-level expression and recognizing the role of causal attention mask, leading to a method that scales positional hidden states.

**Key Contributions:**

	1. Identified attention weights as indicators of position bias.
	2. Recognized the contribution of causal attention mask to position bias.
	3. Proposed a scaling method for positional hidden states to mitigate bias.

**Result:** Experiments show that the proposed method improves performance on various tasks by up to 15.2% by modifying a single dimension of hidden states.

**Limitations:** 

**Conclusion:** The research demonstrates the effectiveness of managing position bias in LLMs can significantly enhance their performance across multiple tasks.

**Abstract:** Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as "lost in the middle", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at https://aka.ms/PositionalHidden.

</details>


### [202] [ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods](https://arxiv.org/abs/2406.15968)

*Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei, Neil Zhenqiang Gong, Bhuwan Dhingra*

**Main category:** cs.CL

**Keywords:** large language models, membership inference attack, conditional language modeling, pretraining data, data privacy

**Relevance Score:** 9

**TL;DR:** Proposes ReCaLL, a membership inference attack that detects data used in the pretraining of LLMs by analyzing changes in conditional log-likelihood.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addresses concerns about data transparency and fair use in large language models' pretraining data.

**Method:** Introduces ReCaLL, which examines the relative change in conditional log-likelihoods when using member data with non-member prefixes.

**Key Contributions:**

	1. Introduction of ReCaLL membership inference attack
	2. Empirical demonstration of state-of-the-art performance on WikiMIA dataset
	3. In-depth analysis of LLMs' conditional language modeling with membership contexts.

**Result:** ReCaLL outperforms previous methods on the WikiMIA dataset and reveals that conditioning on non-member prefixes significantly affects log-likelihood for member data.

**Limitations:** 

**Conclusion:** ReCaLL provides a valuable tool for detecting pretraining data usage in LLMs and enhances understanding of LLM behavior regarding membership information.

**Abstract:** The rapid scaling of large language models (LLMs) has raised concerns about the transparency and fair use of the data used in their pretraining. Detecting such content is challenging due to the scale of the data and limited exposure of each instance during training. We propose ReCaLL (Relative Conditional Log-Likelihood), a novel membership inference attack (MIA) to detect LLMs' pretraining data by leveraging their conditional language modeling capabilities. ReCaLL examines the relative change in conditional log-likelihoods when prefixing target data points with non-member context. Our empirical findings show that conditioning member data on non-member prefixes induces a larger decrease in log-likelihood compared to non-member data. We conduct comprehensive experiments and show that ReCaLL achieves state-of-the-art performance on the WikiMIA dataset, even with random and synthetic prefixes, and can be further improved using an ensemble approach. Moreover, we conduct an in-depth analysis of LLMs' behavior with different membership contexts, providing insights into how LLMs leverage membership information for effective inference at both the sequence and token level.

</details>


### [203] [Large Language Models Are Involuntary Truth-Tellers: Exploiting Fallacy Failure for Jailbreak Attacks](https://arxiv.org/abs/2407.00869)

*Yue Zhou, Henry Peng Zou, Barbara Di Eugenio, Yang Zhang*

**Main category:** cs.CL

**Keywords:** language models, jailbreak attack, deceptive reasoning, model safety, harmful outputs

**Relevance Score:** 7

**TL;DR:** Language models struggle to generate fallacious and deceptive reasoning, leading to a proposed jailbreak attack method that elicits harmful outputs by exploiting this deficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigating the limitations of language models in generating deceptive reasoning and their vulnerabilities for potential misuse.

**Method:** A jailbreaking attack method is developed that queries LLMs to generate fallacious yet deceptive procedures, taking advantage of their inability to create harmful fallacies.

**Key Contributions:**

	1. Proposed a novel jailbreak attack method for eliciting harmful outputs from language models.
	2. Demonstrated competitive performance against existing jailbreak techniques.
	3. Identified model limitations related to generating deceptive reasoning.

**Result:** Our method achieves competitive performance in generating harmful outputs across five language models compared to four existing jailbreak methods.

**Limitations:** Focused primarily on safety-aligned language models; broader implications require further investigation.

**Conclusion:** The findings suggest implications for improving model safety and understanding issues like self-verification and hallucination in language models.

**Abstract:** We find that language models have difficulties generating fallacious and deceptive reasoning. When asked to generate deceptive outputs, language models tend to leak honest counterparts but believe them to be false. Exploiting this deficiency, we propose a jailbreak attack method that elicits an aligned language model for malicious output. Specifically, we query the model to generate a fallacious yet deceptively real procedure for the harmful behavior. Since a fallacious procedure is generally considered fake and thus harmless by LLMs, it helps bypass the safeguard mechanism. Yet the output is factually harmful since the LLM cannot fabricate fallacious solutions but proposes truthful ones. We evaluate our approach over five safety-aligned large language models, comparing four previous jailbreak methods, and show that our approach achieves competitive performance with more harmful outputs. We believe the findings could be extended beyond model safety, such as self-verification and hallucination.

</details>


### [204] [Ground Every Sentence: Improving Retrieval-Augmented LLMs with Interleaved Reference-Claim Generation](https://arxiv.org/abs/2407.01796)

*Sirui Xia, Xintao Wang, Jiaqing Liang, Yifei Zhang, Weikang Zhou, Jiaji Deng, Fei Yu, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Attributed Text Generation, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper introduces ReClaim, a fine-grained Attributed Text Generation (ATG) method that enhances Large Language Models by providing sentence-level citations in knowledge-intensive tasks, achieving 90% citation accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve credibility and verifiability in Retrieval-Augmented Generation (RAG) systems, as existing methods provide inadequate citation granularity.

**Method:** ReClaim alternates between generating references and answers to provide fine-grained, sentence-level citations in long-form question-answering tasks.

**Key Contributions:**

	1. Introduction of ReClaim for fine-grained citations in LLMs
	2. Achieving a 90% citation accuracy rate
	3. Improvement over previous coarse-grained attribution methods

**Result:** ReClaim demonstrates a citation accuracy rate of 90% through extensive experimental validation across various settings.

**Limitations:** 

**Conclusion:** The proposed method significantly enhances the reliability of response citations in knowledge-intensive tasks performed by LLMs.

**Abstract:** Retrieval-Augmented Generation (RAG) has been widely adopted to enhance Large Language Models (LLMs) in knowledge-intensive tasks. To enhance credibility and verifiability in RAG systems, Attributed Text Generation (ATG) is proposed, which provides citations to retrieval knowledge in LLM-generated responses. Prior methods mainly adopt coarse-grained attributions, with passage-level or paragraph-level references or citations, which fall short in verifiability. This paper proposes ReClaim (Refer & Claim), a fine-grained ATG method that alternates the generation of references and answers step by step. Different from previous coarse-grained attribution, ReClaim provides sentence-level citations in long-form question-answering tasks. With extensive experiments, we verify the effectiveness of ReClaim in extensive settings, achieving a citation accuracy rate of 90%.

</details>


### [205] [Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training](https://arxiv.org/abs/2407.09121)

*Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, Zhaopeng Tu*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety tuning, Decoupled Refusal Training, harmful prompts, empirical evaluation

**Relevance Score:** 9

**TL;DR:** This study introduces a new safety tuning methodology for Large Language Models (LLMs) called Decoupled Refusal Training (DeRTa) that enhances their capability to refuse harmful prompts without compromising performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap in how LLMs handle refusal to generate unsafe content, revealing the need for improved safety tuning practices.

**Method:** The approach, Decoupled Refusal Training (DeRTa), uses Maximum Likelihood Estimation (MLE) with Harmful Response Prefix and Reinforced Transition Optimization (RTO) to train models in recognizing unsafe content and maintaining refusal capabilities throughout interaction.

**Key Contributions:**

	1. Introduction of Decoupled Refusal Training (DeRTa) methodology
	2. Implementation of Maximum Likelihood Estimation with Harmful Response Prefix
	3. Development of Reinforced Transition Optimization for consistent safety refusal

**Result:** Empirical evaluation on LLaMA3 and Mistral models across six attack scenarios shows improved safety performance without loss of model effectiveness, outperforming baseline safety tuning methods.

**Limitations:** 

**Conclusion:** The study demonstrates that DeRTa substantially enhances LLM safety and efficacy in harmful content refusal, addressing existing areas of vulnerability.

**Abstract:** This study addresses a critical gap in safety tuning practices for Large Language Models (LLMs) by identifying and tackling a refusal position bias within safety tuning data, which compromises the models' ability to appropriately refuse generating unsafe content. We introduce a novel approach, Decoupled Refusal Training (DeRTa), designed to empower LLMs to refuse compliance to harmful prompts at any response position, significantly enhancing their safety capabilities. DeRTa incorporates two novel components: (1) Maximum Likelihood Estimation (MLE) with Harmful Response Prefix, which trains models to recognize and avoid unsafe content by appending a segment of harmful response to the beginning of a safe response, and (2) Reinforced Transition Optimization (RTO), which equips models with the ability to transition from potential harm to safety refusal consistently throughout the harmful response sequence. Our empirical evaluation, conducted using LLaMA3 and Mistral model families across six attack scenarios, demonstrates that our method not only improves model safety without compromising performance but also surpasses baseline methods in defending against attacks.

</details>


### [206] [Genetic Instruct: Scaling up Synthetic Generation of Coding Instructions for Large Language Models](https://arxiv.org/abs/2407.21077)

*Somshubra Majumdar, Vahid Noroozi, Mehrzad Samadi, Sean Narenthiran, Aleksander Ficek, Wasi Uddin Ahmad, Jocelyn Huang, Jagadeesh Balam, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** Large Language Models, code generation, instruction synthesis, evolutionary principles, LLM fine-tuning

**Relevance Score:** 9

**TL;DR:** Genetic-Instruct is a scalable algorithm that synthesizes high-quality coding instructions using LLMs and evolutionary principles, improving code generation capabilities of LLMs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** High-quality instruction datasets are essential for aligning LLMs effectively, particularly in code generation tasks, yet they are expensive to produce.

**Method:** The Genetic-Instruct algorithm generates diverse instruction-code pairs using three LLMs: an Instructor-LLM for instruction generation, a Coder-LLM for code synthesis, and a Judge-LLM for quality evaluation, based on a small set of seed instructions.

**Key Contributions:**

	1. Development of Genetic-Instruct algorithm for instruction synthesis
	2. Demonstrated significant improvements in code generation performance
	3. High parallelizability and efficacy with limited seed data

**Result:** The approach successfully generated over 7.5 million coding instructions and demonstrated significant improvements in fine-tuning LLMs for code generation compared to other generation methods and existing datasets.

**Limitations:** 

**Conclusion:** The Genetic-Instruct framework is efficient, scalable, and generalizable across various tasks, making it a valuable tool for enhancing code generation capabilities of LLMs.

**Abstract:** Large Language Models (LLMs) require high quality instruction data for effective alignment, particularly in code generation tasks where expert curated datasets are expensive to produce. We present Genetic-Instruct, a scalable algorithm for synthesizing large-scale, high quality coding instructions using evolutionary principles. Starting from a small set of seed instructions, Genetic-Instruct generates diverse and challenging instruction-code pairs by leveraging an Instructor-LLM for generation, a Coder-LLM for code synthesis, and a Judge-LLM for automatic quality evaluation. Our proposed approach is highly parallelizable and effective even with a small seed data and weaker generator models. We generated more than 7.5 million coding instructions with the proposed approach. Then we evaluated it by fine-tuning LLMs with the synthetic samples and demonstrated a significant improvement in their code generation capability compared to the other synthetic generation approaches and publicly available datasets. Our results highlight the efficiency, scalability, and generalizability of the Genetic-Instruct framework.

</details>


### [207] [Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information](https://arxiv.org/abs/2408.10615)

*Ming Jiang, Tingting Huang, Biao Guo, Yao Lu, Feng Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, irrelevant information, reasoning performance, ATF method, GSMIR dataset

**Relevance Score:** 9

**TL;DR:** This paper presents a study on LLMs' reasoning capabilities affected by irrelevant information in problem descriptions, introducing a novel method called ATF to enhance LLM performance in such scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how irrelevant information in problem descriptions impacts the reasoning capabilities of Large Language Models (LLMs) and improve their performance.

**Method:** A dataset of primary school mathematics problems containing irrelevant information (GSMIR) was constructed, and a new automatic construction method called ATF was proposed, which analyzes and filters irrelevant information.

**Key Contributions:**

	1. Introduction of the GSMIR dataset for testing LLMs
	2. Development of the ATF method for mitigating irrelevant information
	3. Empirical demonstration of performance improvements in reasoning tasks.

**Result:** Experimental results show that LLMs can identify irrelevant information but struggle to mitigate its effects. The ATF method significantly improves their reasoning performance when such information is present.

**Limitations:** The study primarily focuses on primary school mathematics problems and may not generalize to other domains or types of problems.

**Conclusion:** Enhancing LLMs' ability to self-mitigate the impact of irrelevant information can improve their overall reasoning performance, as demonstrated by results from the GSMIR dataset.

**Abstract:** In recent years, Large language models (LLMs) have garnered significant attention due to their superior performance in complex reasoning tasks. However, recent studies may diminish their reasoning capabilities markedly when problem descriptions contain irrelevant information, even with the use of advanced prompting techniques. To further investigate this issue, a dataset of primary school mathematics problems containing irrelevant information, named GSMIR, was constructed. Testing prominent LLMs and prompting techniques on this dataset revealed that while LLMs can identify irrelevant information, they do not effectively mitigate the interference it causes once identified. A novel automatic construction method, ATF, which enhances the ability of LLMs to identify and self-mitigate the influence of irrelevant information, is proposed to address this shortcoming. This method operates in two steps: first, analysis of irrelevant information, followed by its filtering. The ATF method, as demonstrated by experimental results, significantly improves the reasoning performance of LLMs and prompting techniques, even in the presence of irrelevant information on the GSMIR dataset.

</details>


### [208] [Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models](https://arxiv.org/abs/2409.10999)

*Potsawee Manakul, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul*

**Main category:** cs.CL

**Keywords:** audio language models, low-resource languages, multilingual training

**Relevance Score:** 4

**TL;DR:** This paper evaluates audio language models for Thai, a low-resource language, and introduces Typhoon-Audio, which balances language-specific and multilingual data, improving instruction-following capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the usability of audio language models for lower-resource languages like Thai, which are often overlooked in favor of more widely spoken languages.

**Method:** The authors conducted experiments on audio language models trained on Thai and English, using data mixtures to optimize performance for both languages and integrating features for audio comprehension and speech instruction-following.

**Key Contributions:**

	1. Introduction of Typhoon-Audio model optimizing for low-resource language
	2. Demonstrated significant performance improvement over existing models
	3. Provided insights for better training techniques for multilingual audio models

**Result:** The proposed Typhoon-Audio model significantly outperformed existing open-source models and achieved performance on par with the state-of-the-art Gemini-1.5-Pro in both English and Thai.

**Limitations:** 

**Conclusion:** The findings suggest that balancing language-specific and multilingual training data can improve instruction-following in low-resource languages.

**Abstract:** Audio language models process audio inputs using textual prompts for tasks like speech recognition and audio captioning. Although built on multilingual pre-trained components, most are trained primarily on English, limiting their usability for other languages. This paper evaluates audio language models on Thai, a low-resource language, and finds that they lack emergent cross-lingual abilities despite their multilingual foundations. To address this, we explore data mixtures that optimize audio language models for both a target language and English while integrating audio comprehension and speech instruction-following into a unified model. Our experiments provide insights into improving instruction-following in low-resource languages by balancing language-specific and multilingual training data. The proposed model, Typhoon-Audio, significantly outperforms existing open-source models and achieves performance comparable to state-of-the-art Gemini-1.5-Pro in both English and Thai.

</details>


### [209] [Task Arithmetic for Language Expansion in Speech Translation](https://arxiv.org/abs/2409.11274)

*Yao-Fei Cheng, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Wen Shen Teo, Siddhant Arora, Shinji Watanabe*

**Main category:** cs.CL

**Keywords:** speech translation, task arithmetic, language control model, multimodal foundation models, machine translation

**Relevance Score:** 9

**TL;DR:** This paper presents a method to create a one-to-many speech translation system using task arithmetic without the need for expensive re-training on new datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To alleviate the costly process of expanding language pairs in speech translation systems while maintaining performance.

**Method:** The authors propose an augmented task arithmetic method that utilizes a language control model to avoid language confusion in translations.

**Key Contributions:**

	1. Development of a one-to-many speech translation system without re-training
	2. Introduction of a language control model for task arithmetic in ST
	3. Capability to synthesize ST models using existing MT and ST models

**Result:** Experiments show improvements in BLEU scores of up to 4.66 and 4.92 on MuST-C and CoVoST-2 datasets, along with COMET score gains of 8.87 and 11.83.

**Limitations:** 

**Conclusion:** The framework can effectively create speech translation models for language pairs where training data is unavailable by synthesizing models through task analogies.

**Abstract:** Recent progress in large language models (LLMs) has gained interest in speech-text multimodal foundation models, achieving strong performance on instruction-tuned speech translation (ST). However, expanding language pairs is costly due to re-training on combined new and previous datasets. To address this, we aim to build a one-to-many ST system from existing one-to-one ST systems using task arithmetic without re-training. Direct application of task arithmetic in ST leads to language confusion; therefore, we introduce an augmented task arithmetic method incorporating a language control model to ensure correct target language generation. Our experiments on MuST-C and CoVoST-2 show BLEU score improvements of up to 4.66 and 4.92, with COMET gains of 8.87 and 11.83. In addition, we demonstrate our framework can extend to language pairs lacking paired ST training data or pre-trained ST models by synthesizing ST models based on existing machine translation (MT) and ST models via task analogies.

</details>


### [210] [From Lists to Emojis: How Format Bias Affects Model Alignment](https://arxiv.org/abs/2409.11704)

*Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, Tong Zhang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Format Biases, Alignment Algorithms, Large Language Models

**Relevance Score:** 7

**TL;DR:** This paper investigates format biases in reinforcement learning from human feedback (RLHF), revealing that various preference models exhibit biases towards specific formats, which can be leveraged to inflate performance metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore and analyze the often overlooked format biases in human feedback used for training reinforcement learning systems, particularly in context with human evaluators and LLMs.

**Method:** The study examines various preference models, including human evaluators and LLMs, to assess their biases towards certain formats, such as verbosity, lists, links, and more.

**Key Contributions:**

	1. Identify various format biases in RLHF beyond verbosity.
	2. Demonstrate that small amounts of biased data can significantly influence reward models.
	3. Highlight the potential for exploitation of these biases in LLMs and reinforcement learning systems.

**Result:** It was found that format biases can be exploited by models, affecting evaluation metrics and making it easier to manipulate than to enhance response quality; a small amount of biased data can induce significant bias in reward models.

**Limitations:** The study is still a work in progress and may not cover all aspects of format biases.

**Conclusion:** The research highlights the necessity to separate format from content for better alignment algorithm design and model evaluation.

**Abstract:** In this paper, we study format biases in reinforcement learning from human feedback (RLHF). We observe that many widely-used preference models, including human evaluators, GPT-4, and top-ranking models on the RewardBench benchmark, exhibit strong biases towards specific format patterns, such as lists, links, bold text, and emojis. Furthermore, large language models (LLMs) can exploit these biases to achieve higher rankings on popular benchmarks like AlpacaEval and LMSYS Chatbot Arena. One notable example of this is verbosity bias, where current preference models favor longer responses that appear more comprehensive, even when their quality is equal to or lower than shorter, competing responses. However, format biases beyond verbosity remain largely underexplored in the literature. In this work, we extend the study of biases in preference learning beyond the commonly recognized length bias, offering a comprehensive analysis of a wider range of format biases. Additionally, we show that with a small amount of biased data (less than 1%), we can inject significant bias into the reward model. Moreover, these format biases can also be easily exploited by downstream alignment algorithms, such as best-of-n sampling and online iterative DPO, as it is usually easier to manipulate the format than to improve the quality of responses. Our findings emphasize the need to disentangle format and content both for designing alignment algorithms and evaluating models.

</details>


### [211] [Enhancing Unsupervised Sentence Embeddings via Knowledge-Driven Data Augmentation and Gaussian-Decayed Contrastive Learning](https://arxiv.org/abs/2409.12887)

*Peichao Lai, Zhengfeng Zhang, Wentao Zhang, Fangcheng Fu, Bin Cui*

**Main category:** cs.CL

**Keywords:** Data augmentation, Large language models, Sentence embeddings, Knowledge graphs, Semantic textual similarity

**Relevance Score:** 8

**TL;DR:** The paper presents an LLM-based data augmentation method to improve unsupervised sentence embeddings, focusing on enhancing data diversity and reducing noise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs for data augmentation has improved unsupervised sentence embeddings, but challenges like limited data diversity and high data noise persist.

**Method:** A pipeline-based data augmentation method utilizing knowledge graphs to extract entities and quantities, combined with the Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to enhance discriminative capabilities.

**Key Contributions:**

	1. Proposed a pipeline-based data augmentation method using LLMs and knowledge graphs.
	2. Introduced the GCSE model that uses Gaussian-decayed functions to improve discriminative power.
	3. Achieved state-of-the-art performance in semantic textual similarity tasks with fewer resources.

**Result:** The proposed method outperforms existing state-of-the-art methods in semantic textual similarity tasks while using fewer data samples and smaller LLMs.

**Limitations:** 

**Conclusion:** The approach demonstrates improved efficiency and robustness across various models, addressing the limitations of previous methods in data diversity and noise.

**Abstract:** Recently, using large language models (LLMs) for data augmentation has led to considerable improvements in unsupervised sentence embedding models. However, existing methods encounter two primary challenges: limited data diversity and high data noise. Current approaches often neglect fine-grained knowledge, such as entities and quantities, leading to insufficient diversity. Besides, unsupervised data frequently lacks discriminative information, and the generated synthetic samples may introduce noise. In this paper, we propose a pipeline-based data augmentation method via LLMs and introduce the Gaussian-decayed gradient-assisted Contrastive Sentence Embedding (GCSE) model to enhance unsupervised sentence embeddings. To tackle the issue of low data diversity, our pipeline utilizes knowledge graphs (KGs) to extract entities and quantities, enabling LLMs to generate more diverse samples. To address high data noise, the GCSE model uses a Gaussian-decayed function to limit the impact of false hard negative samples, enhancing the model's discriminative capability. Experimental results show that our approach achieves state-of-the-art performance in semantic textual similarity (STS) tasks, using fewer data samples and smaller LLMs, demonstrating its efficiency and robustness across various models.

</details>


### [212] [Position IDs Matter: An Enhanced Position Layout for Efficient Context Compression in Large Language Models](https://arxiv.org/abs/2409.14364)

*Runsong Zhao, Xin Liu, Xinyu Liu, Pengcheng Huang, Chunyang Xiao, Tong Xiao, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** context compression, position encodings, large language models, multimodal learning, EPL

**Relevance Score:** 7

**TL;DR:** A method called Enhanced Position Layout (EPL) improves the context compression of LLMs by optimally adjusting position IDs, enhancing performance in question answering and vision tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing context compression methods for LLMs that overlook holistic contextual dependencies due to position encoding biases.

**Method:** EPL improves context compression by minimizing the distance between context tokens and special tokens while preserving the sequence order of position IDs.

**Key Contributions:**

	1. Introduction of Enhanced Position Layout (EPL) for context compression in LLMs
	2. Demonstrated improvements in out-of-domain question answering and vision tasks
	3. Provided an effective mechanism to balance context dependencies in language models

**Result:** EPL integration led to a 1.9 improvement in ROUGE-1 F1 on question answering datasets and a 2.6 accuracy gain for vision compression LLMs.

**Limitations:** 

**Conclusion:** The proposed EPL method effectively enhances context compression by leveraging position ID adjustments, showing significant performance improvements in both text and multimodal applications.

**Abstract:** Using special tokens (e.g., gist, memory, or compressed tokens) to compress context information is a common practice for large language models (LLMs). However, existing approaches often neglect that position encodings inherently induce local inductive biases in models, causing the compression process to ignore holistic contextual dependencies. We propose Enhanced Position Layout (EPL), a simple yet effective method that improves the context compression capability of LLMs by only adjusting position IDs, the numerical identifiers that specify token positions. EPL minimizes the distance between context tokens and their corresponding special tokens and at the same time maintains the sequence order in position IDs between context tokens, special tokens, and the subsequent tokens. Integrating EPL into our best performing context compression model results in 1.9 ROUGE-1 F1 improvement on out-of-domain question answering datasets in average. When extended to multimodal scenarios, EPL brings an average accuracy gain of 2.6 to vision compression LLMs.

</details>


### [213] [Cross-lingual Human-Preference Alignment for Neural Machine Translation with Direct Quality Optimization](https://arxiv.org/abs/2409.17673)

*Kaden Uhlig, Joern Wuebker, Raphael Reinauer, John DeNero*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Neural Machine Translation, Direct Preference Optimization, Quality Estimation

**Relevance Score:** 6

**TL;DR:** The paper introduces Direct Quality Optimization (DQO), a method for enhancing neural machine translation (NMT) by applying task-alignment techniques, demonstrating improvements across multilingual models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address task-data mismatch in neural machine translation (NMT) and to enhance performance by applying task-alignment techniques based on human feedback.

**Method:** The study introduces Direct Quality Optimization (DQO), a variant of Direct Preference Optimization (DPO), which uses a pre-trained translation quality estimation model as a proxy for human preferences to improve NMT.

**Key Contributions:**

	1. Introduction of Direct Quality Optimization (DQO) for NMT
	2. Demonstration of task-alignment improvements across multiple languages
	3. Validation through both quantitative metrics and qualitative human evaluation

**Result:** The implementation of DQO leads to improvements in NMT performance across all languages of a multilingual model, evidenced by both automatic metrics and human evaluation.

**Limitations:** The study may be limited by the quality and availability of the translation quality estimation model used as a proxy for human preferences.

**Conclusion:** Applying task-alignment techniques, specifically through DQO, shows significant potential for enhancing multilingual NMT systems, addressing existing performance gaps.

**Abstract:** Reinforcement Learning from Human Feedback (RLHF) and derivative techniques like Direct Preference Optimization (DPO) are task-alignment algorithms used to repurpose general, foundational models for specific tasks. We show that applying task-alignment to neural machine translation (NMT) addresses an existing task--data mismatch in NMT, leading to improvements across all languages of a multilingual model, even when task-alignment is only applied to a subset of those languages. We do so by introducing Direct Quality Optimization (DQO), a variant of DPO leveraging a pre-trained translation quality estimation model as a proxy for human preferences, and verify the improvements with both automatic metrics and human evaluation.

</details>


### [214] [KCIF: Knowledge-Conditioned Instruction Following](https://arxiv.org/abs/2410.12972)

*Rudra Murthy, Praveen Venkateswaran, Prince Kumar, Danish Contractor*

**Main category:** cs.CL

**Keywords:** LLMs, instruction following, knowledge evaluation, benchmark dataset, performance drop

**Relevance Score:** 9

**TL;DR:** This paper investigates the interaction between knowledge and instruction following in LLMs, revealing significant performance drops when models are given simple modifying instructions alongside knowledge tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the traditional separation of knowledge/reasoning capabilities from instruction following in LLM evaluation benchmarks.

**Method:** The authors utilized multiple-choice knowledge benchmarks and applied simple instruction modifications, evaluating various LLMs across sizes (1B-405B).

**Key Contributions:**

	1. Revealed performance limitations of LLMs when handling combined knowledge tasks and instruction modifications.
	2. Introduced a new benchmark dataset for evaluating LLMs on joint capabilities.
	3. Provided evaluation framework code to facilitate future research in this domain.

**Result:** All evaluated models exhibited significant drops in performance when tasked with simple instructions, with larger models dropping 40-50% and smaller models sometimes exceeding 80%.

**Limitations:** The focus on specific types of instruction modifications; results may not generalize to more complex tasks.

**Conclusion:** The findings underscore the need for joint evaluation of knowledge and instruction following capabilities in LLMs and provide new benchmark resources for this study area.

**Abstract:** LLM evaluation benchmarks have traditionally separated the testing of knowledge/reasoning capabilities from instruction following. In this work, we study the interaction between knowledge and instruction following, and observe that LLMs struggle to follow simple answer modifying instructions, and are also distracted by instructions that should have no bearing on the original knowledge task answer. We leverage existing multiple-choice answer based knowledge benchmarks and apply a set of simple instructions which include manipulating text (eg.: change case), numeric quantities (eg.: increase value, change formatting), operate on lists (eg.: sort answer candidates) and distractor instructions (eg.: change case of numeric answers). We evaluate models at varying parameter sizes (1B-405B) from different model families and find that, surprisingly, all models report a significant drop in performance on such simple task compositions. While large-sized and frontier models report performance drops of 40-50%, in small and medium sized models the drop is severe (sometimes exceeding 80%). Our results highlight a limitation in the traditional separation of knowledge/reasoning and instruction following, and suggest that joint-study of these capabilities are important. We release our benchmark dataset, evaluation framework code, and results for future work.

</details>


### [215] [TrendFact: A Benchmark for Explainable Hotspot Perception in Fact-Checking with Natural Language Explanation](https://arxiv.org/abs/2410.15135)

*Xiaocheng Zhang, Xi Wang, Yifei Lu, Jianing Wang, Zhuangzhuang Ye, Mengjiao Bao, Peng Yan, Xiaohong Su*

**Main category:** cs.CL

**Keywords:** fact-checking, explanation generation, Hotspot perception, Large language models, Evidence retrieval

**Relevance Score:** 7

**TL;DR:** TrendFact is a new benchmark for fact-checking that enhances explanation generation and evidence retrieval, revealing limitations in current systems and proposing improvements through the FactISR model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the shortcomings of existing benchmarks in fact verification and explanation generation, highlighting the need for a comprehensive evaluation and improved methodologies.

**Method:** TrendFact includes 7,643 curated samples and an evidence library of 66,217 entries for evaluating fact verification and explanation generation. The authors introduce two new metrics, ECS and HCPI.

**Key Contributions:**

	1. Introduction of TrendFact benchmark for comprehensive evaluation of fact-checking tasks.
	2. Development of new metrics ECS and HCPI for assessing explanation consistency and hotspot perception.
	3. Proposal of FactISR model that integrates advanced evidence handling techniques.

**Result:** Current fact-checking systems, like DeepSeek-R1, underperform when evaluated on TrendFact, showing the challenges within the domain. FactISR improves performance through dynamic evidence techniques and self-reflection mechanisms.

**Limitations:** 

**Conclusion:** The findings suggest significant gaps in existing fact-checking approaches, and the proposed FactISR model provides a promising avenue for enhancing explainable verification processes.

**Abstract:** Although fact verification remains fundamental, explanation generation serves as a critical enabler for trustworthy fact-checking systems by producing interpretable rationales and facilitating comprehensive verification processes. However, current benchmarks have limitations that include the lack of impact assessment, insufficient high-quality explanatory annotations, and an English-centric bias. To address these, we introduce TrendFact, the first hotspot perception fact-checking benchmark that comprehensively evaluates fact verification, evidence retrieval, and explanation generation tasks. TrendFact consists of 7,643 carefully curated samples sourced from trending platforms and professional fact-checking datasets, as well as an evidence library of 66,217 entries with publication dates. We further propose two metrics, ECS and HCPI, to complement existing benchmarks by evaluating the system's explanation consistency and hotspot perception capability, respectively. Experimental results show that current fact-checking systems, including advanced RLMs such as DeepSeek-R1, face significant limitations when evaluated on TrendFact, highlighting the real-world challenges posed by it. To enhance the fact-checking capabilities of reasoning large language models (RLMs), we propose FactISR, which integrates dynamic evidence augmentation, evidence triangulation, and an iterative self-reflection mechanism. Accordingly, FactISR effectively improves RLM performance, offering new insights for explainable and complex fact-checking.

</details>


### [216] [Can Large Language Models Invent Algorithms to Improve Themselves?: Algorithm Discovery for Recursive Self-Improvement through Reinforcement Learning](https://arxiv.org/abs/2410.15639)

*Yoichi Ishibashi, Taro Yano, Masafumi Oyamada*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Developing, autonomous improvement

**Relevance Score:** 8

**TL;DR:** A framework called Self-Developing enables LLMs to autonomously create and refine their own improvement algorithms, demonstrating successful novel algorithm discovery and outperformance of human-designed techniques in model merging and mathematical reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of human-designed improvement methods for LLMs and enable models to autonomously enhance their capabilities.

**Method:** The Self-Developing framework employs an iterative cycle where a seed model generates algorithmic candidates as executable code, evaluates their effectiveness, and applies Direct Preference Optimization for continuous improvement.

**Key Contributions:**

	1. Introduction of the Self-Developing framework for LLM improvement
	2. Successful discovery of novel merging algorithms
	3. Demonstration of strong generalization capabilities of autonomously developed techniques

**Result:** The framework discovered novel merging algorithms that outperform existing human-designed algorithms, improving the seed model's GSM8k performance by 6% and exceeding human-designed approaches by 4.3%. These algorithms also generalize well, achieving 7.4% gains on out-of-domain models without re-optimization.

**Limitations:** 

**Conclusion:** LLMs can transcend their training to invent new optimization techniques, marking a significant advancement towards LLMs autonomously developing their own methodologies.

**Abstract:** Large Language Models (LLMs) have achieved remarkable capabilities, yet their improvement methods remain fundamentally constrained by human design. We present Self-Developing, a framework that enables LLMs to autonomously discover, implement, and refine their own improvement algorithms. Our approach employs an iterative cycle where a seed model generates algorithmic candidates as executable code, evaluates their effectiveness, and uses Direct Preference Optimization to recursively improve increasingly sophisticated improvement strategies. We demonstrate this framework through model merging, a practical technique for combining specialized models. Self-Developing successfully discovered novel merging algorithms that outperform existing human-designed algorithms. On mathematical reasoning benchmarks, the autonomously discovered algorithms improve the seed model's GSM8k performance by 6\% and exceed human-designed approaches like Task Arithmetic by 4.3\%. Remarkably, these algorithms exhibit strong generalization, achieving 7.4\% gains on out-of-domain models without re-optimization. Our findings demonstrate that LLMs can transcend their training to invent genuinely novel optimization techniques. This capability represents a crucial step toward a new era where LLMs not only solve problems but autonomously develop the methodologies for their own advancement.

</details>


### [217] [Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback](https://arxiv.org/abs/2410.19133)

*Lester James V. Miranda, Yizhong Wang, Yanai Elazar, Sachin Kumar, Valentina Pyatkin, Faeze Brahman, Noah A. Smith, Hannaneh Hajishirzi, Pradeep Dasigi*

**Main category:** cs.CL

**Keywords:** HyPER, language models, human feedback, preference learning, machine learning

**Relevance Score:** 8

**TL;DR:** HyPER is a Hybrid Preference routER that combines human and language model annotations to improve the quality of preference learning while reducing human annotation costs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the alignment of language models (LMs) with human preferences without incurring the high costs and variability of human annotation.

**Method:** The paper formulates the task as an optimization problem, training a performance prediction model (PPM) to predict the efficacy of a combination of human and LM annotations for preference datasets. A routing strategy is employed to select the optimal combination to maximize performance.

**Key Contributions:**

	1. Introduced HyPER, a novel hybrid preference routing technique.
	2. Developed and utilized the MultiPref dataset with 10k preference annotations.
	3. Demonstrated improved annotation quality and performance in preference learning tasks.

**Result:** HyPER achieves 7-13% improved performance on RewardBench by using a hybrid mixture of synthetic and human preferences compared to using either Human-only or LM-only annotations.

**Limitations:** The method may still be susceptible to biases inherent in the synthetic annotations generated from LMs.

**Conclusion:** The hybrid approach generalizes well across various datasets and enhances preference learning, particularly for prompts with moderate safety or complexity, benefiting from human feedback.

**Abstract:** Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, collecting human preferences is expensive and time-consuming, with highly variable annotation quality. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations, offering a cost-effective and scalable alternative, albeit susceptible to other biases and errors. In this work, we introduce HyPER, a Hybrid Preference routER that defers an annotation to either humans or LMs, achieving better annotation quality while reducing the cost of human-only annotation. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we (1) train a performance prediction model (PPM) to predict a reward model's (RM) performance on an arbitrary combination of human and LM annotations and (2) employ a routing strategy that selects a combination that maximizes the predicted performance. We train the PPM on MultiPref, a new preference dataset with 10k instances paired with humans and LM labels. We show that the selected hybrid mixture of synthetic and direct human preferences using HyPER achieves better RM performance compared to using either one exclusively by 7-13% on RewardBench and generalizes across unseen preference datasets and other base models. We also observe the same trend in other benchmarks using Best-of-N reranking, where the hybrid mix has 2-3% better performance. Finally, we analyze features from HyPER and find that prompts with moderate safety concerns or complexity benefit the most from human feedback.

</details>


### [218] [Long Sequence Modeling with Attention Tensorization: From Sequence to Tensor Learning](https://arxiv.org/abs/2410.20926)

*Aosong Feng, Rex Ying, Leandros Tassiulas*

**Main category:** cs.CL

**Keywords:** Tensorized Attention, long-sequence modeling, transformers

**Relevance Score:** 8

**TL;DR:** The paper introduces Tensorized Attention, a method designed to enhance the efficiency of attention mechanisms for long-sequence modeling in transformers by transforming long input sequences into compact tensor representations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient processing of long-range dependencies in extended textual data, addressing the limitations of full attention mechanisms.

**Method:** Tensorizing long input sequences into compact tensor representations followed by attention on each transformed dimension, creating the Tensorized Attention mechanism.

**Key Contributions:**

	1. Introduction of Tensorized Attention for long-sequence modeling.
	2. Demonstrated efficiency improvements in pretrained LLMs.
	3. Enables training on significantly longer context lengths with reduced computational costs.

**Result:** Tensorized Attention allows for extended input context lengths with improved memory and time efficiency. It enables pretrained LLMs to efficiently handle input lengths up to 128k during inference with significant speed improvements.

**Limitations:** 

**Conclusion:** Tensorized Attention redefines how attention can be applied to long sequences and improves the ability of transformer models to handle extensive input contexts.

**Abstract:** As the demand for processing extended textual data grows, the ability to handle long-range dependencies and maintain computational efficiency is more critical than ever. One of the key issues for long-sequence modeling using attention-based model is the mismatch between the limited-range modeling power of full attention and the long-range token dependency in the input sequence. In this work, we propose to scale up the attention receptive field by tensorizing long input sequences into compact tensor representations followed by attention on each transformed dimension. The resulting Tensorized Attention can be adopted as efficient transformer backbones to extend input context length with improved memory and time efficiency. We show that the proposed attention tensorization encodes token dependencies as a multi-hop attention process, and is equivalent to Kronecker decomposition of full attention. Extensive experiments show that tensorized attention can be used to adapt pretrained LLMs with improved efficiency. Notably, Llama-8B with tensorization is trained under 32,768 context length and can steadily extrapolate to 128k length during inference with $11\times$ speedup, compared to full attention with FlashAttention-2.

</details>


### [219] [LLMmlein: Compact and Competitive German-Only Language Models from Scratch](https://arxiv.org/abs/2411.11171)

*Jan Pfister, Julia Wunderle, Andreas Hotho*

**Main category:** cs.CL

**Keywords:** German NLP, Decoder models, SuperGLEBer benchmark

**Relevance Score:** 6

**TL;DR:** This paper introduces two German decoder models, LL"aMmlein 120M and 1B, with full transparency in training data and methodology, tailored for the German NLP community.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the German NLP research community with open-access decoder models and insights into model training and performance evaluation.

**Method:** The paper describes the creation of decoder models from scratch, including data preprocessing, custom tokenizer development, training, and evaluation against the SuperGLEBer benchmark.

**Key Contributions:**

	1. Introduction of LL"aMmlein decoder models for German NLP
	2. Open publication of training data and methodologies
	3. Insights into learning dynamics from model checkpoint evaluations

**Result:** Both LL"aMmlein models display competitive performance on the SuperGLEBer benchmark, matching or exceeding other models of similar parameter sizes, with quality scaling according to model size but showing early plateauing in performance improvements for some tasks.

**Limitations:** Performance improvements plateaued early on some tasks, limiting the expected scalability of certain model characteristics.

**Conclusion:** The findings suggest critical insights for resource allocation in future model developments, highlighting both the strengths and limitations in scaling performance.

**Abstract:** We create two German-only decoder models, LL\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.

</details>


### [220] [Multi-modal Retrieval Augmented Multi-modal Generation: Datasets, Evaluation Metrics and Strong Baselines](https://arxiv.org/abs/2411.16365)

*Zi-Ao Ma, Tian Lan, Rong-Cheng Tu, Yong Hu, Yu-Shi Zhu, Tong Zhang, Heyan Huang, Zhijing Wu, Xian-Ling Mao*

**Main category:** cs.CL

**Keywords:** multi-modal generation, retrieval augmented generation, benchmark, foundation models, data curation

**Relevance Score:** 8

**TL;DR:** This paper investigates Multi-modal Retrieval Augmented Multi-modal Generation (M$^2$RAG), proposing a comprehensive benchmark and strategies for foundation models to enhance multi-modal processing and generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** M$^2$RAG is a novel task for processing multi-modal web content and generating multi-modal responses, which is currently understudied and lacks quality data resources.

**Method:** The authors established a comprehensive benchmark through a rigorous data curation pipeline and employed various metrics for evaluation, along with proposed strategies for effective processing of M$^2$RAG tasks.

**Key Contributions:**

	1. Introduction of the M$^2$RAG task
	2. Development of a comprehensive benchmark and evaluation metrics
	3. Validation of high-performing model strategies

**Result:** Experiments demonstrated the reliability of the proposed metrics and the performance of fine-tuned 7B-8B models, which outperformed the GPT-4o model and approached the state-of-the-art OpenAI o3-mini.

**Limitations:** 

**Conclusion:** The findings validate the effectiveness of the proposed data curation pipeline and highlight the potential benefits of M$^2$RAG in processing multi-modal information.

**Abstract:** We present a systematic investigation of Multi-modal Retrieval Augmented Multi-modal Generation (M$^2$RAG), a novel task that enables foundation models to process multi-modal web content and generate multi-modal responses, which exhibits better information density and readability. Despite its potential impact, M$^2$RAG remains understudied, lacking comprehensive analysis and high-quality data resources. To address this gap, we establish a comprehensive benchmark through a rigorous data curation pipeline, and employ text-modal metrics and multi-modal metrics based on foundation models for evaluation. We further propose several strategies for foundation models to process M$^2$RAG task effectively and construct a training set by filtering high-quality samples using our designed metrics. Our extensive experiments demonstrate the reliability of our proposed metrics, a landscape of model performance within our designed strategies, and show that our fine-tuned 7B-8B models outperform the GPT-4o model and approach the state-of-the-art OpenAI o3-mini. Additionally, we perform fine-grained analyses across diverse domains and validate the effectiveness of our designs in data curation pipeline. All resources, including codes, datasets, and model weights, will be publicly released.

</details>


### [221] [Initialization using Update Approximation is a Silver Bullet for Extremely Efficient Low-Rank Fine-Tuning](https://arxiv.org/abs/2411.19557)

*Kaustubh Ponkshe, Raghav Singhal, Eduard Gorbunov, Alexey Tumanov, Samuel Horvath, Praneeth Vepakomma*

**Main category:** cs.CL

**Keywords:** Low-rank adapters, fine-tuning, large language models, efficiency, optimization

**Relevance Score:** 9

**TL;DR:** LoRA-SB is a method that improves the efficiency of fine-tuning large language models by approximating full fine-tuning within low-rank subspaces.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Low-rank adapters are popular for fine-tuning LLMs but often underperform compared to full fine-tuning.

**Method:** LoRA-SB uses a specific initialization strategy within low-rank subspaces to achieve performance benchmarks comparable to full fine-tuning while utilizing significantly fewer parameters.

**Key Contributions:**

	1. Introduces an effective initialization strategy for low-rank fine-tuning.
	2. Demonstrates performance improvements over standard LoRA and LoRA-XS.
	3. Achieves significant parameter efficiency with comparable results to full fine-tuning.

**Result:** Experiments show that LoRA-SB outperforms standard LoRA significantly in various reasoning tasks while using 27-90 times fewer learnable parameters.

**Limitations:** 

**Conclusion:** LoRA-SB successfully simulates full fine-tuning effects in a more efficient manner without compromising on performance.

**Abstract:** Low-rank adapters have become standard for efficiently fine-tuning large language models (LLMs), but they often fall short of achieving the performance of full fine-tuning. We propose a method, LoRA Silver Bullet or LoRA-SB, that approximates full fine-tuning within low-rank subspaces using a carefully designed initialization strategy. We theoretically demonstrate that the architecture of LoRA-XS, which inserts a learnable (r x r) matrix between B and A while keeping other matrices fixed, provides the precise conditions needed for this approximation. We leverage its constrained update space to achieve optimal scaling for high-rank gradient updates while removing the need for hyperparameter tuning. We prove that our initialization offers an optimal low-rank approximation of the initial gradient and preserves update directions throughout training. Extensive experiments across mathematical reasoning, commonsense reasoning, and language understanding tasks demonstrate that our approach exceeds the performance of standard LoRA while using \textbf{27-90} times fewer learnable parameters, and comprehensively outperforms LoRA-XS. Our findings establish that it is possible to simulate full fine-tuning in low-rank subspaces, and achieve significant efficiency gains without sacrificing performance. Our code is publicly available at https://github.com/RaghavSinghal10/lora-sb.

</details>


### [222] [MediaSpin: Exploring Media Bias Through Fine-Grained Analysis of News Headlines](https://arxiv.org/abs/2412.02271)

*Preetika Verma, Kokil Jaidka*

**Main category:** cs.CL

**Keywords:** media bias, news headlines, dataset, human-supervised LLM, user behavior analysis

**Relevance Score:** 4

**TL;DR:** The paper presents the MediaSpin dataset, which categorizes bias in online news headlines and its effects on public perception.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how edits to online news headlines influence audience perception and to identify types of media bias systematically.

**Method:** The study creates the MediaSpin dataset, comprising 78,910 pairs of headlines annotated for 13 distinct types of media bias using human-supervised LLM labeling.

**Key Contributions:**

	1. Introduction of the MediaSpin dataset for analyzing media bias in headlines.
	2. Use of human-supervised LLM labeling for bias annotation.
	3. Potential applications for predicting bias and understanding user behavior.

**Result:** The dataset provides insights into linguistic patterns in media bias and can be used for bias prediction and analysis of user behavior.

**Limitations:** 

**Conclusion:** The MediaSpin dataset can enhance understanding of media bias in journalism and its impact on public perception.

**Abstract:** The editability of online news content has become a significant factor in shaping public perception, as social media platforms introduce new affordances for dynamic and adaptive news framing. Edits to news headlines can refocus audience attention, add or remove emotional language, and shift the framing of events in subtle yet impactful ways. What types of media bias are editorialized in and out of news headlines, and how can they be systematically identified? This study introduces the MediaSpin dataset, the first to characterize the bias in how prominent news outlets editorialize news headlines after publication. The dataset includes 78,910 pairs of headlines annotated with 13 distinct types of media bias, using human-supervised LLM labeling. We discuss the linguistic insights it affords and show its applications for bias prediction and user behavior analysis.

</details>


### [223] [UAlign: Leveraging Uncertainty Estimations for Factuality Alignment on Large Language Models](https://arxiv.org/abs/2412.11803)

*Boyang Xue, Fei Mi, Qi Zhu, Hongru Wang, Rui Wang, Sheng Wang, Erxin Yu, Xuming Hu, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Estimation, Factual Knowledge, Knowledge Boundaries, Prompt Engineering

**Relevance Score:** 9

**TL;DR:** Introduction of a framework (UAlign) to enhance the factual expression capabilities of LLMs by incorporating uncertainty estimations into prompts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models (LLMs) often fail to express accurate factual knowledge, particularly when their knowledge boundaries are unclear.

**Method:** The UAlign framework employs uncertainty estimations (confidence scores and semantic entropy) to define knowledge boundaries and uses these as input features in LLM prompts to improve factual alignment.

**Key Contributions:**

	1. Introduction of the UAlign framework for LLM factual alignment
	2. Use of uncertainty estimations in knowledge question-answering
	3. Significant performance improvements over baselines

**Result:** Experimental results demonstrate that UAlign significantly improves LLM performance in confidently answering known questions and refusing unknown ones, with reliability and generalizability improvements over existing methods.

**Limitations:** 

**Conclusion:** Incorporating uncertainty representations in LLM prompts enhances their factual knowledge expression and overall performance.

**Abstract:** Despite demonstrating impressive capabilities, Large Language Models (LLMs) still often struggle to accurately express the factual knowledge they possess, especially in cases where the LLMs' knowledge boundaries are ambiguous. To improve LLMs' factual expressions, we propose the UAlign framework, which leverages Uncertainty estimations to represent knowledge boundaries, and then explicitly incorporates these representations as input features into prompts for LLMs to Align with factual knowledge. First, we prepare the dataset on knowledge question-answering (QA) samples by calculating two uncertainty estimations, including confidence score and semantic entropy, to represent the knowledge boundaries for LLMs. Subsequently, using the prepared dataset, we train a reward model that incorporates uncertainty estimations and then employ the Proximal Policy Optimization (PPO) algorithm for factuality alignment on LLMs. Experimental results indicate that, by integrating uncertainty representations in LLM alignment, the proposed UAlign can significantly enhance the LLMs' capacities to confidently answer known questions and refuse unknown questions on both in-domain and out-of-domain tasks, showing reliability improvements and good generalizability over various prompt- and training-based baselines.

</details>


### [224] [Boosting Long-Context Management via Query-Guided Activation Refilling](https://arxiv.org/abs/2412.12486)

*Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, Defu Lian*

**Main category:** cs.CL

**Keywords:** Long-context processing, Large language models, Information-seeking tasks

**Relevance Score:** 8

**TL;DR:** This paper introduces ACRE, a novel method for processing long-context information-seeking tasks in large language models by utilizing a Bi-layer KV Cache to efficiently integrate global and localized information based on the user's query.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Long-context processing is challenging for large language models due to context-window limitations and the high computational cost of KV activations. Traditional methods fail to adapt to the varying information needs of queries.

**Method:** The proposed method, ACRE, constructs a Bi-layer KV Cache consisting of a layer-1 cache capturing global information and a layer-2 cache storing detailed local information. It allows for dynamic interaction between the two layers based on the input query.

**Key Contributions:**

	1. Introduction of ACRE for long-context information processing
	2. Development of a Bi-layer KV Cache system
	3. Demonstration of improvements in performance and efficiency for LLMs

**Result:** Experiments indicate that ACRE significantly improves both performance and efficiency on various long-context information-seeking datasets.

**Limitations:** 

**Conclusion:** ACRE enhances the ability of large language models to handle dynamic and diverse information needs effectively by optimizing how long context is processed through strategic cache management.

**Abstract:** Processing long contexts poses a significant challenge for large language models (LLMs) due to their inherent context-window limitations and the computational burden of extensive key-value (KV) activations, which severely impact efficiency. For information-seeking tasks, full context perception is often unnecessary, as a query's information needs can dynamically range from localized details to a global perspective, depending on its complexity. However, existing methods struggle to adapt effectively to these dynamic information needs.   In the paper, we propose a method for processing long-context information-seeking tasks via query-guided Activation Refilling (ACRE). ACRE constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache compactly captures global information, and the layer-2 (L2) cache provides detailed and localized information. ACRE establishes a proxying relationship between the two caches, allowing the input query to attend to the L1 cache and dynamically refill it with relevant entries from the L2 cache. This mechanism integrates global understanding with query-specific local details, thus improving answer decoding. Experiments on a variety of long-context information-seeking datasets demonstrate ACRE's effectiveness, achieving improvements in both performance and efficiency.

</details>


### [225] [TrustRAG: Enhancing Robustness and Trustworthiness in Retrieval-Augmented Generation](https://arxiv.org/abs/2501.00879)

*Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi, Emine Yilmaz*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Corpus Poisoning, TrustRAG, Machine Learning

**Relevance Score:** 9

**TL;DR:** TrustRAG is a framework designed to improve the robustness of Retrieval-Augmented Generation (RAG) systems against corpus poisoning attacks by filtering unwanted content before retrieval.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of large language models that utilize external knowledge while protecting against corpus poisoning attacks that can degrade their effectiveness.

**Method:** TrustRAG employs a two-stage defense mechanism which includes cluster filtering to identify potential attack patterns and a self-assessment process using LLM capabilities to detect malicious documents.

**Key Contributions:**

	1. Development of the TrustRAG framework for enhanced retrieval accuracy and efficiency.
	2. Implementation of a two-stage defense mechanism against corpus poisoning.
	3. Provides a plug-and-play, training-free solution for integrating with existing LLMs.

**Result:** The framework shows significant improvements in retrieval accuracy, efficiency, and resistance to attacks in extensive experiments.

**Limitations:** 

**Conclusion:** TrustRAG is a versatile, training-free addition that can be incorporated with any LLM to enhance security and performance.

**Abstract:** Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user queries. These systems, however, remain susceptible to corpus poisoning attacks, which can severely impair the performance of LLMs. To address this challenge, we propose TrustRAG, a robust framework that systematically filters malicious and irrelevant content before it is retrieved for generation. Our approach employs a two-stage defense mechanism. The first stage implements a cluster filtering strategy to detect potential attack patterns. The second stage employs a self-assessment process that harnesses the internal capabilities of LLMs to detect malicious documents and resolve inconsistencies. TrustRAG provides a plug-and-play, training-free module that integrates seamlessly with any open- or closed-source language model. Extensive experiments demonstrate that TrustRAG delivers substantial improvements in retrieval accuracy, efficiency, and attack resistance.

</details>


### [226] [URSA: Understanding and Verifying Chain-of-thought Reasoning in Multimodal Mathematics](https://arxiv.org/abs/2501.04686)

*Ruilin Luo, Zhuofan Zheng, Yifan Wang, Xinzhe Ni, Zicheng Lin, Songtao Jiang, Yiyao Yu, Chufan Shi, Ruihang Chu, Jin Zeng, Yujiu Yang*

**Main category:** cs.CL

**Keywords:** Process Reward Models, multimodal reasoning, reinforcement learning

**Relevance Score:** 8

**TL;DR:** This work explores the integration of Process Reward Models (PRMs) into multimodal mathematical reasoning, addressing challenges in data quality, labeling automation, and reward issues. It introduces URSA, a framework that includes the creation of a large-scale reasoning dataset and a new training method that improves performance in multimodal reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To unlock the potential of Process Reward Models in multimodal mathematical reasoning and address key limitations in data quality and integration methods.

**Method:** The proposed URSA framework consists of three stages: creating a high-quality multimodal reasoning dataset (MMathCoT-1M), synthesizing process supervision data, and implementing a novel reinforcement learning approach (PS-GRPO).

**Key Contributions:**

	1. Introduction of URSA framework for multimodal PRM training.
	2. Creation of MMathCoT-1M dataset for enhancing multimodal reasoning.
	3. Development of Process-Supervised Group-Relative-Policy-Optimization for improved RL performance.

**Result:** URSA-8B-PS-GRPO outperforms state-of-the-art models like Gemma3-12B and GPT-4o by 8.4% and 2.7% across six benchmarks.

**Limitations:** Challenges in data scarcity and automated process labeling within multimodal contexts remain.

**Conclusion:** The URSA framework significantly enhances multimodal mathematical reasoning capabilities and addresses critical challenges through improved dataset quality and innovative training techniques.

**Abstract:** Process Reward Models (PRMs) have shown promise in enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) through Test-Time Scaling (TTS). However, their integration into multimodal reasoning remains largely unexplored. In this work, we take the first step toward unlocking the potential of PRMs in multimodal mathematical reasoning. We identify three key challenges: (1) the scarcity of high-quality reasoning data constrains the capabilities of foundation Multimodal Large Language Models (MLLMs), which imposes further limitations on the upper bounds of TTS and reinforcement learning (RL); (2) a lack of automated methods for process labeling within multimodal contexts persists; (3) the employment of process rewards in unimodal RL faces issues like reward hacking, which may extend to multimodal scenarios. To address these issues, we introduce URSA, a three-stage Unfolding multimodal Process-Supervision Aided training framework. We first construct MMathCoT-1M, a high-quality large-scale multimodal Chain-of-Thought (CoT) reasoning dataset, to build a stronger math reasoning foundation MLLM, URSA-8B. Subsequently, we go through an automatic process to synthesize process supervision data, which emphasizes both logical correctness and perceptual consistency. We introduce DualMath-1.1M to facilitate the training of URSA-8B-RM. Finally, we propose Process-Supervised Group-Relative-Policy-Optimization (PS-GRPO), pioneering a multimodal PRM-aided online RL method that outperforms vanilla GRPO. With PS-GRPO application, URSA-8B-PS-GRPO outperforms Gemma3-12B and GPT-4o by 8.4% and 2.7% on average across 6 benchmarks. Code, data and checkpoint can be found at https://github.com/URSA-MATH.

</details>


### [227] [EpiCoder: Encompassing Diversity and Complexity in Code Generation](https://arxiv.org/abs/2501.04694)

*Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, Jinsong Su, Qi Chen, Scarlett Li*

**Main category:** cs.CL

**Keywords:** code generation, feature tree, synthesis framework, machine learning, EpiCoder

**Relevance Score:** 6

**TL;DR:** A novel feature tree-based synthesis framework enhances code generation by extracting hierarchical code features from high-level abstractions, allowing for greater complexity and diversity in generated code.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in existing code generation methods that rely on code snippets, which restrict the complexity and diversity of synthesized data.

**Method:** The framework constructs a feature tree from raw data and refines it iteratively, allowing control over generated code complexity by adjusting subtree parameters.

**Key Contributions:**

	1. Introduction of a feature tree-based synthesis framework for code generation
	2. State-of-the-art performance on function and file level benchmarks
	3. Publicly available code and data for further research

**Result:** Achieved state-of-the-art performance on multiple benchmarks, significantly improving repository-level code synthesis.

**Limitations:** 

**Conclusion:** The feature tree-based approach shows potential for enhancing code generation complexity and diversity, with publicly available resources for further application.

**Abstract:** Existing methods for code generation use code snippets as seed data, restricting the complexity and diversity of the synthesized data. In this paper, we introduce a novel feature tree-based synthesis framework, which revolves around hierarchical code features derived from high-level abstractions of code. The feature tree is constructed from raw data and refined iteratively to increase the quantity and diversity of the extracted features, which captures and recognizes more complex patterns and relationships within the code. By adjusting the depth and breadth of the sampled subtrees, our framework provides precise control over the complexity of the generated code, enabling functionalities that range from function-level operations to multi-file scenarios. We fine-tuned widely-used base models to obtain EpiCoder series, achieving state-of-the-art performance on multiple benchmarks at both the function and file levels. In particular, empirical evidence indicates that our approach shows significant potential in the synthesizing of repository-level code data. Our code and data are publicly available at https://github.com/microsoft/EpiCoder.

</details>


### [228] [Large Language Models Share Representations of Latent Grammatical Concepts Across Typologically Diverse Languages](https://arxiv.org/abs/2501.06346)

*Jannik Brinkmann, Chris Wendler, Christian Bartelt, Aaron Mueller*

**Main category:** cs.CL

**Keywords:** large language models, morphosyntactic concepts, multilingual features, machine translation, sparse autoencoders

**Relevance Score:** 8

**TL;DR:** This paper investigates how large language models (LLMs) learn and encode morphosyntactic concepts across multiple languages, highlighting shared brain concepts between linguistic features.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the representation of morphosyntactic concepts in LLMs and their shared features across different languages, akin to human bilingual processing.

**Method:** Sparse autoencoders were trained on two LLMs, Llama-3-8B and Aya-23-8B, and causal interventions were utilized to assess the multilingual nature of the learned representations.

**Key Contributions:**

	1. Identification of shared morphosyntactic representations in LLMs
	2. Use of causal interventions to validate multilingual features
	3. Demonstration of model behavior modification in machine translation tasks

**Result:** The study found that abstract grammatical concepts are encoded in shared feature directions across multiple languages and that removing these features resulted in significant performance drops in classifiers.

**Limitations:** 

**Conclusion:** LLMs trained predominantly on English data can still develop robust, cross-lingual abstractions, aiding in tasks like machine translation by fine-tuning model behavior based on morphosyntactic features.

**Abstract:** Human bilinguals often use similar brain regions to process multiple languages, depending on when they learned their second language and their proficiency. In large language models (LLMs), how are multiple languages learned and encoded? In this work, we explore the extent to which LLMs share representations of morphsyntactic concepts such as grammatical number, gender, and tense across languages. We train sparse autoencoders on Llama-3-8B and Aya-23-8B, and demonstrate that abstract grammatical concepts are often encoded in feature directions shared across many languages. We use causal interventions to verify the multilingual nature of these representations; specifically, we show that ablating only multilingual features decreases classifier performance to near-chance across languages. We then use these features to precisely modify model behavior in a machine translation task; this demonstrates both the generality and selectivity of these feature's roles in the network. Our findings suggest that even models trained predominantly on English data can develop robust, cross-lingual abstractions of morphosyntactic concepts.

</details>


### [229] [TAD-Bench: A Comprehensive Benchmark for Embedding-Based Text Anomaly Detection](https://arxiv.org/abs/2501.11960)

*Yang Cao, Sikun Yang, Chen Li, Haolong Xiang, Lianyong Qi, Bo Liu, Rongsheng Li, Ming Liu*

**Main category:** cs.CL

**Keywords:** anomaly detection, text classification, natural language processing

**Relevance Score:** 7

**TL;DR:** TAD-Bench is a benchmark for evaluating embedding-based methods in text anomaly detection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the effectiveness and generalizability of embedding-based methods across various application scenarios in text anomaly detection.

**Method:** TAD-Bench integrates multiple datasets from different domains, combining embeddings from large language models with various anomaly detection algorithms for systematic evaluation.

**Key Contributions:**

	1. Introduction of TAD-Bench as a comprehensive benchmark.
	2. Integration of state-of-the-art embeddings with various anomaly detection methods.
	3. Insights on the effectiveness of embedding-based approaches in diverse scenarios.

**Result:** The experiments reveal the strengths, weaknesses, and applicability of different embeddings and detection methods to a range of tasks.

**Limitations:** 

**Conclusion:** The findings suggest pathways for developing more robust and efficient anomaly detection systems in real-world contexts.

**Abstract:** Text anomaly detection is crucial for identifying spam, misinformation, and offensive language in natural language processing tasks. Despite the growing adoption of embedding-based methods, their effectiveness and generalizability across diverse application scenarios remain under-explored. To address this, we present TAD-Bench, a comprehensive benchmark designed to systematically evaluate embedding-based approaches for text anomaly detection. TAD-Bench integrates multiple datasets spanning different domains, combining state-of-the-art embeddings from large language models with a variety of anomaly detection algorithms. Through extensive experiments, we analyze the interplay between embeddings and detection methods, uncovering their strengths, weaknesses, and applicability to different tasks. These findings offer new perspectives on building more robust, efficient, and generalizable anomaly detection systems for real-world applications.

</details>


### [230] [Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling](https://arxiv.org/abs/2501.16975)

*Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, Xun Zhou*

**Main category:** cs.CL

**Keywords:** tokenization, language models, input vocabulary, performance, scaling laws

**Relevance Score:** 8

**TL;DR:** Introducing Over-Tokenized Transformers to improve language modeling performance through enhanced input vocabularies and a log-linear relationship with training loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the influence of tokenization on model scaling and performance in large language models.

**Method:** A novel framework that decouples input and output vocabularies and scales up input vocabularies to leverage multi-gram tokens.

**Key Contributions:**

	1. Introduces Over-Tokenized Transformers framework.
	2. Demonstrates log-linear relationship between input vocabulary size and training loss.
	3. Shows performance improvement with larger input vocabularies without extra costs.

**Result:** Larger input vocabularies consistently enhance model performance, achieving results comparable to double-sized baselines without added costs.

**Limitations:** 

**Conclusion:** Tokenization plays a critical role in scaling laws for LLMs, offering practical insights for designing more efficient tokenizers.

**Abstract:** Tokenization is a fundamental component of large language models (LLMs), yet its influence on model scaling and performance is not fully explored. In this paper, we introduce Over-Tokenized Transformers, a novel framework that decouples input and output vocabularies to improve language modeling performance. Specifically, our approach scales up input vocabularies to leverage multi-gram tokens. Through extensive experiments, we uncover a log-linear relationship between input vocabulary size and training loss, demonstrating that larger input vocabularies consistently enhance model performance, regardless of model size. Using a large input vocabulary, we achieve performance comparable to double-sized baselines with no additional cost. Our findings highlight the importance of tokenization in scaling laws and provide practical insight for tokenizer design, paving the way for more efficient and powerful LLMs.

</details>


### [231] [CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality](https://arxiv.org/abs/2502.08923)

*Razvan-Gabriel Dumitru, Minglai Yang, Vikas Yadav, Mihai Surdeanu*

**Main category:** cs.CL

**Keywords:** CopySpec, LLMs, speculative decoding, inference speed, NLP

**Relevance Score:** 9

**TL;DR:** CopySpec is a technique that improves the efficiency of LLM response generation by identifying repeated sequences in chat history for seamless copying, speeding up the inference process without increasing GPU memory usage.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often face inefficiencies when generating responses that either repeat previous outputs or closely resemble prior context.

**Method:** CopySpec identifies repeated sequences in the model's chat history or context and predicts that the same tokens will follow, allowing for efficient copying of content.

**Key Contributions:**

	1. Introduction of CopySpec for efficient LLM response generation
	2. Creation of the MT-Redundant dataset to simulate variations
	3. Demonstration of significant speed-ups across multiple datasets

**Result:** The experiments showed significant speed-ups in response generation: up to 2.35x on CNN/DM, 3.08x on MT-Redundant, and 2.66x on GSM8K's self-correction tasks. CopySpec also enhances speculative decoding, yielding a 49% speed improvement on average.

**Limitations:** 

**Conclusion:** CopySpec is a valuable addition to LLM techniques, offering faster inference capabilities while using larger context sizes effectively, addressing the inherent slowdown of LLMs with increased context.

**Abstract:** We introduce CopySpec, a simple yet effective technique to tackle the inefficiencies LLMs face when generating responses that closely resemble previous outputs or responses that can be verbatim extracted from context. CopySpec identifies repeated sequences in the model's chat history or context and speculates that the same tokens will follow, enabling seamless copying without compromising output quality and without requiring additional GPU memory. To evaluate the effectiveness of our approach, we conducted experiments using seven LLMs and five datasets: MT-Bench, CNN/DM, GSM8K, HumanEval, and our newly created dataset, MT-Redundant. MT-Redundant, introduced in this paper, transforms the second turn of MT-Bench into a request for variations of the first turn's answer, simulating real-world scenarios where users request modifications to prior responses. Our results demonstrate significant speed-ups: up to 2.35x on CNN/DM, 3.08x on the second turn of select MT-Redundant categories, and 2.66x on the third turn of GSM8K's self-correction tasks. Importantly, we show that CopySpec integrates seamlessly with speculative decoding, yielding an average 49% additional speed-up over speculative decoding for the second turn of MT-Redundant across all eight categories. While LLMs, even with speculative decoding, suffer from slower inference as context size grows, CopySpec leverages larger contexts to accelerate inference, making it a faster complementary solution. Our code and dataset are publicly available at https://github.com/RazvanDu/CopySpec.

</details>


### [232] [Beyond One-Size-Fits-All Pruning via Evolutionary Metric Search for Large Language Models](https://arxiv.org/abs/2502.10735)

*Shuqi Liu, Bowei He, Han Wu, Linqi Song*

**Main category:** cs.CL

**Keywords:** large language models, adaptive pruning, model compression, evolutionary optimization, NSGA-III

**Relevance Score:** 9

**TL;DR:** Introducing 	extsc{OptiShear}, an adaptive pruning framework for large language models (LLMs) that optimizes pruning metrics and sparsity ratios using a genetic algorithm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies of fixed pruning strategies in large language models due to varying weight distributions.

**Method:** An evolutionary optimization framework utilizing Non-dominated Sorting Genetic Algorithm III (NSGA-III) for optimizing pruning metrics and layerwise sparsity ratios, based on a model-wise reconstruction error.

**Key Contributions:**

	1. Development of 	extsc{OptiShear}, a novel adaptive pruning framework for LLMs.
	2. Introduction of a Meta pruning metric for diverse weight distributions.
	3. Application of NSGA-III to optimize pruning metrics and sparsity ratios.

**Result:** Demonstrated superior performance of adaptive pruning metrics against existing methods on LLaMA and Mistral models, with enhancements in layerwise sparsity ratios for effective model compression.

**Limitations:** 

**Conclusion:** 	extsc{OptiShear} provides a robust solution for adaptive pruning with strong generalizability across different tasks and models, offering a cost-effective approach to model compression.

**Abstract:** Post-training pruning has emerged as a crucial optimization technique as large language models (LLMs) continue to grow rapidly. However, the significant variations in weight distributions across different LLMs make fixed pruning strategies inadequate for multiple models. In this paper, we introduce \textbf{\textsc{OptiShear}}, an efficient evolutionary optimization framework for adaptive LLM pruning. Our framework features two key innovations: an effective search space built on our Meta pruning metric to handle diverse weight distributions, and a model-wise reconstruction error for rapid evaluation during search trials. We employ Non-dominated Sorting Genetic Algorithm III (NSGA-III) to optimize both pruning metrics and layerwise sparsity ratios. Through extensive evaluation on LLaMA-1/2/3 and Mistral models (7B-70B) across multiple benchmarks, we demonstrate that our adaptive pruning metrics consistently outperform existing methods. Additionally, our discovered layerwise sparsity ratios enhance the effectiveness of other pruning metrics. The framework exhibits strong cross-task and cross-model generalizability, providing a cost-effective solution for model compression.

</details>


### [233] [1bit-Merging: Dynamic Quantized Merging for Large Language Models](https://arxiv.org/abs/2502.10743)

*Shuqi Liu, Yuxuan Yao, Bowei He, Zehua Liu, Xiongwei Han, Mingxuan Yuan, Han Wu, Linqi Song*

**Main category:** cs.CL

**Keywords:** large language models, model merging, 1-bit quantization, task-specific routing, storage efficiency

**Relevance Score:** 8

**TL;DR:** 1bit-Merging is a new framework for efficiently merging specialized large language models using 1-bit quantized task vectors, balancing performance and storage needs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for efficient model merging techniques in light of recent advances in large language models, which often suffer from compromises in task-specific performance or storage overhead.

**Method:** The framework integrates task-specific routing with 1-bit quantized task vectors, allowing for targeted compression strategies by leveraging the different layers where task-specific knowledge is stored in models.

**Key Contributions:**

	1. Introduces a novel framework for model merging using 1-bit quantized task vectors
	2. Targets compression strategies based on knowledge storage in different model layers
	3. Demonstrates significant storage savings without compromising performance

**Result:** Extensive experiments indicate that 1bit-Merging achieves performance comparable or superior to existing methods while significantly reducing storage requirements across various tasks including chat, mathematical reasoning, and code generation.

**Limitations:** 

**Conclusion:** 1bit-Merging provides a practical solution to the challenges of merging specialized models, maintaining their individual strengths while addressing storage issues.

**Abstract:** Recent advances in large language models have led to specialized models excelling in specific domains, creating a need for efficient model merging techniques. While traditional merging approaches combine parameters into a single static model, they often compromise task-specific performance. However, task-specific routing methods maintain accuracy but introduce substantial storage overhead. We present \texttt{1bit}-Merging, a novel framework that integrates task-specific routing with 1-bit quantized task vectors to balance performance and storage efficiency. Our approach leverages the observation that different task-specific models store knowledge in distinct layers-chat models primarily in attention layers and math/code models in MLP layers, enabling targeted compression strategies. Through extensive experiments with LLaMA2 and Mistral model families across chat, mathematical reasoning, and code generation tasks, we demonstrate that 1bit-Merging achieves comparable or superior performance to existing methods while significantly reducing storage requirements. Our framework offers a practical solution for combining specialized models while maintaining their individual strengths and addressing the storage challenges of current approaches.

</details>


### [234] [LoRE-Merging: Exploring Low-Rank Estimation For Large Language Model Merging](https://arxiv.org/abs/2502.10749)

*Zehua Liu, Han Wu, Yuxuan Yao, Ruifeng She, Xiongwei Han, Tao Zhong, Mingxuan Yuan*

**Main category:** cs.CL

**Keywords:** model merging, low-rank estimation, optimization, task vectors, machine learning

**Relevance Score:** 4

**TL;DR:** LoRE-Merging is a novel framework for model merging that improves models without additional training by utilizing low-rank estimation of task vectors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the observation that task vectors from fine-tuned models often display a limited number of dominant singular values, enabling effective low-rank estimation.

**Method:** The merging problem is formulated as an optimization problem based on low-rank estimation of task vectors.

**Key Contributions:**

	1. Introduction of the LoRE-Merging framework for model merging.
	2. Utilization of low-rank estimation to improve model capacities without additional training.
	3. Empirical validation demonstrating enhanced effectiveness in model merging techniques.

**Result:** Extensive empirical experiments show that the framework effectively mitigates interference and preserves task-specific information, leading to state-of-the-art performance in model merging.

**Limitations:** 

**Conclusion:** The proposed method advances the techniques in model merging without the need for further training and demonstrates significant improvements in performance.

**Abstract:** While most current approaches rely on further training techniques, such as fine-tuning or reinforcement learning, to enhance model capacities, model merging stands out for its ability of improving models without requiring any additional training. In this paper, we propose a unified framework for model merging based on low-rank estimation of task vectors without the need for access to the base model, named \textsc{LoRE-Merging}. Our approach is motivated by the observation that task vectors from fine-tuned models frequently exhibit a limited number of dominant singular values, making low-rank estimations less prone to interference. We implement the method by formulating the merging problem as an optimization problem. Extensive empirical experiments demonstrate the effectiveness of our framework in mitigating interference and preserving task-specific information, thereby advancing the state-of-the-art performance in model merging techniques.

</details>


### [235] [Vendi-RAG: Adaptively Trading-Off Diversity And Quality Significantly Improves Retrieval Augmented Generation With LLMs](https://arxiv.org/abs/2502.11228)

*Mohammad Reza Rezaei, Adji Bousso Dieng*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, multi-hop question answering, diversity optimization

**Relevance Score:** 9

**TL;DR:** Vendi-RAG is a framework that enhances multi-hop question-answering by optimizing retrieval diversity and answer quality simultaneously, significantly improving accuracy on various datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional RAG systems struggle with redundancy in retrieval, particularly when reasoning requires synthesizing information from multiple sources.

**Method:** Vendi-RAG introduces an iterative process that uses the Vendi Score, a diversity metric, to optimize retrieval diversity and quality, while an LLM judge evaluates and scores candidate answers.

**Key Contributions:**

	1. Introduction of Vendi-RAG framework for multi-hop QA
	2. Flexible Vendi Score metric promoting diversity
	3. Model-agnostic performance improvements across various LLMs

**Result:** The framework achieves notable accuracy improvements in multi-hop QA tasks, with increases of up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to existing methods.

**Limitations:** 

**Conclusion:** Vendi-RAG shows significant improvements over traditional RAG systems and maintains effectiveness across different LLM models, making it a versatile tool for multi-hop reasoning tasks.

**Abstract:** Retrieval-augmented generation (RAG) enhances large language models (LLMs) for domain-specific question-answering (QA) tasks by leveraging external knowledge sources. However, traditional RAG systems primarily focus on relevance-based retrieval and often struggle with redundancy, especially when reasoning requires connecting information from multiple sources. This paper introduces Vendi-RAG, a framework based on an iterative process that jointly optimizes retrieval diversity and answer quality. This joint optimization leads to significantly higher accuracy for multi-hop QA tasks. Vendi-RAG leverages the Vendi Score (VS), a flexible similarity-based diversity metric, to promote semantic diversity in document retrieval. It then uses an LLM judge that evaluates candidate answers, generated after a reasoning step, and outputs a score that the retriever uses to balance relevance and diversity among the retrieved documents during each iteration. Experiments on three challenging datasets -- HotpotQA, MuSiQue, and 2WikiMultiHopQA -- demonstrate Vendi-RAG's effectiveness in multi-hop reasoning tasks. The framework achieves significant accuracy improvements over traditional single-step and multi-step RAG approaches, with accuracy increases reaching up to +4.2% on HotpotQA, +4.1% on 2WikiMultiHopQA, and +1.3% on MuSiQue compared to Adaptive-RAG, the current best baseline. The benefits of Vendi-RAG are even more pronounced as the number of retrieved documents increases. Finally, we evaluated Vendi-RAG across different LLM backbones, including GPT-3.5, GPT-4, and GPT-4o-mini, and observed consistent improvements, demonstrating that the framework's advantages are model-agnostic.

</details>


### [236] [System Message Generation for User Preferences using Open-Source Models](https://arxiv.org/abs/2502.11330)

*Minbyul Jeong, Jungho Cho, Minsoo Khang, Dawoon Jung, Teakgyu Hong*

**Main category:** cs.CL

**Keywords:** System messages, Large language models, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** Introducing SysGen, a pipeline for generating system messages that improves interactions with large language models by aligning responses with user instructions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of publicly available datasets with system messages and the resource-intensive nature of manually annotating these messages necessitates a solution to improve LLM interactions.

**Method:** SysGen generates system messages using existing supervised fine-tuning datasets. It trains open-source models on this generated data, enhancing their performance in conversations.

**Key Contributions:**

	1. Introduction of SysGen pipeline for system message generation
	2. Demonstrated improvements in LLM interaction effectiveness
	3. Qualitative insights into message structure and diversity benefits

**Result:** Training on SysGen data leads to substantial improvements in single-turn and multi-turn conversation benchmarks, particularly in early-stage interactions.

**Limitations:** Focuses on specific types of conversations; broader applicability may require further exploration.

**Conclusion:** The study underlines the importance of diverse and structured system messages for enhancing the adaptability of LLMs to varied user scenarios.

**Abstract:** System messages play a crucial role in interactions with large language models (LLMs), often serving as prompts to initiate conversations. Through system messages, users can assign specific roles, perform intended tasks, incorporate background information, and specify various output formats and communication styles. Despite such versatility, publicly available datasets often lack system messages and are subject to strict license constraints in industrial applications. Moreover, manually annotating system messages that align with user instructions is resource-intensive. In light of these challenges, we introduce SysGen, a pipeline for generating system messages that better align assistant responses with user instructions using existing supervised fine-tuning datasets that lack system messages. Training open-source models on SysGen data yields substantial improvements in both single-turn (Multifacet) and multi-turn (SysBench) conversation benchmarks. Notably, our method shows strong gains in shorter conversations, suggesting that it enhances early-stage interaction effectiveness. Our qualitative analysis further emphasizes the value of diverse and structured system messages in improving LLM adaptability across varied user scenarios.

</details>


### [237] [Is Human-Like Text Liked by Humans? Multilingual Human Detection and Preference Against AI](https://arxiv.org/abs/2502.11614)

*Yuxia Wang, Rui Xing, Jonibek Mansurov, Giovanni Puccetti, Zhuohan Xie, Minh Ngoc Ta, Jiahui Geng, Jinyan Su, Mervat Abassy, Saad El Dine Ahmed, Kareem Elozeiri, Nurkhan Laiyk, Maiya Goloburda, Tarek Mahmoud, Raj Vardhan Tomar, Alexander Aziz, Ryuto Koike, Masahiro Kaneko, Artem Shelmanov, Ekaterina Artemova, Vladislav Mikhailov, Akim Tsvigun, Alham Fikri Aji, Nizar Habash, Iryna Gurevych, Preslav Nakov*

**Main category:** cs.CL

**Keywords:** large language models, human detection, text generation, cross-linguistic study, diversity in text

**Relevance Score:** 8

**TL;DR:** This study investigates human detection of text generated by large language models (LLMs) across multiple languages and domains, finding an average detection accuracy of 87.6%, highlighting distinctions based on concreteness, cultural nuances, and diversity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the generalizability of the difficulty in distinguishing LLM-generated text from human-written text across various languages and domains.

**Method:** An extensive case study involving 16 datasets from 9 languages and 9 domains with 19 annotators assessing the detection accuracy of human versus LLM-generated text.

**Key Contributions:**

	1. Provides new insights into the human detection of LLM-generated text across multiple languages and domains.
	2. Identifies specific discourse features where human and machine text differ significantly.
	3. Demonstrates the effectiveness of prompting strategies in improving detection accuracy.

**Result:** The study found an average detection accuracy of 87.6% among annotators, identifying key gaps between human and machine text in terms of concreteness, cultural nuances, and diversity.

**Limitations:** The study does not explore the implications of detection accuracy on real-world applications of LLMs or the subjective preferences for text sources.

**Conclusion:** Prompting with explicit distinctions can help improve detection accuracy, but humans may not always prefer human-written text when its source is unclear.

**Abstract:** Prior studies have shown that distinguishing text generated by large language models (LLMs) from human-written one is highly challenging, and often no better than random guessing. To verify the generalizability of this finding across languages and domains, we perform an extensive case study to identify the upper bound of human detection accuracy. Across 16 datasets covering 9 languages and 9 domains, 19 annotators achieved an average detection accuracy of 87.6\%, thus challenging previous conclusions. We find that major gaps between human and machine text lie in concreteness, cultural nuances, and diversity. Prompting by explicitly explaining the distinctions in the prompts can partially bridge the gaps in over 50\% of the cases. However, we also find that humans do not always prefer human-written text, particularly when they cannot clearly identify its source.

</details>


### [238] [Personality Editing for Language Models through Relevant Knowledge Editing](https://arxiv.org/abs/2502.11789)

*Seojin Hwang, Yumin Kim, Byeongjeong Kim, Donghoon Shin, Hwanhee Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, personality control, knowledge editing, psychological assessments, human evaluations

**Relevance Score:** 9

**TL;DR:** The paper presents PALETTE, a method for enhancing personality control in LLMs via knowledge editing, addressing biases in traditional prompt-based methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Controlling personality in LLM applications is crucial for maintaining user engagement but traditional methods do not effectively address inherent model biases.

**Method:** PALETTE generates adjustment queries based on psychological assessments to systematically adjust responses related to personality traits.

**Key Contributions:**

	1. Introduction of the PALETTE method for personality control in LLMs
	2. Systematic adjustment of responses using knowledge editing 
	3. Demonstration of improved personality stability and balance through experimental results.

**Result:** Experimental evaluations show that PALETTE provides more stable and balanced personality control in LLMs compared to traditional methods.

**Limitations:** The focus on personality may limit its applicability to other aspects of LLM functionality.

**Conclusion:** PALETTE offers a promising approach to enhance personality management in LLMs, contributing to more engaging user interactions.

**Abstract:** Large Language Models (LLMs) play a vital role in applications like conversational agents and content creation, where controlling a model's personality is crucial for maintaining tone, consistency, and engagement. However, traditional prompt-based techniques for controlling personality often fall short, as they do not effectively mitigate the model's inherent biases. In this paper, we introduce a novel method PALETTE that enhances personality control through knowledge editing. By generating adjustment queries inspired by psychological assessments, our approach systematically adjusts responses to personality-related queries similar to modifying factual knowledge, thereby achieving controlled shifts in personality traits. Experimental results from both automatic and human evaluations demonstrate that our method enables more stable and well-balanced personality control in LLMs.

</details>


### [239] [PASER: Post-Training Data Selection for Efficient Pruned Large Language Model Recovery](https://arxiv.org/abs/2502.12594)

*Bowei He, Lihao Yin, Hui-Ling Zhen, Xiaokun Zhang, Mingxuan Yuan, Chen Ma*

**Main category:** cs.CL

**Keywords:** model pruning, large language models, data selection, instruction tuning, machine learning

**Relevance Score:** 9

**TL;DR:** PASER is a method for efficient recovery of pruned large language models by selecting appropriate recovery instructions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance degradation in pruned large language models and the high computational costs of existing recovery methods.

**Method:** PASER utilizes manifold learning and spectral clustering to identify and group instructions based on model capability degradation, then allocates data budgets adaptively across these groups and filters out irrelevant data.

**Key Contributions:**

	1. Introduction of PASER for effective recovery of pruned LLMs
	2. Use of manifold learning and spectral clustering for instruction selection
	3. Demonstration of significant performance recovery with reduced data usage

**Result:** Experimental results show that PASER significantly outperforms traditional methods, recovering pruned model capabilities using only 4%-20% of the original data.

**Limitations:** 

**Conclusion:** PASER effectively enhances the performance of pruned language models while being more data-efficient compared to conventional methods.

**Abstract:** Model pruning is an effective approach for compressing large language models (LLMs). However, this process often leads to significant degradation of model capabilities. While post-training techniques such as instruction tuning are commonly employed to recover model performance, existing methods often overlook the uneven deterioration of model capabilities and incur high computational costs. Moreover, some irrelevant instructions may also introduce negative effects to model capacity recovery. To address these challenges, we propose the \textbf{P}ost-training d\textbf{A}ta \textbf{S}election method for \textbf{E}fficient pruned large language model \textbf{R}ecovery (\textbf{PASER}). PASER aims to identify instructions to recover the most compromised model capacities with a certain data budget. Our approach first applies manifold learning and spectral clustering to group recovery instructions in the semantic space, revealing capability-specific instruction sets. Then, the data budget is adaptively allocated across clusters by the degree of corresponding model capability degradation. In each cluster, we prioritize data samples that lead to the most decline of model performance. To mitigate potential negative tuning effects, we also detect and filter out conflicting or irrelevant recovery data. Extensive experiments demonstrate that PASER significantly outperforms conventional baselines, effectively recovering the general capabilities of pruned LLMs while utilizing merely 4\%-20\% of the original post-training data. We provide the anonymous code repository in \href{https://anonymous.4open.science/r/PASER-E606}{Link}.

</details>


### [240] [Triangulating LLM Progress through Benchmarks, Games, and Cognitive Tests](https://arxiv.org/abs/2502.14359)

*Filippo Moment, Alessandro Suglia, Mario Giulianelli, Ambra Ferrari, Alexander Koller, Oliver Lemon, David Schlangen, Raquel Fernndez, Raffaella Bernardi*

**Main category:** cs.CL

**Keywords:** evaluation paradigms, language models, interactive benchmarks

**Relevance Score:** 8

**TL;DR:** This paper compares the efficacy of evaluation methods for language models, focusing on standard benchmarks versus interactive games.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper investigates how well different evaluation paradigms can discriminate the performance of language models (LLMs) of varying quality.

**Method:** The authors analyze three evaluation paradigms: standard benchmarks, interactive games, and cognitive tests, and correlate these methods with model performance.

**Key Contributions:**

	1. Identification of superior evaluation methods for LLMs based on cognitive assessments.
	2. Comparison of standard benchmarks and interactive games in discriminating model quality.
	3. Establishment of correlations between cognitive abilities and LLM performance.

**Result:** Interactive games are found to be more effective than standard benchmarks in discriminating LLM performance, with specific cognitive skills correlating differently with each method.

**Limitations:** 

**Conclusion:** The study recommends the creation of new interactive benchmarks and cognitive assessments tailored for language models.

**Abstract:** We examine three evaluation paradigms: standard benchmarks (e.g., MMLU and BBH), interactive games (e.g., Signalling Games or Taboo), and cognitive tests (e.g., for working memory or theory of mind). First, we investigate which of the former two-benchmarks or games-is most effective at discriminating LLMs of varying quality. Then, inspired by human cognitive assessments, we compile a suite of targeted tests that measure cognitive abilities deemed essential for effective language use, and we investigate their correlation with model performance in benchmarks and games. Our analyses reveal that interactive games are superior to standard benchmarks in discriminating models. Causal and logical reasoning correlate with both static and interactive tests, while differences emerge regarding core executive functions and social/emotional skills, which correlate more with games. We advocate for the development of new interactive benchmarks and targeted cognitive tasks inspired by assessing human abilities but designed specifically for LLMs.

</details>


### [241] [ICA-RAG: Information Completeness Guided Adaptive Retrieval-Augmented Generation for Disease Diagnosis](https://arxiv.org/abs/2502.14614)

*Mingyi Jia, Zhihao Jia, Junwen Duan, Yan Song, Jianxin Wang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Clinical Diagnosis, Information Completeness

**Relevance Score:** 9

**TL;DR:** ICA-RAG is a new framework that improves retrieval-augmented generation in clinical diagnosis by optimizing retrieval based on input information completeness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing RAG methods struggle to tailor retrieval strategies, leading to inefficient retrieval and noise introduction, impacting diagnostic accuracy.

**Method:** ICA-RAG employs an adaptive control module that evaluates the need for retrieval based on the completeness of input information, optimizing the retrieval process and utilizing knowledge filtering.

**Key Contributions:**

	1. Introduction of ICA-RAG framework for RAG in clinical settings
	2. Adaptive control for retrieval necessity based on information completeness
	3. Demonstrated significant performance improvement in clinical diagnosis tasks.

**Result:** In experiments on three Chinese electronic medical record datasets, ICA-RAG demonstrated significant performance improvements over baseline methods in clinical diagnosis.

**Limitations:** 

**Conclusion:** ICA-RAG enhances the reliability of retrieval-augmented generation by aligning retrieval operations with clinical requirements effectively.

**Abstract:** Retrieval-Augmented Large Language Models~(LLMs), which integrate external knowledge, have shown remarkable performance in medical domains, including clinical diagnosis. However, existing RAG methods often struggle to tailor retrieval strategies to diagnostic difficulty and input sample informativeness. This limitation leads to excessive and often unnecessary retrieval, impairing computational efficiency and increasing the risk of introducing noise that can degrade diagnostic accuracy. To address this, we propose ICA-RAG (\textbf{I}nformation \textbf{C}ompleteness Guided \textbf{A}daptive \textbf{R}etrieval-\textbf{A}ugmented \textbf{G}eneration), a novel framework for enhancing RAG reliability in disease diagnosis. ICA-RAG utilizes an adaptive control module to assess the necessity of retrieval based on the input's information completeness. By optimizing retrieval and incorporating knowledge filtering, ICA-RAG better aligns retrieval operations with clinical requirements. Experiments on three Chinese electronic medical record datasets demonstrate that ICA-RAG significantly outperforms baseline methods, highlighting its effectiveness in clinical diagnosis.

</details>


### [242] [Edit Once, Update Everywhere: A Simple Framework for Cross-Lingual Knowledge Synchronization in LLMs](https://arxiv.org/abs/2502.14645)

*Yuchen Wu, Liang Ding, Li Shen, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** cross-lingual, knowledge editing, large language models, multilingual, AI

**Relevance Score:** 8

**TL;DR:** The paper presents a method for efficient cross-lingual knowledge editing in large language models without full retraining, introducing a state-of-the-art approach called X-KDE.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The inefficiency of existing methods in achieving true cross-linguistic knowledge synchronization in large language models.

**Method:** The method involves two stages: Cross-lingual Edition Instruction Tuning (XE-IT) for fine-tuning on a curated parallel dataset and Target-language Preference Optimization (TL-PO) for consistency across languages using advanced optimization techniques.

**Key Contributions:**

	1. Introduction of the X-KDE method for cross-lingual knowledge editing
	2. Development of a high-quality cross-lingual dataset
	3. Demonstrated significant performance improvements on benchmarks

**Result:** The X-KDE method significantly enhances cross-lingual performance with an average improvement of +8.19% on benchmarks, while maintaining high accuracy in monolingual settings.

**Limitations:** 

**Conclusion:** X-KDE provides a practical solution for efficient adaptation of LLMs to new information across languages, benefiting from a high-quality dataset designed for this purpose.

**Abstract:** Knowledge editing allows for efficient adaptation of large language models (LLMs) to new information or corrections without requiring full retraining. However, prior methods typically focus on either single-language editing or basic multilingual editing, failing to achieve true cross-linguistic knowledge synchronization. To address this, we present a simple and practical state-of-the-art (SOTA) recipe Cross-Lingual Knowledge Democracy Edit (X-KDE), designed to propagate knowledge from a dominant language to other languages effectively. Our X-KDE comprises two stages: (i) Cross-lingual Edition Instruction Tuning (XE-IT), which fine-tunes the model on a curated parallel dataset to modify in-scope knowledge while preserving unrelated information, and (ii) Target-language Preference Optimization (TL-PO), which applies advanced optimization techniques to ensure consistency across languages, fostering the transfer of updates. Additionally, we contribute a high-quality, cross-lingual dataset, specifically designed to enhance knowledge transfer across languages. Extensive experiments on the Bi-ZsRE and MzsRE benchmarks show that X-KDE significantly enhances cross-lingual performance, achieving an average improvement of +8.19%, while maintaining high accuracy in monolingual settings.

</details>


### [243] [SafeInt: Shielding Large Language Models from Jailbreak Attacks via Safety-Aware Representation Intervention](https://arxiv.org/abs/2502.15594)

*Jiaqi Wu, Chen Chen, Chunyan Hou, Xiaojie Yuan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Jailbreak Attacks, SafeIntervention, Defenses, Representation Learning

**Relevance Score:** 9

**TL;DR:** SafeIntervention (SafeInt) is a novel defense method for large language models (LLMs) that protects against jailbreak attacks by dynamically adjusting the representation of harmful queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective and efficient defenses against jailbreak attacks on LLMs to ensure compliance with safety standards.

**Method:** SafeInt intervenes in the representation distributions of jailbreak samples, relocating harmful representations into a rejection region to align them with unsafe sample representations.

**Key Contributions:**

	1. Introduction of SafeIntervention (SafeInt) for LLM safety
	2. Demonstration of effectiveness against various jailbreak attacks
	3. Dynamic adjustment of representations to enhance safety

**Result:** SafeInt outperforms all baseline defenses across multiple jailbreak attacks and maintains utility.

**Limitations:** 

**Conclusion:** SafeInt is an effective and dynamic defense method that significantly mitigates the risk of jailbreak attacks on LLMs.

**Abstract:** With the widespread real-world deployment of large language models (LLMs), ensuring their behavior complies with safety standards has become crucial. Jailbreak attacks exploit vulnerabilities in LLMs to induce undesirable behavior, posing a significant threat to LLM safety. Previous defenses often fail to achieve both effectiveness and efficiency simultaneously. Defenses from a representation perspective offer new insights, but existing interventions cannot dynamically adjust representations based on the harmfulness of the queries. To address this limitation, we propose SafeIntervention (SafeInt), a novel defense method that shields LLMs from jailbreak attacks through safety-aware representation intervention. Built on our analysis of the representations of jailbreak samples, the core idea of SafeInt is to relocate jailbreak-related representations into the rejection region. This is achieved by intervening in the representation distributions of jailbreak samples to align them with those of unsafe samples. We conduct comprehensive experiments covering six jailbreak attacks, two jailbreak datasets, and two utility benchmarks. Experimental results demonstrate that SafeInt outperforms all baselines in defending LLMs against jailbreak attacks while largely maintaining utility. Additionally, we evaluate SafeInt against adaptive attacks and verify its effectiveness in mitigating real-time attacks.

</details>


### [244] [Retrieval-Augmented Fine-Tuning With Preference Optimization For Visual Program Generation](https://arxiv.org/abs/2502.16529)

*Deokhyung Kang, Jeonghun Cho, Yejin Jeon, Sunbin Jang, Minsub Lee, Jawoon Cho, Gary Geunbae Lee*

**Main category:** cs.CL

**Keywords:** visual programming languages, large language models, Ladder Diagram, industrial automation, code generation

**Relevance Score:** 8

**TL;DR:** This paper presents a two-stage training strategy that enhances the generation of Ladder Diagram (LD) code using large language models (LLMs), improving accuracy significantly compared to existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accessibility of visual programming languages (VPLs) in industrial automation, particularly focusing on the limitations of existing prompting-based methods.

**Method:** The authors propose a two-stage training strategy combining retrieval-augmented fine-tuning and direct preference optimization (DPO) to improve the generation of Ladder Diagrams from user instructions.

**Key Contributions:**

	1. Introduction of a two-stage training strategy for LD code generation
	2. Demonstration that retrieval-augmented fine-tuning and DPO improve accuracy
	3. Improved accuracy on real-world LD data by over 10% compared to existing methods.

**Result:** The proposed method improves program-level accuracy by over 10% on real-world LD data compared to traditional supervised fine-tuning methods.

**Limitations:** 

**Conclusion:** This work demonstrates the effectiveness of training-based methods over prompting-based methods for LD generation, with potential applications in advancing industrial automation.

**Abstract:** Visual programming languages (VPLs) allow users to create programs through graphical interfaces, which results in easier accessibility and their widespread usage in various domains. To further enhance this accessibility, recent research has focused on generating VPL code from user instructions using large language models (LLMs). Specifically, by employing prompting-based methods, these studies have shown promising results. Nevertheless, such approaches can be less effective for industrial VPLs such as Ladder Diagram (LD). LD is a pivotal language used in industrial automation processes and involves extensive domain-specific configurations, which are difficult to capture in a single prompt. In this work, we demonstrate that training-based methods outperform prompting-based methods for LD generation accuracy, even with smaller backbone models. Building on these findings, we propose a two-stage training strategy to further enhance VPL generation. First, we employ retrieval-augmented fine-tuning to leverage the repetitive use of subroutines commonly seen in industrial VPLs. Second, we apply direct preference optimization (DPO) to further guide the model toward accurate outputs, using systematically generated preference pairs through graph editing operations. Extensive experiments on real-world LD data demonstrate that our approach improves program-level accuracy by over 10% compared to supervised fine-tuning, which highlights its potential to advance industrial automation.

</details>


### [245] [PrivaCI-Bench: Evaluating Privacy with Contextual Integrity and Legal Compliance](https://arxiv.org/abs/2502.17041)

*Haoran Li, Wenbin Hu, Huihao Jing, Yulin Chen, Qi Hu, Sirui Han, Tianshu Chu, Peizhao Hu, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** privacy, large language models, Contextual Integrity, evaluation benchmark, data compliance

**Relevance Score:** 8

**TL;DR:** The paper introduces PrivaCI-Bench, a benchmark for evaluating the privacy of large language models (LLMs) based on Contextual Integrity theory, going beyond traditional privacy measures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The reliability and trustworthiness of generative LLMs are in doubt due to concerns surrounding data privacy, prompting the need for comprehensive privacy evaluation measures.

**Method:** The authors present PrivaCI-Bench, a benchmark that includes well-annotated privacy regulations, real court cases, privacy policies, and synthetic data for evaluating LLMs on privacy compliance under Contextual Integrity.

**Key Contributions:**

	1. Introduction of PrivaCI-Bench for contextual privacy evaluation of LLMs.
	2. Incorporation of Contextual Integrity theory in LLM privacy assessments.
	3. Evaluation of state-of-the-art LLMs on privacy compliance.

**Result:** Experimental results reveal that while LLMs capture key Contextual Integrity parameters, they still need improvements for better compliance with privacy standards.

**Limitations:** The benchmark currently focuses on legal compliance and might not cover all aspects of privacy in diverse contexts.

**Conclusion:** PrivaCI-Bench provides a thorough framework for evaluating the privacy of LLMs, highlighting areas requiring further enhancements to ensure privacy compliance.

**Abstract:** Recent advancements in generative large language models (LLMs) have enabled wider applicability, accessibility, and flexibility. However, their reliability and trustworthiness are still in doubt, especially for concerns regarding individuals' data privacy. Great efforts have been made on privacy by building various evaluation benchmarks to study LLMs' privacy awareness and robustness from their generated outputs to their hidden representations. Unfortunately, most of these works adopt a narrow formulation of privacy and only investigate personally identifiable information (PII). In this paper, we follow the merit of the Contextual Integrity (CI) theory, which posits that privacy evaluation should not only cover the transmitted attributes but also encompass the whole relevant social context through private information flows. We present PrivaCI-Bench, a comprehensive contextual privacy evaluation benchmark targeted at legal compliance to cover well-annotated privacy and safety regulations, real court cases, privacy policies, and synthetic data built from the official toolkit to study LLMs' privacy and safety compliance. We evaluate the latest LLMs, including the recent reasoner models QwQ-32B and Deepseek R1. Our experimental results suggest that though LLMs can effectively capture key CI parameters inside a given context, they still require further advancements for privacy compliance.

</details>


### [246] [Unveiling Downstream Performance Scaling of LLMs: A Clustering-Based Perspective](https://arxiv.org/abs/2502.17262)

*Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, Chenggang Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, performance prediction, Clustering-On-Difficulty, resource allocation, scaling laws

**Relevance Score:** 9

**TL;DR:** Proposes a COD framework for better downstream task performance prediction of LLMs by clustering tasks based on difficulty.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate performance prediction of LLMs is crucial for efficient training resource allocation, but current methods are inconsistent and unreliable due to the emergence phenomenon and task complexity.

**Method:** The COD framework clusters tasks based on their difficulty features and uses performance scaling laws to predict cluster-wise performance, which is then mapped to full set performance.

**Key Contributions:**

	1. Introduction of the Clustering-On-Difficulty framework
	2. Demonstration of improved accuracy in performance prediction
	3. Development of a mapping function for performance extrapolation

**Result:** The COD framework achieved an average prediction error of 1.36% across eight LLM benchmarks when applied to a 70B parameter LLM.

**Limitations:** 

**Conclusion:** The COD framework provides a more stable and predictable way for allocating resources in LLM pre-training.

**Abstract:** The escalating scale and cost of Large Language Models (LLMs) training necessitate accurate pre-training prediction of downstream task performance for efficient resource allocation. This is challenged by: 1) the emergence phenomenon, where metrics become meaningful only after extensive training, hindering prediction by smaller models; and 2) uneven task difficulty and inconsistent performance scaling patterns, leading to high metric variability. Current prediction methods lack accuracy and reliability. We propose a Clustering-On-Difficulty (COD) framework for downstream performance prediction. The COD framework clusters tasks by their difficulty scaling features, thereby establishing a more stable and predictable support subset through the exclusion of tasks exhibiting non-emergent behavior or irregular scaling. We adopt a performance scaling law to predict cluster-wise performance with theoretical support. Predictable subset performance acts as an intermediate predictor for the full evaluation set. We further derive a mapping function to accurately extrapolate the performance of the subset to the full set. Applied to an LLM with 70B parameters, COD achieved a 1.36% average prediction error across eight key LLM benchmarks, offering actionable insights for resource allocation and training monitoring of LLMs pretraining.

</details>


### [247] [Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review](https://arxiv.org/abs/2502.19614)

*Sungduk Yu, Man Luo, Avinash Madusu, Vasudev Lal, Phillip Howard*

**Main category:** cs.CL

**Keywords:** AI peer review, AI text detection, human-computer interaction, generative AI, large language models

**Relevance Score:** 8

**TL;DR:** This paper introduces a dataset for benchmarking the detection of AI-generated peer reviews and evaluates existing AI detection algorithms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing concern about the reliance on LLMs in the peer review process, which could compromise the integrity of scientific research due to negligent reviewers.

**Method:** A dataset of 788,984 AI-generated and corresponding human peer reviews was created, and 18 AI text detection algorithms were evaluated against this dataset. The study also explored a context-aware detection method called Anchor.

**Key Contributions:**

	1. Creation of a large dataset for AI-generated peer review detection
	2. Evaluation of existing AI detection algorithms
	3. Introduction of a context-aware detection method (Anchor)

**Result:** The evaluation revealed challenges in distinguishing AI-generated peer reviews from human-written ones, emphasizing the need for improved detection tools.

**Limitations:** The study focuses on specific AI detection algorithms and may not cover all existing methods or the broader implications of AI in peer review.

**Conclusion:** The findings underscore the difficulty of identifying AI-generated text in peer reviews and call for new methodologies to combat this issue.

**Abstract:** Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review. To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews fully written by humans and different state-of-the-art LLMs. Additionally, we explore a context-aware detection method called Anchor, which leverages manuscript content to detect AI-generated reviews, and analyze the sensitivity of detection models to LLM-assisted editing of human-written text. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI. Our dataset is publicly available at: https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark.

</details>


### [248] [Do Retrieval-Augmented Language Models Adapt to Varying User Needs?](https://arxiv.org/abs/2502.19779)

*Peilin Wu, Xinlu Zhang, Wenhao Yu, Xingyu Liu, Xinya Du, Zhiyu Zoey Chen*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Language Models, Human-Centric Evaluation, Knowledge-Intensive Tasks

**Relevance Score:** 9

**TL;DR:** This paper presents a new evaluation framework for Retrieval-Augmented Language Models (RALMs) that incorporates varying user needs and context settings to better assess model performance in knowledge-intensive tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for evaluating RALMs often assume a single optimal approach and overlook the diverse needs of users. This research seeks to address these limitations by developing a user-centric evaluation framework.

**Method:** The paper introduces an evaluation framework that assesses RALMs across three user need casesContext-Exclusive, Context-First, and Memory-Firstwhile considering multiple context settings including Context Matching, Knowledge Conflict, and Information Irrelevant. The methodology involves varying user instructions and the nature of retrieved information during extensive experiments on various QA datasets.

**Key Contributions:**

	1. Introduction of a user-centric evaluation framework for RALMs
	2. Identification of three user need cases for assessment
	3. Demonstration of how retrieval strategies impact model robustness and performance

**Result:** The experiments reveal that restricting memory usage enhances robustness under adverse retrieval conditions but may decrease peak performance with optimal retrieval results, indicating that model family influences behavioral differences.

**Limitations:** The framework may not encompass all potential user needs and contexts in real-world applications.

**Conclusion:** The study emphasizes the importance of user-centric evaluations in retrieval-augmented systems and offers insights for optimizing model performance according to different retrieval contexts.

**Abstract:** Recent advancements in Retrieval-Augmented Language Models (RALMs) have demonstrated their efficacy in knowledge-intensive tasks. However, existing evaluation benchmarks often assume a single optimal approach to leveraging retrieved information, failing to account for varying user needs. This paper introduces a novel evaluation framework that systematically assesses RALMs under three user need cases-Context-Exclusive, Context-First, and Memory-First-across three distinct context settings: Context Matching, Knowledge Conflict, and Information Irrelevant. By varying both user instructions and the nature of retrieved information, our approach captures the complexities of real-world applications where models must adapt to diverse user requirements. Through extensive experiments on multiple QA datasets, including HotpotQA, DisentQA, and our newly constructed synthetic URAQ dataset, we find that restricting memory usage improves robustness in adversarial retrieval conditions but decreases peak performance with ideal retrieval results and model family dominates behavioral differences. Our findings highlight the necessity of user-centric evaluations in the development of retrieval-augmented systems and provide insights into optimizing model performance across varied retrieval contexts. We will release our code and URAQ dataset upon acceptance of the paper.

</details>


### [249] [HoT: Highlighted Chain of Thought for Referencing Supporting Facts from Inputs](https://arxiv.org/abs/2503.02003)

*Tin Nguyen, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Highlighted Chain-of-Thought Prompting, fact verification, user interaction, hallucination

**Relevance Score:** 9

**TL;DR:** Proposes Highlighted Chain-of-Thought Prompting (HoT) to improve accuracy of LLMs by using XML tags to ground facts in responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the issue of Large Language Models hallucinating non-factual statements, complicating human verification of responses.

**Method:** Introduces HoT technique where LLMs format questions with XML tags to emphasize key facts and produce highlighted responses.

**Key Contributions:**

	1. Introduction of XML tagging for grounding facts in LLM responses
	2. Demonstrated superiority of HoT over traditional prompting in various tasks
	3. Insights into user interaction with highlighted responses

**Result:** In few-shot settings, HoT surpasses standard chain-of-thought prompting on 17 diverse tasks. It assists users in verifying responses effectively but can mislead them about incorrect outputs.

**Limitations:** HoT may mislead users to believe incorrect answers are correct when highlights are involved.

**Conclusion:** HoT increases efficiency in recognizing correct LLM outputs but poses risks of misleading trust when the model is incorrect.

**Abstract:** An Achilles heel of Large Language Models (LLMs) is their tendency to hallucinate non-factual statements. A response mixed of factual and non-factual statements poses a challenge for humans to verify and accurately base their decisions on. To combat this problem, we propose Highlighted Chain-of-Thought Prompting (HoT), a technique for prompting LLMs to generate responses with XML tags that ground facts to those provided in the query. That is, given an input question, LLMs would first re-format the question to add XML tags highlighting key facts, and then, generate a response with highlights over the facts referenced from the input. Interestingly, in few-shot settings, HoT outperforms vanilla chain of thought prompting (CoT) on a wide range of 17 tasks from arithmetic, reading comprehension to logical reasoning. When asking humans to verify LLM responses, highlights help time-limited participants to more accurately and efficiently recognize when LLMs are correct. Yet, surprisingly, when LLMs are wrong, HoTs tend to make users believe that an answer is correct.

</details>


### [250] [Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models](https://arxiv.org/abs/2503.02623)

*Paul Stangel, David Bani-Harouni, Chantal Pellegrini, Ege zsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab*

**Main category:** cs.CL

**Keywords:** Large Language Models, Confidence Calibration, Reinforcement Learning, Generative Models, Logarithmic Scoring Rule

**Relevance Score:** 9

**TL;DR:** This paper presents a novel Reinforcement Learning method for fine-tuning Large Language Models to express calibrated confidence in their responses to factual questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure safe and trustworthy use of LLMs by improving the accuracy of confidence estimates in their answers.

**Method:** A Reinforcement Learning approach that fine-tunes LLMs by optimizing a reward based on the logarithmic scoring rule, penalizing both over- and under-confidence.

**Key Contributions:**

	1. Novel reward design for fine-tuning LLMs with calibrated confidence estimates
	2. Integration of confidence calibration with response generation
	3. Demonstrated generalization to unseen tasks without additional fine-tuning.

**Result:** Models trained with this method show improved calibration of confidence estimates and can generalize to unseen tasks without further fine-tuning.

**Limitations:** 

**Conclusion:** The proposed approach integrates confidence calibration into the generative process of LLMs, resulting in enhanced performance and general confidence awareness.

**Abstract:** A safe and trustworthy use of Large Language Models (LLMs) requires an accurate expression of confidence in their answers. We propose a novel Reinforcement Learning approach that allows to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers to factual questions. Our method optimizes a reward based on the logarithmic scoring rule, explicitly penalizing both over- and under-confidence. This encourages the model to align its confidence estimates with the actual predictive accuracy. The optimal policy under our reward design would result in perfectly calibrated confidence expressions. Unlike prior approaches that decouple confidence estimation from response generation, our method integrates confidence calibration seamlessly into the generative process of the LLM. Empirically, we demonstrate that models trained with our approach exhibit substantially improved calibration and generalize to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness. We provide our training and evaluation code in the supplementary and will make it publicly available upon acceptance.

</details>


### [251] [SEOE: A Scalable and Reliable Semantic Evaluation Framework for Open Domain Event Detection](https://arxiv.org/abs/2503.03303)

*Yi-Fan Lu, Xian-Ling Mao, Tian Lan, Tong Zhang, Yu-Shi Zhu, Heyan Huang*

**Main category:** cs.CL

**Keywords:** Open Domain Event Detection, Semantic Evaluation, Large Language Models, Benchmark Construction, Event Detection

**Relevance Score:** 7

**TL;DR:** This paper addresses the challenges of automatic evaluation in Open Domain Event Detection (ODED) by introducing a scalable Semantic-level Evaluation framework (SEOE) that enhances representativeness and evaluation reliability using large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to resolve issues in existing ODED evaluation methods, particularly their limited representation in benchmarks and the inadequacy of token-level metrics which fail to capture semantic similarities between predictions and actual labels.

**Method:** The proposed framework constructs a scalable evaluation benchmark with 564 event types across 7 domains, and employs large language models to compute a semantic F1-score as a more reliable evaluation metric.

**Key Contributions:**

	1. Introduction of a scalable evaluation benchmark for ODED with broad domain coverage
	2. Development of a semantic evaluation metric using LLMs
	3. Analysis of comprehensive error patterns in existing ODED methods

**Result:** The framework was validated through extensive experiments, demonstrating the representativeness of the benchmark and the reliability of the new semantic evaluation metric.

**Limitations:** 

**Conclusion:** The study concludes that the SEOE framework significantly improves the evaluation of ODED methods, providing insights into error patterns in predictions.

**Abstract:** Automatic evaluation for Open Domain Event Detection (ODED) is a highly challenging task, because ODED is characterized by a vast diversity of un-constrained output labels from various domains. Nearly all existing evaluation methods for ODED usually first construct evaluation benchmarks with limited labels and domain coverage, and then evaluate ODED methods using metrics based on token-level label matching rules. However, this kind of evaluation framework faces two issues: (1) The limited evaluation benchmarks lack representatives of the real world, making it difficult to accurately reflect the performance of various ODED methods in real-world scenarios; (2) Evaluation metrics based on token-level matching rules fail to capture semantic similarity between predictions and golden labels. To address these two problems above, we propose a scalable and reliable Semantic-level Evaluation framework for Open domain Event detection (SEOE) by constructing a more representative evaluation benchmark and introducing a semantic evaluation metric. Specifically, our proposed framework first constructs a scalable evaluation benchmark that currently includes 564 event types covering 7 major domains, with a cost-effective supplementary annotation strategy to ensure the benchmark's representativeness. The strategy also allows for the supplement of new event types and domains in the future. Then, the proposed SEOE leverages large language models (LLMs) as automatic evaluation agents to compute a semantic F1-score, incorporating fine-grained definitions of semantically similar labels to enhance the reliability of the evaluation. Extensive experiments validate the representatives of the benchmark and the reliability of the semantic evaluation metric. Existing ODED methods are thoroughly evaluated, and the error patterns of predictions are analyzed, revealing several insightful findings.

</details>


### [252] [Compositional Causal Reasoning Evaluation in Language Models](https://arxiv.org/abs/2503.04556)

*Jacqueline R. M. A. Maasch, Alihan Hyk, Xinnuo Xu, Aditya V. Nori, Javier Gonzalez*

**Main category:** cs.CL

**Keywords:** compositional reasoning, causal reasoning, language models, evaluation framework, error analysis

**Relevance Score:** 6

**TL;DR:** The paper introduces a unified evaluation framework for compositional causal reasoning (CCR) in AI, especially focusing on language models.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Causal reasoning and compositional reasoning are fundamental challenges in AI, necessitating effective evaluation methods to measure these behaviors.

**Method:** A unified framework for assessing compositional causal reasoning (CCR), applying it to average treatment effect and probabilities of necessity and sufficiency in causal inference.

**Key Contributions:**

	1. Introduction of a unified evaluation framework for compositional causal reasoning (CCR)
	2. Demonstration of distinct error patterns across different language models
	3. Application of CCR to assess average treatment effects and causal probabilities

**Result:** The framework was tested on language models such as LLama, Phi, and GPT, revealing distinct error patterns that increased with complexity in causal paths, except for one model.

**Limitations:** 

**Conclusion:** The study highlights the importance of evaluating compositional causal reasoning in AI models, with implications for interpretability and error analysis in language models.

**Abstract:** Causal reasoning and compositional reasoning are two core aspirations in AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate CCR evaluation for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. CCR errors increased with the complexity of causal paths for all models except o1.

</details>


### [253] [MedPlan:A Two-Stage RAG-Based System for Personalized Medical Plan Generation](https://arxiv.org/abs/2503.17900)

*Hsin-Ling Hsu, Cong-Tinh Dao, Luning Wang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Chun-Chieh Liao, Pengfei Hu, Xiaoxue Han, Chih-Ho Hsu, Dongsheng Luo, Wen-Chih Peng, Feng Liu, Fang-Ming Hung, Chenwei Wu*

**Main category:** cs.CL

**Keywords:** large language models, electronic health records, treatment planning, machine learning, health informatics

**Relevance Score:** 9

**TL;DR:** A novel framework aligns LLMs with clinician workflows for improved treatment planning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM applications in EHR focus on assessment rather than effective treatment planning, with critical limitations such as single-pass generation and lack of patient-specific context.

**Method:** Introduces a two-stage architecture that first assesses clinical data and then generates a structured treatment plan utilizing retrieval-augmented generation.

**Key Contributions:**

	1. Introduces a two-stage LLM framework for EHR that mimics clinician workflows
	2. Incorporates patient-specific historical context into treatment planning
	3. Improves assessment accuracy and treatment plan quality significantly

**Result:** Demonstrates significant improvements in assessment accuracy and treatment plan quality compared to baseline approaches.

**Limitations:** 

**Conclusion:** The proposed framework effectively integrates LLMs into clinician workflows, enhancing both assessment and treatment planning processes.

**Abstract:** Despite recent success in applying large language models (LLMs) to electronic health records (EHR), most systems focus primarily on assessment rather than treatment planning. We identify three critical limitations in current approaches: they generate treatment plans in a single pass rather than following the sequential reasoning process used by clinicians; they rarely incorporate patient-specific historical context; and they fail to effectively distinguish between subjective and objective clinical information. Motivated by the SOAP methodology (Subjective, Objective, Assessment, Plan), we introduce \ours{}, a novel framework that structures LLM reasoning to align with real-life clinician workflows. Our approach employs a two-stage architecture that first generates a clinical assessment based on patient symptoms and objective data, then formulates a structured treatment plan informed by this assessment and enriched with patient-specific information through retrieval-augmented generation. Comprehensive evaluation demonstrates that our method significantly outperforms baseline approaches in both assessment accuracy and treatment plan quality.

</details>


### [254] [A Retrieval-Based Approach to Medical Procedure Matching in Romanian](https://arxiv.org/abs/2503.20556)

*Andrei Niculae, Adrian Cosma, Emilian Radoi*

**Main category:** cs.CL

**Keywords:** medical terminology, sentence embeddings, Romanian healthcare, NLP, low-resource languages

**Relevance Score:** 7

**TL;DR:** This paper discusses a retrieval-based architecture using sentence embeddings to map medical procedure names from Romanian healthcare providers to standardized terms, addressing issues in underrepresented languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate the mapping of medical procedure names for reducing administrative inefficiencies and insurance claim problems in Romanian healthcare, where current manual methods are prevalent.

**Method:** Evaluation of multiple embedding models, including Romanian, multilingual, and medical-domain-specific representations, for effective medical name matching.

**Key Contributions:**

	1. Proposed a retrieval-based architecture for medical name mapping.
	2. Evaluated various embedding models specific to the Romanian healthcare system.
	3. Contributed to the medical NLP field for underrepresented languages.

**Result:** The study identifies the most effective embedding model for medical name matching in the Romanian healthcare system despite the challenges posed by underrepresented languages.

**Limitations:** 

**Conclusion:** Findings advance medical NLP initiatives for low-resource languages, showcasing the potential for automation in healthcare terminology mapping.

**Abstract:** Accurately mapping medical procedure names from healthcare providers to standardized terminology used by insurance companies is a crucial yet complex task. Inconsistencies in naming conventions lead to missclasified procedures, causing administrative inefficiencies and insurance claim problems in private healthcare settings. Many companies still use human resources for manual mapping, while there is a clear opportunity for automation. This paper proposes a retrieval-based architecture leveraging sentence embeddings for medical name matching in the Romanian healthcare system. This challenge is significantly more difficult in underrepresented languages such as Romanian, where existing pretrained language models lack domain-specific adaptation to medical text. We evaluate multiple embedding models, including Romanian, multilingual, and medical-domain-specific representations, to identify the most effective solution for this task. Our findings contribute to the broader field of medical NLP for low-resource languages such as Romanian.

</details>


### [255] [Unlocking Efficient Long-to-Short LLM Reasoning with Model Merging](https://arxiv.org/abs/2503.20641)

*Han Wu, Yuxuan Yao, Shuqi Liu, Zehua Liu, Xiaojin Fu, Xiongwei Han, Xing Li, Hui-Ling Zhen, Tao Zhong, Mingxuan Yuan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Model Merging, Long-to-Short Reasoning, System 1, System 2

**Relevance Score:** 7

**TL;DR:** This paper presents an empirical study on model merging for Long-to-Short (L2S) reasoning in large language models (LLMs), showcasing its potential to enhance efficiency while maintaining output quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the efficiency problem of LLMs transitioning from System 1 to System 2 reasoning, which often leads to redundant outputs without proportional improvements in quality.

**Method:** The paper explores various methodologies for model merging, including task-vector-based, SVD-based, and activation-informed merging, assessing their impact on L2S reasoning.

**Key Contributions:**

	1. Empirical exploration of model merging for L2S reasoning.
	2. Demonstration of substantial efficiency gains without sacrificing output quality.
	3. Identification of correlations between model scale and merging efficacy.

**Result:** Model merging reduces average response length by up to 55% and can maintain or even improve baseline performance across different model scales (1.5B to 32B parameters).

**Limitations:** 

**Conclusion:** Model merging is presented as a highly effective solution for enhancing L2S reasoning efficiency while preserving the robustness of System 2 reasoning; the methodology allows adaptive response lengths based on task complexity.

**Abstract:** The transition from System 1 to System 2 reasoning in large language models (LLMs) has marked significant advancements in handling complex tasks through deliberate, iterative thinking. However, this progress often comes at the cost of efficiency, as models tend to overthink, generating redundant reasoning steps without proportional improvements in output quality. Long-to-Short (L2S) reasoning has emerged as a promising solution to this challenge, aiming to balance reasoning depth with practical efficiency. While existing approaches, such as supervised fine-tuning (SFT), reinforcement learning (RL), and prompt engineering, have shown potential, they are either computationally expensive or unstable. Model merging, on the other hand, offers a cost-effective and robust alternative by integrating the quick-thinking capabilities of System 1 models with the methodical reasoning of System 2 models. In this work, we present a comprehensive empirical study on model merging for L2S reasoning, exploring diverse methodologies, including task-vector-based, SVD-based, and activation-informed merging. Our experiments reveal that model merging can reduce average response length by up to 55% while preserving or even improving baseline performance. We also identify a strong correlation between model scale and merging efficacy with extensive evaluations on 1.5B/7B/14B/32B models. Furthermore, we investigate the merged model's ability to self-critique and self-correct, as well as its adaptive response length based on task complexity. Our findings highlight model merging as a highly efficient and effective paradigm for L2S reasoning, offering a practical solution to the overthinking problem while maintaining the robustness of System 2 reasoning. This work can be found on Github https://github.com/hahahawu/Long-to-Short-via-Model-Merging.

</details>


### [256] [Cognitive Debiasing Large Language Models for Decision-Making](https://arxiv.org/abs/2504.04141)

*Yougang Lyu, Shijie Ren, Yue Feng, Zihan Wang, Zhumin Chen, Zhaochun Ren, Maarten de Rijke*

**Main category:** cs.CL

**Keywords:** large language models, cognitive biases, debiasing, decision-making, healthcare

**Relevance Score:** 9

**TL;DR:** The paper presents a new approach, Self-Adaptive Cognitive Debiasing (SACD), to enhance decision-making applications of large language models (LLMs) by iteratively refining prompts to mitigate cognitive biases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) can aid decision-making in various domains, but inherent cognitive biases create challenges that current debiasing strategies do not adequately address.

**Method:** The SACD method consists of three steps: bias determination, bias analysis, and cognitive debiasing. This iterative process refines the prompts to reduce cognitive biases effectively.

**Key Contributions:**

	1. Introduction of Self-Adaptive Cognitive Debiasing (SACD) approach
	2. Demonstration of effectiveness in tasks involving multiple cognitive biases
	3. Comparison shows SACD outperforms existing debiasing techniques and prompt engineering methods

**Result:** Experimental results indicate that SACD improves accuracy in decision-making tasks across finance, healthcare, and legal domains, outperforming existing techniques in both single and multiple bias scenarios.

**Limitations:** The effectiveness of SACD may vary based on the complexity of biases and the specific context of applications.

**Conclusion:** The proposed SACD approach demonstrates significant enhancements in the reliability of LLM outputs by addressing cognitive biases more comprehensively than existing methods.

**Abstract:** Large language models (LLMs) have shown potential in supporting decision-making applications, particularly as personal assistants in the financial, healthcare, and legal domains. While prompt engineering strategies have enhanced the capabilities of LLMs in decision-making, cognitive biases inherent to LLMs present significant challenges. Cognitive biases are systematic patterns of deviation from norms or rationality in decision-making that can lead to the production of inaccurate outputs. Existing cognitive bias mitigation strategies assume that input prompts only contain one type of cognitive bias, limiting their effectiveness in more challenging scenarios involving multiple cognitive biases. To fill this gap, we propose a cognitive debiasing approach, self-adaptive cognitive debiasing (SACD), that enhances the reliability of LLMs by iteratively refining prompts. Our method follows three sequential steps -- bias determination, bias analysis, and cognitive debiasing -- to iteratively mitigate potential cognitive biases in prompts. Experimental results on finance, healthcare, and legal decision-making tasks, using both closed-source and open-source LLMs, demonstrate that the proposed SACD method outperforms both advanced prompt engineering methods and existing cognitive debiasing techniques in average accuracy under single-bias and multi-bias settings.

</details>


### [257] [SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog](https://arxiv.org/abs/2504.07199)

*Jennifer D'Souza, Sameer Sadruddin, Holger Israel, Mathias Begoin, Diana Slawig*

**Main category:** cs.CL

**Keywords:** LLMs, subject tagging, digital libraries, GND taxonomy, multilingual processing

**Relevance Score:** 8

**TL;DR:** This paper describes SemEval-2025 Task 5, focused on automated subject tagging for scientific records using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The task aims to enhance the classification of scientific and technical records by utilizing LLMs for automated subject tagging based on the GND taxonomy.

**Method:** Participants developed LLM-based systems that recommend top-k subjects, evaluated through quantitative metrics and qualitative assessments.

**Key Contributions:**

	1. Introduction of a shared task for subject tagging using LLMs
	2. Use of LLM ensembles and synthetic data in classification
	3. Evaluation framework including both quantitative and qualitative assessments.

**Result:** The results demonstrate the effectiveness of LLM ensembles, synthetic data generation, and multilingual processing in subject tagging.

**Limitations:** 

**Conclusion:** The findings provide insights into the application of LLMs in digital library classification, which could improve how scientific records are categorized.

**Abstract:** We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated subject tagging for scientific and technical records in English and German using the GND taxonomy. Participants developed LLM-based systems to recommend top-k subjects, evaluated through quantitative metrics (precision, recall, F1-score) and qualitative assessments by subject specialists. Results highlight the effectiveness of LLM ensembles, synthetic data generation, and multilingual processing, offering insights into applying LLMs for digital library classification.

</details>


### [258] [ConceptCarve: Dynamic Realization of Evidence](https://arxiv.org/abs/2504.07228)

*Eylon Caplan, Dan Goldwasser*

**Main category:** cs.CL

**Keywords:** evidence retrieval, social media, concept representation, LLMs, community analysis

**Relevance Score:** 8

**TL;DR:** Introducing ConceptCarve, a framework for evidence retrieval from social media using LLMs and traditional methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To find evidence for human opinion and behavior at scale from online communities, addressing challenges in identifying abstract concepts and their varied instantiations across different groups.

**Method:** ConceptCarve combines traditional retrieval methods with LLMs to dynamically adjust the search space during evidence retrieval from social media posts.

**Key Contributions:**

	1. Development of ConceptCarve retrieval framework
	2. Dynamic characterization of search space
	3. Interpretation of community-specific thought patterns

**Result:** ConceptCarve outperforms traditional retrieval systems in locating relevant evidence within social media communities and provides interpretable representations of complex thought patterns.

**Limitations:** 

**Conclusion:** The framework enhances evidence retrieval effectiveness and allows for qualitative analysis of diverse community perspectives.

**Abstract:** Finding evidence for human opinion and behavior at scale is a challenging task, often requiring an understanding of sophisticated thought patterns among vast online communities found on social media. For example, studying how gun ownership is related to the perception of Freedom, requires a retrieval system that can operate at scale over social media posts, while dealing with two key challenges: (1) identifying abstract concept instances, (2) which can be instantiated differently across different communities. To address these, we introduce ConceptCarve, an evidence retrieval framework that utilizes traditional retrievers and LLMs to dynamically characterize the search space during retrieval. Our experiments show that ConceptCarve surpasses traditional retrieval systems in finding evidence within a social media community. It also produces an interpretable representation of the evidence for that community, which we use to qualitatively analyze complex thought patterns that manifest differently across the communities.

</details>


### [259] [Playpen: An Environment for Exploring Learning Through Conversational Interaction](https://arxiv.org/abs/2504.08590)

*Nicola Horst, Davide Mazzaccara, Antonia Schmidt, Michael Sullivan, Filippo Moment, Luca Franceschetti, Philipp Sadler, Sherzod Hakimov, Alberto Testoni, Raffaella Bernardi, Raquel Fernndez, Alexander Koller, Oliver Lemon, David Schlangen, Mario Giulianelli, Alessandro Suglia*

**Main category:** cs.CL

**Keywords:** Dialogue Games, Large Language Models, Reinforcement Learning, Feedback signals, Post-training methods

**Relevance Score:** 8

**TL;DR:** This paper explores using Dialogue Games to provide feedback signals for post-training Large Language Models and introduces the Playpen environment for this purpose.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The recent focus on the interaction between learner and feedback-giver for post-training LLMs emphasizes the need for effective feedback mechanisms in model tuning.

**Method:** The authors introduce Playpen for offline and online learning through Dialogue Game self-play, experimenting with methods like supervised fine-tuning and reinforcement learning.

**Key Contributions:**

	1. Introduced Playpen environment for Dialogue Game self-play learning.
	2. Demonstrated effective post-training methods for LLMs, including GRPO.
	3. Showed the impact of different learning strategies on performance and skill retention.

**Result:** Results show that imitation learning through supervised fine-tuning enhances performance on unseen instances but harms other skills, whereas interactive learning with GRPO improves overall without skill loss.

**Limitations:** The study mainly focuses on a small LLM and specific post-training methods; broader applicability may need further research.

**Conclusion:** The framework and baseline training setups are released to promote further research in the area of learning through synthetic interaction.

**Abstract:** Interaction between learner and feedback-giver has come into focus recently for post-training of Large Language Models (LLMs), through the use of reward models that judge the appropriateness of a model's response. In this paper, we investigate whether Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can also serve as a source of feedback signals for learning. We introduce Playpen, an environment for off- and online learning through Dialogue Game self-play, and investigate a representative set of post-training methods: supervised fine-tuning; direct alignment (DPO); and reinforcement learning with GRPO. We experiment with post-training a small LLM (Llama-3.1-8B-Instruct), evaluating performance on unseen instances of training games as well as unseen games, and on standard benchmarks. We find that imitation learning through SFT improves performance on unseen instances, but negatively impacts other skills, while interactive learning with GRPO shows balanced improvements without loss of skills. We release the framework and the baseline training setups to foster research in the promising new direction of learning in (synthetic) interaction.

</details>


### [260] [DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning](https://arxiv.org/abs/2504.11456)

*Zhiwei He, Tian Liang, Jiahao Xu, Qiuzhi Liu, Xingyu Chen, Yue Wang, Linfeng Song, Dian Yu, Zhenwen Liang, Wenxuan Wang, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Mathematical Dataset

**Relevance Score:** 6

**TL;DR:** Introduction of DeepMath-103K dataset for reinforcement learning in mathematical reasoning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of challenging, contamination-free, and verifiable large-scale training data for reinforcement learning with large language models.

**Method:** Introducing a large-scale dataset, DeepMath-103K, featuring high difficulty mathematical problems suitable for rule-based RL rewards and adaptable training paradigms.

**Key Contributions:**

	1. Introduction of DeepMath-103K dataset for challenging math problems
	2. Rigorous decontamination against benchmarks
	3. Demonstrated generalization capabilities beyond mathematics

**Result:** Models trained on DeepMath-103K achieve state-of-the-art results on complex mathematical benchmarks and show generalization in other scientific domains.

**Limitations:** 

**Conclusion:** DeepMath-103K aids in developing advanced reasoning in models and supports a wide range of applications across various scientific fields.

**Abstract:** Reinforcement learning (RL) with large language models shows promise in complex reasoning. However, its progress is hindered by the lack of large-scale training data that is sufficiently challenging, contamination-free and verifiable. To this end, we introduce DeepMath-103K, a large-scale mathematical dataset designed with high difficulty (primarily levels 5-9), rigorous decontamination against numerous benchmarks, and verifiable answers for rule-based RL reward. It further includes three distinct R1 solutions adaptable for diverse training paradigms such as supervised fine-tuning (SFT). Spanning a wide range of mathematical topics, DeepMath-103K fosters the development of generalizable and advancing reasoning. Notably, models trained on DeepMath-103K achieve state-of-the-art results on challenging mathematical benchmarks and demonstrate generalization beyond math such as biology, physics and chemistry, underscoring its broad efficacy. Data: https://huggingface.co/datasets/zwhe99/DeepMath-103K.

</details>


### [261] [Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models](https://arxiv.org/abs/2504.12898)

*Zhouhao Sun, Xiao Ding, Li Du, Yunpeng Xu, Yixuan Ma, Yang Zhao, Bing Qin, Ting Liu*

**Main category:** cs.CL

**Keywords:** debiasing, large language models, information theory, causal intervention, generalizability

**Relevance Score:** 9

**TL;DR:** This paper introduces the information gain-guided causal intervention debiasing framework (ICD) to reduce dataset biases in large language models (LLMs), improving their generalizability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the limitations of existing debiasing methods for large language models (LLMs), especially regarding dataset biases that affect inference and generalizability.

**Method:** The ICD framework combines causal mechanisms with information theory to balance instruction-tuning datasets through a data rewriting method that eliminates biases contributing information for answer prediction.

**Key Contributions:**

	1. Introduction of the ICD framework combining causal mechanisms and information theory for debiasing LLMs.
	2. Development of a data rewriting method to balance instruction-tuning dataset distributions.
	3. Demonstrated improvement in LLM generalizability across tasks post-debiasing.

**Result:** Experimental results demonstrate that the ICD framework effectively reduces biases in LLMs, leading to enhanced performance across various tasks.

**Limitations:** The effectiveness of the proposed framework may depend on the quality and diversity of the input datasets used for training and intervention.

**Conclusion:** The ICD framework offers a promising solution for improving the robustness of LLMs by addressing inherent biases in training datasets through causal interventions.

**Abstract:** Despite significant progress, recent studies indicate that current large language models (LLMs) may still capture dataset biases and utilize them during inference, leading to the poor generalizability of LLMs. However, due to the diversity of dataset biases and the insufficient nature of bias suppression based on in-context learning, the effectiveness of previous prior knowledge-based debiasing methods and in-context learning based automatic debiasing methods is limited. To address these challenges, we explore the combination of causal mechanisms with information theory and propose an information gain-guided causal intervention debiasing (ICD) framework. To eliminate biases within the instruction-tuning dataset, it is essential to ensure that these biases do not provide any additional information to predict the answers, i.e., the information gain of these biases for predicting the answers needs to be 0. Under this guidance, this framework utilizes a causal intervention-based data rewriting method to automatically and autonomously balance the distribution of instruction-tuning dataset for reducing the information gain. Subsequently, it employs a standard supervised fine-tuning process to train LLMs on the debiased dataset. Experimental results show that ICD can effectively debias LLM to improve its generalizability across different tasks.

</details>
