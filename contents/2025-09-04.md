# 2025-09-04

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 14]

- [cs.CL](#cs.CL) [Total: 37]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [STRive: An association rule-based system for the exploration of spatiotemporal categorical data](https://arxiv.org/abs/2509.02732)

*Mauro Diaz, Luis Sante, Joel Perca, Jo√£o Victor da Silva, Nivan Ferreira, Jorge Poco*

**Main category:** cs.HC

**Keywords:** spatiotemporal analysis, visual analytics, association rule mining

**Relevance Score:** 5

**TL;DR:** STRive is a visual analytics system that helps users explore spatial and temporal patterns in spatiotemporal datasets using Association Rule Mining, offering two modes of analysis for interpreting complex data insights.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Effective analysis of spatiotemporal data is essential for understanding real-world phenomena and making informed decisions, yet existing tools often fail to combine spatial and temporal analysis.

**Method:** The methodology involves three steps: generating association rules from spatiotemporal datasets, clustering these rules, and providing interactive visualizations to explore the relationships.

**Key Contributions:**

	1. Introduction of STRive for spatiotemporal data analysis
	2. Combines Association Rule Mining with interactive visualization
	3. Evaluation through real-world datasets to showcase effectiveness

**Result:** The evaluation through case studies on fatal vehicle accidents and urban crime demonstrates STRive's capability in discovering and analyzing interpretable patterns from complex datasets.

**Limitations:** 

**Conclusion:** STRive enables users to uncover actionable insights by employing a dual-mode visualization of rule clusters and individual rules, demonstrating its effectiveness in spatiotemporal analysis.

**Abstract:** Effectively analyzing spatiotemporal data plays a central role in understanding real-world phenomena and informing decision-making. Capturing the interaction between spatial and temporal dimensions also helps explain the underlying structure of the data. However, most datasets do not reveal attribute relationships, requiring additional algorithms to extract meaningful patterns. Existing visualization tools often focus either on attribute relationships or spatiotemporal analysis, but rarely support both simultaneously. In this paper, we present STRive (SpatioTemporal Rule Interactive Visual Explorer), a visual analytics system that enables users to uncover and explore spatial and temporal patterns in data. At the core of STRive lies Association Rule Mining (ARM), which we apply to spatiotemporal datasets to generate interpretable and actionable insights. We combine ARM with multiple interactive mechanisms to analyze the extracted relationships. Association rules serve as interpretable guidance mechanisms for visual analytics by highlighting the meaningful aspects of the data that users should investigate. Our methodology includes three key steps: rule generation, rule clustering, and interactive visualization. STRive offers two modes of analysis. The first operates at the rule cluster level and includes four coordinated views, each showing a different facet of a cluster, including its temporal and spatial behavior. The second mode mirrors the first but focuses on individual rules within a selected cluster. We evaluate the effectiveness of STRive through two case studies involving real-world datasets -- fatal vehicle accidents and urban crime. Results demonstrate the system's ability to support the discovery and analysis of interpretable patterns in complex spatiotemporal contexts.

</details>


### [2] [Designing a Lightweight GenAI Interface for Visual Data Analysis](https://arxiv.org/abs/2509.02878)

*Ratanond Koonchanok, Alex Kale, Khairi Reda*

**Main category:** cs.HC

**Keywords:** Generative AI, visual analysis, statistical modeling

**Relevance Score:** 8

**TL;DR:** This paper presents a hybrid visual analysis system that integrates Generative AI (GenAI) to assist in statistical modeling while maintaining transparency and user control.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing systems that overly rely on LLMs, causing issues like hallucination and opaque reasoning.

**Method:** The system uses GenAI for translating natural language intents into statistical formulations, while employing interactive visualizations to reveal model behavior and support exploratory analysis.

**Key Contributions:**

	1. Hybrid approach combining GenAI with visual analysis
	2. Ensures correctness and interpretability through a structured R-based backend
	3. Broadens access to modeling tools without sacrificing rigor.

**Result:** The system enables users to fit models, perform diagnostics, and test hypotheses within a structured R-based backend, thereby ensuring correctness and interpretability of results.

**Limitations:** 

**Conclusion:** The integration of GenAI in a controlled way enhances accessibility to statistical tools while upholding research rigor, with discussions on the implications for future developments.

**Abstract:** Recent advances in Generative AI have transformed how users interact with data analysis through natural language interfaces. However, many systems rely too heavily on LLMs, creating risks of hallucination, opaque reasoning, and reduced user control. We present a hybrid visual analysis system that integrates GenAI in a constrained, high-level role to support statistical modeling while preserving transparency and user agency. GenAI translates natural language intent into formal statistical formulations, while interactive visualizations surface model behavior, residual patterns, and hypothesis comparisons to guide iterative exploration. Model fitting, diagnostics, and hypothesis testing are delegated entirely to a structured R-based backend, ensuring correctness, interpretability, and reproducibility. By combining GenAI-assisted intent translation with visualization-driven reasoning, our approach broadens access to modeling tools without compromising rigor. We present an example use case of the tool and discuss challenges and opportunities for future research.

</details>


### [3] [The Basic B*** Effect: The Use of LLM-based Agents Reduces the Distinctiveness and Diversity of People's Choices](https://arxiv.org/abs/2509.02910)

*Sandra C. Matz, C. Blaine Horton, Sofie Goethals*

**Main category:** cs.HC

**Keywords:** large language models, identity, human agency, distinctiveness, diversity

**Relevance Score:** 8

**TL;DR:** Study on how LLMs impact personal choices and identity, revealing trade-offs between distinctiveness and diversity in user behavior.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effects of large language models on identity-defining choices made by individuals.

**Method:** Analyzed 110,000 social media choices from 1,000 U.S. users, comparing choices made with a generic agent, a personalized agent, and a human baseline.

**Key Contributions:**

	1. Introduced the concept of distinctiveness-diversity trade-offs in agentic LLM use.
	2. Provided empirical evidence on how LLMs affect interpersonal distinctiveness and intrapersonal diversity.
	3. Highlighted the need for careful design of AI systems to safeguard diversity in human experience.

**Result:** Both LLM agents lead to less distinct choices, with personalized agents reducing distinctiveness more than generic agents but compressing the breadth of individual preferences.

**Limitations:** Study focused on social media choices; more research needed across diverse contexts and decision-making scenarios.

**Conclusion:** The design of AI systems must consider the impact on human agency and the trade-offs between distinctiveness and diversity in user choices.

**Abstract:** Large language models (LLMs) increasingly act on people's behalf: they write emails, buy groceries, and book restaurants. While the outsourcing of human decision-making to AI can be both efficient and effective, it raises a fundamental question: how does delegating identity-defining choices to AI reshape who people become? We study the impact of agentic LLMs on two identity-relevant outcomes: interpersonal distinctiveness - how unique a person's choices are relative to others - and intrapersonal diversity - the breadth of a single person's choices over time. Using real choices drawn from social-media behavior of 1,000 U.S. users (110,000 choices in total), we compare a generic and personalized agent to a human baseline. Both agents shift people's choices toward more popular options, reducing the distinctiveness of their behaviors and preferences. While the use of personalized agents tempers this homogenization (compared to the generic AI), it also more strongly compresses the diversity of people's preference portfolios by narrowing what they explore across topics and psychological affinities. Understanding how AI agents might flatten human experience, and how using generic versus personalized agents involves distinctiveness-diversity trade-offs, is critical for designing systems that augment rather than constrain human agency, and for safeguarding diversity in thought, taste, and expression.

</details>


### [4] [Demonstrating Visual Information Manipulation Attacks in Augmented Reality: A Hands-On Miniature City-Based Setup](https://arxiv.org/abs/2509.02933)

*Yanming Xiu, Maria Gorlatova*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Visual Information Manipulation, User Interaction, Security, User Study

**Relevance Score:** 7

**TL;DR:** This paper discusses the impact of Visual Information Manipulation attacks on augmented reality interactions, using a hands-on demo to illustrate its effects.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the security vulnerabilities in augmented reality systems caused by Visual Information Manipulation (VIM) attacks, which can confuse users and lead to poor decision-making.

**Method:** A hands-on demo using a miniature city setup and the Meta Quest 3 allows users to interact with manipulated AR content to observe the effects of VIM attacks.

**Key Contributions:**

	1. Hands-on demonstration of VIM attacks in a controlled environment.
	2. Identification of user confusion as a consequence of manipulated AR content.
	3. Proposal for future user studies and cross-platform testing to validate findings.

**Result:** The demo illustrates significant user confusion and misdirected actions as a result of VIM attacks on augmented reality content.

**Limitations:** 

**Conclusion:** The study underscores the necessity for effective security measures in AR systems to protect users from vulnerabilities introduced by VIM attacks.

**Abstract:** Augmented reality (AR) enhances user interaction with the real world but also presents vulnerabilities, particularly through Visual Information Manipulation (VIM) attacks. These attacks alter important real-world visual cues, leading to user confusion and misdirected actions. In this demo, we present a hands-on experience using a miniature city setup, where users interact with manipulated AR content via the Meta Quest 3. The demo highlights the impact of VIM attacks on user decision-making and underscores the need for effective security measures in AR systems. Future work includes a user study and cross-platform testing.

</details>


### [5] [OPRA-Vis: Visual Analytics System to Assist Organization-Public Relationship Assessment with Large Language Models](https://arxiv.org/abs/2509.03164)

*Sangbong Yoo, Seongbum Seo, Chanyoung Yoon, Hyelim Lee, Jeong-Nam Kim, Chansoo Kim, Yun Jang, Takanori Fujiwara*

**Main category:** cs.HC

**Keywords:** Organization-Public Relationship Assessment, Large Language Models, Public Relations Analysis

**Relevance Score:** 8

**TL;DR:** OPRA-Vis is a visual analytics system that utilizes Large Language Models for public relations analysis without the need for large labeled datasets, enabling effective analysis of public opinions and providing insightful visualizations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Organizations need to understand public opinions to maintain positive relationships, but adapting LLMs requires extensive labeled datasets which are hard to obtain in PR contexts.

**Method:** The OPRA-Vis framework employs Chain-of-Thought prompting to utilize LLMs for analyzing public opinion data while directly incorporating PR expertise into the reasoning process.

**Key Contributions:**

	1. Introduction of OPRA-Vis as a visual analytics system for PR analysis using LLMs.
	2. Utilization of Chain-of-Thought prompting to integrate PR expertise into LLM reasoning.
	3. Visualizations that allow users to explore and critique LLM decisions.

**Result:** The effectiveness of OPRA-Vis was demonstrated with two real-world use cases, showing its usability, effectiveness, and positive expert feedback, as well as performance comparisons with alternative LLMs.

**Limitations:** 

**Conclusion:** OPRA-Vis facilitates more accessible and effective public relations analysis by minimizing the dependency on large labeled datasets while still leveraging the power of LLMs.

**Abstract:** Analysis of public opinions collected from digital media helps organizations maintain positive relationships with the public. Such public relations (PR) analysis often involves assessing opinions, for example, measuring how strongly people trust an organization. Pre-trained Large Language Models (LLMs) hold great promise for supporting Organization-Public Relationship Assessment (OPRA) because they can map unstructured public text to OPRA dimensions and articulate rationales through prompting. However, adapting LLMs for PR analysis typically requires fine-tuning on large labeled datasets, which is both labor-intensive and knowledge-intensive, making it difficult for PR researchers to apply these models. In this paper, we present OPRA-Vis, a visual analytics system that leverages LLMs for OPRA without requiring extensive labeled data. Our framework employs Chain-of-Thought prompting to guide LLMs in analyzing public opinion data by incorporating PR expertise directly into the reasoning process. Furthermore, OPRA-Vis provides visualizations that reveal the clues and reasoning paths used by LLMs, enabling users to explore, critique, and refine model decisions. We demonstrate the effectiveness of OPRA-Vis through two real-world use cases and evaluate it quantitatively, through comparisons with alternative LLMs and prompting strategies, and qualitatively, through assessments of usability, effectiveness, and expert feedback.

</details>


### [6] [Beyond Words: Interjection Classification for Improved Human-Computer Interaction](https://arxiv.org/abs/2509.03181)

*Yaniv Goren, Yuval Cohen, Alexander Apartsin, Yehudit Aperstein*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Interjection Classification, Automatic Speech Recognition, Deep Learning, Data Augmentation

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel task in human-computer interaction focused on classifying interjections like "mmm" and "hmm," addressing their dismissal in ASR systems by presenting a dedicated dataset and baseline deep learning model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the natural dialogue in human-computer interactions by recognizing the importance of interjections, which are usually ignored by ASR engines.

**Method:** A dataset of interjection signals was collected and annotated for interjection classification. A baseline deep learning model was trained and evaluated using this dataset, with performance enhancements through tempo and pitch transformation techniques for data augmentation.

**Key Contributions:**

	1. Introduction of a novel interjection classification task in ASR systems
	2. Publication of a dedicated interjection dataset for research use
	3. Development of a baseline deep learning model with performance enhancement techniques

**Result:** The baseline model achieved improved classification accuracy thanks to the data augmentation strategies, indicating the potential of these techniques for creating more robust models.

**Limitations:** 

**Conclusion:** The work highlights the significance of interjections in conversation and provides valuable resources, including an interjection dataset, a Python library for augmentation, and evaluation scripts, to facilitate further research in this area.

**Abstract:** In the realm of human-computer interaction, fostering a natural dialogue between humans and machines is paramount. A key, often overlooked, component of this dialogue is the use of interjections such as "mmm" and "hmm". Despite their frequent use to express agreement, hesitation, or requests for information, these interjections are typically dismissed as "non-words" by Automatic Speech Recognition (ASR) engines. Addressing this gap, we introduce a novel task dedicated to interjection classification, a pioneer in the field to our knowledge. This task is challenging due to the short duration of interjection signals and significant inter- and intra-speaker variability. In this work, we present and publish a dataset of interjection signals collected specifically for interjection classification. We employ this dataset to train and evaluate a baseline deep learning model. To enhance performance, we augment the training dataset using techniques such as tempo and pitch transformation, which significantly improve classification accuracy, making models more robust. The interjection dataset, a Python library for the augmentation pipeline, baseline model, and evaluation scripts, are available to the research community.

</details>


### [7] [Finding My Way: Influence of Different Audio Augmented Reality Navigation Cues on User Experience and Subjective Usefulness](https://arxiv.org/abs/2509.03199)

*Sina Hinzmann, Francesco Vona, Juliane Henning, Mohamed Amer, Omar Abdellatif, Tanja Kojic, Jan-Niklas Voigt-Antons*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Auditory Cues, User Experience, Navigation, AR Systems

**Relevance Score:** 7

**TL;DR:** The study explores how different types of auditory cues affect user experience during AR navigation tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the role of auditory cues in enhancing navigation experiences in augmented reality (AR) applications.

**Method:** Twenty participants used a Meta Quest 3 headset to navigate five outdoor routes with various audio cues, including Artificial Sounds, Nature Sounds, Spearcons, Musical Instruments, and Auditory Icons. Subjective evaluations were collected to assess perceived effectiveness and user experience of each sound type.

**Key Contributions:**

	1. Examines the impact of audio cue types on user navigation in AR environments.
	2. Highlights the importance of novelty and stimulation in auditory feedback design.
	3. Provides empirical insights for designing effective AR navigation systems.

**Result:** Significant differences were found in perceived novelty and stimulation among sound types. Artificial Sounds and Musical Instruments were rated higher for novelty, while Artificial Sounds outperformed Spearcons in stimulation. Overall preference was split between Nature Sounds and Artificial Sounds.

**Limitations:** Limited sample size and specific context (outdoor navigation), may not generalize to all AR applications.

**Conclusion:** Incorporating novelty and user engagement into auditory feedback design can enhance the effectiveness of AR navigation systems.

**Abstract:** As augmented reality (AR) becomes increasingly prevalent in mobile and context-aware applications, the role of auditory cues in guiding users through physical environments is becoming critical. This study investigates the effectiveness and user experience of various categories of audio cues, including fully non-verbal sounds and speech-derived Spearcons, during outdoor navigation tasks using the Meta Quest 3 headset. Twenty participants navigated five outdoor routes using audio-only cue types: Artificial Sounds, Nature Sounds, Spearcons, Musical Instruments, and Auditory Icons. Subjective evaluations were collected to assess the perceived effectiveness and user experience of each sound type. Results revealed significant differences in perceived novelty and stimulation across sound types. Artificial Sounds and Musical Instruments were rated higher than Spearcons in novelty, while Artificial Sounds were also rated higher than Spearcons in stimulation. Overall preference was evenly split between Nature Sounds and Artificial Sounds. These findings suggest that incorporating aspects of novelty and user engagement in auditory feedback design may enhance the effectiveness of AR navigation systems.

</details>


### [8] [Card Sorting with Fewer Cards and the Same Mental Models? A Re-examination of an Established Practice](https://arxiv.org/abs/2509.03232)

*Eduard Kuric, Peter Demcak, Matus Krajcovic*

**Main category:** cs.HC

**Keywords:** card sorting, mental models, randomized subsets, sample size, personality traits

**Relevance Score:** 6

**TL;DR:** This paper investigates the impact of using randomized subsets in card sorting on mental models, comparing these to full card sets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically examine the effects of using fewer randomly selected cards in card sorting on data outcomes, given its common use in gauging mental models.

**Method:** An experiment with 160 participants compared full card sets to randomized 60% card subsets, analyzing sample size requirements and the effects of individual cognitive factors.

**Key Contributions:**

	1. Demonstrated that randomized subsets can provide similar data outcomes as full sets in card sorting.
	2. Identified the need for larger sample sizes when using random subsets due to increased data variability.
	3. Showed the interaction of personality traits and cognitive reflection with card sorting outcomes.

**Result:** Randomized subsets yield comparable similarity matrices, but thematic patterns differ, indicating increased data variability necessitates larger sample sizes (25-35).

**Limitations:** Limited to specific personality traits measured; results may vary with different participant demographics.

**Conclusion:** The study highlights the importance of considering individual differences and study design when interpreting card sorting results, providing evidence-based practices.

**Abstract:** To keep card sorting with a lot of cards concise, a common strategy for gauging mental models involves presenting participants with fewer randomly selected cards instead of the full set. This is a decades-old practice, but its effects lacked systematic examination. To assess how randomized subsets affect data, we conducted an experiment with 160 participants. We compared results between full and randomized 60\% card sets, then analyzed sample size requirements and the impacts of individual personality and cognitive factors. Our results demonstrate that randomized subsets can yield comparable similarity matrices to standard card sorting, but thematic patterns in categories can differ. Increased data variability also warrants larger sample sizes (25-35 for 60% card subset). Results indicate that personality traits and cognitive reflection interact with card sorting. Our research suggests evidence-based practices for conducting card sorting while exposing the influence of study design and individual differences on measurement of mental models.

</details>


### [9] [Beyond Quantification: Navigating Uncertainty in Professional AI Systems](https://arxiv.org/abs/2509.03271)

*Sylvie Delacroix, Diana Robinson, Umang Bhatt, Jacopo Domenicucci, Jessica Montgomery, Gael Varoquaux, Carl Henrik Ek, Vincent Fortuin, Yulan He, Tom Diethe, Neill Campbell, Mennatallah El-Assady, Soren Hauberg, Ivana Dusparic, Neil Lawrence*

**Main category:** cs.HC

**Keywords:** large language models, uncertainty, participatory design, healthcare, education

**Relevance Score:** 8

**TL;DR:** The paper discusses how large language models (LLMs) impact decision-making in various professional fields and argues for richer expressions of uncertainty rather than simple quantification.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of large language models in fields like healthcare and education necessitates a better understanding of how to communicate uncertainty in decisions, as traditional quantification methods fall short.

**Method:** The paper advocates for participatory refinement processes where professional communities collaboratively define and shape the expression of uncertainty in their respective domains.

**Key Contributions:**

	1. Proposed participatory refinement processes for uncertainty communication.
	2. Emphasized the need for richer expressions of uncertainty in decision-making.
	3. Critiqued the limitations of current probabilistic measures in professional contexts.

**Result:** It highlights the inadequacy of probabilistic measures for capturing complex uncertainties faced by professionals, suggesting alternative approaches that foster collaborative sense-making.

**Limitations:** The paper does not provide empirical data to support the proposed methods.

**Conclusion:** The authors conclude that effectively communicating different forms of uncertainty requires collective input from professionals rather than relying solely on algorithmic solutions.

**Abstract:** The growing integration of large language models across professional domains transforms how experts make critical decisions in healthcare, education, and law. While significant research effort focuses on getting these systems to communicate their outputs with probabilistic measures of reliability, many consequential forms of uncertainty in professional contexts resist such quantification. A physician pondering the appropriateness of documenting possible domestic abuse, a teacher assessing cultural sensitivity, or a mathematician distinguishing procedural from conceptual understanding face forms of uncertainty that cannot be reduced to percentages. This paper argues for moving beyond simple quantification toward richer expressions of uncertainty essential for beneficial AI integration. We propose participatory refinement processes through which professional communities collectively shape how different forms of uncertainty are communicated. Our approach acknowledges that uncertainty expression is a form of professional sense-making that requires collective development rather than algorithmic optimization.

</details>


### [10] [More AI Assistance Reduces Cognitive Engagement: Examining the AI Assistance Dilemma in AI-Supported Note-Taking](https://arxiv.org/abs/2509.03392)

*Xinyue Chen, Kunlin Ruan, Kexin Phyllis Ju, Nathan Yap, Xu Wang*

**Main category:** cs.HC

**Keywords:** AI Assistance, Cognitive Engagement, Note-Taking, Human-Computer Interaction, Learning Outcomes

**Relevance Score:** 8

**TL;DR:** The paper investigates the impact of different levels of AI assistance in note-taking on user engagement and comprehension, finding that Intermediate AI support produced the best cognitive results despite users preferring more automated solutions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how varying AI support levels influence cognitive engagement and learning outcomes during note-taking. The research addresses the implications of using AI tools in cognitively demanding tasks.

**Method:** Conducted a within-subject experiment with 30 participants taking notes under three AI assistance conditions: Automated AI, Intermediate AI, and Minimal AI.

**Key Contributions:**

	1. Investigates the 'AI Assistance Dilemma' in note-taking.
	2. Demonstrates the trade-off between user preference for convenience and cognitive benefits of AI tools.
	3. Offers design implications for AI support in cognitive tasks.

**Result:** Intermediate AI resulted in the highest post-test scores, while Automated AI led to the lowest comprehension scores. Preferences leaned towards Automated AI despite its lower cognitive benefits.

**Limitations:** Limited sample size (N=30), focused solely on note-taking during lecture videos, potentially undermining external validity.

**Conclusion:** The findings suggest a need for careful design of AI assistance to enhance cognitive engagement, advocating for moderate AI support in tasks requiring cognitive effort.

**Abstract:** As AI tools become increasingly embedded in cognitively demanding tasks such as note-taking, questions remain about whether they enhance or undermine cognitive engagement. This paper examines the "AI Assistance Dilemma" in note-taking, investigating how varying levels of AI support affect user engagement and comprehension. In a within-subject experiment, we asked participants (N=30) to take notes during lecture videos under three conditions: Automated AI (high assistance with structured notes), Intermediate AI (moderate assistance with real-time summary, and Minimal AI (low assistance with transcript). Results reveal that Intermediate AI yields the highest post-test scores and Automated AI the lowest. Participants, however, preferred the automated setup due to its perceived ease of use and lower cognitive effort, suggesting a discrepancy between preferred convenience and cognitive benefits. Our study provides insights into designing AI assistance that preserves cognitive engagement, offering implications for designing moderate AI support in cognitive tasks.

</details>


### [11] [EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared Shadow Casting](https://arxiv.org/abs/2509.03430)

*Vimal Mollyn, Nathan DeVrio, Chris Harrison*

**Main category:** cs.HC

**Keywords:** mixed reality, human-computer interaction, touch detection, infrared sensing, structured shadows

**Relevance Score:** 7

**TL;DR:** A new headset-integrated technique for detecting touch events on uninstrumented surfaces using structured shadows.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the interaction experience in mixed reality systems by detecting touch events on everyday surfaces, enhancing performance and ergonomics.

**Method:** The proposed technique employs a computer-triggered camera and infrared emitters to create structured shadows for estimating hover distance and touch contact.

**Key Contributions:**

	1. Introduction of a headset-integrated method for touch detection on uninstrumented surfaces
	2. Demonstrated high accuracy and low error in hover distance estimation
	3. Validated effectiveness across varied materials and environmental conditions.

**Result:** Achieved a mean error of 6.9 mm in hover distance estimation and 98.0% accuracy in touch contact detection.

**Limitations:** 

**Conclusion:** The technique is versatile, performing well under various conditions including different surface materials and lighting environments.

**Abstract:** The ability to detect touch events on uninstrumented, everyday surfaces has been a long-standing goal for mixed reality systems. Prior work has shown that virtual interfaces bound to physical surfaces offer performance and ergonomic benefits over tapping at interfaces floating in the air. A wide variety of approaches have been previously developed, to which we contribute a new headset-integrated technique called \systemname. We use a combination of a computer-triggered camera and one or more infrared emitters to create structured shadows, from which we can accurately estimate hover distance (mean error of 6.9~mm) and touch contact (98.0\% accuracy). We discuss how our technique works across a range of conditions, including surface material, interaction orientation, and environmental lighting.

</details>


### [12] [SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using UWB and IMU Data](https://arxiv.org/abs/2509.03451)

*Nathan DeVrio, Vimal Mollyn, Chris Harrison*

**Main category:** cs.HC

**Keywords:** arm pose tracking, ultra-wideband, smartphone, smartwatch, user privacy

**Relevance Score:** 8

**TL;DR:** The paper presents a method for tracking arm pose using a smartphone and smartwatch, leveraging ultra-wideband technology for accuracy.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Tracking arm pose is crucial for various applications but existing methods have privacy concerns or require multiple devices.

**Method:** The approach uses smartphones and smartwatches together, utilizing ultra-wideband functionality to measure absolute distance and complement inertial data.

**Key Contributions:**

	1. Utilization of ultra-wideband technology for arm pose estimation
	2. Combining smartphone and smartwatch for accurate tracking
	3. No need for user training data

**Result:** The system estimates the wrist and elbow joints with a median positional error of 11.0 cm, and does so without needing user-provided training data.

**Limitations:** 

**Conclusion:** This method provides a practical and privacy-conscious solution for arm pose tracking using commonly available consumer devices.

**Abstract:** The ability to track a user's arm pose could be valuable in a wide range of applications, including fitness, rehabilitation, augmented reality input, life logging, and context-aware assistants. Unfortunately, this capability is not readily available to consumers. Systems either require cameras, which carry privacy issues, or utilize multiple worn IMUs or markers. In this work, we describe how an off-the-shelf smartphone and smartwatch can work together to accurately estimate arm pose. Moving beyond prior work, we take advantage of more recent ultra-wideband (UWB) functionality on these devices to capture absolute distance between the two devices. This measurement is the perfect complement to inertial data, which is relative and suffers from drift. We quantify the performance of our software-only approach using off-the-shelf devices, showing it can estimate the wrist and elbow joints with a \hl{median positional error of 11.0~cm}, without the user having to provide training data.

</details>


### [13] [From Metrics to Meaning: Time to Rethink Evaluation in Human-AI Collaborative Design](https://arxiv.org/abs/2402.07911)

*Sean P. Walton, Ben J. Evans, Alma A. M. Rahat, James Stovold, Jakub Vincalek*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, Interactive intelligent systems, User engagement

**Relevance Score:** 7

**TL;DR:** The paper challenges conventional evaluation methods for human-AI collaborative systems, advocating for a multidimensional approach by analyzing a large field study of a co-creative design tool.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding of human engagement with AI systems in creative design and to propose improved evaluation methods for human-AI collaborations.

**Method:** A field study with 808 participants and a controlled lab study with 12 participants, examining the effects of an interactive evolutionary algorithm on design quality and user engagement.

**Key Contributions:**

	1. Introduction of a holistic evaluation approach for human-AI collaborative systems.
	2. Findings from a large field study demonstrating the impact of AI on design outcomes.
	3. Highlighting the importance of user engagement metrics beyond traditional design quality.

**Result:** Exposure to design suggestions from the MAP-Elites algorithm significantly improved cognitive and behavioral engagement, resulting in higher-quality design outcomes compared to a random control.

**Limitations:** The study is limited to specific design contexts and may not generalize across all human-AI interactions.

**Conclusion:** Conventional evaluation methods inadequately capture user engagement in human-AI systems; a holistic approach considering emotional, behavioral, and cognitive aspects is needed.

**Abstract:** As AI systems increasingly shape decision making in creative design contexts, understanding how humans engage with these tools has become a critical challenge for interactive intelligent systems research. This paper contributes a challenge to rethink how to evaluate human--AI collaborative systems, advocating for a more nuanced and multidimensional approach. Findings from one of the largest field studies to date (n = 808) of a human--AI co-creative system, The Genetic Car Designer, complemented by a controlled lab study (n = 12) are presented. The system is based on an interactive evolutionary algorithm where participants were tasked with designing a simple two dimensional representation of a car. Participants were exposed to galleries of design suggestions generated by an intelligent system, MAP--Elites, and a random control. Results indicate that exposure to galleries generated by MAP--Elites significantly enhanced both cognitive and behavioural engagement, leading to higher-quality design outcomes. Crucially for the wider community, the analysis reveals that conventional evaluation methods, which often focus on solely behavioural and design quality metrics, fail to capture the full spectrum of user engagement. By considering the human--AI design process as a changing emotional, behavioural and cognitive state of the designer, we propose evaluating human--AI systems holistically and considering intelligent systems as a core part of the user experience -- not simply a back end tool.

</details>


### [14] [Talk, Listen, Connect: How Humans and AI Evaluate Empathy in Responses to Emotionally Charged Narratives](https://arxiv.org/abs/2409.15550)

*Mahnaz Roshanaei, Rezvaneh Rezapour, Magy Seif El-Nasr*

**Main category:** cs.HC

**Keywords:** empathy, human-AI interaction, emotionally responsive AI, social support, AI design ethics

**Relevance Score:** 8

**TL;DR:** This study investigates how empathy is perceived in responses from AI versus humans, revealing differences in emotional expression and sensitivity to context.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of emotionally responsive AI systems in supporting social and emotional well-being, particularly in situations where human interaction is limited.

**Method:** The study utilized personal narratives to compare empathy evaluations between AI-generated responses and human responses, focusing on persona attributes and narrative qualities.

**Key Contributions:**

	1. Examining empathy perception in AI vs human interactions
	2. Identifying the impact of persona attributes on empathy ratings
	3. Informing design principles for ethically aware AI tools for social support

**Result:** Results indicate that humans are more sensitive to emotional nuances and shared experiences in narratives, while AI responses often lack this subtlety in empathy expression.

**Limitations:** AI responses often miss nuanced emotional expressions and deeper context-driven empathy, potentially leading to user dissatisfaction.

**Conclusion:** The findings emphasize the need for carefully designed emotionally intelligent AI systems that can meaningfully engage with users, promoting social connection.

**Abstract:** Social interactions promote well-being, yet barriers like geographic distance, time limitations, and mental health conditions can limit face-to-face interactions. Emotionally responsive AI systems, such as chatbots, offer new opportunities for social and emotional support, but raise critical questions about how empathy is perceived and experienced in human-AI interactions. This study examines how empathy is evaluated in AI-generated versus human responses. Using personal narratives, we explored how persona attributes (e.g., gender, empathic traits, shared experiences) and story qualities affect empathy ratings. We compared responses from standard and fine-tuned AI models with human judgments. Results show that while humans are highly sensitive to emotional vividness and shared experience, AI-responses are less influenced by these cues, often lack nuance in empathic expression. These findings highlight challenges in designing emotionally intelligent systems that respond meaningfully across diverse users and contexts, and informs the design of ethically aware tools to support social connection and well-being.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [15] [DrDiff: Dynamic Routing Diffusion with Hierarchical Attention for Breaking the Efficiency-Quality Trade-off](https://arxiv.org/abs/2509.02785)

*Jusheng Zhang, Yijia Fan, Kaitong Cai, Zimeng Huang, Xiaofei Sun, Jian Wang, Chengpei Tang, Keze Wang*

**Main category:** cs.CL

**Keywords:** long-text generation, dynamic scheduling, sparse attention, absorption guidance, ML

**Relevance Score:** 8

**TL;DR:** DrDiff is a novel framework for efficient long-text generation that improves efficiency and quality through expert scheduling, hierarchical sparse attention, and absorption guidance optimization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the efficiency-quality trade-off in long-text generation tasks, aiming to enhance the performance of text generation models.

**Method:** The method includes a dynamic expert scheduling mechanism, a Hierarchical Sparse Attention (HSA) mechanism, and a soft absorption guidance optimization strategy.

**Key Contributions:**

	1. Dynamic expert scheduling for text complexity-based resource allocation
	2. Hierarchical Sparse Attention mechanism for reducing computational complexity
	3. Soft absorption guidance optimization to speed up generation

**Result:** DrDiff demonstrates superior performance on long-text generation benchmarks compared to existing state-of-the-art methods.

**Limitations:** 

**Conclusion:** The proposed DrDiff framework significantly improves generation speed and maintains high model performance, providing a viable solution for efficient long-text generation.

**Abstract:** This paper introduces DrDiff, a novel framework for long-text generation that overcomes the efficiency-quality trade-off through three core technologies. First, we design a dynamic expert scheduling mechanism that intelligently allocates computational resources during the diffusion process based on text complexity, enabling more efficient handling of text generation tasks of varying difficulty. Second, we introduce a Hierarchical Sparse Attention (HSA) mechanism that adaptively adjusts attention patterns according to a variety of input lengths, reducing computational complexity from O($n^2$) to O($n$) while maintaining model performance. Finally, we propose a soft absorption guidance optimization strategy that combines with DPM-solver++ to reduce diffusion steps, significantly improving generation speed. Comprehensive experiments on various long-text generation benchmarks demonstrate the superiority of our DrDiff over the existing SOTA methods.

</details>


### [16] [SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR](https://arxiv.org/abs/2509.02830)

*Pu Wang, Shinji Watanabe, Hugo Van hamme*

**Main category:** cs.CL

**Keywords:** Parameter-efficient fine-tuning, speech recognition, domain adaptation

**Relevance Score:** 5

**TL;DR:** This paper benchmarks various parameter-efficient fine-tuning methods for speech recognition, including a new structured SVD-guided approach to improve domain adaptation efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To integrate and benchmark parameter-efficient fine-tuning methods for better performance in speech recognition tasks.

**Method:** The paper introduces structured SVD-guided (SSVD) fine-tuning, which selectively adjusts specific singular vectors during training to enhance efficiency and domain adaptation.

**Key Contributions:**

	1. First comprehensive benchmarking of PEFT methods in speech recognition
	2. Introduction of structured SVD-guided fine-tuning for efficient domain adaptation
	3. Evaluation of multiple PEFT methods across various model scales.

**Result:** The methods were evaluated on tasks involving child speech and dialectal variations, demonstrating improved performance and efficiency across different model sizes.

**Limitations:** The focus is predominantly on speech applications, potentially limiting applicability to other areas like language or vision.

**Conclusion:** The proposed SSVD fine-tuning method provides a robust approach for domain adaptation in speech recognition while maintaining low parameter counts.

**Abstract:** Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution for adapting large foundation models. While low-rank adaptation (LoRA) is widely used in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA, PiSSA, and SVFT, are developed mainly for language and vision tasks, with limited validation in speech. This work presents the first comprehensive integration and benchmarking of these PEFT methods within ESPnet. We further introduce structured SVD-guided (SSVD) fine-tuning, which selectively rotates input-associated right singular vectors while keeping output-associated vectors fixed to preserve semantic mappings. This design enables robust domain adaptation with minimal trainable parameters and improved efficiency. We evaluate all methods on domain-shifted speech recognition tasks, including child speech and dialectal variation, across model scales from 0.1B to 2B. All implementations are released in ESPnet to support reproducibility and future work.

</details>


### [17] [Clustering Discourses: Racial Biases in Short Stories about Women Generated by Large Language Models](https://arxiv.org/abs/2509.02834)

*Gustavo Bonil, Jo√£o Gondim, Marina dos Santos, Simone Hashiguti, Helena Maia, Nadia Silva, Helio Pedrini, Sandra Avila*

**Main category:** cs.CL

**Keywords:** Large language models, Discourse analysis, Narrative construction, Social inequalities, Machine learning

**Relevance Score:** 6

**TL;DR:** This study explores narrative constructions involving Black and white women in Portuguese short stories generated by LLaMA 3.2-3B, revealing social and historical inequalities.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how large language models construct narratives that reflect historical inequalities, especially regarding gender and race, in automatically generated texts.

**Method:** Analyzed 2100 texts generated using LLaMA 3.2-3B, applying computational methods to group semantically similar stories for qualitative analysis.

**Key Contributions:**

	1. Introduces a framework for analyzing AI-generated narratives with a focus on gender and race.
	2. Identifies key discursive representations in AI-generated stories about women.
	3. Proposes a novel integrated methodology combining qualitative and computational analysis.

**Result:** The analysis identified three main discursive representations: social overcoming, ancestral mythification, and subjective self-realization, highlighting how these texts reinforce historical inequalities.

**Limitations:** Focuses specifically on narratives generated in Portuguese, which may limit generalizability to other languages or contexts.

**Conclusion:** The study suggests integrating machine learning techniques with qualitative discourse analysis to better understand how narratives in AI-generated texts reflect social issues.

**Abstract:** This study investigates how large language models, in particular LLaMA 3.2-3B, construct narratives about Black and white women in short stories generated in Portuguese. From 2100 texts, we applied computational methods to group semantically similar stories, allowing a selection for qualitative analysis. Three main discursive representations emerge: social overcoming, ancestral mythification and subjective self-realization. The analysis uncovers how grammatically coherent, seemingly neutral texts materialize a crystallized, colonially structured framing of the female body, reinforcing historical inequalities. The study proposes an integrated approach, that combines machine learning techniques with qualitative, manual discourse analysis.

</details>


### [18] [IDEAlign: Comparing Large Language Models to Human Experts in Open-ended Interpretive Annotations](https://arxiv.org/abs/2509.02855)

*Hyunji Nam, Lucia Langlois, James Malamut, Mei Tan, Dorottya Demszky*

**Main category:** cs.CL

**Keywords:** Large language models, interpretive annotation, evaluation metrics, education, expert judgments

**Relevance Score:** 9

**TL;DR:** The paper introduces IDEAlgin, a benchmarking paradigm for evaluating the similarity of LLM-generated annotations to expert human annotations in interpretive tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of validated measures for assessing the alignment of LLM-generated annotations with expert judgments, which is crucial for their effective deployment in tasks like educational feedback and thematic analysis.

**Method:** IDEAlgin uses a 'pick-the-odd-one-out' triplet judgment task to capture expert similarity ratings and compares various similarity metrics including vector-based ones and LLM-as-a-judge against human benchmarks.

**Key Contributions:**

	1. Introduction of IDEAlgin for evaluating interpretive annotation by LLMs
	2. Demonstration of significant improvements in expert alignment using IDEAlgin
	3. Critique of existing vector-based metrics in measuring nuanced similarity

**Result:** The study reveals that traditional vector-based metrics are inadequate in capturing expert nuance, whereas IDEAlgin significantly boosts alignment with expert responses (9-30% improvement).

**Limitations:** 

**Conclusion:** IDEAlgin is positioned as a vital tool for scaling the evaluation of LLMs in interpretive annotation tasks, promoting responsible utilization in educational contexts and beyond.

**Abstract:** Large language models (LLMs) are increasingly applied to open-ended, interpretive annotation tasks, such as thematic analysis by researchers or generating feedback on student work by teachers. These tasks involve free-text annotations requiring expert-level judgments grounded in specific objectives (e.g., research questions or instructional goals). Evaluating whether LLM-generated annotations align with those generated by expert humans is challenging to do at scale, and currently, no validated, scalable measure of similarity in ideas exists. In this paper, we (i) introduce the scalable evaluation of interpretive annotation by LLMs as a critical and understudied task, (ii) propose IDEAlgin, an intuitive benchmarking paradigm for capturing expert similarity ratings via a "pick-the-odd-one-out" triplet judgment task, and (iii) evaluate various similarity metrics, including vector-based ones (topic models, embeddings) and LLM-as-a-judge via IDEAlgin, against these human benchmarks. Applying this approach to two real-world educational datasets (interpretive analysis and feedback generation), we find that vector-based metrics largely fail to capture the nuanced dimensions of similarity meaningful to experts. Prompting LLMs via IDEAlgin significantly improves alignment with expert judgments (9-30% increase) compared to traditional lexical and vector-based metrics. These results establish IDEAlgin as a promising paradigm for evaluating LLMs against open-ended expert annotations at scale, informing responsible deployment of LLMs in education and beyond.

</details>


### [19] [A-SEA3L-QA: A Fully Automated Self-Evolving, Adversarial Workflow for Arabic Long-Context Question-Answer Generation](https://arxiv.org/abs/2509.02864)

*Kesen Wang, Daulet Toibazar, Pedro J. Moreno*

**Main category:** cs.CL

**Keywords:** Arabic, Question-Answer Generation, Large Vision Language Models, Self-evolving Workflow, Benchmark

**Relevance Score:** 5

**TL;DR:** An automated, self-evolving workflow for long-context QA generation in Arabic using LVLMs to improve performance without human intervention.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance question-answering capabilities on long-context Arabic documents by creating an automated workflow that refines itself dynamically.

**Method:** The system utilizes multiple LVLMs including a question generator, answer generators, and an evaluator, working in a feedback loop to continuously learn and improve output quality.

**Key Contributions:**

	1. Introduction of a self-evolving adversarial workflow for QA in Arabic
	2. Release of AraLongBench, a benchmark for Arabic question-answering
	3. Demonstration of significant performance improvements over existing static methods

**Result:** The proposed workflow significantly outperforms static pipelines and boosts comprehension in long-context scenarios for Arabic LVLMs.

**Limitations:** 

**Conclusion:** The release of AraLongBench offers a benchmark for Arabic QA tasks and demonstrates the efficacy of a fully automated, agentic workflow.

**Abstract:** We present an end-to-end, self-evolving adversarial workflow for long-context Question-Answer (QA) Generation in Arabic. By orchestrating multiple specialized LVLMs: a question generator, an evaluator, and a swarm of answer generators, our system iteratively refines its own performance without any human intervention. Starting from raw, multi-page Arabic documents across diverse domains, the question generator produces fine-grained, context-aware queries to be tackled by the answer generator swarm, and the evaluator assesses and feeds back quality metrics. This closed-loop cycle enables continuous learning: low-confidence outputs trigger automated re-generation and model updates, progressively enhancing question difficulty and relevance. Moreover, we set the quality metrics as a tunable hyperparameter, enabling question generation at controllable and customizable difficulty levels. We release AraLongBench, a large-scale Arabic benchmark of single- and multi-page challenges spanning hundreds of pages, and demonstrate that our self-evolving workflow substantially outperform static pipelines, markedly boosting the long-context comprehension capabilities of leading Arabic Large Vision Language Models (LVLMs). Lastly, we also meticulously architect a fully automated agentic workflow for long-context Arabic document collection.

</details>


### [20] [Advancing Minority Stress Detection with Transformers: Insights from the Social Media Datasets](https://arxiv.org/abs/2509.02908)

*Santosh Chapagain, Cory J Cascalheira, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi, Jillian R. Scheer*

**Main category:** cs.CL

**Keywords:** minority stress, transformer models, graph augmentation, machine learning, public health

**Relevance Score:** 8

**TL;DR:** Evaluation of transformer-based architectures for detecting minority stress in online discourse, showing that graph-augmented models improve performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the health disparities faced by sexual and gender minority groups and evaluate the effectiveness of machine learning models in detecting minority stress in online discourse.

**Method:** Benchmarking multiple transformer models (ELECTRA, BERT, RoBERTa, BART) against traditional machine learning and graph-augmented models, testing zero-shot and few-shot learning on Reddit corpora.

**Key Contributions:**

	1. First comprehensive evaluation of transformer architectures for minority stress detection
	2. Demonstrated improvement in detection performance with graph-augmented models
	3. Highlighted importance of incorporating social connectivity and conversational context.

**Result:** Graph-augmented transformers consistently outperform traditional models in detecting minority stress, especially with supervised fine-tuning that integrates relational context.

**Limitations:** Study limited to online discourse within specific Reddit corpora; generalizability to other contexts may vary.

**Conclusion:** Graph-enhanced transformers provide a reliable foundation for interventions in digital health and public health policy by effectively identifying linguistic markers of minority stress.

**Abstract:** Individuals from sexual and gender minority groups experience disproportionately high rates of poor health outcomes and mental disorders compared to their heterosexual and cisgender counterparts, largely as a consequence of minority stress as described by Meyer's (2003) model. This study presents the first comprehensive evaluation of transformer-based architectures for detecting minority stress in online discourse. We benchmark multiple transformer models including ELECTRA, BERT, RoBERTa, and BART against traditional machine learning baselines and graph-augmented variants. We further assess zero-shot and few-shot learning paradigms to assess their applicability on underrepresented datasets. Experiments are conducted on the two largest publicly available Reddit corpora for minority stress detection, comprising 12,645 and 5,789 posts, and are repeated over five random seeds to ensure robustness. Our results demonstrate that integrating graph structure consistently improves detection performance across transformer-only models and that supervised fine-tuning with relational context outperforms zero and few-shot approaches. Theoretical analysis reveals that modeling social connectivity and conversational context via graph augmentation sharpens the models' ability to identify key linguistic markers such as identity concealment, internalized stigma, and calls for support, suggesting that graph-enhanced transformers offer the most reliable foundation for digital health interventions and public health policy.

</details>


### [21] [English Pronunciation Evaluation without Complex Joint Training: LoRA Fine-tuned Speech Multimodal LLM](https://arxiv.org/abs/2509.02915)

*Taekyung Ahn, Hosung Nam*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Model, Low-Rank Adaptation, Automatic Pronunciation Assessment, Mispronunciation Detection, Computer-Assisted Pronunciation Training

**Relevance Score:** 8

**TL;DR:** A study on adapting a Multimodal Large Language Model (MLLM) using Low-Rank Adaptation (LoRA) for simultaneous Automatic Pronunciation Assessment (APA) and Mispronunciation Detection and Diagnosis (MDD).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a more efficient and integrated system for pronunciation assessment without complex training processes.

**Method:** The model was fine-tuned using LoRA on the Speechocean762 dataset to simultaneously perform APA and MDD, achieving strong correlation with human scores.

**Key Contributions:**

	1. Introduced a simplified method for integrating APA and MDD tasks using LoRA with a MLLM.
	2. Achieved strong performance metrics indicating effective pronunciation assessment for L2 learners.
	3. Demonstrated the potential for accessible CAPT technologies through efficient fine-tuning.

**Result:** The fine-tuned model exhibited a Pearson Correlation Coefficient (PCC > 0.7) with human scores and low Word and Phoneme Error Rates (both < 0.15).

**Limitations:** 

**Conclusion:** This research demonstrates that LoRA can effectively enable integrated pronunciation assessment without full model fine-tuning, making technology more accessible for English L2 learners.

**Abstract:** This study demonstrates that a Multimodal Large Language Model (MLLM) adapted via Low-Rank Adaptation (LoRA) can perform both Automatic Pronunciation Assessment (APA) and Mispronunciation Detection and Diagnosis (MDD) simultaneously. Leveraging Microsoft's Phi-4-multimodal-instruct, our fine-tuning method eliminates the need for complex architectural changes or separate training procedures conventionally required for these distinct tasks. Fine-tuned on the Speechocean762 dataset, the pronunciation evaluation scores predicted by the model exhibited a strong Pearson Correlation Coefficient (PCC > 0.7) with human-assigned scores, while achieving low Word Error Rate (WER) and Phoneme Error Rate (PER) (both < 0.15). Notably, fine-tuning only the LoRA layers was sufficient to achieve performance levels comparable to those achieved by fine-tuning all audio layers. This research highlights that an integrated pronunciation assessment system can be established by adapting large multimodal models without full fine-tuning, utilizing a significantly simpler training methodology compared to previous joint models designed for simultaneous APA and MDD. This efficient LoRA-based approach paves the way for more accessible, integrated, and effective Computer-Assisted Pronunciation Training (CAPT) technologies for English L2 learners.

</details>


### [22] [Decoding the Rule Book: Extracting Hidden Moderation Criteria from Reddit Communities](https://arxiv.org/abs/2509.02926)

*Youngwoo Kim, Himanshu Beniwal, Steven L. Johnson, Thomas Hartvigsen*

**Main category:** cs.CL

**Keywords:** content moderation, implicit criteria, online communities, moderation patterns, neural models

**Relevance Score:** 7

**TL;DR:** This paper introduces a method for extracting implicit moderation criteria from historical moderation data in online communities, providing insights into community-specific standards.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effective content moderation requires explicit classification criteria, but communities like subreddits often have diverse, implicit standards.

**Method:** The authors developed an interpretable architecture that identifies and extracts moderation criteria from historical moderation data, represented as score tables of lexical expressions.

**Key Contributions:**

	1. Novel method for extracting implicit moderation criteria
	2. Transparent insights into decision-making processes in moderation
	3. Identification of community-specific moderation patterns

**Result:** The extracted lexical patterns replicate the performance of neural moderation models while offering transparent insights into the decision-making process, revealing community-specific moderation patterns.

**Limitations:** The study focuses on historical data; current moderation practices may vary.

**Conclusion:** This approach uncovers significant variations in moderation norms across communities and identifies previously undocumented patterns.

**Abstract:** Effective content moderation systems require explicit classification criteria, yet online communities like subreddits often operate with diverse, implicit standards. This work introduces a novel approach to identify and extract these implicit criteria from historical moderation data using an interpretable architecture. We represent moderation criteria as score tables of lexical expressions associated with content removal, enabling systematic comparison across different communities. Our experiments demonstrate that these extracted lexical patterns effectively replicate the performance of neural moderation models while providing transparent insights into decision-making processes. The resulting criteria matrix reveals significant variations in how seemingly shared norms are actually enforced, uncovering previously undocumented moderation patterns including community-specific tolerances for language, features for topical restrictions, and underlying subcategories of the toxic speech classification.

</details>


### [23] [ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly](https://arxiv.org/abs/2509.02949)

*Kimihiro Hasegawa, Wiradee Imrattanatrai, Masaki Asada, Susan Holm, Yuran Wang, Vincent Zhou, Ken Fukuda, Teruko Mitamura*

**Main category:** cs.CL

**Keywords:** multimodal QA, assembly tasks, human-activity recordings, instruction manuals, dataset

**Relevance Score:** 7

**TL;DR:** The paper presents ProMQA-Assembly, a multimodal QA dataset for evaluating assembly tasks involving human activity and instruction manuals.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of practical testbeds for evaluating assistants in assembly tasks across various settings.

**Method:** The dataset consists of 391 QA pairs created through a semi-automated QA annotation process, where LLMs generate candidates and humans verify them, combined with fine-grained action labels to improve diversity in question types.

**Key Contributions:**

	1. Introduction of a new multimodal QA dataset for assembly tasks.
	2. Semi-automated QA annotation approach leveraging LLMs.
	3. Development of instruction task graphs to aid in benchmarking and annotation.

**Result:** Benchmarking experiments with existing models indicate significant potential for improvement in multimodal understanding for assembly tasks.

**Limitations:** Results indicate current models underperform, suggesting limitations in existing multimodal understanding capabilities.

**Conclusion:** The ProMQA-Assembly dataset is expected to enhance the development of procedural-activity assistants.

**Abstract:** Assistants on assembly tasks have a large potential to benefit humans from everyday tasks to industrial settings. However, no testbeds support application-oriented system evaluation in a practical setting, especially in assembly. To foster the development, we propose a new multimodal QA dataset on assembly activities. Our dataset, ProMQA-Assembly, consists of 391 QA pairs that require the multimodal understanding of human-activity recordings and their instruction manuals in an online-style manner. In the development, we adopt a semi-automated QA annotation approach, where LLMs generate candidates and humans verify them, as a cost-effective method, and further improve it by integrating fine-grained action labels to diversify question types. Furthermore, we create instruction task graphs for the target tasks of assembling toy vehicles. These newly created task graphs are used in our benchmarking experiment, as well as to facilitate the human verification process in the QA annotation. Utilizing our dataset, we benchmark models, including competitive proprietary multimodal models. Our results suggest great room for improvement for the current models. We believe our new evaluation dataset can contribute to the further development of procedural-activity assistants.

</details>


### [24] [DiaCBT: A Long-Periodic Dialogue Corpus Guided by Cognitive Conceptualization Diagram for CBT-based Psychological Counseling](https://arxiv.org/abs/2509.02999)

*Yougen Zhou, Ningning Zhou, Qin Chen, Jie Zhou, Aimin Zhou, Liang He*

**Main category:** cs.CL

**Keywords:** mental health, large language models, Cognitive Behavioral Therapy, conversational agents, dataset

**Relevance Score:** 9

**TL;DR:** This study introduces DiaCBT, a dialogue corpus for counseling based on CBT, aimed at enhancing LLMs for mental health applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited access to psychotherapy due to social stigma and therapist availability, leveraging LLMs equipped with psychotherapeutic skills is proposed as a solution.

**Method:** A long-periodic dialogue corpus is constructed for counseling, integrating multiple sessions and cognitive conceptualization diagrams to facilitate client simulation.

**Key Contributions:**

	1. Introduction of a long-periodic dialogue corpus for therapy based on CBT
	2. Integration of cognitive conceptualization diagrams in client simulations
	3. Development of a benchmark evaluation framework for counseling models

**Result:** The training of an in-depth counseling model using the DiaCBT dataset shows significant improvement in LLMs' capability to simulate psychologists skilled in CBT.

**Limitations:** 

**Conclusion:** DiaCBT demonstrates the potential to improve the training of conversational agents for professional counseling by employing established psychological criteria for evaluation.

**Abstract:** Psychotherapy reaches only a small fraction of individuals suffering from mental disorders due to social stigma and the limited availability of therapists. Large language models (LLMs), when equipped with professional psychotherapeutic skills, offer a promising solution to expand access to mental health services. However, the lack of psychological conversation datasets presents significant challenges in developing effective psychotherapy-guided conversational agents. In this paper, we construct a long-periodic dialogue corpus for counseling based on cognitive behavioral therapy (CBT). Our curated dataset includes multiple sessions for each counseling and incorporates cognitive conceptualization diagrams (CCDs) to guide client simulation across diverse scenarios. To evaluate the utility of our dataset, we train an in-depth counseling model and present a comprehensive evaluation framework to benchmark it against established psychological criteria for CBT-based counseling. Results demonstrate that DiaCBT effectively enhances LLMs' ability to emulate psychologists with CBT expertise, underscoring its potential for training more professional counseling agents.

</details>


### [25] [Mitigating Data Imbalance in Automated Speaking Assessment](https://arxiv.org/abs/2509.03010)

*Fong-Chun Tsai, Kuan-Tang Huang, Bi-Cheng Yan, Tien-Hong Lo, Berlin Chen*

**Main category:** cs.CL

**Keywords:** Automated Speaking Assessment, class imbalance, BERT, machine learning, language assessment

**Relevance Score:** 6

**TL;DR:** This paper introduces a Balancing Logit Variation (BLV) loss to improve Automated Speaking Assessment (ASA) models by addressing class imbalance without changing the dataset.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the fairness and accuracy of ASA models for L2 learners by addressing bias in predictions due to class imbalance.

**Method:** The paper proposes a novel training objective, the BLV loss, which perturbs model predictions to improve feature representation for minority classes in ASA models.

**Key Contributions:**

	1. Introduction of the Balancing Logit Variation (BLV) loss for ASA models
	2. Demonstrated significant improvements in model accuracy and fairness
	3. Novel approach to handling class imbalance without altering the dataset

**Result:** Using the BLV loss with a BERT model on the ICNALE benchmark dataset, the accuracy and fairness of automated speech evaluation were significantly improved.

**Limitations:** 

**Conclusion:** The BLV loss makes ASA models more robust and equitable for diverse learners by effectively addressing class imbalance without modifying existing datasets.

**Abstract:** Automated Speaking Assessment (ASA) plays a crucial role in evaluating second-language (L2) learners proficiency. However, ASA models often suffer from class imbalance, leading to biased predictions. To address this, we introduce a novel objective for training ASA models, dubbed the Balancing Logit Variation (BLV) loss, which perturbs model predictions to improve feature representation for minority classes without modifying the dataset. Evaluations on the ICNALE benchmark dataset show that integrating the BLV loss into a celebrated text-based (BERT) model significantly enhances classification accuracy and fairness, making automated speech evaluation more robust for diverse learners.

</details>


### [26] [Training LLMs to be Better Text Embedders through Bidirectional Reconstruction](https://arxiv.org/abs/2509.03020)

*Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, Zhouhan Lin*

**Main category:** cs.CL

**Keywords:** large language models, text embeddings, generative reconstruction, contrastive learning, information retrieval

**Relevance Score:** 9

**TL;DR:** This paper proposes a new training stage for large language models to enhance text embeddings by utilizing generative reconstruction tasks before contrastive learning, leading to improved performance on embedding benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM-based embeddings often rely on the final token, which limits their semantic capture for retrieval tasks. The motivation is to enhance this process to improve document and query handling.

**Method:** The authors introduce bidirectional generative reconstruction tasks (EBQ2D and EBD2Q) that enrich the final token embedding by reconstructing aspects of Query-Document pairs.

**Key Contributions:**

	1. Introduction of new training stage for LLMs to improve text embeddings.
	2. Implementation of EBQ2D and EBD2Q tasks for embedding enrichment.
	3. Demonstration of state-of-the-art results on the MTEB.

**Result:** The proposed method achieved state-of-the-art results on the Massive Text Embedding Benchmark (MTEB) across various LLM models and scales, showcasing significant performance improvements.

**Limitations:** 

**Conclusion:** The added training stage enhances the semantic richness of LLM embeddings, providing a more robust foundation for retrieval tasks.

**Abstract:** Large language models (LLMs) have increasingly been explored as powerful text embedders. Existing LLM-based text embedding approaches often leverage the embedding of the final token, typically a reserved special token such as [EOS]. However, these tokens have not been intentionally trained to capture the semantics of the whole context, limiting their capacity as text embeddings, especially for retrieval and re-ranking tasks. We propose to add a new training stage before contrastive learning to enrich the semantics of the final token embedding. This stage employs bidirectional generative reconstruction tasks, namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based Document-to-Query), which interleave to anchor the [EOS] embedding and reconstruct either side of Query-Document pairs. Experimental results demonstrate that our additional training stage significantly improves LLM performance on the Massive Text Embedding Benchmark (MTEB), achieving new state-of-the-art results across different LLM base models and scales.

</details>


### [27] [Structure-Learnable Adapter Fine-Tuning for Parameter-Efficient Large Language Models](https://arxiv.org/abs/2509.03057)

*Ming Gong, Yingnan Deng, Nia Qi, Yujun Zou, Zhihao Xue, Yun Zi*

**Main category:** cs.CL

**Keywords:** fine-tuning, large language models, multi-task learning, adapter-based, sparsity

**Relevance Score:** 8

**TL;DR:** This paper proposes an adapter-based fine-tuning method for large language models to improve flexibility and performance in multi-task settings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues of parameter redundancy, rigid structure, and limited task adaptability in fine-tuning large language models.

**Method:** An adapter-based fine-tuning method is proposed, utilizing a structure-learnable mechanism with differentiable gating functions and structural sparsity control variables to optimize adapter insertion points and module combinations.

**Key Contributions:**

	1. Adapter-based fine-tuning method
	2. Differentiable gating functions and sparsity control
	3. Dynamic construction of task-specific efficient substructures

**Result:** The method demonstrates improved parameter utilization and representational capacity, outperforming mainstream parameter-efficient tuning techniques on various multi-task natural language understanding tasks.

**Limitations:** 

**Conclusion:** The proposed method shows enhanced stability and robustness across different tasks while maintaining a good balance between accuracy and compression rate.

**Abstract:** This paper addresses the issues of parameter redundancy, rigid structure, and limited task adaptability in the fine-tuning of large language models. It proposes an adapter-based fine-tuning method built on a structure-learnable mechanism. By introducing differentiable gating functions and structural sparsity control variables, the method enables automatic optimization of adapter insertion points, activation paths, and module combinations. This allows the model to adjust its structure flexibly in multi-task settings to match different task characteristics. With the backbone parameters kept frozen, the method uses a structure search mechanism to guide the dynamic construction of task-specific efficient substructures during training. This significantly improves parameter utilization and representational capacity. In addition, the paper designs a set of sensitivity analysis experiments to systematically evaluate the effects of sparsity weight, noise injection ratio, and data perturbation on model performance. These experiments verify the stability and robustness of the proposed method across various multi-task natural language understanding tasks. The experimental results show that the proposed method outperforms mainstream parameter-efficient tuning techniques on multiple tasks. It achieves a better balance among accuracy, compression rate, and robustness to noise and perturbation.

</details>


### [28] [A Long Short-Term Memory (LSTM) Model for Business Sentiment Analysis Based on Recurrent Neural Network](https://arxiv.org/abs/2509.03060)

*Md. Jahidul Islam Razin, Md. Abdul Karim, M. F. Mridha, S M Rafiuddin, Tahira Alam*

**Main category:** cs.CL

**Keywords:** Sentiment Analysis, LSTM, Business Intelligence, Machine Learning, E-commerce

**Relevance Score:** 7

**TL;DR:** The paper presents a modified LSTM model for business sentiment analysis, achieving 91.33% accuracy and outperforming conventional RNN models.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To improve business sentiment analysis by applying a modified LSTM model that addresses the vanishing gradient problem common in traditional RNNs.

**Method:** The paper implements a modified long short-term memory (LSTM) model on a product review dataset, using 70% of the data for training and 30% for testing.

**Key Contributions:**

	1. Introduction of a modified LSTM approach for business sentiment analysis
	2. Demonstration of improved accuracy over conventional RNN models
	3. Application of the model to product reviews for business insights

**Result:** The modified LSTM model achieved an accuracy of 91.33%, outperforming traditional RNN models such as KNN, SVM, and Naive Bayes in sentiment analysis tasks.

**Limitations:** 

**Conclusion:** The proposed modified RNN model can effectively evaluate customer feedback, enabling businesses to enhance their marketing strategies based on sentiment analysis.

**Abstract:** Business sentiment analysis (BSA) is one of the significant and popular topics of natural language processing. It is one kind of sentiment analysis techniques for business purposes. Different categories of sentiment analysis techniques like lexicon-based techniques and different types of machine learning algorithms are applied for sentiment analysis on different languages like English, Hindi, Spanish, etc. In this paper, long short-term memory (LSTM) is applied for business sentiment analysis, where a recurrent neural network is used. An LSTM model is used in a modified approach to prevent the vanishing gradient problem rather than applying the conventional recurrent neural network (RNN). To apply the modified RNN model, product review dataset is used. In this experiment, 70\% of the data is trained for the LSTM and the rest 30\% of the data is used for testing. The result of this modified RNN model is compared with other conventional RNN models, and a comparison is made among the results. It is noted that the proposed model performs better than the other conventional RNN models. Here, the proposed model, i.e., the modified RNN model approach has achieved around 91.33\% of accuracy. By applying this model, any business company or e-commerce business site can identify the feedback from their customers about different types of products that customers like or dislike. Based on the customer reviews, a business company or e-commerce platform can evaluate its marketing strategy.

</details>


### [29] [Measuring Scalar Constructs in Social Science with LLMs](https://arxiv.org/abs/2509.03116)

*Hauke Licht, Rupak Sarkar, Patrick Y. Wu, Pranav Goel, Niklas Stoehr, Elliott Ash, Alexander Miserlis Hoyle*

**Main category:** cs.CL

**Keywords:** Large Language Models, Scalar Constructs, Social Science, Measurement Techniques, Finetuning

**Relevance Score:** 8

**TL;DR:** This paper evaluates LLM-based approaches for measuring scalar constructs in social science, finding that token probability-weighted pointwise scoring significantly improves measurement quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of using large language models for measuring scalar constructs like complexity and emotionality in public speech.

**Method:** The study evaluates four LLM-based approaches: unweighted direct pointwise scoring, aggregation of pairwise comparisons, token-probability-weighted pointwise scoring, and finetuning.

**Key Contributions:**

	1. Evaluation of LLM approaches for scalar construct measurement
	2. Identification of token probability weighting as a key improvement
	3. Demonstration that finetuning smaller models can match LLM performance

**Result:** Pairwise comparisons improve measurement quality, but token probability-weighted scores provide even better results. Finetuning smaller models with a limited dataset can equal or exceed prompted LLMs performance.

**Limitations:** 

**Conclusion:** LLMs can effectively measure scalar constructs in social science, particularly when utilizing token probability weighting and appropriate finetuning.

**Abstract:** Many constructs that characterize language, like its complexity or emotionality, have a naturally continuous semantic structure; a public speech is not just "simple" or "complex," but exists on a continuum between extremes. Although large language models (LLMs) are an attractive tool for measuring scalar constructs, their idiosyncratic treatment of numerical outputs raises questions of how to best apply them. We address these questions with a comprehensive evaluation of LLM-based approaches to scalar construct measurement in social science. Using multiple datasets sourced from the political science literature, we evaluate four approaches: unweighted direct pointwise scoring, aggregation of pairwise comparisons, token-probability-weighted pointwise scoring, and finetuning. Our study yields actionable findings for applied researchers. First, LLMs prompted to generate pointwise scores directly from texts produce discontinuous distributions with bunching at arbitrary numbers. The quality of the measurements improves with pairwise comparisons made by LLMs, but it improves even more by taking pointwise scores and weighting them by token probability. Finally, finetuning smaller models with as few as 1,000 training pairs can match or exceed the performance of prompted LLMs.

</details>


### [30] [From Evaluation to Defense: Constructing Persistent Edit-Based Fingerprints for Large Language Models](https://arxiv.org/abs/2509.03122)

*Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo, Xiaoling Wang, Linlin Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, intellectual property, knowledge editing, fine-tuning, fingerprinting

**Relevance Score:** 7

**TL;DR:** This paper introduces a novel approach to intellectual property protection for LLMs using knowledge editing and proposes Fingerprint Subspace-aware Fine-Tuning (FSFT) to enhance fingerprint persistence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for effective IP protection mechanisms for Large Language Models without significantly degrading their performance.

**Method:** The paper applies knowledge editing to fingerprint injection and proposes a new method called Fingerprint Subspace-aware Fine-Tuning (FSFT) to enhance the robustness of the fingerprinting process.

**Key Contributions:**

	1. Introduction of knowledge editing for fingerprint injection in LLMs
	2. Proposal of Fingerprint Subspace-aware Fine-Tuning (FSFT)
	3. Demonstration of FSFT's effectiveness in reducing degradation and improving performance.

**Result:** FSFT reduces fingerprint degradation by constraining updates and demonstrates a performance improvement of 10% compared to traditional fine-tuning methods, even in challenging scenarios.

**Limitations:** Fingerprint degradation still occurs under large-scale fine-tuning even with the proposed method.

**Conclusion:** The study highlights the necessity for advanced fingerprinting techniques for LLMs, addressing performance degradation issues that occur with current methods.

**Abstract:** The intellectual property (IP) protection of Large Language Models (LLMs) is increasingly critical. Injecting specialized fingerprints into LLMs through instruction tuning is a common IP protection technique. However, this may significantly degrade model performance, requires substantial computational resources, and exhibits poor persistence under model modifications. We argue that knowledge editing offers a lightweight alternative that is more suitable for fingerprint injection. Accordingly, we apply knowledge editing to fingerprint injection for the first time and demonstrate its strong capability. Despite using scrambled text as fingerprints to prevent them from being overwritten during fine-tuning, degradation still occurs under large-scale fine-tuning. To address this, we propose Fingerprint Subspace-aware Fine-Tuning (FSFT), which reduces fingerprint degradation by constraining the update of the fingerprint subspace. The performance of FSFT exceeds fine-tuning by 10% even in the worst-case scenario. Additionally, we observe that the fingerprint-injected models struggle to distinguish between fingerprints and similar texts due to the high similarity of their features. This finding underscores the urgent need for more robust and fine-grained fingerprinting injection methods for LLMs.

</details>


### [31] [An experimental and computational study of an Estonian single-person word naming](https://arxiv.org/abs/2509.03143)

*Kaidi L√µo, Arvi Tavast, Maria Heitmeier, Harald Baayen*

**Main category:** cs.CL

**Keywords:** lexical processing, Discriminative Lexicon Model, eye-tracking, word naming task, deep learning

**Relevance Score:** 4

**TL;DR:** This study analyzes lexical processing in Estonian through a large-scale experiment combining word naming tasks and eye-tracking, focusing on predictors of response variables.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the predictiveness of the Discriminative Lexicon Model (DLM) in lexical processing compared to classical predictors like word frequency and neighborhood size.

**Method:** A single-subject experiment with word naming tasks and eye-tracking, analyzing five response variables using a generalized additive model.

**Key Contributions:**

	1. Demonstration of the predictive power of DLM-based measures for lexical processing in Estonian.
	2. Comparison of DLM measures with classical predictors.
	3. Insights into the role of meaning in lexical processing tasks.

**Result:** DLM-based measures are powerful predictors for certain lexical processing tasks, but classical predictors offer better precision overall, except in specific cases of total fixation duration.

**Limitations:** 

**Conclusion:** Meaning plays a significant role in word naming tasks, as indicated by the effectiveness of DLM-based measures for various response variables.

**Abstract:** This study investigates lexical processing in Estonian. A large-scale single-subject experiment is reported that combines the word naming task with eye-tracking. Five response variables (first fixation duration, total fixation duration, number of fixations, word naming latency, and spoken word duration) are analyzed with the generalized additive model. Of central interest is the question of whether measures for lexical processing generated by a computational model of the mental lexicon (the Discriminative Lexicon Model, DLM) are predictive for these response variables, and how they compare to classical predictors such as word frequency, neighborhood size, and inflectional paradigm size. Computational models were implemented both with linear and deep mappings. Central findings are, first, that DLM-based measures are powerful predictors for lexical processing, second, that DLM-measures using deep learning are not necessarily more precise predictors of lexical processing than DLM-measures using linear mappings, third, that classical predictors tend to provide somewhat more precise fits compared to DLM-based predictors (except for total fixation duration, where the two provide equivalent goodness of fit), and fourth, that in the naming task lexical variables are not predictive for first fixation duration and the total number of fixations. As the DLM works with mappings from form to meaning, the predictivity of DLM-based measures for total fixation duration, naming latencies, and spoken word duration indicates that meaning is heavily involved in the present word naming task.

</details>


### [32] [Expanding the WMT24++ Benchmark with Rumantsch Grischun, Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader](https://arxiv.org/abs/2509.03148)

*Jannis Vamvas, Ignacio P√©rez Prat, Not Battesta Soliva, Sandra Baltermia-Guetg, Andrina Beeli, Simona Beeli, Madlaina Capeder, Laura Decurtins, Gian Peder Gregori, Flavia Hobi, Gabriela Holderegger, Arina Lazzarini, Viviana Lazzarini, Walter Rosselli, Bettina Vital, Anna Rutkiewicz, Rico Sennrich*

**Main category:** cs.CL

**Keywords:** Romansh, machine translation, benchmark, language evaluation, WMT

**Relevance Score:** 3

**TL;DR:** This paper presents a benchmark for evaluating machine translation of the Romansh language and its varieties.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of resources for machine translation evaluation in the Romansh language, which is spoken in Switzerland.

**Method:** The paper develops a benchmark for six varieties of Romansh using human-generated reference translations, based on the WMT24++ benchmark for parallelism with over 55 languages.

**Key Contributions:**

	1. Established a benchmark for machine translation evaluation for six varieties of Romansh.
	2. Provided insight into the translation effectiveness from Romansh to German and the challenges of translation into Romansh.
	3. Created human reference translations based on a recognized benchmark.

**Result:** The evaluation shows that translation from Romansh to German is relatively effective across varieties, while translating into Romansh remains challenging.

**Limitations:** The study only focuses on translation between Romansh and German, potentially overlooking other language pairs.

**Conclusion:** More development is needed to improve machine translation systems for Romansh, particularly for translations into the language.

**Abstract:** The Romansh language, spoken in Switzerland, has limited resources for machine translation evaluation. In this paper, we present a benchmark for six varieties of Romansh: Rumantsch Grischun, a supra-regional variety, and five regional varieties: Sursilvan, Sutsilvan, Surmiran, Puter, and Vallader. Our reference translations were created by human translators based on the WMT24++ benchmark, which ensures parallelism with more than 55 other languages. An automatic evaluation of existing MT systems and LLMs shows that translation out of Romansh into German is handled relatively well for all the varieties, but translation into Romansh is still challenging.

</details>


### [33] [Domain Adaptation of LLMs for Process Data](https://arxiv.org/abs/2509.03161)

*Rafael Seidi Oyamada, Jari Peeperkorn, Jochen De Weerdt, Johannes De Smedt*

**Main category:** cs.CL

**Keywords:** Large Language Models, Process Mining, Predictive Process Monitoring, Parameter-efficient fine-tuning, Machine Learning

**Relevance Score:** 7

**TL;DR:** This study explores the direct application of pretrained Large Language Models to process mining data, using parameter-efficient fine-tuning to enhance predictive performance in Predictive Process Monitoring tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage the capabilities of Large Language Models in Process Mining without natural language reformulation and to improve predictive monitoring performance.

**Method:** The study employs parameter-efficient fine-tuning techniques on pretrained LLMs to adapt them for Predictive Process Monitoring, focusing on both single- and multi-task predictions.

**Key Contributions:**

	1. Direct adaptation of LLMs to process mining without natural language reformulation
	2. Parameter-efficient fine-tuning techniques that lower computational overhead
	3. Improved predictive performance over traditional models and methods in multi-task settings

**Result:** The results show improved predictive performance compared to state-of-the-art RNN approaches and narrative-style solutions, especially in a multi-task setting, along with faster convergence and reduced need for hyperparameter optimization.

**Limitations:** 

**Conclusion:** Direct adaptation of LLMs to process data can enhance predictive capabilities in process mining while minimizing computational costs associated with fine-tuning.

**Abstract:** In recent years, Large Language Models (LLMs) have emerged as a prominent area of interest across various research domains, including Process Mining (PM). Current applications in PM have predominantly centered on prompt engineering strategies or the transformation of event logs into narrative-style datasets, thereby exploiting the semantic capabilities of LLMs to address diverse tasks. In contrast, this study investigates the direct adaptation of pretrained LLMs to process data without natural language reformulation, motivated by the fact that these models excel in generating sequences of tokens, similar to the objective in PM. More specifically, we focus on parameter-efficient fine-tuning techniques to mitigate the computational overhead typically associated with such models. Our experimental setup focuses on Predictive Process Monitoring (PPM), and considers both single- and multi-task predictions. The results demonstrate a potential improvement in predictive performance over state-of-the-art recurrent neural network (RNN) approaches and recent narrative-style-based solutions, particularly in the multi-task setting. Additionally, our fine-tuned models exhibit faster convergence and require significantly less hyperparameter optimization.

</details>


### [34] [SinhalaMMLU: A Comprehensive Benchmark for Evaluating Multitask Language Understanding in Sinhala](https://arxiv.org/abs/2509.03162)

*Ashmari Pramodya, Nirasha Nelki, Heshan Shalinda, Chamila Liyanage, Yusuke Sakai, Randil Pushpananda, Ruvan Weerasinghe, Hidetaka Kamigaito, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** Large Language Models, SinhalaMMLU, low-resource languages, cultural knowledge, evaluation benchmarks

**Relevance Score:** 7

**TL;DR:** Introduction of SinhalaMMLU, the first benchmark for evaluating LLMs on Sinhala with a focus on academic and culturally specific knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of multilingual evaluations for low-resource languages and the shortcomings in understanding culturally specific content in large language models.

**Method:** Development of SinhalaMMLU, a multiple-choice question answering dataset for Sinhala with 7,000 questions across various educational levels and subjects, aligned with the national curriculum.

**Key Contributions:**

	1. Creation of the first Sinhala language benchmark for LLM evaluation
	2. In-depth evaluation of existing LLMs on culturally and academically relevant questions
	3. Highlighting the performance limitations of LLMs in low-resource language contexts

**Result:** 26 LLMs were evaluated on SinhalaMMLU, with Claude 3.5 sonnet and GPT-4o achieving the highest accuracies but overall performance remaining limited, particularly in humanities subjects.

**Limitations:** 

**Conclusion:** Models exhibit insufficient understanding of culturally rich domains, indicating the need for improvements in adapting LLMs for low-resource and culturally specific languages.

**Abstract:** Large Language Models (LLMs) demonstrate impressive general knowledge and reasoning abilities, yet their evaluation has predominantly focused on global or anglocentric subjects, often neglecting low-resource languages and culturally specific content. While recent multilingual benchmarks attempt to bridge this gap, many rely on automatic translation, which can introduce errors and misrepresent the original cultural context. To address this, we introduce SinhalaMMLU, the first multiple-choice question answering benchmark designed specifically for Sinhala, a low-resource language. The dataset includes over 7,000 questions spanning secondary to collegiate education levels, aligned with the Sri Lankan national curriculum, and covers six domains and 30 subjects, encompassing both general academic topics and culturally grounded knowledge. We evaluate 26 LLMs on SinhalaMMLU and observe that, while Claude 3.5 sonnet and GPT-4o achieve the highest average accuracies at 67% and 62% respectively, overall model performance remains limited. In particular, models struggle in culturally rich domains such as the Humanities, revealing substantial room for improvement in adapting LLMs to low-resource and culturally specific contexts.

</details>


### [35] [Comparison of End-to-end Speech Assessment Models for the NOCASA 2025 Challenge](https://arxiv.org/abs/2509.03256)

*Aleksei ≈Ωavoronkov, Tanel Alum√§e*

**Main category:** cs.CL

**Keywords:** pronunciation assessment, Siamese architecture, goodness-of-pronunciation

**Relevance Score:** 4

**TL;DR:** This paper analyzes three end-to-end models for automatic word-level pronunciation assessment for children learning Norwegian, achieving high performance with a novel model that integrates GOP features.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The objective is to enhance automatic pronunciation assessment tools for children learning Norwegian as a second language.

**Method:** Three models were developed: an encoder-decoder Siamese architecture, a prefix-tuned direct classification model using wav2vec2.0, and a novel model with alignment-free GOP features via CTC, optimized using a tailored loss function.

**Key Contributions:**

	1. Development of three unique models for pronunciation assessment
	2. Integration of alignment-free GOP features via CTC
	3. Introduction of a specific weighted ordinal cross-entropy loss function

**Result:** The GOP-CTC-based model demonstrated the highest performance, exceeding challenge baselines and achieving top leaderboard scores.

**Limitations:** 

**Conclusion:** The introduction of a weighted ordinal cross-entropy loss improved the model's effectiveness in pronunciation assessment.

**Abstract:** This paper presents an analysis of three end-to-end models developed for the NOCASA 2025 Challenge, aimed at automatic word-level pronunciation assessment for children learning Norwegian as a second language. Our models include an encoder-decoder Siamese architecture (E2E-R), a prefix-tuned direct classification model leveraging pretrained wav2vec2.0 representations, and a novel model integrating alignment-free goodness-of-pronunciation (GOP) features computed via CTC. We introduce a weighted ordinal cross-entropy loss tailored for optimizing metrics such as unweighted average recall and mean absolute error. Among the explored methods, our GOP-CTC-based model achieved the highest performance, substantially surpassing challenge baselines and attaining top leaderboard scores.

</details>


### [36] [LatPhon: Lightweight Multilingual G2P for Romance Languages and English](https://arxiv.org/abs/2509.03300)

*Luis Felipe Chary, Miguel Arjona Ramirez*

**Main category:** cs.CL

**Keywords:** Grapheme-to-Phoneme, Text-to-Speech, Automatic Speech Recognition, Multilingual, Transformer

**Relevance Score:** 6

**TL;DR:** LatPhon is a compact multilingual Grapheme-to-Phoneme model achieving competitive performance in TTS and ASR systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance performance in text-to-speech and speech recognition systems across multiple Latin-script languages while maintaining a manageable model size for on-device deployment.

**Method:** LatPhon is a 7.5 M-parameter Transformer model trained jointly on English, Spanish, French, Italian, Portuguese, and Romanian languages, leveraging a public corpus for evaluation.

**Key Contributions:**

	1. Introduction of a multilingual G2P model that is efficient and compact.
	2. Joint training across six major languages to improve multilingual G2P accuracy.
	3. Demonstration of on-device deployment feasibility for practical applications.

**Result:** LatPhon achieves a mean phoneme error rate (PER) of 3.5%, outperforming the byte-level ByT5 baseline (5.4%) and nearing the language-specific WFSTs (3.2%).

**Limitations:** 

**Conclusion:** The results suggest that LatPhon can serve as an efficient and effective front-end for multilingual speech systems.

**Abstract:** Grapheme-to-phoneme (G2P) conversion is a key front-end for text-to-speech (TTS), automatic speech recognition (ASR), speech-to-speech translation (S2ST) and alignment systems, especially across multiple Latin-script languages.We present LatPhon, a 7.5 M - parameter Transformer jointly trained on six such languages--English, Spanish, French, Italian, Portuguese, and Romanian. On the public ipa-dict corpus, it attains a mean phoneme error rate (PER) of 3.5%, outperforming the byte-level ByT5 baseline (5.4%) and approaching language-specific WFSTs (3.2%) while occupying 30 MB of memory, which makes on-device deployment feasible when needed. These results indicate that compact multilingual G2P can serve as a universal front-end for Latin-language speech pipelines.

</details>


### [37] [AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?](https://arxiv.org/abs/2509.03312)

*Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan*

**Main category:** cs.CL

**Keywords:** agentic systems, failure attribution, large language models, reinforcement learning, multi-agent interactions

**Relevance Score:** 9

**TL;DR:** AgenTracer automates failure attribution in multi-agent systems, improving error diagnosis and performance of LLM-based agents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragility of large language model-based agentic systems during execution and improve failure attribution accuracy, which is currently below 10%.

**Method:** The paper introduces an automated framework called AgenTracer for annotating failed multi-agent trajectories, using counterfactual replay and programmed fault injection to create the TracerTraj dataset. AgenTracer-8B is trained with multi-granular reinforcement learning for error diagnosis.

**Key Contributions:**

	1. Introduction of the AgenTracer framework for failure attribution in multi-agent systems.
	2. Development of the TracerTraj dataset for training failure diagnosis models.
	3. Creation of AgenTracer-8B, a lightweight failure tracer that surpasses existing proprietary LLMs.

**Result:** AgenTracer-8B outperforms proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18% on the Who&When benchmark, and offers performance improvements of 4.8-14.2% for multi-agent systems.

**Limitations:** 

**Conclusion:** AgenTracer-8B sets a new standard for LLM agentic failure attribution and provides actionable feedback for enhancing multi-agent system performance.

**Abstract:** Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI.

</details>


### [38] [LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations](https://arxiv.org/abs/2509.03405)

*Daniela Gottesman, Alon Gilae-Dotan, Ido Cohen, Yoav Gur-Arieh, Marius Mosbach, Ori Yoran, Mor Geva*

**Main category:** cs.CL

**Keywords:** Language Models, Knowledge Acquisition, Pretraining, Entity Retrieval, Machine Learning

**Relevance Score:** 9

**TL;DR:** LMEnt is a tool for analyzing knowledge acquisition in language models during pretraining, introducing a knowledge-rich corpus, an improved retrieval method, and multiple pretrained models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how language models acquire and represent knowledge from data, which could improve their consistency and robustness.

**Method:** LMEnt provides a fully annotated pretraining corpus, an entity-based retrieval method, and includes 12 pretrained models to study knowledge acquisition and influence on performance.

**Key Contributions:**

	1. Knowledge-rich pretraining corpus annotated with entity mentions
	2. Novel entity-based retrieval method
	3. Release of multiple pretrained models for knowledge analysis

**Result:** The entity-based retrieval method outperformed previous approaches by 80.4%, and the pretrained models show comparable performance to open-sourced models on knowledge benchmarks.

**Limitations:** 

**Conclusion:** LMEnt supports research on knowledge in language models including aspects such as representations, plasticity, and learning dynamics.

**Abstract:** Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics.

</details>


### [39] [Learning Mechanism Underlying NLP Pre-Training and Fine-Tuning](https://arxiv.org/abs/2509.03407)

*Yarden Tzach, Ronit D. Gross, Ella Koresh, Shalom Rosner, Or Shpringer, Tal Halevi, Ido Kanter*

**Main category:** cs.CL

**Keywords:** natural language processing, pre-training, fine-tuning, BERT-6, classification

**Relevance Score:** 8

**TL;DR:** This paper explores the mechanisms of successful pre-training in NLP, highlighting the relationship between pre-training accuracy and fine-tuning performance across classification tasks using the BERT-6 architecture.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms driving effective pre-training in natural language processing and its relationship with fine-tuning performance on classification tasks.

**Method:** The study utilized the BERT-6 architecture, pre-training it on the Wikipedia dataset and fine-tuning it on FewRel and DBpedia classification tasks to observe accuracy trends.

**Key Contributions:**

	1. Proposed a novel metric (accuracy per token) for pre-training success.
	2. Demonstrated the grouping of tokens into strong match clusters during pre-training.
	3. Showed that mechanisms of pre-training in NLP can apply to image classification.
	4. Identified independence of prediction confidence from average input accuracy.

**Result:** The accuracy per token increased with its frequency, leading to enhanced performance as noted in the confusion matrix and its effect along transformer blocks, resulting in improved fine-tuning accuracy.

**Limitations:** Focus mainly on the BERT-6 architecture limits generalizability; insights might vary for other models.

**Conclusion:** Pre-training mechanisms in NLP show similarities to techniques used in image classification, indicating a universal pattern across different machine learning tasks.

**Abstract:** Natural language processing (NLP) enables the understanding and generation of meaningful human language, typically using a pre-trained complex architecture on a large dataset to learn the language and next fine-tune its weights to implement a specific task. Twofold goals are examined; to understand the mechanism underlying successful pre-training and to determine the interplay between the pre-training accuracy and the fine-tuning of classification tasks. The following main results were obtained; the accuracy per token (APT) increased with its appearance frequency in the dataset, and its average over all tokens served as an order parameter to quantify pre-training success, which increased along the transformer blocks. Pre-training broke the symmetry among tokens and grouped them into finite, small, strong match token clusters, as inferred from the presented token confusion matrix. This feature was sharpened along the transformer blocks toward the output layer, enhancing its performance considerably compared with that of the embedding layer. Consequently, higher-order language structures were generated by pre-training, even though the learning cost function was directed solely at identifying a single token. These pre-training findings were reflected by the improved fine-tuning accuracy along the transformer blocks. Additionally, the output label prediction confidence was found to be independent of the average input APT, as the input meaning was preserved since the tokens are replaced primarily by strong match tokens. Finally, although pre-training is commonly absent in image classification tasks, its underlying mechanism is similar to that used in fine-tuning NLP classification tasks, hinting at its universality. The results were based on the BERT-6 architecture pre-trained on the Wikipedia dataset and fine-tuned on the FewRel and DBpedia classification tasks.

</details>


### [40] [Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases LLM Judges](https://arxiv.org/abs/2509.03419)

*Weiyuan Li, Xintao Wang, Siyu Yuan, Rui Xu, Jiangjie Chen, Qingqing Dong, Yanghua Xiao, Deqing Yang*

**Main category:** cs.CL

**Keywords:** large language models, evaluation, bias, complex tasks, benchmark

**Relevance Score:** 9

**TL;DR:** This paper investigates biases in large language models (LLMs) during complex evaluations, presenting a benchmark called ComplexEval.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of reliably evaluating LLMs on diverse and complex tasks, where existing evaluations have proven inadequate.

**Method:** Developed ComplexEval benchmark to expose and quantify Auxiliary Information Induced Biases across various scenarios.

**Key Contributions:**

	1. Introduction of ComplexEval benchmark for evaluating LLMs
	2. Identification of 6 novel biases
	3. Analysis of LLM susceptibility to biases related to task complexity

**Result:** Identified 6 previously unknown biases affecting LLM evaluations, with significant susceptibility linked to task complexity.

**Limitations:** Focuses primarily on biases and does not propose solutions for all identified issues.

**Conclusion:** The findings provide insights to improve LLM evaluation accuracy and reliability, highlighting the need for better evaluation models.

**Abstract:** As large language models (LLMs) grow more capable, they face increasingly diverse and complex tasks, making reliable evaluation challenging. The paradigm of LLMs as judges has emerged as a scalable solution, yet prior work primarily focuses on simple settings. Their reliability in complex tasks--where multi-faceted rubrics, unstructured reference answers, and nuanced criteria are critical--remains understudied. In this paper, we constructed ComplexEval, a challenge benchmark designed to systematically expose and quantify Auxiliary Information Induced Biases. We systematically investigated and validated 6 previously unexplored biases across 12 basic and 3 advanced scenarios. Key findings reveal: (1) all evaluated models exhibit significant susceptibility to these biases, with bias magnitude scaling with task complexity; (2) notably, Large Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth analysis offers crucial insights for improving the accuracy and verifiability of evaluation signals, paving the way for more general and robust evaluation models.

</details>


### [41] [Continuous Saudi Sign Language Recognition: A Vision Transformer Approach](https://arxiv.org/abs/2509.03467)

*Soukeina Elhassen, Lama Al Khuzayem, Areej Alhothali, Ohoud Alzamzami, Nahed Alowaidi*

**Main category:** cs.CL

**Keywords:** Saudi Sign Language, SL recognition, transformer model, dataset, communication technology

**Relevance Score:** 7

**TL;DR:** This paper presents the KAU-CSSL dataset, the first continuous Saudi Sign Language dataset, along with a novel transformer-based model for accurate SSL recognition and translation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the urgent need for precise and reliable translation techniques for Saudi Sign Language, which has been overlooked in favor of non-Arabic sign languages.

**Method:** A transformer-based model is proposed, utilizing a pretrained ResNet-18 for spatial feature extraction and a Transformer Encoder with Bidirectional LSTM for temporal dependencies.

**Key Contributions:**

	1. Introduction of the KAU-CSSL dataset focusing on continuous SSL.
	2. Development of a transformer-based model for SSL recognition.
	3. High accuracy rates in both signer dependent and independent settings.

**Result:** The model achieves an accuracy of 99.02% in signer dependent mode and 77.71% in signer independent mode, demonstrating its effectiveness.

**Limitations:** 

**Conclusion:** The KAU-CSSL dataset and the proposed model significantly advance the capabilities of communication tools for the hearing-impaired community and contribute to the field of sign language technology.

**Abstract:** Sign language (SL) is an essential communication form for hearing-impaired and deaf people, enabling engagement within the broader society. Despite its significance, limited public awareness of SL often leads to inequitable access to educational and professional opportunities, thereby contributing to social exclusion, particularly in Saudi Arabia, where over 84,000 individuals depend on Saudi Sign Language (SSL) as their primary form of communication. Although certain technological approaches have helped to improve communication for individuals with hearing impairments, there continues to be an urgent requirement for more precise and dependable translation techniques, especially for Arabic sign language variants like SSL. Most state-of-the-art solutions have primarily focused on non-Arabic sign languages, resulting in a considerable absence of resources dedicated to Arabic sign language, specifically SSL. The complexity of the Arabic language and the prevalence of isolated sign language datasets that concentrate on individual words instead of continuous speech contribute to this issue. To address this gap, our research represents an important step in developing SSL resources. To address this, we introduce the first continuous Saudi Sign Language dataset called KAU-CSSL, focusing on complete sentences to facilitate further research and enable sophisticated recognition systems for SSL recognition and translation. Additionally, we propose a transformer-based model, utilizing a pretrained ResNet-18 for spatial feature extraction and a Transformer Encoder with Bidirectional LSTM for temporal dependencies, achieving 99.02\% accuracy at signer dependent mode and 77.71\% accuracy at signer independent mode. This development leads the way to not only improving communication tools for the SSL community but also making a substantial contribution to the wider field of sign language.

</details>


### [42] [Design and Optimization of Reinforcement Learning-Based Agents in Text-Based Games](https://arxiv.org/abs/2509.03479)

*Haonan Wang, Mingjia Zhao, Junfeng Sun, Wei Liu*

**Main category:** cs.CL

**Keywords:** reinforcement learning, text-based games, agent design, deep learning, policy gradient

**Relevance Score:** 5

**TL;DR:** This paper presents a novel agent design and learning approach for text-based games using deep reinforcement learning, introducing a new model for state value conversion to optimal policy that improves game performance.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To advance research in agent performance in text-based games using reinforcement learning techniques.

**Method:** A model of deep learning processes the game text to build a world model, followed by a policy gradient-based deep reinforcement learning method for agent training.

**Key Contributions:**

	1. Introduction of a deep learning model for world state processing in text games
	2. Development of an optimized policy gradient training method for agents
	3. Demonstration of significant improvements in game metrics compared to previous agents

**Result:** The enhanced agent significantly outperforms previous agents in game completion ratio and win rate in text-based game experiments.

**Limitations:** 

**Conclusion:** This research provides empirical evidence and a new understanding of applying reinforcement learning techniques to text-based games, fostering future development in broader applications.

**Abstract:** As AI technology advances, research in playing text-based games with agents has becomeprogressively popular. In this paper, a novel approach to agent design and agent learning ispresented with the context of reinforcement learning. A model of deep learning is first applied toprocess game text and build a world model. Next, the agent is learned through a policy gradient-based deep reinforcement learning method to facilitate conversion from state value to optimal policy.The enhanced agent works better in several text-based game experiments and significantlysurpasses previous agents on game completion ratio and win rate. Our study introduces novelunderstanding and empirical ground for using reinforcement learning for text games and sets thestage for developing and optimizing reinforcement learning agents for more general domains andproblems.

</details>


### [43] [Learn and Unlearn: Addressing Misinformation in Multilingual LLMs](https://arxiv.org/abs/2406.13748)

*Taiming Lu, Philipp Koehn*

**Main category:** cs.CL

**Keywords:** multilingual LLMs, harmful content, unlearning techniques

**Relevance Score:** 9

**TL;DR:** This paper examines harmful information propagation in multilingual large language models (LLMs) and the effectiveness of unlearning methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the spread of harmful content across languages in multilingual LLMs and the shortcomings of existing unlearning methodologies.

**Method:** Analysis of the propagation of fake information in multilingual contexts and evaluation of various unlearning techniques.

**Key Contributions:**

	1. Investigates the multilingual spread of harmful information in LLMs.
	2. Evaluates the efficacy of various unlearning methods in a multilingual context.
	3. Proposes a necessity for comprehensive unlearning strategies for multilingual LLMs.

**Result:** Findings indicate that traditional unlearning methods fail to mitigate the spread of harmful content across languages and may reinforce it instead.

**Limitations:** Focused primarily on the evaluation of existing unlearning techniques without proposing new algorithms.

**Conclusion:** Comprehensive unlearning strategies are essential to enhance the safety and reliability of multilingual LLMs by addressing harmful responses in both English and original languages.

**Abstract:** This paper investigates the propagation of harmful information in multilingual large language models (LLMs) and evaluates the efficacy of various unlearning methods. We demonstrate that fake information, regardless of the language it is in, once introduced into these models through training data, can spread across different languages, compromising the integrity and reliability of the generated content. Our findings reveal that standard unlearning techniques, which typically focus on English data, are insufficient in mitigating the spread of harmful content in multilingual contexts and could inadvertently reinforce harmful content across languages. We show that only by addressing harmful responses in both English and the original language of the harmful data can we effectively eliminate generations for all languages. This underscores the critical need for comprehensive unlearning strategies that consider the multilingual nature of modern LLMs to enhance their safety and reliability across diverse linguistic landscapes.

</details>


### [44] [SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention](https://arxiv.org/abs/2406.15486)

*Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Guanyu Feng, Xin Lv, Xiao Chuanfu, Dahua Lin, Chao Yang*

**Main category:** cs.CL

**Keywords:** large language models, sparse attention, Time-to-First-Token

**Relevance Score:** 9

**TL;DR:** This paper introduces SampleAttention, a method that significantly reduces Time-to-First-Token (TTFT) latency in large language models (LLMs) by implementing adaptive sparse attention without sacrificing model accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the long TTFT latency caused by the quadratic complexity of vanilla attention in LLMs, while maintaining model accuracy and avoiding the need for additional pretraining or finetuning.

**Method:** The paper proposes SampleAttention, which captures head-specific sparse patterns at runtime with decreased overhead. It leverages local window patterns by attending to a fixed percentage of adjacent tokens and employs a two-stage query-guided filtering method to select minimal key-values for capturing patterns.

**Key Contributions:**

	1. Introduced SampleAttention for adaptive and near-lossless sparse attention in LLMs.
	2. Demonstrated significant reduction in TTFT latency without accuracy loss.
	3. Provided both theoretical and empirical foundations for the method.

**Result:** SampleAttention replaces vanilla attention in LLMs with nearly no accuracy loss and achieves a reduction in TTFT by as much as 2.42 times compared to existing methods like FlashAttention.

**Limitations:** 

**Conclusion:** The proposed SampleAttention is an effective solution for reducing TTFT in LLMs by utilizing adaptive sparse attention while maintaining model performance.

**Abstract:** Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\times$ compared with FlashAttention.

</details>


### [45] [Banishing LLM Hallucinations Requires Rethinking Generalization](https://arxiv.org/abs/2406.17642)

*Johnny Li, Saksham Consul, Eda Zhou, James Wong, Naila Farooqui, Yuxin Ye, Nithyashree Manohar, Zhuxiaona Wei, Tian Wu, Ben Echols, Sharon Zhou, Gregory Diamos*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucinations, Mixture of Memory Experts, fact retrieval, Lamini-1

**Relevance Score:** 8

**TL;DR:** The paper investigates hallucinations in Large Language Models (LLMs), demonstrating that conventional mitigating methods are ineffective. It introduces a new model, Lamini-1, designed to minimize hallucinations by using a massive mixture of memory experts to store and retrieve facts dynamically.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the phenomenon of hallucinations in Large Language Models (LLMs) and understand their underlying causes.

**Method:** Systematic experiments and theoretical analysis were conducted to understand the behavior of LLMs and to design a novel model called Lamini-1 that uses a massive mixture of memory experts for fact retrieval.

**Key Contributions:**

	1. Demonstration that traditional methods do not explain LLM hallucinations
	2. Introduction of the Lamini-1 model for dynamic fact retrieval
	3. Theoretical construction linking training loss to hallucination frequency

**Result:** The findings reveal that traditional methods fail to mitigate hallucinations effectively and that LLMs can easily memorize large datasets when augmented with a Mixture of Memory Experts. The new model, Lamini-1, was developed to dynamically retrieve facts and reduce hallucinations significantly.

**Limitations:** The study is based on the experimental analysis of specific datasets and may not generalize across all use cases.

**Conclusion:** The study provides insights into why LLMs hallucinate and emphasizes the need for new approaches, like Lamini-1, for improving LLM reliability.

**Abstract:** Despite their powerful chat, coding, and reasoning abilities, Large Language Models (LLMs) frequently hallucinate. Conventional wisdom suggests that hallucinations are a consequence of a balance between creativity and factuality, which can be mitigated, but not eliminated, by grounding the LLM in external knowledge sources. Through extensive systematic experiments, we show that these traditional approaches fail to explain why LLMs hallucinate in practice. Specifically, we show that LLMs augmented with a massive Mixture of Memory Experts (MoME) can easily memorize large datasets of random numbers. We corroborate these experimental findings with a theoretical construction showing that simple neural networks trained to predict the next token hallucinate when the training loss is above a threshold as it usually does in practice when training on internet scale data. We interpret our findings by comparing against traditional retrieval methods for mitigating hallucinations. We use our findings to design a first generation model for removing hallucinations -- Lamini-1 -- that stores facts in a massive mixture of millions of memory experts that are retrieved dynamically.

</details>


### [46] [TreeBoN: Enhancing Inference-Time Alignment with Speculative Tree-Search and Best-of-N Sampling](https://arxiv.org/abs/2410.16033)

*Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Chenhao Zhu, Xinzhe Juan, Ling Yang, Huazheng Wang, Kaixuan Huang, Yue Wu, Mengdi Wang*

**Main category:** cs.CL

**Keywords:** Best-of-N sampling, TreeBoN, large language models, computational efficiency, Direct Preference Optimization

**Relevance Score:** 7

**TL;DR:** TreeBoN is a novel framework enhancing Best-of-N sampling for large language models by incorporating a speculative tree-search strategy, improving response quality while reducing computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Inference-time alignment is crucial for enhancing large language model performance without additional training, but it often struggles with computational efficiency versus output quality.

**Method:** TreeBoN integrates a speculative tree-search strategy into Best-of-N sampling, maintaining parent nodes and pruning low-quality responses, while using token-level rewards from Direct Preference Optimization to guide the process.

**Key Contributions:**

	1. Introduces TreeBoN framework for improved Best-of-N sampling
	2. Reduces computational cost while maintaining high quality outputs
	3. Leverages token-level rewards for efficient tree expansion

**Result:** TreeBoN demonstrated consistent improvements across various datasets, achieving the highest win rate of 65% on TutorEval and around 60% on others, while matching the computational costs of standard BoN.

**Limitations:** 

**Conclusion:** TreeBoN effectively balances computational overhead and response quality, making it a scalable and efficient solution for large language model inference.

**Abstract:** Inference-time alignment enhances the performance of large language models without requiring additional training or fine-tuning but presents challenges due to balancing computational efficiency with high-quality output. Best-of-N (BoN) sampling, as a simple yet powerful approach, generates multiple responses and selects the best one, achieving improved performance but with a high computational cost. We propose TreeBoN, a novel framework that integrates a speculative tree-search strategy into Best-of-N (BoN) Sampling. TreeBoN maintains a set of parent nodes, iteratively branching and pruning low-quality responses, thereby reducing computational overhead while maintaining high output quality. Our approach also leverages token-level rewards from Direct Preference Optimization (DPO) to guide tree expansion and prune low-quality paths. We evaluate TreeBoN using AlpacaFarm, HH-RLHF, UltraFeedback, GSM8K, and TutorEval datasets, demonstrating consistent improvements. Specifically, TreeBoN achieves the highest win rate of 65% on TutorEval and around 60% win rates across other different datasets, outperforming standard BoN with the same computational cost and showcasing its scalability and alignment efficacy.

</details>


### [47] [Attacking Misinformation Detection Using Adversarial Examples Generated by Language Models](https://arxiv.org/abs/2410.20940)

*Piotr Przyby≈Ça, Euan McGill, Horacio Saggion*

**Main category:** cs.CL

**Keywords:** large language models, content moderation, adversarial examples, text classification, robustness testing

**Relevance Score:** 8

**TL;DR:** This paper explores the use of large language models to generate adversarial examples that can deceive content-filtering algorithms on social media, particularly those detecting low-credibility content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to determine if large language models can effectively attack content-filtering algorithms, thereby testing their robustness against various low-credibility content types.

**Method:** The proposed method, TREPAT, involves generating initial rephrasings through large language models using prompts based on meaning-preserving NLP tasks. These are then refined through a beam search procedure to induce changes in the victim classifier's decisions.

**Key Contributions:**

	1. Introduction of TREPAT for generating adversarial examples
	2. Demonstration of the effectiveness of LLMs in deceiving content filtering algorithms
	3. Comprehensive evaluation across quantitative, qualitative, and linguistic analyses

**Result:** The experiments demonstrate that the proposed approach outperforms traditional methods in constrained scenarios, particularly with longer input texts such as news articles.

**Limitations:** The study assumes a controlled environment which may not fully represent real-world scenarios; further research is needed to generalize findings.

**Conclusion:** The findings highlight the potential threats posed by large language models to content moderation systems, necessitating improvements in the robustness of such algorithms.

**Abstract:** Large language models have many beneficial applications, but can they also be used to attack content-filtering algorithms in social media platforms? We investigate the challenge of generating adversarial examples to test the robustness of text classification algorithms detecting low-credibility content, including propaganda, false claims, rumours and hyperpartisan news. We focus on simulation of content moderation by setting realistic limits on the number of queries an attacker is allowed to attempt. Within our solution (TREPAT), initial rephrasings are generated by large language models with prompts inspired by meaning-preserving NLP tasks, such as text simplification and style transfer. Subsequently, these modifications are decomposed into small changes, applied through beam search procedure, until the victim classifier changes its decision. We perform (1) quantitative evaluation using various prompts, models and query limits, (2) targeted manual assessment of the generated text and (3) qualitative linguistic analysis. The results confirm the superiority of our approach in the constrained scenario, especially in case of long input text (news articles), where exhaustive search is not feasible.

</details>


### [48] [Attention-guided Self-reflection for Zero-shot Hallucination Detection in Large Language Models](https://arxiv.org/abs/2501.09997)

*Qiang Liu, Xinlong Chen, Yue Ding, Bowen Song, Weiqiang Wang, Shu Wu, Liang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination detection, attention mechanisms, zero-shot learning

**Relevance Score:** 9

**TL;DR:** Introducing a method for zero-shot hallucination detection in Large Language Models using attention contributions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant barrier of hallucination in Large Language Models (LLMs) which impacts their effective application.

**Method:** The Attention-Guided SElf-Reflection (AGSER) approach uses attention contributions to categorize queries and compute consistency scores to estimate hallucination.

**Key Contributions:**

	1. Introduction of AGSER for zero-shot hallucination detection
	2. Utilization of attention contributions for categorizing queries
	3. Demonstrated performance improvements across multiple LLMs and benchmarks.

**Result:** The AGSER method significantly outperforms existing zero-shot hallucination detection methods across several benchmarks while reducing computational costs.

**Limitations:** 

**Conclusion:** AGSER effectively detects hallucinations in LLMs with fewer computational passes and improved performance.

**Abstract:** Hallucination has emerged as a significant barrier to the effective application of Large Language Models (LLMs). In this work, we introduce a novel Attention-Guided SElf-Reflection (AGSER) approach for zero-shot hallucination detection in LLMs. The AGSER method utilizes attention contributions to categorize the input query into attentive and non-attentive queries. Each query is then processed separately through the LLMs, allowing us to compute consistency scores between the generated responses and the original answer. The difference between the two consistency scores serves as a hallucination estimator. In addition to its efficacy in detecting hallucinations, AGSER notably reduces computational overhead, requiring only three passes through the LLM and utilizing two sets of tokens. We have conducted extensive experiments with four widely-used LLMs across three different hallucination benchmarks, demonstrating that our approach significantly outperforms existing methods in zero-shot hallucination detection.

</details>


### [49] [FedP$^2$EFT: Federated Learning to Personalize PEFT for Multilingual LLMs](https://arxiv.org/abs/2502.04387)

*Royson Lee, Minyoung Kim, Fady Rezk, Rui Li, Stylianos I. Venieris, Timothy Hospedales*

**Main category:** cs.CL

**Keywords:** Federated learning, Multilingual LLMs, Personalization, Bayesian sparse rank selection, Parameter-efficient fine-tuning

**Relevance Score:** 8

**TL;DR:** FedP^2EFT is a method for personalizing multilingual LLMs in federated learning settings, using Bayesian sparse rank selection for optimal structure learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of personalizing multilingual LLMs in low-resource language environments without the need for manual configuration of personalization strategies.

**Method:** FedP^2EFT employs a federated learning framework that uses Bayesian sparse rank selection to learn optimal personalized parameter-efficient fine-tuning (PEFT) structures for each client.

**Key Contributions:**

	1. Introduction of FedP^2EFT for multilingual LLMs in federated learning
	2. Utilization of Bayesian sparse rank selection for personalized fine-tuning
	3. Demonstrated superiority over existing personalized fine-tuning approaches.

**Result:** FedP^2EFT demonstrates significant improvements over existing personalized fine-tuning methods in multilingual federated learning benchmarks.

**Limitations:** 

**Conclusion:** The evaluations suggest that FedP^2EFT offers a robust solution for optimizing PEFT structures in multilingual federated learning settings, successfully avoiding overfitting to low-data issues.

**Abstract:** Federated learning (FL) has enabled the training of multilingual large language models (LLMs) on diverse and decentralized multilingual data, especially on low-resource languages. To improve client-specific performance, personalization via the use of parameter-efficient fine-tuning (PEFT) modules such as LoRA is common. This involves a personalization strategy (PS), such as the design of the PEFT adapter structures (e.g., in which layers to add LoRAs and what ranks) and choice of hyperparameters (e.g., learning rates) for fine-tuning. Instead of manual PS configuration, we propose FedP$^2$EFT, a federated learning-to-personalize method for multilingual LLMs in cross-device FL settings. Unlike most existing PEFT structure selection methods, which are prone to overfitting low-data regimes, FedP$^2$EFT collaboratively learns the optimal personalized PEFT structure for each client via Bayesian sparse rank selection. Evaluations on both simulated and real-world multilingual FL benchmarks demonstrate that FedP$^2$EFT largely outperforms existing personalized fine-tuning methods, while complementing other existing FL methods. Code is available at https://github.com/SamsungLabs/fedp2eft.

</details>


### [50] [FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching](https://arxiv.org/abs/2502.11128)

*Hui Wang, Shujie Liu, Lingwei Meng, Jinyu Li, Yifan Yang, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, Yan Lu, Yong Qin*

**Main category:** cs.CL

**Keywords:** Token modeling, Text-to-speech, Flow matching, Continuous-valued tokens, Autoregressive models

**Relevance Score:** 4

**TL;DR:** FELLE is an autoregressive model that combines language modeling with token-wise flow matching to improve the generation of continuous-valued tokens, specifically mel-spectrograms, enhancing temporal coherence and synthesis quality in TTS applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve continuous-valued token modeling and enforce temporal coherence in text-to-speech (TTS) generation.

**Method:** FELLE integrates autoregressive language modeling with token-wise flow matching, employing a coarse-to-fine flow-matching mechanism to predict continuous-valued tokens hierarchically based on language model outputs.

**Key Contributions:**

	1. Introduction of FELLE for continuous-valued token modeling
	2. Combination of autoregressive modeling with flow matching for improved coherence
	3. Coarse-to-fine flow-matching mechanism for hierarchical token prediction

**Result:** FELLE demonstrates significant improvements in TTS generation quality by incorporating flow-matching techniques, as evidenced by experimental results.

**Limitations:** 

**Conclusion:** The research highlights the effectiveness of using flow matching in autoregressive mel-spectrogram modeling, leading to enhanced TTS synthesis quality.

**Abstract:** To advance continuous-valued token modeling and temporal-coherence enforcement, we propose FELLE, an autoregressive model that integrates language modeling with token-wise flow matching. By leveraging the autoregressive nature of language models and the generative efficacy of flow matching, FELLE effectively predicts continuous-valued tokens (mel-spectrograms). For each continuous-valued token, FELLE modifies the general prior distribution in flow matching by incorporating information from the previous step, improving coherence and stability. Furthermore, to enhance synthesis quality, FELLE introduces a coarse-to-fine flow-matching mechanism, generating continuous-valued tokens hierarchically, conditioned on the language model's output. Experimental results demonstrate the potential of incorporating flow-matching techniques in autoregressive mel-spectrogram modeling, leading to significant improvements in TTS generation quality, as shown in https://aka.ms/felle.

</details>


### [51] [Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs](https://arxiv.org/abs/2502.18179)

*Gaye Colakoglu, G√ºrkan Solmaz, Jonathan F√ºrst*

**Main category:** cs.CL

**Keywords:** information extraction, layout-aware, large language models, benchmarking, test suite

**Relevance Score:** 9

**TL;DR:** This paper explores the design space for information extraction from layout-rich documents using LLMs, identifying core challenges and proposing a new test suite for benchmarking.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of information extraction from layout-rich documents using LLMs.

**Method:** The study analyzes methods for input representation, chunking, prompting, model selection, and output refinement, using LayIE-LLM, a new test suite.

**Key Contributions:**

	1. Development of LayIE-LLM, a layout-aware IE test suite
	2. Proposed one-factor-at-a-time method for optimized configurations
	3. Demonstration that LLMs can achieve specialized model performance with proper configuration

**Result:** The optimized configuration from LayIE-LLM shows a significant performance improvement, achieving 13.3‚Äì37.5 F1 points beyond a baseline.

**Limitations:** 

**Conclusion:** Well-configured general-purpose LLMs can match specialized models for layout-aware information extraction, offering a cost-effective approach without the need for fine-tuning.

**Abstract:** This paper defines and explores the design space for information extraction (IE) from layout-rich documents using large language models (LLMs). The three core challenges of layout-aware IE with LLMs are 1) data structuring, 2) model engagement, and 3) output refinement. Our study investigates the sub-problems and methods within these core challenges, such as input representation, chunking, prompting, selection of LLMs, and multimodal models. It examines the effect of different design choices through LayIE-LLM, a new, open-source, layout-aware IE test suite, benchmarking against traditional, fine-tuned IE models. The results on two IE datasets show that LLMs require adjustment of the IE pipeline to achieve competitive performance: the optimized configuration found with LayIE-LLM achieves 13.3--37.5 F1 points more than a general-practice baseline configuration using the same LLM. To find a well-working configuration, we develop a one-factor-at-a-time (OFAT) method that achieves near-optimal results. Our method is only 0.8--1.8 points lower than the best full factorial exploration with a fraction (2.8%) of the required computation. Overall, we demonstrate that, if well-configured, general-purpose LLMs match the performance of specialized models, providing a cost-effective, finetuning-free alternative. Our test-suite is available at https://github.com/gayecolakoglu/LayIE-LLM.

</details>
